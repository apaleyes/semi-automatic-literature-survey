doi,publication_date,publication,publisher,title,abstract,database
10.1145/2487575.2488200,2013-08-11,p,ACM,ad click prediction a view from the trenches," Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.",project-academic
10.1109/ICRA.2019.8794127,2019-05-20,p,IEEE,residual reinforcement learning for robot control," Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.",project-academic
,2018-12-07,a,,residual reinforcement learning for robot control," Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.",project-academic
,2017-07-13,b,,display advertising with real time bidding rtb and behavioural targeting," Online advertising is now one of the fastest advancing areas in the IT industry. In display and mobile advertising, the most significant technical development in recent years is the growth of Real-Time Bidding (RTB), which facilitates a real-time auction for a display opportunity. RTB essentially facilitates buying an individual ad impression in real time while it is still being generated from a users visit. RTB not only scales up the buying process by aggregating a large number of available inventories across publishers but, most importantly, enables direct targeting of individual users. As such, RTB has fundamentally changed the landscape of digital marketing. Scientifically, the demand for automation, integration and optimization in RTB also brings new research opportunities in information retrieval, data mining, machine learning and other related fields. Despite its rapid growth and huge potential, many aspects of RTB remain unknown to the research community for a variety of reasons. This monograph offers insightful knowledge of real-world systems, to bridge the gaps between industry and academia, and to provide an overview of the fundamental infrastructure, algorithms, and technical and research challenges of the new frontier of computational advertising. The topics covered include user response prediction, bid landscape forecasting, bidding algorithms, revenue optimization, statistical arbitrage, dynamic pricing, and ad fraud detection. This is an invaluable text for researchers and practitioners alike. Academic researchers will get a better understanding of the real-time online advertising systems currently deployed in industry. While industry practitioners are introduced to the research challenges, the state of the art algorithms and potential future systems in this field.",project-academic
10.1109/JIOT.2019.2912022,2019-04-18,a,IEEE,machine learning based network vulnerability analysis of industrial internet of things," It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",project-academic
,2019-11-13,a,,machine learning based network vulnerability analysis of industrial internet of things," It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of machine learning in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using machine learning models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a machine learning based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",project-academic
10.1109/LRA.2019.2903261,2019-03-06,p,IEEE,primal pathfinding via reinforcement and imitation multi agent learning," Multi-agent path finding (MAPF) is an essential component of many large-scale, real-world robot deployments, from aerial swarms to warehouse automation. However, despite the community's continued efforts, most state-of-the-art MAPF planners still rely on centralized planning and scale poorly past a few hundred agents. Such planning approaches are maladapted to real-world deployments, where noise and uncertainty often require paths be recomputed online, which is impossible when planning times are in seconds to minutes. We present PRIMAL, a novel framework for MAPF that combines reinforcement and imitation learning to teach fully decentralized policies, where agents reactively plan paths online in a partially observable world while exhibiting implicit coordination. This framework extends our previous work on distributed learning of collaborative policies by introducing demonstrations of an expert MAPF planner during training, as well as careful reward shaping and environment sampling. Once learned, the resulting policy can be copied onto any number of agents and naturally scales to different team sizes and world dimensions. We present results on randomized worlds with up to 1024 agents and compare success rates against state-of-the-art MAPF planners. Finally, we experimentally validate the learned policies in a hybrid simulation of a factory mockup, involving both real world and simulated robots.",project-academic
10.2139/SSRN.3547322,2020-01-01,a,,politics of adversarial machine learning," In addition to their security properties, adversarial machine-learning attacks and defenses have political dimensions. They enable or foreclose certain options for both the subjects of the machine learning systems and for those who deploy them, creating risks for civil liberties and human rights. In this paper, we draw on insights from science and technology studies, anthropology, and human rights literature, to inform how defenses against adversarial attacks can be used to suppress dissent and limit attempts to investigate machine learning systems. To make this concrete, we use real-world examples of how attacks such as perturbation, model inversion, or membership inference can be used for socially desirable ends. Although the predictions of this analysis may seem dire, there is hope. Efforts to address human rights concerns in the commercial spyware industry provide guidance for similar measures to ensure ML systems serve democratic, not authoritarian ends.",project-academic
,2020-02-01,a,,politics of adversarial machine learning," In addition to their security properties, adversarial machine-learning attacks and defenses have political dimensions. They enable or foreclose certain options for both the subjects of the machine learning systems and for those who deploy them, creating risks for civil liberties and human rights. In this paper, we draw on insights from science and technology studies, anthropology, and human rights literature, to inform how defenses against adversarial attacks can be used to suppress dissent and limit attempts to investigate machine learning systems. To make this concrete, we use real-world examples of how attacks such as perturbation, model inversion, or membership inference can be used for socially desirable ends. Although the predictions of this analysis may seem dire, there is hope. Efforts to address human rights concerns in the commercial spyware industry provide guidance for similar measures to ensure ML systems serve democratic, not authoritarian ends",project-academic
10.1109/BIGDATA.2015.7363874,2015-10-29,p,IEEE,automotive big data applications workloads and infrastructures," Data is increasingly affecting the automotive industry, from vehicle development, to manufacturing and service processes, to online services centered around the connected vehicle. Connected, mobile and Internet of Things devices and machines generate immense amounts of sensor data. The ability to process and analyze this data to extract insights and knowledge that enable intelligent services, new ways to understand business problems, improvements of processes and decisions, is a critical capability. Hadoop is a scalable platform for compute and storage and emerged as de-facto standard for Big Data processing at Internet companies and in the scientific community. However, there is a lack of understanding of how and for what use cases these new Hadoop capabilities can be efficiently used to augment automotive applications and systems. This paper surveys use cases and applications for deploying Hadoop in the automotive industry. Over the years a rich ecosystem emerged around Hadoop comprising tools for parallel, in-memory and stream processing (most notable MapReduce and Spark), SQL and NOSQL engines (Hive, HBase), and machine learning (Mahout, MLlib). It is critical to develop an understanding of automotive applications and their characteristics and requirements for data discovery, integration, exploration and analytics. We then map these requirements to a confined technical architecture consisting of core Hadoop services and libraries for data ingest, processing and analytics. The objective of this paper is to address questions, such as: What applications and datasets are suitable for Hadoop? How can a diverse set of frameworks and tools be managed on multi-tenant Hadoop cluster? How do these tools integrate with existing relational data management systems? How can enterprise security requirements be addressed? What are the performance characteristics of these tools for real-world automotive applications? To address the last question, we utilize a standard benchmark (TPCx-HS), and two application benchmarks (SQL and machine learning) that operate on a dataset of multiple Terabytes and billions of rows.",project-academic
10.1109/ACCESS.2017.2678990,2017-03-07,a,IEEE,adaptive scheme for caching youtube content in a cellular network machine learning approach," Content caching at base stations is a promising solution to address the large demands for mobile data services over cellular networks. Content caching is a challenging problem as it requires predicting the future popularity of the content and the operating characteristics of the cellular networks. In this paper, we focus on constructing an algorithm that improves the users’ quality of experience (QoE) and reduces network traffic. The algorithm accounts for users’ behavior and properties of the cellular network (e.g. cache size, bandwidth, and load). The constructed content and network aware adaptive caching scheme uses an extreme-learning machine neural network to estimate the popularity of content, and mixed-integer linear programming to compute where to place the content and select the physical cache sizes in the network. The proposed caching scheme simultaneously performs efficient cache deployment and content caching. Additionally, a simultaneous perturbation stochastic approximation method is developed to reduce the number of neurons in the extreme-learning machine method while ensuring a sufficient predictive performance is maintained. Using real-world data from YouTube and a NS-3 simulator, we demonstrate how the caching scheme improves the QoE of users and network performance compared with industry standard caching schemes.",project-academic
10.1109/SANER.2019.8668044,2019-03-15,p,Institute of Electrical and Electronics Engineers Inc.,deepct tomographic combinatorial testing for deep learning systems," Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.",project-academic
10.1145/3357223.3362707,2019-11-20,p,ACM,bigdl a distributed deep learning framework for big data," ThispaperpresentsBigDL (adistributeddeeplearning framework for Apache Spark), which has been used by a variety of users in the industry for building deep learning applications on production big data platforms. It allows deep learning applications to run on the Apache Hadoop/Spark cluster so as to directly process the production data, and as a part of the end-to-end data analysis pipeline for deployment and management. Unlike existing deep learning frameworks, BigDL implements distributed, data parallel training directly on top of the functional compute model (with copy-on-write and coarse-grained operations) of Spark. We also share real-world experience and ""war stories"" of users that havead-optedBigDLtoaddresstheirchallenges(i.e., howtoeasilybuildend-to-enddataanalysisanddeep learning pipelines for their production data).",project-academic
10.1145/3292500.3330725,2019-07-25,p,ACM,towards knowledge based personalized product description generation in e commerce," Quality product descriptions are critical for providing competitive customer experience in an E-commerce platform. An accurate and attractive description not only helps customers make an informed decision but also improves the likelihood of purchase. However, crafting a successful product description is tedious and highly time-consuming. Due to its importance, automating the product description generation has attracted considerable interest from both research and industrial communities. Existing methods mainly use templates or statistical methods, and their performance could be rather limited. In this paper, we explore a new way to generate personalized product descriptions by combining the power of neural networks and knowledge base. Specifically, we propose a KnOwledge Based pErsonalized (or KOBE) product description generation model in the context of E-commerce. In KOBE, we extend the encoder-decoder framework, the Transformer, to a sequence modeling formulation using self-attention. In order to make the description both informative and personalized, KOBE considers a variety of important factors during text generation, including product aspects, user categories, and knowledge base. Experiments on real-world datasets demonstrate that the proposed method outperforms the baseline on various metrics. KOBE can achieve an improvement of 9.7% over state-of-the-arts in terms of BLEU. We also present several case studies as the anecdotal evidence to further prove the effectiveness of the proposed approach. The framework has been deployed in Taobao, the largest online E-commerce platform in China.",project-academic
10.1109/IROS45743.2020.9340876,2020-10-24,p,IEEE,mapper multi agent path planning with evolutionary reinforcement learning in mixed dynamic environments," Multi-agent navigation in dynamic environments is of great industrial value when deploying a large scale fleet of robot to real-world applications. This paper proposes a decentralized partially observable multi-agent path planning with evolutionary reinforcement learning (MAPPER) method to learn an effective local planning policy in mixed dynamic environments. Reinforcement learning-based methods usually suffer performance degradation on long-horizon tasks with goal-conditioned sparse rewards, so we decompose the long-range navigation task into many easier sub-tasks under the guidance of a global planner, which increases agents’ performance in large environments. Moreover, most existing multi-agent planning approaches assume either perfect information of the surrounding environment or homogeneity of nearby dynamic agents, which may not hold in practice. Our approach models dynamic obstacles’ behavior with an image-based representation and trains a policy in mixed dynamic environments without homogeneity assumption. To ensure multi-agent training stability and performance, we propose an evolutionary training approach that can be easily scaled to large and complex environments. Experiments show that MAPPER is able to achieve higher success rates and more stable performance when exposed to a large number of non-cooperative dynamic obstacles compared with traditional reaction-based planner LRA* and the state-of-the-art learning-based method.",project-academic
,2020-07-30,a,,mapper multi agent path planning with evolutionary reinforcement learning in mixed dynamic environments," Multi-agent navigation in dynamic environments is of great industrial value when deploying a large scale fleet of robot to real-world applications. This paper proposes a decentralized partially observable multi-agent path planning with evolutionary reinforcement learning (MAPPER) method to learn an effective local planning policy in mixed dynamic environments. Reinforcement learning-based methods usually suffer performance degradation on long-horizon tasks with goal-conditioned sparse rewards, so we decompose the long-range navigation task into many easier sub-tasks under the guidance of a global planner, which increases agents' performance in large environments. Moreover, most existing multi-agent planning approaches assume either perfect information of the surrounding environment or homogeneity of nearby dynamic agents, which may not hold in practice. Our approach models dynamic obstacles' behavior with an image-based representation and trains a policy in mixed dynamic environments without homogeneity assumption. To ensure multi-agent training stability and performance, we propose an evolutionary training approach that can be easily scaled to large and complex environments. Experiments show that MAPPER is able to achieve higher success rates and more stable performance when exposed to a large number of non-cooperative dynamic obstacles compared with traditional reaction-based planner LRA* and the state-of-the-art learning-based method.",project-academic
10.1145/2623330.2623377,2014-08-24,p,ACM,a system to grade computer programming skills using machine learning," The automatic evaluation of computer programs is a nascent area of research with a potential for large-scale impact. Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In this paper, we present a system to grade computer programs automatically. In addition to grading a program on its programming practices and complexity, the key kernel of the system is a machine-learning based algorithm which determines closeness of the logic of the given program to a correct program. This algorithm uses a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts. We show that the regression models provide much better grading than the ubiquitous test-case-pass based grading and rivals the grading accuracy of other open-response problems such as essay grading . We also show that our novel features add significant value over and above basic keyword/expression count features. In addition to this, we propose a novel way of posing computer-program grading as a one-class modeling problem and report encouraging preliminary results. We show the value of the system through a case study in a real-world industrial deployment. To the best of the authors' knowledge, this is the first time a system using machine learning has been developed and used for grading programs. The work is timely with regard to the recent boom in Massively Online Open Courseware (MOOCs), which promises to produce a significant amount of hand-graded digitized data.",project-academic
10.1109/ICDM50108.2020.00084,2020-11-01,p,IEEE,autonomous graph mining algorithm search with best speed accuracy trade off," Graph data is ubiquitous in academia and industry, from social networks to bioinformatics. The pervasiveness of graphs today has raised the demand for algorithms that can answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers to increase their public reputation? Myriads of new graph mining algorithms are proposed every year to answer such questions — each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for a practitioner to compare different algorithms and pick the most suitable one for a specific application. These challenges — even more severe for non-experts — create a gap in which state-of-the-art techniques developed in academic settings fail to be optimally deployed in real-world applications. To bridge this gap, we propose AutoGM, an automated system for graph mining algorithm development. We first define a unified framework UnifiedGM that integrates various message-passing based graph algorithms, ranging from conventional algorithms like PageRank to graph neural networks. Then UnifiedGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AutoGM explicitly optimizes for the optimal parameter set of UnifiedGM using Bayesian Optimization. AutoGM defines a novel budget-aware objective function for the optimization to incorporate a practical issue — finding the best speed-accuracy trade-off under a computation budget - into the graph algorithm generation problem. Experiments on real-world benchmark datasets demonstrate that AutoGM generates novel graph mining algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.",project-academic
,2020-11-26,a,,autonomous graph mining algorithm search with best speed accuracy trade off," Graph data is ubiquitous in academia and industry, from social networks to bioinformatics. The pervasiveness of graphs today has raised the demand for algorithms that can answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers to increase their public reputation? Myriads of new graph mining algorithms are proposed every year to answer such questions - each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for a practitioner to compare different algorithms and pick the most suitable one for a specific application. These challenges - even more severe for non-experts - create a gap in which state-of-the-art techniques developed in academic settings fail to be optimally deployed in real-world applications. To bridge this gap, we propose AUTOGM, an automated system for graph mining algorithm development. We first define a unified framework UNIFIEDGM that integrates various message-passing based graph algorithms, ranging from conventional algorithms like PageRank to graph neural networks. Then UNIFIEDGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AUTOGM explicitly optimizes for the optimal parameter set of UNIFIEDGM using Bayesian Optimization. AUTOGM defines a novel budget-aware objective function for the optimization to incorporate a practical issue - finding the best speed-accuracy trade-off under a computation budget - into the graph algorithm generation problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM generates novel graph mining algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.",project-academic
10.1109/TII.2020.3017668,2021-07-01,a,IEEE,low latency federated learning and blockchain for edge association in digital twin empowered 6g networks," Emerging technologies, such as digital twins and 6th generation (6G) mobile networks, have accelerated the realization of edge intelligence in industrial Internet of Things (IIoT). The integration of digital twin and 6G bridges the physical system with digital space and enables robust instant wireless connectivity. With increasing concerns on data privacy, federated learning has been regarded as a promising solution for deploying distributed data processing and learning in wireless networks. However, unreliable communication channels, limited resources, and lack of trust among users hinder the effective application of federated learning in IIoT. In this article, we introduce the digital twin wireless networks (DTWN) by incorporating digital twins into wireless networks, to migrate real-time data processing and computation to the edge plane. Then, we propose a blockchain empowered federated learning framework running in the DTWN for collaborative computing, which improves the reliability and security of the system and enhances data privacy. Moreover, to balance the learning accuracy and time cost of the proposed scheme, we formulate an optimization problem for edge association by jointly considering digital twin association, training data batch size, and bandwidth allocation. We exploit multiagent reinforcement learning to find an optimal solution to the problem. Numerical results on real-world dataset show that the proposed scheme yields improved efficiency and reduced cost compared to benchmark learning methods.",project-academic
10.1145/3397271.3401440,2020-07-25,p,ACM,user behavior retrieval for click through rate prediction," Click-through rate (CTR) prediction plays a key role in modern online personalization services. In practice, it is necessary to capture user's drifting interests by modeling sequential user behaviors to build an accurate CTR prediction model. However, as the users accumulate more and more behavioral data on the platforms, it becomes non-trivial for the sequential models to make use of the whole behavior history of each user. First, directly feeding the long behavior sequence will make online inference time and system load infeasible. Second, there is much noise in such long histories to fail the sequential model learning. The current industrial solutions mainly truncate the sequences and just feed recent behaviors to the prediction model, which leads to a problem that sequential patterns such as periodicity or long-term dependency are not embedded in the recent several behaviors but in far back history. To tackle these issues, in this paper we consider it from the data perspective instead of just designing more sophisticated yet complicated models and propose User Behavior Retrieval for CTR prediction (UBR4CTR) framework. In UBR4CTR, the most relevant and appropriate user behaviors will be firstly retrieved from the entire user history sequence using a learnable search method. These retrieved behaviors are then fed into a deep model to make the final prediction instead of simply using the most recent ones. It is highly feasible to deploy UBR4CTR into industrial model pipeline with low cost. Experiments on three real-world large-scale datasets demonstrate the superiority and efficacy of our proposed framework and models.",project-academic
10.1145/3269206.3269310,2018-10-17,p,Association for Computing Machinery,correlated time series forecasting using multi task deep neural networks," Cyber-physical systems often consist of entities that interact with each other over time. Meanwhile, as part of the continued digitization of industrial processes, various sensor technologies are deployed that enable us to record time-varying attributes (a.k.a., time series) of such entities, thus producing correlated time series. To enable accurate forecasting on such correlated time series, this paper proposes two models that combine convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The first model employs a CNN on each individual time series, combines the convoluted features, and then applies an RNN on top of the convoluted features in the end to enable forecasting. The second model adds additional auto-encoders into the individual CNNs, making the second model a multi-task learning model, which provides accurate and robust forecasting. Experiments on a large real-world correlated time series data set suggest that the proposed two models are effective and outperform baselines in most settings.",project-academic
10.1109/JIOT.2019.2963635,2020-01-01,a,IEEE,toward edge based deep learning in industrial internet of things," As a typical application of the Internet of Things (IoT), the Industrial IoT (IIoT) connects all the related IoT sensing and actuating devices ubiquitously so that the monitoring and control of numerous industrial systems can be realized. Deep learning, as one viable way to carry out big-data-driven modeling and analysis, could be integrated in IIoT systems to aid the automation and intelligence of IIoT systems. As deep learning requires large computation power, it is commonly deployed in cloud servers. Thus, the data collected by IoT devices must be transmitted to the cloud for training process, contributing to network congestion and affecting the IoT network performance as well as the supported applications. To address this issue, in this article, we leverage the fog/edge computing paradigm and propose an edge computing-based deep learning model, which utilizes edge computing to migrate the deep learning process from cloud servers to edge nodes, reducing data transmission demands in the IIoT network and mitigating network congestion. Since edge nodes have limited computation ability compared to servers, we design a mechanism to optimize the deep learning model so that its requirements for computational power can be reduced. To evaluate our proposed solution, we design a testbed implemented in the Google cloud and deploy the proposed convolutional neural network (CNN) model, utilizing a real-world IIoT data set to evaluate our approach. 1 None Our experimental results confirm the effectiveness of our approach, which cannot only reduce the network traffic overhead for IIoT but also maintain the classification accuracy in comparison with several baseline schemes. None 1 None Certain commercial equipment, instruments, or materials are identified in this article in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.",project-academic
,2018-07-06,a,,testing untestable neural machine translation an industrial case," Neural Machine Translation (NMT) has been widely adopted recently due to its advantages compared with the traditional Statistical Machine Translation (SMT). However, an NMT system still often produces translation failures due to the complexity of natural language and sophistication in designing neural networks. While in-house black-box system testing based on reference translations (i.e., examples of valid translations) has been a common practice for NMT quality assurance, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracle for in-vivo testing of an NMT system, in this paper, we propose a new approach for automatically identifying translation failures, without requiring reference translations for a translation task; our approach can directly serve as a test oracle for in-vivo testing. Our approach focuses on properties of natural language translation that can be checked systematically and uses information from both the test inputs (i.e., the texts to be translated) and the test outputs (i.e., the translations under inspection) of the NMT system. Our evaluation conducted on real-world datasets shows that our approach can effectively detect targeted property violations as translation failures. Our experiences on deploying our approach in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our approach along with high industry impact.",project-academic
,2020-08-14,a,,loghub a large collection of system log datasets towards automated log analytics," Logs have been widely adopted in software system development and maintenance because of the rich system runtime information they contain. In recent years, the increase of software size and complexity leads to the rapid growth of the volume of logs. To handle these large volumes of logs efficiently and effectively, a line of research focuses on intelligent log analytics powered by AI (artificial intelligence) techniques. However, only a small fraction of these techniques have reached successful deployment in industry because of the lack of public log datasets and necessary benchmarking upon them. To fill this significant gap between academia and industry and also facilitate more research on AI-powered log analytics, we have collected and organized loghub, a large collection of log datasets. In particular, loghub provides 17 real-world log datasets collected from a wide range of systems, including distributed systems, supercomputers, operating systems, mobile systems, server applications, and standalone software. In this paper, we summarize the statistics of these datasets, introduce some practical log usage scenarios, and present a case study on anomaly detection to demonstrate how loghub facilitates the research and practice in this field. Up to the time of this paper writing, loghub datasets have been downloaded over 15,000 times by more than 380 organizations from both industry and academia.",project-academic
10.1145/3292500.3330746,2019-07-25,p,ACM,dynamic pricing for airline ancillaries with customer context," Ancillaries have become a major source of revenue and profitability in the travel industry. Yet, conventional pricing strategies are based on business rules that are poorly optimized and do not respond to changing market conditions. This paper describes the dynamic pricing model developed by Deepair solutions, an AI technology provider for travel suppliers. We present a pricing model that provides dynamic pricing recommendations specific to each customer interaction and optimizes expected revenue per customer. The unique nature of personalized pricing provides the opportunity to search over the market space to find the optimal price-point of each ancillary for each customer, without violating customer privacy. In this paper, we present and compare three approaches for dynamic pricing of ancillaries, with increasing levels of sophistication: (1) a two-stage forecasting and optimization model using a logistic mapping function; (2) a two-stage model that uses a deep neural network for forecasting, coupled with a revenue maximization technique using discrete exhaustive search; (3) a single-stage end-to-end deep neural network that recommends the optimal price. We describe the performance of these models based on both offline and online evaluations. We also measure the real-world business impact of these approaches by deploying them in an A/B test on an airline's internet booking website. We show that traditional machine learning techniques outperform human rule-based approaches in an online setting by improving conversion by 36% and revenue per offer by 10%. We also provide results for our offline experiments which show that deep learning algorithms outperform traditional machine learning techniques for this problem. Our end-to-end deep learning model is currently being deployed by the airline in their booking system.",project-academic
,2019-02-06,a,,dynamic pricing for airline ancillaries with customer context," Ancillaries have become a major source of revenue and profitability in the travel industry. Yet, conventional pricing strategies are based on business rules that are poorly optimized and do not respond to changing market conditions. This paper describes the dynamic pricing model developed by Deepair solutions, an AI technology provider for travel suppliers. We present a pricing model that provides dynamic pricing recommendations specific to each customer interaction and optimizes expected revenue per customer. The unique nature of personalized pricing provides the opportunity to search over the market space to find the optimal price-point of each ancillary for each customer, without violating customer privacy. In this paper, we present and compare three approaches for dynamic pricing of ancillaries, with increasing levels of sophistication: (1) a two-stage forecasting and optimization model using a logistic mapping function; (2) a two-stage model that uses a deep neural network for forecasting, coupled with a revenue maximization technique using discrete exhaustive search; (3) a single-stage end-to-end deep neural network that recommends the optimal price. We describe the performance of these models based on both offline and online evaluations. We also measure the real-world business impact of these approaches by deploying them in an A/B test on an airline's internet booking website. We show that traditional machine learning techniques outperform human rule-based approaches in an online setting by improving conversion by 36% and revenue per offer by 10%. We also provide results for our offline experiments which show that deep learning algorithms outperform traditional machine learning techniques for this problem. Our end-to-end deep learning model is currently being deployed by the airline in their booking system.",project-academic
10.1145/2365952.2365958,2012-09-09,p,ACM,building industrial scale real world recommender systems," In 2006, Netflix announced the Netflix Prize, a machine learning and data mining competition for movie rating prediction. We offered $1 million to whoever improved the accuracy of our existing system called Cinematch by 10%. We conducted this competition to find new ways to improve the recommendations we provide to our members, which is a key part of our business. However, we had to come up with a proxy question that was easier to evaluate and quantify: the root mean squared error (RMSE) of the predicted rating. A year into the competition, the Korbell team won the first Progress Prize with an 8.43% improvement. They reported more than 2000 hours of work in order to come up with the final combination of 107 algorithms that gave them this prize. And, they gave us the source code. We looked at the two underlying algorithms with the best performance in the ensemble. To put these algorithms to use, we had to work to overcome some limitations, for instance that they were built to handle 100 million ratings, instead of the more than 5 billion that we have, and that they were not built to adapt as members added more ratings. But once we overcame those challenges, we put the two algorithms into production, where they are still used as part of our recommendation engine.You might be wondering what happened with the final Grand Prize ensemble that won the $1M two years later. This is a truly impressive compilation and culmination of years of work, blending hundreds of predictive models to finally cross the finish line. We evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment.This example highlights the fact that, besides improving offline metrics such as the RMSE, recommender systems need to take into account other practical issues such as scalability or deployment. In this tutorial, we go over some of those practical issues that many times are as important as the theory, if not more, in order to build an industrial-scale real-world recommender system.",project-academic
10.14722/NDSS.2021.24525,2021-01-01,p,Internet Society,data poisoning attacks to deep learning based recommender systems," Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance. 
In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three real-world datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.",project-academic
10.1016/J.ISCI.2018.06.010,2018-07-27,a,Elsevier,data and power efficient intelligence with neuromorphic learning machines," The success of deep networks and recent industry involvement in brain-inspired computing is igniting a widespread interest in neuromorphic hardware that emulates the biological processes of the brain on an electronic substrate. This review explores interdisciplinary approaches anchored in machine learning theory that enable the applicability of neuromorphic technologies to real-world, human-centric tasks. We find that (1) recent work in binary deep networks and approximate gradient descent learning are strikingly compatible with a neuromorphic substrate; (2) where real-time adaptability and autonomy are necessary, neuromorphic technologies can achieve significant advantages over main-stream ones; and (3) challenges in memory technologies, compounded by a tradition of bottom-up approaches in the field, block the road to major breakthroughs. We suggest that a neuromorphic learning framework, tuned specifically for the spatial and temporal constraints of the neuromorphic substrate, will help guiding hardware algorithm co-design and deploying neuromorphic hardware for proactive learning of real-world data.",project-academic
10.1016/J.INFFUS.2018.11.020,2019-12-01,a,Elsevier,data fusion based coverage optimization in heterogeneous sensor networks a survey," Abstract None None Sensor networks, as a promising network paradigm, have been widely applied in a great deal of critical real-world applications. A key challenge in sensor networks is how to improve and optimize coverage quality which is a fundamental metric to characterize how well a point or a region or a barrier can be sensed by the geographically deployed heterogeneous sensors. Because of the resource-limited, battery-powered and type-diverse features of the sensors, maintaining and optimizing coverage quality includes a significant amount of challenges in heterogeneous sensor networks. Many researchers from both academic and industrial communities have performed numerous significant works on coverage optimization problem in the past decades. Some of them also have surveyed the current models, theories and solutions on the problem of coverage optimization. However, most of the existing surveys and analytical studies ignore how to exploit data fusion and cooperation of the deployed sensors to enhance coverage performance. In this paper, we provide an insightful and comprehensive summarization and classification on the data fusion based coverage optimization problem and techniques. Aiming at overcoming the shortcomings existed in current solutions, we also discuss the future issues and challenges in this area and sketch a general research framework in the context of reinforcement learning.",project-academic
,2019-08-27,a,,real world conversational ai for hotel bookings," In this paper, we present a real-world conversational AI system to search for and book hotels through text messaging. Our architecture consists of a frame-based dialogue management system, which calls machine learning models for intent classification, named entity recognition, and information retrieval subtasks. Our chatbot has been deployed on a commercial scale, handling tens of thousands of hotel searches every day. We describe the various opportunities and challenges of developing a chatbot in the travel industry.",project-academic
10.1109/AI4I46381.2019.00022,2019-09-01,p,IEEE,real world conversational ai for hotel bookings," In this paper, we present a real-world conversational AI system to search for and book hotels through text messaging. Our architecture consists of a frame-based dialogue management system, which calls machine learning models for intent classification, named entity recognition, and information retrieval subtasks. Our chatbot has been deployed on a commercial scale, handling tens of thousands of hotel searches every day. We describe the various opportunities and challenges of developing a chatbot in the travel industry.",project-academic
10.1016/J.ENGAPPAI.2014.10.010,2015-02-01,a,Pergamon,efficient web service qos prediction using local neighborhood matrix factorization," Abstract None None In the era of Big Data, companies worldwide are actively deploying web services in both intranet and internet environments. Quality-of-Service (QoS), the fundamental aspect of web service has thus attracted numerous attention in industry and academia. The study on sufficient QoS data keeps advancing the state in Service-Oriented Computing (SOC) area. To collect a large amount of resource in practice, QoS prediction applications are designed and built. Nevertheless, how to generate accurate results in high productivity is still a main challenge to existing frameworks. In this paper, we propose LoNMF, a Local Neighborhood Matrix Factorization application that incorporates domain knowledge in modern Artificial Intelligence (AI) technique to tackle this challenge. LoNMF first proposes a two-level selection mechanism that can identify a set of highly relevant local neighbors for target user. And then, it integrates the geographical information to build up an extended Matrix Factorization (MF) approach for personalized QoS prediction. Finally, it iteratively generates results by utilizing hints from previous round computations, a gradient boosting strategy that directly accelerates solving process. Experimental evidence on large-scale real-world QoS data shows that LoNMF is scalable, and consistently outperforming other state-of-the-art applications in prediction accuracy and efficiency.",project-academic
10.1145/3460418.3480415,2021-09-21,p,ACM,piwims physics informed warehouse inventory monitory via synthetic data generation," State-of-the-art camera-based deep learning methods for inventory monitoring tend to fail to generalize across different domains due to the high variance of scene settings. Large amounts of human labor are required to label and parameterize the models, making a real-world deployment impractical. In a third-party warehouse setting, supervised learning approaches are either too costly and/or inaccurate to deploy due to the need for human labor to address the diverse set of environmental factors (i.e, lighting conditions, product motion, deployment limitations). None We introduce, a realistic synthetic dataset generation technique that combines the physical constraints of the scene in real-world deployments, drastically reducing the need for human labeling. In contrast to other generative techniques, where the generative parameters are learned from a large sample of available data, this compositive approach defines the parameters based on physical characteristics of the particular task, which requires minimal human annotation. We demonstrate performance in a 4-month real operating warehouse deployment and show that with only 32 manually labeled images per object, can achieve an accuracy of up to 87% in inventory tracking, which is a 28% increase when compared to traditional data augmentation techniques and 31% error reduction when compared to the third-party warehouse industry average. Furthermore, we demonstrate the ability of to generalize across different camera angles and positions by achieving an accuracy of 85% in inventory tracking while varying the position and angle of the camera.",project-academic
,2019-09-24,a,,synthetic dataset generation for object to model deep learning in industrial applications," The availability of large image data sets has been a crucial factor in the success of deep learning-based classification and detection methods. While data sets for everyday objects are widely available, data for specific industrial use-cases (e.g. identifying packaged products in a warehouse) remains scarce. In such cases, the data sets have to be created from scratch, placing a crucial bottleneck on the deployment of deep learning techniques in industrial applications. 
We present work carried out in collaboration with a leading UK online supermarket, with the aim of creating a computer vision system capable of detecting and identifying unique supermarket products in a warehouse setting. To this end, we demonstrate a framework for using synthetic data to create an end-to-end deep learning pipeline, beginning with real-world objects and culminating in a trained model. 
Our method is based on the generation of a synthetic dataset from 3D models obtained by applying photogrammetry techniques to real-world objects. Using 100k synthetic images generated from 60 real images per class, an InceptionV3 convolutional neural network (CNN) was trained, which achieved classification accuracy of 95.8% on a separately acquired test set of real supermarket product images. The image generation process supports automatic pixel annotation. This eliminates the prohibitively expensive manual annotation typically required for detection tasks. Based on this readily available data, a one-stage RetinaNet detector was trained on the synthetic, annotated images to produce a detector that can accurately localize and classify the specimen products in real-time.",project-academic
10.7717/PEERJ-CS.222,2019-10-14,a,"PeerJ, Inc",synthetic dataset generation for object to model deep learning in industrial applications," The availability of large image data sets has been a crucial factor in the success of deep learning-based classification and detection methods. Yet, while data sets for everyday objects are widely available, data for specific industrial use-cases (e.g., identifying packaged products in a warehouse) remains scarce. In such cases, the data sets have to be created from scratch, placing a crucial bottleneck on the deployment of deep learning techniques in industrial applications. We present work carried out in collaboration with a leading UK online supermarket, with the aim of creating a computer vision system capable of detecting and identifying unique supermarket products in a warehouse setting. To this end, we demonstrate a framework for using data synthesis to create an end-to-end deep learning pipeline, beginning with real-world objects and culminating in a trained model. Our method is based on the generation of a synthetic dataset from 3D models obtained by applying photogrammetry techniques to real-world objects. Using 100K synthetic images for 10 classes, an InceptionV3 convolutional neural network was trained, which achieved accuracy of 96% on a separately acquired test set of real supermarket product images. The image generation process supports automatic pixel annotation. This eliminates the prohibitively expensive manual annotation typically required for detection tasks. Based on this readily available data, a one-stage RetinaNet detector was trained on the synthetic, annotated images to produce a detector that can accurately localize and classify the specimen products in real-time.",project-academic
10.1145/3459637.3481911,2021-10-26,p,,easytransfer a simple and scalable deep transfer learning platform for nlp applications," The literature has witnessed the success of leveraging Pre-trained Language Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural Language Processing (NLP) applications, yet it is not easy to build an easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to develop deep TL algorithms for NLP applications. EasyTransfer is backended with a high-performance and scalable engine for efficient training and inference, and also integrates comprehensive deep TL algorithms, to make the development of industrial-scale TL applications easier. In EasyTransfer, the built-in data and model parallelism strategies, combined with AI compiler optimization, show to be 4.0x faster than the community version of distributed training. EasyTransfer supports various NLP models in the ModelZoo, including mainstream PLMs and multi-modality models. It also features various in-house developed TL algorithms, together with the AppZoo for NLP applications. The toolkit is convenient for users to quickly start model training, evaluation, and online deployment. EasyTransfer is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, conversational question answering, etc. Extensive experiments on real-world datasets and online applications show that EasyTransfer is suitable for online production with cutting-edge performance for various applications. The source code of EasyTransfer is released at Github1.",project-academic
,2020-11-18,a,,easytransfer a simple and scalable deep transfer learning platform for nlp applications," The literature has witnessed the success of leveraging Pre-trained Language Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural Language Processing (NLP) applications, yet it is not easy to build an easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to develop deep TL algorithms for NLP applications. EasyTransfer is backended with a high-performance and scalable engine for efficient training and inference, and also integrates comprehensive deep TL algorithms, to make the development of industrial-scale TL applications easier. In EasyTransfer, the built-in data and model parallelism strategies, combined with AI compiler optimization, show to be 4.0x faster than the community version of distributed training. EasyTransfer supports various NLP models in the ModelZoo, including mainstream PLMs and multi-modality models. It also features various in-house developed TL algorithms, together with the AppZoo for NLP applications. The toolkit is convenient for users to quickly start model training, evaluation, and online deployment. EasyTransfer is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, conversational question answering, etc. Extensive experiments on real-world datasets and online applications show that EasyTransfer is suitable for online production with cutting-edge performance for various applications. The source code of EasyTransfer is released at Github (this https URL).",project-academic
10.4230/DAGREP.8.7.62,2019-01-01,a,Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik,extreme classification dagstuhl seminar 18291," Extreme classification is a rapidly growing research area within machine learning focusing on multi-class and multi-label problems involving an extremely large number of labels (even more than a million). Many applications of extreme classification have been found in diverse areas ranging from language modeling to document tagging in NLP, face recognition to learning universal feature representations in computer vision, gene function prediction in bioinformatics, etc. Extreme classification has also opened up a new paradigm for key industrial applications such as ranking and recommendation by reformulating them as multi-label learning tasks where each item to be ranked or recommended is treated as a separate label. Such reformulations have led to significant gains over traditional collaborative filtering and content-based recommendation techniques. Consequently, extreme classifiers have been deployed in many real-world applications in industry.

Extreme classification has raised many new research challenges beyond the pale of traditional machine learning including developing log-time and log-space algorithms, deriving theoretical bounds that scale logarithmically with the number of labels, learning from biased training data, developing performance metrics, etc. The seminar aimed at bringing together experts in machine learning, NLP, computer vision, web search and recommendation from academia and industry to make progress on these problems. We believe that this seminar has encouraged the inter-disciplinary collaborations in the area of extreme classification, started discussion on identification of thrust areas and important research problems, motivated to improve the algorithms upon the state-of-the-art, as well to work on the theoretical foundations of extreme classification.",project-academic
,2018-06-14,a,,transfer learning for context aware question matching in information seeking conversations in e commerce," Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist (this https URL) and observed a significant improvement over the existing online model.",project-academic
10.1016/J.COMPIND.2020.103226,2020-09-01,a,Elsevier,perspective on holonic manufacturing systems prosa becomes arti," Abstract None None Looking back at 30 years of research into holonic manufacturing systems, these explorations made a lasting scientific contribution to the overall architecture of intelligent manufacturing systems. Most notably, holonic architectures are defined in terms of their world-of-interest ( Van Brussel et al., 1998 ). They do not have an information layer, a communication layer, etc. Instead, they have components that relate to real-world assets (e.g. machine tools) and activities (e.g. assembly). And, they mirror and track the structure of their world-of-interest, which allows them to scale and adapt accordingly. None This research has wandered around, at times learning from its mistakes, and progressively carved out an invariant structure while it translated and applied scientific insights from complex-adaptive systems theory (e.g. autocatalytic sets) and from bounded rationality (e.g. holons). This paper presents and discusses the outcome of these research efforts. None At the top level, the holonic structure distinguishes intelligent beings (or digital twins) from intelligent agents. These digital twins inherit the consistency from reality, which they mirror. They are intelligent beings when they reflect what exists in the world without imposing artificial limitations in this reality. Consequently, a conflict with a digital twin is a conflict with reality. None In contrast, intelligent agents typically transform NP-hard challenges into computations with low-polynomial complexity. Unavoidably, this involves arbitrariness (e.g. don’t care choices). Likewise, relying on case-specific properties, to ensure an outcome in polynomial time, usually renders the validity of an agent’s choices both short-lived and situation-dependent. Here, intelligent agents create conflicts by imposing limitations of their own making in their world-of-interest. None Real-world smart systems are aggregates comprising both intelligent beings and intelligent agents. They are performers. Inside these performers, digital twins may constitute the foundations, supporting walls, support beams and pillars because these intelligent beings are protected by their real-world counterpart. Further refining the top-level of this architecture, a holonic structure enables these digital twins to shadow their real-world counterpart whenever it changes, adapts and evolves. None In contrast, the artificial limitations, imposed by the intelligent agents, cannot be allowed to build up inertia, which would hamper the undoing of arbitrary or case-specific limitations. To this end, performers explicitly manage the rights over their assets. Revoking such rights from a limitation-imposing agent will free the assets. This will be at the cost of reduced services from the agent. When other service providers rely on this agent, their services may be affected as well; that’s how the inertia builds up and how harmful legacy is created. Thus, the services of digital twins are to be preferred over the services of an intelligent agent by developers of holonic manufacturing systems. None Finally, digital twins corresponding to the decision making in the world-of-interest (a non-physical asset) allow to mirror the world-of-interest in a predictive mode (in addition to track and trace). It allows to generate short-term forecasts while preserving the benefits of intelligent beings. These twins are the intentions of the decision-making intelligent agents. Evidently, when intentions change, the forecasts needs to be regenerated (i.e. tracking the corresponding reality by the twin). This advanced feature can be deployed in a number of configurations (cf. annex).",project-academic
10.1145/3372923.3404862,2020-07-13,p,ACM,knowledge infused deep learning," Deep Learning has shown remarkable success during the last decade for essential tasks in computer vision and natural language processing. Yet, challenges remain in the development and deployment of artificial intelligence (AI) models in real-world cases, such as dependence on extensive data and trust, explainability, traceability, and interactivity. These challenges are amplified in high-risk fields, including healthcare, cyber threats, crisis response, autonomous driving, and future manufacturing. On the other hand, symbolic computing with knowledge graphs has shown significant growth in specific tasks with reliable performance. This tutorial (a) discusses the novel paradigm of knowledge-infused deep learning to synthesize neural computing with symbolic computing (b) describes different forms of knowledge and infusion methods in deep learning, and (c) discusses application-specific evaluation methods to assure explainability and reasoning using benchmark datasets and knowledge-resources. The resulting paradigm of ""knowledge-infused learning'' combines knowledge from both domain expertise and physical models. A wide variety of techniques involving shallow, semi-deep, and deep infusion will be discussed along with the corresponding intuitions, limitations, use cases, and applications. More details can be found \urlhttp://kidl2020.aiisc.ai/.",project-academic
10.1557/S43577-021-00122-3,2021-06-17,a,Springer International Publishing,engineered neuromuscular actuators for medicine meat and machines," Movement is central to life. Neuromuscular tissues control voluntary movement in humans and many other living creatures, offering significant advantages in adaptability and robustness as compared to abiotic actuators. The impressive functional capabilities of neuromuscular tissues have inspired researchers to attempt de novo synthesis of the biological motor system via tissue engineering. This article highlights key recent advances in tissue engineering skeletal muscle and discusses promising strategies to control engineered muscle via biological neural networks and abiotic soft electronic interfaces. Challenges associated with cell sourcing, biomaterials design, and scalable precision manufacturing, along with emerging strategies to address those challenges, are presented. Finally, we highlight how engineered neuromuscular tissues have enabled studying, controlling, and deploying them as actuators in a range of real-world applications including drug discovery, regenerative medicine, cellular agriculture, and soft robotics.",project-academic
10.1109/ICDMW.2017.40,2017-11-01,p,IEEE Computer Society,identifying irregular power usage by turning predictions into holographic spatial visualizations," Power grids are critical infrastructure assets that face non-technical losses (NTL) such as electricity theft or faulty meters. NTL may range up to 40% of the total electricity distributed in emerging countries. Industrial NTL detection systems are still largely based on expert knowledge when deciding whether to carry out costly on-site inspections of customers. Electricity providers are reluctant to move to large-scale deployments of automated systems that learn NTL profiles from data due to the latter's propensity to suggest a large number of unnecessary inspections. In this paper, we propose a novel system that combines automated statistical decision making with expert knowledge. First, we propose a machine learning framework that classifies customers into NTL or non-NTL using a variety of features derived from the customers' consumption data. The methodology used is specifically tailored to the level of noise in the data. Second, in order to allow human experts to feed their knowledge in the decision loop, we propose a method for visualizing prediction results at various granularity levels in a spatial hologram. Our approach allows domain experts to put the classification results into the context of the data and to incorporate their knowledge for making the final decisions of which customers to inspect. This work has resulted in appreciable results on a real-world data set of 3.6M customers. Our system is being deployed in a commercial NTL detection software.",project-academic
,2017-09-09,a,,identifying irregular power usage by turning predictions into holographic spatial visualizations," Power grids are critical infrastructure assets that face non-technical losses (NTL) such as electricity theft or faulty meters. NTL may range up to 40% of the total electricity distributed in emerging countries. Industrial NTL detection systems are still largely based on expert knowledge when deciding whether to carry out costly on-site inspections of customers. Electricity providers are reluctant to move to large-scale deployments of automated systems that learn NTL profiles from data due to the latter's propensity to suggest a large number of unnecessary inspections. In this paper, we propose a novel system that combines automated statistical decision making with expert knowledge. First, we propose a machine learning framework that classifies customers into NTL or non-NTL using a variety of features derived from the customers' consumption data. The methodology used is specifically tailored to the level of noise in the data. Second, in order to allow human experts to feed their knowledge in the decision loop, we propose a method for visualizing prediction results at various granularity levels in a spatial hologram. Our approach allows domain experts to put the classification results into the context of the data and to incorporate their knowledge for making the final decisions of which customers to inspect. This work has resulted in appreciable results on a real-world data set of 3.6M customers. Our system is being deployed in a commercial NTL detection software.",project-academic
10.1109/IEDEC.2013.6526763,2013-03-04,p,IEEE,hands on teaching of software and web applications security," Well-educated and technically skilled engineers, developers and programmers of secure software and Web systems are in high demand in industry these days. As a result, there is a need for a design and development of learning content aimed, on one hand, at software and Web security concepts, models, methods, algorithms, schemes, technologies, techniques, and tools used to design, develop, deploy, and maintain highly secure software and Web systems. On the other hand, due to multiple reports by professional societies, agencies and consulting firms in information/data security, students should obtain deep knowledge and excellent hands-on technical skills for a reliable protection of real-world software, Web and computer information systems against advanced types of modern computer attacks (that are, sometime, called Attacks 2.0). The purpose of this paper is to present designed, developed and tested elements of “Software and Web Applications Security” undergraduate and graduate courses that are based on active hands-on teaching approach; it is focused on developed learning framework for each type of computer attack discussed. This framework includes 1) analysis of relevant vulnerabilities in software and Web systems; 2) an overview of computer attack; 3) demonstration of an attack in a real time in lab environment; 4) attack's step-by-step algorithm (procedure); 5) software implementation of an attack; 6) prevention of an attack and defense mechanism(s); 7) advanced types of an attack; and 8) relevant hands-on exercises.",project-academic
10.1109/TCYB.2020.2964011,2020-01-30,a,Institute of Electrical and Electronics Engineers (IEEE),hierarchical granular computing based model and its reinforcement structural learning for construction of long term prediction intervals," As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.",project-academic
10.1109/JIOT.2020.2983050,2020-03-24,a,Institute of Electrical and Electronics Engineers (IEEE),modified densenet for automatic fabric defect detection with edge computing for minimizing latency," As an essential step in quality control, fabric defect detection plays an important role in the textile manufacturing industry. The traditional manual detection method is inaccurate and incurs a high cost; as a result, it is gradually being replaced by deep learning algorithms based on cloud computing. However, a high data transmission latency between end devices and the cloud has a significant impact on textile production efficiency. In contrast, edge computing, which provides services near end devices by deploying network, computing and storage facilities at the edge of the Internet, can effectively solve the above-mentioned problem. In this article, we propose a deep-learning-based fabric defect detection method for edge computing scenarios. First, this article modifies the structure of None DenseNet None to better suit a resource-constrained edge computing scenario. To better assess the proposed model, an optimized cross-entropy loss function is also formulated. Afterward, six feasible expansion schemes are utilized to enhance the data set according to the characteristics of various defects in fabric samples. To balance the distribution of samples, proportions of various defect types are used to determine the number of enhancements. Finally, a fabric defect detection system is established to test the performance of the optimized model used on edge devices in a real-world textile industry scenario. Experimental results demonstrate that compared with the conventional convolutional neural network (CNN), the proposed optimized model attains an average improvement of 18% in the area under the curve (AUC) metric for 11 defects. Data transmission is reduced by approximately 50% and latency is reduced by 32% in the Cambricon 1H8 platform compared with a cloud platform.",project-academic
10.1186/S13677-020-00168-9,2020-12-01,a,SpringerOpen,deep learning driven wireless communication for edge cloud computing opportunities and challenges," Future wireless communications are becoming increasingly complex with different radio access technologies, transmission backhauls, and network slices, and they play an important role in the emerging edge computing paradigm, which aims to reduce the wireless transmission latency between end-users and edge clouds. Deep learning techniques, which have already demonstrated overwhelming advantages in a wide range of internet of things (IoT) applications, show significant promise for solving such complicated real-world scenarios. Although the convergence of radio access networks and deep learning is still in the preliminary exploration stage, it has already attracted tremendous concern from both academia and industry. To address emerging theoretical and practical issues, ranging from basic concepts to research directions in future wireless networking applications and architectures, this paper mainly reviews the latest research progress and major technological deployment of deep learning in the development of wireless communications. We highlight the intuitions and key technologies of deep learning-driven wireless communication from the aspects of end-to-end communication, signal detection, channel estimation and compression sensing, encoding and decoding, and security and privacy. Main challenges, potential opportunities and future trends in incorporating deep learning schemes in wireless communications environments are further illustrated.",project-academic
10.1016/J.PROMFG.2020.05.146,2020-01-01,a,Elsevier,one shot recognition of manufacturing defects in steel surfaces," Abstract None None Quality control is an essential process in manufacturing to make the product defect-free as well as to meet customer needs. The automation of this process is important to maintain high quality along with the high manufacturing throughput. With recent developments in deep learning and computer vision technologies, it has become possible to detect various features from the images with near-human accuracy. However, many of these approaches are data intensive. Training and deployment of such a system on manufacturing floors may become expensive and time-consuming. The need for large amounts of training data is one of the limitations of the applicability of these approaches in real-world manufacturing systems. In this work, we propose the application of a Siamese convolutional neural network to do one-shot recognition for such a task. Our results demonstrate how one-shot learning can be used in quality control of steel by identification of defects on the steel surface. This method can significantly reduce the requirements of training data and can also be run in real-time.",project-academic
10.1017/S0269964819000147,2021-01-01,a,Cambridge University Press (CUP),accurate energy efficient classification with spiking random neural network," Artificial Neural Networks (ANNs)-based techniques have dominated state-of-the-art results in most problems related to computer vision, audio recognition, and natural language processing in the past few years, resulting in strong industrial adoption from all leading technology companies worldwide. One of the major obstacles that have historically delayed large-scale adoption of ANNs is the huge computational and power costs associated with training and testing (deploying) them. In the mean-time, Neuromorphic Computing platforms have recently achieved remarkable performance running the bio-realistic Spiking Neural Networks at high throughput and very low power consumption making them a natural alternative to ANNs. Here, we propose using the Random Neural Network, a spiking neural network with both theoretical and practical appealing properties, as a general purpose classifier that can match the classification power of ANNs on a number of tasks while enjoying all the features of being a spiking neural network. This is demonstrated on a number of real-world classification datasets.",project-academic
10.1145/3442381.3449944,2021-04-19,p,"Association for Computing Machinery, Inc",aid active distillation machine to leverage pre trained black box models in private data settings," This paper presents an active distillation method for a local institution (e.g., hospital) to find the best queries within its given budget to distill an on-server black-box model’s predictive knowledge into a local surrogate with transparent parameterization. This allows local institutions to understand better the predictive reasoning of the black-box model in its own local context or to further customize the distilled knowledge with its private dataset that cannot be centralized and fed into the server model. The proposed method thus addresses several challenges of deploying machine learning (ML) in many industrial settings (e.g., healthcare analytics) with strong proprietary constraints. These include: (1) the opaqueness of the server model’s architecture which prevents local users from understanding its predictive reasoning in their local data contexts; (2) the increasing cost and risk of uploading local data on the cloud for analysis; and (3) the need to customize the server model with private onsite data. We evaluated the proposed method on both benchmark and real-world healthcare data where significant improvements over existing local distillation methods were observed. A theoretical analysis of the proposed method is also presented.",project-academic
10.1145/3351095.3375667,2020-01-27,p,ACM,ai explainability 360 hands on tutorial," This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models. Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space. Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below). In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general. Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks. Here is a rough agenda of the tutorial: 1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms. 2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers). 3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications. 4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions. 5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks. 6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit. 7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.",project-academic
10.1109/CCGRID.2015.41,2015-05-04,p,IEEE,service clustering for autonomic clouds using random forest," Managing and optimising cloud services is one of the main challenges faced by industry and academia. A possible solution is resorting to self-management, as fostered by autonomic computing. However, the abstraction layer provided by cloud computing obfuscates several details of the provided services, which, in turn, hinders the effectiveness of autonomic managers. Data-driven approaches, particularly those relying on service clustering based on machine learning techniques, can assist the autonomic management and support decisions concerning, for example, the scheduling and deployment of services. One aspect that complicates this approach is that the information provided by the monitoring contains both continuous (e.g. CPU load) and categorical (e.g. VM instance type) data. Current approaches treat this problem in a heuristic fashion. This paper, instead, proposes an approach, which uses all kinds of data and learns in a data-driven fashion the similarities and resource usage patterns among the services. In particular, we use an unsupervised formulation of the Random Forest algorithm to calculate similarities and provide them as input to a clustering algorithm. For the sake of efficiency and meeting the dynamism requirement of autonomic clouds, our methodology consists of two steps: (i) off-line clustering and (ii) on-line prediction. Using datasets from real-world clouds, we demonstrate the superiority of our solution with respect to others and validate the accuracy of the on-line prediction. Moreover, to show the applicability of our approach, we devise a service scheduler that uses the notion of similarity among services and evaluate it in a cloud test-bed.",project-academic
,2021-01-13,a,,mlgo a machine learning guided compiler optimizations framework," Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.",project-academic
,2019-11-08,a,,the pitfall of evaluating performance on emerging ai accelerators," In recent years, domain-specific hardware has brought significant performance improvements in deep learning (DL). Both industry and academia only focus on throughput when evaluating these AI accelerators, which usually are custom ASICs deployed in datacenter to speed up the inference phase of DL workloads. Pursuing higher hardware throughput such as OPS (Operation Per Second) using various optimizations seems to be their main design target. However, they ignore the importance of accuracy in the DL nature. Motivated by this, this paper argue that a single throughput metric can not comprehensively reflect the real-world performance of AI accelerators. To reveal this pitfall, we evaluates several frequently-used optimizations on a typical AI accelerator and quantifies their impact on accuracy and throughout under representative DL inference workloads. Based on our experimental results, we find that some optimizations cause significant loss on accuracy in some workloads, although it can improves the throughout. Furthermore, our results show the importance of end-to-end evaluation in DL.",project-academic
,2021-01-19,a,Preprints,towards reliable ieee 802 15 4g sun with re transmission shaping and adaptive modulation selection," In this paper, we propose and evaluate two mechanisms aimed at improving the communication reliability of IEEE 802.15.4g SUN (Smart Utility Networks) in industrial scenarios: RTS (Re-Transmission Shaping), which uses acknowledgements to track channel conditions and dynamically adapt the number of re-transmissions per packet, and AMS (Adaptive Modulation Selection), which makes use of reinforcement learning based on MAB (Multi-Armed Bandits) to choose the modulation that provides the best reliability for each packet re-transmission. The evaluation of both mechanisms is performed through computer simulations using a dataset obtained from a real-world deployment and two widely used metrics, the PDR (Packet Delivery Ratio) and the RNP (Required Number of Packet transmissions). The PDR measures the ratio between received and transmitted packets, whereas the RNP is the number of packet repetitions before a successful transmission. The results show that both mechanisms allow to increase the communication reliability while not jeopardizing the battery life-time constraints of end devices. For example, when three re-transmissions per packet are allowed, the PDR reaches 98/96% with a RNP of 2.03/1.32 using RTS and AMS, respectively. Additionally, the combination of both proposed mechanisms allows to reach a 99% PDR with a RNP of 1.7, making IEEE 802.15.4g SUN compliant with the stringent data delivery requirements of industrial applications.",project-academic
10.1007/S11265-021-01665-Z,2021-05-14,a,Springer US,towards reliable ieee 802 15 4g sun with re transmission shaping and adaptive modulation selection," In this paper, we propose and evaluate two mechanisms aimed at improving the communication reliability of IEEE 802.15.4g SUN (Smart Utility Networks) in industrial scenarios: RTS (Re-Transmission Shaping), which uses acknowledgements to track channel conditions and dynamically adapt the number of re-transmissions per packet, and AMS (Adaptive Modulation Selection), which makes use of reinforcement learning based on MAB (Multi-Armed Bandits) to choose the modulation that provides the best reliability for each packet re-transmission. The evaluation of both mechanisms is performed through computer simulations using a dataset obtained from a real-world deployment and two widely used metrics, the PDR (Packet Delivery Ratio) and the RNP (Required Number of Packet transmissions). The PDR measures the ratio between received and transmitted packets, whereas the RNP is the number of packet repetitions before a successful transmission. The results show that both mechanisms allow to increase the communication reliability while not jeopardizing the battery life-time constraints of end devices. For example, when three re-transmissions per packet are allowed, the PDR reaches 98/96% with a RNP of 2.03/1.32 using RTS and AMS, respectively. Additionally, the combination of both proposed mechanisms allows to reach a 99% PDR with a RNP of 1.7, making IEEE 802.15.4g SUN compliant with the stringent data delivery requirements of industrial applications.",project-academic
10.18653/V1/P18-2034,2018-07-01,p,Association for Computational Linguistics,transfer learning for context aware question matching in information seeking conversations in e commerce," Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist and observed a significant improvement over the existing online model.",project-academic
10.1109/CIMSA.2006.250751,2006-07-12,p,IEEE,management of complex dynamic systems based on model predictive multi objective optimization," Over the past two decades, model predictive control and decision-making strategies have established themselves as powerful methods for optimally managing the behavior of complex dynamic industrial systems and processes. This paper presents a novel model-based multi-objective optimization and decision-making approach to model-predictive decision-making. The approach integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and decision-making based on Pareto frontier techniques. The predictive models are adaptive, and continually update themselves to reflect with high fidelity the gradually changing underlying system dynamics. The integrated approach, embedded in a real-time plant optimization and control software environment has been deployed to dynamically optimize emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant. While this approach is described in the context of power plants, the method is adaptable to a wide variety of industrial process control and management applications",project-academic
10.2139/SSRN.3518854,2020-01-13,a,,from average customer to individual traveler a field experiment in airline ancillary pricing," Ancillaries in the travel industry are now a major stream for revenue and profitability. Ancillaries are optional products or services whose sales depend on an individual's personal preference and their trip context. Conventional pricing strategies for ancillaries based on poorly optimized or static business rules do not respond to changing market conditions or trip context.

We present a dynamic pricing model developed in conjunction with Deepair solutions, an AI technology provider for travel suppliers. Our models provide dynamic, customer-interaction-specific pricing recommendations, to increase revenue. The unique nature of personalized pricing provides the opportunity to search over the market space to find the optimal price-point for each customer, without violating customer privacy.

We present an A/B testing deployment framework on an airline's website. Embedded in it are three models for dynamic pricing of ancillaries, with increasing levels of sophistication: (1) a two-stage forecasting and pricing model using a logistic mapping function; (2) a two-stage model with a deep neural network for forecasting, followed by pricing using discrete exhaustive search; (3) a single-stage end-to-end deep neural network that recommends the optimal price. In an outer loop, we introduce an online adaptive model-selection framework that adaptively routes customer requests to the above models. This is modeled as multi-armed bandit problem, which we solve using Thompson sampling.

We evaluate the performance of these models based on offfine and online evaluations, and their real-world business impact. Offline experiments show that deep learning algorithms outperform traditional machine learning techniques for this problem. In online testing, our AI-driven pricing outperforms human rule-based approaches, improving conversion by 17% and revenue per offer by 25%. Additionally, our adaptive model-selection approach outperforms a uniformly random selection policy by improving the expected revenue per offer by 43% and conversion score by 58% in a simulation environment.",project-academic
10.1109/CVPR.2019.00715,2019-06-15,p,IEEE,shieldnets defending against adversarial attacks using probabilistic adversarial robustness," Defending adversarial attack is a critical step towards reliable deployment of deep learning empowered solutions for industrial applications. Probabilistic adversarial robustness (PAR), as a theoretical framework, is introduced to neutralize adversarial attacks by concentrating sample probability to adversarial-free zones. Distinct to most of the existing defense mechanisms that require modifying the architecture/training of the target classifier which is not feasible in the real-world scenario, e.g., when a model has already been deployed, PAR is designed in the first place to provide proactive protection to an existing fixed model. ShieldNet is implemented as a demonstration of PAR in this work by using PixelCNN. Experimental results show that this approach is generalizable, robust against adversarial transferability and resistant to a wide variety of attacks on the Fashion-MNIST and CIFAR10 datasets, respectively.",project-academic
,2021-03-22,a,,real time end to end federated learning an automotive case study," With the development and the increasing interests in ML/DL fields, companies are eager to utilize these methods to improve their service quality and user experience. Federated Learning has been introduced as an efficient model training approach to distribute and speed up time-consuming model training and preserve user data privacy. However, common Federated Learning methods apply a synchronized protocol to perform model aggregation, which turns out to be inflexible and unable to adapt to rapidly evolving environments and heterogeneous hardware settings in real-world systems. In this paper, we introduce an approach to real-time end-to-end Federated Learning combined with a novel asynchronous model aggregation protocol. We validate our approach in an industrial use case in the automotive domain focusing on steering wheel angle prediction for autonomous driving. Our results show that asynchronous Federated Learning can significantly improve the prediction performance of local edge models and reach the same accuracy level as the centralized machine learning method. Moreover, the approach can reduce the communication overhead, accelerate model training speed and consume real-time streaming data by utilizing a sliding training window, which proves high efficiency when deploying ML/DL components to heterogeneous real-world embedded systems.",project-academic
10.1109/COMPSAC51774.2021.00070,2021-07-01,p,IEEE,real time end to end federated learning an automotive case study," With the development and the increasing interests in ML/DL fields, companies are eager to apply Machine Learning/Deep Learning approaches to increase service quality and customer experience. Federated Learning was implemented as an effective model training method for distributing and accelerating time-consuming model training while protecting user data privacy. However, common Federated Learning approaches, on the other hand, use a synchronous protocol to conduct model aggregation, which is inflexible and unable to adapt to rapidly changing environments and heterogeneous hardware settings in real-world scenarios. In this paper, we present an approach to real-time end-to-end Federated Learning combined with a novel asynchronous model aggregation protocol. Our method is validated in an industrial use case in the automotive domain, focusing on steering wheel angle prediction for autonomous driving. Our findings show that asynchronous Federated Learning can significantly improve the prediction performance of local edge models while maintaining the same level of accuracy as centralized machine learning. Furthermore, by using a sliding training window, the approach can minimize communication overhead, accelerate model training speed and consume real-time streaming data, proving high efficiency when deploying ML/DL components to heterogeneous real-world embedded systems.",project-academic
,2021-09-16,a,,enabling risk aware reinforcement learning for medical interventions through uncertainty decomposition," Reinforcement Learning (RL) is emerging as tool for tackling complex control and decision-making problems. However, in high-risk environments such as healthcare, manufacturing, automotive or aerospace, it is often challenging to bridge the gap between an apparently optimal policy learnt by an agent and its real-world deployment, due to the uncertainties and risk associated with it. Broadly speaking RL agents face two kinds of uncertainty, 1. aleatoric uncertainty, which reflects randomness or noise in the dynamics of the world, and 2. epistemic uncertainty, which reflects the bounded knowledge of the agent due to model limitations and finite amount of information/data the agent has acquired about the world. These two types of uncertainty carry fundamentally different implications for the evaluation of performance and the level of risk or trust. Yet these aleatoric and epistemic uncertainties are generally confounded as standard and even distributional RL is agnostic to this difference. Here we propose how a distributional approach (UA-DQN) can be recast to render uncertainties by decomposing the net effects of each uncertainty. We demonstrate the operation of this method in grid world examples to build intuition and then show a proof of concept application for an RL agent operating as a clinical decision support system in critical care",project-academic
10.1145/3365609.3365857,2019-11-14,p,ACM,an effort to democratize networking research in the era of ai ml," A growing concern within today's networking community is that with the proliferation of Artificial Intelligence/Machine Learning (AI/ML) techniques, a lack of access to real-world production networks is putting academic researchers at a significant disadvantage. Indeed, compared to a select few research groups in industry that can leverage access to their global-scale production networks in their data-driven efforts to develop and evaluate learning models, academic researchers not only struggle to get their hands on real-world data sets but find it almost impossible to adequately train and assess their learning models under realistic conditions.In this paper, we argue that when appropriately instrumented and properly managed, enterprise networks in the form of university or campus networks can serve as real-world production networks and can, because of their ubiquity, help create a more level playing field for academic researchers. Their various limitations notwithstanding, as real-world production networks, such enterprise networks can (i) serve as unique sources for some of the rich data that will enable these researchers to influence or advance the current state-of-the-art in AI/ML for networking and (ii) also function as much-needed test beds where newly developed AI/ML-based tools can be evaluated or ""road-tested"" prior to their actual deployment in the production network. We discuss new research challenges that arise from this proposed dual role of campus networks and comment on the opportunities our proposal affords for both academic and industry researchers to benefit from the advantages and limitations of their respective production environments in their common quest to advance the development and evaluation of AI/ML-based tools to the point where they can be deployed in practice.",project-academic
,2006-07-16,p,AAAI Press,machine translation for manufacturing a case study at ford motor company," Machine Translation was one of the first applications of Artificial Intelligence technology that was deployed to solve real-world problems. Since the early 1960s, researchers have been building and utilizing computer systems that can translate from one language to another without extensive human intervention. In the late 1990s, Ford Vehicle Operations began working with Systran Software Inc to adapt and customize their Machine Translation (MT) technology in order to translate Ford's vehicle assembly build instructions from English to German, Spanish, Dutch and Portuguese. The use of Machine Translation (MT) was made necessary by the vast amount of dynamic information that needed to be translated in a timely fashion. Our MT system has already translated over 5 million instructions into these target languages and is an integral part of our manufacturing process planning to support Ford's assembly plants in Europe, Mexico and South America. In this paper, we focus on how AI techniques, such as knowledge representation (Iwanska & Shapiro 2000) and natural language processing (Gazdar & Mellish 1989), can improve the accuracy of Machine Translation in a dynamic environment such as auto manufacturing.",project-academic
10.1609/AIMAG.V28I3.2053,2007-09-15,a,American Association for Artificial Intelligence,machine translation for manufacturing a case study at ford motor company," Machine translation (MT) was one of the first applications of artificial intelligence technology that was deployed to solve real-world problems. Since the early 1960s, researchers have been building and utilizing computer systems that can translate from one language to another without requiring extensive human intervention. In the late 1990s, Ford Vehicle Operations began working with Systran Software Inc. to adapt and customize its machine-translation technology in order to translate Ford's vehicle assembly build instructions from English to German, Spanish, Dutch, and Portuguese. The use of machine translation was made necessary by the vast amount of dynamic information that needed to be translated in a timely fashion. The assembly build instructions at Ford contain text written in a controlled language as well as unstructured remarks and comments. The MT system has already translated more than 7 million instructions into these languages and is an integral part of the overall manufacturing process-planning system used to support Ford's assembly plants in Europe, Mexico and South America. In this paper, we focus on how AI techniques, such as knowledge representation and natural language processing can improve the accuracy of machine translation in a dynamic environment such as auto manufacturing.",project-academic
10.1109/TPDS.2021.3104255,2020-04-23,a,,taskflow a lightweight parallel and heterogeneous task graph computing system," Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5x less memory, and 1.9x higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.",project-academic
10.1007/978-981-15-9460-1_10,2019-07-12,a,EasyChair,towards precise robotic grasping by probabilistic post grasp displacement estimation," Precise robotic grasping is important for many industrial applications, such as assembly and palletizing, where the location of the object needs to be controlled and known. However, achieving precise grasps is challenging due to noise in sensing and control, as well as unknown object properties. We propose a method to plan robotic grasps that are both robust and precise by training two convolutional neural networks—one to predict the robustness of a grasp and another to predict a distribution of post-grasp object displacements. Our networks are trained with depth images in simulation on a dataset of over 1000 industrial parts and were successfully deployed on a real robot without having to be further fine-tuned. The proposed displacement estimator achieves a mean prediction errors of 0.68 cm and \(3.42\) \(^\circ \) on novel objects in real-world experiments .",project-academic
10.1049/CP.2019.0159,2019-01-01,p,Institution of Engineering and Technology,impact assessment for data driven innovation in cognitive iot architectures," Data-driven innovation in sectors such as built environment and transport is gaining traction in research, government and industrial communities. A number of industry-academic collaborations are emphasizing on use of data to empower emerging technologies. Large scale deployments of IoT devices and sensors are making data available at an unprecedented scale. This is leading to design and development of governed data infrastructures to harness the value of socio-technological processes and maximise the economic impact. Application of cognitive technologies such as machine learning and artificial intelligence is paving way for newer ways of data-driven innovation for decision making, predictive analysis and scenario modelling. The paper describes the various building blocks of the data-innovation engine and substantiates it through an anonymised real-world use-case from the built environment. It describes the emerging data-driven innovation models in these sectors and the corresponding value created by them. Finally, it proposes an impact assessment framework to understand and evaluate data-driven innovation in an organisation.",project-academic
10.1109/ICSPCC.2018.8567797,2018-09-01,p,IEEE,attack detection for wireless enterprise network a machine learning approach," An increasing number of enterprises are adopting wireless technology to deploy networks. However, wireless enterprise networks are more vulnerable than wired networks because of the broadcast feature. Thus, illegal attacks such as data theft and information forgery seriously threaten the property and information security of users and enterprises; these phenomena are attracting increasing attention from both academia and industry. Additionally, effectively detecting the attacks in the wireless enterprise networks is one of todays most important and challenging problems, especially in Wi-Fi networks, as attacks become increasingly covert and diverse. Fortunately, WiFi networks produce large amounts of data, providing copious big data for researchers. In this paper, using the Aegean Wi-Fi Intrusion Dataset (AWID), which is derived from the real-world Wi-Fi network, we introduce machine learning to detect network attacks. To significantly increase the training and convergence speeds, we deploy two-dimensional data cleaning and select 18 useful attributes from the original set of 154. Then, we introduce support vector machine (SVM) to detect attacks based on the cleaned dataset. The detection accuracy for flooding attacks, injection attacks, and normal data reached 89.18%, 87.34%, and 99.88% respectively. To the best of our knowledge, this is the first study to introduce a two-dimensional data cleaning method with an SVM to improve the detection accuracy for attacks. Finally, our detection results are comparable with the existing studies; however, our method operates with simpler data attributes with faster and more efficient training speed.",project-academic
10.1109/WF-IOT48130.2020.9221446,2020-06-02,p,IEEE,learner s dilemma iot devices training strategies in collaborative deep learning," With the growth of Internet of Things (IoT) and mobile edge computing, billions of smart devices are interconnected to develop applications used in various domains including smart homes, healthcare and smart manufacturing. Deep learning has been extensively utilized in various IoT applications which require huge amount of data for model training. Due to privacy requirements, smart IoT devices do not release data to a remote third party for their use. To overcome this problem, collaborative approach to deep learning, also known as Collaborative Deep Learning (CDL) has been largely employed in data-driven applications. This approach enables multiple edge IoT devices to train their models locally on mobile edge devices. In this paper, we address IoT device training problem in CDL by analyzing the behavior of mobile edge devices using a game-theoretic model, where each mobile edge device aims at maximizing the accuracy of its local model at the same time limiting the overhead of participating in CDL. We analyze the Nash Equilibrium in an N-player static game model. We further present a novel clusterbased fair strategy to approximately solve the CDL game to enforce mobile edge devices for cooperation. Our experimental results and evaluation analysis in a real-world smart home deployment show that 80% mobile edge devices are ready to cooperate in CDL, while 20% of them do not train their local models collaboratively.",project-academic
,2020-07-30,a,,learner s dilemma iot devices training strategies in collaborative deep learning," With the growth of Internet of Things (IoT) and mo-bile edge computing, billions of smart devices are interconnected to develop applications used in various domains including smart homes, healthcare and smart manufacturing. Deep learning has been extensively utilized in various IoT applications which require huge amount of data for model training. Due to privacy requirements, smart IoT devices do not release data to a remote third party for their use. To overcome this problem, collaborative approach to deep learning, also known as Collaborative DeepLearning (CDL) has been largely employed in data-driven applications. This approach enables multiple edge IoT devices to train their models locally on mobile edge devices. In this paper,we address IoT device training problem in CDL by analyzing the behavior of mobile edge devices using a game-theoretic model,where each mobile edge device aims at maximizing the accuracy of its local model at the same time limiting the overhead of participating in CDL. We analyze the Nash Equilibrium in anN-player static game model. We further present a novel cluster-based fair strategy to approximately solve the CDL game to enforce mobile edge devices for cooperation. Our experimental results and evaluation analysis in a real-world smart home deployment show that 80% mobile edge devices are ready to cooperate in CDL, while 20% of them do not train their local models collaboratively.",project-academic
10.1145/3459637.3481933,2021-10-26,p,,learning to pack a data driven tree search algorithm for large scale 3d bin packing problem," The 3-dimensional bin packing problem (3D-BPP) is not only fundamental in combinatorial optimization but also widely applied in real world logistics. In the modern logistics industry, the complexity of constraints, heterogeneity of cargoes and scale of orders are dramatically increased, leading to great challenges to devise packing plans up to standard. While the tree search algorithm is proved to be a successful paradigm to solve the 3D-BPP, it is too time-consuming to be applied in the aforementioned large-scale scenarios. To overcome the limitation, we propose a data-driven tree search algorithm (DDTS) to tackle the 3D-BPP. The solution space with complicated constraints is explored by a tree search algorithm, and a convolutional neural network trained with historical data guides pruning the tree so as to accelerate the search process. Computational experiments on real-world datasets show that our algorithm outperforms the state-of-the-art approach with a loading rate improvement of 2.47%. Moreover, the deep learning technique increases searching efficiency by 37.14% with only 0.04% performance loss. The algorithm has been deployed in Huawei Logistics System, which increases the loading rate by 3% and could reduce the logistics cost by millions of dollars per year. To the best of our knowledge, we are the first to embed pruning networks into tree search for the large-scale 3D-BPP.",project-academic
10.1109/ACCESS.2017.2783118,2017-01-01,a,IEEE,ieee access special section editorial health informatics for the developing world," We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases.",project-academic
10.1007/S11263-021-01463-X,2021-05-20,a,Springer US,deep cocktail networks," Transferable deep representations for visual domain adaptation (DA) provides a route to learn from labeled source images to recognize target images without the aid of target-domain supervision. Relevant researches increasingly arouse a great amount of interest due to its potential industrial prospect for non-laborious annotation and remarkable generalization. However, DA presumes source images are identically sampled from a single source while Multi-Source DA (MSDA) is ubiquitous in the real-world. In MSDA, the domain shifts exist not only between source and target domains but also among the sources; especially, the multi-source and target domains may disagree on their semantics (e.g., category shifts). This issue challenges the existing solutions for MSDAs. In this paper, we propose Deep CockTail Network (DCTN), a universal and flexibly-deployed framework to address the problems. DCTN uses a multi-way adversarial learning pipeline to minimize the domain discrepancy between the target and each of the multiple in order to learn domain-invariant features. The derived source-specific perplexity scores measure how similar each target feature appears as a feature from one of source domains. The multi-source category classifiers are integrated with the perplexity scores to categorize target images. We accordingly derive a theoretical analysis towards DCTN, including the interpretation why DCTN can be successful without precisely crafting the source-specific hyper-parameters, and target expected loss upper bounds in terms of domain and category shifts. In our experiments, DCTNs have been evaluated on four benchmarks, whose empirical studies involve vanilla and three challenging category-shift transfer problems in MSDA, i.e., source-shift, target-shift and source-target-shift scenarios. The results thoroughly reveal that DCTN significantly boosts classification accuracies in MSDA and performs extraordinarily to resist negative transfers across different MSDA scenarios.",project-academic
10.1109/BIGDATA.2018.8622583,2018-12-01,p,IEEE,ensemble machine learning systems for the estimation of steel quality control," Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out.",project-academic
10.1016/J.ESWA.2016.10.014,2017-02-01,a,Pergamon,semantic maps from multiple visual cues," Semantic mapping based on place and object recognition strategies.Place recognition involves learning through bag of visual words.Object recognition relies on HTM deep learning.Decision making endorses time embedded voting for objects and places.Topometric map is associated with place belief distribution. Future service robots targeted to operate in domestic or industrial environment and in close collaboration with humans should possess the ability to produce meaningful internal perceptual representations of their own surroundings, enabling them to fulfill a variety of real-world tasks. For this purpose, we present here a semantic mapping framework featuring geometrical and semasiological attributes that reveal the relationships between objects and places in a real-life environment. The geometrical component consists of a 3D metric map, onto which a topological map is deployed. The semasiological part is realized by putting together a place recognition algorithm and an object recognition one. The categorization of the different places relies on the resolution of appearance-based consistency histograms, while for the recognition of objects in the scene, a hierarchical temporal memory (HTM) network boosted by a saliency attentional model, is utilized. These semantic attributes are then deposited on the topological map to augment it with the belief distributions regarding the visited places, enabling thus the agent to act in an intelligent manner in human populated environments. Thus, the proposed framework outlines a proficient system in the construction of human conceivable environment representations, which has been successfully assessed on real-world scenarios, proving its ability to provide a consistent solution to the emerging problem of the human-robot cohabitation.",project-academic
10.1016/J.ENGAPPAI.2013.04.006,2013-09-01,a,Pergamon,post design analysis for building and refining ai planning systems," The growth of industrial applications of artificial intelligence has raised the need for design tools to aid in the conception and implementation of such complex systems. The design of automated planning systems faces several engineering challenges including the proper modeling of the domain knowledge: the creation of a model that represents the problem to be solved, the world that surrounds the system, and the ways the system can interact with and change the world in order to solve the problem. Knowledge modeling in AI planning is a hard task that involves acquiring the system requirements and making design decisions that can determine the behavior and performance of the resulting system. In this paper we investigate how knowledge acquired during a post-design phase of modeling can be used to improve the prospective model. A post-design framework is introduced which combines a knowledge engineering tool and a virtual prototyping environment for the analysis and simulation of plans. This framework demonstrates that post-design analysis supports the discovery of missing requirements and can guide the model refinement cycle. We present three case studies using benchmark domains and eight state-of-the-art planners. Our results demonstrate that significant improvements in plan quality and an increase in planning speed of up to three orders of magnitude can be achieved through a careful post-design process. We argue that such a process is critical for the deployment of AI planning technology in real-world engineering applications.",project-academic
10.1145/3269206.3272021,2018-02-27,a,,real time bidding with multi agent reinforcement learning in display advertising," Real-time advertising allows advertisers to bid for each impression for a visiting user. To optimize specific goals such as maximizing revenue and return on investment (ROI) led by ad placements, advertisers not only need to estimate the relevance between the ads and user's interests, but most importantly require a strategic response with respect to other advertisers bidding in the market. In this paper, we formulate bidding optimization with multi-agent reinforcement learning. To deal with a large number of advertisers, we propose a clustering method and assign each cluster with a strategic bidding agent. A practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed and implemented to balance the tradeoff between the competition and cooperation among advertisers. The empirical study on our industry-scaled real-world data has demonstrated the effectiveness of our methods. Our results show cluster-based bidding would largely outperform single-agent and bandit approaches, and the coordinated bidding achieves better overall objectives than purely self-interested bidding agents.",project-academic
,2011-04-28,b,,semantic web services," A paradigm shift is taking place in computer science: one generation ago, we learned to abstract from hardware to software, now we are abstracting from software to serviceware implemented through service-oriented computing. Yet ensuring interoperability in open, heterogeneous, and dynamically changing environments, such as the Internet, remains a major challenge for actual machine-to-machine integration. Usually significant problems in aligning data, processes, and protocols appear as soon as a specific piece of functionality is used within a different application context. The Semantic Web Services (SWS) approach is about describing services with metadata on the basis of domain ontologies as a means to enable their automatic location, execution, combination, and use. Fensel and his coauthors provide a comprehensive overview of SWS in line with actual industrial practice. They introduce the main sociotechnological components that ground the SWS vision (like Web Science, Service Science, and service-oriented architectures) and several approaches that realize it, e.g. the Web Service Modeling Framework, OWL-S, and RESTful services. The real-world relevance is emphasized through a series of case studies from large-scale R&D projects and a business-oriented proposition from the SWS technology provider Seekda.Each chapter of the book is structured according to a predefined template, covering both theoretical and practical aspects, and including walk-through examples and hands-on exercises. Additional learning material is available on the book website www.swsbook.org. With its additional features, the book is ideally suited as the basis for courses or self-study in this field, and it may also serve as a reference for researchers looking for a state-of-the-art overview of formalisms, methods, tools, and applications related to SWS.",project-academic
,2020-07-01,a,,prototypeml a neural network integrated design and development environment," Neural network architectures are most often conceptually designed and described in visual terms, but are implemented by writing error-prone code. PrototypeML is a machine learning development environment that bridges the dichotomy between the design and development processes: it provides a highly intuitive visual neural network design interface that supports (yet abstracts) the full capabilities of the PyTorch deep learning framework, reduces model design and development time, makes debugging easier, and automates many framework and code writing idiosyncrasies. In this paper, we detail the deep learning development deficiencies that drove the implementation of PrototypeML, and propose a hybrid approach to resolve these issues without limiting network expressiveness or reducing code quality. We demonstrate the real-world benefits of a visual approach to neural network design for research, industry and teaching. Available at this https URL",project-academic
10.1016/J.ESWA.2020.113653,2020-12-15,a,Pergamon Press,cost sensitive learning classification strategy for predicting product failures," Abstract None None In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem.",project-academic
10.1109/PACET48583.2019.8956270,2019-11-01,p,IEEE,a smart recycling bin for waste classification," As there is an obvious and increasing need to preserve valuable resources and reduce waste and pollution, several researches are focusing into this area. However, the solutions provided are neither budget-friendly nor effective to be practical in a real-world application. In this paper, we present a Smart Recycling Bin using modern approaches for waste classification. The design of the system permits the low-cost manufacturing of the final product, while uses state of the art technologies such as neural networks and the LoRaWAN protocol. We implemented a low-cost Smart Bin prototype able to classify different types of waste with an accuracy of 92.1%. The system also remotely transmits valuable data to the corresponding authorities, which can increase their effectiveness in waste management. Index Terms-Smartbin, Recycling, LoRa, Raspberry Pi, embedded system",project-academic
10.1007/978-3-030-64330-0_5,2020-09-14,a,"Springer, Cham",cybersecurity awareness platform with virtual coach and automated challenge assessment," Over the last years, the number of cyber-attacks on industrial control systems has been steadily increasing. Among several factors, proper software development plays a vital role in keeping these systems secure. To achieve secure software, developers need to be aware of secure coding guidelines and secure coding best practices. This work presents a platform geared towards software developers in the industry that aims to increase awareness of secure software development. The authors also introduce an interactive game component, a virtual coach, which implements a simple artificial intelligence engine based on the laddering technique for interviews. Through a survey, a preliminary evaluation of the implemented artifact with real-world players (from academia and industry) shows a positive acceptance of the developed platform. Furthermore, the players agree that the platform is adequate for training their secure coding skills. The impact of our work is to introduce a new automatic challenge evaluation method together with a virtual coach to improve existing cybersecurity awareness training programs. These training workshops can be easily held remotely or off-line.",project-academic
,2021-02-20,a,,cybersecurity awareness platform with virtual coach and automated challenge assessment," Over the last years, the number of cyber-attacks on industrial control systems has been steadily increasing. Among several factors, proper software development plays a vital role in keeping these systems secure. To achieve secure software, developers need to be aware of secure coding guidelines and secure coding best practices. This work presents a platform geared towards software developers in the industry that aims to increase awareness of secure software development. The authors also introduce an interactive game component, a virtual coach, which implements a simple artificial intelligence engine based on the laddering technique for interviews. Through a survey, a preliminary evaluation of the implemented artifact with real-world players (from academia and industry) shows a positive acceptance of the developed platform. Furthermore, the players agree that the platform is adequate for training their secure coding skills. The impact of our work is to introduce a new automatic challenge evaluation method together with a virtual coach to improve existing cybersecurity awareness training programs. These training workshops can be easily held remotely or off-line.",project-academic
10.1016/J.ENCONMAN.2019.111932,2019-11-01,a,Pergamon,cultural coyote optimization algorithm applied to a heavy duty gas turbine operation," Abstract None None In the past decades, the quantity of researches regarding industrial gas turbines (GT) has increased exponentially in terms of number of publications and diversity of applications. The GTs offer high power output along with a high combined cycle efficiency and high fuel flexibility. As consequence, the energy efficiency, the pressure oscillations, the pollutant emissions and the fault diagnosis have become some of the recent concerns related to this type of equipment. In order to solve these GTs related problems and many other real-world engineering and industry 4.0 issues, a set of new technological approaches have been tested, such as the combination of Artificial Neural Networks (ANN) and metaheuristics for global optimization. In this paper, the recently proposed metaheuristic denoted Coyote Optimization Algorithm (COA) is applied to the operation optimization of a heavy duty gas turbine placed in Brazil and used in power generation. The global goal is to find the best valves setup to reduce the fuel consumption while coping with environmental and physical constraints from its operation. In order to treat it as an optimization problem, an integrated simulation model is implemented from original data-driven models and others previously proposed in literature. Moreover, a new version of the COA that links some concepts from Cultural Algorithms (CA) is proposed, which is validated under a set of benchmarks functions from the Institute of Electrical and Electronics Engineers (IEEE) Congress on Evolutionary Computation (CEC) 2017 and tested to the GT problem. The results show that the proposed Cultural Coyote Optimization Algorithm (CCOA) outperforms its counterpart for benchmark functions. Further, non-parametric statistical significance tests prove that the CCOA’s performance is competitive when compared to other state-of-the-art metaheuristics after a set of experiments for five case studies. In addition, the convergence analysis shows that the cultural mechanism employed in the CCOA has improved the COA balance between exploration and exploitation. As a result, the CCOA can improve the current GT operation significantly, reducing the fuel consumption up to None None None None 3.6 None % None None None None meanwhile all constraints are accomplished.",project-academic
10.1007/S10845-020-01549-2,2020-02-25,a,Springer US,quality analysis in metal additive manufacturing with deep learning," As a promising modern technology, additive manufacturing (AM) has been receiving increasing research and industrial attention in the recent years. With its rapid development, the importance of quality monitoring in AM process has been recognized, which significantly affects the property of the manufactured parts. Since the conventional hand-crafted features for quality identification are generally costly, time-consuming and sensitive to noises, the intelligent data-driven automatic process monitoring methods are becoming more and more popular at present. This paper proposes a deep learning-based quality identification method for metal AM process. To alleviate the requirement for large amounts of high-quality labeled training data by most existing data-driven methods, an identification consistency-based approach is proposed to better explore the semi-supervised training data. The proposed method is able to achieve promising performance using limited supervised samples with low quality, such as noisy and blurred images. Experiments on a real-world metal AM dataset are implemented to validate the effectiveness of the proposed method, which offers a promising tool for real industrial applications.",project-academic
10.1145/3356250.3360035,2019-11-10,p,ACM,faho deep learning enhanced holographic localization for rfid tags," In recent years, radio frequency identification (RFID)-based approaches have been demonstrated to be a promising indoor localization techniques for many valuable applications, such as tracking tagged objects on the manufacturing lines, locating items in smart warehouses, and so on. In the near future, many applications will gain great benefits from knowing the positions of RFID-tagged objects. However, existing localization approaches often suffer from severe accuracy degradation in real-world environments due to the prevalent environmental interferences, such as the multipath effects. To this end, we designed an RFID-based localization system FaHo, which leverages a deep learning enhanced holographic technique for locating RFID tags accurately even in complex indoor environments. By carefully analyzing the features of the traditional holographic method, we created a new hologram-based algorithm called joint hologram, which yields a robust likelihood for each assumed position to be the true tag position. FaHo then adopts a deep convolutional neural network for analyzing the whole hologram, and subsequently estimate the true location of the RFID tag rather than simply seek for the largest-likelihood location. Furthermore, we implemented FaHo and evaluated its performance in several multipath-rich scenarios. The experimental results show that FaHo can achieve centimeter-level accuracy in both the lateral and radial directions using only one moving antenna. More importantly, our work also demonstrates that hologram-based localization is a highly effective technique for RFID indoor localization tasks.",project-academic
10.1109/CASE48305.2020.9216781,2020-08-01,p,IEEE,weak scratch detection of optical components using attention fusion network," Scratches on the optical surface can directly affect the reliability of the optical system. Machine vision-based methods have been widely applied in various industrial surface defect inspection scenarios. Since weak scratches imaging in the dark field has an ambiguous edge and low contrast, which brings difficulty in automatic defect detection. Recently, many existing visual inspection methods based on deep learning cannot effectively inspect weak scratches due to the lack of attention-aware features. To address the problems arising from industry-specific characteristics, this paper proposes “Attention Fusion Network;”, a convolutional neural network using attention mechanism built by hard and soft attention modules to generate attention-aware features. The hard attention module is implemented by integrating the brightness adjustment operation in the network, and the soft attention module is composed of scale attention and channel attention. The proposed model is trained on a real-world industrial scratch dataset and compared with other defect inspection methods. The proposed method can achieve the best performance to detect the weak scratch inspection of optical components compared to the traditional scratch detection methods and other deep learning-based methods",project-academic
10.1109/MIC.2020.2979620,2020-03-01,a,Institute of Electrical and Electronics Engineers (IEEE),knowledge graph semantic enhancement of input data for improving ai," Intelligent systems designed using machine learning algorithms require a large number of labeled data. Background knowledge provides complementary, real-world factual information that can augment the limited labeled data to train a machine learning algorithm. The term Knowledge Graph (KG) is in vogue as for many practical applications, it is convenient and useful to organize this background knowledge in the form of a graph. Recent academic research and implemented industrial intelligent systems have shown promising performance for machine learning algorithms that combine training data with a knowledge graph. In this article, we discuss the use of relevant KGs to enhance the input data for two applications that use machine learning—recommendation and community detection. The KG improves both accuracy and explainability.",project-academic
10.1007/S40290-018-0251-9,2018-10-13,a,Springer International Publishing,training augmented intelligent capabilities for pharmacovigilance applying deep learning approaches to individual case safety report processing," Regulations are increasing the scope of activities that fall under the remit of drug safety. Currently, individual case safety report (ICSR) collection and collation is done manually, requiring pharmacovigilance professionals to perform many transactional activities before data are available for assessment and aggregated analyses. For a biopharmaceutical company to meet its responsibilities to patients and regulatory bodies regarding the safe use and distribution of its products, improved business processes must be implemented to drive the industry forward in the best interest of patients globally. Augmented intelligent capabilities have already demonstrated success in capturing adverse events from diverse data sources. It has potential to provide a scalable solution for handling the ever-increasing ICSR volumes experienced within the industry by supporting pharmacovigilance professionals’ decision-making. The aim of this study was to train and evaluate a consortium of cognitive services to identify key characteristics of spontaneous ICSRs satisfying an acceptable level of accuracy determined by considering business requirements and effective use in a real-world setting. The results of this study will serve as supporting evidence for or against implementing augmented intelligence in case processing to increase operational efficiency and data quality consistency. A consortium of ten cognitive services to augment aspects of ICSR processing were identified and trained through deep-learning approaches. The input data for model training were 20,000 ICSRs received by Celgene drug safety over a 2-year period. The data were manually made machine-readable through the process of transcription, which converts images into text. The machine-readable documents were manually annotated for pharmacovigilance data elements to facilitate the training and testing of the cognitive services. Once trained by cognitive developers, the cognitive services’ output was reviewed by pharmacovigilance subject-matter experts against the accepted ground-truth for correctness and completeness. To be considered adequately trained and functional, each cognitive service was required to reach a threshold of F1 or accuracy score ≥ 75%. All ten cognitive services under development have reached an evaluative score ≥ 75% for spontaneous ICSRs. All cognitive services under development have achieved the minimum evaluative threshold to be considered adequately trained, demonstrating how machine-learning and natural language processing techniques together provide accurate outputs that may augment pharmacovigilance professionals’ processing of spontaneous ICSRs quickly and accurately. The intention of augmented intelligence is not to replace the pharmacovigilance professional, but rather support them in their consistent decision-making so that they may better handle the overwhelming amount of data otherwise manually curated and monitored for ongoing drug surveillance requirements. Through this supported decision-making, pharmacovigilance professionals may have more time to apply their knowledge in assessing the case rather than spending it performing transactional tasks to simply capture the pertinent data within a safety database. By capturing data consistently and efficiently, we begin to build a corpus of data upon which analyses may be conducted and insights gleaned. Cognitive services may be key to an organization’s transformation to more proactive decision-making needed to meet regulatory requirements and enhance patient safety.",project-academic
10.1109/ACCESS.2019.2924030,2019-06-20,a,IEEE,demand response management for industrial facilities a deep reinforcement learning approach," As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.",project-academic
10.1007/978-1-4471-2303-3_10,2009-11-01,a,"Springer, London",bifurcation analysis and control of a class of hybrid biological economic models," For many real-world power systems with digital controllers, the dynamics of the generators as well as their continuous-time controllers and the load dynamics together define the ordinary differential equations while algebraic equalities are defined by the power balance equations of the transmission networks [4]. On the other hand, the difference equations govern the dynamics of the digital devices. These digital control devices require both continuous and discrete time descriptions that can be formulated as differential-difference-algebraic equations(or a hybrid dynamical system). Besides such industrial systems, models of a biological system such as neural networks [1, 2, 13] and genetic networks [11] can also be described as hybrid dynamical systems. For instance, chaotic dynamics of biological neurons [8] have been implemented by electronic circuit models both with discrete-time [7] and with continuous-time [10]. A hybrid system of chaotic neural networks obtained by combining these hardware neuron models can be described by DDA equations with electrical circuit constraints due to Kirchhoff’s laws. Moreover, gene expression processes [5] and genome-proteome networks [9] are also typical models mixed with both continuous and discrete time sequences. Therefore, it is necessary to analyze nonlinear properties of such dynamical systems.",project-academic
,2019-05-28,a,,implementing scada scenarios and introducing attacks to obtain training data for intrusion detection methods," There are hardly any data sets publicly available that can be used to evaluate intrusion detection algorithms. The biggest threat for industrial applications arises from state-sponsored and criminal groups. Often, formerly unknown exploits are employed by these attackers, so-called 0-day exploits. They cannot be discovered with signature-based intrusion detection. Thus, statistical or machine learning based anomaly detection lends itself readily. These methods especially, however, need a large amount of labelled training data. In this work, an exemplary industrial use case with real-world industrial hardware is presented. Siemens S7 Programmable Logic Controllers are used to control a real world-based control application using the OPC UA protocol: A pump, filling and emptying water tanks. This scenario is used to generate application specific network data. Furthermore, attacks are introduced into this data set. This is done in three ways: First, the normal process is monitored and captured. Common attacks are then synthetically introduced into this data set. Second, malicious behaviour is implemented on the Programmable Logic Controller program and executed live, the traffic is captured as well. Third, malicious behaviour is implemented on the Programmable Logic Controller while still keeping the same output behaviour as in normal operation. An attacker could exploit an application but forge valid sensor output so that no anomaly is detected. Sensors are employed, capturing temperature, sound and flow of water to create data that can be correlated to the network data and used to still detect the attack. All data is labelled, containing the ground truth, meaning all attacks are known and no unknown attacks occur. This makes them perfect for training of anomaly detection algorithms. The data is published to enable security researchers to evaluate intrusion detection solutions.",project-academic
10.1109/RTSI.2017.8065980,2017-09-01,p,IEEE,opeb open physical environment benchmark for artificial intelligence," Artificial Intelligence methods to solve continuous-control tasks have made significant progress in recent years. However, these algorithms have important limitations and still need significant improvement to be used in industry and real-world applications. This means that this area is still in an active research phase. To involve a large number of research groups, standard benchmarks are needed to evaluate and compare proposed algorithms. In this paper, we propose a physical environment benchmark framework to facilitate collaborative research in this area by enabling different research groups to integrate their designed benchmarks in a unified cloud-based repository and also share their actual implemented benchmarks via the cloud. We demonstrate the proposed framework using an actual implementation of the classical mountain-car example and present the results obtained using a Reinforcement Learning algorithm.",project-academic
10.1145/3459637.3482246,2021-10-26,p,,dcap deep cross attentional product network for user response prediction," User response prediction, which aims to predict the probability that a user will provide a predefined positive response in a given context such as clicking on an ad or purchasing an item, is crucial to many industrial applications such as online advertising, recommender systems, and search ranking. For these tasks and many other machine learning tasks, an indispensable part of success is feature engineering, where cross features are a significant type of feature transformations. However, due to the high dimensionality and super sparsity of the data collected in these tasks, handcrafting cross features is inevitably time expensive. Prior studies in predicting user response leveraged the feature interactions by enhancing feature vectors with products of features to model second-order or high-order cross features, either explicitly or implicitly. However, these existing methods can be hindered by not learning sufficient cross features due to model architecture limitations or modeling all high-order feature interactions with equal weights. Different features should contribute differently to the prediction, and not all cross features are with the same prediction power. This work aims to fill this gap by proposing a novel architecture Deep Cross Attentional Product Network (DCAP), which keeps cross network's benefits in modeling high-order feature interactions explicitly at the vector-wise level. By computing the inner product or outer product between attentional feature embeddings and original input embeddings as each layer's output, we can model cross features with a higher degree of order as the network's depth increases. We concatenate all the outputs from each layer, which further helps the model capture much information on cross features of different orders. Beyond that, it can differentiate the importance of different cross features in each network layer inspired by the multi-head attention mechanism and Product Neural Network (PNN), allowing practitioners to perform a more in-depth analysis of user behaviors. Additionally, our proposed model can be easily implemented and train in parallel. We conduct comprehensive experiments on three real-world datasets. The results have robustly demonstrated that our proposed model DCAP achieves superior prediction performance compared with the state-of-the-art models. Public codes are available at https://github.com/zachstarkk/DCAP.",project-academic
,2015-01-01,b,,cognitive computing and big data analytics," A comprehensive guide to learning technologies that unlock the value in big data Cognitive Computing provides detailed guidance toward building a new class of systems that learn from experience and derive insights to unlock the value of big data. This book helps technologists understand cognitive computing's underlying technologies, from knowledge representation techniques and natural language processing algorithms to dynamic learning approaches based on accumulated evidence, rather than reprogramming. Detailed case examples from the financial, healthcare, and manufacturing walk readers step-by-step through the design and testing of cognitive systems, and expert perspectives from organizations such as Cleveland Clinic, Memorial Sloan-Kettering, as well as commercial vendors that are creating solutions. These organizations provide insight into the real-world implementation of cognitive computing systems. The IBM Watson cognitive computing platform is described in a detailed chapter because of its significance in helping to define this emerging market. In addition, the book includes implementations of emerging projects from Qualcomm, Hitachi, Google and Amazon. Today's cognitive computing solutions build on established concepts from artificial intelligence, natural language processing, ontologies, and leverage advances in big data management and analytics. They foreshadow an intelligent infrastructure that enables a new generation of customer and context-aware smart applications in all industries. Cognitive Computing is a comprehensive guide to the subject, providing both the theoretical and practical guidance technologists need. * Discover how cognitive computing evolved from promise to reality * Learn the elements that make up a cognitive computing system * Understand the groundbreaking hardware and software technologies behind cognitive computing * Learn to evaluate your own application portfolio to find the best candidates for pilot projects * Leverage cognitive computing capabilities to transform the organization Cognitive systems are rightly being hailed as the new era of computing. Learn how these technologies enable emerging firms to compete with entrenched giants, and forward-thinking established firms to disrupt their industries. Professionals who currently work with big data and analytics will see how cognitive computing builds on their foundation, and creates new opportunities. Cognitive Computing provides complete guidance to this new level of human-machine interaction.",project-academic
10.1109/TASE.2012.2198057,2012-05-22,a,Institute of Electrical and Electronics Engineers Inc.,neural network based optimal control for a class of unknown discrete time nonlinear systems using globalized dual heuristic programming," In this paper, a neuro-optimal control scheme for a class of unknown discrete-time nonlinear systems with discount factor in the cost function is developed. The iterative adaptive dynamic programming algorithm using globalized dual heuristic programming technique is introduced to obtain the optimal controller with convergence analysis in terms of cost function and control law. In order to carry out the iterative algorithm, a neural network is constructed first to identify the unknown controlled system. Then, based on the learned system model, two other neural networks are employed as parametric structures to facilitate the implementation of the iterative algorithm, which aims at approximating at each iteration the cost function and its derivatives and the control law, respectively. Finally, a simulation example is provided to verify the effectiveness of the proposed optimal control approach. Note to Practitioners-The increasing complexity of the real-world industry processes inevitably leads to the occurrence of nonlinearity and high dimensions, and their mathematical models are often difficult to build. How to design the optimal controller for nonlinear systems without the requirement of knowing the explicit model has become one of the main foci of control practitioners. However, this problem cannot be handled by only relying on the traditional dynamic programming technique because of the ""curse of dimensionality"". To make things worse, the backward direction of solving process of dynamic programming precludes its wide application in practice. Therefore, in this paper, the iterative adaptive dynamic programming algorithm is proposed to deal with the optimal control problem for a class of unknown nonlinear systems forward-in-time. Moreover, the detailed implementation of the iterative ADP algorithm through the globalized dual heuristic programming technique is also presented by using neural networks. Finally, the effectiveness of the control strategy is illustrated via simulation study.",project-academic
,2001-11-15,b,,stable adaptive control and estimation for nonlinear systems neural and fuzzy approximator techniques," From the Publisher:
A powerful, yet easy-to-use design methodology for the control of nonlinear dynamic systems
A key issue in the design of control systems is proving that the resulting closed-loop system is stable, especially in cases of high consequence applications, where process variations or failure could result in unacceptable risk. Adaptive control techniques provide a proven methodology for designing stable controllers for systems that may possess a large amount of uncertainty. At the same time, the benefits of neural networks and fuzzy systems are generating much excitement-and impressive innovations-in almost every engineering discipline.
Stable Adaptive Control and Estimation for Nonlinear Systems: Neural and Fuzzy Approximator Techniques brings together these two different but equally useful approaches to the control of nonlinear systems in order to provide students and practitioners with the background necessary to understand and contribute to this emerging field.
The text presents a control methodology that may be verified with mathematical rigor while possessing the flexibility and ease of implementation associated with ""intelligent control"" approaches. The authors show how these methodologies may be applied to many real-world systems including motor control, aircraft control, industrial automation, and many other challenging nonlinear systems. They provide explicit guidelines to make the design and application of the various techniques a practical and painless process.
Design techniques are presented for nonlinear multi-input multi-output (MIMO) systems in state-feedback, output-feedback, continuous or discrete-time, or even decentralized form. To help students and practitioners new to the field grasp and sustain mastery of the material, the book features:
Background material on fuzzy systems and neural networksStep-by-step controller designNumerous examplesCase studies using ""real world"" applicationsHomework problems and design projects",project-academic
10.1109/IPDPS.2019.00105,2019-05-20,p,IEEE,cpp taskflow fast task based parallel programming using modern c," In this paper we introduce Cpp-Taskflow, a new C++ tasking library to help developers quickly write parallel programs using task dependency graphs. Cpp-Taskflow leverages the power of modern C++ and task-based approaches to enable efficient implementations of parallel decomposition strategies. Our programming model can quickly handle not only traditional loop-level parallelism, but also irregular patterns such as graph algorithms, incremental flows, and dynamic data structures. Compared with existing libraries, Cpp-Taskflow is more cost efficient in performance scaling and software integration. We have evaluated Cpp-Taskflow on both micro-benchmarks and real-world applications with million-scale tasking. In a machine learning example, Cpp-Taskflow achieved 1.5–2.7× less coding complexity and 14–38% speed-up over two industrial-strength libraries OpenMP Tasking and Intel Threading Building Blocks (TBB).",project-academic
10.1109/CVPR.2019.00655,2019-06-15,p,IEEE,l3 net towards learning based lidar localization for autonomous driving," We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.",project-academic
10.1002/J.2168-9830.2008.TB00949.X,2008-01-01,a,Blackwell Publishing Ltd,2006 bernard m gordon prize lecture the learning factory industry partnered active learning," On February 21, 2006, the National Academy of Engineering recognized the achievements of the Learning Factory with the Bernard M. Gordon Prize for Innovation in Engineering and Technology Education. The co-founders were commended “for creating the Learning Factory, where multidisciplinary student teams develop engineering leadership skills by working with industry to solve real-world problems.” This paper describes the origins, motivation, philosophy, and implementation of the Learning Factory.

The specific innovations of the Learning Factory partnership were: active learning facilities, called Learning Factories, that provide experiential reinforcement of engineering science, and a realization of its limitations; strong collaborations with industry through advisory boards, engineers in the classroom, and industry-sponsored capstone design projects; practice-based engineering courses integrating analytical and theoretical knowledge with manufacturing, design, business concepts, and professional skills; and dissemination to other academic institutions (domestic and international), government and industry.",project-academic
10.1145/3447548.3467437,2021-08-14,p,Association for Computing Machinery,global neighbor sampling for mixed cpu gpu training on giant graphs," Graph neural networks (GNNs) are powerful tools for learning from graph data and are widely used in various applications such as social network recommendation, fraud detection, and graph search. The graphs in these applications are typically large, usually containing hundreds of millions of nodes. Training GNN models on such large graphs efficiently remains a big challenge. Despite a number of sampling-based methods have been proposed to enable mini-batch training on large graphs, these methods have not been proved to work on truly industry-scale graphs, which require GPUs or mixed CPU-GPU training. The state-of-the-art sampling-based methods are usually not optimized for these real-world hardware setups, in which data movement between CPUs and GPUs is a bottleneck. To address this issue, we propose Global Neighborhood Sampling that aims at training GNNs on giant graphs specifically for mixed CPU-GPU training. The algorithm samples a global cache of nodes periodically for all mini-batches and stores them in GPUs. This global cache allows in-GPU importance sampling of mini-batches, which drastically reduces the number of nodes in a mini-batch, especially in the input layer, to reduce data copy between CPU and GPU and mini-batch computation without compromising the training convergence rate or model accuracy. We provide a highly efficient implementation of this method and show that our implementation outperforms an efficient node-wise neighbor sampling baseline by a factor of 2× ~ 4× on giant graphs. It outperforms an efficient implementation of LADIES with small layers by a factor of 2× ~ 14× while achieving much higher accuracy than LADIES. We also theoretically analyze the proposed algorithm and show that with cached node data of a proper size, it enjoys a comparable convergence rate as the underlying node-wise sampling method.",project-academic
,2021-06-11,a,,global neighbor sampling for mixed cpu gpu training on giant graphs," Graph neural networks (GNNs) are powerful tools for learning from graph data and are widely used in various applications such as social network recommendation, fraud detection, and graph search. The graphs in these applications are typically large, usually containing hundreds of millions of nodes. Training GNN models on such large graphs efficiently remains a big challenge. Despite a number of sampling-based methods have been proposed to enable mini-batch training on large graphs, these methods have not been proved to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU training. The state-of-the-art sampling-based methods are usually not optimized for these real-world hardware setups, in which data movement between CPUs and GPUs is a bottleneck. To address this issue, we propose Global Neighborhood Sampling that aims at training GNNs on giant graphs specifically for mixed-CPU-GPU training. The algorithm samples a global cache of nodes periodically for all mini-batches and stores them in GPUs. This global cache allows in-GPU importance sampling of mini-batches, which drastically reduces the number of nodes in a mini-batch, especially in the input layer, to reduce data copy between CPU and GPU and mini-batch computation without compromising the training convergence rate or model accuracy. We provide a highly efficient implementation of this method and show that our implementation outperforms an efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant graphs. It outperforms an efficient implementation of LADIES with small layers by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also theoretically analyze the proposed algorithm and show that with cached node data of a proper size, it enjoys a comparable convergence rate as the underlying node-wise sampling method.",project-academic
,2017-01-01,a,AI Now Institute at New York University,ai now 2017 report," These recommendations reflect the views and research of the AI Now Institute at New York University. We thank the experts who contributed to the AI Now 2017 Symposium and Workshop for informing these perspectives, and our research team for helping shape the AI Now 2017 Report. Artificial intelligence (AI) technologies are in a phase of rapid development, and are being adopted widely. While the concept of artificial intelligence has existed for over sixty years, real-world applications have only accelerated in the last decade due to three concurrent developments: better algorithms, increases in networked computing power and the tech industry’s ability to capture and store massive amounts of data. AI systems are already integrated in everyday technologies like smartphones and personal assistants, making predictions and determinations that help personalize experiences and advertise products. Beyond the familiar, these systems are also being introduced in critical areas like law, finance, policing and the workplace, where they are increasingly used to predict everything from our taste in music to our likelihood of committing a crime to our fitness for a job or an educational opportunity. AI companies promise that the technologies they create can automate the toil of repetitive work, identify subtle behavioral patterns and much more. However, the analysis and understanding of artificial intelligence should not be limited to its technical capabilities. The design and implementation of this next generation of computational tools presents deep normative and ethical challenges for our existing social, economic and political relationships and institutions, and these changes are already underway. Simply put, AI does not exist in a vacuum. We must also ask how broader phenomena like widening inequality, an intensification of concentrated geopolitical power and populist political movements will shape and be shaped by the development and application of AI technologies.",project-academic
10.1201/B11431,2011-12-20,b,CRC Press,manifold learning theory and applications," Trained to extract actionable information from large volumes of high-dimensional data, engineers and scientists often have trouble isolating meaningful low-dimensional structures hidden in their high-dimensional observations. Manifold learning, a groundbreaking technique designed to tackle these issues of dimensionality reduction, finds widespread application in machine learning, neural networks, pattern recognition, image processing, and computer vision. Filling a void in the literature, Manifold Learning Theory and Applications incorporates state-of-the-art techniques in manifold learning with a solid theoretical and practical treatment of the subject. Comprehensive in its coverage, this pioneering work explores this novel modality from algorithm creation to successful implementationoffering examples of applications in medical, biometrics, multimedia, and computer vision. Emphasizing implementation, it highlights the various permutations of manifold learning in industry including manifold optimization, large scale manifold learning, semidefinite programming for embedding, manifold models for signal acquisition, compression and processing, and multi scale manifold. Beginning with an introduction to manifold learning theories and applications, the book includes discussions on the relevance to nonlinear dimensionality reduction, clustering, graph-based subspace learning, spectral learning and embedding, extensions, and multi-manifold modeling. It synergizes cross-domain knowledge for interdisciplinary instructions, offers a rich set of specialized topics contributed by expert professionals and researchers from a variety of fields. Finally, the book discusses specific algorithms and methodologies using case studies to apply manifold learning for real-world problems.",project-academic
,2005-12-01,b,Addison-Wesley Professional,opengl r shading language 2nd edition," ""As the 'Red Book' is known to be the gold standard for OpenGL, the 'Orange Book' is considered to be the gold standard for the OpenGL Shading Language. With Randi's extensive knowledge of OpenGL and GLSL, you can be assured you will be learning from a graphics industry veteran. Within the pages of the second edition you can find topics from beginning shader development to advanced topics such as the spherical harmonic lighting model and more.""-David Tommeraasen, CEO/Programmer, Plasma Software""This will be the definitive guide for OpenGL shaders; no other book goes into this detail. Rost has done an excellent job at setting the stage for shader development, what the purpose is, how to do it, and how it all fits together. The book includes great examples and details, and good additional coverage of 2.0 changes!""-Jeffery Galinovsky, Director of Emerging Market Platform Development, Intel Corporation""The coverage in this new edition of the book is pitched just right to help many new shader-writers get started, but with enough deep information for the 'old hands.'""-Marc Olano, Assistant Professor, University of Maryland""This is a really great book on GLSL-well written and organized, very accessible, and with good real-world examples and sample code. The topics flow naturally and easily, explanatory code fragments are inserted in very logical places to illustrate concepts, and all in all, this book makes an excellent tutorial as well as a reference.""-John Carey, Chief Technology Officer, C.O.R.E. Feature AnimationOpenGL® Shading Language, Second Edition, extensively updated for OpenGL 2.0, is the experienced application programmer's guide to writing shaders. Part reference, part tutorial, this book thoroughly explains the shift from fixed-functionality graphics hardware to the new era of programmable graphics hardware and the additions to the OpenGL API that support this programmability. With OpenGL and shaders written in the OpenGL Shading Language, applications can perform better, achieving stunning graphics effects by using the capabilities of both the visual processing unit and the central processing unit.In this book, you will find a detailed introduction to the OpenGL Shading Language (GLSL) and the new OpenGL function calls that support it. The text begins by describing the syntax and semantics of this high-level programming language. Once this foundation has been established, the book explores the creation and manipulation of shaders using new OpenGL function calls.OpenGL® Shading Language, Second Edition, includes updated descriptions for the language and all the GLSL entry points added to OpenGL 2.0; new chapters that discuss lighting, shadows, and surface characteristics; and an under-the-hood look at the implementation of RealWorldz, the most ambitious GLSL application to date. The second edition also features 18 extensive new examples of shaders and their underlying algorithms, including Image-based lighting Lighting with spherical harmonics Ambient occlusion Shadow mapping Volume shadows using deferred lighting Ward's BRDF modelThe color plate section illustrates the power and sophistication of the OpenGL Shading Language. The API Function Reference at the end of the book is an excellent guide to the API entry points that support the OpenGL Shading Language. Also included is a convenient Quick Reference Card to GLSL.",project-academic
10.1145/2830772.2830789,2015-12-05,p,ACM,neuromorphic accelerators a comparison between neuroscience and machine learning approaches," A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality.Within the limit of our study (current SNN and machine-learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.",project-academic
10.1111/TBJ.13718,2020-01-01,a,"John Wiley & Sons, Ltd",natural language processing to facilitate breast cancer research and management," The medical literature has been growing exponentially, and its size has become a barrier for physicians to locate and extract clinically useful information. As a promising solution, natural language processing (NLP), especially machine learning (ML)-based NLP is a technology that potentially provides a promising solution. ML-based NLP is based on training a computational algorithm with a large number of annotated examples to allow the computer to ""learn"" and ""predict"" the meaning of human language. Although NLP has been widely applied in industry and business, most physicians still are not aware of the huge potential of this technology in medicine, and the implementation of NLP in breast cancer research and management is fairly limited. With a real-world successful project of identifying penetrance papers for breast and other cancer susceptibility genes, this review illustrates how to train and evaluate an NLP-based medical abstract classifier, incorporate it into a semiautomatic meta-analysis procedure, and validate the effectiveness of this procedure. Other implementations of NLP technology in breast cancer research, such as parsing pathology reports and mining electronic healthcare records, are also discussed. We hope this review will help breast cancer physicians and researchers to recognize, understand, and apply this technology to meet their own clinical or research needs.",project-academic
,2008-01-01,a,,the learning factory industry partnered active learning," On February 21, 2006, the National Academy of Engineering recognized the achievements of the Learning Factory with the Bernard M. Gordon Prize for Innovation in Engineering and Technology Education. The co-founders were commended “for creating the Learning Factory, where multidisciplinary student teams develop engineering leadership skills by working with industry to solve real-world problems.” This paper describes the origins, motivation, philosophy, and implementation of the Learning Factory. The specific innovations of the Learning Factory partnership were: active learning facilities, called Learning Factories, that provide experiential reinforcement of engineering science, and a realization of its limitations; strong collaborations with industry through advisory boards, engineers in the classroom, and industry-sponsored capstone design projects; practice-based engineering courses integrating analytical and theoretical knowledge with manufacturing, design, business concepts, and professional skills; and dissemination to other academic institutions (domestic and international), government and industry.",project-academic
10.1109/MM.2021.3066343,2021-05-01,a,IEEE Computer Society,the vision behind mlperf understanding ai inference performance," Deep learning has sparked a renaissance in computer systems and architecture. Despite the breakneck pace of innovation, there is a crucial issue concerning the research and industry communities at large: how to enable neutral and useful performance assessment for machine learning (ML) software frameworks, ML hardware accelerators, and ML systems comprising both the software stack and the hardware. The ML field needs systematic methods for evaluating performance that represents real-world use cases and useful for making comparisons across different software and hardware implementations. MLPerf answers the call. MLPerf is an ML benchmark standard driven by academia and industry (70+ organizations). Built out of the expertise of multiple organizations, MLPerf establishes a standard benchmark suite with proper metrics and benchmarking methodologies to level the playing field for ML system performance measurement of different ML inference hardware, software, and services.",project-academic
10.1109/TFUZZ.2014.2315656,2015-04-01,a,IEEE,interval type 2 radial basis function neural network a modeling framework," In this paper, an interval type-2 radial basis function neural network (IT2-RBF-NN) is proposed as a new modeling framework. We take advantage of the functional equivalence of radial basis function neural networks (RBF-NNs) to a class of type-1 fuzzy logic systems (T1-FLS) to propose a new interval type-2 equivalent system; it is systematically shown that the type equivalence (between RBF and FLS) of the new modeling structure is maintained in the case of the IT2 system. The new IT2-RBF-NN incorporates interval type-2 fuzzy sets within the radial basis function layer of the neural network in order to account for linguistic uncertainty in the system's variables. The antecedent part in each rule in the IT2-RBF-NN is an interval type-2 fuzzy set, and the consequent part is of Mamdani type with interval weights, which are used for the Karnik and Mendel type-reduction process in the output layer of the network. The structural and parametric optimization of the IT2-RBF-NN parameters is carried out by a hybrid approach that is based on estimating the initial rule base and footprint of uncertainty (FOU) directly via a granular computing approach and an adaptive back propagation approach. The effectiveness of the new modeling framework is assessed in two parts. First, the IT2-RBF-NN is tested against a number of popular benchmark datasets, and second, it is demonstrated in a real-world industrial application that has particular challenges that are related to the uncertainty of the raw information. Via simulation results, it is shown that the proposed modeling framework performs well as compared with its T1 equivalent system. In addition, a very good computational efficiency is demonstrated as a result of the systematic and automatic creation of IT2 linguistic information and the FOU. Crucially, the proposed modeling framework opens up a host of opportunities for the academic community that already uses the popular T1-RBF-NN-based structure to try the new IT2-RBF-NN and take advantage of the numerous existing RBF-based adaptive learning algorithms, RBF-based multiobjective optimization techniques, granular computing-based information capture techniques, and real-world FLS implementations, and, in general, take advantage of the computational efficiency of the fusion of IT2-FLS and RBF-NN.",project-academic
10.1145/3324884.3415294,2020-12-21,p,IEEE,express an energy efficient and secure framework for mobile edge computing and blockchain based smart systems," As most smart systems such as smart logistic and smart manufacturing are delay sensitive, the current mainstream cloud computing based system architecture is facing the critical issue of high latency over the Internet. Meanwhile, as huge amount of data is generated by smart devices with limited battery and computing power, the increasing demand for energy-efficient machine learning and secure data communication at the network edge has become a hurdle to the success of smart systems. To address these challenges with using smart UAV (Unmanned Aerial Vehicle) delivery system as an example, we propose EXPRESS, a novel energy-efficient and secure framework based on mobile edge computing and blockchain technologies. We focus on computation and data (resource) management which are two of the most prominent components in this framework. The effectiveness of the EXPRESS framework is demonstrated through the implementation of a real-world UAV delivery system. As an open-source framework, EXPRESS can help researchers implement their own prototypes and test their computation and data management strategies in different smart systems. The demo video can be found at https://youtu.be/r3U1iU8tSmk.",project-academic
10.1109/BIGDATA.2017.8258076,2017-12-01,p,IEEE,empirical evaluations of active learning strategies in legal document review," One type of machine learning, text classification, is now regularly applied in the legal matters involving voluminous document populations because it can reduce the time and expense associated with the review of those documents. One form of machine learning — Active Learning — has drawn attention from the legal community because it offers the potential to make the machine learning process even more effective. Active Learning, applied to legal documents, is considered a new technology in the legal domain and is continuously applied to all documents in a legal matter until an insignificant number of relevant documents are left for review. This implementation is slightly different than traditional implementations of Active Learning where the process stops once achieving acceptable model performance. The purpose of this paper is twofold: (i) to question whether Active Learning actually is a superior learning methodology and (ii) to highlight the ways that Active Learning can be most effectively applied to real legal industry data. Unlike other studies, our experiments were performed against large data sets taken from recent, real-world legal matters covering a variety of areas. We conclude that, although these experiments show the Active Learning strategy popularly used in legal document review can quickly identify informative training documents, it becomes less effective over time. In particular, our findings suggest this most popular form of Active Learning in the legal arena, where the highest-scoring documents are selected as training examples, is in fact not the most efficient approach in most instances. Ultimately, a different Active Learning strategy may be best suited to initiate the predictive modeling process but not to continue through the entire document review.",project-academic
10.1097/APO.0000000000000400,2021-05-01,a,Ovid Technologies (Wolters Kluwer Health),considerations for artificial intelligence real world implementation in ophthalmology providers and patients perspectives," ABSTRACT Artificial Intelligence (AI), in particular deep learning, has made waves in the health care industry, with several prominent examples shown in ophthalmology. Despite the burgeoning reports on the development of new AI algorithms for detection and management of various eye diseases, few have reached the stage of regulatory approval for real-world implementation. To better enable real-world translation of AI systems, it is important to understand the demands, needs, and concerns of both health care professionals and patients, as providers and recipients of clinical care are impacted by these solutions. This review outlines the advantages and concerns of incorporating AI in ophthalmology care delivery, from both the providers' and patients' perspectives, and the key enablers for seamless transition to real-world implementation.",project-academic
10.1109/INTELLISYS.2017.8324372,2017-09-01,p,IEEE,machine learning and deep neural network artificial intelligence core for lab and real world test and validation for adas and autonomous vehicles ai for efficient and quality test and validation," Autonomous vehicles are now the future of automobile industry. Human drivers can be completely taken out of the loop through the implementation of safe and intelligent autonomous vehicles. Although we can say that HW and SW development continues to play a large role in the automotive industry, test and validation of these systems is a must. The ability to test these vehicles thoroughly and efficiently will ensure their proper and flawless operation. When a large number of people with heterogeneous knowledge and skills try to develop an autonomous vehicle together, it is important to use a sensible engineering process. State of the art techniques for such development include Waterfall, Agile & V-model, where test & validation (T&V) process is an integral part of such a development cycle. This paper will propose a new methodology using machine learning & deep neural network (AI-core) for lab & real-world T&V for ADAS (Advanced driver assistance system) and autonomous vehicles. The methodology will initially connect T&V of individual systems in each level of development and that of complete system efficiently, by using the proposed phase methodology, in which autonomous driving functions are grouped under categories, special T&V processes are carried on simulation as well as in HIL systems. The complete transition towards AI in the field of T&V will be a sequence of steps. Initially the AI-core is fed with available test scenarios, boundary conditions for the test cases and scenarios, and examples, the AI-core will conduct virtual tests on simulation environment using available test scenarios and further generates new test cases and scenarios for efficient and precise tests. These test cases and scenarios are meant to cover all available cases and concentrate on the area where bugs or failures occur. The complete surrounding environment in the simulation is also controlled by the AI-core which means that the system can attain endless/all-possible combinations of the surrounding environment which is necessary. Results of the tests are sorted and stored, critical and important tests are again repeated in the real-world environment using automated cars with other real subsystems to depict the surrounding environment, which are all controlled by the AI-core, and meanwhile the AI-core is always in the loop and learning from each and every executed test case and its results/outcomes. The main goal is to achieve efficient and high quality test and validation of systems for automated driving, which can save precious time in the development process. As a future scope of this methodology, we can step-up to make most parts of test and validation completely autonomous.",project-academic
10.1186/S40887-019-0029-5,2019-03-01,a,SpringerOpen,the quality management ecosystem for predictive maintenance in the industry 4 0 era," The Industry 4.0 era requires new quality management systems due to the ever increasing complexity of the global business environment and the advent of advanced digital technologies. This study presents new ideas for predictive quality management based on an extensive review of the literature on quality management and five real-world cases of predictive quality management based on new technologies. The results of the study indicate that advanced technology enabled predictive maintenance can be applied in various industries by leveraging big data analytics, smart sensors, artificial intelligence (AI), and platform construction. Such predictive quality management systems can become living ecosystems that can perform cause-effect analysis, big data monitoring and analytics, and effective decision-making in real time. This study proposes several practical implications for actual design and implementation of effective predictive quality management systems in the Industry 4.0 era. However, the living predictive quality management ecosystem should be the product of the organizational culture that nurtures collaborative efforts of all stakeholders, sharing of information, and co-creation of shared goals.",project-academic
,2009-11-28,b,,applying computational intelligence how to create value," The flow of academic ideas in the area of computational intelligence is impacting industrial practice at considerable speed. Practitioners face the challenge of tracking, understanding and applying the latest techniques, which often prove their value even before the underlying theories are fully understood. This book offers realistic guidelines on creating value from the application of computational intelligence methods. In Part I, the author offers simple explanations of the key computational intelligence technologies: fuzzy logic, neural networks, support vector machines, evolutionary computation, swarm intelligence, and intelligent agents. In Part II, he defines the typical business environment and analyzes the competitive advantages these techniques offer. In Part III, he introduces a methodology for effective real-world application of computational intelligence while minimizing development cost, and he outlines the critical, underestimated technology marketing efforts required. The methodology can improve the existing capabilities of Six Sigma, one of the most popular work processes in industry. Finally, in Part IV the author looks to technologies still in the research domain, such as perception-based computing, artificial immune systems, and systems with evolved structure, and he examines the future for computational intelligence applications while taking into account projected industrial needs. The author adopts a light tone in the book, visualizes many of the techniques and ideas, and supports the text with notes from successful implementations. The book is ideal for engineers implementing these techniques in the real world, managers charged with creating value and reducing costs in the related industries, and scientists in computational intelligence looking towards the application of their research.",project-academic
10.1145/2507157.2507195,2013-10-12,p,ACM,distributed matrix factorization with mapreduce using a series of broadcast joins," The efficient, distributed factorization of large matrices on clusters of commodity machines is crucial to applying latent factor models in industrial-scale recommender systems. We propose an efficient, data-parallel low-rank matrix factorization with Alternating Least Squares which uses a series of broadcast-joins that can be efficiently executed with MapReduce.We empirically show that the performance of our solution is suitable for real-world use cases. We present experiments on two publicly available datasets and on a synthetic dataset termed Bigflix, generated from the Netflix dataset. Bigflix contains 25 million users and more than 5 billion ratings, mimicking data sizes recently reported as Netflix' production workload. We demonstrate that our approach is able to run an iteration of Alternating Least Squares in six minutes on this dataset. Our implementation has been contributed to the open source machine learning library Apache Mahout.",project-academic
10.1016/J.CIE.2019.106031,2019-11-01,a,Pergamon,machine learning based concept drift detection for predictive maintenance," Abstract None None In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope.",project-academic
10.5555/2755753.2755784,2015-03-09,p,IEEE,scandalee a side channel based disassembler using local electromagnetic emanations," Side-channel analysis has become a well-established topic in the scientific community and industry over the last one and a half decade. Somewhat surprisingly, the vast majority of work on side-channel analysis has been restricted to the ""use case"" of attacking cryptographic implementations through the recovery of keys. In this contribution, we show how side-channel analysis can be used for extracting code from embedded systems based on a CPU's electromagnetic emanation. There are many applications within and outside the security community where this is desirable. In cryptography, it can, e.g., be used for recovering proprietary ciphers and security protocols. Another broad application field is general security and reverse engineering, e.g., for detecting IP violations of firmware or for debugging embedded systems when there is no debug interface or it is proprietary.A core feature of our approach is that we take localized electromagnetic measurements that are spatially distributed over the IC being analyzed. Given these multiple inputs, we model code extraction as a classification problem that we solve with supervised learning algorithms. We apply a variant of linear discriminant analysis to distinguish between the multiple classes. In contrast to previous approaches, which reported instruction recognition rates between 40--70%, our approach detects more than 95% of all instructions for test code, and close to 90% for real-world code. The methods are thus very relevant for use in practice. Our method performs dynamic code recognition, which has both advantages (only the program parts that are actually executed are observed) but also limitations (rare code executions are difficult to observe).",project-academic
10.1016/J.YMSSP.2020.107510,2021-06-16,a,Academic Press,metric based meta learning model for few shot fault diagnosis under multiple limited data conditions," Abstract None None The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model.",project-academic
10.1167/TVST.9.2.45,2020-08-11,a,Association for Research in Vision and Ophthalmology (ARVO),current challenges and barriers to real world artificial intelligence adoption for the healthcare system provider and the patient," Artificial intelligence (AI), or the use of automated systems that display the ability to correctly interpret, to learn from, and to achieve specific goals by use of external data, is an emerging technology that has myriad implications for changing the way we interact with the world. Although this technology is already being used in many fields such as banking, retail, and education, AI has the potential to transform other fields including healthcare. Within healthcare, ophthalmology is uniquely positioned to benefit from AI not only through clinical decision support technology but also through improved image processing innovations such as real-time segmentation, automated image quality improvements, and assisted or autonomous disease screening tools.1,2 Although there are now Food and Drug Administration–approved technologies within ophthalmology, such as IDx-DR (Coralville, IA, USA) for early diagnosis of diabetic retinopathy and diabetic macular edema, numerous challenges still exist to realize the potentially transformative impact of these technologies in day to day practice.

The need for the ophthalmology community to take a thoughtful approach to AI innovation and implementation is accentuated by the high stakes involved. The impact of misleading patients and clinicians on a health condition is much greater than a retail store misinterpreting the next book you may like to buy. As a result, we need to increase discussion about the issues surrounding who, what, when, how, and why we might use AI in practice, including ethical and liability considerations, to determine how best to implement AI for all stakeholders including practitioners, patients, practices/hospitals, and industry. This article aims to highlight the challenges and barriers to real-world AI adoption that impact the technology's utility. We examine these specific challenges that will face health care organizations, providers, and patients.",project-academic
10.1108/ET-06-2018-0138,2019-05-15,a,"Emerald Group Publishing Limited. Howard House, Wagon Lane, Bingley, West Yorkshire, BD16 1WA, UK. Tel: +44-1274-777700; Fax: +44-1274-785201; e-mail: emerald@emeraldinsight.com; Web site: http://www.emeraldinsight.com",students learning experience in a multidisciplinary innovation project," Purpose




Collaboration between universities and industry is increasingly perceived as a vehicle to enhance innovation. Educational institutions are encouraged to build partnerships and multidisciplinary projects based around real-world open problems. Projects need to benefit student learning, not only the organisations looking for innovations. The context of this study is a multidisciplinary innovation project, as experienced by the students of an University of Applied Sciences in Finland. The purpose of this paper is to unfold students’ conceptions of the learning experience, to help teachers and curriculum designers to organise optimal conditions and processes, and support competence development. The research question was: How do students in higher professional education experience their learning in a multidisciplinary innovation project?




Design/methodology/approach




The study took a phenomenographic approach. The data were collected in the form of weekly diaries, maintained by the cultural management and social services students (n=74) in a mandatory multidisciplinary innovation project in professional higher education in Finland. The diary data were analysed using thematic inductive analysis.




Findings




The results of the study revealed that students’ understood the learning experience in relation to solvable conflicts and unusual situations they experienced during the project, while becoming aware of and claiming their collaborative agency and internalising phases of an innovation process. The competences as learning outcomes that students could name as developed related to content knowledge, different personal characteristics, social skills, emerging leadership skills, creativity, future orientation, social skills, technical, crafting and testing skills and innovation implementation-related skills, such as marketing, sales and entrepreneurship planning skills. However, future orientation and implementation planning skills showed more weakly than other variables in the data.




Practical implications




The findings suggest that curriculum design should enable networked, student-led and teacher supported pedagogical innovation processes that involve a whole path from future thinking and idea development through prototyping to implementation planning of the novel solution. Teachers promote deep comprehension of the innovation process, monitor and ease the pain of conflict if it threatens motivation, offer assessment tools and help in recognising gaps in individual competences and development needs, promote more future-oriented, concrete and implementable outcomes, and facilitate in bridging from innovation towards entrepreneurship planning.




Originality/value




The multidisciplinary innovation project described in this study provides a pedagogical way to connect higher education to the practises of society. These results provide encouraging findings for organising multidisciplinary project activities between education and working life. The paper, therefore, has significant value for teachers and entrepreneurship educators in designing curriculum and facilitating projects. The study promotes the dissemination of innovation development programmes in between education and work organisations also in other than technical and commercial fields.",project-academic
10.5923/J.AJIS.20160601.01,2016-01-01,a,Scientific & Academic Publishing,machine learning algorithms in heavy process manufacturing," In a global economy, manufacturers mainly compete with cost efficiency of production, as the price of raw materials are similar worldwide. Heavy industry has two big issues to deal with. On the one hand there is lots of data which needs to be analyzed in an effective manner, and on the other hand making big improvements via investments in cooperate structure or new machinery is neither economically nor physically viable. Machine learning offers a promising way for manufacturers to address both these problems as they are in an excellent position to employ learning techniques with their massive resource of historical production data. However, choosing modelling a strategy in this setting is far from trivial and this is the objective of this article. The article investigates characteristics of the most popular classifiers used in industry today. Support Vector Machines, Multilayer Perceptron, Decision Trees, Random Forests, and the meta-algorithms Bagging and Boosting are mainly investigated in this work. Lessons from real-world implementations of these learners are also provided together with future directions when different learners are expected to perform well. The importance of feature selection and relevant selection methods in an industrial setting are further investigated. Performance metrics have also been discussed for the sake of completion.",project-academic
10.1007/978-981-15-4624-2_48,2019-09-22,a,"Springer, Singapore",linking seakeeping performance predictions with onboard measurements for surface platform digital twins," Despite the hype surrounding digital twin technology and its implementation in other fields, the marine industry has published very few successful, full-scale applications of this technology to date. The development of useful digital twin technology within the industry requires fundamental exploration of data fusion techniques in the marine context. Future development of a useful surface platform digital twin that incorporates advanced machine learning techniques requires preliminary experimentation with more basic, well-understood learning models using full-scale, real-world data. The goal of this work was to lay a foundation for addressing the current knowledge gap through fusion of seakeeping predictions with full-scale measured motions using linear least-squares and neural network corrections. Motion data were obtained from an oceanographic research cruise and these correction methods were applied to frequency-domain response predictions. The success of these relatively simplistic correction models provided insight for the next steps of surface platform data fusion algorithm development.",project-academic
10.1145/3203217.3203241,2018-05-08,p,ACM,ganfuzz a gan based industrial network protocol fuzzing framework," In this paper, we attempt to improve industrial safety from the perspective of communication security. We leverage the protocol fuzzing technology to reveal errors and vulnerabilities inside implementations of industrial network protocols(INPs). Traditionally, to effectively conduct protocol fuzzing, the test data has to be generated under the guidance of protocol grammar, which is built either by interpreting the protocol specifications or reverse engineering from network traces. In this study, we propose an automated test case generation method, in which the protocol grammar is learned by deep learning. Generative adversarial network(GAN) is employed to train a generative model over real-world protocol messages to enable us to learn the protocol grammar. Then we can use the trained generative model to produce fake but plausible messages, which are promising test cases. Based on this approach, we present an automatical and intelligent fuzzing framework(GANFuzz) for testing implementations of INPs. Compared to prior work, GANFuzz offers a new way for this problem. Moreover, GANFuzz does not rely on protocol specification, so that it can be applied to both public and proprietary protocols, which outperforms many previous frameworks. We use GANFuzz to test several simulators of the Modbus-TCP protocol and find some errors and vulnerabilities.",project-academic
10.1109/83.791960,1999-10-01,a,IEEE,real time dsp implementation for mrf based video motion detection," This paper describes the real time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling, MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach.",project-academic
10.1016/J.PROCIR.2019.02.101,2019-01-01,a,Elsevier BV,autonomous order dispatching in the semiconductor industry using reinforcement learning," Abstract None None Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system.",project-academic
10.3390/S19163602,2019-08-19,a,Multidisciplinary Digital Publishing Institute,bin picking for planar objects based on a deep learning network a case study of usb packs," Random bin-picking is a prominent, useful, and challenging industrial robotics application. However, many industrial and real-world objects are planar and have oriented surface points that are not sufficiently compact and discriminative for those methods using geometry information, especially depth discontinuities. This study solves the above-mentioned problems by proposing a novel and robust solution for random bin-picking for planar objects in a cluttered environment. Different from other research that has mainly focused on 3D information, this study first applies an instance segmentation-based deep learning approach using 2D image data for classifying and localizing the target object while generating a mask for each instance. The presented approach, moreover, serves as a pioneering method to extract 3D point cloud data based on 2D pixel values for building the appropriate coordinate system on the planar object plane. The experimental results showed that the proposed method reached an accuracy rate of 100% for classifying two-sided objects in the unseen dataset, and 3D appropriate pose prediction was highly effective, with average translation and rotation errors less than 0.23 cm and 2.26°, respectively. Finally, the system success rate for picking up objects was over 99% at an average processing time of 0.9 s per step, fast enough for continuous robotic operation without interruption. This showed a promising higher successful pickup rate compared to previous approaches to random bin-picking problems. Successful implementation of the proposed approach for USB packs provides a solid basis for other planar objects in a cluttered environment. With remarkable precision and efficiency, this study shows significant commercialization potential.",project-academic
10.1111/CEO.13971,2021-07-16,a,"John Wiley & Sons, Ltd",emergence of non artificial intelligence digital health innovations in ophthalmology a systematic review," The prominent rise of digital health in ophthalmology is evident in the current age of Industry 4.0. Despite the many facets of digital health, there has been a greater slant in interest and focus on artificial intelligence recently. Other major elements of digital health like wearables could also substantially impact patient-focused outcomes but have been relatively less explored and discussed. In this review, we comprehensively evaluate the use of non-artificial intelligence digital health tools in ophthalmology. 53 papers were included in this systematic review - 25 papers discuss virtual or augmented reality, 14 discuss mobile applications and 14 discuss wearables. Most papers focused on the use of technologies to detect or rehabilitate visual impairment, glaucoma and age-related macular degeneration. Overall, the findings on patient-focused outcomes with the adoption of these technologies are encouraging. Further validation, large-scale studies and earlier consideration of real-world barriers are warranted to enable better real-world implementation.",project-academic
10.1109/ICAC.2017.21,2017-07-01,p,IEEE Computer Society,ananke a q learning based portfolio scheduler for complex industrial workflows," Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for datacenters with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",project-academic
,2021-07-29,b,,an introduction to statistical learning with applications in r," An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.",project-academic
,2019-07-24,a,hgpu.org,benchmarking tpu gpu and cpu platforms for deep learning," Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.",project-academic
10.1007/S10845-019-01476-X,2020-03-01,a,Springer US,segmentation based deep learning approach for surface defect detection," Automated surface-anomaly detection using machine learning has become an interesting and promising area of research, with a very high and direct impact on the application domain of visual inspection. Deep-learning methods have become the most suitable approaches for this task. They allow the inspection system to learn to detect the surface anomaly by simply showing it a number of exemplar images. This paper presents a segmentation-based deep-learning architecture that is designed for the detection and segmentation of surface anomalies and is demonstrated on a specific domain of surface-crack detection. The design of the architecture enables the model to be trained using a small number of samples, which is an important requirement for practical applications. The proposed model is compared with the related deep-learning methods, including the state-of-the-art commercial software, showing that the proposed approach outperforms the related methods on the specific domain of surface-crack detection. The large number of experiments also shed light on the required precision of the annotation, the number of required training samples and on the required computational cost. Experiments are performed on a newly created dataset based on a real-world quality control case and demonstrates that the proposed approach is able to learn on a small number of defected surfaces, using only approximately 25–30 defective training samples, instead of hundreds or thousands, which is usually the case in deep-learning applications. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. The dataset is also made publicly available to encourage the development and evaluation of new methods for surface-defect detection.",project-academic
10.1126/SCIENCE.AAT8414,2019-06-21,a,American Association for the Advancement of Science,trends and challenges in robot manipulation," BACKGROUND None Humans have a fantastic ability to manipulate objects of various shapes, sizes, and materials and can control the objects’ position in confined spaces with the advanced dexterity capabilities of our hands. Building machines inspired by human hands, with the functionality to autonomously pick up and manipulate objects, has always been an essential component of robotics. The first robot manipulators date back to the 1960s and are some of the first robotic devices ever constructed. In these early days, robotic manipulation consisted of carefully prescribed movement sequences that a robot would execute with no ability to adapt to a changing environment. As time passed, robots gradually gained the ability to automatically generate movement sequences, drawing on artificial intelligence and automated reasoning. Robots would stack boxes according to size, weight, and so forth, extending beyond geometric reasoning. This task also required robots to handle errors and uncertainty in sensing at run time, given that the slightest imprecision in the position and orientation of stacked boxes might cause the entire tower to topple. Methods from control theory also became instrumental for enabling robots to comply with the environment’s natural uncertainty by empowering them to adapt exerted forces upon contact. The ability to stably vary forces upon contact expanded robots’ manipulation repertoire to more-complex tasks, such as inserting pegs in holes or hammering. However, none of these actions truly demonstrated fine or in-hand manipulation capabilities, and they were commonly performed using simple two-fingered grippers. To enable multipurpose fine manipulation, roboticists focused their efforts on designing humanlike hands capable of using tools. Wielding a tool in-hand became a problem of its own, and a variety of advanced algorithms were developed to facilitate stable holding of objects and provide optimality guarantees. Because optimality was difficult to achieve in a stochastic environment, from the 1990s onward researchers aimed to increase the robustness of object manipulation at all levels. These efforts initiated the design of sensors and hardware for improved control of hand–object contacts. Studies that followed were focused on robust perception for coping with object occlusion and noisy measurements, as well as on adaptive control approaches to infer an object’s physical properties, so as to handle objects whose properties are unknown or change as a result of manipulation. None ADVANCES None Roboticists are still working to develop robots capable of sorting and packaging objects, chopping vegetables, and folding clothes in unstructured and dynamic environments. Robots used for modern manufacturing have accomplished some of these tasks in structured settings that still require fences between the robots and human operators to ensure safety. Ideally, robots should be able to work side by side with humans, offering their strength to carry heavy loads while presenting no danger. Over the past decade, robots have gained new levels of dexterity. This enhancement is due to breakthroughs in mechanics with sensors for perceiving touch along a robot’s body and new mechanics for soft actuation to offer natural compliance. Most notably, this development leverages the immense progress in machine learning to encapsulate models of uncertainty and support further advances in adaptive and robust control. Learning to manipulate in real-world settings is costly in terms of both time and hardware. To further elaborate on data-driven methods but avoid generating examples with real, physical systems, many researchers use simulation environments. Still, grasping and dexterous manipulation require a level of reality that existing simulators are not yet able to deliver—for example, in the case of modeling contacts for soft and deformable objects. Two roads are hence pursued: The first draws inspiration from the way humans acquire interaction skills and prompts robots to learn skills from observing humans performing complex manipulation. This allows robots to acquire manipulation capabilities in only a few trials. However, generalizing the acquired knowledge to apply to actions that differ from those previously demonstrated remains difficult. The second road constructs databases of real object manipulation, with the goal to better inform the simulators and generate examples that are as realistic as possible. Yet achieving realistic simulation of friction, material deformation, and other physical properties may not be possible anytime soon, and real experimental evaluation will be unavoidable for learning to manipulate highly deformable objects. None OUTLOOK None Despite many years of software and hardware development, achieving dexterous manipulation capabilities in robots remains an open problem—albeit an interesting one, given that it necessitates improved understanding of human grasping and manipulation techniques. We build robots to automate tasks but also to provide tools for humans to easily perform repetitive and dangerous tasks while avoiding harm. Achieving robust and flexible collaboration between humans and robots is hence the next major challenge. Fences that currently separate humans from robots will gradually disappear, and robots will start manipulating objects jointly with humans. To achieve this objective, robots must become smooth and trustable partners that interpret humans’ intentions and respond accordingly. Furthermore, robots must acquire a better understanding of how humans interact and must attain real-time adaptation capabilities. There is also a need to develop robots that are safe by design, with an emphasis on soft and lightweight structures as well as control and planning methodologies based on multisensory feedback.",project-academic
10.1038/S41586-021-03213-Y,2021-02-04,a,Nature Publishing Group,bayesian reaction optimization as a tool for chemical synthesis," Reaction optimization is fundamental to synthetic chemistry, from optimizing the yield of industrial processes to selecting conditions for the preparation of medicinal candidates1. Likewise, parameter optimization is omnipresent in artificial intelligence, from tuning virtual personal assistants to training social media and product recommendation systems2. Owing to the high cost associated with carrying out experiments, scientists in both areas set numerous (hyper)parameter values by evaluating only a small subset of the possible configurations. Bayesian optimization, an iterative response surface-based global optimization algorithm, has demonstrated exceptional performance in the tuning of machine learning models3. Bayesian optimization has also been recently applied in chemistry4–9; however, its application and assessment for reaction optimization in synthetic chemistry has not been investigated. Here we report the development of a framework for Bayesian reaction optimization and an open-source software tool that allows chemists to easily integrate state-of-the-art optimization algorithms into their everyday laboratory practices. We collect a large benchmark dataset for a palladium-catalysed direct arylation reaction, perform a systematic study of Bayesian optimization compared to human decision-making in reaction optimization, and apply Bayesian optimization to two real-world optimization efforts (Mitsunobu and deoxyfluorination reactions). Benchmarking is accomplished via an online game that links the decisions made by expert chemists and engineers to real experiments run in the laboratory. Our findings demonstrate that Bayesian optimization outperforms human decisionmaking in both average optimization efficiency (number of experiments) and consistency (variance of outcome against initially available data). Overall, our studies suggest that adopting Bayesian optimization methods into everyday laboratory practices could facilitate more efficient synthesis of functional chemicals by enabling better-informed, data-driven decisions about which experiments to run. Bayesian optimization is applied in chemical synthesis towards the optimization of various organic reactions and is found to outperform scientists in both average optimization efficiency and consistency.",project-academic
10.1007/978-3-030-58666-9_2,2020-09-13,p,"Springer, Cham",characterizing machine learning processes a maturity framework," Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises. For example, existing machine learning processes cannot address how to define business use cases for an AI application, how to convert business requirements from product managers into data requirements for data scientists, and how to continuously improve AI applications in term of accuracy and fairness, how to customize general purpose machine learning models with industry, domain, and use case specific data to make them more accurate for specific situations etc. Making AI work for enterprises requires special considerations, tools, methods and processes. In this paper we present a maturity framework for machine learning model lifecycle management for enterprises. Our framework is a re-interpretation of the software Capability Maturity Model (CMM) for machine learning model development process. We present a set of best practices from authors’ personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point.",project-academic
10.1145/3338906.3338958,2019-08-12,p,"Association for Computing Machinery, Inc",reinam reinforcement learning for input grammar inference," Program input grammars (i.e., grammars encoding the language of valid program inputs) facilitate a wide range of applications in software engineering such as symbolic execution and delta debugging. Grammars synthesized by existing approaches can cover only a small part of the valid input space mainly due to unanalyzable code (e.g., native code) in programs and lacking high-quality and high-variety seed inputs. To address these challenges, we present REINAM, a reinforcement-learning approach for synthesizing probabilistic context-free program input grammars without any seed inputs. REINAM uses an industrial symbolic execution engine to generate an initial set of inputs for the given target program, and then uses an iterative process of grammar generalization to proactively generate additional inputs to infer grammars generalized from these initial seed inputs. To efficiently search for target generalizations in a huge search space of candidate generalization operators, REINAM includes a novel formulation of the search problem as a reinforcement learning problem. Our evaluation on eleven real-world benchmarks shows that REINAM outperforms an existing state-of-the-art approach on precision and recall of synthesized grammars, and fuzz testing based on REINAM substantially increases the coverage of the space of valid inputs. REINAM is able to synthesize a grammar covering the entire valid input space for some benchmarks without decreasing the accuracy of the grammar.",project-academic
10.1109/SSCI.2017.8280935,2017-11-01,p,IEEE,a benchmark environment motivated by industrial control problems," In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB's dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.",project-academic
10.1145/2950290.2983954,2016-11-01,p,ACM,learning for test prioritization an industrial case study," Modern cloud-software providers, such as Salesforce.com, increasingly adopt large-scale continuous integration environments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimization mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in industrial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce.com shows that our approach significantly outperforms existing individual techniques.",project-academic
,2020-06-30,a,University of Oulu,6g white paper on edge intelligence," In this white paper we provide a vision for 6G Edge Intelligence. Moving towards 5G and beyond the future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence become crucial for several real-world applications including but not limited to, more efficient manufacturing, novel personal smart device environments and experiences, urban computing and autonomous traffic settings. We present edge computing along with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies as shown in this series of 6G White Papers. 
In this white paper, we focus in the domains of edge computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, along with security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges and identify the key research questions for the development of the Intelligent Edge services. As a main outcome of this white paper, we envision a transition from Internet of Things to Intelligent Internet of Intelligent Things and provide a roadmap for development of 6G Intelligent Edge.",project-academic
10.1007/S13740-018-0096-0,2019-06-01,a,Springer Berlin Heidelberg,automated planning for business process management," Business Process Management (BPM) is a central element of today’s organizations. Over the years, its main focus has been the support of business processes (BPs) in highly controlled domains. However—in the current era of Big Data and Internet-of-Things—several real-world domains are becoming cyber-physical (e.g., consider the shift from traditional manufacturing to Industry 4.0), characterized by ever-changing requirements, unpredictable environments and increasing amounts of data and events that influence the enactment of BPs. In such unconstrained settings, BPM professionals lack the needed knowledge to model all possible BP variants/contingencies at the outset. Consequently, BPM systems must increase their level of automation to provide the reactivity and flexibility necessary for process management. On the other hand, the Artificial Intelligence (AI) community has concentrated its efforts on investigating dynamic domains that involve active control of computational entities and physical devices (e.g., robots, software agents). In this context, automated planning, which is one of the oldest areas in AI, is conceived as a model-based approach to synthesize autonomous behaviors in automated way from a model. In this paper, we discuss how automated planning techniques can be leveraged to enable new levels of automation and support for solving concrete problems in the BPM field that were previously tackled with hard-coded solutions. To this aim, we first propose a methodology that shows how a researcher/practitioner should approach the task of encoding a concrete problem as an appropriate planning problem. Then, we discuss the required steps to integrate the planning technology in BPM environments. Finally, we show some concrete examples of the successful application of planning techniques to the different stages of the BPM life cycle.",project-academic
,2018-10-10,a,Elsevier Limited,secure deep learning engineering a software quality assurance perspective," Over the past decades, deep learning (DL) systems have achieved tremendous success and gained great popularity in various applications, such as intelligent machines, image processing, speech processing, and medical diagnostics. Deep neural networks are the key driving force behind its recent success, but still seem to be a magic black box lacking interpretability and understanding. This brings up many open safety and security issues with enormous and urgent demands on rigorous methodologies and engineering practice for quality enhancement. A plethora of studies have shown that the state-of-the-art DL systems suffer from defects and vulnerabilities that can lead to severe loss and tragedies, especially when applied to real-world safety-critical applications. In this paper, we perform a large-scale study and construct a paper repository of 223 relevant works to the quality assurance, security, and interpretation of deep learning. We, from a software quality assurance perspective, pinpoint challenges and future opportunities towards universal secure deep learning engineering. We hope this work and the accompanied paper repository can pave the path for the software engineering community towards addressing the pressing industrial demand of secure intelligent applications.",project-academic
10.1016/0952-1976(93)90016-Q,1993-08-01,a,Pergamon,transforming standalone expert systems into a community of cooperating agents," Abstract None None Distributed Artificial Intelligence (DAI) systems in which multiple problem-solving agents cooperate to achieve a common objective is a rapidly emerging and promising technology. However, as yet, there have been relatively few reported cases of such systems being employed to tackle real-world problems in realistic domains. One of the reasons for this is that DAI researchers have given virtually no consideration to the process of incorporating pre-existing systems into a community of cooperating agents. Yet reuse is a primary consideration for any organisation with a large software base. None To redress the balance, this paper reports on an experiment undertaken at the CERN laboratories in which two pre-existing and standalone expert systems for diagnosing faults in a particle accelerator were transformed into a community of cooperating agents. The experiences and insights gained during this process provide a valuable first step towards satisfying the needs of potential users of DAI technology—identifying the types of changes required for cooperative problem solving, quantifying the effort involved in transforming standalone systems to ones suitable for cooperation and highlighting the benefits of a cooperating systems approach in a realistic industrial application.",project-academic
10.1109/MSMC.2018.2881233,2020-01-15,a,IEEE,cloud fog and mist computing advanced robot applications," As we are witnessing the dawn of the artificial intelligence revolution, it is a good time to discuss the system-architecture challenges of the new age of robotics. Most of the disputes are related to the wide variety of cloud-computing arrangements along the questions of why, with what, and how to exploit existing information technologies to cope effectively with the increasing demands. Traditional industrial robot makers are very conservative, relying exclusively on onboard execution. However, the rising user demand for advanced robot skills, such as dexterous manipulation and free navigation in an unstructured environment, has inspired newcomers to break with tradition and move robot software architecture to the next level. Inspired by the talks of Prof. Imre Rudas on cloud robotics, this article highlights some substantial factors of system design and discusses a real-world industrial example from my RaD practice.",project-academic
10.1109/ICSE-SEIP.2017.8,2017-05-20,p,IEEE Press,domain adaptation for test report classification in crowdsourced testing," In crowdsourced testing, it is beneficial to automatically classify the test reports that actually reveal a fault – a true fault, from the large number of test reports submitted by crowd workers. Most of the existing approaches toward this task simply leverage historical data to train a machine learning classifier and classify the new incoming reports. However, our observation on real industrial data reveals that projects under crowdsourced testing come from various domains, and the submitted reports usually contain different technical terms to describe the software behavior for each domain. The different data distribution across domains could significantly degrade the performance of classification models when utilized for cross-domain report classification. To build an effective cross-domain classification model, we leverage deep learning to discover the intermediate representation that is shared across domains, through the co-occurrence between domain-specific terms and domain-unaware terms. Specifically, we use the Stacked Denoising Autoencoders to automatically learn the high-level features from raw textual terms, and utilize these features for classification. Our evaluation on 58 commercial projects of 10 domains from one of the Chinese largest crowdsourced testing platforms shows that our approach can generate promising results, compared to three commonly-used and state-of-the-art baselines. Moreover, we also evaluate its usefulness using real-world case studies. The feedback from real-world testers demonstrates its practical value.",project-academic
10.1109/TII.2019.2942800,2020-03-01,a,Institute of Electrical and Electronics Engineers,cyber vulnerability intelligence for internet of things binary," Internet of Things (IoT) integrates a variety of software (e.g., autonomous vehicles and military systems) in order to enable the advanced and intelligent services. These software increase the potential of cyber-attacks because an adversary can launch an attack using system vulnerabilities. Existing software vulnerability analysis methods used to be relying on human experts crafted features, which usually miss many vulnerabilities. It is important to develop an automatic vulnerability analysis system to improve the countermeasures. However, source code is not always available (e.g., most IoT related industry software are closed source). Therefore, vulnerability detection on binary code is a demanding task. This article addresses the automatic binary-level software vulnerability detection problem by proposing a deep learning-based approach. The proposed approach consists of two phases: binary function extraction, and model building. First, we extract binary functions from the cleaned binary instructions obtained by using IDA Pro. Then, we employ the attention mechanism on top of a bidirectional long short-term memory for building the predictive model. To show the effectiveness of the proposed approach, we have collected datasets from several different sources. We have compared our proposed approach with a series of baselines including source code-based techniques and binary code-based techniques. We have also applied the proposed approach to real-world IoT related software such as VLC media player and LibTIFF project that used on Autonomous Vehicles. Experimental results show that our proposed approach betters the baselines and is able to detect more vulnerabilities.",project-academic
10.1109/ICSE43902.2021.00120,2021-05-22,p,IEEE,enhancing genetic improvement of software with regression test selection," Genetic improvement uses artificial intelligence to automatically improve software with respect to non-functional properties (AI for SE). In this paper, we propose the use of existing software engineering best practice to enhance Genetic Improvement (SE for AI). We conjecture that existing Regression Test Selection (RTS) techniques (which have been proven to be efficient and effective) can and should be used as a core component of the GI search process for maximising its effectiveness. To assess our idea, we have carried out a thorough empirical study assessing the use of both dynamic and static RTS techniques with GI to improve seven real-world software programs. The results of our empirical evaluation show that incorporation of RTS within GI significantly speeds up the whole GI process, making it up to 68% faster on our benchmark set, being still able to produce valid software improvements. Our findings are significant in that they can save hours to days of computational time, and can facilitate the uptake of GI in an industrial setting, by significantly reducing the time for the developer to receive feedback from such an automated technique. Therefore, we recommend the use of RTS in future test-based automated software improvement work. Finally, we hope this successful application of SE for AI will encourage other researchers to investigate further applications in this area.",project-academic
10.1109/MALTESQUE.2018.8368453,2018-03-20,p,IEEE,machine learning based run time anomaly detection in software systems an industrial evaluation," Anomalies are an inevitable occurrence while operating enterprise software systems. Traditionally, anomalies are detected by threshold-based alarms for critical metrics, or health probing requests. However, fully automated detection in complex systems is challenging, since it is very difficult to distinguish truly anomalous behavior from normal operation. To this end, the traditional approaches may not be sufficient. Thus, we propose machine learning classifiers to predict the system's health status. We evaluated our approach in an industrial case study, on a large, real-world dataset of 7.5 • 106 data points for 231 features. Our results show that recurrent neural networks with long short-term memory (LSTM) are more effective in detecting anomalies and health issues, as compared to other classifiers. We achieved an area under precision-recall curve of 0.44. At the default threshold, we can automatically detect 70% of the anomalies. Despite the low precision of 31 %, the rate in which false positives occur is only 4 %.",project-academic
10.1117/12.2558947,2020-05-19,p,International Society for Optics and Photonics,modeling realistic virtual objects within a high throughput x ray simulation framework," X-ray simulation of realistic object models is relevant across all areas in which X-ray systems are employed, including medical, industrial, and security applications. A particularly exciting area of impact stems from the development of machine learning approaches to classification, detection, and data processing. The continued development of these techniques requires large labeled datasets. Traditionally, this data needed to be collected with physical machines, creating steep logistical challenges. Moreover, the testing and evaluation of such X-ray scanners present their own challenges, as machines need to be shipped to a site capable of handling certain anomalies. To help alleviate these burdens, virtual models and simulations can be used in lieu of empirical measurements. The confluence of powerful computers and advanced data processing techniques presents an opportunity to develop tools to aid in dataset creation as well as system analysis. We present efforts toward the maturity of such tools. Building on previous work to validate the performance of simulation software, we show how modeling realistic virtual objects can produce data representative of real-world measurements. Furthermore, we present the efficiency of such an approach that leverages advances in computer graphics, ray-tracing utilities, and GPU hardware.",project-academic
10.1109/LCN.2018.8638081,2018-10-01,p,Institute of Electrical and Electronics Engineers (IEEE),real time performance evaluation of lte for iiot," Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-to-end transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%.",project-academic
,2016-08-05,b,Springer,anaphora resolution algorithms resources and applications," This book lays out a path leading from the linguistic and cognitive basics, to classical rule-based and machine learning algorithms, to todays state-of-the-art approaches, which use advanced empirically grounded techniques, automatic knowledge acquisition, and refined linguistic modeling to make a real difference in real-world applications. Anaphora and coreference resolution both refer to the process of linking textual phrases (and, consequently, the information attached to them) within as well as across sentence boundaries, and to the same discourse referent. The book offers an overview of recent research advances, focusing on practical, operational approaches and their applications. In part I (Background), it provides a general introduction, which succinctly summarizes the linguistic, cognitive, and computational foundations of anaphora processing and the key classical rule- and machine-learning-based anaphora resolution algorithms. Acknowledging the central importance of shared resources, part II (Resources) covers annotated corpora, formal evaluation, preprocessing technology, and off-the-shelf anaphora resolution systems. Part III (Algorithms) provides a thorough description of state-of-the-art anaphora resolution algorithms, covering enhanced machine learning methods as well as techniques for accomplishing important subtasks such as mention detection and acquisition of relevant knowledge. Part IV (Applications) deals with a selection of important anaphora and coreference resolution applications, discussing particular scenarios in diverse domains and distilling a best-practice model for systematically approaching new application cases. In the concluding part V (Outlook), based on a survey conducted among the contributing authors, the prospects of the research field of anaphora processing are discussed, and promising new areas of interdisciplinary cooperationand emerging application scenarios are identified. Given the books design, it can be used both as an accompanying text for advanced lectures in computational linguistics, natural language engineering, and computer science, and as a reference work for research and independent study. It addresses an audience that includes academic researchers, university lecturers, postgraduate students, advanced undergraduate students, industrial researchers, and software engineers.",project-academic
,1997-02-01,b,,human computer interaction," Contents Foreword Preface to the third edition Preface to the second edition Preface to the first edition Introduction Part 1 Foundations Chapter 1 The human 1.1 Introduction 1.2 Input-output channels Design Focus: Getting noticed Design Focus: Where's the middle? 1.3 Human memory Design Focus: Cashing in Design Focus: 7 +- 2 revisited 1.4 Thinking: reasoning and problem solving Design Focus: Human error and false memories 1.5 Emotion 1.6 Individual differences 1.7 Psychology and the design of interactive systems 1.8 Summary Exercises Recommended reading Chapter 2 The computer 2.1 Introduction Design Focus: Numeric keypads 2.2 Text entry devices 2.3 Positioning, pointing and drawing 2.4 Display devices Design Focus: Hermes: a situated display 2.5 Devices for virtual reality and 3D interaction 2.6 Physical controls, sensors and special devices Design Focus: Feeling the road Design Focus: Smart-Its - making sensors easy 2.7 Paper: printing and scanning Design Focus: Readability of text 2.8 Memory 2.9 Processing and networks Design Focus: The myth of the infinitely fast machine 2.10 Summary Exercises Recommended reading Chapter 3 The interaction 3.1 Introduction 3.2 Models of interaction Design Focus: Video recorder 3.3 Frameworks and HCI 3.4 Ergonomics Design Focus: Industrial interfaces 3.5 Interaction styles Design Focus: Navigation in 3D and 2D 3.6 Elements of the WIMP interface Design Focus: Learning toolbars 3.7 Interactivity 3.8 The context of the interaction Design Focus: Half the picture? 3.9 Experience, engagement and fun 3.10 Summary Exercises Recommended reading Chapter 4 Paradigms 4.1 Introduction 4.2 Paradigms for interaction 4.3 Summary Exercises Recommended reading Part 2 Design process Chapter 5 Interaction design basics 5.1 Introduction 5.2 What is design? 5.3 The process of design 5.4 User focus Design Focus: Cultural probes 5.5 Scenarios 5.6 Navigation design Design Focus: Beware the big button trap Design Focus: Modes 5.7 Screen design and layout Design Focus: Alignment and layout matter Design Focus: Checking screen colors 5.8 Iteration and prototyping 5.9 Summary Exercises Recommended reading Chapter 6 HCI in the software process 6.1 Introduction 6.2 The software life cycle 6.3 Usability engineering 6.4 Iterative design and prototyping Design Focus: Prototyping in practice 6.5 Design rationale 6.6 Summary Exercises Recommended reading Chapter 7 Design rules 7.1 Introduction 7.2 Principles to support usability 7.3 Standards 7.4 Guidelines 7.5 Golden rules and heuristics 7.6 HCI patterns 7.7 Summary Exercises Recommended reading Chapter 8 Implementation support 8.1 Introduction 8.2 Elements of windowing systems 8.3 Programming the application Design Focus: Going with the grain 8.4 Using toolkits Design Focus: Java and AWT 8.5 User interface management systems 8.6 Summary Exercises Recommended reading Chapter 9 Evaluation techniques 9.1 What is evaluation? 9.2 Goals of evaluation 9.3 Evaluation through expert analysis 9.4 Evaluation through user participation 9.5 Choosing an evaluation method 9.6 Summary Exercises Recommended reading Chapter 10 Universal design 10.1 Introduction 10.2 Universal design principles 10.3 Multi-modal interaction Design Focus: Designing websites for screen readers Design Focus: Choosing the right kind of speech Design Focus: Apple Newton 10.4 Designing for diversity Design Focus: Mathematics for the blind 10.5 Summary Exercises Recommended reading Chapter 11 User support 11.1 Introduction 11.2 Requirements of user support 11.3 Approaches to user support 11.4 Adaptive help systems Design Focus: It's good to talk - help from real people 11.5 Designing user support systems 11.6 Summary Exercises Recommended reading Part 3 Models and theories Chapter 12 Cognitive models 12.1 Introduction 12.2 Goal and task hierarchies Design Focus: GOMS saves money 12.3 Linguistic models 12.4 The challenge of display-based systems 12.5 Physical and device models 12.6 Cognitive architectures 12.7 Summary Exercises Recommended reading Chapter 13 Socio-organizational issues and stakeholder requirements 13.1 Introduction 13.2 Organizational issues Design Focus: Implementing workflow in Lotus Notes 13.3 Capturing requirements Design Focus: Tomorrow's hospital - using participatory design 13.4 Summary Exercises Recommended reading Chapter 14 Communication and collaboration models 14.1 Introduction 14.2 Face-to-face communication Design Focus: Looking real - Avatar Conference 14.3 Conversation 14.4 Text-based communication 14.5 Group working 14.6 Summary Exercises Recommended reading Chapter 15 Task analysis 15.1 Introduction 15.2 Differences between task analysis and other techniques 15.3 Task decomposition 15.4 Knowledge-based analysis 15.5 Entity-relationship-based techniques 15.6 Sources of information and data collection 15.7 Uses of task analysis 15.8 Summary Exercises Recommended reading Chapter 16 Dialog notations and design 16.1 What is dialog? 16.2 Dialog design notations 16.3 Diagrammatic notations Design Focus: Using STNs in prototyping Design Focus: Digital watch - documentation and analysis 16.4 Textual dialog notations 16.5 Dialog semantics 16.6 Dialog analysis and design 16.7 Summary Exercises Recommended reading Chapter 17 Models of the system 17.1 Introduction 17.2 Standard formalisms 17.3 Interaction models 17.4 Continuous behavior 17.5 Summary Exercises Recommended reading Chapter 18 Modeling rich interaction 18.1 Introduction 18.2 Status-event analysis 18.3 Rich contexts 18.4 Low intention and sensor-based interaction Design Focus: Designing a car courtesy light 18.5 Summary Exercises Recommended reading Part 4 Outside the box Chapter 19 Groupware 19.1 Introduction 19.2 Groupware systems 19.3 Computer-mediated communication Design Focus: SMS in action 19.4 Meeting and decision support systems 19.5 Shared applications and artifacts 19.6 Frameworks for groupware Design Focus: TOWER - workspace awareness Exercises Recommended reading Chapter 20 Ubiquitous computing and augmented realities 20.1 Introduction 20.2 Ubiquitous computing applications research Design Focus: Ambient Wood - augmenting the physical Design Focus: Classroom 2000/eClass - deploying and evaluating ubicomp 20.3 Virtual and augmented reality Design Focus: Shared experience Design Focus: Applications of augmented reality 20.4 Information and data visualization Design Focus: Getting the size right 20.5 Summary Exercises Recommended reading Chapter 21 Hypertext, multimedia and the world wide web 21.1 Introduction 21.2 Understanding hypertext 21.3 Finding things 21.4 Web technology and issues 21.5 Static web content 21.6 Dynamic web content 21.7 Summary Exercises Recommended reading References Index",project-academic
10.1016/J.CIE.2017.09.016,2017-11-01,a,Pergamon,smart operators in industry 4 0 a human centered approach to enhance operators capabilities and competencies within the new smart factory context," Abstract None None As the Industry 4.0 takes shape, human operators experience an increased complexity of their daily tasks: they are required to be highly flexible and to demonstrate adaptive capabilities in a very dynamic working environment. It calls for tools and approaches that could be easily embedded into everyday practices and able to combine complex methodologies with high usability requirements. In this perspective, the proposed research work is focused on the design and development of a practical solution, called Sophos-MS, able to integrate augmented reality contents and intelligent tutoring systems with cutting-edge fruition technologies for operators’ support in complex man-machine interactions. After establishing a reference methodological framework for the smart operator concept within the Industry 4.0 paradigm, the proposed solution is presented, along with its functional and non-function requirements. Such requirements are fulfilled through a structured design strategy whose main outcomes include a multi-layered modular solution, Sophos-MS, that relies on Augmented Reality contents and on an intelligent personal digital assistant with vocal interaction capabilities. The proposed approach has been deployed and its training potentials have been investigated with field experiments. The experimental campaign results have been firstly checked to ensure their statistical relevance and then analytically assessed in order to show that the proposed solution has a real impact on operators’ learning curves and can make the difference between who uses it and who does not.",project-academic
10.1109/ACCESS.2019.2942390,2019-09-19,a,Institute of Electrical and Electronics Engineers (IEEE),machine learning for 5g b5g mobile and wireless communications potential limitations and future directions," Driven by the demand to accommodate today’s growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",project-academic
10.1016/J.PROMFG.2018.04.009,2018-01-01,a,Elsevier,mixed reality in learning factories," Abstract None None Supported by rapid technological development, mixed reality (MR) applications are increasingly deployed in industrial practice. In manufacturing, MR can be utilized for information visualization, remote collaboration, human-machine-interfaces, design tools and education and training. This development makes new demands on learning factories in two major fields: One is the empowerment of users to work with MR in industrial applications. The second field is the utilization of the potential of MR for teaching and learning in learning factories. A great potential lies in the new possibilities of connecting digital content with the physical world. To analyze the potential applications of MR in learning factories in a structured way, an overview of potential MR applications based on the reality-virtuality continuum is presented with an analysis of case studies of applications in a learning factory including a mixed-reality-hackathon.",project-academic
10.1109/ROBOSOFT48309.2020.9116004,2020-05-01,p,IEEE,scalable sim to real transfer of soft robot designs," The manual design of soft robots and their controllers is notoriously challenging, but it could be augmented—or, in some cases, entirely replaced—by automated design tools. Machine learning algorithms can automatically propose, test, and refine designs in simulation, and the most promising ones can then be manufactured in reality (sim2real). However, it is currently not known how to guarantee that behavior generated in simulation can be preserved when deployed in reality. Although many previous studies have devised training protocols that facilitate sim2real transfer of control polices, little to no work has investigated the simulation-reality gap as a function of morphology. This is due in part to an overall lack of tools capable of systematically designing and rapidly manufacturing robots. Here we introduce a low cost, open source, and modular soft robot design and construction kit, and use it to simulate, fabricate, and measure the simulation-reality gap of minimally complex yet soft, locomoting machines. We prove the scalability of this approach by transferring an order of magnitude more robot designs from simulation to reality than any other method. The kit and its instructions can be found here: github.com/skriegman/sim2real4designs",project-academic
,2019-11-23,a,,scalable sim to real transfer of soft robot designs," The manual design of soft robots and their controllers is notoriously challenging, but it could be augmented---or, in some cases, entirely replaced---by automated design tools. Machine learning algorithms can automatically propose, test, and refine designs in simulation, and the most promising ones can then be manufactured in reality (sim2real). However, it is currently not known how to guarantee that behavior generated in simulation can be preserved when deployed in reality. Although many previous studies have devised training protocols that facilitate sim2real transfer of control polices, little to no work has investigated the simulation-reality gap as a function of morphology. This is due in part to an overall lack of tools capable of systematically designing and rapidly manufacturing robots. Here we introduce a low cost, open source, and modular soft robot design and construction kit, and use it to simulate, fabricate, and measure the simulation-reality gap of minimally complex yet soft, locomoting machines. We prove the scalability of this approach by transferring an order of magnitude more robot designs from simulation to reality than any other method. The kit and its instructions can be found here: this https URL",project-academic
10.1016/J.COMCOM.2020.01.018,2020-02-01,a,Elsevier,enhanced resource allocation in mobile edge computing using reinforcement learning based moaco algorithm for iiot," Abstract None None The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.",project-academic
10.1109/JAS.2020.1003021,2020-02-27,a,IEEE,artificial intelligence applications in the development of autonomous vehicles a survey," The advancement of artificial intelligence &#x0028 AI &#x0029 has truly stimulated the development and deployment of autonomous vehicles &#x0028 AVs &#x0029 in the transportation industry. Fueled by big data from various sensing devices and advanced computing resources, AI has become an essential component of AVs for perceiving the surrounding environment and making appropriate decision in motion. To achieve goal of full automation &#x0028 i.e., self-driving &#x0028 , it is important to know how AI works in AV systems. Existing research have made great efforts in investigating different aspects of applying AI in AV development. However, few studies have offered the research community a thorough examination of current practices in implementing AI in AVs. Thus, this paper aims to shorten the gap by providing a comprehensive survey of key studies in this research avenue. Specifically, it intends to analyze their use of AIs in supporting the primary applications in AVs: 1&#x0029 perception; 2&#x0029 localization and mapping; and 3&#x0029 decision making. It investigates the current practices to understand how AI can be used and what are the challenges and issues associated with their implementation. Based on the exploration of current practices and technology advances, this paper further provides insights into potential opportunities regarding the use of AI in conjunction with other emerging technologies: 1&#x0029 high definition maps, big data, and high performance computing; 2&#x0029 augmented reality&#x0028 AR &#x0029 &#x002F virtual reality &#x0028 VR &#x0029 enhanced simulation platform; and 3&#x0029 5G communication for connected AVs. This paper is expected to offer a quick reference for researchers interested in understanding the use of AI in AV research.",project-academic
,2018-10-17,a,,quality 4 0 let s get digital the many ways the fourth industrial revolution is reshaping the way we think about quality," The technology landscape is richer and more promising than ever before. In many ways, cloud computing, big data, virtual reality (VR), augmented reality (AR), blockchain, additive manufacturing, artificial intelligence (AI), machine learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and the Internet of Things (IoT) all represent new frontiers. These technologies can help improve product and service quality, and organizational performance. In many regions, the internet is now as ubiquitous as electricity. Components are relatively cheap. A robust ecosystem of open-source software libraries means that engineers can solve problems 100 times faster than just two decades ago. This digital transformation is leading us toward connected intelligent automation: smart, hyperconnected agents deployed in environments where humans and machines cooperate, and leverage data, to achieve shared goals. This is not the worlds first industrial revolution. In fact, it is its fourth, and the disruptive changes it will bring suggest we will need a fresh perspective on quality to adapt to it.",project-academic
10.1016/J.NEUCOM.2020.10.097,2021-03-21,a,Elsevier,3d rvp a method for 3d object reconstruction from a single depth view using voxel and point," Abstract None None Three-dimensional object reconstruction technology has a wide range of applications such as augment reality, virtual reality, industrial manufacturing and intelligent robotics. Although deep learning-based 3D object reconstruction technology has developed rapidly in recent years, there remain important problems to be solved. One of them is that the resolution of reconstructed 3D models is hard to improve because of the limitation of memory and computational efficiency when deployed on resource-limited devices. In this paper, we propose 3D-RVP to reconstruct a complete and accurate 3D geometry from a single depth view, where R, V and P represent Reconstruction, Voxel and Point, respectively. It is a novel two-stage method that combines a 3D encoder-decoder network with a point prediction network. In the first stage, we propose a 3D encoder-decoder network with residual learning to output coarse prediction results. In the second stage, we propose an iterative subdivision algorithm to predict the labels of adaptively selected points. The proposed method can output high-resolution 3D models by increasing a small number of parameters. Experiments are conducted on widely used benchmarks of a ShapeNet dataset in which four categories of models are selected to test the performance of neural networks. Experimental results show that our proposed method outperforms the state-of-the-arts, and achieves about None None None None 2.7 None % None None None None improvement in terms of the intersection-over-union metric.",project-academic
10.1007/S10115-006-0063-1,2007-07-12,a,Springer-Verlag,intelligent systems in the automotive industry applications and trends," There is a common misconception that the automobile industry is slow to adapt new technologies, such as artificial intelligence (AI) and soft computing. The reality is that many new technologies are deployed and brought to the public through the vehicles that they drive. This paper provides an overview and a sampling of many of the ways that the automotive industry has utilized AI, soft computing and other intelligent system technologies in such diverse domains like manufacturing, diagnostics, on-board systems, warranty analysis and design.",project-academic
10.1016/J.COSE.2017.11.014,2018-03-01,a,Elsevier Advanced Technology,intelligent agents defending for an iot world a review," Transition to the Internet of Things (IoT) is progressing without realization. In light of this securing traditional systems is still a challenging role requiring a mixture of solutions which may negatively impact, or simply, not scale to a desired operational level. Rule and signature based intruder detection remains prominent in commercial deployments, while the use of machine learning for anomaly detection has been an active research area. Behavior detection means have also benefited from the widespread use of mobile and wireless applications. For the use of smart defense systems we propose that we must widen our perspective to not only security, but also to the domains of artificial intelligence and the IoT in better understanding the challenges that lie ahead in hope of achieving autonomous defense. We investigate how intruder detection fits within these domains, particularly as intelligent agents. How current approaches of intruder detection fulfill their role as intelligent agents, the needs of autonomous action regarding compromised nodes that are intelligent, distributed and data driven. The requirements of detection agents among IoT security are vulnerabilities, challenges and their applicable methodologies. In answering aforementioned questions, a survey of recent research work is presented in avoiding refitting old solutions into new roles. This survey is aimed toward security researchers or academics, IoT developers and information officers concerned with the covered areas. Contributions made within this review are the review of literature of traditional and distributed approaches to intruder detection, modeled as intelligent agents for an IoT perspective; defining a common reference of key terms between fields of intruder detection, artificial intelligence and the IoT, identification of key defense cycle requirements for defensive agents, relevant manufacturing and security challenges; and considerations to future development. As the turn of the decade draws nearer we anticipate 2020 as the turning point where deployments become common, not merely just a topic of conversation but where the need for collective, intelligent detection agents work across all layers of the IoT becomes a reality.",project-academic
10.1016/J.COMPIND.2020.103244,2020-09-01,a,Elsevier,machine learning for predictive scheduling and resource allocation in large scale manufacturing systems," Abstract None None The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud. None Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data.",project-academic
10.1186/GB4141,2013-11-26,a,BioMed Central,rumors of the death of consumer genomics are greatly exaggerated," The idea of computation is not new. Gottfried Leibniz in the 17th century had already outlined its basic elements in a formal sense [1], though one could argue that various mechanical calculating devices in Classical Antiquity and in Imperial China prefigured the difference engines of later ages [2]. By the mid-20th century science fiction writers were imagining a few, perhaps even only one, mega computer driving human civilization into the future [3]. Obviously they were wrong. In the first decades of the 21st century computation has proliferated and dispersed; becoming ubiquitous, seamless, almost beneath notice. Rather than exciting exotica, computation is the banal undergirding of modern post-industrial civilization. Devices that in the past would be properly labeled powerful computers are today termed ‘phones’ , ‘cars’ or ‘espresso machines’. We have transitioned from an abstraction of rarified reflection to a plethora of devices geared toward unthinking consumption.

This is the trajectory one might project for what we now term direct to consumer (DTC) or beyond the clinic genomics [4], the ancestor of what is likely to become a ubiquitous form of information technology in the coming decades. If Gregor Mendel was the Leibniz of genomics, we are in the Apple I era, as hobbyists jockey with firms that harbor dreams of massive market share. There is no telling who will be the Microsoft or Apple of this industry, but that is partly because we do not have the equivalent of the VisiCalc yet. The Apple II has not arrived, let alone the Macintosh.

While the sequencing hardware behind the technological revolution is transitioning from the analog of corporate mainframes to desktop-ready devices, the productivity suites of consumer genomics have yet to be fully conceived, let alone implemented. So what applications will drive the industry in the next few years? Front and center in terms of candidates for the next ‘killer app’ is the relatively mature market for genetic genealogy. More broadly, genealogy is a maturing sector with billions in revenue and nearly 100 million serious enthusiasts [5]. Rather than creating a demand de novo, genetic genealogy serves to extend the reach and power of those with deep questions as to their ancestry. The appeal of this sector is unsurprising. Cultures as distinctive and varied as the Jews and Chinese are preoccupied with issues of descent. One could argue that a genealogical mindset is a human universal playing to our inner desire to understand ourselves, our origins and our purpose. The tools of modern genomics add depth and insight to answers that most humans crave in their bones. Inclusive of the older genomic techniques, such as microsatellite markers, consumers of genetic genealogy likely number into the millions, with the richer offerings of firms such as Family Tree DNA and 23andMe approaching the order of one million customers next year [6].

We are all products of a past that is sketched out in the broadest sense by phylogenetics. Our pedigrees expand rapidly and then coalesce back together again, as the same individuals show up over and over. This is what evolutionary genomics is telling us [7], but our basic human intuitions often fail to grasp the multigenerational story, which goes beyond great-grandparents, and into the deepest mists of our species’ past. DTC genomics in the domain of genetic genealogy can bridge the chasm between the abstruse inferences of population and phylogenetics, and the lived and familiar reality of human familial warmth. But it can only do so by reducing abstract scientific theory into a more human shape, relatable, palatable and compelling. It is trite to assert that ‘we are all one big family’, but what genomics can do is make this truth concrete, and add some scientific heft behind sentiment. The industry needs to refine and perfect the middle layer between the science and the consumer, to the point where the technical guts of the operating system become as encapsulated, mysterious and ubiquitous as the magic of an iPhone.

As analysis of ancestry and genealogy saturate over the next decade, it seems likely that a natural segue will be made to medical genomics, as massive sample sizes can finally be leveraged into fine-grained prediction. Genetic genealogy is an applied enterprise of phylogenetics, but adding phenotypic information to the rich implicit pedigrees may finally allow for both true and surprising outcomes in regards to health risk prediction and the general interpretation of genome variants. Eventually the personal information ecology in which we are embedded will be rich, tagged and amenable to being synthesized and reduced down to actionable units. Genetic genealogy complements written records, but its power is such that it can yield information as a standalone tool. Not so when it comes to medical and assorted trait predictions. Most genetic traits of interest are complex, with multiple, sometimes obscure, lines of causality. Modern studies that use hundreds of individuals are notoriously underpowered, and are rapidly being superseded by pooled data sets of hundreds of thousands. Yet this is nothing compared to what will no doubt be the norm in the next decade, as diverse strands of information continuously thread together, and generate a perpetual stream of novel insightful predictions. Imagine that biomarkers are constantly recording your feedback to a particular diet or activity, and this information is intersected with millions with varying levels of genetic relatedness and lifestyle. The computational requirements are immense, but the past few decades have shown us that we can always be surprised how far Moore’s Law can take us.

Of course such a scenario above begs the question whether people would be comfortable in a semipermeable soup of information floating in the future cloud. But fortunately, or unfortunately, some of the most powerful and innovative firms today are making Scott McNealy’s prediction of a post-privacy future come alive. In the simplest of terms, expectation of privacy evolves with technology. In particular, both Google and Facebook are pushing the public toward the direction of transparency in terms of personal data. For obvious reasons there is sensitivity in regards to medical information in the United States, but with universal coverage some of the concerns are likely to be obviated, while the body politic will also have an incentive to allow for greater information exchange to be leveraged into informing citizens as to the optimal path for a healthier lifestyle [8].

Like the thin edge of the wedge, DTC genomics over the next decade will be defined by first movers who are passionate. Some will be enthusiasts for genealogy, others for experimental bio-hacking, and many for self-improvement through total self-knowledge. These hobbyists will drive a market segment that caters to their needs by transforming the digital information in their genomes into actionable representations and projections. But once the DTC genomics industry matures it will likely be swallowed in toto by the emerging total information ecology. This is not just likely, but inevitable, because though your genes do not determine who you are, you cannot understand who you are without your genes. It may be tired to declare that the ‘information must be free’, but in a deep and fundamental sense emergence of ‘big data’ and personalization are validating that prediction. And yet just because information flows freely does not imply that it cannot be monetized; both Google and Facebook illustrate that. In the next decade one will see firms that make these companies seem like niche players, as they develop and deploy information utilities that are going to be similar to helper artificial intelligence. This is a future where basic day-to-day decisions will be offloaded to these automated systems, which will receive as inputs both environmental and hereditary variables. Where does that leave humans? Our lot will be to enjoy the good life, reflecting upon the deeper and more important things that are not so reducible by data analysis engines. In contrast to the idea of a post-human future, the information ecology of the next few decades, driven in part by personal genomics, will be even more human as the banal and impersonal decisions that define modern life are taken over by automatons.",project-academic
10.1109/TCAD.2018.2884992,2020-01-01,a,IEEE,gpgpu based atpg system myth or reality," General-purpose computing on graphics processing units (GPGPUs) is a programming model that uses graphics cards to perform computations traditionally done by CPU. It began to become practical with the advent of programmable shaders and floating-point support on GPU in around 2001. The spread of GPGPU has been accelerated with introduction of CUDA from NVIDIA in 2006 and later OpenCL in 2009. Nowadays GPGPU is widely deployed in various applications, such as data mining, artificial intelligence, and many scientific computations. GPGPU seemingly promises immense parallelism with massive concurrent cores, and thus much shorter run times. This is true for algorithms that bear intrinsic data and task parallelism, such as image and video processing. For an ATPG system where some algorithms are sequential in nature, the speedup is not easy to achieve in the real world. Flaws in setting up speedup evaluation can lead to false promises. Will GPGPU-based ATPG system become a reality? Or it is just a myth. In this paper, we try to provide an answer by surveying state-of-the-art works and by analyzing practical aspects of today’s industrial designs.",project-academic
10.1109/MWC.2020.9085257,2020-05-13,a,IEEE,artificial intelligence driven fog radio access networks recent advances and future trends," The articles in this special section focus on artificial intelligence-driven fog radio access networks. To satisfy the explosively increasing demands of highspeed data applications and access requirements from a massive number of Internet-of-Things (IoT) devices, a paradigm of fog computing-based radio access network (F-RAN) has emerged as a promising evolution path for the fifth generation (5G) radio access networks. By taking full advantage of distributed caching and centralized processing, F-RANs provide great flexibility to satisfy the quality-of-service requirements of various 5G services. With the rapid deployment of 5G communication networks, the application of F-RANs to the envisioned sixth-generation (6G) mobile network has attracted extensive attention from academia, industry, and government agencies. The 6G network progress in enhanced mobile broadband, massive machine-type communications and ultra-reliable and low-latency communications will lead to the fast development of new applications, including augmented reality (AR), virtual reality (VR), holographic communications, vehicle-to-everything (V2X), self-driving cars, massive sensors connective on the ground and several tens of thousands of satellites connective in the sky.",project-academic
,2017-06-16,p,,proceedings of the 15th annual international conference on mobile systems applications and services," It is our pleasure and privilege to serve as program chairs for ACM MobiSys 2017 -- the 15th ACM International Conference on Mobile Systems, Applications, and Services. We hope you enjoy this technical material behind the conference that attracts a diverse set of attendees from both academia and industry and is a leading venue for publications and idea exchange on mobile systems. ACM MobiSys 2017 has a highly selective, single-track program featuring research related to mobile systems and applications. It is an ideal venue to address research challenges facing the design, development, deployment, use, and fundamental limits of these systems.

Notes on the review process: Since its inception in 2003 MobiSys has had a single-blind review policy where the identity of the reviewers is not revealed to the authors but the reviewers know the names, affiliations and contact information of the authors. Beginning 2017, we modified this policy so that the identity of the authors was not revealed to the reviewers during the initial review of the paper. However, once preliminary outcomes were decided, identities of the authors was revealed to enable referees to ask appropriate questions, making it easier to compare the new results with the author(s) previously published work and to ensure that a true advance was being reported.

Our paper review process this year was highly selective. Out of 188 submissions, the technical program committee accepted only 34 for publication and presentation as full papers, yielding an acceptance rate around 18%. Submitted papers underwent a rigorous, multi-stage review process. First, we checked all submissions for compliance, general quality, and topic match. We administratively rejected those not meeting our submission criteria. We assigned 3 reviewers to papers that survived this stage from the program committee and the external review committee. At the conclusion of this stage, those papers where none of the reviewers were enthusiastic about acceptance were rejected. We then assigned at least 2 additional reviewers from the program committee to papers that survived, thus totaling at least 5 reviews per paper. An online discussion phase then ensued, resulting in the PC recommending 60 papers for discussion at the PC meeting. The PC meeting was held in person in Sonoma CA, USA, the day after ACM HotMobile 2017. At the conclusion of the PC meeting, we accepted 34 papers to the conference. Several accepted papers were assigned shepherds to help ensure that the authors produce a final manuscript that satisfactorily addresses reviewer comments.

The program: Our program this year covers an exciting set of topics including sensing using acoustic, RF, and light signals, novel communication techniques, deep learning on mobiles, mobile performance, security and privacy, and operating systems. It also includes a keynote by Pattie Maes on human augmentation i.e. how systems can actively assist people with memory, learning, decision making, communication and physical skills, a SIGMOBILE Outstanding Contribution Award talk by Norman Abramson, recognized for his pioneering work on the ALOHAnet wireless networking system, five invited talks on trends in deep learning on mobiles, virtual reality, internet of things, biomedical diagnostics using phones, and large-scale wireless testbeds, brief talks by five Test-of-Time award winners, as well as an extensive poster/demo session.",project-academic
10.1109/OJCOMS.2020.3009023,2020-01-01,a,Institute of Electrical and Electronics Engineers (IEEE),enabling remote human to machine applications with ai enhanced servers over access networks," The recent research trends for achieving ultra-reliable and low-latency communication networks are largely driven by smart manufacturing and industrial Internet-of-Things applications. Such applications are being realized through Tactile Internet that allows users to control remote things and involve the bidirectional transmission of video, audio, and haptic data. However, the end-to-end propagation latency presents a stubborn bottleneck, which can be alleviated by using various artificial intelligence-based application layer and network layer prediction algorithms, e.g., forecasting and preempting haptic feedback transmission. In this paper, we study the experimental data on traffic characteristics of control signals and haptic feedback samples obtained through virtual reality-based human-to-machine teleoperation. Moreover, we propose the installation of edge-intelligence servers between master and slave devices to implement the preemption of haptic feedback from control signals. Harnessing virtual reality-based teleoperation experiments, we further propose a two-stage artificial intelligence-based module for forecasting haptic feedback samples. The first-stage unit is a supervised binary classifier that detects if haptic sample forecasting is necessary and the second-stage unit is a reinforcement learning unit that ensures haptic feedback samples are forecasted accurately when different types of material are present. Furthermore, by evaluating analytical expressions, we show the feasibility of deploying remote human-to-machine teleoperation over fiber backhaul by using our proposed artificial intelligence-based module, even under heavy traffic intensity.",project-academic
10.1109/MWC.001.2000303,2021-05-14,a,Institute of Electrical and Electronics Engineers (IEEE),wireless virtual reality in beyond 5g systems with the internet of intelligence," Virtual reality (VR) over wireless has promising applications in healthcare, education, entertainment, and industrial production. However, it is difficult for the existing wireless systems to meet the needs of massive content transmission, ultra-low latency, and high computation in wireless VR. In this article, with the recent advances of edge intelligence and the Internet of Intelligence, we propose a novel framework that can jointly provide computation, storage, and communication resources for wireless VR in beyond 5G systems. In this framework, intelligence can be fully exploited to coordinate the computing, caching, and transmission systems to enable ubiquitous deployments of wireless VR. We present some key techniques and propose specific methods to support wireless VR. In addition, we propose a novel quantum-inspired RL reinforcement learning (QRL) algorithm for the multidimensional resource provisioning issue in wireless VR. In the simulations, some essential performance metrics are evaluated and some interesting results are presented, showing the effectiveness of the proposed strategy.",project-academic
10.1109/MC.2007.372,2007-11-01,a,IEEE,automated killers and the computing profession," When will we realize that our artificial-intelligence and autonomous-robotics research projects have been harnessed to manufacture killing machines? This is not terminator-style science fiction but grim reality: South Korea and Israel have both deployed armed robot border guards, while other nations-including China, India, Russia, Singapore, and the UK-increasingly use military robots. Fully autonomous robots that make their own decisions about lethality appear high on the US military agenda. They benefit the US military by enabling a single battlefield soldier to initiate a large-scale robot attack from the air or on the ground.",project-academic
10.5555/2484920.2484930,2013-05-06,p,International Foundation for Autonomous Agents and Multiagent Systems,humanoid robots learning to walk faster from the real world to simulation and back," Simulation is often used in research and industry as a low cost, high efficiency alternative to real model testing. Simulation has also been used to develop and test powerful learning algorithms. However, parameters learned in simulation often do not translate directly to the application, especially because heavy optimization in simulation has been observed to exploit the inevitable simulator simplifications, thus creating a gap between simulation and application that reduces the utility of learning in simulation.This paper introduces Grounded Simulation Learning (GSL), an iterative optimization framework for speeding up robot learning using an imperfect simulator. In GSL, a behavior is developed on a robot and then repeatedly: 1) the behavior is optimized in simulation; 2) the resulting behavior is tested on the real robot and compared to the expected results from simulation, and 3) the simulator is modified, using a machine-learning approach to come closer in line with reality. This approach is fully implemented and validated on the task of learning to walk using an Aldebaran Nao humanoid robot. Starting from a set of stable, hand-coded walk parameters, four iterations of this three-step optimization loop led to more than a 25% increase in the robot's walking speed.",project-academic
10.1364/JOCN.403205,2021-01-01,a,Optical Society of America,open whitebox architecture for smart integration of optical networking and data center technology invited," In this paper, we identify challenges in developing future optical network infrastructure for new services based on technologies such as 5G, virtual reality, and artificial intelligence, and we suggest approaches to handling these challenges that include a business model, architecture, and diversity. Through activities in multiservice agreement and de facto standard organizations, we have shown how the hardware abstraction layer interfaces of optical transceivers are implemented for multivendor and heterogeneous environments, coherent digital signal processor interoperability, and optical transport whiteboxes. We have driven the effort to define the transponder abstraction interface with partners. The feasibility of such implementation was verified through demonstrations and trials. In addition, we are constructing an open-transport platform by combining existing open-source software and implementing software components that automate and enhance operations. An open architecture maintains a healthy ecosystem for industry and allows for a flexible, operator-driven network.",project-academic
10.1109/TASE.2018.2877499,2019-10-01,a,IEEE,coarse to fine uav target tracking with deep reinforcement learning," The aspect ratio of a target changes frequently during an unmanned aerial vehicle (UAV) tracking task, which makes the aerial tracking very challenging. Traditional trackers struggle from such a problem as they mainly focus on the scale variation issue by maintaining a certain aspect ratio. In this paper, we propose a coarse-to-fine deep scheme to address the aspect ratio variation in UAV tracking. The coarse-tracker first produces an initial estimate for the target object, then a sequence of actions are learned to fine-tune the four boundaries of the bounding box. The coarse-tracker and the fine-tracker are designed to have different action spaces and operating target. The former dominates the entire bounding box and the latter focuses on the refinement of each boundary. They are trained jointly by sharing the perception network with an end-to-end reinforcement learning architecture. Experimental results on benchmark aerial data set prove that the proposed approach outperforms existing trackers and produces significant accuracy gains in dealing with the aspect ratio variation in UAV tracking. None Note to Practitioners —During the past years, unmanned aerial vehicle (UAV) have gained much attention for both industrial and consumer uses. It is in urgent demand to endow the UAV with intelligent vision-based techniques, and the automatic target following via visual tracking methods as one of the most fundamental intelligent features could promote various applications of UAVs, such as surveillance, augmented reality, and behavior modeling. Nonetheless, the primary issue of a UAV-based tracking method is the platform itself: it is not stable, it tends to have sudden movements, it generates nonhomogeneous data (scale, angle, rotation, depth, and so on), all of them tend to change the aspect ratio of the target frequently and further increase the difficulty of object tracking. This paper aims to address the aspect ratio change (ARC) problem in UAV tracking. We present a coarse-to-fine strategy for UAV tracking. Specifically, the coarse bounding box is obtained to locate the target firstly. Then, a refinement scheme is performed on each boundary to further improve the position estimate. The tracker is proved to be effective to increase the resistance to the ARC. Such a method can be implemented on UAV to improve the target-following performance.",project-academic
10.1016/J.RCIM.2019.101887,2020-06-01,a,Pergamon,deep learning based smart task assistance in wearable augmented reality," Abstract None None Wearable augmented reality (AR) smart glasses have been utilized in various applications such as training, maintenance, and collaboration. However, most previous research on wearable AR technology did not effectively supported situation-aware task assistance because of AR marker-based static visualization and registration. In this study, a smart and user-centric task assistance method is proposed, which combines deep learning-based object detection and instance segmentation with wearable AR technology to provide more effective visual guidance with less cognitive load. In particular, instance segmentation using the Mask R-CNN and markerless AR are combined to overlay the 3D spatial mapping of an actual object onto its surrounding real environment. In addition, 3D spatial information with instance segmentation is used to provide 3D task guidance and navigation, which helps the user to more easily identify and understand physical objects while moving around in the physical environment. Furthermore, 2.5D or 3D replicas support the 3D annotation and collaboration between different workers without predefined 3D models. Therefore, the user can perform more realistic manufacturing tasks in dynamic environments. To verify the usability and usefulness of the proposed method, we performed quantitative and qualitative analyses by conducting two user studies: 1) matching a virtual object to a real object in a real environment, and 2) performing a realistic task, that is, the maintenance and inspection of a 3D printer. We also implemented several viable applications supporting task assistance using the proposed deep learning-based task assistance in wearable AR.",project-academic
10.1109/EIT.2018.8500102,2018-05-03,p,IEEE,behavioral cloning for lateral motion control of autonomous vehicles using deep learning," Current trend of the automotive industry combined with research by the major tech companies has proved that self-driving vehicles are the future. With successful demonstration of neural network based autonomous driving, NVIDIA has introduced a new paradigm for autonomous driving software. The biggest challenge for self-driving cars is autonomous lateral control. An end-to-end model seems very promising in providing a complete software stack for autonomous driving. Although this system is not ready to be provided as a feature in the market today, it is one of the many steps in the right direction to make self-driving cars a reality. The work described in this paper focusses on how an end-to-end model is implemented. The subtleties of training a successful end-to-end model are highlighted with the aim of providing an insight on deep learning and software required for neural network training. Detailed analyses of data acquisition and training systems are provided and installation procedures for all required tools and software discussed. TORCS is used for developing and testing the end-to-end model. Approximately ten hours of driving data was collected from two different tracks. Using four hours of data from a track, we trained a deep neural network to steer a car inside simulation. Even with such a small training set, the end-to-end model developed demonstrated capabilities to maintain lanes and complete laps in different tracks. For a multilane track, like the one used for training, the model demonstrated an autonomy of 96.62%. For single lane unknown tracks, the model steered the vehicle successfully for 89.02% of the time. The results indicate that end-to-end learning and behavioral cloning can be used to drive autonomously in new and unknown scenarios.",project-academic
10.1007/S12667-015-0155-7,2016-05-01,a,Springer Berlin Heidelberg,a comparative study on applications of artificial intelligence based multiple models predictive control schemes to a class of industrial complicated systems," The purpose of the present research is to demonstrate a comparative study on applications of artificial intelligence-based multiple models predictive control schemes, which are considered in a number of referenced materials. These control schemes are to implement on a class of industrial complicated systems, as long as the traditional related cases are not to guarantee the desired tracking performance, efficiently. In reality, the research proposed here, in its present form, outlines the achieved results of the control schemes, which are all organized based on both the multiple models strategy and the linear model based predictive control approach, as well. In one such case, the outcomes are focused on an industrial tubular heat exchanger system, which has so many applicabilities in real and academic environments. The traditional schemes are almost implemented on the system to control the outlet temperature of the inner tube by either the temperature or the flow of the fluid flowing, concurrently, through the shell tube. In some situations, the appropriate control scheme realization is not possible, due to the fact that the whole of system coefficients variation cannot quite be covered by the control action. In case of the matter presented, the techniques need to be organized, which the tracking performance both in the system coefficients and also in the desired set point variations could acceptably be guaranteed. Hereinafter, the performance mentioned and also the weight of realization of each one of the proposed control schemes have been surveyed, while all of them are presented in shortened version and therefore their details are not thoroughly given here. In such a case, some schemes are now available in the corresponding research, that are fully referenced, in the present investigation. In agreement with the acquired results, the validity of the control schemes are tangibly verified and also compared with respect to each other. Consequently, the finalized control schemes are suggested, where the advantages and its disadvantages of each one of them over the system are accurately investigated in line with the related reasons.",project-academic
10.1007/978-981-15-4474-3_31,2021-01-01,a,"Springer, Singapore",artificial intelligence prospect in mechanical engineering field a review," With the continuous progress of science and technology, the mechanical field is also constantly upgrading from traditional mechanical engineering to the mechatronics engineering and artificial intelligence (AI) is one of them. AI deals with a computer program that possesses own decision-making capability to solve a problem of interest with imitates the intelligent behavior of expertise which finally turns into higher productivity with better quality output. From the inception, various developments have been done on AI system which nowadays widely implemented in the mechanical and/or manufacturing industries with broaden area of application such as pattern recognition, automation, computer vision, virtual reality, diagnosis, image processing, nonlinear control, robotics, automated reasoning, data mining and process control systems. In this study, review attempt has been made for AI technologies used in various mechanical fields such as thermal, manufacturing, design, quality control and various connected fields of mechanical engineering. The study shows the blend mixed of AI technologies like deep convolutional neural network (DCNN), convolutional neural network (CNN), artificial neural network (ANN), fuzzy logic and many more to control the process parameters, process planning, machining, quality control and optimization in the mechanical era for smooth development of product or system. With the implementation of AI in mechanical engineering applications, the error, rejection of components can be minimized or eliminated and system optimization can be achieved effectively turn in economical better quality products.",project-academic
10.1109/COMPSAC.2018.10204,2018-07-23,p,IEEE,indoor augmented reality using deep learning for industry 4 0 smart factories," This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",project-academic
,2019-05-15,a,,domain adaptive transfer learning for fault diagnosis," Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models fromone machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although data-driven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.",project-academic
10.1109/PHM-PARIS.2019.00054,2019-05-02,p,IEEE,domain adaptive transfer learning for fault diagnosis," Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models from one machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although datadriven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.",project-academic
10.1259/BJR.20190855,2020-01-22,a,Br J Radiol,artificial intelligence reshaping the practice of radiological sciences in the 21st century," Advances in computing hardware and software platforms have led to the recent resurgence in artificial intelligence (AI) touching almost every aspect of our daily lives by its capability for automating complex tasks or providing superior predictive analytics. AI applications are currently spanning many diverse fields from economics to entertainment, to manufacturing, as well as medicine. Since modern AI's inception decades ago, practitioners in radiological sciences have been pioneering its development and implementation in medicine, particularly in areas related to diagnostic imaging and therapy. In this anniversary article, we embark on a journey to reflect on the learned lessons from past AI's chequered history. We further summarize the current status of AI in radiological sciences, highlighting, with examples, its impressive achievements and effect on re-shaping the practice of medical imaging and radiotherapy in the areas of computer-aided detection, diagnosis, prognosis, and decision support. Moving beyond the commercial hype of AI into reality, we discuss the current challenges to overcome, for AI to achieve its promised hope of providing better precision healthcare for each patient while reducing cost burden on their families and the society at large.",project-academic
10.1109/ACCESS.2019.2957245,2019-12-02,a,IEEE,photonic neural networks a survey," Photonic solutions are today a mature industrial reality concerning high speed, high throughput data communication and switching infrastructures. It is still a matter of investigation to what extent photonics will play a role in next-generation computing architectures. In particular, due to the recent outstanding achievements of artificial neural networks, there is a big interest in trying to improve their speed and energy efficiency by exploiting photonic-based hardware instead of electronic-based hardware. In this work we review the state-of-the-art of photonic artificial neural networks. We propose a taxonomy of the existing solutions (categorized into multilayer perceptrons, convolutional neural networks, spiking neural networks, and reservoir computing) with emphasis on proof-of-concept implementations. We also survey the specific approaches developed for training photonic neural networks. Finally we discuss the open challenges and highlight the most promising future research directions in this field.",project-academic
10.33407/ITLT.V70I2.2907,2019-04-27,a,Institute of Information Technologies and Learning Tools of NAES of Ukraine,стан технології та перспективи дистанційного навчання у вищій освіті україни," On the basis of the analysis of queries in Google Trends, Google Scholar, and the database of Volodymyr Vernadskyi National Library of Ukraine, a sample of historically structured knowledge systems has been obtained which can be used to describe the contribution of distance learning studies during the time slots available for each resource. According to the survey of students of four institutions of different education types (classical, technological, pedagogical and marine ones), the modern state-of-the-art of the application of distance learning technologies at higher education institutions is established. The analysis of the progressive ideas and practical achievements of countries of Europe, North America and Asia, which in recent years have made the significant progress in reforming their educational systems and the implementation of innovative technologies, has allowed to distinguish the following technological advances for the implementation of distance learning technologies: the adaptive learning technology, mobile learning, the virtual, supplemented and hybrid reality, the Internet of Things, systems of the next generation learning management, artificial intelligence and natural user interfaces. The following perspectives of the distance learning development in Ukraine are highlighted: updating of software and technical support and material resources of higher education institutions; provision of higher education institutions of Ukraine with the broadband Internet access; organization of cooperation of software developers for distance learning, distance education methodologists and lecturer; improvement the staffing of distance learning; development and distribution of platforms with intuitive non-complex software interfaces for creating distance courses; creation or adaptation of information technologies and electronic educational and methodological developments for the support of new technologies of distance learning at higher education institutions of Ukraine; studying the effectiveness of technological advances in the IT industry in the process of higher education teaching and learning, ensuring the process of obtaining an educational degree (bachelor’s degree, master’s degree) at higher education institutions of Ukraine through training at mass open distance courses.",project-academic
10.1007/978-3-030-22312-0_1,2019-06-25,p,"Springer, Cham",hunting brand domain forgery a scalable classification for homograph attack," Visual homograph attack is a way that the attackers deceive victims about what domain they are communicating with by exploiting the fact that many characters look alike. The attack is growing into a serious problem and raising broad attention in reality when recently many brand domains have been attacked such as apple.com (Apple Inc.), adobe.com (Adobe Systems Incorporated), lloydsbank.co.uk (Lloyds Bank), etc. Therefore, how to detect visual homograph becomes a hot topic both in industry and research community. Several existing papers and tools have been proposed to find some homographs of a given domain based on different subsets of certain look-alike characters, or based on an analysis on the registered International Domain Name (IDN) database. However, we still lack a scalable and systematic approach that can detect sufficient homographs registered by attackers with a high accuracy and low false positive rate. In this paper, we construct a classification model to detect homographs and potential homographs registered by attackers using machine learning on feasible and novel features which are the visual similarity on each character and some selected information from Whois. The implementation results show that our approach can bring up to 95.90% of accuracy with merely 3.27% of false positive rate. Furthermore, we also make an empirical analysis on the collected homographs and found some interesting statistics along with concrete misbehaviors and purposes of the attackers.",project-academic
10.1109/JPROC.2018.2856739,2018-08-14,a,IEEE,navigating the landscape for real time localization and mapping for robotics and virtual and augmented reality," Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",project-academic
10.1109/JPROC.2018.2856739,2018-08-20,a,,navigating the landscape for real time localisation and mapping for robotics and virtual and augmented reality," Visual understanding of 3D environments in real-time, at low power, is a huge computational challenge. Often referred to as SLAM (Simultaneous Localisation and Mapping), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are (1) tools and methodology for systematic quantitative evaluation of SLAM algorithms, (2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives, (3) end-to-end simulation tools to enable optimisation of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches, and (4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",project-academic
10.17747/2078-8886-2018-3-88-107,2018-10-09,a,Общество с ограниченной ответственностью «Издательский дом «Реальная экономика»,технологии виртуальной и дополненной реальности возможности и препятствия применения," The first attempts to create devices that allow interacting with the imitated reality, as well as augmenting reality with superimposed information, were made at the beginning of the 20th century, the very concept of mixed reality (the “reality-virtuality continuum”), which elements are virtual (VR) and augmented (AR) reality, is quite young (24 years), as well as the market of these technologies. The concept of virtual and augmented reality hasn’t changed radically in the past 30 years, but VR and AR devices and software, and content have gone through a significant evolutionary path, and have already experienced several growth spikes. VR and AR technologies can be applied not only in entertainment and games. Many experts believe that virtual and augmented reality, along with Big Data, cloud technologies, artificial intelligence and some others, will become the key technologies of the 4th industrial revolution. VR and AR also have the potential to become the next big computing platform. Today VR and AR technologies help not only to create conceptually new markets, but also to disrupt existing ones. This article discusses the evolution of the VR and AR concepts and technologies and current market trends. The results of the survey show the key obstacles for the mass distribution of AR and VR technologies: high implementation and operational costs of AR/VR solutions; lack of high-quality content and imperfect devices, implicit effectiveness of their use. Based on the empirical study, a rather extensive list of benefits from using virtual and augmented reality technologies has been drawn up: faster and cheaper learning, training and guiding processes, increase in their efficiency, the reduction of the costs of elements and supplies needed, training support personnel; reducing potential risks to life and health of employees and other people while special training (medical operations and invasive procedures, evacuation, security, rescue in various emergencies) and the related optimization of the compensations; reducing the number of errors and accelerating the processes of assembling, repairing and operating special equipment, searching for information, necessary details, product location in the warehouse; significant reduction of accidents rate, as well as the exploration costs, due to the early identification of malfunctions; accelerating the pace of the designing and prototyping objects, significantly reducing the cost and duration of physical modeling process; improving customer experience, product and trading platforms design, that leads to corresponding increase in volume of sales; improving (simplifying) of communication and increasing its effectiveness.",project-academic
10.3390/BUILDINGS10110204,2020-11-01,a,MDPI AG,a systematic review of digital technology adoption in off site construction current status and future direction towards industry 4 0," Off-site construction (OSC) is known as an efficient construction method that could save time and cost, reduce waste of resources, and improve the overall productivity of projects. Coupled with digital technologies associated with the Industry 4.0 concept, OSC can offer a higher rate of productivity and safety. While there is a rich literature focusing on both OSC and Industry 4.0, the implementation of associated digital technologies in the OSC context has not been fully evaluated. This paper intends to evaluate the current literature of digital technology applications in OSC. Scientometric analyses and a systematic review were carried out evaluating fifteen typical digital technologies adopted by OSC projects, including building information modelling (BIM), radio frequency identification devices (RFID), global positioning systems (GPS), the Internet of Things (IoT), geographic information systems (GIS), sensors, augmented reality (AR), virtual reality (VR), photogrammetry, laser scanning, artificial intelligence (AI), 3D printing, robotics, big data, and blockchain. This review formulates a clear picture of the current practice of these digital technologies and summarizes the main area of application and limitations of each technology when utilized in OSC. The review also points out their potential and how they can be better adopted to improve OSC practice in the future.",project-academic
10.1109/MIM.2017.8121947,2017-12-01,a,Institute of Electrical and Electronics Engineers (IEEE),smart metrology the importance of metrology of decisions in the big data era," The beginning of the 21st century has seen the birth of a new industrial revolution, the digital revolution: the ability to store data from various sources (in particular through related items) in unprecedented big quantities and to exploit them through more and more high computing capabilities using artificial intelligence techniques (IA). In this new scenario called “Big Data,” data reliability becomes an indispensable property: the large amount of data collected and their analysis are completely useless if untrusted information is stored that cannot help understanding a complex reality. Here is the role of metrology: to ensure the reliability of measured data. Smart Metrology is the implementation of a revised metrological function, oriented to reliability, rather than simply on complying with standards: ensuring reliability of measurements, knowing and mastering measurement uncertainty to make relevant decisions. This paper is a summary of issues for business metrology.",project-academic
,2018-01-17,a,,the case for automatic database administration using deep reinforcement learning," Like any large software system, a full-fledged DBMS offers an overwhelming amount of configuration knobs. These range from static initialisation parameters like buffer sizes, degree of concurrency, or level of replication to complex runtime decisions like creating a secondary index on a particular column or reorganising the physical layout of the store. To simplify the configuration, industry grade DBMSs are usually shipped with various advisory tools, that provide recommendations for given workloads and machines. However, reality shows that the actual configuration, tuning, and maintenance is usually still done by a human administrator, relying on intuition and experience. Recent work on deep reinforcement learning has shown very promising results in solving problems, that require such a sense of intuition. For instance, it has been applied very successfully in learning how to play complicated games with enormous search spaces. Motivated by these achievements, in this work we explore how deep reinforcement learning can be used to administer a DBMS. First, we will describe how deep reinforcement learning can be used to automatically tune an arbitrary software system like a DBMS by defining a problem environment. Second, we showcase our concept of NoDBA at the concrete example of index selection and evaluate how well it recommends indexes for given workloads.",project-academic
10.1007/S005300050109,1999-01-01,a,"Springer-Verlag New York, Inc.",interactive environments for music and multimedia," Multimodal Environments (MEs) are systems capable of establishing creative, multimodal user interactionby exhibiting real-time adaptive behaviour. In a typical scenario, one or more users are immersed in an environmentallowing them to communicate by means of full-body movement, singing or playing. Users get feedback from the environment in real time in terms of sound, music, visual media,and actuators, i.e. movement of semi-autonomous mobilesystems including mobile scenography, on-stage robots behaving as actors or players, possibly equipped with musicand multimedia output. MEs are therefore a sort of extension of augmented reality environments. From another viewpoint, an ME can be seen as a sort of prolongation of thehuman mind and senses. From an artificial intelligence perspective, an ME consists of a population of physical andas software agents capable of changing their reactions andtheir social interaction over time. For example, a gesture ofthe user(s) can mean different things in different situations,and can produce changes in the agents populating the ME.The paradigm adopted for movement recognition is that ofa human observer of the dance, where the focus of attentionchanges according to the evolution of the dance itself andof the music produced. MEs are therefore agents able to observe the user, extract ""gesture gestalts"", and change theirstate, including artificial emotions, over time. MEs open newniches of application, many still to be discovered, includingmusic, dance, theatre, interactive arts, entertainment, interactive exhibitions and museal installations, information atelier, edutainment, training, industrial applications and cognitive rehabilitation (e.g. for autism). The environment canbe a theatre, a museum, a discotheque, a school classroom,a rehabilitation centre for patients with a variety of sensory/motor and cognitive impairments, etc. The ME conceptgeneralizes the bio-feedback methods which already havefound widespread applications. The paper introduces MEs,then a flexible ME architecture, with a special focus on themodeling of the emotional component of the agents formingan ME. Description of four applications we recently developed, currently used in several real testbeds, conclude thepaper.",project-academic
10.1007/978-90-481-9151-2,2010-07-12,b,"Springer Publishing Company, Incorporated",technological developments in networking education and automation," Technological Developments in Networking, Education and Automation includes a set of rigorously reviewed world-class manuscripts addressing and detailing state-of-the-art research projects in the following areas: Computer Networks: Access Technologies, Medium Access Control, Network architectures and Equipment, Optical Networks and Switching, Telecommunication Technology, and Ultra Wideband Communications. Engineering Education and Online Learning: including development of courses and systems for engineering, technical and liberal studies programs; online laboratories; intelligent testing using fuzzy logic; taxonomy of e-courses; and evaluation of online courses. Pedagogy: including benchmarking; group-learning; active learning; teaching of multiple subjects together; ontology; and knowledge management. Instruction Technology: including internet textbooks; virtual reality labs, instructional design, virtual models, pedagogy-oriented markup languages; graphic design possibilities; open source classroom management software; automatic email response systems; tablet-pcs; personalization using web mining technology; intelligent digital chalkboards; virtual room concepts for cooperative scientific work; and network technologies, management, and architecture. Coding and Modulation: Modeling and Simulation, OFDM technology , Space-time Coding, Spread Spectrum and CDMA Systems. Wireless technologies: Bluetooth , Cellular Wireless Networks, Cordless Systems and Wireless Local Loop, HIPERLAN, IEEE 802.11, Mobile Network Layer, Mobile Transport Layer, and Spread Spectrum. Network Security and applications: Authentication Applications, Block Ciphers Design Principles, Block Ciphers Modes of Operation, Electronic Mail Security, Encryption & Message Confidentiality, Firewalls, IP Security, Key Cryptography & Message Authentication, and Web Security. Robotics, Control Systems and Automation: Distributed Control Systems, Automation, Expert Systems, Robotics, Factory Automation, Intelligent Control Systems, Man Machine Interaction, Manufacturing Information System, Motion Control, and Process Automation. Vision Systems: for human action sensing, face recognition, and image processing algorithms for smoothing of high speed motion. Electronics and Power Systems: Actuators, Electro-Mechanical Systems, High Frequency Converters, Industrial Electronics, Motors and Drives, Power Converters, Power Devices and Components, and Power Electronics.",project-academic
10.1109/AIVR46125.2019.00063,2019-12-01,p,IEEE,structuring and inspecting 3d anchors for seismic volume into hyperknowledge base in virtual reality," Seismic data is a source of information geoscientists use to investigate underground regions to look for resources to explore. Such data are volumetric and noisy, and thus challenging to visualize, which motivated the research of new computational systems to assist the expert, such as visualization methods, signal processing, and machine learning models. We propose a system that aids geologists, geophysicists, and related experts in the domain in interpreting seismic data in virtual reality (VR). The system uses a hyperknowledge base (HKBase), which structures regions of interest (ROIs) as anchors with semantics from the user to the system and vice-versa. For instance, through the HKBase, the user can load and inspect the output from AI systems or give new inputs and feedback in the same way. We ran tests with experts to evaluate the system in their tasks to collect feedback and new insights on how the software could transform their routines. In accordance with our results, we claim we took one step forward for VR in the oil & gas industry by creating a valuable experience in the task of seismic interpretation.",project-academic
10.3390/SU12187272,2020-09-04,a,Multidisciplinary Digital Publishing Institute,information and communication technology solutions for the circular economy," The concept of circular economy (CE) is becoming progressively popular with academia, industry, and policymakers, as a potential path towards a more sustainable economic system. Information and communication technology (ICT) systems have influenced every aspect of modern life and the CE is no exception. Cutting-edge technologies, such as big data, cloud computing, cyber-physical systems, internet of things, virtual and augmented reality, and blockchain, can play an integral role in the embracing of CE concepts and the rollout of CE programs by governments, organizations, and society as a whole. The current paper conducts an extensive academic literature review on prominent ICT solutions paving the way towards a CE. For the categorization of the solutions, a novel two-fold approach is introduced, focusing on both the technological aspect of the solutions (e.g., communications, computing, data analysis, etc.), and the main CE concept(s) employed (i.e., reduce, reuse, recycle and restore) that each solution is the most relevant to. The role of each solution in the transition to CE is highlighted. Results suggest that ICT solutions related to data collection and data analysis, and in particular to the internet of things, blockchain, digital platforms, artificial intelligence algorithms, and software tools, are amongst the most popular solutions proposed by academic researchers. Results also suggest that greater emphasis is placed on the “reduce” component of the CE, although ICT solutions for the other “R” components, as well as holistic ICT-based solutions, do exist as well. Specific important challenges impeding the adoption of ICT solutions for the CE are also identified and reviewed, with consumer and business attitude, economic costs, possible environmental impacts, lack of education around the CE, and lack of familiarization with modern technologies being found among the most prominent ones.",project-academic
10.1145/3368089.3409737,2020-11-08,p,ACM,mining assumptions for software components using machine learning," Software verification approaches aim to check a software component under analysis for all possible environments. In reality, however, components are expected to operate within a larger system and are required to satisfy their requirements only when their inputs are constrained by environment assumptions. In this paper, we propose EPIcuRus, an approach to automatically synthesize environment assumptions for a component under analysis (i.e., conditions on the component inputs under which the component is guaranteed to satisfy its requirements). EPIcuRus combines search-based testing, machine learning and model checking. The core of EPIcuRus is a decision tree algorithm that infers environment assumptions from a set of test results including test cases and their verdicts. The test cases are generated using search-based testing, and the assumptions inferred by decision trees are validated through model checking. In order to improve the efficiency and effectiveness of the assumption generation process, we propose a novel test case generation technique, namely Important Features Boundary Test (IFBT), that guides the test generation based on the feedback produced by machine learning. We evaluated EPIcuRus by assessing its effectiveness in computing assumptions on a set of study subjects that include 18 requirements of four industrial models. We show that, for each of the 18 requirements, EPIcuRus was able to compute an assumption to ensure the satisfaction of that requirement, and further, ≈78% of these assumptions were computed in one hour.",project-academic
10.1109/TASE.2019.2938316,2020-04-01,a,IEEE,semiautomatic labeling for deep learning in robotics," In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35 000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets. None Note to Practitioners —This article was motivated by the lack of a simple and effective solution for the generation of data sets usable to train a data-driven model, such as a modern deep neural network, so as to make them accessible in an industrial environment. Specifically, a deep learning robot guidance vision system would require such a large amount of manually labeled images that it would be too expensive and impractical for a real use case, where system reconfigurability is a fundamental requirement. With our system, on the other hand, especially in the field of industrial robotics, the cost of image labeling can be reduced, for the first time, to nearly zero, thus paving the way for self-reconfiguring systems with very high performance (as demonstrated by our experimental results). One of the limitations of this approach is the need to use a manual method for the detection of objects of interest in the preliminary stages of the pipeline (ARP or graphical interface). A feasible extension, related to the field of collaborative robotics, could be used to exploit the robot itself, manually moved by the user, even for this preliminary stage, so as to eliminate any source of inaccuracy.",project-academic
10.1109/MCI.2018.2807019,2018-04-11,a,IEEE,is evolutionary computation evolving fast enough," Evolutionary Computation (EC) has been an active research area for over 60 years, yet its commercial/home uptake has not been as prolific as we might have expected. By way of comparison, technologies such as 3D printing, which was introduced about 35 years ago, has seen much wider uptake, to the extent that it is now available to home users and is routinely used in manufacturing. Other technologies, such as immersive reality and artificial intelligence have also seen commercial uptake and acceptance by the general public. In this paper we provide a brief history of EC, recognizing the significant contributions that have been made by its pioneers. We focus on two methodologies (Genetic Programming and Hyper-heuristics), which have been proposed as being suitable for automated software development, and question why they are not used more widely by those outside of the academic community. We suggest that different research strands need to be brought together into one framework before wider uptake is possible. We hope that this position paper will serve as a catalyst for automated software development that is used on a daily basis by both companies and home users.",project-academic
,2008-12-17,a,,implementation and evaluation of a vr task based training tool for conveyor belt safety training," Conveyor belts are dangerous to the working environment and the source of many injuries and fatalities that end up costing the industry a lot of money. In order to reduce the injuries that occur on an annual basis, better training techniques are examined. None Virtual reality is examined as a way to enhance current training practices that consist mainly of slide show presentations and videos. None Virtual reality is examined for task-based training where the user can interact with a working environment safely. None The user is given tasks to complete within the virtual environment similar to tasks a worker would have to complete on a day to day basis. None The purpose of the program is to be a method of rapidly training younger miners while supplementing on-the-job training practices and ultimately reduce accidents. None The model was developed using CAD software and then imported into Deep Creators Right Hemisphere where animations were added and functions were programmed to objects. None The users performance is tracked through the use of LISP programming and scores are tallied to judge user performance. None The application was tested for quality of information, usability, and learning potential over other training methods.This paper is an expansion of a paper presented at the CONVR 2007 conference at Penn State University, State College, PA, USA on October 22-23, 2007 (Lucas and Thabet, 2007).",project-academic
10.1109/ISMAR.2019.00-22,2019-10-18,p,IEEE,vr props an end to end pipeline for transporting real objects into virtual and augmented environments," Improvements in both software and hardware, as well as an increase in consumer suitable equipment, have resulted in great advances in the fields of virtual and augmented reality. Typically, systems use controllers or hand gestures to interact with virtual objects. However, these motions are often unnatural and diminish the immersion of the experience. Moreover, these approaches offer limited tactile feedback. There does not currently exist a platform to bring an arbitrary physical object into the virtual world without additional peripherals or the use of expensive motion capture systems. Such a system could be used for immersive experiences within the entertainment industry as well as being applied to VR or AR training experiences, in the fields of health and engineering. We propose an end-to-end pipeline for creating an interactive virtual prop from rigid and non-rigid physical objects. This includes a novel method for tracking the deformations of rigid and non-rigid objects at interactive rates using a single RGBD camera. We scan our physical object and process the point cloud to produce a triangular mesh. A range of possible deformations can be obtained by using a finite element method simulation and these are reduced to a low dimensional basis using principal component analysis. Machine learning approaches, in particular neural networks, have become key tools in computer vision and have been used on a range of tasks. Moreover, there has been an increased trend in training networks on synthetic data. To this end, we use a convolutional neural network, trained on synthetic data, to track the movement and potential deformations of an object in unlabelled RGB images from a single RGBD camera. We demonstrate our results for several objects with different sizes and appearances.",project-academic
10.1145/3219819.3219823,2018-07-19,p,ACM,deep interest network for click through rate prediction," Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",project-academic
10.1016/J.ZEMEDI.2018.11.002,2018-11-25,a,,an overview of deep learning in medical imaging focusing on mri," What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. 
Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of machine learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.",project-academic
10.1126/SCIENCE.1208365,2012-01-06,a,American Association for the Advancement of Science,the technology path to deep greenhouse gas emissions cuts by 2050 the pivotal role of electricity," The Technology Path to Deep Greenhouse Gas Emissions Cuts by 2050: The Pivotal Role of Electricity James H. Williams, 1,2 Andrew DeBenedictis, 1 Rebecca Ghanadan, 1,3 Amber Mahone, 1 Jack Moore, 1 William R. Morrow III, 4 Snuller Price, 1 Margaret S. Torn 3 * Several states and countries have adopted targets for deep reductions in greenhouse gas emissions by 2050, but there has been little physically realistic modeling of the energy and economic transformations required. We analyzed the infrastructure and technology path required to meet California’s goal of an 80% reduction below 1990 levels, using detailed modeling of infrastructure stocks, resource constraints, and electricity system operability. We found that technically feasible levels of energy efficiency and decarbonized energy supply alone are not sufficient; widespread electrification of transportation and other sectors is required. Decarbonized electricity would become the dominant form of energy supply, posing challenges and opportunities for economic growth and climate policy. This transformation demands technologies that are not yet commercialized, as well as coordination of investment, technology development, and infrastructure deployment. n 2004, Pacala and Socolow (1) proposed a way to stabilize climate using existing green- house gas (GHG) mitigation technologies, vi- sualized as interchangeable, global-scale “wedges” of equivalent emissions reductions. Subsequent work has produced more detailed analyses, but none combines the sectoral granularity, physical and resource constraints, and geographic scale needed for developing realistic technology and policy roadmaps (2–4). We addressed this gap by analyzing the specific changes in infrastructure, technology, cost, and governance required to de- carbonize a major economy, at the state level, that has primary jurisdiction over electricity supply, transportation planning, building standards, and other key components of an energy transition. California is the world’s sixth largest econ- omy and 12th largest emitter of GHGs. Its per capita GDP and GHG emissions are similar to those of Japan and western Europe, and its policy and technology choices have broad rele- vance nationally and globally (5, 6). California’s Assembly Bill 32 (AB32) requires the state to reduce GHG emissions to 1990 levels by 2020, a reduction of 30% relative to business-as-usual assumptions (7). Previous modeling work we per- formed for California’s state government formed the analytical foundation for the state’s AB32 implementation plan in the electricity and natural gas sectors (8, 9). California has also set a target of reducing 2050 emissions 80% below the 1990 level, con- I Energy and Environmental Economics, 101 Montgomery Street, Suite 1600, San Francisco, CA 94104, USA. 2 Monterey Institute of International Studies, 460 Pierce Street, Monterey, CA 93940, USA. 3 Energy and Resources Group, University of Cali- fornia,& Earth Sciences Division, Lawrence Berkeley National Laboratory (LBNL),, Berkeley, CA 94720, USA. 4 Environmental Energy Technologies Division, LBNL, Berkeley, CA 94720, USA. *To whom correspondence should be addressed. E-mail: mstorn@lbl.gov sistent with an Intergovernmental Panel on Cli- mate Change (IPCC) emissions trajectory that would stabilize atmospheric GHG concentrations at 450 parts per million carbon dioxide equivalent (CO 2 e) and reduce the likelihood of dangerous an- thropogenic interference with climate (10). Work- ing at both time scales, we found a pressing need for methodologies that bridge the analytical gap between planning for shallower, near-term GHG reductions, based entirely on existing commercialized technology, and deeper, long-term GHG reduc- tions, which will depend substantially on technol- ogies that are not yet commercialized. We used a stock-rollover methodology that simulated physical infrastructure at an aggregate level, and built scenarios to explore mitigation options (11, 12). Our model divided California’s economy into six energy demand sectors and two energy supply sectors, plus cross-sectoral eco- nomic activities that produce non-energy and non-CO 2 GHG emissions. The model adjusted the infrastructure stock (e.g., vehicle fleets, build- ings, power plants, and industrial equipment) in each sector as new infrastructure was added and old infrastructure was retired, each year from 2008 to 2050. We constructed a baseline scenario from government forecasts of population and gross state product, combined with regression-based infra- structure characteristics and emissions intensities, producing a 2050 emissions baseline of 875 Mt CO 2 e (Fig. 1). In mitigation scenarios, we used backcasting, setting 2050 emissions at the state target of 85 Mt CO 2 e as a constrained outcome, and altered the emissions intensities of new in- frastructure over time as needed to meet the tar- get, employing 72 types of physical mitigation measures (13). In the short term, measure selec- tion was driven by implementation plans for AB32 and other state policies (table S1). In the long term, technological progress and rates of in- troduction were constrained by physical feasi- bility, resource availability, and historical uptake rates rather than relative prices of technology, en- ergy, or carbon as in general equilibrium models (14). Technology penetration levels in our model are within the range of technological feasibility for the United States suggested by recent assess- ments (table S20) (15, 16). We did not include technologies expected to be far from commercial- ization in the next few decades, such as fusion- based electricity. Mitigation cost was calculated as the difference between total fuel and measure costs in the mitigation and baseline scenarios. Our fuel and technology cost assumptions, including learning curves (tables S4, S5, S11, and S12, and fig. S29), are comparable to those in other recent studies (17). Clearly, future costs are very uncertain over such a long time horizon, especially for technologies that are not yet commercialized. We did not assume explicit life-style changes (e.g., vegetarianism, bicycle transportation), which could have a substantial effect on mitigation requirements and costs (18); behavior change in our model is subsumed within conservation measures and en- ergy efficiency (EE). To ensure that electricity supply scenarios met the technical requirements for maintaining reli- able service, we included an electricity system dispatch algorithm that tested grid operability. Without a dispatch model, it is difficult to de- termine whether a generation mix has infeasibly high levels of intermittent generation. We devel- oped an electricity demand curve bottom-up from sectoral demand, by season and time of day. On the basis of the demand curve, the model con- strained generation scenarios to satisfy in succes- sion the energy, capacity, and system-balancing requirements for reliable operation. The operabil- ity constraint set physical limits on the penetra- tion of different types of generation and specified the requirements for peaking generation, on-grid energy storage, transmission capacity, and out-of- state imports and exports for a given generation mix (table S13 and figs.S20 to S31). It was as- sumed that over the long run, California would not “go it alone” in pursuing deep GHG reduc- tions, and thus that neighboring states would de- carbonize their generation such that the carbon intensity of imports would be comparable to that of California in-state generation (19). Electrification required to meet 80% reduc- tion target. Three major energy system transfor- mations were necessary to meet the target (Fig. 2). First, EE had to improve by at least 1.3% year −1 over 40 years. Second, electricity supply had to be nearly decarbonized, with 2050 emissions in- tensity less than 0.025 kg CO 2 e/kWh. Third, most existing direct fuel uses had to be electrified, with electricity constituting 55% of end-use energy in 2050 versus 15% today. Results for a mitigation scenario, including these and other measures, are shown in Fig. 1. Of the emissions reductions relative to 2050 baseline emissions, 28% came from EE, 27% from decarbonization of electricity generation, 14% from a combination of energy",project-academic
,2016-05-01,b,,an architecture for fast and general data processing on large clusters," Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out these computations to clusters of hundreds of machines.

At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too.

This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing.

We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective.

This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, drawing of illustrations, and links for the references have been added.",project-academic
,2016-08-13,p,,proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining," It is our great pleasure to welcome you to the 2016 ACM Conference on Knowledge Discovery and Data Mining -- KDD'16. We hope that the content and the professional network at KDD'16 will help you succeed professionally by enabling you to: identify technology trends early; make new/creative contributions; increase your productivity by using newer/better tools, processes or ways of organizing teams; identify new job opportunities; and hire new team members.

We are living in an exciting time for our profession. On the one hand, we are witnessing the industrialization of data science, and the emergence of the industrial assembly line processes characterized by the division of labor, integrated processes/pipelines of work, standards, automation, and repeatability. Data science practitioners are organizing themselves in more sophisticated ways, embedding themselves in larger teams in many industry verticals, improving their productivity substantially, and achieving a much larger scale of social impact. On the other hand we are also witnessing astonishing progress from research in algorithms and systems -- for example the field of deep neural networks has revolutionized speech recognition, NLP, computer vision, image recognition, etc. By facilitating interaction between practitioners at large companies & startups on the one hand, and the algorithm development researchers including leading academics on the other, KDD'16 fosters technological and entrepreneurial innovation in the area of data science.

This year's conference continues its tradition of being the premier forum for presentation of results in the field of data mining, both in the form of cutting edge research, and in the form of insights from the development and deployment of real world applications. Further, the conference continues with its tradition of a strong tutorial and workshop program on leading edge issues of data mining. The mission of this conference has broadened in recent years even as we placed a significant amount of focus on both the research and applied aspects of data mining. As an example of this broadened focus, this year we have introduced a strong hands-on tutorial program nduring the conference in which participants will learn how to use practical tools for data mining. KDD'16 also gives researchers and practitioners a unique opportunity to form professional networks, and to share their perspectives with others interested in the various aspects of data mining. For example, we have introduced office hours for budding entrepreneurs from our community to meet leading Venture Capitalists investing in this area. We hope that KDD 2016 conference will serve as a meeting ground for researchers, practitioners, funding agencies, and investors to help create new algorithms and commercial products.

The call for papers attracted a significant number of submissions from countries all over the world. In particular, the research track attracted 784 submissions and the applied data science track attracted 331 submissions. Papers were accepted either as full papers or as posters. The overall acceptance rate either as full papers or posters was less than 20%. For full papers in the research track, the acceptance rate was lower than 10%. This is consistent with the fact that the KDD Conference is a premier conference in data mining and the acceptance rates historically tend to be low. It is noteworthy that the applied data science track received a larger number of submissions compared to previous years. We view this as an encouraging sign that research in data mining is increasingly becoming relevant to industrial applications. All papers were reviewed by at least three program committee members and then discussed by the PC members in a discussion moderated by a meta-reviewer. Borderline papers were thoroughly reviewed by the program chairs before final decisions were made.",project-academic
10.1109/ICNP.2016.7784407,2016-11-01,p,IEEE Computer Society,characterizing industrial control system devices on the internet," Industrial control system (ICS) devices with IP addresses are accessible on the Internet and play a crucial role for critical infrastructures like power grid. However, there is a lack of deep understanding of these devices' characteristics in the cyberspace. In this paper, we take a first step in this direction by investigating these accessible industrial devices on the Internet. Because of critical nature of industrial control systems, the detection of online ICS devices should be done in a real-time and non-intrusive manner. Thus, we first analyze 17 industrial protocols widely used in industrial control systems, and train a probability model through the learning algorithm to improve detection accuracy. Then, we discover online ICS devices in the IPv4 space while reducing the noise of industrial honeypots. To observe the dynamics of ICS devices in a relatively long run, we have deployed our discovery system on Amazon EC2 and detected online ICS devices in the whole IPv4 space for eight times from August 2015 to March 2016. Based on the ICS device data collection, we conduct a comprehensive data analysis to characterize the usage of ICS devices, especially in the answer to the following three questions: (1) what are the distribution features of ICS devices, (2) who use these ICS devices, and (3) what are the functions of these ICS devices.",project-academic
10.1109/32.391379,1995-06-01,a,IEEE Press,reusing software issues and research directions," Software productivity has been steadily increasing over the past 30 years, but not enough to close the gap between the demands placed on the software industry and what the state of the practice can deliver; nothing short of an order of magnitude increase in productivity will extricate the software industry from its perennial crisis. Several decades of intensive research in software engineering and artificial intelligence left few alternatives but software reuse as the (only) realistic approach to bring about the gains of productivity and quality that the software industry needs. In this paper, we discuss the implications of reuse on the production, with an emphasis on the technical challenges. Software reuse involves building software that is reusable by design and building with reusable software. Software reuse includes reusing both the products of previous software projects and the processes deployed to produce them, leading to a wide spectrum of reuse approaches, from the building blocks (reusing products) approach, on one hand, to the generative or reusable processor (reusing processes), on the other. We discuss the implication of such approaches on the organization, control, and method of software development and discuss proposed models for their economic analysis. Software reuse benefits from methodologies and tools to: (1) build more readily reusable software and (2) locate, evaluate, and tailor reusable software, the last being critical for the building blocks approach. Both sets of issues are discussed in this paper, with a focus on application generators and OO development for the first and a thorough discussion of retrieval techniques for software components, component composition (or bottom-up design), and transformational systems for the second. We conclude by highlighting areas that, in our opinion, are worthy of further investigation. >",project-academic
10.1109/COMST.2021.3066905,2021-03-17,a,IEEE,a vision and framework for the high altitude platform station haps networks of the future," A High Altitude Platform Station (HAPS) is a network node that operates in the stratosphere at an of altitude around 20 km and is instrumental for providing communication services. Precipitated by technological innovations in the areas of autonomous avionics, array antennas, solar panel efficiency levels, and battery energy densities, and fueled by flourishing industry ecosystems, the HAPS has emerged as an indispensable component of next-generations of wireless networks. In this article, we provide a vision and framework for the HAPS networks of the future supported by a comprehensive and state-of-the-art literature review. We highlight the unrealized potential of HAPS systems and elaborate on their unique ability to serve metropolitan areas. The latest advancements and promising technologies in the HAPS energy and payload systems are discussed. The integration of the emerging Reconfigurable Smart Surface (RSS) technology in the communications payload of HAPS systems for providing a cost-effective deployment is proposed. A detailed overview of the radio resource management in HAPS systems is presented along with synergistic physical layer techniques, including Faster-Than-Nyquist (FTN) signaling. Numerous aspects of handoff management in HAPS systems are described. The notable contributions of Artificial Intelligence (AI) in HAPS, including machine learning in the design, topology management, handoff, and resource allocation aspects are emphasized. The extensive overview of the literature we provide is crucial for substantiating our vision that depicts the expected deployment opportunities and challenges in the next 10 years (next-generation networks), as well as in the subsequent 10 years (next-next-generation networks).",project-academic
10.1109/ICC.2018.8422788,2018-05-20,p,IEEE,auto scaling vnfs using machine learning to improve qos and reduce cost," Virtualization of network functions (as virtual routers, virtual firewalls, etc.) enables network owners to efficiently respond to the increasing dynamicity of network services. Virtual Network Functions (VNFs) are easy to deploy, update, monitor, and manage. The number of VNF instances, similar to generic computing resources in cloud, can be easily scaled based on load. Auto-scaling (of resources without human intervention) has been investigated in academia and industry. Prior studies on auto-scaling use measured network traffic load to dynamically react to traffic changes. In this study, we propose a proactive Machine Learning (ML) based approach to perform auto-scaling of VNFs in response to dynamic traffic changes. Our proposed ML classifier learns from past VNF scaling decisions and seasonal/spatial behavior of network traffic load to generate scaling decisions ahead of time. Compared to existing approaches for ML-based auto- scaling, our study explores how the properties (e.g., start-up time) of underlying virtualization technology impacts QoS and cost savings. We consider four different virtualization technologies: Xen and KVM, based on hypervisor virtualization, and Docker and LXC, based on container virtualization. Our results show promising accuracy of the ML classifier. We also demonstrate using realistic traffic load traces and optical backbone network that our ML method improves QoS and saves significant cost for network owners as well as leasers.",project-academic
10.1109/ICRA.2019.8793690,2019-05-20,p,IEEE,a fog robotics approach to deep robot learning application to object recognition and grasp planning in surface decluttering," The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a ‘Fog Robotics’ approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by $4\times$ to successfully declutter 86% of objects over 213 attempts.",project-academic
,2019-03-22,a,,a fog robotics approach to deep robot learning application to object recognition and grasp planning in surface decluttering," The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4\times to successfully declutter 86% of objects over 213 attempts.",project-academic
10.1186/S42400-020-00052-8,2020-06-02,a,SpringerOpen,cyber risk at the edge current and future trends on cyber risk analytics and artificial intelligence in the industrial internet of things and industry 4 0 supply chains," Digital technologies have changed the way supply chain operations are structured. In this article, we conduct systematic syntheses of literature on the impact of new technologies on supply chains and the related cyber risks. A taxonomic/cladistic approach is used for the evaluations of progress in the area of supply chain integration in the Industrial Internet of Things and Industry 4.0, with a specific focus on the mitigation of cyber risks. An analytical framework is presented, based on a critical assessment with respect to issues related to new types of cyber risk and the integration of supply chains with new technologies. This paper identifies a dynamic and self-adapting supply chain system supported with Artificial Intelligence and Machine Learning (AI/ML) and real-time intelligence for predictive cyber risk analytics. The system is integrated into a cognition engine that enables predictive cyber risk analytics with real-time intelligence from IoT networks at the edge. This enhances capacities and assist in the creation of a comprehensive understanding of the opportunities and threats that arise when edge computing nodes are deployed, and when AI/ML technologies are migrated to the periphery of IoT networks.",project-academic
10.2139/SSRN.3346528,2019-03-04,a,,cyber risk at the edge current and future trends on cyber risk analytics and artificial intelligence in the industrial internet of things and industry 4 0 supply chains," Digital technologies have changed the way supply chain operations are structured. In this article, we conduct systematic syntheses of literature on the impact of new technologies on supply chains and the related cyber risks. A taxonomic/cladistic approach is used for the evaluations of progress in supply chain integration in the Industrial Internet of Things and Industry 4.0, with a specific focus on the mitigation of cyber risks. An analytical framework is presented, based on a critical assessment with respect to issues related to new types of cyber risk and the integration of supply chains with new technologies. This paper identifies a dynamic and self-adapting supply chain system supported with Artificial Intelligence and Machine Learning (AI/ML) and real-time intelligence for predictive cyber risk analytics. The system is integrated into a cognition engine that enables predictive cyber risk analytics with real-time intelligence from IoT networks at the edge. This enhances capacities and assist in the creation of a comprehensive understanding of the opportunities and threats that arise when edge computing nodes are deployed, and when AI/ML technologies are migrated to the periphery of IoT networks.",project-academic
,2020-12-23,a,Preprints,cyber risk at the edge current and future trends on cyber risk analytics and artificial intelligence in the industrial internet of things and industry 4 0 supply chains," Digital technologies have changed the way supply chain operations are structured. In this article, we conduct systematic syntheses of literature on the impact of new technologies on supply chains and the related cyber risks. A taxonomic/cladistic approach is used for the evaluations of progress in the area of supply chain integration in the Industrial Internet of Things and Industry 4.0, with a specific focus on the mitigation of cyber risks. An analytical framework is presented, based on a critical assessment with respect to issues related to new types of cyber risk and the integration of supply chains with new technologies. This paper identifies a dynamic and self-adapting supply chain system supported with Artificial Intelligence and Machine Learning (AI/ML) and real-time intelligence for predictive cyber risk analytics. The system is integrated into a cognition engine that enables predictive cyber risk analytics with real-time intelligence from IoT networks at the edge. This enhances capacities and assist in the creation of a comprehensive understanding of the opportunities and threats that arise when edge computing nodes are deployed, and when AI/ML technologies are migrated to the periphery of IoT networks.",project-academic
10.1109/ISGT.2016.7781159,2016-09-01,p,IEEE,large scale detection of non technical losses in imbalanced data sets," Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",project-academic
,2016-02-26,a,,large scale detection of non technical losses in imbalanced data sets," Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",project-academic
10.1016/J.COMPELECENG.2012.05.013,2012-09-01,a,COMPUTERS AND ELECTRICAL ENGINEERING,automatic network intrusion detection current techniques and open issues," Automatic network intrusion detection has been an important research topic for the last 20years. In that time, approaches based on signatures describing intrusive behavior have become the de-facto industry standard. Alternatively, other novel techniques have been used for improving automation of the intrusion detection process. In this regard, statistical methods, machine learning and data mining techniques have been proposed arguing higher automation capabilities than signature-based approaches. However, the majority of these novel techniques have never been deployed on real-life scenarios. The fact is that signature-based still is the most widely used strategy for automatic intrusion detection. In the present article we survey the most relevant works in the field of automatic network intrusion detection. In contrast to previous surveys, our analysis considers several features required for truly deploying each one of the reviewed approaches. This wider perspective can help us to identify the possible causes behind the lack of acceptance of novel techniques by network security experts.",project-academic
10.1109/MCOM.2018.1700632,2018-02-13,a,IEEE,big data analytics in industrial iot using a concentric computing model," The unprecedented proliferation of miniaturized sensors and intelligent communication, computing, and control technologies have paved the way for the development of the Industrial Internet of Things. The IIoT incorporates machine learning and massively parallel distributed systems such as clouds, clusters, and grids for big data storage, processing, and analytics. In IIoT, end devices continuously generate and transmit data streams, resulting in increased network traffic between device-cloud communication. Moreover, it increases in-network data transmissions. requiring additional efforts for big data processing, management, and analytics. To cope with these engendered issues, this article first introduces a novel concentric computing model (CCM) paradigm composed of sensing systems, outer and inner gateway processors, and central processors (outer and inner) for the deployment of big data analytics applications in IIoT. Second, we investigate, highlight, and report recent research efforts directed at the IIoT paradigm with respect to big data analytics. Third, we identify and discuss indispensable challenges that remain to be addressed for employing CCM in the IIoT paradigm. Lastly, we provide several future research directions (e.g., real-time data analytics, data integration, transmission of meaningful data, edge analytics, real-time fusion of streaming data, and security and privacy).",project-academic
10.1145/3269206.3271748,2018-10-17,p,ACM,budget constrained bidding by model free reinforcement learning in display advertising," Real-time bidding (RTB) is an important mechanism in online display advertising, where a proper bid for each page view plays an essential role for good marketing results. Budget constrained bidding is a typical scenario in RTB where the advertisers hope to maximize the total value of the winning impressions under a pre-set budget constraint. However, the optimal bidding strategy is hard to be derived due to the complexity and volatility of the auction environment. To address these challenges, in this paper, we formulate budget constrained bidding as a Markov Decision Process and propose a model-free reinforcement learning framework to resolve the optimization problem. Our analysis shows that the immediate reward from environment is misleading under a critical resource constraint. Therefore, we innovate a reward function design methodology for the reinforcement learning problems with constraints. Based on the new reward design, we employ a deep neural network to learn the appropriate reward so that the optimal policy can be learned effectively. Different from the prior model-based work, which suffers from the scalability problem, our framework is easy to be deployed in large-scale industrial applications. The experimental evaluations demonstrate the effectiveness of our framework on large-scale real datasets.",project-academic
10.1007/S10664-017-9547-8,2018-06-01,a,Springer US,inference of development activities from interaction with uninstrumented applications," Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.",project-academic
10.1016/J.ENBUILD.2019.07.029,2019-09-15,a,Elsevier,whole building energy model for hvac optimal control a practical framework based on deep reinforcement learning," Abstract None None Whole building energy model (BEM) is a physics-based modeling method for building energy simulation. It has been widely used in the building industry for code compliance, building design optimization, retrofit analysis, and other uses. Recent research also indicates its strong potential for the control of heating, ventilation and air-conditioning (HVAC) systems. However, its high-order nature and slow computational speed limit its practical application in real-time HVAC optimal control. Therefore, this study proposes a practical control framework (named BEM-DRL) that is based on deep reinforcement learning. The framework is implemented and assessed in a novel radiant heating system in an existing office building as a case study. The complete implementation process is presented in this study, including: building energy modeling for the novel heating system, multi-objective BEM calibration using the Bayesian method and the Genetic Algorithm, deep reinforcement learning training and simulation results evaluation, and control deployment. By analyzing the real-life control deployment data, it is found that BEM-DRL achieves 16.7% heating demand reduction with more than 95% probability compared to the old rule-based control. However, the framework still faces the practical challenges including building energy modeling of novel HVAC systems and multi-objective model calibration. Systematic study is also needed for the design of deep reinforcement learning training to provide a guideline for practitioners.",project-academic
10.1109/BIGDATA.2016.7841045,2016-12-01,p,hgpu.org,deep learning in the automotive industry applications and tools," Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-the-art in libraries, tools and infrastructures (e. g. GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.",project-academic
10.1145/3292500.3330666,2019-07-25,p,ACM,practice on long sequential user behavior modeling for click through rate prediction," Click-through rate (CTR) prediction is critical for industrial applications such as recommender system and online advertising. Practically, it plays an important role for CTR modeling in these applications by mining user interest from rich historical behavior data. Driven by the development of deep learning, deep CTR models with ingeniously designed architecture for user interest modeling have been proposed, bringing remarkable improvement of model performance over offline metric. However, great efforts are needed to deploy these complex models to online serving system for realtime inference, facing massive traffic request. Things turn to be more difficult when it comes to long sequential user behavior data, as the system latency and storage cost increase approximately linearly with the length of user behavior sequence. In this paper, we face directly the challenge of long sequential user behavior modeling and introduce our hands-on practice with the co-design of machine learning algorithm and online serving system for CTR prediction task. (i) From serving system view, we decouple the most resource-consuming part of user interest modeling from the entire model by designing a separate module named UIC (User Interest Center). UIC maintains the latest interest state for each user, whose update depends on realtime user behavior trigger event, rather than on traffic request. Hence UIC is latency free for realtime CTR prediction. (ii) From machine learning algorithm view, we propose a novel memory-based architecture named MIMN (Multi-channel user Interest Memory Network) to capture user interests from long sequential behavior data, achieving superior performance over state-of-the-art models. MIMN is implemented in an incremental manner with UIC module. Theoretically, the co-design solution of UIC and MIMN enables us to handle the user interest modeling with unlimited length of sequential behavior data. Comparison between model performance and system efficiency proves the effectiveness of proposed solution. To our knowledge, this is one of the first industrial solutions that are capable of handling long sequential user behavior data with length scaling up to thousands. It now has been deployed in the display advertising system in Alibaba.",project-academic
10.3390/S19092047,2019-05-02,a,Multidisciplinary Digital Publishing Institute,design and implementation of cloud analytics assisted smart power meters considering advanced artificial intelligence as edge analytics in demand side management for smart homes," In a smart home linked to a smart grid (SG), demand-side management (DSM) has the potential to reduce electricity costs and carbon/chlorofluorocarbon emissions, which are associated with electricity used in today’s modern society. To meet continuously increasing electrical energy demands requested from downstream sectors in an SG, energy management systems (EMS), developed with paradigms of artificial intelligence (AI) across Internet of things (IoT) and conducted in fields of interest, monitor, manage, and analyze industrial, commercial, and residential electrical appliances efficiently in response to demand response (DR) signals as DSM. Usually, a DSM service provided by utilities for consumers in an SG is based on cloud-centered data science analytics. However, such cloud-centered data science analytics service involved for DSM is mostly far away from on-site IoT end devices, such as DR switches/power meters/smart meters, which is usually unacceptable for latency-sensitive user-centric IoT applications in DSM. This implies that, for instance, IoT end devices deployed on-site for latency-sensitive user-centric IoT applications in DSM should be aware of immediately analytical, interpretable, and real-time actionable data insights processed on and identified by IoT end devices at IoT sources. Therefore, this work designs and implements a smart edge analytics-empowered power meter prototype considering advanced AI in DSM for smart homes. The prototype in this work works in a cloud analytics-assisted electrical EMS architecture, which is designed and implemented as edge analytics in the architecture described and developed toward a next-generation smart sensing infrastructure for smart homes. Two different types of AI deployed on-site on the prototype are conducted for DSM and compared in this work. The experimentation reported in this work shows the architecture described with the prototype in this work is feasible and workable.",project-academic
10.1145/3213344.3213345,2018-06-10,p,ACM,edgeeye an edge service framework for real time intelligent video analytics," Deep learning with Deep Neural Networks (DNNs) can achieve much higher accuracy on many computer vision tasks than classic machine learning algorithms. Because of the high demand for both computation and storage resources, DNNs are often deployed in the cloud. Unfortunately, executing deep learning inference in the cloud, especially for real-time video analysis, often incurs high bandwidth consumption, high latency, reliability issues, and privacy concerns. Moving the DNNs close to the data source with an edge computing paradigm is a good approach to address those problems. The lack of an open source framework with a high-level API also complicates the deployment of deep learning-enabled service at the Internet edge. This paper presents EdgeEye, an edge-computing framework for real-time intelligent video analytics applications. EdgeEye provides a high-level, task-specific API for developers so that they can focus solely on application logic. EdgeEye does so by enabling developers to transform models trained with popular deep learning frameworks to deployable components with minimal effort. It leverages the optimized inference engines from industry to achieve the optimized inference performance and efficiency.",project-academic
10.5334/DSJ-2019-032,2019-07-08,a,Ubiquity Press,an automated machine learning based decision support system to predict hotel booking cancellations," Booking cancellations negatively contribute to the production of accurate forecasts, which comprise a critical tool in the hospitality industry. Research has shown that with today’s computational power and advanced machine learning algorithms it is possible to build models to predict bookings cancellation likelihood. However, the effectiveness of these models has never been evaluated in a real environment. To fill this gap and investigate how these models can be implemented in a decision support system and its impact on demand-management decisions, a prototype was built and deployed in two hotels. The prototype, based on an automated machine learning system designed to learn continuously, lead to two important research contributions. First, the development of a training method and weighting mechanism designed to capture changes in cancellations patterns over time and learn from previous days’ predictions hits and errors. Second, the creation of a new measure – Minimum Frequency – to measure the precision of predictions over time. From a business standpoint, the prototype demonstrated its effectiveness, with results exceeding 84% in accuracy, 82% in precision, and 88% in Area Under the Curve (AUC). The system allowed hotels to predict their net demand and thus making better decisions about which bookings to accept and reject, what prices to make, and how many rooms to oversell. The systematic prediction of bookings with high probability of being canceled allowed hotels to reduce cancellations by 37 percentage points by acting to avoid their cancellation.",project-academic
10.1109/IECON.2013.6699377,2013-01-01,p,IEEE,fuel cells prognostics using echo state network," One remaining technological bottleneck to develop industrial Fuel Cell (FC) applications resides in the system limited useful lifetime. Consequently, it is important to develop failure diagnostic and prognostic tools enabling the optimization of the FC. Among all the existing prognostics approaches, datamining methods such as artificial neural networks aim at estimating the process' behavior without huge knowledge about the underlying physical phenomena. Nevertheless, this kind of approach needs huge learning dataset. Also, the deployment of such an approach can be long (trial and error method), which represents a real problem for industrial applications where real-time complying algorithms must be developed. According to this, the aim of this paper is to study the application of a reservoir computing tool (the Echo State Network) as a prognostics system enabling the estimation of the Remaining Useful Life of a Proton Exchange Membrane Fuel Cell. Developments emphasize on the prediction of the mean voltage cells of a degrading FC. Accuracy and time consumption of the approach are studied, as well as sensitivity of several parameters of the ESN. Results appear to be very promising.",project-academic
,2010-06-14,b,,fundamentals of predictive text mining," One consequence of the pervasive use of computers is that most documents originate in digital form. Widespread use of the Internet makes them readily available. Text mining the process of analyzing unstructured natural-language text is concerned with how to extract information from these documents. Developed from the authors highly successful Springer reference on text mining, Fundamentals of Predictive Text Mining is an introductory textbook and guide to this rapidly evolving field. Integrating topics spanning the varied disciplines of data mining, machine learning, databases, and computational linguistics, this uniquely useful book also provides practical advice for text mining. In-depth discussions are presented on issues of document classification, information retrieval, clustering and organizing documents, information extraction, web-based data-sourcing, and prediction and evaluation. Background on data mining is beneficial, but not essential. Where advanced concepts are discussed that require mathematical maturity for a proper understanding, intuitive explanations are also provided for less advanced readers. Topics and features: presents a comprehensive, practical and easy-to-read introduction to text mining; includes chapter summaries, useful historical and bibliographic remarks, and classroom-tested exercises for each chapter; explores the application and utility of each method, as well as the optimum techniques for specific scenarios; provides several descriptive case studies that take readers from problem description to systems deployment in the real world; includes access to industrial-strength text-mining software that runs on any computer; describes methods that rely on basic statistical techniques, thus allowing for relevance to all languages (not just English); contains links to free downloadable software and other supplementary instruction material. Fundamentals of Predictive Text Mining is an essential resource for IT professionals and managers, as well as a key text for advanced undergraduate computer science students and beginning graduate students. Dr. Sholom M. Weiss is a Research Staff Member with the IBM Predictive Modeling group, in Yorktown Heights, New York, and Professor Emeritus of Computer Science at Rutgers University. Dr. Nitin Indurkhya is Professor at the School of Computer Science and Engineering, University of New South Wales, Australia, as well as founder and president of data-mining consulting company Data-Miner Pty Ltd. Dr. Tong Zhang is Associate Professor at the Department of Statistics and Biostatistics at Rutgers University, New Jersey.",project-academic
10.1109/ASE.2019.00080,2019-11-10,p,Institute of Electrical and Electronics Engineers Inc.,an empirical study towards characterizing deep learning development and deployment across different frameworks and platforms," Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.",project-academic
,2019-09-15,a,,an empirical study towards characterizing deep learning development and deployment across different frameworks and platforms," Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions.",project-academic
10.1109/LRA.2020.2969927,2018-11-17,a,,augmented lidar simulator for autonomous driving," In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this paper, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., cars, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG models and game engines, our augmented simulator bypasses the requirement to create high-fidelity background CAD models. Instead, we can simply deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background point cloud, based on which annotated point cloud can be automatically generated. This unique ""scan-and-simulate"" capability makes our approach scalable and practical, ready for large-scale industrial applications. In this paper, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95% accuracy.",project-academic
10.1145/3394486.3403288,2020-08-23,p,ACM,neural input search for large scale recommendation models," Recommendation problems with large numbers of discrete items, such as products, webpages, or videos, are ubiquitous in the technology industry. Deep neural networks are being increasingly used for these recommendation problems. These models use embeddings to represent discrete items as continuous vectors, and the vocabulary sizes and embedding dimensions, despite their heavy influence on the model's accuracy, are often manually selected in a heuristical manner. None We present Neural Input Search (NIS), a technique for learning the optimal vocabulary sizes and embedding dimensions for categorical features. The goal is to maximize prediction accuracy subject to a constraint on the total memory used by all embeddings. Moreover, we argue that the traditional Single-size Embedding (SE), which uses the same embedding dimension for all values of a feature, suffers from inefficient usage of model capacity and training data. We propose a novel type of embedding, namely Multi-size Embedding (ME), which allows the embedding dimension to vary for different values of the feature. During training we use reinforcement learning to find the optimal vocabulary size for each feature and embedding dimension for each value of the feature. Experimentation on two public recommendation datasets shows that NIS can find significantly better models with much fewer embedding parameters. We also deployed NIS in production to a real world large scale App ranking model in our company's App store, Google Play, resulting in +1.02% App Install with 30% smaller model size.",project-academic
10.22541/AU.159986472.28149413,2020-09-11,a,Authorea,a genome scale metabolic network model and machine learning predict amino acid concentrations in chinese hamster ovary cell cultures," The control of nutrient availability is critical to large-scale manufacturing of biotherapeutics. However, the quantification of proteinogenic amino acids is time-consuming and thus is difficult to implement for real-time in situ bioprocess control. Genome-scale metabolic models describe the metabolic conversion from media nutrients to proliferation and recombinant protein production, and therefore are a promising platform for in silico monitoring and prediction of amino acid concentrations. This potential has not been realized due to unresolved challenges: (1) the models assume an optimal and highly efficient metabolism, and therefore tend to underestimate amino acid consumption, and (2) the models assume a steady state, and therefore have a short forecast range. We address these challenges by integrating machine learning with the metabolic models. Through this we demonstrate accurate and time-course dependent prediction of individual amino acid concentration in culture medium throughout the production process. Thus, these models can be deployed to control nutrient feeding to avoid premature nutrient depletion or provide early predictions of failed bioreactor runs.",project-academic
10.1002/BIT.27714,2021-02-19,a,"John Wiley & Sons, Ltd",a genome scale metabolic network model and machine learning predict amino acid concentrations in chinese hamster ovary cell cultures," The control of nutrient availability is critical to large-scale manufacturing of biotherapeutics. However, the quantification of proteinogenic amino acids is time-consuming and thus is difficult to implement for real-time in situ bioprocess control. Genome-scale metabolic models describe the metabolic conversion from media nutrients to proliferation and recombinant protein production, and therefore are a promising platform for in silico monitoring and prediction of amino acid concentrations. This potential has not been realized due to unresolved challenges: (1) the models assume an optimal and highly efficient metabolism, and therefore tend to underestimate amino acid consumption, and (2) the models assume a steady state, and therefore have a short forecast range. We address these challenges by integrating machine learning with the metabolic models. Through this we demonstrate accurate and time-course dependent prediction of individual amino acid concentration in culture medium throughout the production process. Thus, these models can be deployed to control nutrient feeding to avoid premature nutrient depletion or provide early predictions of failed bioreactor runs.",project-academic
10.1101/2020.09.02.279687,2020-12-01,a,Cold Spring Harbor Laboratory,a genome scale metabolic network model and machine learning predict amino acid concentrations in chinese hamster ovary cell cultures," Abstract None The control of nutrient availability is critical to large-scale manufacturing of biotherapeutics. However, the quantification of proteinogenic amino acids is time-consuming and thus is difficult to implement for real-time in situ bioprocess control. Genome-scale metabolic models describe the metabolic conversion from media nutrients to proliferation and recombinant protein production, and therefore are a promising platform for in silico monitoring and prediction of amino acid concentrations. This potential has not been realized due to unresolved challenges: (1) the models assume an optimal and highly efficient metabolism, and therefore tend to underestimate amino acid consumption, and (2) the models assume a steady state, and therefore have a short forecast range. We address these challenges by integrating machine learning with the metabolic models. Through this we demonstrate accurate and time-course dependent prediction of individual amino acid concentration in culture medium throughout the production process. Thus, these models can be deployed to control nutrient feeding to avoid premature nutrient depletion or provide early predictions of failed bioreactor runs.",project-academic
10.1109/ICCC.2018.00010,2018-07-02,p,IEEE,an edge based smart parking solution using camera networks and deep learning," The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systems—including license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.",project-academic
,2018-07-19,a,,improving simple models with confidence profiles," In this paper, we propose a new method called ProfWeight for transferring information from a pre-trained deep neural network that has a high test accuracy to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. We are motivated by applications in interpretability and model deployment in severely memory constrained environments (like sensors). Our method uses linear probes to generate confidence scores through flattened intermediate representations. Our transfer method involves a theoretically justified weighting of samples during the training of the simple model using confidence scores of these intermediate layers. The value of our method is first demonstrated on CIFAR-10, where our weighting method significantly improves (3-4%) networks with only a fraction of the number of Resnet blocks of a complex Resnet model. We further demonstrate operationally significant results on a real manufacturing problem, where we dramatically increase the test accuracy of a CART model (the domain standard) by roughly 13%.",project-academic
10.1016/J.MICPRO.2020.103301,2021-02-01,a,Elsevier,iot enabled cancer prediction system to enhance the authentication and security using cloud computing," Abstract None None In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results.",project-academic
,2020-04-21,a,,industrial robot grasping with deep learning using a programmable logic controller plc," Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95 percent with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world s largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",project-academic
10.1109/CASE48305.2020.9216902,2020-08-01,p,IEEE,industrial robot grasping with deep learning using a programmable logic controller plc," Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world’s largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",project-academic
10.3390/MACHINES6030038,2018-09-01,a,Multidisciplinary Digital Publishing Institute,machine learning applications on agricultural datasets for smart farm enhancement," This work aims to show how to manage heterogeneous information and data coming from real datasets that collect physical, biological, and sensory values. As productive companies—public or private, large or small—need increasing profitability with costs reduction, discovering appropriate ways to exploit data that are continuously recorded and made available can be the right choice to achieve these goals. The agricultural field is only apparently refractory to the digital technology and the “smart farm” model is increasingly widespread by exploiting the Internet of Things (IoT) paradigm applied to environmental and historical information through time-series. The focus of this study is the design and deployment of practical tasks, ranging from crop harvest forecasting to missing or wrong sensors data reconstruction, exploiting and comparing various machine learning techniques to suggest toward which direction to employ efforts and investments. The results show how there are ample margins for innovation while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agriculture industrial business, investing not only in technology, but also in the knowledge and in skilled workforce required to take the best out of it.",project-academic
10.1016/J.JPOWSOUR.2016.05.092,2016-08-30,a,Elsevier,prognostics of proton exchange membrane fuel cells stack using an ensemble of constraints based connectionist networks," Proton Exchange Membrane Fuel Cell (PEMFC) is considered the most versatile among available fuel cell technologies, which qualify for diverse applications. However, the large-scale industrial deployment of PEMFCs is limited due to their short life span and high exploitation costs. Therefore, ensuring fuel cell service for a long duration is of vital importance, which has led to Prognostics and Health Management of fuel cells. More precisely, prognostics of PEMFC is major area of focus nowadays, which aims at identifying degradation of PEMFC stack at early stages and estimating its Remaining Useful Life (RUL) for life cycle management. This paper presents a data-driven approach for prognostics of PEMFC stack using an ensemble of constraint based Summation Wavelet- Extreme Learning Machine (SW-ELM) models. This development aim at improving the robustness and applicability of prognostics of PEMFC for an online application, with limited learning data. The proposed approach is applied to real data from two different PEMFC stacks and compared with ensembles of well known connectionist algorithms. The results comparison on long-term prognostics of both PEMFC stacks validates our proposition.",project-academic
10.1109/ICIT.2015.7125235,2015-03-17,p,IEEE,improving accuracy of long term prognostics of pemfc stack to estimate remaining useful life," Proton Exchange Membrane Fuel cells (PEMFC) are energy systems that facilitate electrochemical reactions to create electrical energy from chemical energy of hydrogen. PEMFC are promising source of renewable energy that can operate on low temperature and have the advantages of high power density and low pollutant emissions. However, PEMFC technology is still in the developing phase, and its large-scale industrial deployment requires increasing the life span of fuel cells and decreasing their exploitation costs. In this context, Prognostics and Health Management of fuel cells is an emerging field, which aims at identifying degradation at early stages and estimating the Remaining Useful Life (RUL) for life cycle management. Indeed, due to prognostics capability, the accurate estimates of RUL enables safe operation of the equipment and timely decisions to prolong its life span. This paper contributes data-driven prognostics of PEMFC by an ensemble of constraint based Summation Wavelet-Extreme Learning Machine (SW-ELM) algorithm to improve accuracy and robustness of long-term prognostics. The SW-ELM is used for ensemble modeling due to its enhanced applicability for real applications as compared to conventional data-driven algorithms. The proposed prognostics model is validated on run-to-failure data of PEMFC stack, which had the life span of 1750 hours. The results confirm capability of the prognostics model to achieve accurate RUL estimates.",project-academic
,2002-05-01,b,Prentice Hall PTR,multimedia communication systems techniques standards and networks," From the Book:
Preface
The past years have seen an explosion in the use of digital media. Industry is making significant investments to deliver digital audio, image and video information to consumers and customers. A new infrastructure of digital audio, image and video recorders and players; online services and electronic commerce is rapidly being deployed. At the same time, major corporations are converting their audio, image and video archives to an electronic form. Digital media offer several distinct advantages over analog media. The quality of digital audio, image and video signals is higher than that of their analog counterparts. Editing is easy because one can access the exact discrete locations that need to be changed. Copying is simple with no loss of fidelity. A copy of digital media is identical to the original. Digital audio, image and video are easily transmitted across networked information systems. These advantages have opened up many new possibilities.

Multimedia consists of Multimedia data + Set of interactions. Multimedia data is informally considered as the collection of three Ms: multisource, multitype and multiformat data. The interactions among the multimedia components consist of complex relationships without which multimedia would be a simple set of visual, audio and other data.

Multimedia and multimedia communication can be globally viewed as a hierarchical system. The multimedia software and applications provide a direct interactive environment for users. When a computer requires information from remote computers or servers, multimedia information must travel through computer networks. Because the amount of information involved inthe transmission of video and audio can be substantial, the multimedia information must be compressed before it can be sent through the network in order to reduce the communication delay. Constraints, such as limited delay and jitter, are used to ensure a reasonable video and audio effect at the receiving end. Therefore, communication networks are undergoing constant improvements in order to provide for multimedia communication capabilities. LANs are used to connect local computers and other equipment, and Wide Area Networks (WANs) and the Internet connect the LANs together. Better standards are constantly being developed, in order to provide a global information superhighway across which multimedia information will travel.
Organization of the Book
The book is organized into six chapters:

Chapter 1 describes the concept of multimedia communication modeling. It presents a brief description of elements for multimedia systems. After that, we discuss user and network requirements together with the packet transfer concept. An overview of multimedia terminals is also given.

Chapter 2 explains that multimedia communication is more than simply putting together text, audio, images and video. It reviews a recent trend in multimedia research to exploit the audio-visual interaction and to build the link between audio and video processing. The emphasis is on lip reading, synchronization and tracing audio-to-visual mapping as well as the bimodal person verification.

Chapter 3 is devoted to multimedia processing in communication. We present and analyze digital media and signal processing elements. Next, we describe a general framework for image copyright protection through digital watermarking. We then review the key attributes of neural processing essential to intelligent multimedia processing. Finally, this chapter concludes with recent large-scale-integration programmable processors designed for multimedia processing such as real-time compression and decompression of audio and video as well as the next generation of computer graphics.

Chapter 4 deals with the issues concerning distributed multimedia systems. We give an overview: main features, resource management, networking and multimedia operating systems. Next, we identify the applications like interactive television, telecooperation and hypermedia, and we survey the important enabling technologies.

Chapter 5 focuses on multimedia communication standards. We discuss Moving Pictures Experts Group (MPEG)-1, MPEG-2, MPEG-4, MPEG-4 Visual Texture Coding (VTC), Joint Photographic Experts Group (JPEG)-2000, MPEG-7, MPEG-21, International Telecommunications Union-Telecommunication Sector (ITU-T) and Internet standards. We discuss the ITU-T standardization process in multimedia communications from the video and speech coding, as well as from multimedia, multiplex and synchronization points of view (H.320, H.321, H.322, H.323, H.262, H.263, H.26L, H.221, H.222, H.223 and H.225).

Chapter 6 concentrates on multimedia communication across networks. After an introduction about packet audio-video in the network environment, we discuss the concept of video transport across generic networks. Multimedia transport over ATM networks is described, too. We then move to multimedia across IP networks, including video transmission, traffic specification for MPEG video transmission on the Internet and bandwidth allocation mechanism. We present and illustrate the concepts of Internet access networks. In addition, we discuss special issues relating to multimedia across wireless networks such as wireless broadband communication for multimedia audiovisual solutions, mobile and broadcasting networks and digital TV infrastructure for interactive multimedia services.
Appendix/Web Site
Appendix A contains useful information available on the Internet: standardization organizations, associations, alliances, fora and consortia; documents, software and hardware reference, and a products and services list. No software is provided. The appendix can be downloaded at the following Web site: www.phptr.com/rao
References
The references are grouped according to the various chapters. Special efforts have been taken to make this list as up to date and exhaustive as possible.

A number of forces are driving communications, such as the following: 

The evolution of communications and data networks in today's modern Plain Old Telephone Service (POTS) network and packet (including the Internet) networks, with major forces driving these networks into an integrated structure
The increasing availability of almost unlimited bandwidth demand in the office, the home and eventually on the road, based on high-speed data modems, cable modems, hybrid fiber-mix systems, and, recently, a number of fixed wireless access systems
The availability of ubiquitous access to the network through Local Area Networks (LANs), wireline and wireless networks providing the promise of anywhere, anytime access
The ever-increasing amount of memory and computation brought to bear on virtually any communications or computing system
The terminals, including sophisticated screen phones; digital telephones; multimedia personal computers (PCs) that handle a wide range of text, image, audio and video signals; network computers and other low-cost Internet-access terminals and Personal Digital Assistants (PDAs) of all types that can access and interact with the network using wired and wireless connections
The digitalization of virtually all devices, including cameras, video capture devices, video playback devices, handwriting terminals, sound capture devices and so forth


Multimedia Communication Systems provides a comprehensive coverage of various surveys of the current issues relating to multimedia communications. This book addresses the fundamentals of the major topics of the multimedia communication systems: audio-visual integration, multimedia processing in communications, distributed multimedia systems, multimedia communication standards and multimedia communications across networks.

We have focused our attention on these topics with the hope that the level of discussion provided will enable an engineer or a scientist to design multimedia communication systems or to conduct research on advanced and newly emerging topics. The objective of this book is not only to familiarize the reader with multimedia communication systems, but also to provide the underlying theory, concepts and principles related to these disciplines, including the power and the practical utility of the topics.

A major challenge during the preparation of this book was the rapid pace of development, both in software and hardware related to multimedia communication systems. We have tried to keep pace by including many of the latest developments. In this way, it is hoped that the book is timely and appeals to a wide audience in the engineering, scientific and technical communities. In addition, we have included more than 270 figures and more than 800 references. Although this book is primarily for graduate students, it can be also very useful for academia, researchers, scientists and engineers dealing with multimedia communication systems.",project-academic
10.20944/PREPRINTS202103.0135.V1,2021-03-03,a,Preprints,towards crisp ml q a machine learning process model with quality assurance methodology," Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.",project-academic
10.3390/MAKE3020020,2021-04-22,a,MDPI AG,towards crisp ml q a machine learning process model with quality assurance methodology," Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.",project-academic
,2017-06-21,a,,deep interest network for click through rate prediction," Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",project-academic
10.1109/CNS.2019.8802785,2019-06-10,p,IEEE,deep learning based network intrusion detection for scada systems," Supervisory Control and Data Acquisition (SCADA)networks are widely deployed in modern industrial control systems (ICSs)such as energy-delivery systems. As an increasing number of field devices and computing nodes get interconnected, network-based cyber attacks have become major cyber threats to ICS network infrastructure. Field devices and computing nodes in ICSs are subjected to both conventional network attacks and specialized attacks purposely crafted for SCADA network protocols. In this paper, we propose a deep-learning-based network intrusion detection system for SCADA networks to protect ICSs from both conventional and SCADA specific network-based attacks. Instead of relying on hand-crafted features for individual network packets or flows, our proposed approach employs a convolutional neural network (CNN)to characterize salient temporal patterns of SCADA traffic and identify time windows where network attacks are present. In addition, we design a re-training scheme to handle previously unseen network attack instances, enabling SCADA system operators to extend our neural network models with site-specific network attack traces. Our results using realistic SCADA traffic data sets show that the proposed deep-learning-based approach is well-suited for network intrusion detection in SCADA systems, achieving high detection accuracy and providing the capability to handle newly emerged threats.",project-academic
10.17863/CAM.51696,2021-07-28,a,,multi agent system for machine learning under uncertainty in cyber physical manufacturing system," Recent advancements in predictive machine learning has led to its application in various use cases in manufacturing. Most research focused on maximising predictive accuracy without addressing the uncertainty associated with it. While accuracy is important, focusing primarily on it poses an overfitting danger, exposing manufacturers to risk, ultimately hindering the adoption of these techniques. In this paper, we determine the sources of uncertainty in machine learning and establish the success criteria of a machine learning system to function well under uncertainty in a cyber-physical manufacturing system (CPMS) scenario. Then, we propose a multi-agent system architecture which leverages probabilistic machine learning as a means of achieving such criteria. We propose possible scenarios for which our proposed architecture is useful and discuss future work. Experimentally, we implement Bayesian Neural Networks for multi-tasks classification on a public dataset for the real-time condition monitoring of a hydraulic system and demonstrate the usefulness of the system by evaluating the probability of a prediction being accurate given its uncertainty. We deploy these models using our proposed agent-based framework and integrate web visualisation to demonstrate its real-time feasibility.",project-academic
10.1007/978-3-030-27477-1_19,2019-10-03,a,"Springer, Cham",multi agent system for machine learning under uncertainty in cyber physical manufacturing system," Recent advancement in predictive machine learning has led to its application in various use cases in manufacturing. Most research focused on maximising predictive accuracy without addressing the uncertainty associated with it. While accuracy is important, focusing primarily on it poses an overfitting danger, exposing manufacturers to risk, ultimately hindering the adoption of these techniques. In this paper, we determine the sources of uncertainty in machine learning and establish the success criteria of a machine learning system to function well under uncertainty in a cyber-physical manufacturing system (CPMS) scenario. Then, we propose a multi-agent system architecture which leverages probabilistic machine learning as a means of achieving such criteria. We propose possible scenarios for which our architecture is useful and discuss future work. Experimentally, we implement Bayesian Neural Networks for multi-tasks classification on a public dataset for the real-time condition monitoring of a hydraulic system and demonstrate the usefulness of the system by evaluating the probability of a prediction being accurate given its uncertainty. We deploy these models using our proposed agent-based framework and integrate web visualisation to demonstrate its real-time feasibility.",project-academic
10.1016/J.ADHOC.2019.102047,2020-03-01,p,Elsevier,an intelligent edge iot platform for monitoring livestock and crops in a dairy farming scenario," Abstract None None Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud.",project-academic
,2020-01-28,a,,online lidar slam for legged robots with robust registration and deep learned loop closure," In this paper, we present a factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. These facilities can be badly lit and comprised of indistinct metallic structures, thus our system uses only LiDAR sensing and was developed to run on the quadruped robot's navigation PC. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",project-academic
10.1109/ICRA40945.2020.9196769,2020-05-01,p,,online lidar slam for legged robots with robust registration and deep learned loop closure," In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",project-academic
10.1016/J.ESWA.2020.113405,2020-10-01,a,Pergamon,a data analytic framework for physical fatigue management using wearable sensors," Abstract None None The use of expert systems in optimizing and transforming human performance has been limited in practice due to the lack of understanding of how an individual’s performance deteriorates with fatigue accumulation, which can vary based on both the worker and the workplace conditions. As a first step toward realizing the human-centered approach to artificial intelligence and expert systems, this paper lays the foundation for a data analytic approach to managing fatigue in physically-demanding workplaces. The proposed framework capitalizes on continuously collected human performance data from wearable sensor technologies, and is centered around four distinct phases of fatigue: (a) detection, where machine learning methodologies are deployed to detect the occurrence of fatigue; (b) identification, where key features relating to the fatigue occurrence is to be identified; (c) diagnosis, where the fatigue mode is identified based on the knowledge generated in the previous two phases; and (d) recovery, where a suitable intervention is applied to return the worker to mitigate the detrimental effects of fatigue on the worker. Moreover, the framework establishes criteria for feature and machine learning algorithm selection for fatigue management. Two specific application cases of the framework, for two types of manufacturing-related tasks, are presented. Based on the proposed framework and a large number of test sets used in the two case studies, we have shown that: (i) only one wearable sensor is needed for fatigue detection with an average accuracy of  ≥ 0.850 and a random forest model comprised of",project-academic
10.1109/TII.2010.2060732,2010-08-19,a,IEEE,an industrial strength novelty detection framework for autonomous equipment monitoring and diagnostics," This paper presents a practical framework for autonomous monitoring of industrial equipment based on novelty detection. It overcomes limitations of current equipment monitoring technology by developing a “generic” structure that is relatively independent of the type of physical equipment under consideration. The kernel of the proposed approach is an “evolving” model based on unsupervised learning methods (reducing the need for human intervention). The framework employs procedures designed to temporally evolve the critical model parameters with experience for enhanced monitoring accuracy (a critical ability for mass deployment of the technology on a variety of equipment/hardware without needing extensive initial tune-up). Proposed approach makes explicit provision to characterize the distinct operating modes of the equipment, when necessary, and provides the ability to predict both abrupt as well as gradually developing (incipient) changes. The framework is realized as an autonomous software agent that continuously updates its decision model implementing an unsupervised recursive learning algorithm. Results of validation of the proposed methodology by accelerated testing experiments are also discussed.",project-academic
10.1007/978-981-10-5026-8_8,2017-01-01,p,"Springer, Singapore",dynamic selection of virtual machines for application servers in cloud environments," Autoscaling is a hallmark of cloud computing as it allows flexible just-in-time allocation and release of computational resources in response to dynamic and often unpredictable workloads. This is especially important for web applications, whose workload is time dependent and prone to flash crowds. Most of them follow the 3-tier architectural pattern, and are divided into presentation, application/domain and data layers. In this work, we focus on the application layer. Reactive autoscaling policies of the type “Instantiate a new Virtual Machine (VM) when the average server CPU utilisation reaches X%” have been used successfully since the dawn of cloud computing. But which VM type is the most suitable for the specific application at the moment remains an open question. In this work, we propose an approach for dynamic VM type selection. It uses a combination of online machine learning techniques, works in real time and adapts to changes in the users’ workload patterns, application changes as well as middleware upgrades and reconfigurations. We have developed a prototype, which we tested with the CloudStone benchmark deployed on AWS EC2. Results show that our method quickly adapts to workload changes and reduces the total cost compared to the industry standard approach.",project-academic
,2016-02-07,a,,dynamic selection of virtual machines for application servers in cloud environments," Autoscaling is a hallmark of cloud computing as it allows flexible just-in-time allocation and release of computational resources in response to dynamic and often unpredictable workloads. This is especially important for web applications whose workload is time dependent and prone to flash crowds. Most of them follow the 3-tier architectural pattern, and are divided into presentation, application/domain and data layers. In this work we focus on the application layer. Reactive autoscaling policies of the type ""Instantiate a new Virtual Machine (VM) when the average server CPU utilisation reaches X%"" have been used successfully since the dawn of cloud computing. But which VM type is the most suitable for the specific application at the moment remains an open question. In this work, we propose an approach for dynamic VM type selection. It uses a combination of online machine learning techniques, works in real time and adapts to changes in the users' workload patterns, application changes as well as middleware upgrades and reconfigurations. We have developed a prototype, which we tested with the CloudStone benchmark deployed on AWS EC2. Results show that our method quickly adapts to workload changes and reduces the total cost compared to the industry standard approach.",project-academic
10.23919/DATE.2019.8714959,2019-03-01,p,IEEE,learning to infer rl based search for dnn primitive selection on heterogeneous embedded systems," Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks’ accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs’ inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",project-academic
10.1109/EISIC.2017.21,2017-09-01,p,IEEE,adversarial machine learning in malware detection arms race between evasion attack and defense," Since malware has caused serious damages and evolving threats to computer and Internet users, its detection is of great interest to both anti-malware industry and researchers. In recent years, machine learning-based systems have been successfully deployed in malware detection, in which different kinds of classifiers are built based on the training samples using different feature representations. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we explore the adversarial machine learning in malware detection. In particular, on the basis of a learning-based classifier with the input of Windows Application Programming Interface (API) calls extracted from the Portable Executable (PE) files, we present an effective evasion attack model (named EvnAttack) by considering different contributions of the features to the classification problem. To be resilient against the evasion attack, we further propose a secure-learning paradigm for malware detection (named SecDefender), which not only adopts classifier retraining technique but also introduces the security regularization term which considers the evasion cost of feature manipulations by attackers to enhance the system security. Comprehensive experimental results on the real sample collections from Comodo Cloud Security Center demonstrate the effectiveness of our proposed methods.",project-academic
10.1145/3292500.3330670,2019-07-25,p,ACM,buying or browsing predicting real time purchasing intent using attention based deep network with multiple behavior," E-commerce platforms are becoming a primary place for people to find, compare and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to predict user purchasing intent, which is an important part of user understanding and allows for providing better services for both sellers and customers. However, previous work cannot predict real-time user purchasing intent with a high accuracy, limited by the representation capability of traditional browse-interactive behavior adopted. In this paper, we propose a novel end-to-end deep network, named Deep Intent Prediction Network (DIPN), to predict real-time user purchasing intent. In particular, besides the traditional browse-interactive behavior, we collect a new type of user interactive behavior, called touch-interactive behavior, which can capture more fine-grained real-time user features. To combine these behavior effectively, we propose a hierarchical attention mechanism, where the bottom attention layer focuses on the inner parts of each behavior sequence while the top attention layer learns the inter-view relations between different behavior sequences. In addition, we propose to train DIPN with multi-task learning to better distinguish user behavior patterns. In the experiments conducted on a large-scale industrial dataset, DIPN significantly outperforms the baseline solutions. Notably, DIPN gains about 18.96% improvement on AUC than the state-of-the-art solution only using traditional browse-interactive behavior sequences. Moreover, DIPN has been deployed in the operational system of Taobao. Online A/B testing results with more than 12.9 millions of users reveal the potential of knowing users' real-time purchasing intent.",project-academic
10.1186/S42400-020-00064-4,2020-12-01,a,SpringerOpen,sifu a cybersecurity awareness platform with challenge assessment and intelligent coach," Software vulnerabilities, when actively exploited by malicious parties, can lead to catastrophic consequences. Proper handling of software vulnerabilities is essential in the industrial context, particularly when the software is deployed in critical infrastructures. Therefore, several industrial standards mandate secure coding guidelines and industrial software developers’ training, as software quality is a significant contributor to secure software. CyberSecurity Challenges (CSC) form a method that combines serious game techniques with cybersecurity and secure coding guidelines to raise secure coding awareness of software developers in the industry. These cybersecurity awareness events have been used with success in industrial environments. However, until now, these coached events took place on-site. In the present work, we briefly introduce cybersecurity challenges and propose a novel platform that allows these events to take place online. The introduced cybersecurity awareness platform, which the authors call Sifu, performs automatic assessment of challenges in compliance to secure coding guidelines, and uses an artificial intelligence method to provide players with solution-guiding hints. Furthermore, due to its characteristics, the Sifu platform allows for remote (online) learning, in times of social distancing. The CyberSecurity Challenges events based on the Sifu platform were evaluated during four online real-life CSC events. We report on three surveys showing that the Sifu platform’s CSC events are adequate to raise industry software developers awareness on secure coding.",project-academic
10.1109/EEEIC.2019.8783632,2019-06-10,p,IEEE,the smart grid semantic platform synergy between iec common information model cim and big data," Recent innovations in Information Technology, Cloud Computing, Big Data analysis, Internet of Things (IoT) and Artificial Intelligence (AI) are enabling technological solutions that were previously unimaginable, or usually available only for big IT companies, with impacts in both industry and services. The demonstrator named Smart Grid Semantic Platform (SGSP) has been created using these technologies integrating several aspects of the management of an electricity distribution network. The services deployed on the platform concern the visualization of the topographic data and electricity grid assets, as well as the processing and displaying of historical data related to the operation of the networks and/or the result of Big Data analysis. This paper demonstrates how useful the synergy is between the standard semantic model of the electric network (IEC CIM 61968) and the historical data of the time series of medium voltage electricity consumption analyzed using a Big Data platform. Specifically it illustrates the realization of a use case about the calculation of a load index of the electric lines.",project-academic
10.1109/ACCESS.2019.2927082,2019-07-05,a,IEEE,a decade of internet of things analysis in the light of healthcare applications," Impressive growth in the number of wearable health monitoring devices has affected global health industry as they provide rapid and intricate details related to physical examinations, such as discomfort, heart rate, and blood glucose level, which enable doctors to efficiently diagnose sensitive heart troubles. The Internet of Medical Things (IoMT) is a phenomenon wherein computer networks and medical equipment are connected through the Internet to provide real-time interaction between physicians and patients. In this article, we present a comprehensive view of the IoMT and its related Machine Learning (ML)-based developed frameworks designed, or being utilized, in the last decade, i.e., from 2010 to 2019. The presented techniques are designed for monitoring limbs, controlling rural healthcare, identifying e-health applications, monitoring health through mobile apps, classifying heart sounds, detecting stress in drivers, monitoring cardiac diseases, making the decision to predict heart attacks, recognizing human activities, and classifying breast cancer. The aim is to provide a clear picture of the existing IoMT environment so that the analysis may pave the way for the diagnosis of critical disorders such as cancer, heart attack, and blood pressure among others. In the end, we also provide some unresolved challenges that are confronted in the deployment of the secure IoMT-based healthcare systems.",project-academic
,2008-11-01,b,,distributed intelligent systems a coordination perspective," Distributed Intelligent Systems: A Coordination Perspective addresses and comprehensively answers commonly asked questions about coordination in agent-oriented distributed systems. Characterizing the state-of-the-art research in the field of coordination with regard to the development of distributed agent-oriented systems is a particularly complex endeavour, as the space of available approaches is indeed considerable, and research is independently conducted in a great number of domains. While existing books deal with specific aspects of coordination, the major contribution of this book lies in the attempt to provide an in-depth review covering a wide range of issues regarding multi-agent coordination in Distributed Artificial Intelligence. In addition to reporting various sources of confusion, this book outlines the existence of a plethora of strategies and techniques adapted to different problems, agent-oriented systems, environments, and domains. In short, the current book identifies the absence of a single unified approach in addressing multi-agent coordination problems arising in any system or organization. This book, written by world-class leaders in this field, dedicates itself to providing a state-of-the-art review of current coordination strategies and techniques. The book also describes a broad range of application domains which implement many of the coordination strategies and techniques from the field of multi-agent systems. The application domains include defense, transportation, health care, telecommunication and e-business. Based on current practical deployed applications and existing capabilities, this book also identifies and thoroughly examines trends, challenges, and future agent-oriented research directions. Key features: Unveils the lack of coherence and order that characterizes the area of research pertaining to coordination of distributed intelligent systems Examines coordination models, frameworks, strategies and techniques to enable the development of distributed intelligent agent-oriented systems. Provides specific recommendations to realize more widespread deployment of agent-based systems Distributed Intelligent Systems: A Coordination Perspective is designed for a professional audience composed of practitioners and researchers in industry. This book is also suitable as a reference or secondary textbook for advanced-level students in computer science and engineering.",project-academic
10.3389/FNBOT.2011.00001,2011-07-12,a,Frontiers Media SA,robot cognitive control with a neurophysiologically inspired reinforcement learning model," A major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real world. The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e. cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources – expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex (LPFC and ACC) reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human-robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to “cheating” by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real world.",project-academic
10.1016/J.MICPRO.2019.102938,2020-02-01,a,Elsevier,towards collaborative intelligent iot ehealth from device to fog and cloud," Abstract None None The relationship between technology and healthcare due to the rise of intelligent Internet of Things (IoT), Artificial Intelligence (AI), and the rapid public embracement of medical-grade wearables has been dramatically transformed in the past few years. AI-powered IoT enabled disruptive changes and unique opportunities to the healthcare industry through personalized services, tailored content, improved availability and accessibility, and cost-effective delivery. Despite these exciting advancements in the transition from clinic-centric to patient-centric healthcare, many challenges still need to be tackled. The key to successfully unlock and enable this horizon shift is adopting hierarchical and collaborative architectures to provide a high level of quality in key attributes such as latency, availability, and real-time analytics. In this paper, we propose a holistic AI-driven IoT eHealth architecture based on the concept of Collaborative Machine Learning approach in which the intelligence is distributed across Device layer, Edge/Fog layer, and Cloud layer. This solution enables healthcare professionals to continuously monitor health-related data of subjects anywhere at any time and provide real-time actionable insights which ultimately improves the decision-making power. The feasibility of such architecture is investigated using a comprehensive ECG-based arrhythmia detection case study. This illustrative example discusses and addresses all important aspects of the proposed architecture from design implications such as corresponding overheads, energy consumption, latency, and performance, to mapping and deploying advanced machine learning techniques (e.g., Convolutional Neural Network) to such architecture.",project-academic
,2021-04-27,a,,one backward from ten forward subsampling for large scale deep learning," Deep learning models in large-scale machine learning systems are often continuously trained with enormous data from production environments. The sheer volume of streaming training data poses a significant challenge to real-time training subsystems and ad-hoc sampling is the standard practice. Our key insight is that these deployed ML systems continuously perform forward passes on data instances during inference, but ad-hoc sampling does not take advantage of this substantial computational effort. Therefore, we propose to record a constant amount of information per instance from these forward passes. The extra information measurably improves the selection of which data instances should participate in forward and backward passes. A novel optimization framework is proposed to analyze this problem and we provide an efficient approximation algorithm under the framework of Mini-batch gradient descent as a practical solution. We also demonstrate the effectiveness of our framework and algorithm on several large-scale classification and regression tasks, when compared with competitive baselines widely used in industry.",project-academic
10.1109/HPEC.2019.8916576,2019-05-24,a,,deploying ai frameworks on secure hpc systems with containers," The increasing interest in the usage of Artificial Intelligence techniques (AI) from the research community and industry to tackle ""real world"" problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high-level scripting languages or frameworks such as TensorFlow and the installation process often requires connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API's such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",project-academic
,2020-05-11,a,,using computer vision to enhance safety of workforce in manufacturing in a post covid world," The COVID-19 pandemic forced governments across the world to impose lockdowns to prevent virus transmissions. This resulted in the shutdown of all economic activity and accordingly the production at manufacturing plants across most sectors was halted. While there is an urgency to resume production, there is an even greater need to ensure the safety of the workforce at the plant site. Reports indicate that maintaining social distancing and wearing face masks while at work clearly reduces the risk of transmission. We decided to use computer vision on CCTV feeds to monitor worker activity and detect violations which trigger real time voice alerts on the shop floor. This paper describes an efficient and economic approach of using AI to create a safe environment in a manufacturing setup. We demonstrate our approach to build a robust social distancing measurement algorithm using a mix of modern-day deep learning and classic projective geometry techniques. We have deployed our solution at manufacturing plants across the Aditya Birla Group (ABG). We have also described our face mask detection approach which provides a high accuracy across a range of customized masks.",project-academic
10.1145/3447548.3467196,2021-08-14,p,ACM,reinforced iterative knowledge distillation for cross lingual named entity recognition," Named entity recognition (NER) is a fundamental component in many applications, such as Web Search and Voice Assistants. Although deep neural networks greatly improve the performance of NER, due to the requirement of large amounts of training data, deep neural networks can hardly scale out to many languages in an industry setting. To tackle this challenge, cross-lingual NER transfers knowledge from a rich-resource language to languages with low resources through pre-trained multilingual language models. Instead of using training data in target languages, cross-lingual NER has to rely on only training data in source languages, and optionally adds the translated training data derived from source languages. However, the existing cross-lingual NER methods do not make good use of rich unlabeled data in target languages, which is relatively easy to collect in industry applications. To address the opportunities and challenges, in this paper we describe our novel practice in Microsoft to leverage such large amounts of unlabeled data in target languages in real production settings. To effectively extract weak supervision signals from the unlabeled data, we develop a novel approach based on the ideas of semi-supervised learning and reinforcement learning. The empirical study on three benchmark data sets verifies that our approach establishes the new state-of-the-art performance with clear edges. Now, the NER techniques reported in this paper are on their way to become a fundamental component for Web ranking, Entity Pane, Answers Triggering, and Question Answering in the Microsoft Bing search engine. Moreover, our techniques will also serve as part of the Spoken Language Understanding module for a commercial voice assistant. We plan to open source the code of the prototype framework after deployment.",project-academic
,2021-06-01,a,,reinforced iterative knowledge distillation for cross lingual named entity recognition," Named entity recognition (NER) is a fundamental component in many applications, such as Web Search and Voice Assistants. Although deep neural networks greatly improve the performance of NER, due to the requirement of large amounts of training data, deep neural networks can hardly scale out to many languages in an industry setting. To tackle this challenge, cross-lingual NER transfers knowledge from a rich-resource language to languages with low resources through pre-trained multilingual language models. Instead of using training data in target languages, cross-lingual NER has to rely on only training data in source languages, and optionally adds the translated training data derived from source languages. However, the existing cross-lingual NER methods do not make good use of rich unlabeled data in target languages, which is relatively easy to collect in industry applications. To address the opportunities and challenges, in this paper we describe our novel practice in Microsoft to leverage such large amounts of unlabeled data in target languages in real production settings. To effectively extract weak supervision signals from the unlabeled data, we develop a novel approach based on the ideas of semi-supervised learning and reinforcement learning. The empirical study on three benchmark data sets verifies that our approach establishes the new state-of-the-art performance with clear edges. Now, the NER techniques reported in this paper are on their way to become a fundamental component for Web ranking, Entity Pane, Answers Triggering, and Question Answering in the Microsoft Bing search engine. Moreover, our techniques will also serve as part of the Spoken Language Understanding module for a commercial voice assistant. We plan to open source the code of the prototype framework after deployment.",project-academic
10.1109/ICRA48506.2021.9561020,2021-05-30,p,IEEE,sqrp sensing quality aware robot programming system for non expert programmers," Robot programming typically makes use of a set of mechanical skills that is acquired by machine learning. Because there is in general no guarantee that machine learning produces robot programs that are free of surprising behavior, the safe execution of a robot program must utilize monitoring modules that take sensor data as inputs in real time to ensure the correctness of the skill execution. Owing to the fact that sensors and monitoring algorithms are usually subject to physical restrictions and that effective robot programming is sensitive to the selection of skill parameters, these considerations may lead to different sensor input qualities such as the view coverage of a vision system that determines whether a skill can be successfully deployed in performing a task. Choosing improper skill parameters may cause the monitoring modules to delay or miss the detection of important events such as a mechanical failure. These failures may reduce the throughput in robotic manufacturing and could even cause a destructive system crash. To address above issues, we propose a sensing quality-aware robot programming system that automatically computes the sensing qualities as a function of the robot’s environment and uses the information to guide non-expert users to select proper skill parameters in the programming phase. We demonstrate our system framework on a 6DOF robot arm for an object pick-up task.",project-academic
,2021-06-30,a,,sqrp sensing quality aware robot programming system for non expert programmers," Robot programming typically makes use of a set of mechanical skills that is acquired by machine learning. Because there is in general no guarantee that machine learning produces robot programs that are free of surprising behavior, the safe execution of a robot program must utilize monitoring modules that take sensor data as inputs in real time to ensure the correctness of the skill execution. Owing to the fact that sensors and monitoring algorithms are usually subject to physical restrictions and that effective robot programming is sensitive to the selection of skill parameters, these considerations may lead to different sensor input qualities such as the view coverage of a vision system that determines whether a skill can be successfully deployed in performing a task. Choosing improper skill parameters may cause the monitoring modules to delay or miss the detection of important events such as a mechanical failure. These failures may reduce the throughput in robotic manufacturing and could even cause a destructive system crash. To address above issues, we propose a sensing quality-aware robot programming system that automatically computes the sensing qualities as a function of the robot's environment and uses the information to guide non-expert users to select proper skill parameters in the programming phase. We demonstrate our system framework on a 6DOF robot arm for an object pick-up task.",project-academic
10.1109/BIGDATA50022.2020.9378036,2020-12-10,p,IEEE,pairs autogeo an automated machine learning framework for massive geospatial data," An automated machine learning framework for geospatial data named PAIRS AutoGeo is introduced on IBM PAIRS Geoscope big data and analytics platform. The frame-work simplifies the development of industrial machine learning solutions leveraging geospatial data to the extent that the user inputs are minimized to merely a text file containing labeled GPS coordinates. PAIRS AutoGeo automatically gathers required data at the location coordinates, assembles the training data, performs quality check, and trains multiple machine learning models for subsequent deployment. The framework is validated using a realistic industrial use case of tree species classification. Open-source tree species data are used as the input to train a random forest classifier and a modified ResNet model for 10-way tree species classification based on aerial imagery, which leads to an accuracy of 59.8% and 81.4%, respectively. This use case exemplifies how PAIRS AutoGeo enables users to leverage machine learning without extensive geospatial expertise.",project-academic
,2020-12-12,a,,pairs autogeo an automated machine learning framework for massive geospatial data," An automated machine learning framework for geospatial data named PAIRS AutoGeo is introduced on IBM PAIRS Geoscope big data and analytics platform. The framework simplifies the development of industrial machine learning solutions leveraging geospatial data to the extent that the user inputs are minimized to merely a text file containing labeled GPS coordinates. PAIRS AutoGeo automatically gathers required data at the location coordinates, assembles the training data, performs quality check, and trains multiple machine learning models for subsequent deployment. The framework is validated using a realistic industrial use case of tree species classification. Open-source tree species data are used as the input to train a random forest classifier and a modified ResNet model for 10-way tree species classification based on aerial imagery, which leads to an accuracy of $59.8\%$ and $81.4\%$, respectively. This use case exemplifies how PAIRS AutoGeo enables users to leverage machine learning without extensive geospatial expertise.",project-academic
,2018-01-17,a,,on the reduction of biases in big data sets for the detection of irregular power usage," In machine learning, a bias occurs whenever training sets are not representative for the test data, which results in unreliable models. The most common biases in data are arguably class imbalance and covariate shift. In this work, we aim to shed light on this topic in order to increase the overall attention to this issue in the field of machine learning. We propose a scalable novel framework for reducing multiple biases in high-dimensional data sets in order to train more reliable predictors. We apply our methodology to the detection of irregular power usage from real, noisy industrial data. In emerging markets, irregular power usage, and electricity theft in particular, may range up to 40% of the total electricity distributed. Biased data sets are of particular issue in this domain. We show that reducing these biases increases the accuracy of the trained predictors. Our models have the potential to generate significant economic value in a real world application, as they are being deployed in a commercial software for the detection of irregular power usage.",project-academic
10.1142/9789813273238_0057,2018-01-01,p,WORLD SCIENTIFIC,on the reduction of biases in big data sets for the detection of irregular power usage," In machine learning, a bias occurs whenever training sets are not representative for the test data, which results in unreliable models. The most common biases in data are arguably class imbalance and covariate shift. In this work, we aim to shed light on this topic in order to increase the overall attention to this issue in the field of machine learning. We propose a scalable novel framework for reducing multiple biases in high-dimensional data sets in order to train more reliable predictors. We apply our methodology to the detection of irregular power usage from real, noisy industrial data. In emerging markets, irregular power usage, and electricity theft in particular, may range up to 40% of the total electricity distributed. Biased data sets are of particular issue in this domain. We show that reducing these biases increases the accuracy of the trained predictors. Our models have the potential to generate significant economic value in a real world application, as they are being deployed in a commercial software for the detection of irregular power usage.",project-academic
,2021-04-20,a,,iiot enabled health monitoring for integrated heat pump system using mixture slow feature analysis," The sustaining evolution of sensing and advancement in communications technologies have revolutionized prognostics and health management for various electrical equipment towards data-driven ways. This revolution delivers a promising solution for the health monitoring problem of heat pump (HP) system, a vital device widely deployed in modern buildings for heating use, to timely evaluate its operation status to avoid unexpected downtime. Many HPs were practically manufactured and installed many years ago, resulting in fewer sensors available due to technology limitations and cost control at that time. It raises a dilemma to safeguard HPs at an affordable cost. We propose a hybrid scheme by integrating industrial Internet-of-Things (IIoT) and intelligent health monitoring algorithms to handle this challenge. To start with, an IIoT network is constructed to sense and store measurements. Specifically, temperature sensors are properly chosen and deployed at the inlet and outlet of the water tank to measure water temperature. Second, with temperature information, we propose an unsupervised learning algorithm named mixture slow feature analysis (MSFA) to timely evaluate the health status of the integrated HP. Characterized by frequent operation switches of different HPs due to the variable demand for hot water, various heating patterns with different heating speeds are observed. Slowness, a kind of dynamics to measure the varying speed of steady distribution, is properly considered in MSFA for both heating pattern division and health evaluation. Finally, the efficacy of the proposed method is verified through a real integrated HP with five connected HPs installed ten years ago. The experimental results show that MSFA is capable of accurately identifying health status of the system, especially failure at a preliminary stage compared to its competing algorithms.",project-academic
10.1109/BDCLOUD.2018.00136,2018-12-01,p,IEEE COMPUTER SOC,istep an integrated self tuning engine for predictive maintenance in industry 4 0," The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",project-academic
10.1145/3321707.3321735,2019-07-13,p,ACM,evolving deep neural networks by multi objective particle swarm optimization for image classification," In recent years, convolutional neural networks (CNNs) have become deeper in order to achieve better classification accuracy in image classification. However, it is difficult to deploy the state-of-the-art deep CNNs for industrial use due to the difficulty of manually fine-tuning the hyperparameters and the trade-off between classification accuracy and computational cost. This paper proposes a novel multi-objective optimization method for evolving state-of-the-art deep CNNs in real-life applications, which automatically evolves the non-dominant solutions at the Pareto front. Three major contributions are made: Firstly, a new encoding strategy is designed to encode one of the best state-of-the-art CNNs; With the classification accuracy and the number of floating point operations as the two objectives, a multi-objective particle swarm optimization method is developed to evolve the non-dominant solutions; Last but not least, a new infrastructure is designed to boost the experiments by concurrently running the experiments on multiple GPUs across multiple machines, and a Python library is developed and released to manage the infrastructure. The experimental results demonstrate that the non-dominant solutions found by the proposed method form a clear Pareto front, and the proposed infrastructure is able to almost linearly reduce the running time.",project-academic
,2019-03-21,a,,evolving deep neural networks by multi objective particle swarm optimization for image classification," In recent years, convolutional neural networks (CNNs) have become deeper in order to achieve better classification accuracy in image classification. However, it is difficult to deploy the state-of-the-art deep CNNs for industrial use due to the difficulty of manually fine-tuning the hyperparameters and the trade-off between classification accuracy and computational cost. This paper proposes a novel multi-objective optimization method for evolving state-of-the-art deep CNNs in real-life applications, which automatically evolves the non-dominant solutions at the Pareto front. Three major contributions are made: Firstly, a new encoding strategy is designed to encode one of the best state-of-the-art CNNs; With the classification accuracy and the number of floating point operations as the two objectives, a multi-objective particle swarm optimization method is developed to evolve the non-dominant solutions; Last but not least, a new infrastructure is designed to boost the experiments by concurrently running the experiments on multiple GPUs across multiple machines, and a Python library is developed and released to manage the infrastructure. The experimental results demonstrate that the non-dominant solutions found by the proposed algorithm form a clear Pareto front, and the proposed infrastructure is able to almost linearly reduce the running time.",project-academic
10.1109/JIOT.2021.3090513,2021-06-18,a,IEEE,edge based video surveillance with graph assisted reinforcement learning in smart construction," The smart construction site is developing repidly with the intelligentization of industrial management. Intelligent devices are beening widely deployed in construction industry to support artificial intelligence applications. Video surveillance is a core function of smart construction, which demands both high accuracy and low latency. The challenge is that the computation and networking resources in a construction site are often limited, and the inefficient scheduling policies create congestions in network and brings additional delay that is unbearable to realtime surveillance. Adaptive video configuration and edge computing have been proposed to improve accuracy and reduce latency with limited resources. However, optimizing the video configuration and task scheduling in edge computing involves several factors that often interfere with each other, which significantly decreases the performance of video surveillance. In this paper, we present an edge-based solution of video surveillance in smart construction site assisted by Graph Neural Network. It leverages the distributed computing model to realize flexible allocation of resources. A graph-assisted hierarchical reinforcement learning algorithm is developed to illustrate the feature of mobile edge network and optimize the scheduling policy by Deep-Q Network. We implement and test the proposed solution in the commercial residential buildings of a fortune global 500 real estate company, and observe that the proposed algorithm is efficient to maintain a reliable accuracy and keep lower delay. We further conduct a case study to demonstrate the superiority of the proposed solution by comparing it with traditional mechanisms.",project-academic
10.1007/978-981-13-5758-9_1,2018-09-19,a,"Springer, Singapore",deep learning locally trained wildlife sensing in real acoustic wetland environment," We describe ‘Tidzam’, an application of deep learning that leverages a dense, multimodal sensor network installed at a large wetland restoration performed at Tidmarsh, a 600-acre former industrial-scale cranberry farm in Southern Massachusetts. Wildlife acoustic monitoring is a crucial metric during post-restoration evaluation of the processes, as well as a challenge in such a noisy outdoor environment. This article presents the entire Tidzam system, which has been designed in order to identify in real-time the ambient sounds of weather conditions as well as sonic events such as insects, small animals and local bird species from microphones deployed on the site. This experiment provides insight on the usage of deep learning technology in a real deployment. The originality of this work concerns the system’s ability to construct its own database from local audio sampling under the supervision of human visitors and bird experts.",project-academic
10.18034/EI.V3I2.519,2015-12-21,a,ABC Journals,enabling trustworthiness in artificial intelligence a detailed discussion," Artificial intelligence (AI) delivers numerous chances to add to the prosperity of people and the stability of economies and society, yet besides, it adds up a variety of novel moral, legal, social, and innovative difficulties. Trustworthy AI (TAI) bases on the possibility that trust builds the establishment of various societies, economies, and sustainable turn of events, and that people, organizations, and societies can along these lines just at any point understand the maximum capacity of AI, if trust can be set up in its development, deployment, and use. The risks of unintended and negative outcomes related to AI are proportionately high, particularly at scale. Most AI is really artificial narrow intelligence, intended to achieve a specific task on previously curated information from a certain source. Since most AI models expand on correlations, predictions could fail to sum up to various populations or settings and might fuel existing disparities and biases. As the AI industry is amazingly imbalanced, and experts are as of now overpowered by other digital devices, there could be a little capacity to catch blunders. With this article, we aim to present the idea of TAI and its five essential standards (1) usefulness, (2) non-maleficence, (3) autonomy, (4) justice, and (5) logic. We further draw on these five standards to build up a data-driven analysis for TAI and present its application by portraying productive paths for future research, especially as to the distributed ledger technology-based acknowledgment of TAI.",project-academic
10.1002/BIT.26605,2018-08-01,a,Biotechnol Bioeng,advances in industrial biopharmaceutical batch process monitoring machine learning methods for small data problems," Biopharmaceutical manufacturing comprises of multiple distinct processing steps that require effective and efficient monitoring of many variables simultaneously in real-time. The state-of-the-art real-time multivariate statistical batch process monitoring (BPM) platforms have been in use in recent years to ensure comprehensive monitoring is in place as a complementary tool for continued process verification to detect weak signals. This article addresses a longstanding, industry-wide problem in BPM, referred to as the ""Low-N"" problem, wherein a product has a limited production history. The current best industrial practice to address the Low-N problem is to switch from a multivariate to a univariate BPM, until sufficient product history is available to build and deploy a multivariate BPM platform. Every batch run without a robust multivariate BPM platform poses risk of not detecting potential weak signals developing in the process that might have an impact on process and product performance. In this article, we propose an approach to solve the Low-N problem by generating an arbitrarily large number of in silico batches through a combination of hardware exploitation and machine-learning methods. To the best of authors' knowledge, this is the first article to provide a solution to the Low-N problem in biopharmaceutical manufacturing using machine-learning methods. Several industrial case studies from bulk drug substance manufacturing are presented to demonstrate the efficacy of the proposed approach for BPM under various Low-N scenarios.",project-academic
10.1016/J.COMPIND.2012.11.005,2013-04-01,a,Elsevier,imaqcs design and implementation of an intelligent multi agent system for monitoring and controlling quality of cement production processes," In cement plant, since all processes are chemical and irreversible, monitoring and control is a critical factor. If the process is not controlled at any stage, the final product can be damaged or lost. Thus, in such environments, considering the quality of the product at each state is essential. Also, to control the process, communication among different parts of production line is essential. The wasted time in production line has a direct effect on process correction time and cement production performance. Here, a model of a new intelligent multi-agent quality control system (IMAQCS) for controlling the quality of cement production processes is suggested. This model, using of rule-based artificial intelligence technique, concentrates on relationship between departments in cement production line to monitor multi-attribute quality factors. With the presence of agents for controlling the quality of cement processes, real-time analyzing and decision making in a fault condition will be provided. In order to validate the proposed model, IMAQCS is deployed in real plants of a cement industries complex in Iran. The ability of the system in the process production environment is assessed. The effectiveness and efficiency of the system are demonstrated by reducing the process correction time and increasing the cement production performance. Finally, this system can effectively impact on factory resources and cost saving.",project-academic
10.1155/2017/8728209,2017-06-14,a,Hindawi,introducing a novel hybrid artificial intelligence algorithm to optimize network of industrial applications in modern manufacturing," Recent advances in modern manufacturing industries have created a great need to track and identify objects and parts by obtaining real-time information. One of the main technologies which has been utilized for this need is the Radio Frequency Identification (RFID) system. As a result of adopting this technology to the manufacturing industry environment, RFID Network Planning (RNP) has become a challenge. Mainly RNP deals with calculating the number and position of antennas which should be deployed in the RFID network to achieve full coverage of the tags that need to be read. The ultimate goal of this paper is to present and evaluate a way of modelling and optimizing nonlinear RNP problems utilizing artificial intelligence (AI) techniques. This effort has led the author to propose a novel AI algorithm, which has been named “hybrid AI optimization technique,” to perform optimization of RNP as a hard learning problem. The proposed algorithm is composed of two different optimization algorithms: Redundant Antenna Elimination (RAE) and Ring Probabilistic Logic Neural Networks (RPLNN). The proposed hybrid paradigm has been explored using a flexible manufacturing system (FMS), and results have been compared with Genetic Algorithm (GA) that demonstrates the feasibility of the proposed architecture successfully.",project-academic
,2016-01-01,a,,anomaly detection in industrial networks using machine learning a roadmap," With the advent of 21st Century, we stepped into the fourth industrial revolution of cyber physical systems. There is the need of secured network systems and intrusion detection systems in order to detect network attacks. Use of machine learning for anomaly detection in industrial networks faces challenges which restricts its large-scale commercial deployment. ADIN Suite proposes a roadmap to overcome these challenges with multi-module solution. It solves the need for real world network traffic, an adaptive hybrid analysis to reduce error rates in diverse network traffic and alarm correlation for semantic description of detection results to the network operator.",project-academic
10.1007/978-3-662-53806-7_8,2017-01-01,a,"Springer Vieweg, Berlin, Heidelberg",anomaly detection in industrial networks using machine learning a roadmap," With the advent of 21st Century, we stepped into the fourth industrial revolution of cyber physical systems. There is the need of secured network systems and intrusion detection systems in order to detect network attacks. Use of machine learning for anomaly detection in industrial networks faces challenges which restricts its large-scale commercial deployment. ADIN Suite proposes a roadmap to overcome these challenges with multi-module solution. It solves the need for real world network traffic, an adaptive hybrid analysis to reduce error rates in diverse network traffic and alarm correlation for semantic description of detection results to the network operator.",project-academic
10.1007/S007790200040,2002-01-12,p,Springer-Verlag,special issue on ubiquitous games," Computer games have grown during recent years into a popular entertainment form with a wide variety of game types and a large consumer group spread across the world. An increasing number of people are playing electronic games, placing them among other favorite leisure activities. When surveyed on the most fun entertainment activities in the year 2000, 35 percent of all Americans identified computer and video games, where watching television fell second at 18 percent, followed by surfing the internet (15%), reading books (13 %) and going to the movies (11%) [8]. On-line gaming has offered people new means of having social interaction with people in faraway locations and let them access and play out fantasy-driven identities they are unable to manifest in the real world [12]. In 2002, the percentage of frequent game players that play games online rose to 31 percent up from 24 percent the year before [8]. Within the games, or through the use of websites based around the games, dedicated and long-lived communities have formed which have created new content, sometimes leading to commercial ventures. Electronic game play, however, is not limited to home use. Game parlors and LAN parties are becoming a popular means to play online games. Also, 37 percent of Americans who own game consoles (or computers used to play games), report that they also play games on mobile devices such as PDAs and mobile phones [8]. The popularity of computer games has thus not only made them important carriers of culture and trends but also a financially interesting area. They also function as a vehicle for the development and deployment of new hardware, software and user interface techniques. According to Interactive Digital Software Association (www.idsa.com), the computer and video game software industry in the US generated $6.35 billon in sales from 225 million units in 2001 (up 7.9% and 4.5% from 2000 respectively). In the same year in the US, movie box office grosses were $8.41 billion [8]. In the UK in the year 2000, the entertainment software industries grossed £300 million more than the UK cinema box offices and almost double that of home video rentals [3]. The world entertainment software market is estimated to be almost $18 billion and to grow to $26.7 billion by the year 2005 [3]. Computer games are some of the applications available to the general public that have the very highest demands on ease-of-use, engagement, and aesthetical appearance. These demands have made developers of computer games use the latest techniques within artificial intelligence, multimedia, computer graphics, computer simulation, and human-computer interaction to be competitive against other games developers. For example, recently IBM, Sony, and Toshiba have partnered on creation of the ‘Cell’. The Cell is a unique type of computer processor that contains several types of computing cores that are optimized for one type of task (e.g. communications, video processing). These processing cells can be interconnected to optimize a specific task [11]. This exciting research has not been driven for use in supercomputers, or high-end business PCs, but for use in the Playstation, a gaming console.",project-academic
10.1007/S00170-012-3903-Z,2012-02-05,a,Springer-Verlag,predicting surface roughness of hardened aisi 1040 based on cutting parameters using neural networks and multiple regression," In this study, models for predicting the surface roughness of AISI 1040 steel material using artificial neural networks (ANN) and multiple regression (MRM) are developed. The models are optimized using cutting parameters as input and corresponding surface roughness values as output. Cutting parameters considered in this study include cutting speed, feed rate, depth of cut, and nose radius. Surface roughness is characterized by the mean (R a) and total (R t) of the recorded roughness values at different locations on the surface. A total of 81 different experiments were performed, each with a different setting of the cutting parameters, and the corresponding R a and R t values for each case are measured. Input–output pairs obtained through these 81 experiments are used to train an ANN is achieved at the 200,00th epoch. Mean squared error of 0.002917120% achieved using the developed ANN outperforms error rates reported in earlier studies and can also be considered admissible for real-time deployment of the developed ANN algorithm for robust prediction of the surface roughness in industrial settings.",project-academic
10.1155/2018/3928080,2018-07-03,a,Hindawi,intelligent healthcare systems assisted by data analytics and mobile computing," It is entering an era of big data, which facilitated great improvement in various sectors. Particularly, assisted by wireless communications and mobile computing, mobile devices have emerged with a great potential to renovate the healthcare industry. Although the advanced techniques will make it possible to understand what is happening in our body more deeply, it is extremely difficult to handle and process the big health data anytime and anywhere. Therefore, data analytics and mobile computing are significant for the healthcare systems to meet many technical challenges and problems that need to be addressed to realize this potential. Furthermore, the advanced healthcare systems have to be upgraded with new capabilities such as machine learning, data analytics, and cognitive power for providing human with more intelligent and professional healthcare services. To explore recent advances and disseminate state-of-the-art techniques related to data analytics and mobile computing on designing, building, and deploying novel technologies, to enable intelligent healthcare services and applications, this paper presents the detailed design for developing intelligent healthcare systems assisted by data analytics and mobile computing. Moreover, some representative intelligent healthcare applications are discussed to show that data analytics and mobile computing are available to enhance the performance of the healthcare services.",project-academic
10.1109/TENCONSPRING.2017.8070069,2017-07-01,p,,smart real time meeting room," The monumental changes happening in present infrastructure industry can mostly be attributed to developments in Internet of Things. Developing smarter conference rooms in offices have a great scope for innovation and digital transformation. In this paper, we have developed a Smart Meeting Room (SMR) architecture and addressed various imminent issues pertaining to a meeting room environment by providing automation. The Smart Room requirements at real-time are automatically customized and act according to the customer's needs. Due to the developments in Machine to Machine (M2M) communication, we can establish a connection between the devices and the customers on a real-time basis. This enables the smart rooms to interact with the users and act according to their needs as well as automatically establish communication with other meeting rooms. Various types of sensors like temperature sensor, humidity sensor, PIR (Passive Infrared) based motion detector etc. are deployed in the smart room to monitor and enable the actuators. Ericsson's APPIoT framework facilitates smooth discovery, attachment and data sharing between devices in a close proximity. Gnokii and Pidgin systems are used with our architecture to ease the customization and execution of different events. A microcontroller serves as the brain and gateway for the control operations and provides users with additional features such as setting up of the room environment, loading presentations on the display in advance etc. By continuously learning from the environment within the room, the actuators trigger a responsive event automatically to control the room's ambience and the bookings.",project-academic
10.1109/ISCA45697.2020.00038,2020-05-30,p,IEEE,spinalflow an architecture and dataflow tailored for spiking neural networks," Spiking neural networks (SNNs) are expected to be part of the future AI portfolio, with heavy investment from industry and government, e.g., IBM TrueNorth, Intel Loihi. While Artificial Neural Network (ANN) architectures have taken large strides, few works have targeted SNN hardware efficiency. Our analysis of SNN baselines shows that at modest spike rates, SNN implementations exhibit significantly lower efficiency than accelerators for ANNs. This is primarily because SNN dataflows must consider neuron potentials for several ticks, introducing a new data structure and a new dimension to the reuse pattern. We introduce a novel SNN architecture, SpinalFlow, that processes a compressed, time-stamped, sorted sequence of input spikes. It adopts an ordering of computations such that the outputs of a network layer are also compressed, time-stamped, and sorted. All relevant computations for a neuron are performed in consecutive steps to eliminate neuron potential storage overheads. Thus, with better data reuse, we advance the energy efficiency of SNN accelerators by an order of magnitude. Even though the temporal aspect in SNNs prevents the exploitation of some reuse patterns that are more easily exploited in ANNs, at 4-bit input resolution and 90% input sparsity, SpinalFlow reduces average energy by $1.8 \times$, compared to a 4-bit Eyeriss baseline. These improvements are seen for a range of networks and sparsity/resolution levels; SpinalFlow consumes $5 \times$ less energy and $5.4 \times$ less time than an 8-bit version of Eyeriss. We thus show that, depending on the level of observed sparsity, SNN architectures can be competitive with ANN architectures in terms of latency and energy for inference, thus lowering the barrier for practical deployment in scenarios demanding real-time learning.",project-academic
10.1145/3022099.3022101,2016-04-19,a,,intelligent agent based stimulation for testing robotic software in human robot interactions," The challenges of robotic software testing extend beyond conventional software testing. Valid, realistic and interesting tests need to be generated for multiple programs and hardware running concurrently, deployed into dynamic environments with people. We investigate the use of Belief-Desire-Intention (BDI) agents as models for test generation, in the domain of human-robot interaction (HRI) in simulations. These models provide rational agency, causality, and a reasoning mechanism for planning, which emulate both intelligent and adaptive robots, as well as smart testing environments directed by humans. We introduce reinforcement learning (RL) to automate the exploration of the BDI models using a reward function based on coverage feedback. Our approach is evaluated using a collaborative manufacture example, where the robotic software under test is stimulated indirectly via a simulated human co-worker. We conclude that BDI agents provide intuitive models for test generation in the HRI domain. Our results demonstrate that RL can fully automate BDI model exploration, leading to very effective coverage-directed test generation.",project-academic
10.1016/J.IS.2021.101840,2021-07-01,a,Pergamon,the convergence and interplay of edge fog and cloud in the ai driven internet of things iot," Abstract None None The Internet of Things (IoT) tsunami, public embracement, and the ubiquitous adoption of smart devices are affecting virtually every industry, directly or indirectly. The success of the current and future landscape of IoT and connected devices requires service provision characterized by scalability, ubiquity, reliability, and high-performance, among others. In order to achieve this attribution, the integration of IoT and Cloud Computing (CC), known as cloud IoT, has emerged as a new paradigm providing advanced services specific to aggregating, storing, and processing data generated by IoT. While the convergence of IoT and Cloud brings opportunities, it suffers from specific limitations such as bandwidth, latency, and connectivity. The increasing need for supporting interaction between cloud and IoT led to Edge and Fog Computing (FC) in which computing and storage resources are located not only in the cloud but also at the edges near the source of data. The hierarchical and collaborative edge–fog–cloud architecture brings tremendous benefits as it enables us to distribute the intelligence and computation – including Artificial Intelligence (AI), Machine Learning (ML), and big data analytics – to achieve an optimal solution while satisfying the given constraints e.g., delay-energy tradeoff. Due to the hierarchical, cross-layer, and distributed nature of this model, achieving an osmotic and effective convergence of IoT, edge, fog, and cloud computing requires overcoming many challenges with respect to design and implementation, as well as deployment and evaluation. This paper provides a comprehensive insight into the edge-fog-cloud computing paradigm by providing a blend of discussions on all important aspects of the underlying technologies to offer opportunities for more holistic studies and to accelerate knowledge acquisition. To gain a deep understanding of edge–fog–cloud, we will begin this paper by providing an in-depth tutorial and presenting the main requirements, state-of-the-art reference architectures, building blocks, components, protocols, applications, and other similar computing paradigms, including their similarities and differences. Following this, a holistic reference architecture for edge–fog–cloud IoT is presented and the major corresponding design and deployment considerations (e.g., service models, infrastructure design, provisioning, resource allocation, offloading, service migration, performance evaluation, and security concerns) are discussed. Next, we will take a look at the role of privacy-preserving, distributed, and collaborative analytics as well as the interaction between edge, fog, and cloud computing. Finally, we will overview the main challenges in the field of edge–fog–cloud computing that need to be tackled to realize the full potential of IoT.",project-academic
10.1007/978-3-030-10997-4_23,2018-09-10,p,"Springer, Cham",solving the false positives problem in fraud prediction using automated feature engineering," In this paper, we present an automated feature engineering based approach to dramatically reduce false positives in fraud prediction. False positives plague the fraud prediction industry. It is estimated that only 1 in 5 declared as fraud are actually fraud and roughly 1 in every 6 customers have had a valid transaction declined in the past year. To address this problem, we use the Deep Feature Synthesis algorithm to automatically derive behavioral features based on the historical data of the card associated with a transaction. We generate 237 features (>100 behavioral patterns) for each transaction, and use a random forest to learn a classifier. We tested our machine learning model on data from a large multinational bank and compared it to their existing solution. On an unseen data of 1.852 million transactions, we were able to reduce the false positives by 54% and provide a savings of 190K euros. We also assess how to deploy this solution, and whether it necessitates streaming computation for real time scoring. We found that our solution can maintain similar benefits even when historical features are computed once every 7 days.",project-academic
10.1101/2020.09.02.279687,2020-09-03,a,Cold Spring Harbor Laboratory,a genome scale metabolic network model synergizes with statistical learning to predict amino acid concentrations in chinese hamster ovary cell cultures," The control of nutrient availability is critical to large-scale manufacturing of biotherapeutics. However, the quantification of proteinogenic amino acids is time-consuming and thus is difficult to implement for real-time in situ bioprocess control. Genome-scale metabolic models describe the metabolic conversion from media nutrients to proliferation and recombinant protein production, and therefore are a promising platform for in silico monitoring and prediction of amino acid concentrations. This potential has not been realized due to unresolved challenges: (1) the models assume an optimal and highly efficient metabolism, and therefore tend to underestimate amino acid consumption, and (2) the models assume a steady state, and therefore have a short forecast range. We address these challenges by integrating machine learning with the metabolic models. Through this we demonstrate accurate and time-course dependent prediction of individual amino acid concentration in culture medium throughout the production process. Thus, these models can be deployed to control nutrient feeding to avoid premature nutrient depletion or provide early predictions of failed bioreactor runs.",project-academic
10.1145/3289175.3289177,2018-12-10,p,ACM,towards emergent microservices for client tailored design," Contemporary systems are increasingly complex, with both large codebases and constantly changing environments which make them challenging to develop, deploy and manage. We consider two recent efforts to tackle this complexity: microservices and emergent software. Microservices have gained recent popularity in industry, in which monoliths of software are broken down into compositions of single-objective, end-to-end services running on HTTP which can be scaled out on cloud hosting systems. From the research community, the emergent systems concept demonstrates promise in using real-time learning to autonomously compose and optimise software systems from small building blocks, rapidly finding the best behavioural composition to match the current deployment conditions. We argue that emergent software and microservice architectures have strong potential for synergy in complex systems, offering mutually compatible lessons in dealing with complexity via scale-out design and real-time client-tailored behaviour. We explore self-designing microservices, built with emergent software, to demonstrate the complementary boundaries of both concepts - and how future intersections may offer novel architectures that lie at a compelling point between human- and machine-designed systems. We present the conceptual synergy and demonstrate a specific microservice architecture for a smart city example where scoped microservices are continually self-composed according to the demands of the applications and operating environment. For the purpose of reproducibility of the study, we make available all the code used in the evaluation of the proposed approach.",project-academic
10.1109/TDEI.2020.009070,2020-12-14,a,IEEE,condition monitoring based on partial discharge diagnostics using machine learning methods a comprehensive state of the art review," This paper presents a state-of-the-art review on machine learning (ML) based intelligent diagnostics that have been applied for partial discharge (PD) detection, localization, and pattern recognition. ML techniques, particularly those developed in the last five years, are examined and classified as conventional ML or deep learning (DL). Important features of each method, such as types of input signal, sampling rate, core methodology, and accuracy, are summarized and compared in detail. Advantages and disadvantages of different ML algorithms are discussed. Moreover, technical roadblocks preventing intelligent PD diagnostics from being applied to industry are identified, such as insufficient/imbalanced dataset, data inconsistency, and difficulties in cost-effective real-time deployment. Finally, potential solutions are proposed, and future research directions are suggested.",project-academic
10.1109/TII.2021.3050041,2021-01-08,a,Institute of Electrical and Electronics Engineers (IEEE),network traffic prediction in industrial internet of things backbone networks a multitask learning mechanism," Industrial Internet of Things (IIoT), as a common industrial application of Internet of Things, has been widely deployed in recent years. End-to-end network traffic is an essential information for many network security and management functions. This article investigates the issues of IIoT-oriented backbone network traffic prediction. Predicting the traffic of IIoT backbone networks is intractable because of the large number of prior network traffic information, which needs to consume expensive network resources for sampling. Motivated by that, we propose an effective prediction mechanism using multitask learning (MTL), which is a special paradigm of transfer learning. A deep learning architecture constructed by MTL and long short-term memory is designed. This deep architecture takes advantage of link loads as additional information to improve prediction accuracy. We provide a theoretical analysis for the MTL mechanism. The effectiveness is evaluated by implementing our mechanism on real network.",project-academic
10.1557/S43577-020-00006-Y,2020-09-16,a,Springer Science and Business Media LLC,a deep learning approach to the inverse problem of modulus identification in elasticity," The inverse elasticity problem of identifying elastic modulus distribution based on measured displacement/strain fields plays a key role in various non-destructive evaluation (NDE) techniques used in geological exploration, quality control, and medical diagnosis (e.g., elastography). Conventional methods in this field are often computationally costly and cannot meet the increasing demand for real-time and high-throughput solutions for advanced manufacturing and clinical practices. Here, we propose a deep learning (DL) approach to address this challenge. By constructing representative sampling spaces of shear modulus distribution and adopting a conditional generative adversarial net, we demonstrate that the DL model can learn high-dimensional mapping between strain and modulus via training over a limited portion of the sampling space. The proposed DL approach bypasses the costly iterative solver in conventional methods and can be rapidly deployed with high accuracy, making it particularly suitable for applications such as real-time elastography and highthroughput NDE techniques.",project-academic
10.1145/3318464.3389768,2020-06-11,p,ACM,active learning for ml enhanced database systems," Recent research has shown promising results by using machine learning (ML) techniques to improve the performance of database systems, e.g., in query optimization or index recommendation. However, in many production deployments, the ML models' performance degrades significantly when the test data diverges from the data used to train these models. In this paper, we address this performance degradation by using B-instances to collect additional data during deployment. We propose an active data collection platform, ADCP, that employs active learning (AL) to gather relevant data cost-effectively. We develop a novel AL technique, Holistic Active Learner (HAL), that robustly combines multiple noisy signals for data gathering in the context of database applications. HAL applies to various ML tasks, budget sizes, cost types, and budgeting interfaces for database applications. We evaluate ADCP on both industry-standard benchmarks and real customer workloads. Our evaluation shows that, compared with other baselines, our technique improves ML models' prediction performance by up to 2x with the same cost budget. In particular, on production workloads, our technique reduces the prediction error of ML models by 75% using about 100 additionally collected queries.",project-academic
10.1145/2783258.2788579,2015-08-10,p,ACM,one pass ranking models for low latency product recommendations," Purchase logs collected in e-commerce platforms provide rich information about customer preferences. These logs can be leveraged to improve the quality of product recommendations by feeding them to machine-learned ranking models. However, a variety of deployment constraints limit the naive applicability of machine learning to this problem. First, the amount and the dimensionality of the data make in-memory learning simply not possible. Second, the drift of customers' preference over time require to retrain the ranking model regularly with freshly collected data. This limits the time that is available for training to prohibitively short intervals. Third, ranking in real-time is necessary whenever the query complexity prevents us from caching the predictions. This constraint requires to minimize prediction time (or equivalently maximize the data throughput), which in turn may prevent us from achieving the accuracy necessary in web-scale industrial applications. In this paper, we investigate how the practical challenges faced in this setting can be tackled via an online learning to rank approach. Sparse models will be the key to reduce prediction latency, whereas one-pass stochastic optimization will minimize the training time and restrict the memory footprint. Interestingly, and perhaps surprisingly, extensive experiments show that one-pass learning preserves most of the predictive performance. Additionally, we study a variety of online learning algorithms that enforce sparsity and provide insights to help the practitioner make an informed decision about which approach to pick. We report results on a massive purchase log dataset from the Amazon retail website, as well as on several benchmarks from the LETOR corpus.",project-academic
10.1109/VTCFALL.2018.8691023,2018-07-02,p,Institute of Electrical and Electronics Engineers Inc.,deep learning for super resolution doa estimation in massive mimo systems," The requirement of the increasing capacity of the communication networks promotes the massive multiple input multiple output (MIMO), which has attracted a lot of attention among academic and industry communities. Due to the inherent sparsity features of channel structure in uplink massive MIMO systems, conventional methods often bring about high computational complexity and also fail to make full use of the structural information. In order to solve this problem, this paper proposes a novel deep learning (DL) based super-resolution direction of arrivals (DOA) estimation method. Specifically, it is realized with the aids of the well-designed deep neural network (DNN). Then we employ the DNN to carry out offline learning and online deployment procedures. This learning mechanism can learn the features of the wireless channel and the spacial structures efficiently. Finally, simulation results are provided to show that the proposed DL based scheme can achieve better performance in terms of the DOA estimation compared with conventional methods.",project-academic
10.1109/DTPI52967.2021.9540195,2021-07-15,p,IEEE,parallel mining operating systems from digital twins to mining intelligence," With the rapid development and modernization requirement of global coal industry, there is an emerging need for intelligent and unmanned mining systems. In this paper, the Intelligent Mining Operating System (IMOS) is proposed and developed, based on the parallel management and control of mining operating infrastructure that integrates the intelligent mining theory, the ACP-based (Artificial societies, Computational experiments, Parallel execution) parallel intelligence approaches, and the new generation of artificial intelligence (AI) technologies. To satisfy the intelligent and unmanned demand of open-pit mines, the IMOS architecture is developed by integrating the theory of digital quadruplets. The main subsystems and functions of IMOS are elaborated in detail, including a single-vehicle operating subsystem, multi-vehicle collaboration subsystem, vehicle-road collaboration subsystem, unmanned intelligent subsystem, dispatch management subsystem, parallel management and control subsystem, supervisory subsystem, remote takeover subsystem, and communication subsystem. The IMOS presented in this paper is the first integrated solution for intelligent and unmanned mines in China, and has been implemented over ten main open pits in the past few years. Its deployment and utilization will effectively improve the production efficiency and safety level of open-pit mines, promote the construction of ecological mines, and bring great significance to the realization of sustainable mining development.",project-academic
,2020-08-26,a,,at your service coffee beans recommendation from a robot assistant," With advances in the field of machine learning, precisely algorithms for recommendation systems, robot assistants are envisioned to become more present in the hospitality industry. Additionally, the COVID-19 pandemic has also highlighted the need to have more service robots in our everyday lives, to minimise the risk of human to-human transmission. One such example would be coffee shops, which have become intrinsic to our everyday lives. However, serving an excellent cup of coffee is not a trivial feat as a coffee blend typically comprises rich aromas, indulgent and unique flavours and a lingering aftertaste. Our work addresses this by proposing a computational model which recommends optimal coffee beans resulting from the user's preferences. Specifically, given a set of coffee bean properties (objective features), we apply different supervised learning techniques to predict coffee qualities (subjective features). We then consider an unsupervised learning method to analyse the relationship between coffee beans in the subjective feature space. Evaluated on a real coffee beans dataset based on digitised reviews, our results illustrate that the proposed computational model gives up to 92.7 percent recommendation accuracy for coffee beans prediction. From this, we propose how this computational model can be deployed on a service robot to reliably predict customers' coffee bean preferences, starting from the user inputting their coffee preferences to the robot recommending the coffee beans that best meet the user's likings.",project-academic
10.1016/J.ENGAPPAI.2019.03.011,2019-05-01,a,Pergamon,distributed parallel deep learning of hierarchical extreme learning machine for multimode quality prediction with big process data," Abstract None None In this work, the distributed and parallel Extreme Learning Machine (dp-ELM) and Hierarchical Extreme Learning Machine (dp-HELM) are proposed for multimode process quality prediction with big data. The efficient ELM algorithm is transformed into the distributed and parallel modeling form according to the MapReduce framework. Since the deep learning network structure of HELM is more accurate than the single layer of ELM in feature representation, the dp-HELM is further developed through decomposing the ELM-based Auto-encoders (ELM-AE) of deep hidden layers into a loop of MapReduce jobs. Additionally, the multimode issue is solved through the “divide and rule” strategy. The distributed and parallel K-means (dp-K-means) is utilized to divide the process modes, which are further trained in a synchronous parallel way by dp-ELM and dp-HELM. Finally, the Bayesian model fusion technique is utilized to integrate the local models for online prediction. The proposed algorithms are deployed on a Hadoop MapReduce computing cluster and the feasibility and efficiency are illustrated through building a real industrial quality prediction model with big process data.",project-academic
10.1016/J.PROCS.2021.02.026,2021-01-01,a,Elsevier,ddnn based data allocation method for iiot," Abstract None None With the complete application of artificial intelligence in the field of industrial production and manufacturing and the rapid development of edge computing, industrial processing sites often need to deploy machine learning tasks at edges and terminals. We propose a data allocation method based on Distributed Deep Neural Networks (DDNN) framework, which allocates data to edge servers or stays locally for processing. DDNN divides deep learning tasks and deploys pre-trained shallow neural networks and deep neural networks at local or edges, respectively. However, all data is processed locally, and the failure is sent to the edge server or the cloud. It will lead to excessive pressure on local terminal equipment and long-term idle edge servers, which cannot meet industrial production’s real-time requirements on user privacy and time-sensitive tasks. In this paper, the complexity and inference error rate of machine learning model, the data processing speed of local equipment and edge server, and the transmission time are comprehensively considered to establish the system model. A joint optimization problem is proposed to minimize the total data processing delay. The optimal solution is derived analytically, and the optimal data allocation methhod is given. Simulation experiments are designed to verify the method’s effectiveness and study the influence of key parameters on the allocation method.",project-academic
10.1109/INDIN45523.2021.9557389,2021-07-21,p,IEEE,learning based edge computing architecture for regional scheduling in manufacturing system," This paper proposes a novel edge-computing based structure to support learning-based decision-making in industry manufacturing field. This structure consists of four functional layers, respectively realizing model establishment, task allocation and task processing work. In order to take full advantage of the distributed computing resources at the edge, the manufacturing computing task can be further decomposed into several sub-tasks, separating the complex computing problem with large problem size into regional scheduling ones with much smaller problem size. All the sub-tasks are allocated to the edges, accomplished by the algorithm deployed on computing devices of region-related edge node, which contributes to faster data-processing and problem-solving speed. A simulation test has been performed in which a multi-AGV scheduling problem was solved according to a distributed reinforcement learning method configured in such edge computing architecture. The objective of each edge node is to acquire AGV schedule of related region that minimizes the makespan. Simulation results demonstrate that this distributed edge computing system can be enabled to learn satisfying solution and converge much faster when it is compared with conventional method applied in centralized architecture.",project-academic
,2017-09-26,,,safety pre warning method and system for industrial control network," The invention relates to a safety pre-warning method and system for an industrial control network. The safety pre-warning method for the industrial control network comprises steps as follows: acquiring safety related data of a control event of the industrial control network; performing CEP (complex event processing) analysis on the safety related data; adopting a deep learning based classifier to perform control event risk identification on the data subjected to CEP analysis. The safety pre-warning method and system for the industrial control network can be deployed in the resource-constrained industrial control field and are applicable to a single-node or multi-node deployment environment, and the deployment way is more flexible. Furthermore, according to the method deployed in the industrial control field, network delay is reduced, processing response speed of real-time data is increased, and risk pre-warning capacity is improved. Besides, the deep learning based classifier is adopted for risk identification, accuracy rate of identification is higher, and efficiency of risk identification is substantially improved through combination of CEP analysis and deep learning scheme.",project-academic
,2020-12-06,a,,coedge cooperative dnn inference with adaptive workload partitioning over heterogeneous edge devices," Recent advances in artificial intelligence have driven increasing intelligent applications at the network edge, such as smart home, smart factory, and smart city. To deploy computationally intensive Deep Neural Networks (DNNs) on resource-constrained edge devices, traditional approaches have relied on either offloading workload to the remote cloud or optimizing computation at the end device locally. However, the cloud-assisted approaches suffer from the unreliable and delay-significant wide-area network, and the local computing approaches are limited by the constrained computing capability. Towards high-performance edge intelligence, the cooperative execution mechanism offers a new paradigm, which has attracted growing research interest recently. In this paper, we propose CoEdge, a distributed DNN computing system that orchestrates cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes available computation and communication resources at the edge and dynamically partitions the DNN inference workload adaptive to devices' computing capabilities and network conditions. Experimental evaluations based on a realistic prototype show that CoEdge outperforms status-quo approaches in saving energy with close inference latency, achieving up to 25.5%~66.9% energy reduction for four widely-adopted CNN models.",project-academic
10.1109/TNET.2020.3042320,2021-04-01,a,IEEE,coedge cooperative dnn inference with adaptive workload partitioning over heterogeneous edge devices," Recent advances in artificial intelligence have driven increasing intelligent applications at the network edge, such as smart home, smart factory, and smart city. To deploy computationally intensive Deep Neural Networks (DNNs) on resource-constrained edge devices, traditional approaches have relied on either offloading workload to the remote cloud or optimizing computation at the end device locally. However, the cloud-assisted approaches suffer from the unreliable and delay-significant wide-area network, and the local computing approaches are limited by the constrained computing capability. Towards high-performance edge intelligence, the cooperative execution mechanism offers a new paradigm, which has attracted growing research interest recently. In this paper, we propose CoEdge, a distributed DNN computing system that orchestrates cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes available computation and communication resources at the edge and dynamically partitions the DNN inference workload adaptive to devices’ computing capabilities and network conditions. Experimental evaluations based on a realistic prototype show that CoEdge outperforms status-quo approaches in saving energy with close inference latency, achieving up to 25.5% ~ 66.9% energy reduction for four widely-adopted CNN models.",project-academic
10.1007/978-3-030-66218-9_55,2021-01-01,a,"Springer, Cham",edge intelligence a robust reinforcement of edge computing and artificial intelligence," Due to the development of faster and improved modes of communication, technologies, as well as customers, get benefited. The world is moving rapidly toward digitization, and connectivity has been placed under tremendous pressure. The development and implementation of IoT devices benefit all industry sectors, stimulating more areas in terms of convenience, productivity, and communication. But such a huge amount of data generated by IoT devices could result in a breakdown of IT infrastructure. To reach to the desired destination, this massive data travels via some intermediator. When the so-called intermediator that is cloud database is based in a remote location, the data can experience some kind of delay before it reaches the cloud for processing. So in recent years, the IT industry is attracted tremendous attention to improving communication between these technologies. And this is what the aim of edge computing (EC) is. Meanwhile also artificial intelligence (AI) algorithms and models have made breakthrough progress to accelerate the successful deployment of intelligence in the cloud services. By and large, AI services are executed in cloud for dealing with demands, because of the way that most AI models are intricate and difficult to process their induction results in favor of resource-limited devices. Nonetheless, such sort of ‘end–cloud’ architecture cannot address the issues of real-time AI services such as real-time analytics and smart manufacturing. Accordingly, deploying AI applications on the edge can widen the application situations of AI especially as for the low-latency characteristic. Combining the above two-mentioned paradigms, i.e., EC and AI can give rise to a new outlook: Edge Intelligence (EI). This paper provides insights into this new outlook by discussing core definitions, concepts, components, and frameworks. It also describes some necessary background in future research areas and challenges.",project-academic
10.1109/ICST46873.2019.9047745,2019-12-01,p,Institute of Electrical and Electronics Engineers (IEEE),towards machine learning enabled security framework for iot based healthcare," The recent developments in electronic and communication technologies have brought notable revolution in the e-healthcare industry for efficient transmission of the patient's data. One of the emergent applications of telehealth monitoring is the Internet of medical things (IoMTs). They are used to transfer and monitor medical information in patient-centred systems. Patient's data is very critical, so its secure transmission is of paramount requirement in smart healthcare applications. The current era has witnessed the large-scale usage of cryptographic and biometric systems, and machine learning (ML) approaches for authentication and anomaly detection, respectively, for securing medical systems. In IoMTs, sensor devices have limited power and battery, so to provide a balance between security and resource-efficiency is also an important aspect to consider during deploying IoMT. Therefore, this research aims to present an innovate framework to protect medical information from external threats with the consumption of less possible resources of low-powered medical devices. In this study, the ML-based biometric security framework is proposed in which features are extracted from Electrocardiogram (ECG) signals for the training phase. However, in the testing phase, the user authentication will be verified by utilizing generated unique biometric EIs from the ECG and acquired coefficients from polynomial approximation. The proposed framework has got the scientific as well as economic significance; thus, it could be used for real-time healthcare applications.",project-academic
10.1007/978-3-642-25535-9_31,2011-12-05,p,"Springer, Berlin, Heidelberg",goal driven business process derivation," Solutions to the problem of deriving business processes from goals are critical in addressing a variety of challenges facing the services and business process management community, and in particular, the challenge of quickly generating large numbers of effective process designs (often a bottleneck in industry-scale deployment of BPM). The problem is similar to the planning problem that has been extensively studied in the artificial intelligence (AI) community. However, the direct application of AI planning techniques places an onerous burden on the analyst, and has proven to be difficult in practice. We propose a practical yet rigorous (semi-automated) algorithm for business process derivation from goals. Our approach relies on being able to decompose process goals to a more refined collection of sub-goals whose ontology is aligned with that of the effects of available tasks which can be used to construct the business process. Once process goals are refined to this level, we are able to generate a process design using a procedure that leverages our earlier work on semantic effect annotation of process designs. We illustrate our ideas throughout this paper with a real-life running example, and also present a proof-of-concept prototype implementation.",project-academic
10.1002/MP.14198,2019-07-24,a,,development of a real time indoor location system using bluetooth low energy technology and deep learning to facilitate clinical applications," An indoor, real-time location system (RTLS) can benefit both hospitals and patients by improving clinical efficiency through data-driven optimization of procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy and robustness because Bluetooth signal strength is subject to fluctuation. We developed a machine learning-based solution using a Long Short-Term Memory (LSTM) network followed by a Multilayer Perceptron classifier and a posterior constraint algorithm to improve RTLS performance. Training and validation datasets showed that most machine learning models perform well in classifying individual location zones, although LSTM was most reliable. However, when faced with data indicating cross-zone trajectories, all models showed erratic zone switching. Thus, we implemented a history-based posterior constraint algorithm to reduce the variability in exchange for a slight decrease in responsiveness. This network increases robustness at the expense of latency. When latency is less of a concern, we computed the latency-corrected accuracy which is 100% for our testing data, significantly improved from LSTM without constraint which is 96.2%. The balance between robustness and responsiveness can be considered and adjusted on a case-by-case basis, according to the specific needs of downstream clinical applications. This system was deployed and validated in an academic medical center. Industry best practices enabled system scaling without substantial compromises to performance or cost.",project-academic
10.1016/J.PROMFG.2020.04.017,2020-01-01,a,Elsevier,ppe compliance detection using artificial intelligence in learning factories," Abstract None None This project demonstrates the application of Artificial Intelligence (AI) and machine vision for the identification of Personal Protective Equipment (PPE), particularly safety glasses in zones of the Learning Factory, where safety risks exist. The objective is to design and implement an automated system for ensuring the safety of personnel when they are in the vicinity of machinery that presents potential risks to the eyes. Microsoft Azure Custom Vision AI and Intelligent AI Services, in conjunction with low-cost vision devices with lightweight onboard AI capability, provide a platform for a deep learning neural network model using publicly available images under the Creative Commons License. A combination of cloud-based and on-premises AI is used in this proof of concept system to provide a real-time vision-based safety system capable of detecting and recording potential safety breaches, promoting compliance, and ultimately preventing accidents before they happen. This system can be used to initiate different control actions in the event of safety violations and can detect multiple forms of protective wear. The flexibility of the system offers multiple benefits to learning factories and manufacturing organizations such as improved user safety, reduced insurance costs, and better detection and recording of safety violations. The hybrid AI architecture approach allows for flexibility in training and deployment based on the capability of local computing resources.",project-academic
10.1155/2021/4027900,2021-08-18,a,Hindawi Limited,intrusion detection for industrial control systems based on open set artificial neural network," The security of industrial control systems (ICSs) has received a lot of attention in recent years. ICSs were once closed networks. But with the development of IT technologies, ICSs have become connected to the Internet, increasing the potential of cyberattacks. Because ICSs are so tightly linked to human lives, any harm to them could have disastrous implications. As a technique of providing protection, many intrusion detection system (IDS) studies have been conducted. However, because of the complicated network environment and rising means of attack, it is difficult to cover all attack classes, most of the existing classification techniques are hard to deploy in a real environment since they cannot deal with the open set problem. We propose a novel artificial neural network based-methodology to solve this problem. Our suggested method can classify known classes while also detecting unknown classes. We conduct research from two points of view. On the one hand, we use the openmax layer instead of the traditional softmax layer. Openmax overcomes the limitations of softmax, allowing neural networks to detect unknown attack classes. During training, on the other hand, a new loss function termed center loss is implemented to improve detection ability. The neural network model learns better feature representations with the combined supervision of center loss and softmax loss. We evaluate the neural network on NF-BoT-IoT-v2 and Gas Pipeline datasets. The experiments show our proposed method is comparable with the state-of-the-art algorithm in terms of detecting unknown classes. But our method has a better overall classification performance.",project-academic
10.1109/TASE.2019.2909043,2020-01-01,a,Institute of Electrical and Electronics Engineers (IEEE),rfid driven energy efficient control approach of cnc machine tools using deep belief networks," Under the consideration of massive energy consumption of machine tools, many approaches have been proposed, and state control method of machine tools has proved its effectiveness. In order to satisfy the demand of real-time production control, a deep learning methodology for energy-efficient control of CNC machine tools is proposed in RFID-enabled ubiquitous environment. First, the energy-efficient control strategies for multiple machine tools are proposed to reduce the carbon emission of the machining process. Then, through evaluating the process progress in the RFID-enabled environment, a deep learning methodology for energy-efficient strategies selection of CNC machine tools using deep belief networks (DBNs) is established to realize the real-time and accurate control of machine tools. Finally, comparisons between the proposed approach and some state-of-the-art ones are given, and the experiment results indicate that the proposed method is effective and efficient for the energy-efficient control problem of machine tools. The proposed method can realize the real-time control of CNC machine tools based on the interaction information in Industrial 4.0. Furthermore, the machine tools will be converted to smart machines, which can complete self-perception and self-adjustment automatically. None Note to Practitioners —It is significant but challenging work to realize the control of manufacturing processes based on real-time production data. Thus, this paper integrates RFID data of jobs with energy-efficient control of CNC machine tools, and proposes a deep learning methodology of processing the real-time production data in an RFID-enabled ubiquitous environment. Considering the relationship between different jobs, five energy-efficient control strategies for multiple machine tools are put forward to reduce the carbon emission of the machining processes. Then, an RFID-driven process progress evaluation is carried out to quantify the real-time progress, and two RFID data preprocessing algorithms are developed to cleanse and extract the original data. DBNs are adopted to realize the energy-efficient strategies selection of CNC machine tools. The experiments from an actual printing machine manufacturing enterprise indicate that the proposed method is effective and efficient for the energy-efficient control problem of machine tools. The implementation requires RFID-enabled manufacturing environment deployment, RFID data capturing and prepressing, and deep learning model establishment based on historical production data. Beyond the energy conservation of machine tools, the proposed method can also be applied to other industrial problems, e.g., self-perception and self-adjustment of smart machine tools, production rescheduling decisions, and logistics routing optimization.",project-academic
10.1109/NETSOFT48620.2020.9165393,2020-06-01,p,IEEE,benchmarking and profiling 5g verticals applications an industrial iot use case," The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",project-academic
10.1016/J.PROMFG.2020.01.031,2019-01-01,a,Elsevier BV,a deep learning approach for anomaly detection with industrial time series data a refrigerators manufacturing case study," Abstract None None In refrigerators production, vacuum creation is fundamental to guarantee the correct manufacturing of the product. Before inserting the refrigerant in the refrigerator cabinet, the vacuum is tested through a Pirani gauge that assesses the pressure within the cabinet. Such readings are used to evaluate the vacuum creation process and to verify if leakings are present. In this work, we employ a Deep Learning-based Anomaly Detection approach to associate an Anomaly Score to each pressure profile; this score can be exploited to optimize actions performed by human operators like more detailed inspections or unit exclusion from the downstream production stages. We propose a native time series-based approach based on Deep Learning and compare it with classic ones based on hand-craft features. The proposed approach is designed to be deployed in a Decision Support System for assisting human operators in the following testing operations, helping them in reducing evaluation bias and attention losses that are inevitable in production line environment. Moreover, costs associated with false positives (normally operating units detected as anomalous) and false negatives (undetected anomalies) are considered here to optimize decision making in a cost-reduction perspective. We also describe promising results obtained on real industrial data spanning on a 5-month period and consisting of thousands of tested household units.",project-academic
10.1016/J.BUILDENV.2020.107212,2020-10-01,a,Pergamon,anomaly detection based on machine learning in iot based vertical plant wall for indoor climate control," Abstract None None Indoor climate is closely related to human health, comfort and productivity. Vertical plant wall systems, embedded with sensors and actuators, have become a promising application for indoor climate control. In this study, we explore the possibility of applying machine learning based anomaly detection methods to vertical plant wall systems so as to enhance the automation and improve the intelligence to realize predictive maintenance of the indoor climate. Two categories of anomalies, namely point anomalies and contextual anomalies are researched. Prediction-based and pattern recognition-based methods are investigated and applied to indoor climate anomaly detection. The results show that neural network-based models, specifically the autoencoder (AE) and the long short-term memory encoder decoder (LSTM-ED) model surpass the others in terms of detecting point anomalies and contextual anomalies, respectively, therefore can be deployed into vertical plant walls systems in industrial practice. Based on the results, a new data cleaning method is proposed and a prediction-based method is deployed to the cloud in practice as a proof-of-concept. This study showcases the advancements in machine learning and Internet of things can be fully utilized by researches on building environment to accelerate the solution development.",project-academic
10.1016/J.AEI.2019.101013,2020-01-01,a,Elsevier,guidelines for applied machine learning in construction industry a case of profit margins estimation," Abstract None None The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects.",project-academic
10.1109/NOMS.1998.655214,1998-02-15,p,IEEE,alarm correlation engine ace," Networks are growing in size and complexity, resulting in increased alarm volume and number of unfamiliar alarms. Often, there is no proportional increase in monitoring personnel and response time to faults suffers. GTE deployed Telephone Operations Network Integrated Control System (TONICS) in 1993 to support its network management operations. To stay competitive in the face of continued staff reductions, increase in network size, and monitoring complications related to deregulation of the telephone industry, GTE is introducing artificial intelligence techniques into TONICS. Alarm Correlation Engine (ACE), the system described in this paper, is part of the effort. ACE aids network management by correlating alarms on the basis of common cause to provide alarm compression, filtering, and suppression. In conjunction with its ability to carry out prescribed responses, it improves response times and increases productivity. ACE was developed with the following requirements: reliability, speed, versatility (handle alarms from different switches and networks), ease of knowledge engineering (field technicians must be able to construct, test, and modify correlation patterns), handle in real time multiple network problems, and finally, interface smoothly with GTE's TONICS system. ACE's strength lies in its domain specific correlation language which facilitates knowledge engineering and in its asynchronous processing core that enables integration into a real-time monitoring system.",project-academic
10.1109/JSTARS.2015.2442584,2016-05-01,a,IEEE,estimation of seismic vulnerability levels of urban structures with multisensor remote sensing," The ongoing global transformation of human habitats from rural villages to ever growing urban agglomerations induces unprecedented seismic risks in earthquake prone regions. To mitigate affiliated perils requires the seismic assessment of built environments. Numerous studies emphasize that remote sensing can play a valuable role in supporting the extraction of relevant features for preevent vulnerability analysis. However, the majority of approaches operate on building level. This induces the deployment of very high spatial resolution remote sensing data, which hampers, nowadays, utilization capabilities for larger areas due to data costs and processing requirements. In this paper, we alter the spatial scale of analysis and propose concepts and methods to estimate the seismic vulnerability level of homogeneous urban structures. A procedure is designed, which comprises four main steps dedicated to: 1) delineation of urban structures by means of a tailored unsupervised data segmentation procedure with scale optimization; 2) characterization of urban structures by a joint exploitation of multisensor data; 3) selection of most feasible features under consideration of None in situ None vulnerability information; and 4) estimation of seismic vulnerability levels of urban structures within a supervised learning framework. We render the prediction problem in three ways to address operational requirements that can evolve in real-life situations. 1) To discriminate two or more classes based on labeled samples of all classes present in the data under investigation, we use the framework of soft margin support vector machines ( C -SVM). 2) To consider situations, where solely labeled samples are available for the class(es) of interest and not for all classes present in the data, we deploy ensembles of None None $\nu$ None -one-class SVM ( None $\nu $ None -OC-SVM). and 3) To fit data with a higher statistical level of measurement (interval or ratio scale), we utilize a support vector regression (SVR) approach to estimate a regression function from the training samples. Experimental results are obtained for the earthquake-prone mega city Istanbul, Turkey. We use multispectral data from the RapidEye constellation, elevation measurements from the TanDEM-X mission, and spatiotemporal analyses based on data from the Landsat archive to characterize the urban environment. In addition, different None in situ None data sets are incorporated for Istanbul’s district Zeytinburnu and the residual settlement area of Istanbul. When estimating damage grades for Zeytinburnu with SVR, best models are characterized by mean absolute percentage errors less than 11%, and fairly strong goodness of fit ( None ${{R}} > {{0}}.{{75}}$ None ). When aiming to identify different types of urban structures for the remaining settlement area of Istanbul (i.e., urban structures determined by large industrial/commercial buildings and tall detached residential buildings, which can be considered here as highly and slightly vulnerable, respectively), results obtained with None None $C$ None -SVM show a distinctive increase of accuracy compared to results obtained with ensembles of None None $\nu$ None -OC-SVM. The latter were not able to exceed moderate agreements, with None None $\kappa$ None None statistics slightly above 0.45. Instead, None None $C$ None -SVM allowed obtaining None None $\kappa $ None None statistics expressing substantial and even excellent agreements ( None $\kappa > {{0}}.{{6}}$ None None up to None None $\kappa > {{0}}.{{8}}$ None ). Overall, analyzes provide very promising empirical evidence, which confirms the potential of remote sensing to support seismic vulnerability assessment.",project-academic
10.1109/MOBSERV.2016.24,2016-06-01,p,,real time privacy preserving crowd estimation based on sensor data," As one of the popular topics to ensure public safety, crowd estimation has attracted lots of attentions from both industry and academia. Most of traditional crowd estimation approaches rely on sophisticated computer vision algorithms to estimate crowd based on camera data, therefore suffering from privacy issues and high deployment and data processing cost. In this paper we present a sensor fusion based approach to real-time crowd estimation based on privacy-conscious and inexpensive sensors. The approach has been implemented and verified first by a small scale deployment at our lab, and then tested based on a 3-month trial at a shopping mall in Singapore. A deep analysis has been carried out based on the data sets collected from the trial, showing promising results: 1) the data from CO2, sound pressure and infrared sensors are influential in estimating crowd levels for indoor environments, 2) Random Forest and C4.5 are identified as the more suitable supervised learning models, 3) an accuracy of 95% can be achieved by our crowd estimation system in a real scenario. In contrast to the state of the art, our approach is privacy preserving and can provide comparable estimation accuracy with lower deployment and processing cost and better applicability for large scale setups. It can be used either as an alternative solution when user privacy must be enforced or as a complementary solution to camera-based crowd estimation when privacy is less concerned because of pubic safety.",project-academic
10.1109/JPROC.2020.3034519,2021-04-01,a,NIH Public Access,six sigma quality management of additive manufacturing," Quality is a key determinant in deploying new processes, products, or services and influences the adoption of emerging manufacturing technologies. The advent of additive manufacturing (AM) as a manufacturing process has the potential to revolutionize a host of enterprise-related functions from production to the supply chain. The unprecedented level of design flexibility and expanded functionality offered by AM, coupled with greatly reduced lead times, can potentially pave the way for mass customization. However, widespread application of AM is currently hampered by technical challenges in process repeatability and quality management. The breakthrough effect of six sigma (6S) has been demonstrated in traditional manufacturing industries (e.g., semiconductor and automotive industries) in the context of quality planning, control, and improvement through the intensive use of data, statistics, and optimization. 6S entails a data-driven DMAIC methodology of five steps—define, measure, analyze, improve, and control. Notwithstanding the sustained successes of the 6S knowledge body in a variety of established industries ranging from manufacturing, healthcare, logistics, and beyond, there is a dearth of concentrated application of 6S quality management approaches in the context of AM. In this article, we propose to design, develop, and implement the new DMAIC methodology for the 6S quality management of AM. First, we define the specific quality challenges arising from AM layerwise fabrication and mass customization (even one-of-a-kind production). Second, we present a review of AM metrology and sensing techniques, from materials through design, process, and environment, to postbuild inspection. Third, we contextualize a framework for realizing the full potential of data from AM systems and emphasize the need for analytical methods and tools. We propose and delineate the utility of new data-driven analytical methods, including deep learning, machine learning, and network science, to characterize and model the interrelationships between engineering design, machine setting, process variability, and final build quality. Fourth, we present the methodologies of ontology analytics, design of experiments (DOE), and simulation analysis for AM system improvements. In closing, new process control approaches are discussed to optimize the action plans, once an anomaly is detected, with specific consideration of lead time and energy consumption. We posit that this work will catalyze more in-depth investigations and multidisciplinary research efforts to accelerate the application of 6S quality management in AM.",project-academic
10.1016/J.AEI.2020.101101,2020-08-01,a,Elsevier,predictive model based quality inspection using machine learning and edge cloud computing," Abstract None None The supply of defect-free, high-quality products is an important success factor for the long-term competitiveness of manufacturing companies. Despite the increasing challenges of rising product variety and complexity and the necessity of economic manufacturing, a comprehensive and reliable quality inspection is often indispensable. In consequence, high inspection volumes turn inspection processes into manufacturing bottlenecks. None In this contribution, we investigate a new integrated solution of predictive model-based quality inspection in industrial manufacturing by utilizing Machine Learning techniques and Edge Cloud Computing technology. In contrast to state-of-the-art contributions, we propose a holistic approach comprising the target-oriented data acquisition and processing, modelling and model deployment as well as the technological implementation in the existing IT plant infrastructure. A real industrial use case in SMT manufacturing is presented to underline the procedure and benefits of the proposed method. The results show that by employing the proposed method, inspection volumes can be reduced significantly and thus economic advantages can be generated.",project-academic
10.1109/ISSCC19947.2020.9063110,2020-02-01,p,Institute of Electrical and Electronics Engineers Inc.,22 1 a 1 1v 16gb 640gb s hbm2e dram with a data bus window extension technique and a synergetic on die ecc scheme," Rapidly evolving artificial intelligence (Al) technology, such as deep learning, has been successfully deployed in various applications: such as image recognition, health care, and autonomous driving. Such rapid evolution and successful deployment of Al technology have been possible owing to the emergence of accelerators, such as GPUs and TPUs, that have a higher data throughput. This, in turn, requires an enhanced memory system with large capacity and high bandwidth [1]; HBM has been the most preferred high-bandwidth memory technology due to its high-speed and low-power characteristics, and 1024 IOs facilitated by 2.5D silicon interposer technology, as well as large capacity realized by through-silicon via (TSV) stack technology [2]. Previous-generation HBM2 supports 8GB capacity with a stack of 8 DRAM dies (i.e., 8-high stack) and 341GB/s (2.7Gb/s/pin) bandwidth [3]. The HBM industry trend has been a speed improvement of 15~20% every year, while capacity increases by 1.5-2x every two years. In this paper, we present a 16GB HBM2E with circuit and design techniques to increase its bandwidth up to 640GB/s (5Gb/s/pin), while providing stable bit-cell operation in the 2nd generation of a 10nm DRAM process: featuring (1) a data-bus window-extension technique to cope with reduced None $t_{cco}$ , (2) a power delivery network (PDN) designed for stable operation at a high speed, (3) a synergetic on-die ECC scheme to reliably provide large capacity, and (4) an MBIST solution to efficiently test large capacity memory at a high speed.",project-academic
10.1109/SOUTHEASTCON44009.2020.9368285,2020-03-28,p,IEEE,uav aided search and rescue operation using reinforcement learning," Owing to the enhanced flexibility in deployment and decreasing costs of manufacturing, the demand for unmanned aerial vehicles (UAVs) is expected to soar in the upcoming years. In this paper, we explore a UAV aided search and rescue (SAR) operation in indoor environments, where the GPS signals might not be reliable. We consider a SAR scenario where the UAV tries to locate a victim trapped in an indoor environment by sensing the RF signals emitted from a smart device owned by the victim. To locate the victim as fast as possible, we leverage tools from reinforcement learning (RL). Received signal strength (RSS) at the UAV depends on the distance from the source, indoor shadowing and fading parameters, and antenna radiation pattern of the receiver mounted on the UAV. To make our analysis more realistic, we model two indoor scenarios with different dimensions using a commercial ray tracing software. Then, the corresponding RSS values at each possible discrete UAV location are extracted and used in a Q-learning framework. Unlike the traditional location-based navigation approach that exploits GPS coordinates, our method uses the RSS to define the states and rewards of the RL algorithm. We compare the performance of the proposed method where directional and omnidirectional antennas are used. The results reveal that the use of directional antennas provides faster convergence rates than the omnidirectional antennas.",project-academic
,2020-02-19,a,,uav aided search and rescue operation using reinforcement learning," Owing to the enhanced flexibility in deployment and decreasing costs of manufacturing, the demand for unmanned aerial vehicles (UAVs) is expected to soar in the upcoming years. In this paper, we explore a UAV aided search and rescue~(SAR) operation in indoor environments, where the GPS signals might not be reliable. We consider a SAR scenario where the UAV tries to locate a victim trapped in an indoor environment by sensing the RF signals emitted from a smart device owned by the victim. To locate the victim as fast as possible, we leverage tools from reinforcement learning~(RL). Received signal strength~(RSS) at the UAV depends on the distance from the source, indoor shadowing, and fading parameters, and antenna radiation pattern of the receiver mounted on the UAV. To make our analysis more realistic, we model two indoor scenarios with different dimensions using commercial ray-tracing software. Then, the corresponding RSS values at each possible discrete UAV location are extracted and used in a Q-learning framework. Unlike the traditional location-based navigation approach that exploits GPS coordinates, our method uses the RSS to define the states and rewards of the RL algorithm. We compare the performance of the proposed method where directional and omnidirectional antennas are used. The results reveal that the use of directional antennas provides faster convergence rates than the omnidirectional antennas.",project-academic
10.1109/PERCOMW.2018.8480343,2018-03-01,p,IEEE,development of energy efficient sensor networks by minimizing sensors numbers with a machine learning model," With the increasing demand to construct sensor networks for a smart IoT (Internet of Things) world, numerous sensors with sensing and communication capabilities are expected to be deployed in the future. Thanks to the development of hardware manufacture technology, relatively small IoT smart sensors are now commercially available and cost-effective. However, the total power required by operating these sensors is expected to be enormous, due to their large number and frequent activity. Removing “unneeded sensors” is the most direct way to reduce the power consumption of sensor networks. Here, “unneeded sensors” refers to those that can be placed in sleep mode, or even be removed from the network topology entirely, without serious impact on the overall networks data processing performance. In this paper, we report the development of an energy-efficient sensor network by using a machine learning model to determine the actual necessity of all the sensors in a sensor network. Machine learning model is introduced to identify unneeded sensors by comparing the data from neighboring sensors to that from the potentially unneeded ones. For identifying unneeded sensors, different strategies with different computational complexity are also proposed. Numerical experiments conducted in two real indoor environments verify that our proposed scheme can reduce the total number of active sensors by around 1/3, while maintaining more than 90% of the original high monitoring performance of the sensor network.",project-academic
10.1007/S11432-019-9932-3,2019-09-19,a,Science China Press,accelerating dnn based 3d point cloud processing for mobile computing," 3D point cloud data, which are produced by various 3D sensors such as LIDAR and stereo cameras, have been widely deployed by industry leaders such as Google, Uber, Tesla, and Mobileye, for mobile robotic applications such as autonomous driving and humanoid robots. Point cloud data, which are composed of reliable depth information, can provide accurate location and shape characteristics for scene understanding, such as object recognition and semantic segmentation. However, deep neural networks (DNNs), which directly consume point cloud data, are particularly computation-intensive because they have to not only perform multiplication-and-accumulation (MAC) operations but also search neighbors from the irregular 3D point cloud data. Such a task goes beyond the capabilities of general-purpose processors in real-time to figure out the solution as the scales of both point cloud data and DNNs increase from application to application. We present the first accelerator architecture that dynamically configures the hardware on-the-fly to match the computation of both neighbor point search and MAC computation for point-based DNNs. To facilitate the process of neighbor point search and reduce the computation costs, a grid-based algorithm is introduced to search neighbor points from a local region of grids. Evaluation results based on the scene recognition and segmentation tasks show that the proposed design harvests 16.4$\times$ higher performance and saves 99.95% of energy than an NVIDIA Tesla K40 GPU baseline in point cloud scene understanding applications.",project-academic
10.1109/ICDMW.2019.00119,2019-11-01,p,IEEE,data driven insights from predictive analytics on heterogeneous experimental data of industrial magnetic materials," Data-driven methods are becoming increasingly popular in the field of materials science. While most data-driven models are trained on simulation data as it is relatively easier to collect a large amount of data from physics-based simulations, there are many challenges in applying data-driven methods on experiments: 1) experimental data is usually not clean; and 2) it generally has a greater degree of heterogeneity. In this project, we have developed a data-driven methodology to address these challenges on an industrial magnet dataset, where the goal is to predict magnetic properties (forward models) at different stages of the experimental workflow. The data-driven methodology consists of data cleaning, data preprocessing, feature extraction, and model development using traditional machine learning and deep learning methods to accurately predict magnet properties. In particular, we have developed three different types of predictive models: 1) numerical model using only numerical data containing composition and processing information; 2) image model using image data representing structure information; and 3) combination model using both types of data together. In addition to predictive models, the analysis and comparison of results across the models provide several interesting data-driven insights. Such data-driven analytics has the potential to help guide future experiments and realize the inverse models, which could significantly reduce costs and accelerate the discovery of new magnets with superior properties. The proposed models are already deployed in Toyota Motor Corporation.",project-academic
10.1109/ACCESS.2020.2991959,2020-05-06,a,Institute of Electrical and Electronics Engineers (IEEE),syntheticnet a 3gpp compliant simulator for ai enabled 5g and beyond," The rapid evolution of cellular system design towards 5G and beyond gives rise to a need for investigation of the new features, design proposals and solutions in realistic settings for various deployments and use case scenarios. While many system level simulators for 4G and 5G exist today, there is particularly a dire need for a 3GPP compliant system level holistic and realistic simulator that can support evaluation of the plethora of AI based network automation solutions being proposed in literature. In this paper we present such a simulator developed at AI4networks Lab, called SyntheticNET. To the best of authors' knowledge, SyntheticNET is the very first python-based simulator that fully conforms to 3GPP 5G standard release 15 and is upgradable to future releases. The key distinguishing features of SyntheticNET compared to existing simulators include: 1) a modular structure to facilitate cross validation and upgrading to future releases; 2) flexible propagation modeling using measurement based, ray tracing based or AI based propagation modeling; 3) ability to import data sheet based on measurement based realistic vendor specific base station features such as antenna and energy consumption pattern; 4) support for 5G standard based adaptive numerology; 5) realistic and user-specific mobility patterns that are yielded from actual geographical maps; 6) detailed handover (HO) process implementation; and 7) incorporation of database aided edge computing. Another key feature of the SyntheticNET is the ease with which it can be used to test AI based network automation solutions. Being the first python based 5G simulator, this ease, in part stems for SyntheticNET's built-in capability to process and analyze large data sets and integrated access to Machine Learning libraries. Thus, SyntheticNET simulator offers a powerful platform for academia and industry alike to investigate not only new solutions for optimally designing, deploying and operating existing and emerging cellular networks but also for enabling AI empowered deep automation in the future.",project-academic
,2019-08-27,,,dialogue auxiliary system based on team learning and hierarchical reasoning," The invention discloses a dialogue auxiliary system for team learning and hierarchical reasoning. The method is characterized in that firstly a generic industry knowledge map which comprises the universal entities and attribute entries outside the industry is crawled and generated, thereby conveniently meeting the requirement of a user for universal knowledge query, and facilitating the realization of the rapid migration of knowledge bases among the industries; secondly, a multi-level reasoning network provided by the invention can realize the complex semantic reasoning capability, can realizethe man-machine conversation based on a reasoning process, and can carry out the accurate service recommendation and product marketing through the reasoning network at the same time; and finally, thestrategy learning network based on reinforcement learning can learn the sorting strategy by utilizing the accumulated historical interaction experience, so that the user experience is continuously improved. The functional extension and effect upgrading of an original dialogue system can be realized by deploying the system, and the user experience is improved.",project-academic
10.1007/S00170-020-05548-8,2020-06-01,a,Springer London,a tool wear monitoring and prediction system based on multiscale deep learning models and fog computing," Tool condition monitoring (TCM) during the manufacturing process is of great significance for ensuring product quality and plays an important role in intelligent manufacturing. Current TCM systems deployed in the local device or cloud computing environment unable meet the requirements of low response latency and high accuracy at the same time. The emerging fog computing provides new solutions for the above problem. This paper presents a tool wear monitoring and prediction (TWMP) system based on deep learning models and fog computing. In order to improve monitoring and prediction accuracy, we propose a multiscale convolutional long short-term memory model (MCLSTM) to complete the tool wear monitoring task and a bi-directional LSTM model (BiLSTM) to complete the tool wear prediction task. To reduce the response latency of the TWMP system, we deploy the MCLSTM model and the BiLSTM model in a fog computing architecture. The fog computing architecture consists of an edge computing layer, a fog computing layer, and a cloud computing layer. The edge computing layer undertakes real-time signal collection task. The fog computing layer undertakes real-time tool wear monitoring task. The cloud computing layer with powerful computing resources undertakes intensive computing and latency-insensitive tasks such as data storage, tool wear prediction, and model training. A twist drill wear monitoring and prediction experiment is conducted to test the performance of the proposed system in terms of accuracy, response time, and network bandwidth consumption.",project-academic
10.3390/ELECTRONICS9010044,2019-12-28,a,Multidisciplinary Digital Publishing Institute,a comparison analysis of ble based algorithms for localization in industrial environments," Proximity beacons are small, low-power devices capable of transmitting information at a limited distance via Bluetooth low energy protocol. These beacons are typically used to broadcast small amounts of location-dependent data (e.g., advertisements) or to detect nearby objects. However, researchers have shown that beacons can also be used for indoor localization converting the received signal strength indication (RSSI) to distance information. In this work, we study the effectiveness of proximity beacons for accurately locating objects within a manufacturing plant by performing extensive experiments in a real industrial environment. To this purpose, we compare localization algorithms based either on trilateration or environment fingerprinting combined with a machine-learning based regressor (k-nearest neighbors, support-vector machines, or multi-layer perceptron). Each algorithm is analyzed in two different types of industrial environments. For each environment, various configurations are explored, where a configuration is characterized by the number of beacons per square meter and the density of fingerprint points. In addition, the fingerprinting approach is based on a preliminary site characterization; it may lead to location errors in the presence of environment variations (e.g., movements of large objects). For this reason, the robustness of fingerprinting algorithms against such variations is also assessed. Our results show that fingerprint solutions outperform trilateration, showing also a good resilience to environmental variations. Given the similar error obtained by all three fingerprint approaches, we conclude that k-NN is the preferable algorithm due to its simple deployment and low number of hyper-parameters.",project-academic
10.1145/3447548.3467089,2021-08-14,p,ACM,exploration in online advertising systems with deep uncertainty aware learning," Modern online advertising systems inevitably rely on personalization methods, such as click-through rate (CTR) prediction. Recent progress in CTR prediction enjoys the rich representation capabilities of deep learning and achieves great success in large-scale industrial applications. However, these methods can suffer from lack of exploration. Another line of prior work addresses the exploration-exploitation trade-off problem with contextual bandit methods, which are recently less studied in the industry due to the difficulty in extending their flexibility with deep models. In this paper, we propose a novel Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on Gaussian processes, which can provide predictive uncertainty estimations while maintaining the flexibility of deep neural networks. DUAL can be easily implemented on existing models and deployed in real-time systems with minimal extra computational overhead. By linking the predictive uncertainty estimation ability of DUAL to well-known bandit algorithms, we further present DUAL-based Ad-ranking strategies to boost up long-term utilities such as the social welfare in advertising systems. Experimental results on several public datasets demonstrate the effectiveness of our methods. Remarkably, an online A/B test deployed in the Alibaba display advertising platform shows an 8.2% social welfare improvement and an 8.0% revenue lift.",project-academic
10.1145/3474085.3481542,2021-10-17,p,ACM,l2rs a learning to rescore mechanism for hybrid speech recognition," This paper aims to advance the performance of industrial ASR systems by exploring a more effective method for N-best rescoring, a critical step that greatly affects the final recognition accuracy. Existing rescoring approaches suffer the following issues: (i) limited performance since they optimize an unnecessarily harder problem, namely predicting accurate grammatical legitimacy scores of the N-best hypotheses rather than directly predicting their partial orders regarding a specific acoustic input; (ii) hard to incorporate various information by advanced natural language processing (NLP) models such as BERT to achieve a comprehensive evaluation of each N-best candidate. To relieve the above drawbacks, we propose a simple yet effective mechanism, Learning-to-Rescore (L2RS), to empower ASR systems with state-of-the-art information retrieval (IR) techniques. Specifically, L2RS utilizes a wide range of textual information from the state-of-the-art NLP models and automatically deciding their weights to directly learn the ranking order of each N-best hypothesis with respect to a specific acoustic input. We incorporate various features including BERT sentence embeddings, the topic vectors, and perplexity scores produced by an n-gram language model (LM), topic modeling LM, BERT, and RNNLM to train the rescoring model. Experimental results on a public dataset show that L2RS outperforms not only traditional rescoring methods but also its deep neural network counterparts by a substantial margin of 20.85% in terms of NDCG@10. The L2RS toolkit has been successfully deployed for many online commercial services in WeBank Co., Ltd, China's leading digital bank. The efficacy and applicability of L2RS are validated by real-life online customer datasets.",project-academic
10.1145/3394486.3403302,2020-07-23,a,,grale designing networks for graph learning," How can we find the right graph for semi-supervised learning? In real world applications, the choice of which edges to use for computation is the first step in any graph learning process. Interestingly, there are often many types of similarity available to choose as the edges between nodes, and the choice of edges can drastically affect the performance of downstream semi-supervised learning systems. However, despite the importance of graph design, most of the literature assumes that the graph is static. In this work, we present Grale, a scalable method we have developed to address the problem of graph design for graphs with billions of nodes. Grale operates by fusing together different measures of(potentially weak) similarity to create a graph which exhibits high task-specific homophily between its nodes. Grale is designed for running on large datasets. We have deployed Grale in more than 20 different industrial settings at Google, including datasets which have tens of billions of nodes, and hundreds of trillions of potential edges to score. By employing locality sensitive hashing techniques,we greatly reduce the number of pairs that need to be scored, allowing us to learn a task specific model and build the associated nearest neighbor graph for such datasets in hours, rather than the days or even weeks that might be required otherwise. We illustrate this through a case study where we examine the application of Grale to an abuse classification problem on YouTube with hundreds of million of items. In this application, we find that Grale detects a large number of malicious actors on top of hard-coded rules and content classifiers, increasing the total recall by 89% over those approaches alone.",project-academic
10.1145/3447548.3467088,2021-08-14,p,ACM,autosmart an efficient and automatic machine learning framework for temporal relational data," Temporal relational data, perhaps the most commonly used data type in industrial machine learning applications, needs labor-intensive feature engineering and data analyzing for giving precise model predictions. An automatic machine learning framework is needed to ease the manual efforts in fine-tuning the models so that the experts can focus more on other problems that really need humans' engagement such as problem definition, deployment, and business services. However, there are three main challenges for building automatic solutions for temporal relational data: 1) how to effectively and automatically mining useful information from the multiple tables and the relations from them? 2) how to be self-adjustable to control the time and memory consumption within a certain budget? and 3) how to give generic solutions to a wide range of tasks? In this work, we propose our solution that successfully addresses the above issues in an end-to-end automatic way. The proposed framework, AutoSmart, is the winning solution to the KDD Cup 2019 of the AutoML Track, which is one of the largest AutoML competition to date (860 teams with around 4,955 submissions). The framework includes automatic data processing, table merging, feature engineering, and model tuning, with a time and memory controller for efficiently and automatically formulating the models. The proposed framework outperforms the baseline solution significantly on several datasets in various domains.",project-academic
,2021-09-09,a,,autosmart an efficient and automatic machine learning framework for temporal relational data," Temporal relational data, perhaps the most commonly used data type in industrial machine learning applications, needs labor-intensive feature engineering and data analyzing for giving precise model predictions. An automatic machine learning framework is needed to ease the manual efforts in fine-tuning the models so that the experts can focus more on other problems that really need humans' engagement such as problem definition, deployment, and business services. However, there are three main challenges for building automatic solutions for temporal relational data: 1) how to effectively and automatically mining useful information from the multiple tables and the relations from them? 2) how to be self-adjustable to control the time and memory consumption within a certain budget? and 3) how to give generic solutions to a wide range of tasks? In this work, we propose our solution that successfully addresses the above issues in an end-to-end automatic way. The proposed framework, AutoSmart, is the winning solution to the KDD Cup 2019 of the AutoML Track, which is one of the largest AutoML competition to date (860 teams with around 4,955 submissions). The framework includes automatic data processing, table merging, feature engineering, and model tuning, with a time\&memory controller for efficiently and automatically formulating the models. The proposed framework outperforms the baseline solution significantly on several datasets in various domains.",project-academic
,2019-05-21,,,an infrared power equipment identification and online diagnosis method and system," The invention relates to the technical field of intelligent power grid information, in particular to a power equipment identification and online diagnosis method and system based on infrared thermal imaging. The method comprises the following steps: firstly, acquiring infrared image data of power equipment, making virtual data for learning, and then migrating a deep neural network model to form aninfrared recognition deep neural network model of the power equipment; Establishing a defect rule base for classifying the infrared power equipment and parts according to the infrared diagnosis application standard of the live equipment in the power industry; And finally, deploying a mobile power equipment identification deep neural network model at the mobile terminal, carrying out power equipment and component identification on the infrared image acquired in real time, carrying out online analysis and diagnosis on equipment thermal field distribution according to the defect rule base, and evaluating the current operation state of the power equipment.",project-academic
10.1016/J.IJROBP.2021.01.044,2021-02-02,a,Elsevier,clinical natural language processing for radiation oncology a review and practical primer," Natural language processing (NLP), which aims to convert human language into expressions that can be analyzed by computers, is one of the most rapidly developing and widely used technologies in the field of artificial intelligence. Natural language processing algorithms convert unstructured free text data into structured data that can be extracted and analyzed at scale. In medicine, this unlocking of the rich, expressive data within clinical free text in electronic medical records will help untap the full potential of big data for research and clinical purposes. Recent major NLP algorithmic advances have significantly improved the performance of these algorithms, leading to a surge in academic and industry interest in developing tools to automate information extraction and phenotyping from clinical texts. Thus, these technologies are poised to transform medical research and alter clinical practices in the future. Radiation oncology stands to benefit from NLP algorithms if they are appropriately developed and deployed, as they may enable advances such as automated inclusion of radiation therapy details into cancer registries, discovery of novel insights about cancer care, and improved patient data curation and presentation at the point of care. However, challenges remain before the full value of NLP is realized, such as the plethora of jargon specific to radiation oncology, nonstandard nomenclature, a lack of publicly available labeled data for model development, and interoperability limitations between radiation oncology data silos. Successful development and implementation of high quality and high value NLP models for radiation oncology will require close collaboration between computer scientists and the radiation oncology community. Here, we present a primer on artificial intelligence algorithms in general and NLP algorithms in particular; provide guidance on how to assess the performance of such algorithms; review prior research on NLP algorithms for oncology; and describe future avenues for NLP in radiation oncology research and clinics.",project-academic
10.1016/J.ENCONMAN.2021.113856,2021-04-01,a,Pergamon,the mutual benefits of renewables and carbon capture achieved by an artificial intelligent scheduling strategy," Abstract None None Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system.",project-academic
10.1109/IWASI.2017.7974239,2017-06-01,p,IEEE,plenty of room at the bottom micropower deep learning for cognitive cyber physical systems," Deep convolutional neural networks are being regarded today as an extremely effective and flexible approach for extracting actionable, high-level information from the wealth of raw data produced by a wide variety of sensory data sources. CNNs are however computationally demanding: today they typically run on GPU-accelerated compute servers or high-end embedded platforms. Industry and academia are racing to bring CNN inference (first) and training (next) within ever tighter power envelopes, while at the same time meeting real-time requirements. Recent results, including our PULP and ORIGAMI chips, demonstrate there is plenty of room at the bottom: pj/OP (GOPS/mW) computational efficiency, needed for deploying CNNs in the mobile/wearable scenario, is within reach. However, this is not enough: 1000x energy efficiency improvement, within a mW power envelope and with low-cost CMOS processes, is required for deploying CNNs in the most demanding CPS scenarios. The fj/OP milestone will require heterogeneous (3D) integration with ultra-efficient die-to-die communication, mixed-signal pre-processing, event-based approximate computing, while still meeting real-time requirements.",project-academic
10.1145/3465074.3465080,2021-07-20,a,"ACMPUB27New York, NY, USA",applied ai matters ai4code applying artificial intelligence to source code," The marriage of Artificial Intelligence (AI) techniques to problems surrounding the generation, maintenance, and use of source code has come to the fore in recent years as an important AI application area1. A large chunk of this recent attention can be attributed to contemporaneous advancements in Natural Language Processing (NLP) techniques and sub-fields. The naturalness hypothesis, which states that ""software is a form of human communication"" and that code exhibits patterns that are similar to (human) natural languages (Devanbu, 2015; Hindle, Barr, Gabel, Su, & Devanbu, 2016), has allowed for the application of many of these NLP advances to code-centric usecases. This development has contributed to a spate of work in the community --- much of it captured in a survey by Allamanis, Barr, Devanbu, and Sutton (2018) that focuses on classifying these approaches by the type of probabilistic model applied to source code. This increase in the variety of AI techniques applied to source code has found various manifestations in the industry at large. Code and software form the backbone that underpins almost all modern technical advancements: it is thus natural that breakthroughs in this area should reflect in the emergence of real world deployments.",project-academic
10.3390/EN12132523,2019-06-30,a,Multidisciplinary Digital Publishing Institute,sensor data compression using bounded error piecewise linear approximation with resolution reduction," Smart production as one of the key issues for the world to advance toward Industry 4.0 has been a research focus in recent years. In a smart factory, hundreds or even thousands of sensors and smart devices are often deployed to enhance product quality. Generally, sensor data provides abundant information for artificial intelligence (AI) engines to make decisions for these smart devices to collect more data or activate some required activities. However, this also consumes a lot of energy to transmit the sensor data via networks and store them in data centers. Data compression is a common approach to reduce the sensor data size so as to lower transmission energies. Literature indicates that many Bounded-Error Piecewise Linear Approximation (BEPLA) methods have been proposed to achieve this. Given an error bound, they make efforts on how to approximate to the original sensor data with fewer line segments. In this paper, we furthermore consider resolution reduction, which sets a new restriction on the position of line segment endpoints. Swing-RR (Resolution Reduction) is then proposed. It has O(1) complexity in both space and time per data record. In other words, Swing-RR is suitable for compressing sensor data, particularly when the volume of the data is huge. Our experimental results on real world datasets show that the size of compressed data is significantly reduced. The energy consumed follows. When using minimal resolution, Swing-RR has achieved the best compression ratios for all tested datasets. Consequently, fewer bits are transmitted through networks and less disk space is required to store the data in data centers, thus consuming less data transmission and storage power.",project-academic
10.1016/J.GHEART.2013.12.002,2013-12-01,a,Glob Heart,how relevant is point of care ultrasound in lmic," The trajectory of medical ultrasound has beenmarked by quantum decreases in size. In the 1950s, the first ultrasounds were performed using refrigerator-sized machines, with patients subjected to water immersion. Adoption of this technology across many medical specialties was hindered by machine bulk and cost, low-resolution still images, and a steep learning curve for image interpretation. Incremental changes in ultrasound technology through the 1970s and 1980s allowed machines to be moved on wheels, and eventually to sit atop a cart. It was not until the 1990s that an ultrasound machine capable of being transported in a backpack was invented, as a result of a Defense Advanced Research Projects Agency (DARPA) grant. As with other computer technology, ultrasound machines then rapidly grew smaller and more powerful, and a wider user base began adopting them for multiple types of medical applications. By the 1990s, portable ultrasound machines were deployed in combat support hospitals, ambulances, helicopters, and a host of austere environments. Researchers described experiences using ultrasound on medical missions in remote Amazon jungle settlements, high-altitude environments in Nepal’s Himalayan Rescue Association Clinic, and the International Space Station. A report from Guatemala in the aftermath of Hurricane Stan demonstrated that point-ofcare ultrasound confirmed or ruled out emergent pathology in almost half of subjects evaluated. During the disaster relief effort following the 2010 Haitian earthquake, point-of-care ultrasound helped clinicians change management in 70% of cases, with nearly half of those decisions based on acute pathology identified examination. On the basis of experiences such as these, the World Health Organization recommends increased use of ultrasound as a main diagnostic modality, especially in underresourced environments. Ultrasound machine costs are generally lower than x-ray or computed tomography scan capital expenses, and ultrasound requires very little ongoing cost for consumables (such as gel), compared to the ongoing upkeep costs of these other modalities. Increasingly, ultrasound is being used in areaswithout easy access to imaging of any kind. Clinicians in the Lugufo refugee camp in Tanzania identifiedmany tropical infectious disease manifestations on ultrasound. Midwives in rural Rwanda, Zambia, and Liberia have been trained in the use of focused obstetric ultrasound with the goal of identifying common and life-threatening complications of late pregnancy. In areas where maternal and fetal mortality are significant health concerns, accurate pregnancy dating, early identification of breech presentation, and proper placental position could significantly impact the care provided and save lives. Advances in telemedicine have enabled expansion in ultrasound use as well. Recently, health care workers in rural India underwent training in basic cardiac and thoracic ultrasound with the goal of transmitting images to physicians at major hospital centers for real-time interpretation. Ultrasound has been described as a disruptive innovation byHarvard professor ClaytonM.Christensen. The term, originally coined by Christensen in reference to disk drives, refers to an innovation that transforms a market “by introducing simplicity, convenience, accessibility, and affordability where complication and high cost are the status quo.” Often, such innovations take the form of a narrow, niche market, overlooked by industry leaders, but as new users take hold, the new product can claim significant market share. In the case of ultrasound, traditional imagers such as radiologists, obstetricians, and cardiologists controlled a market marked by expensive, immobile machines whose images could only be interpreted by highly trained subspecialists within their respective fields. Hand-held ultrasound devices introduced an alternative concept of relatively inexpensive, easy-to-use machines that could generate images interpretable by a wider spectrum of clinicians at the point of care. Soon, concerns about smaller machines having inferior image quality compared to devicesmany times larger andmore expensive were outweighed by evidence that rapid diagnostic decisions could be made with portable machines. In the 1990s, emergencymedicine physicians joined the ranks of clinician-sonographers and described ultrasound training as part of the core competencies for residency training in 1994. Surgeons used ultrasound for trauma at this time as well. A wave of intensivists, ophthalmologists, internists, and other specialists found utility in point-of-care ultrasound by the 2000s, and a 2011 article in the New England Journal of Medicine by Moore and Copel listed 24 specialties who had adopted the technology into common clinical practice. In 2013, the Agency for Healthcare Research and Quality published “Making Health Care Safer II,” an update of its previous 2001 guidelines for best practices in patient safety. Among the top 10 practices in both publications was using ultrasound to guide central venous access. With mounting evidence that iatrogenic complications of many invasive procedures such as venous access, thoracentesis, paracentesis, and others can be mitigated with point-of-care ultrasound, hospitals are increasingly mandating training in ultrasound by clinicians. As of this writing, a number of medical schools have adopted ultrasound curricula as well, incorporating sonographic assessments of multiple organ systems along with traditional courses on physical examination and clinical reasoning. Many authors have argued that ultrasound has become the stethoscope of the 21st century. Why then, do we not see The authors report no relationships that could be construed as a conflict of interest. From the *Department of Emergency Medicine and yDepartment of Cardiology, Icahn School of Medicine at Mount Sinai, New York, NY, USA. Correspondence: J. Narula (Narula@ mountsinai.org).",project-academic
10.1111/1559-8918.2019.01307,2019-11-01,a,"John Wiley & Sons, Ltd",supporting real time contextual inquiry through sensor data," A key challenge in carrying out product design research is obtaining rich contextual information about use in the wild. We present a method that algorithmically mediates between participants, researchers, and objects in order to enable real-time collaborative sensemaking. It facilitates contextual inquiry, revealing behaviours and motivations that frame product use in the wild. In particular, we are interested in developing a practice of use driven design, where products become research tools that generate design insights grounded in user experiences. The value of this method was explored through the deployment of a collection of Bluetooth speakers that capture and stream live data to remote but co-present researchers about their movement and operation. Researchers monitored a visualisation of the real-time data to build up a picture of how the speakers were being used, responding to moments of activity within the data, initiating text conversations and prompting participants to capture photos and video. Based on the findings of this explorative study, we discuss the value of this method, how it compares to contemporary research practices, and the potential of machine learning to scale it up for use within industrial contexts. As greater agency is given to both objects and algorithms, we explore ways to empower ethnographers and participants to actively collaborate within remote real-time research.",project-academic
10.1016/J.PROCS.2019.08.226,2019-01-01,a,Elsevier,designing an emotionally realistic chatbot framework to enhance its believability with aiml and information states," Abstract None None Chatbot program has been empirically proven help to improve the engagement with users. Moreover, the implementation of a chat-bot program in the industry helps the company to reduce their operational costs in engaging with their customers and employees. There are still quite a number of problems existed in order to build a human-like chatbot program. Understanding a natural conversation and replying back to the conversation the interlocutors, keeping the conversation flowing naturally is a cumbersome task for a computer. This research aims to design an emotionally realistic chatbot system to enhance the believability of the chatbot using Artificial Intelligence Markup Language (AIML) and Information State. The results show that there is a statistically significant improvement to the chatbot believability in the system that has emotions variables induced compare to the one without emotions. Moreover, 63,33% of the respondents perceived Aero and Iris as two different individuals. The future work of this research is to deploy and have an exploration of the chatbot system to other cases.",project-academic
10.1109/IOTDI49375.2020.00026,2020-04-21,p,IEEE,iot id a novel device specific identifier based on unique hardware fingerprints," A significant number of IoT devices are being deployed in the wild, mostly in remote locations and in untrusted conditions. This could include monitoring an electronic perimeter fence or a critical infrastructure such as telecom and power grids. Such applications rely on the fidelity of data reported from the IoT devices, and hence it is imperative to identify the trustworthiness of the remote device before taking decisions. Existing approaches use a secret key usually stored in volatile or non-volatile memory for creating an encrypted digital signature. However, these techniques are vulnerable to malicious attacks and have significant computation and energy overhead. This paper presents a novel device-specific identifier, IoT-ID that captures the device characteristics and can be used towards device identification. IoT-ID is based on physically unclonable functions (PUFs), that exploit variations in the manufacturing process to derive a unique fingerprint for integrated circuits. In this work, we design novel PUFs for Commercially Off the Shelf (COTS) components such as clock oscillators and ADC, to derive IoT-ID for a device. Hitherto, system component PUFs are invasive and rely on additional dedicated hardware circuitry to create a unique fingerprint. A highlight of our PUFs is doing away with special hardware. IoT-ID is non-invasive and can be invoked using simple software APIs running on COTS components. IoT-ID has the following key properties viz., constructability, real-time, uniqueness, and reproducibility, making them robust device-specific identifiers. We present detailed experimental results from our live deployment of 50 IoT devices running over a month. Our edge machine learning algorithm has 100% accuracy in uniquely identifying the 50 devices in our deployment and can run locally on the resource-constrained IoT device. We show the scalability of IoT-ID with the help of numerical analysis on 1000s of IoT devices.",project-academic
10.1109/MED.2017.7984310,2017-07-01,p,Institute of Electrical and Electronics Engineers (IEEE),cloud computing for big data analytics in the process control industry," The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",project-academic
10.1007/S11628-020-00423-8,2020-01-01,a,Springer Berlin Heidelberg,impacts of service robots on service quality," With rapid advances in technologies, especially in artificial intelligence, smart sensors, big data analytics, and robotics, the service industry began introducing robots to perform a variety of functions. While the main purpose of deploying robots has been productivity improvement, the current COVID-19 pandemic has brought more urgent purpose, providing contactless service for social distancing. This study explores the service quality provided by robots based on real data in a hotel setting. A sample of 201 guests provided their expected service quality by robots and the actual performance experience after the service. We analyzed this relationship using importance performance analysis (IPA) and the Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The results revealed that customers’ top priorities for robots’ service quality are assurance and reliability, while tangible and empathy were not as important. Customers were not satisfied with robots’ responsiveness, but this construct was found to be a low priority.",project-academic
10.1109/ACCESS.2019.2963723,2020-01-03,a,Institute of Electrical and Electronics Engineers (IEEE),a smart collaborative routing protocol for delay sensitive applications in industrial iot," In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.",project-academic
10.1109/ACCESS.2021.3049860,2021-01-08,a,Institute of Electrical and Electronics Engineers (IEEE),ensemble bootstrapped deep deterministic policy gradient for vision based robotic grasping," With sufficient practice, humans can grab objects they have never seen before through brain decision-making. However, the manipulators, which has a wide range of applications in industrial production, can still only grab specific objects. Because most of the grasp algorithms rely on prior knowledge such as hand-eye calibration results, object model features, and can only target specific types of objects. When the task scenario and the operation target change, it cannot perform effective redeployment. In order to solve the above problems, academia often uses reinforcement learning to train grasping algorithms. However, the method of reinforcement learning in the field of manipulators grasping mainly encounters these main problems: insufficient sample utilization, poor algorithm stability, and limited exploration. This article uses LfD, BC, and DDPG to improve sample utilization. Use multiple critics to integrate and evaluate input actions to solve the problem of algorithm instability. Finally, inspired by Thompson’s sampling idea, the input action is evaluated from different angles, which increases the algorithm’s exploration of the environment and reduces the number of interactions with the environment. EDDPG and EBDDPG algorithm is designed in the article. In order to further improve the generalization ability of the algorithm, this article does not use extra information that is difficult to obtain directly on the physical platform, such as the real coordinates of the target object and the continuous motion space at the end of the manipulator in the Cartesian coordinate system is used as the output of the decision. The simulation results show that, under the same number of interactions, the manipulators’ success rate in grabbing 1000 random objects has increased more than double and reached state-of-the-art(SOTA) performance.",project-academic
10.1016/J.APENERGY.2018.09.093,2018-12-01,a,Elsevier,data center holistic demand response algorithm to smooth microgrid tie line power fluctuation," Abstract None None With the rapid development of cloud computing, artificial intelligence technologies and big data applications, data centers have become widely deployed. High density IT equipment in data centers consumes a lot of electrical power, and makes data center a hungry monster of energy consumption. To solve this problem, renewable energy is increasingly integrated into data center power provisioning systems. Compared to the traditional power supply methods, renewable energy has its unique characteristics, such as intermittency and randomness. When renewable energy supplies power to the data center industrial park, this kind of power supply not only has negative effects on the normal operation of precision equipment, such as CPU/GPU chips and hard disk, in data center, but it would also impact the stability of the utility power grids operation. To solve this problem, this paper presents a novel tie-line power fluctuation smoothing algorithm with consideration of data center’s holistic demand response. The contributions of this paper are: (1) overcoming the limitations of treating IT load as uncontrollable workload in the traditional demand response research, we design a data center resource scheduling model to realize IT load demand response controllability; (2) two novel mechanisms are proposed: (i) the server cluster workload scheduling method with time shift mechanism, and (ii) the data center UPS (Uninterruptible Power Supply) energy storage dynamic response mechanism. (3) Combining these two mechanisms as holistic demand response of data center, we present a tie-line power fluctuation smoothing algorithm to improve power supply reliability, which is beneficial to both the high density and precision IT equipment in the data center and the utility power grid. In the experiments, the results show that the new algorithm can effectively regulate the tie-line power fluctuations under different server cluster utilization ranges and scenarios of large-scale penetration of distributed renewable energy scenarios. The new algorithm is hence able to contribute beneficially to the reliability and stability of intelligent industrial park micro-grid and utility power grids.",project-academic
10.1109/MWC.001.1900346,2020-05-13,a,IEEE,network intelligence empowered industrial robot control in the f ran environment," Industrial robots are widely adopted in modern industries, including automobile manufacturing, warehousing, and so on. The industrial robot with cellular network connection will be more flexible and intelligent than that without a network connection or with a conventional industrial communication network connection (e.g., fieldbus, real-time Ethernet, and WiFi). This is seen as the direction of development of the next generation of industrial robots. As a promising candidate for the next generation of cellular networks, fog radio access network (F-RAN) can provide artificial intelligence (AI) on the network edge, which is equipped with some computing and storage facilities, thus promoting the development of industrial robots in the future. In this article, we first propose a local-network cooperative control architecture for industrial robots in the F-RAN environment, which divides the robot controller into an onboard controller and a network controller, in order to decouple the basic control functions and application-dependent functions. The network controller is implemented on the fog access node (F-AP), and it possibly uses AI to bring advanced capabilities to the robot control. Then, based on the proposed architecture, a testbed for multiple automated guided vehicle (AGV) coordination in F-RAN has been designed and implemented, and a recurrent neural network (RNN)-empowered multi-AGV coordination policy is also developed. Finally, its performance is examined by several experiments. The results show that network intelligence deployed in F-RAN can improve the scheduling efficiency up to 35 percent, and the proposed control architecture can save huge backhaul traffic compared to the conventional cloud-based control architecture for industrial robots.",project-academic
10.23919/AEIT.2018.8577226,2018-10-01,p,Institute of Electrical and Electronics Engineers Inc.,smart farms for a sustainable and optimized model of agriculture," Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The ‘smart farm’ model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business.",project-academic
10.1109/TII.2019.2946045,2020-02-01,a,Institute of Electrical and Electronics Engineers (IEEE),intelligent quality of service aware traffic forwarding for software defined networking open shortest path first hybrid industrial internet," Driven by the emerging advanced information and communication technologies, e.g., artificial intelligence, 5G wireless communications, big data analytics, etc., industrial Internet serves as a key enabling technology to realize intelligent manufacturing, and has been attracting considerable attentions from academia and industry. However, the traditional industrial networks can hardly satisfy the quality of service (QoS) requirements for some mission-critical industrial applications (e.g., fault detection, advanced control, remote monitoring, predictive maintenance, etc.) due to network heterogeneity, traffic congestion, dynamic end-to-end latency, reliability issues, and so on. The emerging software-defined networking (SDN) has been considered as a promising architecture to improve the QoS of industrial applications by flexibly decoupling the control and data planes to control the network behaviours centrally. Owing to economy and policy considerations, a realistic solution is to incrementally deploy SDN in industrial networks instead of fully replacing traditional industrial routers with SDN-enabled switches. In this article, we consider a hybrid Industrial network consisting of conventional routers (e.g., running OSPF protocol) and SDN-enabled switches (e.g., running OpenFlow protocol), and propose an intelligent QoS-aware forwarding strategy to improve the QoS of industrial applications, by utilizing a single path minimum cost forwarding scheme and a K-path partition algorithm for multipath forwarding. Simulation results demonstrate that the proposed scheme not only guarantees the QoS requirements of industrial services, but also efficiently utilizes bandwidth resources by balancing traffic load in the SDN/OSPF hybrid industrial Internet.",project-academic
10.1007/S10845-018-1417-8,2020-10-01,a,Springer US,hypernetwork based manufacturing service scheduling for distributed and collaborative manufacturing operations towards smart manufacturing," In the future smart manufacturing, both of sensor-based environment in shop floors and cloud-based environment among more and more enterprises are deployed gradually. Various distributed and separated manufacturing facilities are as collaborative cloud services, integrated and aggregated with their real-time information. It provides opportunities for the distributed and collaborative manufacturing operations across lots of distributed but networked enterprises on demand with enough flexibility. To this end, the scheduling problem and its result of those collaborative services for distributed manufacturing operations play an important role in improving manufacturing utilization and efficiency. In this paper, we put forward the hypernetwork-based models introducing the thought of graph coloring and an artificial bee colony algorithm based method for this scheduling problem. Three groups of experiments are carried out respectively to discuss therein different situations of distributed and collaborative manufacturing operations, i.e., in a private cloud, in a public cloud, and in a hybrid cloud. Some future studies with further consideration of collaboration equilibrium, dynamic control and data-based intelligence, are finally pointed out in the conclusion.",project-academic
10.1186/S13336-015-0019-3,2015-03-26,a,BioMed Central,clinical decision support systems for improving diagnostic accuracy and achieving precision medicine," As research laboratories and clinics collaborate to achieve precision medicine, both communities are required to understand mandated electronic health/medical record (EHR/EMR) initiatives that will be fully implemented in all clinics in the United States by 2015. Stakeholders will need to evaluate current record keeping practices and optimize and standardize methodologies to capture nearly all information in digital format. Collaborative efforts from academic and industry sectors are crucial to achieving higher efficacy in patient care while minimizing costs. Currently existing digitized data and information are present in multiple formats and are largely unstructured. In the absence of a universally accepted management system, departments and institutions continue to generate silos of information. As a result, invaluable and newly discovered knowledge is difficult to access. To accelerate biomedical research and reduce healthcare costs, clinical and bioinformatics systems must employ common data elements to create structured annotation forms enabling laboratories and clinics to capture sharable data in real time. Conversion of these datasets to knowable information should be a routine institutionalized process. New scientific knowledge and clinical discoveries can be shared via integrated knowledge environments defined by flexible data models and extensive use of standards, ontologies, vocabularies, and thesauri. In the clinical setting, aggregated knowledge must be displayed in user-friendly formats so that physicians, non-technical laboratory personnel, nurses, data/research coordinators, and end-users can enter data, access information, and understand the output. The effort to connect astronomical numbers of data points, including ‘-omics’-based molecular data, individual genome sequences, experimental data, patient clinical phenotypes, and follow-up data is a monumental task. Roadblocks to this vision of integration and interoperability include ethical, legal, and logistical concerns. Ensuring data security and protection of patient rights while simultaneously facilitating standardization is paramount to maintaining public support. The capabilities of supercomputing need to be applied strategically. A standardized, methodological implementation must be applied to developed artificial intelligence systems with the ability to integrate data and information into clinically relevant knowledge. Ultimately, the integration of bioinformatics and clinical data in a clinical decision support system promises precision medicine and cost effective and personalized patient care.",project-academic
10.1016/J.JMSY.2017.02.011,2017-04-01,a,Elsevier,a fog computing based framework for process monitoring and prognosis in cyber manufacturing," Abstract None None Small- and medium-sized manufacturers, as well as large original equipment manufacturers (OEMs), have faced an increasing need for the development of intelligent manufacturing machines with affordable sensing technologies and data-driven intelligence. Existing monitoring systems and prognostics approaches are not capable of collecting the large volumes of real-time data or building large-scale predictive models that are essential to achieving significant advances in cyber-manufacturing. The objective of this paper is to introduce a new computational framework that enables remote real-time sensing, monitoring, and scalable high performance computing for diagnosis and prognosis. This framework utilizes wireless sensor networks, cloud computing, and machine learning. A proof-of-concept prototype is developed to demonstrate how the framework can enable manufacturers to monitor machine health conditions and generate predictive analytics. Experimental results are provided to demonstrate capabilities and utility of the framework such as how vibrations and energy consumption of pumps in a power plant and CNC machines in a factory floor can be monitored using a wireless sensor network. In addition, a machine learning algorithm, implemented on a public cloud, is used to predict tool wear in milling operations.",project-academic
10.1186/S40537-019-0212-5,2019-12-01,a,SpringerOpen,intelligent video surveillance a review through deep learning techniques for crowd analysis," Big data applications are consuming most of the space in industry and research area. Among the widespread examples of big data, the role of video streams from CCTV cameras is equally important as other sources like social media data, sensor data, agriculture data, medical data and data evolved from space research. Surveillance videos have a major contribution in unstructured big data. CCTV cameras are implemented in all places where security having much importance. Manual surveillance seems tedious and time consuming. Security can be defined in different terms in different contexts like theft identification, violence detection, chances of explosion etc. In crowded public places the term security covers almost all type of abnormal events. Among them violence detection is difficult to handle since it involves group activity. The anomalous or abnormal activity analysis in a crowd video scene is very difficult due to several real world constraints. The paper includes a deep rooted survey which starts from object recognition, action recognition, crowd analysis and finally violence detection in a crowd environment. Majority of the papers reviewed in this survey are based on deep learning technique. Various deep learning methods are compared in terms of their algorithms and models. The main focus of this survey is application of deep learning techniques in detecting the exact count, involved persons and the happened activity in a large crowd at all climate conditions. Paper discusses the underlying deep learning implementation technology involved in various crowd video analysis methods. Real time processing, an important issue which is yet to be explored more in this field is also considered. Not many methods are there in handling all these issues simultaneously. The issues recognized in existing methods are identified and summarized. Also future direction is given to reduce the obstacles identified. The survey provides a bibliographic summary of papers from ScienceDirect, IEEE Xplore and ACM digital library.",project-academic
10.1186/S40537-015-0034-Z,2015-11-16,a,Springer International Publishing,an industrial big data pipeline for data driven analytics maintenance applications in large scale smart manufacturing facilities," The term smart manufacturing refers to a future-state of manufacturing, where the real-time transmission and analysis of data from across the factory creates manufacturing intelligence, which can be used to have a positive impact across all aspects of operations. In recent years, many initiatives and groups have been formed to advance smart manufacturing, with the most prominent being the Smart Manufacturing Leadership Coalition (SMLC), Industry 4.0, and the Industrial Internet Consortium. These initiatives comprise industry, academic and government partners, and contribute to the development of strategic policies, guidelines, and roadmaps relating to smart manufacturing adoption. In turn, many of these
 recommendations may be implemented using data-centric technologies, such as Big Data, Machine Learning, Simulation, Internet of Things and Cyber Physical Systems, to realise smart operations in the factory. Given the importance of machine uptime and availability in smart manufacturing, this research centres on the application of data-driven analytics to industrial equipment maintenance. The main contributions of this research are a set of data and system requirements for implementing equipment maintenance applications in industrial environments, and an information system model that provides a scalable and fault tolerant big data pipeline for integrating, processing and analysing industrial equipment data. These contributions are considered in the context of highly regulated large-scale manufacturing environments, where legacy (e.g. automation controllers) and emerging instrumentation (e.g. internet-aware smart sensors) must be supported to facilitate initial smart manufacturing efforts.",project-academic
10.1016/J.ADDMA.2017.11.009,2018-01-01,a,Elsevier,anomaly detection and classification in a laser powder bed additive manufacturing process using a trained computer vision algorithm," Abstract None None Despite the rapid adoption of laser powder bed fusion (LPBF) Additive Manufacturing by industry, current processes remain largely open-loop, with limited real-time monitoring capabilities. While some machines offer powder bed visualization during builds, they lack automated analysis capability. This work presents an approach for in-situ monitoring and analysis of powder bed images with the potential to become a component of a real-time control system in an LPBF machine. Specifically, a computer vision algorithm is used to automatically detect and classify anomalies that occur during the powder spreading stage of the process. Anomaly detection and classification are implemented using an unsupervised machine learning algorithm, operating on a moderately-sized training database of image patches. The performance of the final algorithm is evaluated, and its usefulness as a standalone software package is demonstrated with several case studies.",project-academic
10.1016/J.JISA.2018.05.002,2018-08-01,p,Elsevier,identification of malicious activities in industrial internet of things based on deep learning models," Abstract None None Internet Industrial Control Systems (IICSs) that connect technological appliances and services with physical systems have become a new direction of research as they face different types of cyber-attacks that threaten their success in providing continuous services to organizations. Such threats cause firms to suffer financial and reputational losses and the stealing of important information. Although Network Intrusion Detection Systems (NIDSs) have been proposed to protect against them, they have the difficult task of collecting information for use in developing an intelligent NIDS which can proficiently detect existing and new attacks. In order to address this challenge, this paper proposes an anomaly detection technique for IICSs based on deep learning models that can learn and validate using information collected from TCP/IP packets. It includes a consecutive training process executed using a deep auto-encoder and deep feedforward neural network architecture which is evaluated using two well-known network datasets, namely, the NSL-KDD and UNSW-NB15. As the experimental results demonstrate that this technique can achieve a higher detection rate and lower false positive rate than eight recently developed techniques, it could be implemented in real IICS environments.",project-academic
10.1109/TSE.2018.2844788,2020-02-01,a,IEEE,machine learning based prototyping of graphical user interfaces for mobile apps," It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: None detection , None classification , and None assembly . First, logical components of a GUI are None detected None from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately None classify None GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically None assembled . We implemented this approach for Android in a system called None ReDraw . Our evaluation illustrates that None ReDraw None achieves an average GUI-component classification accuracy of 91 percent and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.",project-academic
,2018-02-07,a,,machine learning based prototyping of graphical user interfaces for mobile apps," It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91% and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.",project-academic
10.1016/J.NEUCOM.2018.01.002,2018-04-12,a,Elsevier,robot manipulator control using neural networks a survey," Robot manipulators are playing increasingly significant roles in scientific researches and engineering applications in recent years. Using manipulators to save labors and increase accuracies are becoming common practices in industry. Neural networks, which feature high-speed parallel distributed processing, and can be readily implemented by hardware, have been recognized as a powerful tool for real-time processing and successfully applied widely in various control systems. Particularly, using neural networks for the control of robot manipulators have attracted much attention and various related schemes and methods have been proposed and investigated. In this paper, we make a review of research progress about controlling manipulators by means of neural networks. The problem foundation of manipulator control and the theoretical ideas on using neural network to solve this problem are first analyzed and then the latest progresses on this topic in recent years are described and reviewed in detail. Finally, toward practical applications, some potential directions possibly deserving investigation in controlling manipulators by neural networks are pointed out and discussed. (C) 2018 Elsevier B.V. All rights reserved.",project-academic
10.1016/J.ISATRA.2018.12.025,2019-12-01,a,ISA Trans,deep residual learning based fault diagnosis method for rotating machinery," Effective fault diagnosis of rotating machinery has always been an important issue in real industries. In the recent years, data-driven fault diagnosis methods such as neural networks have been receiving increasing attention due to their great merits of high diagnosis accuracy and easy implementation. However, it is mostly difficult to fully train a deep neural network since gradients in optimization may vanish or explode during back-propagation, which results in deterioration and noticeable variance in model performance. In fault diagnosis researches, larger data sequence of machinery vibration signal containing sufficient information is usually preferred and consequently, deep models with large capacity are generally adopted. In order to improve network training, a residual learning algorithm is proposed in this paper. The proposed architecture significantly improves the information flow throughout the network, which is well suited for processing machinery vibration signal with variable sequential length. Little prior expertise on fault diagnosis and signal processing is required, that facilitates industrial applications of the proposed method. Experiments on a popular rolling bearing dataset are implemented to validate the proposed method. The results of this study suggest that the proposed intelligent fault diagnosis method for rotating machinery offers a new and promising approach.",project-academic
10.1145/3299869.3324957,2019-06-25,p,ACM,ai meets ai leveraging query executions to improve index recommendations," State-of-the-art index tuners rely on query optimizer's cost estimates to search for the index configuration with the largest estimated execution cost improvement`. Due to well-known limitations in optimizer's estimates, in a significant fraction of cases, an index estimated to improve a query's execution cost, e.g., CPU time, makes that worse when implemented. Such errors are a major impediment for automated indexing in production systems. We observe that comparing the execution cost of two plans of the same query corresponding to different index configurations is a key step during index tuning. Instead of using optimizer's estimates for such comparison, our key insight is that formulating it as a classification task in machine learning results in significantly higher accuracy. We present a study of the design space for this classification problem. We further show how to integrate this classifier into the state-of-the-art index tuners with minimal modifications, i.e., how artificial intelligence (AI) can benefit automated indexing (AI). Our evaluation using industry-standard benchmarks and a large number of real customer workloads demonstrates up to 5x reduction in the errors in identifying the cheaper plan in a pair, which eliminates almost all query execution cost regressions when the model is used in index tuning.",project-academic
10.5555/1950815.1950963,2011-01-25,p,IEEE,high performance lithographic hotspot detection using hierarchically refined machine learning," Under real and continuously improving manufacturing conditions, lithography hotspot detection faces several key challenges. First, real hotspots become less but harder to fix at post-layout stages; second, false alarm rate must be kept low to avoid excessive and expensive post-processing hotspot removal; third, full chip physical verification and optimization require fast turn-around time. To address these issues, we propose a high performance lithographic hotspot detection flow with ultra-fast speed and high fidelity. It consists of a novel set of hotspot signature definitions and a hierarchically refined detection flow with powerful machine learning kernels, ANN (artificial neural network) and SVM (support vector machine). We have implemented our algorithm with industry-strength engine under real manufacturing conditions in 45nm process, and showed that it significantly out-performs previous state-of-the-art algorithms in hotspot detection false alarm rate (2.4X to 2300X reduction) and simulation run-time (5X to 237X reduction), meanwhile archiving similar or slightly better hotspot detection accuracies. Such high performance lithographic hotspot detection under real manufacturing conditions is especially suitable for guiding lithography friendly physical design.",project-academic
10.1109/TII.2019.2915846,2020-01-01,a,IEEE,a global manufacturing big data ecosystem for fault detection in predictive maintenance," Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",project-academic
10.1016/J.IJPE.2010.07.018,2010-12-01,a,Elsevier,sales forecasts in clothing industry the key success factor of the supply chain management," Abstract None None Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems. None However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust. None After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",project-academic
10.1109/ITST.2007.4295849,2007-06-06,p,IEEE,intelligent fleet management system with concurrent gps gsm real time positioning technology," Fleet management system is a rapid growing industry. This system helps institutions to manage vehicle fleet efficiently and effectively through smart allocation of resources. In this project, an intelligent fleet management system which incorporates the power of concurrent Global Positioning System (GPS) and Global System for Mobile Communications (GSM) real-time positioning, front-end intelligent and web-based management software is proposed. In contrast to systems which depend solely on GPS positioning, the proposed system provides higher positioning accuracy and is capable to track the target at areas where GPS signals are weak or unavailable. The terminal is powered by Front-End Intelligent Technology (FEI), a comprehensive embedded technology that is equipped with necessary artificial intelligence to mimic human intelligence in decision-making for quicker response, better accuracy and less dependence on a backend server. With less dependency on the backend, large scale fleet management system can be implemented more effectively. The proposed system is successfully implemented and evaluated on twenty vehicles including buses and cars in Universiti Teknologi Malaysia (UTM). Results from the test-bed shown that user can monitor and track the real-time physical location and conditions of their vehicles via Internet or Short Message Service (SMS). The web-based fleet management software also helped the user to manage fleets more effectively.",project-academic
,2018-10-12,,,smart city fire protection iot big data management system," The present invention provides a smart city fire protection IOT (Internet of Thighs) big data management system, and relates to the technical field of fire protection. The system comprises a data collection mechanism for collecting historical data and real-time data, a data processing mechanism for performing machine learning on the historical data and establishing a predictive model, a data analysis mechanism for analyzing the real-time data by using the predictive model and making a fire occurrence probability, and a data transmission mechanism for transmitting the fire occurrence probability to customers. The historical data comprises hardware information and fire records. The hardware information comprises the historical usage time of the firefighting hardware corresponding to different moments in the past, historical maintenance of the firefighting hardware, and historical fire alarm information collected by using the firefighting hardware. The fire records comprise the time, theplace and the cause of fire occurrence. The real-time data comprises the real-time usage time of the firefighting hardware, real-time maintenance of the firefighting hardware, and real-time fire alarminformation collected by using the firefighting hardware. According to the system provided by the present invention, the integrated management of the fire protection industry can be implemented, andthe work efficiency and management of fire protection can be improved.",project-academic
10.1109/TASE.2018.2847222,2019-04-01,a,IEEE,integration of robotic vision and tactile sensing for wire terminal insertion tasks," This paper reports the development of a manipulation system for electric wires, implemented by means of a commercial gripper installed on an industrial manipulator and equipped with cameras and suitably designed tactile sensors. The purpose of this system is the execution of wire insertion on commercial electromechanical components. The synergy between computer vision and tactile sensing is necessary because, in a real environment, the tight spaces very often prevent the possibility to use the vision system, also when the same task is performed by a human being. A novel technique to speed up the generation of training data sets for convolutional neural networks (CNNs) is proposed. Therefore, this technique is used to train a CNN in order to detect small objects (such as wire terminals). Moreover, aiming to prevent faults during the task and to interact with the environment safely, several machine learning approaches are used to produce an affordable output from the tactile sensor. The proposed approach shows how a cheap sensor embedded with suitable intelligence can provide information comparable to a more expensive force sensor. None Note to Practitioners —This paper was motivated by the lack of commercial solution for the automatic cabling of switchgears. Existing approaches to this problem are in some way limited to specific large-scale products or simple layouts. This paper investigated a robust and flexible solution, based on the exploitation of multiple sensors and machine learning algorithms, for wire detection, grasping, and connection. The proposed approach is characterized by simple design and self-tuning capabilities, and it can be easily employed on a wide range of switchgear layouts thanks to the large workspace of the manipulator. Experimental results show that the proposed system is able to achieve a 95% success rate within a realistic admissible region. In the future research, we will integrate the proposed solution with an electromechanical component localization module and a terminal fastening system to evaluate the performance on the real production line.",project-academic
10.3390/SMARTCITIES1010008,2018-11-06,a,Multidisciplinary Digital Publishing Institute,digital systems in smart city and infrastructure digital as a service," Digitalization has enabled infrastructure and cities to be “smarter”; the use of physical space and energy, the transmission of information, the management of users, assets and processes, the operation of businesses and companies have been progressively digitalized. The main challenges of a Smart City is its definition, scope and interconnections; there are different approaches to Smart City implementations that vary from collaborative multidisciplinary environments, the addition of Information and Communications Technology (ICT) within its physical fabric to the use of Big Data for higher abstraction decisions. This paper presents the concept of Digital as a Service (DaaS), where any complete digitalization can be implemented independently of its associated physical infrastructure in a Cloud environment; DasS would enable an interoperable Virtual Digital Infrastructure (VDI). In addition, this paper reviews the current Digital Systems, Transmission Networks, Servers and Management Systems. The next Industrial Revolution will be founded on Artificial Intelligence that will entirely replace humans by taking production and management decisions based on the Internet of Things (IoT), the Cloud, BlockChain, Big Data, Virtual Reality and the combination of digital and real infrastructure or city. Digital as a Service would be its enabler by providing the entire interconnection, integration and virtualization of its Space, Services and Structure (3S).",project-academic
10.1109/TVLSI.2018.2797600,2018-02-12,a,IEEE,stream processing dual track cgra for object inference," With the development of machine learning technology, the exploration of energy-efficient and flexible architectures for object inference algorithms is of growing interest in recent years. However, not many publications concentrate on a coarse-grained reconfigurable architecture (CGRA) for object inference algorithms. This paper provides a stream processing, dual-track programming CGRA-based approach to address the inherent computing characteristics of algorithms in object inference. Based on the proposed approach, an architecture called stream dual-track CGRA (SDT-CGRA) is presented as an implementation prototype. To evaluate the performance, the SDT-CGRA is realized in Verilog HDL and implemented in Semiconductor Manufacturing International Corporation 55-nm process, with the footprint of 5.19 mm2 at 450 MHz. Seven object inference algorithms, including convolutional neural network (CNN), None None None ${k}$ None None -means, principal component analysis (PCA), spatial pyramid matching (SPM), linear support vector machine (SVM), Softmax, and Joint Bayesian, are selected as benchmarks. The experimental results show that the SDT-CGRA can gain on average 343.8 times and 17.7 times higher energy efficiency for Softmax, PCA, and CNN, 621.0 times and 1261.8 times higher energy efficiency for None None None ${k}$ None None -means, SPM, linear-SVM, and Joint-Bayesian algorithms when compared with the Intel Xeon E5-2637 CPU and the Nvidia TitanX graphics processing unit. When compared with the state-of-the-art solutions of AlexNet on field-programmable gate array and CGRA, the proposed SDT-CGRA can achieve a 1.78 times increase in energy efficiency and a 13 times speedup, respectively.",project-academic
,2018-09-07,,,industrial device intelligent control system and method," An industrial device intelligent control system and method. The system comprises: an RT module (101), an AI module (102) and an APP module (103), wherein the RT module (101) is connected to an industrial device by means of an interface, acquires field data of the industrial device, processes the field data, and transmits processing result data to the AI module (102) and the APP module (103); the AI module (102) carries out an intelligent analysis on the received processing result data and transmits an intelligent analysis result to the APP module (103) connected to the AI module; the APP module (103) comprises multiple APPs having multiple functions, determines and controls the analysis result, sends a feedback signal to the RT module (101) to achieve a closed-loop control strategy, and sends the feedback signal to the AI module (102) to implement parameter adjustment of an artificial intelligence model; and the APP module (103) further comprises an APP interface for data transmission. By means of the industrial device intelligent control system and method, data collection, monitoring and interaction of an industrial device, data-based model learning and optimization, and real-time update response of a control strategy can be implemented.",project-academic
10.1109/TCSII.2010.2092827,2011-01-20,a,IEEE,current mode analog adaptive mechanism for ultra low power neural networks," Neural networks (NNs) implemented at the transistor level are powerful adaptive systems. They can perform hundreds of operations in parallel but at the expense of a large number of building blocks. In the case of analog realization, an extremely low chip area and low power dissipation can be achieved. To accomplish this, the building blocks should be simple. This brief presents a new current-mode low-complexity flexible adaptive mechanism (ADM) with a strongly reduced leakage in analog memory. Input signals ranging from 0.5 to 20 μA are held for 10-50 ms, with the leakage rate from 0.2%/ms to 0.04%/ms, respectively, depending on temperature. A small storage capacitor of 200 fF enables a short write time ( <; 100 ns). A single ADM cell occupies 1400 μm2 when realized in the Taiwan Semiconductor Manufacturing Company Ltd. CMOS 0.18-μm technology. The potential application of this NN is envisioned in a mobile platform based on a wireless sensor network to be used for online analysis of electrocardiography signals.",project-academic
10.3390/S18072110,2018-06-30,a,Multidisciplinary Digital Publishing Institute,lired a light weight real time fault detection system for edge computing using lstm recurrent neural networks," Monitoring the status of the facilities and detecting any faults are considered an important technology in a smart factory. Although the faults of machine can be analyzed in real time using collected data, it requires a large amount of computing resources to handle the massive data. A cloud server can be used to analyze the collected data, but it is more efficient to adopt the edge computing concept that employs edge devices located close to the facilities. Edge devices can improve data processing and analysis speed and reduce network costs. In this paper, an edge device capable of collecting, processing, storing and analyzing data is constructed by using a single-board computer and a sensor. And, a fault detection model for machine is developed based on the long short-term memory (LSTM) recurrent neural networks. The proposed system called LiReD was implemented for an industrial robot manipulator and the LSTM-based fault detection model showed the best performance among six fault detection models.",project-academic
10.1016/J.FUTURE.2019.04.014,2019-10-01,a,North-Holland,tide time relevant deep reinforcement learning for routing optimization," Abstract None None Routing optimization has been researched in network design for a long time, and plenty of optimization schemes have been proposed from both academia and industry. However, such schemes are either too complicated in applications or far from the optimal performance. In recent years, with the development of Software-defined Networking (SDN) and Artificial Intelligence (AI), AI-based methods of routing strategy are being considered. In this paper, we propose TIDE, an intelligent network control architecture based on deep reinforcement learning that can dynamically optimize routing strategies in an SDN network without human experience. TIDE is implemented and validated on a real network environment. Experiment result shows that TIDE can adjust the routing strategy dynamically according to the network condition and can improve the overall network transmitting delay by about 9% compared with traditional algorithms.",project-academic
10.1186/S13673-019-0190-9,2019-12-01,a,SpringerOpen,ciot net a scalable cognitive iot based smart city network architecture," In the recent era, artificial intelligence (AI) is being used to support numerous solutions for human beings, such as healthcare, autonomous transportation, and so on. Cognitive computing is represented as a next-generation application AI-based solutions which provide human–machine interaction with personalized interactions and services that imitate human behavior. On the other hand, a large volume of data is generated from smart city applications such as healthcare, smart transportation, retail industry, and firefighting. There is always a concern on how to efficiently manage the large volume of generated data. Recently many existing researches discussed the analysis of the large quantity of data using cognitive computing; however, these researches are failed to handle the certain problems, namely scalability, and flexibility of data gathered in a smart city environment. Data captured from millions of sensors can be cross implemented across various cognitive computing applications to ensure real-time responses. In this paper, we study the cognitive internet of things (CIoT) and propose a CIoT-based smart city network (CIoT-Net) architecture which describes how data gathered from smart city applications can be analyzed using cognitive computing and handle the scalability and flexibility problems. We discuss various technologies such as AI and big data analysis to implement the proposed architecture. Finally, we describe the possible research challenges and opportunities while implementing the proposed architecture.",project-academic
10.1145/3209914.3209921,2018-04-27,p,ACM,predicting basketball results using cascading algorithm," Anybody can guess the winners of a basket game. The question is how big the chances are in predicting the real winners. Relying only on the experts' experiences and intuition could not discover all the value and potential of the collected data. Driven by the increasing comprehensive data in sports datasets and data mining technique successfully used in different areas, sports data mining technique emerges and enables us to find hidden knowledge to impact the sports industry. A more scientific approach is needed to use for these data that are collected. Some predictors based only on winning records and some based only on statistical records of both teams. There are also predictors which use both types of data, but the accuracy of applying different individual algorithms is only ranging about 60% - 70%. To achieve better prediction rates and deal with that complexity, a lot of machine learning methods have been implemented over these data. This paper presents an improved technique for predicting basketball game results implementing cascading algorithm. The researchers combined Naive Bayes, Four Factor Analysis, and Fuzzy Logic Algorithms to predict basketball game result in an acceptable level of 69% - 70% accuracy. The researchers tested several times using data sets from NBA game Season 2015-2016, and the cascading algorithm result manages to reach 70% prediction accuracy. The result of this system can be used to assist basketball coaches in making plans for possible team developments. Also, the forecasted results can serve as an aid in building effective gameplay.",project-academic
10.1016/J.JMSY.2017.12.001,2018-01-01,a,Elsevier,data driven cost estimation for additive manufacturing in cybermanufacturing," Abstract None None Cybermanufacturing is a new paradigm that both manufacturing software and hardware tools are seamlessly integrated by enabling information infrastructure and are accessed as services in cyberspace. This paradigm encourages tool sharing and reuse thus can reduce cost and time in product realization. In this research, a new cost estimation framework is developed based on big data analytics tools so that the manufacturing cost associated with a new job can be estimated based on the similar ones in the past. Manufacturers can use this cost analytics service in their job bidding process, which is currently ad hoc and subjective in industry practice. The new framework is implemented and demonstrated for additive manufacturing, where the similarities of 3D geometry of parts and printing processes are established by identifying relevant features. Machine learning algorithms for dynamic clustering, LASSO and elastic net regressions are applied to feature vectors to predict the cost based on historical data.",project-academic
10.1016/J.ENVSOFT.2003.10.003,2004-10-01,a,Elsevier,modelling so2 concentration at a point with statistical approaches," In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy.",project-academic
10.1007/S00170-015-7422-6,2016-03-01,a,Springer London,on line learning of welding bead geometry in industrial robots," In this paper, we propose an architecture based on an artificial neural network (ANN), to learn welding skills automatically in industrial robots. With the aid of an optic camera and a laser-based sensor, the bead geometry (width and height) is measured. We propose a real-time computer vision algorithm to extract training patterns in order to acquire knowledge to later predict specific geometries. The proposal is implemented and tested in an industrial KUKA KR16 robot and a GMAW type machine within a manufacturing cell. Several data analysis are described as well as off-line and on-line training, learning strategies, and testing experimentation. It is demonstrated during our experiments that, after learning the skill, the robot is able to produce the requested bead geometry even without any knowledge about the welding parameters such as arc voltage and current. We implemented an on-line learning test, where the whole experiments and learning process take only about 4 min. Using this knowledge later, we obtained up to 95 % accuracy in prediction.",project-academic
10.1007/S00170-017-1039-X,2018-02-01,a,Springer London,on the use of machine learning methods to predict component reliability from data driven industrial case studies," The reliability estimation of engineered components is fundamental for many optimization policies in a production process. The main goal of this paper is to study how machine learning models can fit this reliability estimation function in comparison with traditional approaches (e.g., Weibull distribution). We use a supervised machine learning approach to predict this reliability in 19 industrial components obtained from real industries. Particularly, four diverse machine learning approaches are implemented: artificial neural networks, support vector machines, random forest, and soft computing methods. We evaluate if there is one approach that outperforms the others when predicting the reliability of all the components, analyze if machine learning models improve their performance in the presence of censored data, and finally, understand the performance impact when the number of available inputs changes. Our experimental results show the high ability of machine learning to predict the component reliability and particularly, random forest, which generally obtains high accuracy and the best results for all the cases. Experimentation confirms that all the models improve their performance when considering censored data. Finally, we show how machine learning models obtain better prediction results with respect to traditional methods when increasing the size of the time-to-failure datasets.",project-academic
10.1109/ACCESS.2020.3004790,2020-06-25,a,Institute of Electrical and Electronics Engineers (IEEE),a novel smart healthcare design simulation and implementation using healthcare 4 0 processes," Blockchain technology is found to have its applicability in almost every domain because of its advantages such as crypto-security, transparency, immutability, decentralized data network. In present times, a smart healthcare system with a blockchain data network and healthcare 4.0 processes provides transparency, easy and faster accessibility, security, efficiency, etc. Healthcare 4.0 trends include industry 4.0 processes such as the internet of things (IoT), industrial IoT (IIoT), cognitive computing, artificial intelligence, cloud computing, fog computing, edge computing, etc. The goal of this work is to design a smart healthcare system and it is found to be possible through integration and interoperability of Blockchain 3.0 and Healthcare 4.0 in consideration with healthcare ground-realities. Here, healthcare 4.0 processes used for data accessibility are targeted to be validated through statistical simulation-optimization methods and algorithms. The blockchain is implemented in the Ethereum network, and with associated programming languages, tools, and techniques such as solidity, web3.js, Athena, etc. Further, this work prepares a comparative and comprehensive survey of state-of-the-art blockchain-based smart healthcare systems. The comprehensive survey includes methodology, applications, requirements, outcomes, future directions, etc. A list of groups, organizations, and enterprises are prepared that are working in electronic health records (EHR), electronic medical records (EMR) or electronic personal records (EPR) mainly, and a comparative analysis is drawn concerning adopting the blockchain technology in their processes. This work has explored optimization algorithms applicable to Healthcare 4.0 trends and improves the performance of blockchain-based decentralized applications for the smart healthcare system. Further, smart contracts and their designs are prepared for the proposed system to expedite the trust-building and payment systems. This work has considered simulation and implementation to validate the proposed approach. Simulation results show that the Gas value required (indicating block size and expenditure) lies within current Etherum network Gas limits. The proposed system is active because block utilization lies above 80%. Automated smart contract execution is below 20 seconds. A good number (average 3 per simulation time) is generated in the network that indicates a health competition. Although there is error observed in simulation and implementation that lies between 0.55% and 4.24%, these errors are not affecting overall system performance because simulated and actual (taken in state-of-the-art) data variations are negligible.",project-academic
10.3390/RS11192252,2019-09-27,a,Multidisciplinary Digital Publishing Institute,sensor reliability in cyber physical systems using internet of things data a review and case study," Nowadays, reliability of sensors is one of the most important challenges for widespread application of Internet-of-things data in key emerging fields such as the automotive and manufacturing sectors. This paper presents a brief review of the main research and innovation actions at the European level, as well as some on-going research related to sensor reliability in cyber-physical systems (CPS). The research reported in this paper is also focused on the design of a procedure for evaluating the reliability of Internet-of-Things sensors in a cyber-physical system. The results of a case study of sensor reliability assessment in an autonomous driving scenario for the automotive sector are also shown. A co-simulation framework is designed in order to enable real-time interaction between virtual and real sensors. The case study consists of an IoT LiDAR-based collaborative map in order to assess the CPS-based co-simulation framework. Specifically, the sensor chosen is the Ibeo Lux 4-layer LiDAR sensor with IoT added capabilities. The modeling library for predicting error with machine learning methods is implemented at a local level, and a self-learning-procedure for decision-making based on Q-learning runs at a global level. The study supporting the experimental evaluation of the co-simulation framework is presented using simulated and real data. The results demonstrate the effectiveness of the proposed method for increasing sensor reliability in cyber-physical systems using Internet-of-Things data.",project-academic
10.1016/J.NEUCOM.2019.12.033,2020-03-28,a,Elsevier,intelligent cross machine fault diagnosis approach with deep auto encoder and domain adaptation," Abstract None None Recently, due to the rising industrial demands for intelligent machinery fault diagnosis with strong generalization, transfer learning techniques have been used to enhance adaptability of data-driven approaches. Particularly, the domain shift problem where training and testing data are sampled from different operating conditions of the same machine is well addressed. However, it is still difficult to prepare sufficient labeled data on the tested machine. Therefore, the idea of transferring fault diagnosis knowledge learned from one machine to different but related machines is motivated, and that is realized through a deep learning-based method in this paper. Features of different equipments are first projected into the same subspace using an auto-encoder structure, and cross-machine adaptation algorithm is adopted for knowledge generalization, where the distribution discrepancy between data from different machines is minimized. Experiments on three rolling bearing datasets are implemented to validate the proposed method. The results suggest it is feasible to transfer fault diagnosis knowledge across different machines, and the proposed method offers a novel and promising approach for knowledge generalization.",project-academic
10.1016/J.COMPIND.2019.04.016,2019-09-01,a,Elsevier,a comparison of fog and cloud computing cyber physical interfaces for industry 4 0 real time embedded machine learning engineering applications," Abstract None None Industrial cyber-physical systems are the primary enabling technology for Industry 4.0, which combine legacy industrial and control engineering, with emerging technology paradigms (e.g. big data, internet-of-things, artificial intelligence, and machine learning), to derive self-aware and self-configuring factories capable of delivering major production innovations. However, the technologies and architectures needed to connect and extend physical factory operations to the cyber world have not been fully resolved. Although cloud computing and service-oriented architectures demonstrate strong adoption, such implementations are commonly produced using information technology perspectives, which can overlook engineering, control and Industry 4.0 design concerns relating to real-time performance, reliability or resilience. Hence, this research compares the latency and reliability performance of cyber-physical interfaces implemented using traditional cloud computing (i.e. centralised), and emerging fog computing (i.e. decentralised) paradigms, to deliver real-time embedded machine learning engineering applications for Industry 4.0. The findings highlight that despite the cloud’s highly scalable processing capacity, the fog’s decentralised, localised and autonomous topology may provide greater consistency, reliability, privacy and security for Industry 4.0 engineering applications, with the difference in observed maximum latency ranging from 67.7%–99.4%. In addition, communication failures rates highlighted differences in both consistency and reliability, with the fog interface successfully responding to 900,000 communication requests (i.e. 0% failure rate), and the cloud interface recording failure rates of 0.11%, 1.42%, and 6.6% under varying levels of stress.",project-academic
10.1109/37.621469,1997-10-01,a,IEEE,human control strategy abstraction verification and replication," In this article, we describe and develop methodologies for modeling and transferring human control strategy. This research has potential application in a variety of areas such as the intelligent vehicle highway system, human-machine interfacing, real-time training, space telerobotics, and agile manufacturing. We specifically address the following issues: (1) how to efficiently model human control strategy through learning cascade neural networks, (2) how to select state inputs in order to generate reliable models, (3) how to validate the computed models through an independent, hidden Markov model-based procedure, and (4) how to effectively transfer human control strategy. We have implemented this approach experimentally in the real-time control of a human driving simulator, and are working to transfer these methodologies for the control of an autonomous vehicle and a mobile robot. In providing a framework for abstracting computational models of human skill, we expect to facilitate analysis of human control, the development of human-like intelligent machines, improved human-robot coordination, and the transfer of skill from one human to another.",project-academic
,2020-05-02,a,,an ensemble deep learning based cyber attack detection in industrial control system," The integration of communication networks and the Internet of Things (IoT) in Industrial Control Systems (ICSs) increases their vulnerability towards cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection Systems (IDSs), which are mainly developed to support Information Technology (IT) systems, count vastly on predefined models and are trained mostly on specific cyber-attacks. Besides, most IDSs do not consider the imbalanced nature of ICS datasets, thereby suffering from low accuracy and high false positive on real datasets. In this paper, we propose a deep representation learning model to construct new balanced representations of the imbalanced dataset. The new representations are fed into an ensemble deep learning attack detection model specifically designed for an ICS environment. The proposed attack detection model leverages Deep Neural Network (DNN) and Decision Tree (DT) classifiers to detect cyber-attacks from the new representations. The performance of the proposed model is evaluated based on 10-fold cross-validation on two real ICS datasets. The results show that the proposed method outperforms conventional classifiers, including Random Forest (RF), DNN, and AdaBoost, as well as recent existing models in the literature. The proposed approach is a generalized technique, which can be implemented in existing ICS infrastructures with minimum changes.",project-academic
10.1109/ACCESS.2020.2992249,2020-05-13,a,IEEE,an ensemble deep learning based cyber attack detection in industrial control system," The integration of communication networks and the Internet of Things (IoT) in Industrial Control Systems (ICSs) increases their vulnerability towards cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection Systems (IDSs), which are mainly developed to support information technology systems, count vastly on predefined models and are trained mostly on specific cyber-attacks. Besides, most IDSs do not consider the imbalanced nature of ICS datasets, thereby suffering from low accuracy and high false-positive when being put to use. In this paper, we propose a deep learning model to construct new balanced representations of the imbalanced datasets. The new representations are fed into an ensemble deep learning attack detection model specifically designed for an ICS environment. The proposed attack detection model leverages Deep Neural Network (DNN) and Decision Tree (DT) classifiers to detect cyber-attacks from the new representations. The performance of the proposed model is evaluated based on 10-fold cross-validation on two real ICS datasets. The results show that the proposed method outperforms conventional classifiers, including Random Forest (RF), DNN, and AdaBoost, as well as recent existing models in the literature. The proposed approach is a generalized technique, which can be implemented in existing ICS infrastructures with minimum effort.",project-academic
,2013-09-18,,,performance prediction method applicable to dynamic scheduling for semiconductor production line," The invention discloses a performance prediction method applicable to dynamic scheduling for a semiconductor production line. An extreme learning machine (ELM) is applied to prediction and modeling in the performance prediction method. Feeding control and scheduling rules are considered in a unified manner in the method, short-term scheduling key performance indexes such as an equipment utilization rate and a movement step number are predicted on the basis of a real-time state of a system, and a foundation is provided for dynamic real-time scheduling. A novel feed-forward neural network of the ELM is introduced into the semiconductor manufacturing system, and a prediction model is built by the aid of available data of the production line. As shown by test results, ideal prediction results can be quickly acquired by the method implemented by the aid of the ELM, the method has obvious advantages and an obvious application prospect in the aspects of parameter selection and learning speed as compared with the traditional neural network modeling method, and a new idea is provided for online optimal control.",project-academic
10.1016/J.IJMULTIPHASEFLOW.2019.103194,2020-05-01,a,Pergamon,bubble patterns recognition using neural networks application to the analysis of a two phase bubbly jet," Abstract None None Gas-liquid two-phase bubbly flows are found in different areas of science and technology such as nuclear energy, chemical industry, or piping systems. Optical diagnostics of two-phase bubbly flows with modern panoramic techniques makes it possible to capture simultaneously instantaneous characteristics of both continuous and dispersed phases with a high spatial resolution. In this paper, we introduce a novel approach based on neural networks to recognize bubble patterns in images and identify their geometric parameters. The originality of the proposed method consists in training of a neural network ensemble using synthetic images that resemble real photographs gathered in experiment. The use of neural networks in combination with automatically generated data allowed us to detect overlapping, blurred, and non-spherical bubbles in a broad range of volume gas fractions. Experiments on a turbulent bubbly jet proved that the implemented method increases the identification accuracy, reducing errors of various kinds, and lowers the processing time compared to conventional recognition methods. Furthermore, utilizing the new method of bubbles recognition, the primary physical parameters of a dispersed phase, such as bubble size distribution and local gas content, were calculated in a near-to-nozzle region of the bubbly jet. The obtained results and integral experimental parameters, especially volume gas fraction, are in good agreement with each other.",project-academic
10.1016/J.COMPAG.2012.07.009,2012-11-01,a,Elsevier B.V.,embedded portable device for herb leaves recognition using image processing techniques and neural network algorithm," Herbs have been widely used in food preparation, medicine and cosmetic industry. Knowing which herbs to be used would be very critical in these applications. Nevertheless, the current way of identification and determination of the types of herbs is still being done manually and prone to human error. Designing a convenient and automatic recognition system of herbs species is essential since this will improve herb species classification efficiency. This research focus on recognition approach to the shape and texture features of the herbs leaves. It aims to realize the computerized method to classify the herbs plants in a very convenient way. Portable herb leaves recognition system through image and data processing techniques is implemented as automated herb plant classification system. It is very easy to use and inexpensive system designed especially for helping scientist in agricultural field. The proposed system employs neural networks algorithm and image processing techniques to perform recognition on twenty species of herbs. One hundred samples for each species went through the system and the recognition accuracy was at 98.9%. Most importantly the system is capable of identifying the herbs leaves species even though they are dried, wet, torn or deformed. The efficiency and effectiveness of the proposed method in recognizing and classifying the different herbs species is demonstrated by experiments.",project-academic
10.1016/J.AEI.2020.101037,2020-01-01,a,Elsevier,a smart surface inspection system using faster r cnn in cloud edge computing environment," Abstract None None Automated surface inspection has become a hot topic with the rapid development of machine vision technologies. Traditional machine vision methods need experts to carefully craft image features for defect detection. This limits their applications to wider areas. The emerging convolutional neural networks (CNN) can automatically extract features and yield good results in many cases. However, the CNN-based image classification methods are more suitable for flat surface texture inspection. It is difficult to accurately locate small defects in geometrically complex products. Furthermore, the computational power required in CNN algorithms is usually high and it is not efficient to be implemented on embedded hardware. To solve these problems, a smart surface inspection system is proposed using faster R-CNN algorithm in the cloud-edge computing environment. The faster R-CNN as a CNN-based object detection method can efficiently identify defects in complex product images and the cloud-edge computing framework can provide fast computation speed and evolving algorithm models. A real industrial case study is presented to illustrate the effectiveness of the proposed method. The results show that the proposed method can provide high detection accuracy within a short time.",project-academic
10.1109/UIC-ATC.2017.8397649,2017-08-01,p,IEEE,a cnn based bagging learning approach to short term load forecasting in smart grid," Short-term load forecasting in smart grid is key to electricity dispatch scheduling, reliability analysis, and maintenance planning for the generators. In this paper, we present a convolutional neural networks (CNN) based bagging model for forecasting hourly loads. We employ CNN to train forecasting models on big load data sets. Then, we segment a real industry load data set into many subsets, fine-tune the forecasting models on these subsets to learn weak forecasting models, and assemble these weak forecasting models to conduct a bagging forecasting model, where the learning and assembling procedures are implemented on Spark. Specifically, all load samples in those data sets are reorganized as images with respect to similarities between relations of pixels in images and those of features in load samples. Experimental results indicate the effectiveness of the proposed method.",project-academic
10.5194/ACP-21-773-2021,2021-01-20,a,Copernicus GmbH,time resolved emission reductions for atmospheric chemistry modelling in europe during the covid 19 lockdowns," Abstract. We quantify the reductions in primary emissions due to the COVID-19 lockdowns in Europe. Our estimates are provided in the form of a dataset of reduction factors varying per country and day that will allow modelling and identifying the associated impacts upon air quality. The country- and daily-resolved reduction factors are provided for each of the following source categories: energy industry (power plants), manufacturing industry, road traffic and aviation (landing and take-off cycle). We computed the reduction factors based on open access and near-real time measured activity data from a wide range of information sources. We also trained a machine learning model with meteorological data to derive weather-normalised electricity consumption reductions. The time period covered is from 21 February, when the first European localised lockdown was implemented in the region of Lombardy (Italy), until 26 April 2020. This period includes five weeks (23 March until 26 April) with the most severe and relatively unchanged restrictions upon mobility and socio-economic activities across Europe. The computed reduction factors were combined with the Copernicus Atmosphere Monitoring Service's European emission inventory using adjusted emission temporal profiles in order to derive time-resolved emission reductions per country and pollutant sector. During the most severe lockdown period, we estimate the average emission reductions to be −33 % for NOx, −8 % for NMVOC, −7 % for SOx and −7 % for PM2.5 at the EU-30 level (EU-28 plus Norway and Switzerland). For all pollutants more than 85 % of the total reduction is attributable to road transport, except SOx. The reductions reached −50 % (NOx), −14 % (NMVOC), −12 % (SOx) and −15 % (PM2.5) in countries where the lockdown restrictions were more severe such as Italy, France or Spain. To show the potential for air quality modelling we simulated and evaluated NO2 concentration decreases in rural and urban background regions across Europe (Italy, Spain, France, Germany, United-Kingdom and Sweden). We found the lockdown measures to be responsible for NO2 reductions of up to −58 % at urban background locations (Madrid, Spain) and −44 % at rural background areas (France), with an average contribution of the traffic sector to total reductions of 86 % and 93 %, respectively. A clear improvement of the modelled results was found when considering the emission reduction factors, especially in Madrid, Paris and London where the bias is reduced with more than 90 %. Future updates will include the extension of the COVID-19 lockdown period covered, the addition of other pollutant sectors potentially affected by the restrictions (commercial/residential combustion and shipping) and the evaluation of other air quality pollutants such as O3 and PM2.5. All the emission reduction factors are provided in the supplementary material.",project-academic
,2019-09-23,a,,machine learning optimization algorithms portfolio allocation," Portfolio optimization emerged with the seminal paper of Markowitz (1952). The original mean-variance framework is appealing because it is very efficient from a computational point of view. However, it also has one well-established failing since it can lead to portfolios that are not optimal from a financial point of view. Nevertheless, very few models have succeeded in providing a real alternative solution to the Markowitz model. The main reason lies in the fact that most academic portfolio optimization models are intractable in real life although they present solid theoretical properties. By intractable we mean that they can be implemented for an investment universe with a small number of assets using a lot of computational resources and skills, but they are unable to manage a universe with dozens or hundreds of assets. However, the emergence and the rapid development of robo-advisors means that we need to rethink portfolio optimization and go beyond the traditional mean-variance optimization approach. Another industry has faced similar issues concerning large-scale optimization problems. Machine learning has long been associated with linear and logistic regression models. Again, the reason was the inability of optimization algorithms to solve high-dimensional industrial problems. Nevertheless, the end of the 1990s marked an important turning point with the development and the rediscovery of several methods that have since produced impressive results. The goal of this paper is to show how portfolio allocation can benefit from the development of these large-scale optimization algorithms. Not all of these algorithms are useful in our case, but four of them are essential when solving complex portfolio optimization problems. These four algorithms are the coordinate descent, the alternating direction method of multipliers, the proximal gradient method and the Dykstra's algorithm.",project-academic
10.1002/9781119751182.CH8,2020-07-24,a,"John Wiley & Sons, Ltd",machine learning optimization algorithms portfolio allocation," Portfolio optimization emerged with the seminal paper of Markowitz (1952). The original mean-variance framework is appealing because it is very efficient from a computational point of view. However, it also has one well-established failing since it can lead to portfolios that are not optimal from a financial point of view. Nevertheless, very few models have succeeded in providing a real alternative solution to the Markowitz model. The main reason lies in the fact that most academic portfolio optimization models are intractable in real life although they present solid theoretical properties. By intractable we mean that they can be implemented for an investment universe with a small number of assets using a lot of computational resources and skills, but they are unable to manage a universe with dozens or hundreds of assets. However, the emergence and the rapid development of robo-advisors means that we need to rethink portfolio optimization and go beyond the traditional mean-variance optimization approach. Another industry has faced similar issues concerning large-scale optimization problems. Machine learning has long been associated with linear and logistic regression models. Again, the reason was the inability of optimization algorithms to solve high-dimensional industrial problems. Nevertheless, the end of the 1990s marked an important turning point with the development and the rediscovery of several methods that have since produced impressive results. The goal of this paper is to show how portfolio allocation can benefit from the development of these large-scale optimization algorithms. Not all of these algorithms are useful in our case, but four of them are essential when solving complex portfolio optimization problems. These four algorithms are the coordinate descent, the alternating direction method of multipliers, the proximal gradient method and the Dykstra's algorithm.",project-academic
,2019-07-17,a,,real time evasion attacks with physical constraints on deep learning based anomaly detectors in industrial control systems," Recently, a number of deep learning-based anomaly detection algorithms were proposed to detect attacks in dynamic industrial control systems. The detectors operate on measured sensor data, leveraging physical process models learned a priori. Evading detection by such systems is challenging, as an attacker needs to manipulate a constrained number of sensor readings in real-time with realistic perturbations according to the current state of the system. In this work, we propose a number of evasion attacks (with different assumptions on the attacker's knowledge), and compare the attacks' cost and efficiency against replay attacks. In particular, we show that a replay attack on a subset of sensor values can be detected easily as it violates physical constraints. In contrast, our proposed attacks leverage manipulated sensor readings that observe learned physical constraints of the system. Our proposed white box attacker uses an optimization approach with a detection oracle, while our black box attacker uses an autoencoder (or a convolutional neural network) to translate anomalous data into normal data. Our proposed approaches are implemented and evaluated on two different datasets pertaining to the domain of water distribution networks. We then demonstrated the efficacy of the real-time attack on a realistic testbed. Results show that the accuracy of the detection algorithms can be significantly reduced through real-time adversarial actions: for the BATADAL dataset, the attacker can reduce the detection accuracy from 0.6 to 0.14. In addition, we discuss and implement an Availability attack, in which the attacker introduces detection events with minimal changes of the reported data, in order to reduce confidence in the detector.",project-academic
10.2139/SSRN.3425827,2019-07-01,a,,machine learning optimization algorithms portfolio allocation," Portfolio optimization emerged with the seminal paper of Markowitz (1952). The original mean-variance framework is appealing because it is very efficient from a computational point of view. However, it also has one well-established failing since it can lead to portfolios that are not optimal from a financial point of view (Michaud, 1989). Nevertheless, very few models have succeeded in providing a real alternative solution to the Markowitz model. The main reason lies in the fact that most academic portfolio optimization models are intractable in real life although they present solid theoretical properties. By intractable we mean that they can be implemented for an investment universe with a small number of assets using a lot of computational resources and skills, but they are unable to manage a universe with dozens or hundreds of assets. However, the emergence and the rapid development of robo-advisors means that we need to rethink portfolio optimization and go beyond the traditional mean-variance optimization approach.

Another industry and branch of science has faced similar issues concerning large-scale optimization problems. Machine learning and applied statistics have long been associated with linear and logistic regression models. Again, the reason was the inability of optimization algorithms to solve high-dimensional industrial problems. Nevertheless, the end of the 1990s marked an important turning point with the development and the rediscovery of several methods that have since produced impressive results. The goal of this paper is to show how portfolio allocation can benefit from the development of these large-scale optimization algorithms. Not all of these algorithms are useful in our case, but four of them are essential when solving complex portfolio optimization problems. These four algorithms are the coordinate descent, the alternating direction method of multipliers, the proximal gradient method and the Dykstra's algorithm. This paper reviews them and shows how they can be implemented in portfolio allocation.",project-academic
10.1016/J.MEASUREMENT.2020.108052,2020-11-01,a,Elsevier,deep learning based prognostic approach for lithium ion batteries with adaptive time series prediction and on line validation," Abstract None None Prognostics for lithium-ion batteries is very critical in many industrial applications, and accurate prediction of battery state of health (SOH) is of great importance for health management. This paper proposes a novel deep learning-based prognostic method for lithium-ion batteries with on-line validation. An effective variant of recurrent neural network, i.e. long short-term memory structure, is used with variable input dimension, that facilitates network training with additional labeled samples. Adaptive time-series predictions are carried out for prognostics. An on-line validation method is further proposed for parameter optimization in real time based on the available system information, which allows for continuous model improvement. Experiments on a popular lithium-ion battery dataset are implemented to validate the effectiveness and superiority of the proposed method. The experimental results show the prognostic performances are promising both for the multi-steps-ahead predictions and long-horizon SOH estimations.",project-academic
10.1007/978-3-319-73751-5_33,2017-10-19,p,"Springer, Cham",an iterative closest point method for measuring the level of similarity of 3d log scans in wood industry," In the Canadian’s lumber industry, simulators are used to predict the lumbers resulting from the sawing of a log at a given sawmill. Giving a log or several logs’ 3D scans as input, simulators perform a real-time job to predict the lumbers. These simulators, however, tend to be slow at processing large volumes of wood. We thus explore an alternative approximation techniques based on the Iterative Closest Point (ICP) algorithm to identify the already processed log to which an unseen log resembles the most. The main benefit of the ICP approach is that it can easily handle 3D scans with a variable number of points. We compare this ICP-based nearest neighbour predictor, to predictors built using machine learning algorithms such as the K-nearest-neighbour (kNN) and Random Forest (RF). The implemented ICP-based predictor enabled us to identify key points in using the 3D scans directly for distance calculation. The long-term goal of this on-going research is to integrated ICP distance calculations and machine learning.",project-academic
,2017-10-23,a,,an iterative closest point method for measuring the level of similarity of 3d log scans in wood industry," In the Canadian's lumber industry, simulators are used to predict the lumbers resulting from the sawing of a log at a given sawmill. Giving a log or several logs' 3D scans as input, simulators perform a real-time job to predict the lumbers. These simulators, however, tend to be slow at processing large volume of wood. We thus explore an alternative approximation techniques based on the Iterative Closest Point (ICP) algorithm to identify the already processed log to which an unseen log resembles the most. The main benefit of the ICP approach is that it can easily handle 3D scans with a variable number of points. We compare this ICP-based nearest neighbor predictor, to predictors built using machine learning algorithms such as the K-nearest-neighbor (kNN) and Random Forest (RF). The implemented ICP-based predictor enabled us to identify key points in using the 3D scans directly for distance calculation. The long-term goal of this ongoing research is to integrated ICP distance calculations and machine learning.",project-academic
10.1016/J.AEI.2020.101044,2020-01-01,a,Elsevier,iot edge computing enabled collaborative tracking system for manufacturing resources in industrial park," Abstract None None In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability.",project-academic
,2008-12-22,b,,representation and management of narrative information theoretical principles and implementation," A big amount of important, economically relevant information, is buried within the huge mass of multimedia documents that correspond to some form of narrative description. Due to the ubiquity of these narrative resources, representing in a general, accurate, and effective way their semantic content i.e., their key meaning is then both conceptually relevant and economically important. This book presents the main properties of NKRL (Narrative Knowledge Representation Language), a language expressly designed for representing and managing, in a standardised way, the meaning of complex multimedia narrative documents. NKRL is a fully implemented language/environment that exists in two versions: a relational database-supported version and a file-oriented one. It constitutes probably the most complete and realistic effort realised so far to deal with the huge industrial potentialities of the narrative domain. Written from a multidisciplinary perspective, this book not only supplies an exhaustive description of NKRL and of the associated knowledge representation principles, it also constitutes an invaluable source of reference for practitioners, researchers and graduates in domains that range over narrative theories, linguistics and computational linguistics, artificial intelligence, knowledge bases, information retrieval, ontologies and the semantic Web.",project-academic
10.1109/BIGDATA.2016.7840831,2016-01-01,p,Institute of Electrical and Electronics Engineers Inc.,cloud based machine learning for predictive analytics tool wear prediction in milling," The proliferation of real-time monitoring systems and the advent of Industrial Internet of Things (IIoT) over the past few years necessitates the development of scalable and parallel algorithms that help predict mechanical failures and remaining useful life of a manufacturing system or system components. Classical model-based prognostics require an in-depth physical understanding of the system of interest and oftentimes assume certain stochastic or random processes. To overcome the limitations of model-based methods, data-driven methods such as machine learning have been increasingly applied to prognostics and health management (PHM). While machine learning algorithms are able to build accurate predictive models, large volumes of training data are required. Consequently, machine learning techniques are not computationally efficient for data-driven PHM. The objective of this research is to create a novel approach for machinery prognostics using a cloud-based parallel machine learning algorithm. Specifically, one of the most popular machine learning algorithms (i.e., random forest) is applied to predict tool wear in dry milling operations. In addition, a parallel random forest algorithm is developed using the MapReduce framework and then implemented on the Amazon Elastic Compute Cloud. Experimental results have shown that the random forest algorithm can generate very accurate predictions. Moreover, significant speedup can be achieved by implementing the parallel random forest algorithm.",project-academic
10.1145/3267851.3267918,2018-11-05,p,ACM,facsvatar an open source modular framework for real time facs based facial animation," Embodied Conversational Agents often employ advanced multimodal analysis of human users for affective inference, however, its facial expressions in response are often pre-made or coded animations. Generated data-driven facial animations have the advantage that they can be more natural and do not require a database at run-time. The open source modular framework FACSvatar is presented, which processes and animates FACS based data in real-time. Tools that create 3D human models are supported and facial data can be visualized in popular tools in the gaming industry. All functionality is split-up into modules set-up in a publisher-subscriber pattern to provide easy integration with other platforms. A deep learning module for real-time generation of AU data for data-driven animation is implemented. A user evaluation of their expressions being animated through our framework was done. Their ratings were slightly positive, but more improvements have to be made in terms of data quality and individual fine-tuning. Also, the modules' latency and performance have been measured. On average, FACS data is visualized in 28.55 ms.",project-academic
10.1109/INFOCT.2018.8356831,2018-03-23,p,IEEE,fault class prediction in unsupervised learning using model based clustering approach," Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",project-academic
10.1109/ICPHYS.2018.8390780,2018-05-15,p,IEEE,industrial cyber physical system for condition based monitoring in manufacturing processes," Nowadays, manufacturing processes is adopting solutions with more sensoring systems on the basis of Industrial Cyber-Physical System (ICPS) approaches in order to carry out real-time process monitoring, optimal parametrization and self-reconfiguration of machine tools, robots and industrial processes from individual equipment's to global production environments. The present article introduces an ICPS architecture for condition-based monitoring to manage alarm and events combining local information extracted by multiple sensors integrated a family of CNC machine tools with a cloud information as a service to manage and update the local parametrization in order to predict failure pattern in CNC machine tools. The architecture is divided in two modes: a local mode embedded in each CNC machine tool and a global mode able to connect and reconfigure the local monitoring system based on global information knowledge. Finally, a case study based on a bearing benchmark is selected to evaluate the behavior and accuracy of the proposed architecture and the implemented condition-based monitoring system in multiple local CNC Machines tool (local modes) with self-learning and optimal parametrization provided by the cloud service (global mode).",project-academic
10.1109/ITHINGS-GREENCOM-CPSCOM-SMARTDATA.2017.72,2017-06-21,p,IEEE,an iot based smart controlling system of air conditioner for high energy efficiency," In current electric energy statistics, the largest power is consumed by heating and cooling air conditioners, which are widely used in residential and commercial buildings. Hence, reducing energy consumption of air conditioners is vitally important for improving power utilization efficiency in global energy perspective. To save electricity consumption of air conditioners, this paper proposes an Internet of Things (IoT)-based smart controlling system including smart meter, smart gateway, and cloud computing modules. We manufacture a smart meter to control the compressor operation of air conditioners based on the specified temperature. Meanwhile, it is able to monitor the real-time power consumption datasets, which are delivered to a cloud server via a wireless gateway. Using Zigbee communication protocol, our developed gateway enables automatic detection of smart meter by a broadcasting method. After gathering the IP addresses of connected smart meters, the gateway dispatches controlling signals to relevant meters. We developed a general programming interface to control the operation of this smart gateway so as to support flexible and extensible IoT development. The collected electricity consumption datasets are transmitted to a cloud server in real time via Internet. Meanwhile, the remote operation signals of smart meters are transmitted to the cloud. An extreme learning machine is implemented to analyze the energy distribution for energy consumption prediction. Based on the analysis results and operation signals, the cloud generates an energy-saving decision to control the distributed air conditioners by the smart meters, which are linked to the Internet through the gateways. This way, an individual smart meter controls cooling and heating operations of its corresponding compressor to realize local energy management. Besides, the energy saving strategy eases power grid burden and decrease load of power station. Thus, our proposed system has positive influence on greenhouse gas reduction.",project-academic
10.1016/J.SNB.2007.12.032,2008-04-14,a,Elsevier,preemptive identification of optimum fermentation time for black tea using electronic nose," During black tea manufacturing, tealeaves pass through the fermentation process, when the grassy smell is transformed into a floral smell. Optimum fermentation is extremely crucial in deciding the final quality of finished tea and it is very important to terminate the fermentation process at the right time. Present day industry practice for monitoring of fermentation is purely subjective and is carried out by experienced personnel. In this paper, a study has been made on real-time smell monitoring of black tea during the fermentation process using electronic nose as well as prediction of the correct fermentation time. The study has been implemented in two steps. First, for prediction of optimum fermentation time, five different time-delay neural networks (TDNNs), named as multiple-time-delay neural networks (m-TDNN), have been used. During the second study, we have investigated the possibility of existence of different smell stages during the fermentation runs of black tea processing using self-organizing map (SOM), and then used three TDNNs for different smell stages. The results show excellent promise for the instrument to be used by the industry.",project-academic
10.1016/J.COMPIND.2019.04.010,2019-08-01,a,Elsevier,managing workflow of customer requirements using machine learning," Abstract None None Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored. None The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier. None The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system. None Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow.",project-academic
10.1016/J.CJCHE.2020.06.015,2020-06-20,a,Elsevier,deep learning technique for process fault detection and diagnosis in the presence of incomplete data," Abstract None None In modern industrial processes, timely detection and diagnosis of process abnormalities are critical for monitoring process operations. Various fault detection and diagnosis (FDD) methods have been proposed and implemented, the performance of which, however, could be drastically influenced by the common presence of incomplete or missing data in real industrial scenarios. This paper presents a new FDD approach based on an incomplete data imputation technique for process fault recognition. It employs the modified stacked autoencoder, a deep learning structure, in the phase of incomplete data treatment, and classifies data representations rather than the imputed complete data in the phase of fault identification. A benchmark process, the Tennessee Eastman process, is employed to illustrate the effectiveness and applicability of the proposed method.",project-academic
10.1016/S0952-1976(01)00033-1,2001-10-01,a,Pergamon,intelligent control of a rotary kiln fired with producer gas generated from biomass," Abstract None None During the past decade, the academic world has been extremely active in developing new algorithms and theories in the field of artificial intelligence (AI) and intelligent systems. In most cases, however, emphasis has been placed more on theoretical frameworks and mathematical bases than on what the individual AI techniques could offer and on how different techniques could be applied to solve real industrial-scale problems. The reputation of intelligent systems has consequently suffered from an inability to transfer new and sophisticated techniques to industrial applications with identifiable benefits. As a result, although a wide range of intelligent control techniques has been available already for many years, most of the applications in the process industry are based on more conventional techniques. Recently, as awareness of intelligent systems has grown, industrial problems and implementations have fortunately received increasing attention. In this paper, an intelligent supervisory-level system implemented at one of the major Finnish pulp mills to control a lime kiln fired with producer gas generated from biomass is presented. First, the major results of a field study are summarised, with special attention paid to burnt lime quality aspects. Next, a novel linguistic equations approach, which provides flexible methods for both modelling and control, is briefly described. The overall structure and main functions of the developed control system are then described with the main emphasis on the control of temperature and lime quality. Finally, the results obtained during the extended testing period of the system are presented and discussed.",project-academic
,2013-04-08,b,,discrete time inverse optimal control for nonlinear systems," Discrete-Time Inverse Optimal Control for Nonlinear Systems proposes a novel inverse optimal control scheme for stabilization and trajectory tracking of discrete-time nonlinear systems. This avoids the need to solve the associated Hamilton-Jacobi-Bellman equation and minimizes a cost functional, resulting in a more efficient controller. Design More Efficient Controllers for Stabilization and Trajectory Tracking of Discrete-Time Nonlinear Systems The book presents two approaches for controller synthesis: the first based on passivity theory and the second on a control Lyapunov function (CLF). The synthesized discrete-time optimal controller can be directly implemented in real-time systems. The book also proposes the use of recurrent neural networks to model discrete-time nonlinear systems. Combined with the inverse optimal control approach, such models constitute a powerful tool to deal with uncertainties such as unmodeled dynamics and disturbances. Learn from Simulations and an In-Depth Case Study The authors include a variety of simulations to illustrate the effectiveness of the synthesized controllers for stabilization and trajectory tracking of discrete-time nonlinear systems. An in-depth case study applies the control schemes to glycemic control in patients with type 1 diabetes mellitus, to calculate the adequate insulin delivery rate required to prevent hyperglycemia and hypoglycemia levels. The discrete-time optimal and robust control techniques proposed can be used in a range of industrial applications, from aerospace and energy to biomedical and electromechanical systems. Highlighting optimal and efficient control algorithms, this is a valuable resource for researchers, engineers, and students working in nonlinear system control.",project-academic
10.1016/J.ENECO.2017.06.020,2017-08-01,a,North-Holland,composite forecasting approach application for next day electricity price forecasting," Abstract None None Accurate forecasting of electricity prices can provide significant benefits to energy suppliers when allocating their assets and to energy consumers for defining an optimal portfolio. There are numerous methods that efficiently support the forecasting of time series, such as electricity prices, which have high volatility. However, the performance of these approaches varies depending on data sets and operational conditions. In this work, the concept of composite forecasting is presented and implemented in a retrospective study, in real industrial forecasting conditions to show the potential of forecast performance improvement and comparable high consistency of a forecast performance across different ‘Day Peak’ and ‘Day Base’ electricity price data sets for different seasons. As individual methods support vector regression, artificial neural networks and ridge regression are implemented. The forecast performances of these methods are evaluated and compared with their forecast combination using different error measures. The results show that composite forecasting processes with ‘inverse root mean squared error’ combination approach can generate, on average, a more accurate and robust forecast than using an individual methods or other combination schemas.",project-academic
10.1109/PARC49193.2020.236616,2020-02-01,p,IEEE,iot and cloud computing based smart water metering system," This paper focuses on the developmental and implementation methodology of smart water meter based on Internet of Things (IoT) and Cloud computing equipped with machine learning algorithms, to differentiate between normal and excessive water usage at industrial, domestic and all other sectors having an abundance of water usage, both for Indian and worldwide context. Recognizing that intelligent metering of water has the potential to alter customer engagement of water usage in urban and rural water supplies, this paper fosters for sustainable water management, a need of the present. With shrinking reserves of clean water resources worldwide, it is becoming cumbersome to cater for this resource to masses in the coming years on a consistent basis. Using our smart water meter, water resources can be managed efficiently and an optimum use could save water for the future generations. Sensors will provide for real time monitoring of hydraulic data, automated control and alarming from Cloud platform in case of events such as water leakages, excessive usage, etc. Analysis of the same will help in taking meaningful actions. Thus we do propose for a smart water metering technology that can be utilized by Indian citizens, and worldwide, to curb wastage of water. With an ease of monitoring and visualization of the data through the Cloud platform combined with machine learning based tools to detect excess water consumption, the server-less architecture we propose can be easily adopted and implemented in a large scale.",project-academic
10.1016/J.MICPRO.2019.102906,2020-02-01,a,Elsevier,area and power efficient pipelined hybrid merged adders for customized deep learning framework for fpga implementation," Abstract None None With the rapid growth of deep learning and neural network algorithms, various fields such as communication, Industrial automation, computer vision system and medical applications have seen the drastic improvements in recent years. However, deep learning and neural network models are increasing day by day, while model parameters are used for representing the models. Although the existing models use efficient GPU for accommodating these models, their implementation in the dedicated embedded devices needs more optimization which remains a real challenge for researchers. Thus paper, carries an investigation of deep learning frameworks, more particularly as review of adders implemented in the deep learning framework. A new pipelined hybrid merged adders (PHMAC) optimized for FPGA architecture which has more efficient in terms of area and power is presented. The proposed adders represent the integration of the principle of carry select and carry look ahead principle of adders in which LUT is re-used for the different inputs which consume less power and provide effective area utilization. The proposed adders were investigated on different FPGA architectures in which the power and area were analyzed. Comparison of the proposed adders with the other adders such as carry select adders (CSA), carry look ahead adder (CLA), Carry skip adders and Koggle Stone adders has been made and results have proved to be highly vital into a 50% reduction in the area, power and 45% when compared with above mentioned traditional adders.",project-academic
10.1109/SMC.2019.8913901,2019-10-01,p,IEEE,explainable machine learning in industry 4 0 evaluating feature importance in anomaly detection to enable root cause analysis," In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a ‘feature importance’ in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",project-academic
10.4236/ICA.2011.23028,2011-08-08,a,Scientific Research Publishing,fuzzy pid controllers using fpga technique for real time dc motor speed control," The design of intelligent control systems has become an area of intense research interest. The development of an effective methodology for the design of such control systems undoubtedly requires the synthesis of many concepts from artificial intelligence. The most commonly used controller in the industry field is the proportional-plus-integral-plus-derivative (PID) controller. Fuzzy logic controller (FLC) provides an alternative to PID controller, especially when the available system models are inexact or unavailable. Also rapid advances in digital technologies have given designers the option of implementing controllers using Field Programmable Gate Array (FPGA) which depends on parallel programming. This method has many advantages over classical microprocessors. In this research, A model of the fuzzy PID control system is implemented in real time with a Xilinx FPGA (Spartan-3A, Xilinx Company, 2007). It is introduced to maintain a constant speed to when the load varies.,The model of a DC motor is considered as a second order system with load variation as a an example for complex model systems. For comparison purpose, two widely used controllers “PID and Fuzzy” have been implemented in the same FPGA card to examine the performance of the proposed system. These controllers have been tested using Matlab/Simulink program under speed and load variation conditions. The controllers were implemented to run the motor as real time application under speed and load variation conditions and showed the superiority of Fuzzy-PID.",project-academic
10.1016/J.IJHEATMASSTRANSFER.2018.12.170,2019-05-01,a,Pergamon,visualization based nucleate boiling heat flux quantification using machine learning," Abstract None None Processes involving complex phenomena are ubiquitous in nature and industry, many of which are difficult to simulate computationally. Nucleate boiling heat transfer, for instance, has numerous practical applications, while the film boiling is an undesirable operation regime. So far, most correlations and computer simulations to quantify boiling heat transfer rely on direct measurement of thermohydraulic data, such as heater temperature, which is often invasive. Here it is demonstrated that neural network-based models can quantify heat transfer using only direct and indirect visual information of the boiling phenomenon, without any prior knowledge of the governing equations, which enables the non-intrusive measurement of heat flux based on boiling process imaging. It is shown that neural networks can encode bubble morphology and its correlation with heat flux returning errors as low as 7% when compared with precise experimental measurements, a significant improvement over current prediction methods of boiling heat transfer. Furthermore, it is shown that these systems may be implemented in inexpensive, compact computers, such as the Raspberry Pi, to infer heat flux in real time from visualization.",project-academic
10.1109/DTPI52967.2021.9540077,2021-07-15,p,IEEE,mechanical design paradigm based on acp method in parallel manufacturing," Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",project-academic
10.1117/12.2533485,2019-09-17,p,SPIE,physically based synthetic image generation for machine learning a review of pertinent literature," The term deep learning is almost on everyone's lips these days, in the area of computer vision manly because of the great advances deep learning approaches have made amongst others in object detection and classification. For general object location or classification tasks there do exist several giant databases containing several millions of labeled images and several thousands of different labels like COCO and ImageNet. In contrast in industrial applications like quality inspection there do hardly ever exist such training data not only for reasons of confidentiality of trade secrets. An obvious way to remedy this deficiency is the synthetic creation of image data. Physically based rendering attempts to achieve photorealistic images by accurately simulating the ow of light of the real world according to various physical laws. Therefor multiple techniques like Ray Tracing and Path Tracing have been implemented and are becoming increasingly widespread as hardware performance increases. The intent of this article is to give a wide but nevertheless preferably comprehensive overview which approaches have been pursued in recent literature to generate realistic synthetic training images. The development of various rendering methods from rasterization to bidirectional Monte Carlo path tracing is outlined, as well as their differences and use. Along with the terminology a few mathematical foundations like the Bidirectional Reflectance Distribution Function (BRDF) are briefly described. Altogether specially concern is given to industrial data and quality control, comparing literature and the practical application of its results.",project-academic
,2020-01-01,a,,internet of robotic things with digital platforms digitization of robotics enterprise," Internet of Things (IoT) provides a strong platform for computer users to connect objects, devices, and people to the Internet for exchanging or sharing of information with each other. IoT is growing rapidly and is expected to adapt to disciplines such as manufacturing, agriculture, healthcare, and robotics. Furthermore, the new concept of IoT is proposed and shown, especially for robotics areas as Internet of Robotics Things (IoRT). IoRT is a mixed structure of diverse technologies such as cloud computing, artificial intelligence, and machine learning. However, to promote and realize IoRT, digitization and digital transformation should be proceeded and implemented in the robotics enterprise. In this paper, we propose an architecture framework for IoRT-based digital platforms and verify it using a planned case in a global robotics enterprise. The associated challenges and future research directions in this field are also presented.",project-academic
10.1007/978-981-15-5784-2_31,2021-01-01,a,"Springer, Singapore",internet of robotic things with digital platforms digitization of robotics enterprise," Internet of Things (IoT) provides a strong platform for computer users to connect objects, devices, and people to the Internet for exchanging or sharing of information with each other. IoT is growing rapidly and is expected to adapt to disciplines such as manufacturing, agriculture, healthcare, and robotics. Furthermore, the new concept of IoT is proposed and shown, especially for robotics areas as Internet of Robotics Things (IoRT). IoRT is a mixed structure of diverse technologies such as cloud computing, artificial intelligence, and machine learning. However, to promote and realize IoRT, digitization and digital transformation should be proceeded and implemented in the robotics enterprise. In this paper, we propose an architecture framework for IoRT-based digital platforms and verify it using a planned case in a global robotics enterprise. The associated challenges and future research directions in this field are also presented.",project-academic
,2013-08-01,a,Advances in Management,knowledge management for the oil and gas industry opportunities and challenges," AbstractIn the current era of knowledge-driven society, knowledge becomes the most critical success factor in the current business environment. It needs to be handled and utilized effectively and efficiently to compete in the global market by creating a sustainable competitive advantage for the organisation. A technology-driven organisation needs to leverage knowledge management process to be effective and competitive, where professional can play an important role while managing the knowledge to handle the challenges comfortably. But the ability of handling cannot be inculcated within a day like technology, it is culture to be cultivated since a long time through experts and their experience they gained practices.The Oil and Gas industry has seen massive changes in the recent years influencing all its sectors, including searching, production, drilling and refining which in turn has great effects on their market and marketing strategies, production strategies and research and development strategies. These changes can only be achieved through effective knowledge management, covering both knowledge production or generation and knowledge sharing or transformation and distribution. That is not the issue today, where making the most of an oil field is a knowledge-intensive affair involving expertise in engineering, earth science and facilities maintenance.Keywords: Knowledge management, Knowledge and information management, Oil and gas industry, Energy sector.IntroductionThe primary objectives of Knowledge Management initiative in any organisation are to enhance the performance of the people involved along with the organisation. It is not mere knowledge sharing but also valuable by-product of the business process, by explicitly designing and implementing tools, processes, systematic approaches, structures, principles to improve the decision making with indirect improvements in identifications, capture validations and transformation of knowledge relevant for decision making.Today we have significant number of the tools to do successful business, methodology, methods to handle knowledge, to analyse its flow in organisation, way to improve the flow, opportunities to utilize up and way to observe.Today, with the help of Knowledge Management, ample number of tools, crisp methodologies, methods to provide ability to blend and various approaches to knowledge management are available to organization to enjoy competitive advantage. This progress gives strength to organization to handle and tackle real business problem easily. Thus, Knowledge Management becomes a solution provider to business problem with drastic difference; there are growing recognition to apply knowledge management to handle any business issues and progress. Apart from that it helps to create user friendly technologies to handle information with collaboration and access but Knowledge Management cannot be measured quantitatively but either impact gauged. Thus, Knowledge Management becomes a systematic approach to make decisions where and how to invest.Knowledge ManagementKnowledge Management can be defined as information practices and learning strategies accepted in organizations as a set of practices which help to improve the applications and use of data and information during decision making. Knowledge Management in any organization can be implemented by hiring a Chief Knowledge Officer (CKO) to improve information sharing, similarly as a system to support staff involved in the organization with help of technology, to enhance the information flow, to enhance evaluation of outcomes, to improve learning strategies and to develop a framework for the betterment of the organizational growth. To commence a project for the application of knowledge towards knowledge sharing also Knowledge Management can be implemented, to understand and estimate the long term effects of learning system.Knowledge Management Approach in the Oil and Gas SectorOrganisation are functioning with help of system, not machines, in organisational development the machine model describes different inputs for specific process into outputs, may not be accurate or useful in understanding the complexity of the functions. …",project-academic
10.1109/MODELS.2017.32,2017-09-17,p,IEEE Computer Society,the next evolution of mde a seamless integration of machine learning into domain modeling," Advances in software and sensors have led to a new generation of systems which can help to minimize human intervention in critical infrastructures, like the power grid. However, they have mainly been designed to face predictable situations, in order to react, for example, to a critical overload. This is called known domain knowledge. However, such systems have also to face events that are unpredictable at design time. For instance, the electric consumption of a house depends on the number of persons living there, their activities, weather conditions, used devices, and so forth. Despite such behaviour is unpredictable at design time, it is identifiable and a hypothesis about it can be already formulated and solved later by observing past situations, once data becomes available. Sutcliffe et al., [1] suggest to call this known unknown. Machine learning algorithms are designed to resolve these unknowns, using fine- or coarse-grained learning. Coarse-grained learning means extracting the average behaviour of a large dataset. Conversely, fine-grained learning means specializing learning algorithms only on specific elements. In cases where datasets are composed of independent and het-erogenous entities, which behave very differently, finding one coarse-grained common behaviour can be difficult or even inappropriate. For example, considering smart grids, the daily consumption of a factory follows a very different pattern than the consumption of an apartment. Thus, coarse-grained learning alone, which is based on the ""law of large numbers"", can be inaccurate for such systems. Additionally, any data changes requires the whole learning process to be recomputed. Instead, following a divide and conquer strategy, learning on finer granularities can be considerably more efficient [2], [3]. In accordance to the pedagogical concept [4], we refer to small fine-grained learning units as ""micro learning"". However, applying micro learning on systems, such as the electric grid, can potentially lead to many fine-grained learning units, that need to be combined and synchronised with domain data. Learning frameworks like TensorFlow focus solely on the learning flow without any relation to the domain model. Consequently, domain data and its structure is expressed in different models than learning tasks, using different languages and tools. This leads to a separation of domain data, knowledge, known unknowns, and associated learning methods. Therefore, an appropriate structure to model learning units and their relationships to domain knowledge is required. To tame such complexity, we propose to weave micro machine learning seamlessly into data modeling. Specifically, our approach aims at: (1) Structuring complex learning tasks with reusable, chainable, and independently computable micro learning units. (2) Seamlessly integrating behavioural models which are known at design time, behavioural models that need to be learned at runtime, and domain models using common modeling concepts. (3) Automating the mapping between the mathematical representation expected by a specific machine learning algorithm and the domain representation [5] and independently updating micro learning units to be fast enough for online learning. As a natural extension of model-driven engineering approaches, we take advantage of relationships between domain data and behavioural elements (learned or known at design time) to implicitly define a fine-grained mapping of learning units and domain data. We implemented and integrated our approach into the open-source modeling framework GreyCat, which is specifically designed for the requirements of CPSs and IoT. We evaluate our approach on a concrete smart grid case study and show that: (1) Micro machine learning for such scenarios can be more accurate than coarse-grained learning (2) Performance is fast enough to be used for real-time analytics. The full paper has been published in [6].",project-academic
10.1109/TIE.2019.2927197,2020-08-01,a,Institute of Electrical and Electronics Engineers (IEEE),data driven modeling based on two stream rm lambda gated recurrent unit network with soft sensor application," Data-driven soft sensors, estimating the pivotal quality variables, have been widely employed in industrial process. This paper proposes a novel soft sensor modeling approach based on a two-stream None None ${\rm{\lambda }}$ None None gated recurrent unit ( None $TS - {\rm{\lambda }}$ None None GRU) network. First, factors None None ${{\rm{\lambda }}_1}$ None None and None None ${{\rm{\lambda }}_2}$ None None are implemented to alter the linear constraint existing in the original GRU unit, enriching the information passing through. Then, a two-stream network structure is designed, equipped with some advanced network parameter adjustment techniques, such as batch normalization and dropout rate, to learn diverse features of the various process data. Finally, the learned features from the two streams are fused and a supervised learning regression layer is employed to decrease the error between the output and label. The application in melt viscosity index estimation for a real polymerization industrial process has demonstrated that the proposed None None $TS - {\rm{\lambda }}$ None None GRUs algorithm for soft sensor modeling is more accurate and promising than other existing methods.",project-academic
10.1109/TII.2018.2850001,2019-03-01,a,IEEE,event triggered globalized dual heuristic programming and its application to networked control systems," Networked control systems (NCSs) provide many benefits, such as higher control accuracy and better robustness with the successively increasing computational complexity and communication burden. This results in the traditional adaptive dynamic programming control method having difficulty meeting the real-time requirements of industrial systems. In this paper, a novel event-triggered globalized dual heuristic programming method is proposed to reduce the required samples while guaranteeing the stability of the system. In the proposed method, the NCSs can communicate and update the control law only when the designed event-triggered condition is violated. Furthermore, the Elman neural network, which is a dynamic feedback network with a memory function is implemented to reconstruct the state variables as an approximator, and it depends only on the input and output data. To obtain fewer event-triggered times, two optimization methods, i.e., the unscented Kalman filter and the multiobjective quantum particle swarm optimization, are used to optimize the initial weights of the networks and the positive constant in the event-triggered condition, respectively. The simulation results on industrial system of aluminum electrolysis production are included to verify the performance of the controller.",project-academic
10.1109/IRI.2019.00040,2019-07-01,p,Institute of Electrical and Electronics Engineers Inc.,a power efficient neural network implementation on heterogeneous fpga and gpu devices," Deep neural networks (DNNs) have seen tremendous industrial successes in various applications, including image recognition, machine translation, audio processing, etc. However, they require massive amounts of computations and take a lot of time to process. This quickly becomes a problem in mobile and handheld devices where real-time multimedia applications such as face detection, disaster management, and CCTV require lightweight, fast, and effective computing solutions. The objective of this project is to utilize specialized devices such as Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) in a heterogeneous computing environment to accelerate the deep learning computations with the constraints of power efficiency. We investigate an efficient DNN implementation and make use of FPGA for fully-connected layer and GPU for floating-point operations. This requires the deep neural network architecture to be implemented in a model parallelism system where the DNN model is broken down and processed in a distributed fashion. The proposed heterogeneous framework idea is implemented using an Nvidia TX2 GPU and a Xilinx Artix-7 FPGA. Experimental results indicate that the proposed framework can achieve faster computation and much lower power consumption.",project-academic
10.1155/2018/9390410,2018-02-11,a,Hindawi,a pruning neural network model in credit classification analysis," Nowadays, credit classification models are widely applied because they can help financial decision-makers to handle credit classification issues. Among them, artificial neural networks (ANNs) have been widely accepted as the convincing methods in the credit industry. In this paper, we propose a pruning neural network (PNN) and apply it to solve credit classification problem by adopting the well-known Australian and Japanese credit datasets. The model is inspired by synaptic nonlinearity of a dendritic tree in a biological neural model. And it is trained by an error back-propagation algorithm. The model is capable of realizing a neuronal pruning function by removing the superfluous synapses and useless dendrites and forms a tidy dendritic morphology at the end of learning. Furthermore, we utilize logic circuits (LCs) to simulate the dendritic structures successfully which makes PNN be implemented on the hardware effectively. The statistical results of our experiments have verified that PNN obtains superior performance in comparison with other classical algorithms in terms of accuracy and computational efficiency.",project-academic
10.3390/APP10010087,2019-12-20,a,Multidisciplinary Digital Publishing Institute,object detection and classification of metal polishing shaft surface defects based on convolutional neural network deep learning," Defective shafts need to be classified because some defective shafts can be reworked to avoid replacement costs. Therefore, the detection and classification of shaft surface defects has important engineering application value. However, in the factory, shaft surface defect inspection and classification are done manually, with low efficiency and reliability. In this paper, a deep learning method based on convolutional neural network feature extraction is used to realize the object detection and classification of metal shaft surface defects. Through image segmentation, the system methods setting of a Fast-R-CNN object detection framework and parameter optimization settings are implemented to realize the classification of 16,384 × 4096 large image little objects. The experiment proves that the method can be applied in practical production and can also be extended to other fields of large image micro-fine defects with a high light surface. In addition, this paper proposes a method to increase the proportion of positive samples by multiple settings of IOU values and discusses the limitations of the system for defect detection.",project-academic
10.3390/S21082598,2021-04-07,a,Multidisciplinary Digital Publishing Institute,a data fusion method for non destructive testing by means of artificial neural networks," In the aeronautics sector, aircraft parts are inspected during manufacture, assembly and service, to detect defects eventually present. Defects can be of different types, sizes and orientations, appearing in materials presenting a complex structure. Among the different inspection techniques, Non Destructive Testing (NDT) presents several advantages as they are noninvasive and cost effective. Within the NDT methods, Ultrasonic (US) waves are widely used to detect and characterize defects. However, due the so-called blind zone, they cannot be easily employed for defects close to the surface being inspected. On the other hand, another NDT technique such Eddy Current (EC) can be used only for detecting flaws close to the surface, due to the presence of the EC skin effect. The work presented in this article aims to combine the use of these two NDT methods, exploiting their complementary advantages. To reach this goal, a data fusion method is developed, by using Machine Learning techniques such as Artificial Neural Networks (ANNs). A simulated training database involving simulations of US and EC signals propagating in an Aluminum block in the presence of Side Drill Holes (SDHs) has been implemented, to train the ANNs. Measurements have been then performed on an Aluminum block, presenting tree different SDHs at specific depths. The trained ANNs were used to characterize the different real SDHs, providing an experimental validation. Eventually, particular attention has been addressed to the estimation errors corresponding to each flaw. Experimental results will show that depths and radii estimations error were confined on average within a range of 4%, recording a peak of 11% for the second SDHs.",project-academic
10.1016/J.CIE.2018.08.018,2019-02-01,a,Pergamon,ensemble based big data analytics of lithofacies for automatic development of petroleum reservoirs," Abstract None None Big data-driven ensemble learning is explored in this paper for quantitative geological lithofacies modeling, which is an integral and challenging part of petroleum reservoir development and characterization. Quantitative lithofacies modeling involves detection and recognition of underlying subsurface rock’s lithofacies. It requires real-time data acquisition, handling, storage, conditioning, analysis, and interpretation of raw sensory petroleum logging data. The real-time well-logs data collected from the sensor-based tools suffer from complications such as noise, nonlinearity, imbalance, and high-dimensionality which makes the prediction task more challenging. The existing literature on quantitative lithofacies modeling includes several data-driven techniques ranging from conventional well-logs to artificial intelligence (AI). Recently, multiple classifiers based Ensemble learners have been found to be more robust and reliable paradigms for detection and identification tasks in various machine learning applications, however, these are not well embraced in the petroleum industry. Ensemble methodology combines diverse expert’s opinions to obtain overall ensemble decision which in turn reduces the risk of a wrong decision. Thus, the uncertainties associated with complex reservoir data can be better handled by the use of Ensemble learners than the existing single learner based conventional models. Ensemble-based big data analytics, proposed in the paper, includes development and comparative performance testing of five popular ensemble methods (viz. Bagging, AdaBoost, Rotation forest, Random subspace, and DECORATE) for quantitative lithofacies modeling. Seven state-of-the-art base classifiers were used as members of different Ensemble learners for the analysis of Kansas (U.S.A.) oil-field data. The proposed techniques have been implemented on the widely used WEKA platform. The comparative performance analysis of the proposed techniques, presented in the paper, confirms its supremacy over the existing techniques used for quantitative lithofacies modeling.",project-academic
10.1016/J.PROMFG.2018.03.168,2018-01-01,a,Elsevier,a concept towards automated data driven reconfiguration of digital assistance systems," Abstract None None Constantly changing assembly tasks and reduced production cycles increase the risk of cognitive stress of operators. A large number of digital assistance systems implemented in assembly lines contribute to operator’s stress reduction. However, small and medium sized companies confront major challenges in implementing digital assistance solutions due to high investment costs and high customization effort. In addition, technological risks are mainly summarized as i) choosing the right assistance system, ii) realizing suitable interfaces within the in-house IT landscape for machine-to-machine and machine-to-human communication and iii) creating assembly instructions and configuring data systems in terms of supplying specific and adaptable information. Considering the aforementioned challenges and related technological risks, this paper presents a concept for automated data-driven reconfiguration of digital assistance systems. We discuss its impact on certain use cases defined in the TU Wien Pilot Factory Industry 4.0. Finally, we outline learning design principles for students and industrial stakeholders to implement automated reconfigurable digital assistance systems.",project-academic
10.1016/J.JMAPRO.2020.12.050,2021-02-01,a,Elsevier BV,online tool condition monitoring for ultrasonic metal welding via sensor fusion and machine learning," Abstract None None In ultrasonic metal welding (UMW), tool wear significantly affects the weld quality and tool maintenance constitutes a substantial part of production cost. Thus, tool condition monitoring (TCM) is crucial for UMW. Despite extensive literature focusing on TCM for other manufacturing processes, limited studies are available on TCM for UMW. Existing TCM methods for UMW require offline high-resolution measurement of tool surface profiles, which leads to undesirable production downtime and delayed decision-making. This paper proposes a completely online TCM system for UMW using sensor fusion and machine learning (ML) techniques. A data acquisition (DAQ) system is designed and implemented to obtain in-situ sensing signals during welding processes. A large feature pool is then extracted from the sensing signals. A subset of features are selected and subsequently used by ML-based classification models. A variety of classification models are trained, validated, and tested using experimental data. The best-performing classification models can achieve close to 100% classification accuracy for both training and test datasets. The proposed TCM system not only provides real-time TCM for UMW but also can support optimal decision-making in tool maintenance. The TCM system can be extended to predict remaining useful life (RUL) of tools and integrated with a controller to adjust welding parameters accordingly.",project-academic
10.1051/E3SCONF/202123503073,2021-01-01,a,EDP Sciences,research on collaborative innovation strategy of smart supply chain in the big data era," With the continuous emergence of new technologies such as Artificial intelligence, big data, Cloud computing and IOT, technology accelerates integration and innovation, and data dividends have continued to emerge. At the same time, China’s “Internet Action Plan”, “Made in China 2025”, “Digital China” and other national strategies have been implemented in depth, China’s social and economic development has entered the era of big data. As the basic industry of the national economy. The logistics industry will also accelerate changes, and it has become a development trend for companies to use new technologies to realize smart supply chain collaborative innovation. The paper analyzes the development opportunities of smart supply chain in the Big Data Era, summarizes the problems encountered in the application of big data in the smart supply chain at this stage, and finally puts forward the collaborative innovation strategy for smart supply chain in the Big Data Era, Providing reference for collaborative innovation development of enterprise supply chain.",project-academic
10.1007/S11277-020-07408-W,2020-04-23,a,Springer US,a novel artificial intelligence based timing synchronization scheme for smart grid applications," The smart grid control applications necessitate real-time communication systems with time efficiency for real-time monitoring, measurement, and control. Time-efficient communication systems should have the ability to function in severe propagation conditions in smart grid applications. The data/packet communications need to be maintained by synchronized timing and reliability through equally considering the signal deterioration occurrences, which are propagation delay, phase errors and channel conditions. Phase synchronization plays a vital part in the digital smart grid to get precise and real-time control measurement information. IEEE C37.118 and IEC 61850 had implemented for the synchronization communication to measure as well as control the smart grid applications. Both IEEE C37.118 and IEC 61850 experienced a huge propagation and packet delays due to synchronization precision issues. Because of these delays and errors, measurement and monitoring of the smart grid application in real-time is not accurate. Therefore, it has been investigated that the time synchronization in real-time is a critical challenge in smart grid applications, and for this issue, other errors raised consequently. The existing communication systems are designed with the phasor measurement unit (PMU) along with communication protocol IEEE C37.118 and uses the GPS timestamps as the reference clock stamps. The absence of GPS increases the clock offsets, which surely can hamper the synchronization process and the full control measurement system that can be imprecise. Therefore, to reduce this clock offsets, a new algorithm is needed which may consider any alternative reference timestamps rather than GPS. The revolutionary Artificial Intelligence (AI) enables the industrial revolution to provide a significant performance to engineering solutions. Therefore, this article proposed the AI-based Synchronization scheme to mitigate smart grid timing issues. The backpropagation neural network is applied as the AI method that employs the timing estimations and error corrections for the precise performances. The novel AIFS scheme is considered the radio communication functionalities in order to connect the external timing server. The performance of the proposed AIFS scheme is evaluated using a MATLAB-based simulation approach. Simulation results show that the proposed scheme performs better than the existing system.",project-academic
10.1109/ICMA.2013.6618173,2013-10-03,p,IEEE,an intelligent object manipulation framework for industrial tasks," This paper presents an intelligent object manipulation framework for industrial tasks, which integrates a sensor-rich multi-fingered robot hand, an industrial robot manipulator, a conveyor belt and employs machine learning algorithms. The framework software architecture is implemented using a Windows 7 operating system with RTX real-time extension for synchronous handling of peripheral devices. The framework uses Scale Invariant Feature Transform (SIFT) image processing algorithm, Support Vector Machine (SVM) machine learning algorithm and 3D point cloud techniques for intelligent object recognition based on RGB camera and laser rangefinder information from the robot hand end effector. The objective is automated manipulation of objects with different shapes and poses with minimum programming effort applied by a user.",project-academic
10.3390/S20174965,2020-09-02,a,Multidisciplinary Digital Publishing Institute,a novel end to end fault diagnosis approach for rolling bearings by integrating wavelet packet transform into convolutional neural network structures," Accidental failures of rotating machinery components such as rolling bearings may trigger the sudden breakdown of the whole manufacturing system, thus, fault diagnosis is vital in industry to avoid these massive economical costs and casualties. Since convolutional neural networks (CNN) are poor in extracting reliable features from original signal data, the time-frequency analysis method is usually called for to transform 1D signal into a 2D time-frequency coefficient matrix in which richer information could be exposed more easily. However, realistic fault diagnosis applications face a dilemma in that signal time-frequency analysis and fault classification cannot be implemented together, which means manual signal conversion work is also needed, which reduces the integrity and robustness of the fault diagnosis method. In this paper, a novel network named WPT-CNN is proposed for end-to-end intelligent fault diagnosis of rolling bearings. WPT-CNN creatively uses the standard deep neural network structure to realize the wavelet packet transform (WPT) time-frequency analysis function, which seamlessly integrates fault diagnosis domain knowledge into deep learning algorithms. The overall network architecture can be trained with gradient descent backpropagation algorithms, indicating that the time-frequency analysis module of WPT-CNN is also able to learn the dataset characteristics, adaptively representing signal information in the most suitable way. Two experimental rolling bearing fault datasets were used to validate the proposed method. Testing results showed that WPT-CNN obtained the testing accuracies of 99.73% and 99.89%, respectively, in two datasets, which exhibited a better and more reliable diagnosis performance than any other existing deep learning and machine learning methods.",project-academic
10.1109/ACCESS.2020.2991225,2020-04-29,a,Institute of Electrical and Electronics Engineers Inc.,design and performance evaluation of an ai based w band suspicious object detection system for moving persons in the iot paradigm," The threat of terrorism has spread all over the world, and the situation has become grave. Suspicious object detection in the Internet of Things (IoT) is an effective way to respond to global terrorist attacks. The traditional solution requires performing security checks one by one at the entrance of each gate, resulting in bottlenecks and crowding. In the IoT paradigm, it is necessary to be able to perform suspicious object detection on moving people. Artificial intelligence (AI) and millimeter-wave imaging are advanced technologies in the global security field. However, suspicious object detection for moving persons in the IoT, which requires the integration of many different imaging technologies, is still a challenge in both academia and industry. Furthermore, increasing the recognition rate of suspicious objects and controlling network congestion are two main issues for such a suspicious object detection system. In this paper, an AI-based W-band suspicious object detection system for moving persons in the IoT paradigm is designed and implemented. In this system, we establish a suspicious object database to support AI technology for improving the probability of identifying suspicious objects. Moreover, we propose an efficient transmission mechanism to reduce system network congestion since a massive amount of data will be generated by 4K cameras during real-time monitoring. The evaluation results indicate that the advantages and efficiency of the proposed scheme are significant.",project-academic
10.1108/17410380610688232,2006-10-01,a,Emerald Group,multiple response optimization using taguchi methodology and neuro fuzzy based model," Purpose – To provide a good insight into solving a multi-response optimization problem using neuro-fuzzy model and Taguchi method of experimental design.

Design/methodology/approach – Over the last few years in many manufacturing organizations, multiple response optimization problems were resolved using the past experience and engineering judgment, which leads to increase in uncertainty during the decision-making process. In this paper, a four-step procedure is proposed to resolve the parameter design problem involving multiple responses. This approach employs the advantage of both artificial intelligence tool (neuro-fuzzy model) and Taguchi method of experimental design to tackle problems involving multiple responses optimization.

Findings – The proposed methodology is validated by revisiting a case study to optimize the three responses for a double-sided surface mount technology of an electronic assembly. Multiple signal-to-noise ratios are mapped into a single performance statistic through neuro-fuzzy based model, to identify the optimal level settings for each parameter. Analysis of variance is finally performed to identify parameters significant to the process.

Research limitations/implications – The proposed model will be validated in future by conducting a real life case study, where multiple responses need to be optimized simultaneously.

Practical implications – It is believed that the proposed procedure in this study can resolve a complex parameter design problem with multiple responses. It can be applied to those areas where there are large data sets and a number of responses are to be optimized simultaneously. In addition, the proposed procedure is relatively simple and can be implemented easily by using ready-made neural and statistical software like Neuro Work II professional and Minitab.

Originality/value – This study adds to the literature of multi-optimization problem, where a combination of the neuro-fuzzy model and Taguchi method is utilized hand-in-hand.",project-academic
10.1007/S11063-018-9888-3,2019-10-01,a,Springer US,a soft sensing scheme of gas utilization ratio prediction for blast furnace via improved extreme learning machine," Gas utilization ratio (GUR) is an important indicator reflecting the operating state and energy consumption of blast furnace (BF). Skilled operators usually refer to changing trends of GUR to guide the next step of production. For these reasons, this paper establishes a soft sensing scheme based on an improved extreme learning machine (ELM) to predict GUR. In order to enhance the modeling capability of ELM for industrial data, an improved ELM, named GR-ELM, is proposed based on grey relational analysis (GRA) and residual modification mechanism. In GR-ELM, considering the different effective information contained in each input attribute for modeling, the input attribute optimization is proposed combining with GRA and entropy weight method. Then, because the modeling capability of ELM is limited and the data collected from industrial process are usually contaminated, the residual modification mechanism is implemented to improve the reliability of the model. In addition, considering the influence of time delay in BF ironmaking process, generalized correlation coefficient method based on mutual information is used for time delay analysis to eliminate the influence. The real data collected from a BF are applied and validated the performance and effectiveness of the proposed soft sensing scheme. The experimental results show that the proposed soft sensing scheme is available and can achieve better performance than some state-of-the-art algorithms. The soft sensing scheme can provide effective decision support and guidance for further optimization operation.",project-academic
10.30534/IJATCSE/2020/33942020,2020-08-25,a,The World Academy of Research in Science and Engineering,real time multi scale facial mask detection and classification using deep transfer learning techniques," In the era of deep learning, object detection plays an influential role for many industries None Detecting minute things are very much essential without human intervention especially at large scale industries None In this paper we have proposed multiple approaches for Multi-scale facial mask real time detection and classification for the hospital industry, crowd surveillance in the streets and malls are more useful in this COVID-19 Pandemic Situation None In our approach we have implemented two different detection models which are FMY3 using Yolov3 Algorithm and FMNMobile using NASNetMobile and Resnet_SSD300 Algorithms and used two different face mask dataset with 680 and 1400 images respectively None We have analyzed both the models by computing various probabilistic accuracy measures and achieved the 34% Mean Average Precision (mAP) and 91 7% Recall rate on FMY3 Model and achieved the 98% and 99% of accuracy and recall rate on FMNMobile Model None Finally we have shown results of various face mask detections from both the models None © 2020, World Academy of Research in Science and Engineering None All rights reserved",project-academic
,2004-01-01,a,,visualisation using game engines," Geographic Information Systems (GIS) and Computer Aided Facility Management-Systems (CAFM) are currently undergoing the transition to storing and processing real 3D geospatial data. Applications for this type of data are, among others, location based services, navigation systems and the planning of large-scale construction projects. For presentation purposes and especially when working in the field, powerful visualisation systems are needed that are also capable of running on mobile devices like notebooks, personal digital assistants (PDA) or even cell phones. In such application areas, the free movement of the viewer’s position and the interaction with the data are of great importance. Real-time visualisation of 3D geospatial data is already well established and also commercially successful in the entertainment industry, namely in the market of 3D video games. The development of software in this field is very cost-intensive, so that the packages are often used for several game products and are therefore universally applicable to a certain extend. These so-called game engines include not only visualisation functionality, but also offer physics, sound, network, artificial intelligence and graphical user interfaces to handle user inand output. As certain portions or sometimes even the whole engine are released as open source software, these engines can be extended to build more serious applications at very little costs. The paper shows how these game engines can be used to create interactive 3D applications that present texture-mapped geospatial data. The integration of 3D data into such systems is discussed. Functionality like thematic queries can be implemented by extending the internal data structures and by modification of the game’s accompanying dynamic link libraries.",project-academic
10.1007/S11740-018-0855-7,2019-02-01,a,Springer Berlin Heidelberg,reinforcement learning for opportunistic maintenance optimization," Intelligent systems, that support the maintenance of production resources, offer real-time data-based approaches to optimize the maintenance effort and to reduce the usage of resources within production systems. However, unused potentials remain regarding maintenance schedules with minimal opportunity costs of the measures taken. This work provides a novel, machine-learning-based approach for the exploitation of these remaining optimization opportunities as an exemplary extension of the current state of the art. The determination of an optimal maintenance schedule for parallel working machines, is based on the data of a production system. The main result of this work is the performance of the implemented reinforcement learning algorithms, both in terms of downtime reduction, which increases the production output, and in terms of reducing maintenance costs compared to existing maintenance strategies. Hence, this work provides a holistic approach to the optimization of maintenance strategies and gives further evidence of a meaningful applicability of reinforcement learning algorithms in manufacturing processes.",project-academic
10.1016/J.PROMFG.2018.12.026,2019-01-01,a,Elsevier,hybrid artificial intelligence system for the design of highly automated production systems," Abstract None None The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype.",project-academic
10.1016/J.JMSY.2020.06.018,2020-07-01,a,Elsevier,reinforcement learning for facilitating human robot interaction in manufacturing," For many contemporary manufacturing processes, autonomous robotic operators have become ubiquitous. Despite this, the number of human operators within these processes remains high, and as a consequence, the number of interactions between humans and robots has increased in this context. This is a problem, as human beings introduce a source of disturbance and unpredictability into these processes in the form of performance variation. Despite the natural human aptitude for flexibility, their presence remains a source of disturbance within the system and make modelling and optimization of these systems considerably more challenging, and in many cases impossible. Improving the ability of robotic operators to adapt their behaviour to variations in human task performance is, therefore, a significant challenge to be overcome to enable many ideas in the larger intelligent manufacturing paradigm to be realised. This work presents the development of a methodology to effectively model these systems and a reinforcement learning agent capable of autonomous decision-making. This decision-making provides the robotic operators with greater adaptability, by enabling its behaviour to change based on observed information, both of its environment and human colleagues. The work extends theoretical knowledge on how learning methods can be implemented for robotic control, and how the capabilities that they enable may be leveraged to improve the interaction between robots and their human counterparts. The work further presents a novel methodology for the implementation of a reinforcement learning-based intelligent agent which enables a change in behavioural policy in robotic operators in response to performance variation in their human colleagues. The development and evaluation are supported by a generalized simulation model, which is parameterized to enable appropriate variation in human performance. The evaluation demonstrates that the reinforcement agent can effectively learn to make adjustments to its behaviour based on the knowledge extracted from observed information, and balance the task demands to optimise these adjustments.",project-academic
10.1007/S11042-017-5349-7,2018-04-01,a,Springer US,adaptable deep learning structures for object labeling tracking under dynamic visual environments," In this paper, we propose a self-adaptive deep neural network architecture suitable for object tracking and labelling. In particular, an adaptation mechanism is introduced that automatically evaluates the performance of the network and then updates its weights to fit the current environmental conditions. The retraining algorithm trusts as much as possible the current conditions (discriminative constraints), while simultaneously providing a minimal degradation of the already obtained knowledge of the network (generative constraints). The underline assumption is that a small weight perturbation is adequate to modify the performance of the network to the new situations, without this constraint implying a small modification of the network output due to the highly non-linear surface that the network models. Under this assumption, we propose a computationally efficient re-training algorithm to tackle the variations of the visual environment, requiring a small number of labelled samples for the adaptation. Weight updating is combined with an unsupervised learning paradigm, implemented through stacked autoencoders, in order to improve convergence, stability and performance of the object tracking and labeling process by propagating the sensory inputs into deep level of hierarchies and therefore structuring the inputs from low representations to more abstract forms. Approximates of current visual environment are provided through a dynamic tracker that combines motion and learning features to automatically create few confident labeled data. The proposed retraining scheme is computationally efficient and able to model non-stationary environments, like the ones appeared in real-life computer vision application scenarios. Experimental results and comparisons are provided on video datasets of very complicated visual content, monitoring industrial workflows of car assembly within a manufactory. The results indicates that our self-adaptive deep neural network architecture is able to correctly label and separate foreground objects from background even under severe visual changes, such as occlusion, illumination variations and change of camera views, and within real time computational constraints.",project-academic
10.1109/ISIE.2015.7281681,2015-06-03,p,IEEE,rfid indoor localization based on support vector regression and k means," Systems need to know the physical locations of objects and people to optimize user experience and solve logistical and security issues. Also, there is a growing demand for applications that need to locate individual assets for industrial automation. This work proposes an indoor positioning system (IPS) able to estimate the item-level location of stationary objects using off-the-shelf equipment. By using RFID technology, a machine learning model based on support vector regression (SVR) is proposed. A multi-frequency technique is developed in order to overcome off-the-shelf equipment constraints. A k-means approach is also applied to improve accuracy. We have implemented our system and evaluated it using real experiments. The localization error is between 17 and 31 cm in 2.25m2 area coverage.",project-academic
10.1109/TIM.2019.2956613,2020-07-01,a,IEEE,intelligent fault diagnosis via semisupervised generative adversarial nets and wavelet transform," Effective fault diagnosis of rotating machinery plays a pretty important role in the enhanced reliability and improved safety of industrial informatics applications. Although traditional intelligent fault diagnosis techniques, such as support vector machine, extreme learning machine, and convolutional neural network, might achieve satisfactory accuracy, a very high price is caused by marking all samples manually. In this article, a novel fault diagnosis method of the rotating machinery is proposed by integrating semisupervised generative adversarial nets with wavelet transform (WT-SSGANs). The proposed WT-SSGANs’ method involves two parts. In the first part, WT is adopted to transform 1-D raw vibration signals into 2-D time–frequency images. In the second part, the 2-D time–frequency images are inputted into the built SSGANs’ model to realize fault diagnosis with little labeled samples. The advantage of the built model is that the unlabeled samples might be made full use of through an adversarial learning mechanism. Finally, two case studies are implemented to verify the proposed method. The results indicate that it can achieve higher accuracy and use less labeled samples than the other existing methods in the literature. In addition, its performance in stability is pretty good as well. Competitive and promising results are still achieved when working conditions are changed.",project-academic
10.1142/S0960313196000093,1996-06-01,a,World Scientific Publishing Company,integrated neural network modeling for electronic manufacturing," This paper addresses issues involved in the modeling of electronic manufacturing processes for optimization and control using artificial neural networks (ANNs). A modeling methodology is presented which integrates a number of techniques to counter the commonly experienced problems of selecting the ‘right’ network structure, over-training and long training times in building economical and accurate ANN models. This methodology has been implemented as an automated user-friendly ANN modeling software — CU-ANN. The main features of our methodology are data pre-processing, ‘simple to complex’ network structure approach and simultaneous training and testing. The neural networks considered have feed forward architecture and use error back-propagation algorithm for training. We have successfully applied this ANN modeling methodology to a number of simulated and real-life electronic manufacturing problems. These include stencil printing and simulated wafer fab. process data. The results indicate that our approach produces accurate, economical models and can handle a wide variety of data sets.",project-academic
10.1109/ISCAS.2009.5118362,2009-05-24,p,IEEE,new cnn based algorithms for the full penetration hole extraction in laser welding processes," In this paper new CNN based visual algorithms for the control of welding processes are proposed. The high dynamics of laser welding in several manufacturing processes ranging from automobile production to precision mechanics requires the introduction of new fast real time controls. In the last few years, analogic circuits like Cellular Neural Networks (CNN) have obtained a primary place in the development of efficient electronic devices because of their real-time signal processing properties. Furthermore, several pixel parallel CNN based architectures are now included within devices like the family of EyeRis systems [1]. In particular, the algorithms proposed in the following have been implemented on the EyeRis system v1.2 with the aim to be run at frame rates up to 20 kHz.",project-academic
10.1007/978-3-319-40663-3_39,2016-07-06,p,"Springer, Cham",learning time optimal anti swing trajectories for overhead crane systems," Considering both state and control constraints, minimum-time trajectory planning (MTTP) can be implemented in an ‘offline’ way for overhead crane systems [1]. In this paper, we aim to establish a real-time trajectory planning model by using machine learning approaches to approximate those results obtained by MTTP. The fusion of machine learning regression approaches into the trajectory planning module is new and the application is promising for intelligent mechatronic systems. In particular, we first reformulate the considered trajectory planning problem in a three-segment form, where the acceleration and deceleration segments are symmetric. Then, the offline MTTP is applied to generate a database of minimum-time trajectories for the acceleration stage, based on which several regression approaches including Extreme Learning Machine (ELM) and Backpropagation Neural Network (BP) are adopt to approximate MTTP results with high accuracy. More important, the resulting model only contains a set of parameters, rather than a large volume of offline data, and thus machine learning based approaches could be implemented in low-cost digital signal processing chips required by industrial applications. Comparative evaluation results are provided to show the superior performance of the selected regression approach.",project-academic
10.1016/J.KNOSYS.2020.105895,2020-06-21,a,Elsevier,single and simultaneous fault diagnosis of gearbox via a semi supervised and high accuracy adversarial learning framework," Abstract None None Gearboxes are the most widely used elements for transferring speed and power in many industrial machines. High-accuracy gearbox fault diagnosis is quite significant for keeping the machine working reliably and safely. Owing to various unseen faults, it is pretty challenging to realize high-accuracy intelligent fault diagnosis of gearboxes using existing methods. In addition, existing intelligent fault diagnosis methods heavily rely on a huge number of labeled samples, and the features extraction and selection are mainly done manually. In this paper, a semi-supervised and high-accuracy adversarial learning framework for the single and simultaneous fault diagnosis of the gearbox based on Generative Adversarial Nets and time-frequency imaging is proposed. The proposed method involves two parts. In the first part, continuous wavelet transform is adopted to transform one-dimensional raw vibration signals into two-dimensional time-frequency images. In the second part, the labeled and unlabeled time-frequency images are inputted into the built adversarial learning model to realize single and simultaneous fault diagnosis of the gearbox. Finally, two case studies are implemented to verify the proposed method. The results indicate that it is higher in accuracy and fewer in training steps of achieving the highest accuracy rate than other existing intelligent fault diagnosis methods in literatures. Moreover, its performance in stability is pretty good as well.",project-academic
10.1016/J.KNOSYS.2020.106679,2021-02-15,a,Elsevier,federated learning for machinery fault diagnosis with dynamic validation and self supervision," Abstract None None Intelligent data-driven machinery fault diagnosis methods have been successfully and popularly developed in the past years. While promising diagnostic performance has been achieved, the existing methods generally require large amounts of high-quality supervised data for training, which are mostly difficult and expensive to collect in real industries. Therefore, it is motivated that the distributed data of multiple clients can be integrated and exploited to build a powerful data-driven model. However, that basically requires data sharing among different users, and is not preferred in most industrial cases due to potential conflict of interests. In order to address the data island problem, a federated learning method for machinery fault diagnosis is proposed in this paper. Model training is locally implemented within each participated client, and a self-supervised learning scheme is proposed to enhance the learning performance. The server aggregates the locally updated models in each training round under the dynamic validation scheme, and a global fault diagnosis model can be established. Only the models are mutually communicated rather than the data, which ensures data privacy among different clients. The experiments on two datasets suggest the proposed method offers a promising approach on confidential decentralized learning.",project-academic
10.1109/TIE.2019.2962437,2020-01-01,a,IEEE,an efficient convolutional neural network model based on object level attention mechanism for casting defect detection on radiography images," Automatic detection of casting defects on radiography images is an important technology to automatize digital radiography defect inspection. Traditionally, in an industrial application, conventional methods are inefficient when the detection targets are small, local, and subtle in the complex scenario. Meanwhile, the outperformance of deep learning models, such as the convolutional neural network (CNN), is limited by a huge volume of data with precise annotations. To overcome these challenges, an efficient CNN model, only trained with image-level labels, is first proposed for detection of tiny casting defects in a complicated industrial scene. Then, in this article, we present a novel training strategy which can form a new object-level attention mechanism for the model during the training phase, and bilinear pooling is utilized to improve the model capability of detecting local contrast casting defects. Moreover, to enhance the interpretability, we extend class activation maps (CAM) to bilinear CAM (Bi-CAM) which is adapted to bilinear architectures as a visualization technique to reason about the model output. Experimental results show that the proposed model achieves superior performance in terms of each quantitative metric and is suitable for most actual applications. The real-time defect detection of castings is efficiently implemented in the complex scenario.",project-academic
10.1109/ACCESS.2019.2959771,2019-12-16,a,Institute of Electrical and Electronics Engineers (IEEE),smart contract based data commodity transactions for industrial internet of things," The evolution of Industrial Internet of things (IIoT) boosts the amount of IIoT data. Machine learning promotes the progress of data analytics services. In order to facilitate the flow and explore the economic value of IIoT data, it is crucial to consider data packet transactions (DPTs) and data analytics service transactions (DASTs) simultaneously. Centralized data trading platforms emerge to realize transactions of data commodities. However, centralized platforms lack trust and robustness. How to realize DPTs and DASTs in a decentralized way is a challenging issue. In this paper, a new transaction solution based on the smart contract-enabled blockchain technology is proposed, which consists of the DPT smart contract and DAST smart contract. The DPT smart contract is implemented to trade data packets. The DAST smart contract provides a competitive way to trade data analytics services. Both smart contracts are designed to enable entities in IIoT to execute DPTs and DASTs automatically and honestly. Moreover, the transaction disputes between different IIoT entities are solved by the big data center off-chain, and the treatment results will be recorded on the blockchain by the big data center. The DPT smart contract and DAST smart contract are implemented and tested on Remix integrated development environment to achieve DPTs and DASTs. The gas costs of smart contracts are estimated and the security of the proposed solution is analyzed. The performance analysis demonstrates that the proposed solution is secure and feasible.",project-academic
10.1007/978-3-540-74829-8_18,2007-09-12,p,"Springer, Berlin, Heidelberg",a neuro fuzzy approach for sensor network data cleaning," Sensor networks have become an important source of data with numerous applications in monitoring various real-life phenomena as well as industrial applications and traffic control. However, sensor data are subject to several sources of errors as the data captured from the physical world through these sensor devices tend to be incomplete, noisy, and unreliable, thus yielding imprecise or even incorrect and misleading answers which can be very significative if they result in immediate critical decisions or activation of actuators. Traditional data cleaning techniques cannot be applied in this context as they do not take into account the strong spatial and temporal correlations typically present in sensor data, so machine learning techniques could greatly be of aid. In this paper, we propose a neuro-fuzzy regression approach to clean sensor network data: the well known ANFIS model is employed for reducing the uncertainty associated with the data thus obtaining a more accurate estimate of sensor readings. The obtained cleaning results show good ANFIS performance compared to other common used model such as kernel methods, and we demonstrate its effectiveness if the cleaning model has to be implemented at sensor level rather than at base-station level.",project-academic
10.1088/0957-0233/23/1/015401,2012-01-01,a,IOP Publishing,a novel spatter detection algorithm based on typical cellular neural network operations for laser beam welding processes," Real-time monitoring of laser beam welding (LBW) has increasingly gained importance in several manufacturing processes ranging from automobile production to precision mechanics. In the latter, a novel algorithm for the real-time detection of spatters was implemented in a camera based on cellular neural networks. The latter can be connected to the optics of commercially available laser machines leading to real-time monitoring of LBW processes at rates up to 15 kHz. Such high monitoring rates allow the integration of other image evaluation tasks such as the detection of the full penetration hole for real-time control of process parameters.",project-academic
10.1109/ACCESS.2020.2976513,2020-02-27,a,IEEE,resource constrained machine learning for adas a systematic review," The advent of machine learning (ML) methods for the industry has opened new possibilities in the automotive domain, especially for Advanced Driver Assistance Systems (ADAS). These methods mainly focus on specific problems ranging from traffic sign and light recognition to pedestrian detection. In most cases, the computational resources and power budget found in ADAS systems are constrained while most machine learning methods are computationally intensive. The usual solution consists in adapting the ML models to comply with the memory and real-time (RT) requirements for inference. Some models are easily adapted to resource-constrained hardware, such as Support Vector Machines, while others, like Neural Networks, need more complex processes to fit into the desired hardware. The ADAS hardware (HW platforms) are diverse, from complex MPSoC CPUs down to classical MCUs, DPSs and application-specific FPGAs and ASICs or specific GPU platforms (such as the NVIDIA families Tegra or Jetson). Therefore, there is a tradeoff between the complexity of the ML model implemented and the selected platform that impacts the performance metrics: function results, energy consumption and speed (latency and throughput). In this paper, a survey in the form of systematic review is conducted to analyze the scope of the published research works that embed ML models into resource-constrained implementations for ADAS applications and what are the achievements regarding the ML performance, energy and speed trade-off.",project-academic
,2017-07-04,a,,opeb open physical environment benchmark for artificial intelligence," Artificial Intelligence methods to solve continuous- control tasks have made significant progress in recent years. However, these algorithms have important limitations and still need significant improvement to be used in industry and real- world applications. This means that this area is still in an active research phase. To involve a large number of research groups, standard benchmarks are needed to evaluate and compare proposed algorithms. In this paper, we propose a physical environment benchmark framework to facilitate collaborative research in this area by enabling different research groups to integrate their designed benchmarks in a unified cloud-based repository and also share their actual implemented benchmarks via the cloud. We demonstrate the proposed framework using an actual implementation of the classical mountain-car example and present the results obtained using a Reinforcement Learning algorithm.",project-academic
10.1016/J.ENBUILD.2018.12.034,2019-02-15,a,Elsevier,intellimav a cloud computing measurement and verification 2 0 application for automated near real time energy savings quantification and performance deviation detection," Abstract None None Energy conservation measures (ECMs) are implemented in all sectors with the objective of improving the efficiency with which energy is consumed. Measurement and verification (M&V) is required to verify the performance of every ECM to ensure its successful implementation and operation. The methodologies implemented to achieve this are currently evolving to a more dynamic state, known as measurement and verification 2.0, through the use of automated and advanced analytics. The primary barrier to the adoption of M&V 2.0 practices are the tools available to practitioners. This paper aims to populate the knowledge gap in the industrial buildings sector by presenting a novel cloud computing-based application, IntelliMaV, that applies advanced machine learning techniques on large datasets to automatically verify the performance of ECMs in near real-time. Additionally, a performance deviation detection system is incorporated, ensuring persistence of savings beyond the typical period of analysis in M&V. None IntelliMaV allows M&V practitioners to quantify energy savings with minimum levels of uncertainty by applying powerful analytics to data readily available in industrial facilities. The use of a cloud computing-based architecture reduces the resources required on-site and decreases the time required to train the baseline energy model through the use of parallel processing. The robust nature of the application ensures it is applicable across the broad spectrum of ECMs in the industrial buildings sector. A case study carried out in a large biomedical manufacturing facility demonstrates the ease of use of the application and the benefits realised through its adoption. The energy savings from an ECM were calculated to be 2,353,225 kWh/yr with 25.5% uncertainty at a 90% confidence interval.",project-academic
10.1109/TII.2018.2807797,2018-02-19,a,IEEE,deep endoscope intelligent duct inspection for the avionic industry," We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly None None $98$ None None % given different tests with different types of residuals and duct structures.",project-academic
10.1109/EMBC.2016.7591089,2016-08-01,p,Conf Proc IEEE Eng Med Biol Soc,development of a real time activity monitoring android application utilizing smartstep," Footwear based activity monitoring systems are becoming popular in academic research as well as consumer industry segments. In our previous work, we had presented developmental aspects of an insole based activity and gait monitoring system-SmartStep, which is a socially acceptable, fully wireless and versatile insole. The present work describes the development of an Android application that captures the SmartStep data wirelessly over Bluetooth Low energy (BLE), computes features on the received data, runs activity classification algorithms and provides real time feedback. The development of activity classification methods was based on the the data from a human study involving 4 participants. Participants were asked to perform activities of sitting, standing, walking, and cycling while they wore SmartStep insole system. Multinomial Logistic Discrimination (MLD) was utilized in the development of machine learning model for activity prediction. The resulting classification model was implemented in an Android Smartphone. The Android application was benchmarked for power consumption and CPU loading. Leave one out cross validation resulted in average accuracy of 96.9% during model training phase. The Android application for real time activity classification was tested on a human subject wearing SmartStep resulting in testing accuracy of 95.4%.",project-academic
10.1109/JSEN.2008.926923,2008-07-16,a,IEEE,data processing method applying principal component analysis and spectral angle mapper for imaging spectroscopic sensors," A data processing method to classify hyperspectral images from an imaging spectroscopic sensor is evaluated. Each image contains the whole diffuse reflectance spectra of the analyzed material for all the spatial positions along a specific line of vision. The implemented linear algorithm comes to solve real time constrains typical of industrial systems. This processing method is composed of two blocks: data compression is performed by means of principal component analysis (PCA) and the spectral interpretation algorithm for classification is the spectral angle mapper (SAM). This strategy, applying PCA and SAM, has been successfully tested for online raw material sorting in the tobacco industry, where the desired raw material (tobacco leaves) should be discriminated from other unwanted spurious materials, such as plastic, cardboard, leather, feathers, candy paper, etc. Hyperspectral images are recorded by a sensor consisting of a monochromatic camera and a passive prism-grating-prism device. Performance results are compared with a spectral interpretation algorithm based on artificial neural networks (ANN).",project-academic
,2016-02-25,,,computer implemented platform for the creation of a virtual product," A computer implemented platform (2) for the creation and display of a variable virtual product (42) that is then intended to be realized as a physical product by an associated manufacturing and/or assembly process, characterized in that it comprises: None at least one database (6) wherein are stored a plurality of variations (33) of individual components (8) which, once joined together, define different variants of said virtual product (42), said database comprising data (9) relating to the mechanical coupling between all the variations (33) of said individual components (8), None a hardware infrastructure (1) wherein are loaded and run: - a software module of artificial intelligence (10) that, using a series of acquired and/or processed information (12) about the user and considering said data (9) relating to the mechanical coupling between all the variations of said individual components, is configured to select, suggest and present to the user, within a graphical user interface of a electronic visual display (82), only certain optimal variations (33) among said plurality of variations stored in said database (6), - a software module (60) for creating said customized variable virtual product (42), said module is configured so that, starting from an initial version of said virtual product (42), the user can interactively and cyclically choose, by a means of an input pointing device (84) connected with said electronic visual display (82), at least one of said variations (33) that has been selected and suggested by said software module of artificial intelligence (10) and that is displayed within said graphical user interface, until reaching a final version of said virtual product (42) that is ideal for the user and that corresponds to the one intended to be physically realized by said associated manufacturing and/or assembly process, None an interface (80) through which the user interacts, monitors and controls said software modules (10, 60), said interface comprising said electronic visual display (82) and said input pointing device for the control and management (84) of said modules.",project-academic
10.1007/S11761-017-0221-1,2018-06-01,p,Springer London,adaptive security architecture for protecting restful web services in enterprise computing environment," In this modern era of enterprise computing, the enterprise application integration (EAI) is a well-known industry-recognized architectural principle that is built based on loosely coupled application architecture, where service-oriented architecture (SOA) is the architectural pattern for the implementation of EAI, whose computational elements are called as “services.” Though SOA can be implemented in a wide range of technologies, the web services implementation of SOA becomes the current selective choice due to its simplicity that works on basic Internet protocols. Web service technology defines several supporting protocols and specifications such as SOAP and WSDL for communication with client and server for data interchange. A new architectural paradigm has emerged in SOA in recent years called REpresentational State Transfer (REST) that is also used to integrate loosely coupled service components, named RESTful web services, by system integration consortiums. This SOA implementation does not possess adequate security solutions within it, and its security is completely dependent on network/transport layer security that is obsolete owing to latest web technologies such as Web 2.0 and its upgraded version, Web 3.0. Vendor security products have major implementation constraints such as they need secured organizational environment and breach to SOA specifications, hence introducing new vulnerabilities. Herein, we examine the security vulnerabilities of RESTful web services in the view of popular OWASP rating methodologies and analyze the gaps in the existing security solutions. We hence propose an adaptive security solution for REST that uses public key infrastructure techniques to enhance the security architecture. The proposed security architecture is constructed as an adaptive way-forward Internet-of-Things (IoT) friendly security solution that is comprised of three cyclic parts: learn, predict and prevent. A novel security component named “intelligent security engine” is introduced which learns the possible occurrences of security threats on SOA using artificial neural networks learning algorithms, then it predicts the potential attacks on SOA based on obtained results by the developed theoretical security model, and the written algorithms as part of security solution prevent the SOA attacks. This paper is written to present one of such algorithms to prevent SOA attacks on RESTful web services along the discussion on the obtained results of the conducted proof-of-concept on the real-time SOA environment. A comparison of the proposed system with other competing solutions demonstrates its superiority.",project-academic
10.1002/ADVS.202100230,2021-07-01,a,"John Wiley & Sons, Ltd",artificial intelligence of things aiot enabled virtual shop applications using self powered sensor enhanced soft robotic manipulator," Rapid advancements of artificial intelligence of things (AIoT) technology pave the way for developing a digital-twin-based remote interactive system for advanced robotic-enabled industrial automation and virtual shopping. The embedded multifunctional perception system is urged for better interaction and user experience. To realize such a system, a smart soft robotic manipulator is presented that consists of a triboelectric nanogenerator tactile (T-TENG) and length (L-TENG) sensor, as well as a poly(vinylidene fluoride) (PVDF) pyroelectric temperature sensor. With the aid of machine learning (ML) for data processing, the fusion of the T-TENG and L-TENG sensors can realize the automatic recognition of the grasped objects with the accuracy of 97.143% for 28 different shapes of objects, while the temperature distribution can also be obtained through the pyroelectric sensor. By leveraging the IoT and artificial intelligence (AI) analytics, a digital-twin-based virtual shop is successfully implemented to provide the users with real-time feedback about the details of the product. In general, by offering a more immersive experience in human-machine interactions, the proposed remote interactive system shows the great potential of being the advanced human-machine interface for the applications of the unmanned working space.",project-academic
10.1109/CLEOE-EQEC.2019.8872523,2019-06-23,p,Optical Society of America,wavelength independent image classification through a multimode fiber using deep neural networks," Deep Neural Networks (DNNs) have been increasingly implemented in different research fields or industrial applications. Large amounts of data are processed daily in order to extract useful information using machine learning techniques. Many research groups have shown impressive results on improving resolution in microscopy and quantitative phase retrieval by training DNNs on real datasets [1,2]. Recently, recovery and reconstruction of images after they have propagated through multimode optical fibers (MMFs) have also been achieved using DNNs [3,4]. When images propagate through MMFs they suffer severe scrambling because the information gets distributed among the different spatial modes that the fiber supports. Furthermore, since the fiber modes propagate with different velocities, the local information of the input decorrelates after a few millimeters along the MMF, thus resulting in the formation of a speckle pattern at the output. Recovery of information from such speckle patterns is of practical interest for integrating the MMFs for endoscopic applications in medicine or for signal recovery in telecommunications.",project-academic
10.1016/J.MEASUREMENT.2018.05.099,2018-11-01,a,Elsevier,parallel three dimensional electrical capacitance data imaging using a nonlinear inversion algorithm and lp norm based model regularization," Abstract None None In order to improve image reconstructions, different classes of nonlinear inversion algorithms are developed and used in different research topics like imaging processes in oil industry or the characterization of complex porous media or multiphase flows. These algorithms are able to avoid local minima and to reach more adapted minima of a given misfit function between observed/measured and computed data. Techniques as different as electrical, ultrasound or potential methods, are used. We present here a nonlinear algorithm that allows us to produce permittivity images by using electrical capacitance tomography (ECT). ECT is a non-invasive technique to image non-conductive permittivity distributions and is used in many oil industry imaging applications such as multiphase flows in pipelines, fluidized bed reactors, mixing vessels, and tanks of phase separation. Even if the ECT technique provides low resolution reconstructions, it is cheap, robust and very fast when compared to other imaging tools. In this method one or more rings of electrodes excite a medium to be imaged at high frequencies, and more particularly at frequencies for which a static electrical potential field has fully developed. In many studies of other research groups only one ring of sources is introduced but the reconstruction accuracy was not totally satisfactory due to the 3D nature of the problem to be solved. Instead of using nonlinear stochastic algorithms like the simulated annealing (SA) technique that we optimized in previous studies to image permittivity distributions of granular or solid materials as well as real oil–gas or two-phase flows in 2D cylindrical vessel configurations, we propose here a new ECT inversion tool to image permittivities in a 3D cylindrical configuration. 3D stochastic optimization methods such as SA, neural networks, genetic algorithms can become computationally too prohibitive, and classical local or linear inversion methods excessively smooth images in many cases. Therefore, we propose here a 3D parallel inversion procedure with different numbers of rings and different None None None None None None L None None None p None None None None None None norms, with None None None 1 None None None p None ⩽ None 2 None None None , applied to the model regularization of the misfit function to increase the resolution of the models after inversion. We are able to better reconstruct two-phase and three-phase (oil, gas and solids) mixtures by combining None None None None None None L None None None p None None None None None -norm regularizations of the misfit function to minimize and several rings of electrodes. All these algorithms have been implemented in a more general parallel framework TOMOFAST-X designed for multi-physics joint inversion purposes, and could also be used in other fields of research such as larger-scale geophysical exploration for instance.",project-academic
10.1016/J.PROCIR.2020.04.056,2020-01-01,a,Elsevier BV,a use case to implement machine learning for life time prediction of manufacturing tools," Abstract None None Current machine learning techniques show a high degree of maturity and can be implemented for applications in manufacturing. In this context, using machine learning to investigate the relationship between process parameters and process performance allows an optimization of production systems. Conventional methods to analyze the life time of a manufacturing tool provide only a vague estimation of tool life. Therefore, this paper introduces an industrial use case for using machine learning to predict individual cutting tool life times. Hence, the life time of every individual manufacturing tool can be realized more accurately and its operation time can be maximized.",project-academic
10.1109/ASCC.2015.7244597,2015-05-01,p,IEEE,online sequential extreme learning machine algorithm based human activity recognition using inertial data," Human activity recognition (HAR) is the basis for many real world applications concerning health care, sports and gaming industry. Different methodological perspectives have been proposed to perform HAR. One appealing methodology is to take an advantage of data that are collected from inertial sensors which are embedded in the individual's smartphone. These data contain rich amount of information about daily activities of the user. However, there is no straightforward analytical mapping between a performed activity and its corresponding data. Besides, online training for the classification in these types of applications is a concern. This paper aims at classifying human activities based on the inertial data collected from a user's smartphone. An Online Sequential Extreme Learning Machine (OSELM) method is implemented to train a single hidden layer feed-forward network (SLFN). Experimental results with an average accuracy of 82.05% are achieved.",project-academic
10.1016/J.COMPIND.2015.05.002,2015-12-01,a,Elsevier,local weather prediction system for a heating plant using cognitive approaches," Graphical abstractDisplay Omitted HighlightsA structure of a weather prediction system is proposed.Realized measuring points for data collection and transfer of weather variables.Designed and tested prediction model based on chained neural networks.Realized means for data modification based on fuzzy logic.A prediction system is implemented in a heating plant. Present-day requirements emphasize the need of saving energy. It relates mainly to industrial companies, where the minimization of energy consumption is one of their most important tasks they face. In our paper, we deal with the design of the so-called weather prediction system (WPS) for the needs of a heating plant. The primary task of such a WPS is timely predicting expected heat consumption to prepare the technology characterized by long delays in advance. Heat prediction depends primarily on weather so the crucial part of WPS is the weather, especially temperature, prediction. However, a prediction system needs a variety of further data, too. Therefore, WPS must be regarded as a complex system, including data collection, its processing, own prediction and eventual decision support. This paper gives the overview about existing data processing systems and prediction methods and then it describes a concrete design of a WPS with distributed data measuring points (stations), which are processed using a structure of neural networks based on multilayer perceptrons (MLP) with a combination of fuzzy logic. Based on real experiments we show that also such simple means as MLPs are able to solve complex problems. The paper contains a basic methodology for designing similar WPS, too.",project-academic
10.1088/1757-899X/603/5/052031,2019-09-18,p,IOP Publishing,utilizing big data for enhancing passenger safety in railway stations," In light of the increasing demand and capacity in the railway industry, it is imperative to maintain safety in relation to the complexities of the substantial railway stations. Thus, it is important to take note of the time where investments in new technologies directed at the safety of the railway enable safety and protection in this area. Novel technological techniques such as big data analysis (BDA), data mining or machine learning (ML) have been developed and applied in many areas such as sales, banking and healthcare. The development of such methods has important benefits within the context of railway safety, however, these new methods need to be implemented and developed with consideration of whether these operational models can help to solve the various difficulties that currently exist in the risk analysis of railway stations. Moreover, as the adoption of the Internet of thing (IoT) grows, it is expected that analytical needs for handling data will also increase. It has been shown that the progression towards automation and applying such innovative new technologies such as BDA may be a powerful tool for integration in the future of transportation in general and the railway industry in particular, whereby analytical predictions can aid in the development of safer railway stations which have greater potential for ensuring the safety of passengers. In this paper a Bow Tie (BT) framework model has been created to combine BDA into the risk assessment process. The BDA can be beneficial to the risk assessment, support the decision makers in real time, and reduce human errors. This method can be fully integrated into passenger data and the business model for the railway station. Employing the existing safety records utilizing BDA is expected to mitigate risks, predict hazards, raise safety and security efficiency and reduce the cost.",project-academic
10.1016/J.ENCONMAN.2018.03.044,2018-06-01,a,Pergamon,adaptive air fuel ratio control of dual injection engines under biofuel blends using extreme learning machine," Abstract None None Dual-injection engines, which allow real-time control and injection of two different fuels, are capable of varying the ratio of biofuel blends at different engine operating conditions for optimal engine performance. However, while many experiments have been carried out on these engines to demonstrate their advantages, very few studies have focused on the corresponding air–fuel ratio (AFR) control strategy. In order to achieve stable engine operation, it is essential to maintain transient AFR during the change of fuel blend ratio. Therefore, this study proposes an adaptive controller for AFR control of dual-injection engines. The proposed controller is designed based on a recently developed machine learning method called extreme learning machine, and its stability is verified with Lyapunov analysis. Simulations have been performed on an industry-level engine simulation software to verify the controller. Since dual-injection engines are not available in the market, a spark-ignition engine has been retrofitted for dual-injection operation so that the proposed controller can be implemented and evaluated experimentally. Both simulation and experiment results show that the proposed controller can effectively regulate the AFR to desired level. The results also show that the proposed controller outperforms the engine built-in AFR controller, indicating its significance for dual-injection engines.",project-academic
10.1007/S10845-018-1433-8,2020-01-01,a,Springer US,literature review of industry 4 0 and related technologies," Manufacturing industry profoundly impact economic and societal progress. As being a commonly accepted term for research centers and universities, the Industry 4.0 initiative has received a splendid attention of the business and research community. Although the idea is not new and was on the agenda of academic research in many years with different perceptions, the term “Industry 4.0” is just launched and well accepted to some extend not only in academic life but also in the industrial society as well. While academic research focuses on understanding and defining the concept and trying to develop related systems, business models and respective methodologies, industry, on the other hand, focuses its attention on the change of industrial machine suits and intelligent products as well as potential customers on this progress. It is therefore important for the companies to primarily understand the features and content of the Industry 4.0 for potential transformation from machine dominant manufacturing to digital manufacturing. In order to achieve a successful transformation, they should clearly review their positions and respective potentials against basic requirements set forward for Industry 4.0 standard. This will allow them to generate a well-defined road map. There has been several approaches and discussions going on along this line, a several road maps are already proposed. Some of those are reviewed in this paper. However, the literature clearly indicates the lack of respective assessment methodologies. Since the implementation and applications of related theorems and definitions outlined for the 4th industrial revolution is not mature enough for most of the reel life implementations, a systematic approach for making respective assessments and evaluations seems to be urgently required for those who are intending to speed this transformation up. It is now main responsibility of the research community to developed technological infrastructure with physical systems, management models, business models as well as some well-defined Industry 4.0 scenarios in order to make the life for the practitioners easy. It is estimated by the experts that the Industry 4.0 and related progress along this line will have an enormous effect on social life. As outlined in the introduction, some social transformation is also expected. It is assumed that the robots will be more dominant in manufacturing, implanted technologies, cooperating and coordinating machines, self-decision-making systems, autonom problem solvers, learning machines, 3D printing etc. will dominate the production process. Wearable internet, big data analysis, sensor based life, smart city implementations or similar applications will be the main concern of the community. This social transformation will naturally trigger the manufacturing society to improve their manufacturing suits to cope with the customer requirements and sustain competitive advantage. A summary of the potential progress along this line is reviewed in introduction of the paper. It is so obvious that the future manufacturing systems will have a different vision composed of products, intelligence, communications and information network. This will bring about new business models to be dominant in industrial life. Another important issue to take into account is that the time span of this so-called revolution will be so short triggering a continues transformation process to yield some new industrial areas to emerge. This clearly puts a big pressure on manufacturers to learn, understand, design and implement the transformation process. Since the main motivation for finding the best way to follow this transformation, a comprehensive literature review will generate a remarkable support. This paper presents such a review for highlighting the progress and aims to help improve the awareness on the best experiences. It is intended to provide a clear idea for those wishing to generate a road map for digitizing the respective manufacturing suits. By presenting this review it is also intended to provide a hands-on library of Industry 4.0 to both academics as well as industrial practitioners. The top 100 headings, abstracts and key words (i.e. a total of 619 publications of any kind) for each search term were independently analyzed in order to ensure the reliability of the review process. Note that, this exhaustive literature review provides a concrete definition of Industry 4.0 and defines its six design principles such as interoperability, virtualization, local, real-time talent, service orientation and modularity. It seems that these principles have taken the attention of the scientists to carry out more variety of research on the subject and to develop implementable and appropriate scenarios. A comprehensive taxonomy of Industry 4.0 can also be developed through analyzing the results of this review.",project-academic
10.1088/0964-1726/25/5/053001,2016-03-30,a,IOP Publishing,guided wave based structural health monitoring a review," The paper provides a state of the art review of guided wave based structural health monitoring (SHM). First, the fundamental concepts of guided wave propagation and its implementation for SHM is explained. Following sections present the different modeling schemes adopted, developments in the area of transducers for generation, and sensing of wave, signal processing and imaging technique, statistical and machine learning schemes for feature extraction. Next, a section is presented on the recent advancements in nonlinear guided wave for SHM. This is followed by section on Rayleigh and SH waves. Next is a section on real-life implementation of guided wave for industrial problems. The paper, though briefly talks about the early development for completeness,. is primarily focussed on the recent progress made in the last decade. The paper ends by discussing and highlighting the future directions and open areas of research in guided wave based SHM.",project-academic
,2014-11-22,b,"Springer Publishing Company, Incorporated",identification of dynamic systems an introduction with applications," Precise dynamic models of processes are required for many applications, ranging from control engineering to the natural sciences and economics. Frequently, such precise models cannot be derived using theoretical considerations alone. Therefore, they must be determined experimentally. This book treats the determination of dynamic models based on measurements taken at the process, which is known as system identification or process identification. Both offline and online methods are presented, i.e. methods that post-process the measured data as well as methods that provide models during the measurement. The book is theory-oriented and application-oriented and most methods covered have been used successfully in practical applications for many different processes. Illustrative examples in this book with real measured data range from hydraulic and electric actuators up to combustion engines. Real experimental data is also provided on the Springer webpage, allowing readers to gather their first experience with the methods presented in this book. Among others, the book covers the following subjects: determination of the non-parametric frequency response, (fast) Fourier transform, correlation analysis, parameter estimation with a focus on the method of Least Squares and modifications, identification of time-variant processes, identification in closed-loop, identification of continuous time processes, and subspace methods. Some methods for nonlinear system identification are also considered, such as the Extended Kalman filter and neural networks. The different methods are compared by using a real three-mass oscillator process, a model of a drive train. For many identification methods, hints for the practical implementation and application are provided. The book is intended to meet the needs of students and practicing engineers working in research and development, design and manufacturing.",project-academic
10.1016/J.MFGLET.2018.09.002,2018-10-01,a,Elsevier,industrial artificial intelligence for industry 4 0 based manufacturing systems," Abstract None None The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.",project-academic
,2020-06-01,,,adaptive control of additive production processes in real time using machine learning," FIELD: control systems.SUBSTANCE: invention relates to a method and a system for controlling application process for making random-shaped articles after designing or a process of connecting after designing. Method comprises providing an input design geometry for an object, a set of training data, sensors which output real-time data for object properties when the object is physically made, and providing a processor programmed to predict an optimum set of process control parameters for initiating an application process for making articles of an arbitrary shape or a connection process, which is obtained using machine learning algorithm, removing noise from data on object properties, issued by sensors, before their inclusion in algorithm of machine learning, real-time classification of detected object defects and provision of commands for application process implementation for production of free-form products after design or connection process after designing to make object.EFFECT: technical result consists in control of product manufacture.19 cl, 25 dwg",project-academic
10.1007/S10462-018-09679-Z,2019-01-19,a,Springer Netherlands,machine learning and deep learning frameworks and libraries for large scale data mining a survey," The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.",project-academic
,1989-01-01,b,,constraint satisfaction in logic programming," This book tackles classic problems from operations research and circuit design using a logic programming language embedding consistency techniques, a paradigm emerging from artificial intelligence research. Van Hentenryck proposes a new approach to solving discrete combinatorial problems using these techniques.Logic programming serves as a convenient language for stating combinatorial problems, but its ""generate and test"" paradigm leads to inefficient programs. Van Hentenryck's approach preserves one of the most useful features of logic programming - the duality of its semantics - yet allows a short development time for the programs while preserving most of the efficiency of special purpose programs written in a procedural language.Embedding consistency techniques in logic programming allows for ease and flexibility of programming and short development time because constraint propagation and tree-search programming are abstracted away from the user. It also enables logic programs to be executed efficiently as consistency techniques permit an active use of constraints to remove combinations of values that cannot appear in a solution Van Hentenryck presents a comprehensive overview of this new approach from its theoretical foundations to its design and implementation, including applications to real life combinatorial problems.The ideas introduced in ""Constraint Satisfaction in Logic Programming ""have been used successfully to solve more than a dozen practical problems in operations research and circuit design, including disjunctive scheduling, warehouse location, cutting stock car sequencing, and microcode labeling problems.Pascal Van Hentenryck is a member of the research staff at the European Computer Industry Research Centre. ""Constraint Satisfaction in Logic Programming"" is based on research for the Centre's CHIP project. As an outgrowth of this project, a new language (CHIP) that will include consistency techniques has been developed for commercial use. The book is included in the Logic Programming series edited by Ehud Shapiro.",project-academic
,2011-04-18,b,,data mining and statistics for decision making," Data mining is the process of automatically searching large volumes of data for models and patterns using computational techniques from statistics, machine learning and information theory; it is the ideal tool for such an extraction of knowledge. Data mining is usually associated with a business or an organization's need to identify trends and profiles, allowing, for example, retailers to discover patterns on which to base marketing objectives. This book looks at both classical and recent techniques of data mining, such as clustering, discriminant analysis, logistic regression, generalized linear models, regularized regression, PLS regression, decision trees, neural networks, support vector machines, Vapnik theory, naive Bayesian classifier, ensemble learning and detection of association rules. They are discussed along with illustrative examples throughout the book to explain the theory of these methods, as well as their strengths and limitations. Key Features: * Presents a comprehensive introduction to all techniques used in data mining and statistical learning, from classical to latest techniques. * Starts from basic principles up to advanced concepts. * Includes many step-by-step examples with the main software (R, SAS, IBM SPSS) as well as a thorough discussion and comparison of those software. * Gives practical tips for data mining implementation to solve real world problems. * Looks at a range of tools and applications, such as association rules, web mining and text mining, with a special focus on credit scoring. * Supported by an accompanying website hosting datasets and user analysis. Statisticians and business intelligence analysts, students as well as computer science, biology, marketing and financial risk professionals in both commercial and government organizations across all business and industry sectors will benefit from this book.",project-academic
10.1109/TIM.2019.2915404,2020-04-01,a,IEEE,an end to end steel surface defect detection approach via fusing multiple hierarchical features," A complete defect detection task aims to achieve the specific class and precise location of each defect in an image, which makes it still challenging for applying this task in practice. The defect detection is a composite task of classification and location, leading to related methods is often hard to take into account the accuracy of both. The implementation of defect detection depends on a special detection data set that contains expensive manual annotations. In this paper, we proposed a novel defect detection system based on deep learning and focused on a practical industrial application: steel plate defect inspection. In order to achieve strong classification ability, this system employs a baseline convolution neural network (CNN) to generate feature maps at each stage, and then the proposed multilevel feature fusion network (MFN) combines multiple hierarchical features into one feature, which can include more location details of defects. Based on these multilevel features, a region proposal network (RPN) is adopted to generate regions of interest (ROIs). For each ROI, a detector, consisting of a classifier and a bounding box regressor, produces the final detection results. Finally, we set up a defect detection data set NEU-DET for training and evaluating our method. On the NEU-DET, our method achieves 74.8/82.3 mAP with baseline networks ResNet34/50 by using 300 proposals. In addition, by using only 50 proposals, our method can detect at 20 ft/s on a single GPU and reach 92% of the above performance, hence the potential for real-time detection.",project-academic
10.1016/J.VEHCOM.2019.100198,2020-01-01,a,Elsevier,in vehicle network intrusion detection using deep convolutional neural network," Abstract None None The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.",project-academic
10.1109/ICNSURV.2016.7486356,2016-04-19,p,IEEE,anomaly detection in aircraft data using recurrent neural networks rnn," Anomaly Detection in multivariate, time-series data collected from aircraft's Flight Data Recorder (FDR) or Flight Operational Quality Assurance (FOQA) data provide a powerful means for identifying events and trends that reduce safety margins. The industry standard “Exceedance Detection” algorithm uses a list of specified parameters and their thresholds to identify known deviations. In contrast, Machine Learning algorithms detect unknown unusual patterns in the data either through semi-supervised or unsupervised learning. The Multiple Kernel Anomaly Detection (MKAD) algorithm based on One-class SVM identified 6 of 11 canonical anomalies in a large dataset but is limited by the need for dimensionality reduction, poor sensitivity to short term anomalies, and inability to detect anomalies in latent features. This paper describes the application of Recurrent Neural Networks (RNN) with Long Term Short Term Memory (LTSM) and Gated Recurrent Units (GRU) architectures which can overcome the limitations described above. The RNN algorithms detected 9 out the 11 anomalies in the test dataset with Precision = 1, Recall = 0.818 and F1 score = 0.89. RNN architectures, designed for time-series data, are suited for implementation on the flight deck to provide real-time anomaly detection. The implications of these results are discussed.",project-academic
10.1109/TITS.2019.2906038,2020-03-01,a,IEEE,real time sensor anomaly detection and identification in automated vehicles," Connected and automated vehicles (CAVs) are expected to revolutionize the transportation industry, mainly through allowing for a real-time and seamless exchange of information between vehicles and roadside infrastructure. Although connectivity and automation are projected to bring about a vast number of benefits, they can give rise to new challenges in terms of safety, security, and privacy. To navigate roadways, CAVs need to heavily rely on their sensor readings and the information received from other vehicles and roadside units. Hence, anomalous sensor readings caused by either malicious cyber attacks or faulty vehicle sensors can result in disruptive consequences and possibly lead to fatal crashes. As a result, before the mass implementation of CAVs, it is important to develop methodologies that can detect anomalies and identify their sources seamlessly and in real time. In this paper, we develop an anomaly detection approach through combining a deep learning method, namely convolutional neural network (CNN), with a well-established anomaly detection method, and Kalman filtering with a None None None $\chi ^{2}$ None None -detector, to detect and identify anomalous behavior in CAVs. Our numerical experiments demonstrate that the developed approach can detect anomalies and identify their sources with high accuracy, sensitivity, and F1 score. In addition, this developed approach outperforms the anomaly detection and identification capabilities of both CNNs and Kalman filtering with a None None None $\chi ^{2}$ None None -detector method alone. It is envisioned that this research will contribute to the development of safer and more resilient CAV systems that implement a holistic view toward intelligent transportation system (ITS) concepts.",project-academic
10.1063/1.5108912,2019-09-24,a,AIP Publishing LLC AIP Publishing,novel frontier of photonics for data processing photonic accelerator," In the emerging Internet of things cyber-physical system-embedded society, big data analytics needs huge computing capability with better energy efficiency. Coming to the end of Moore’s law of the electronic integrated circuit and facing the throughput limitation in parallel processing governed by Amdahl’s law, there is a strong motivation behind exploring a novel frontier of data processing in post-Moore era. Optical fiber transmissions have been making a remarkable advance over the last three decades. A record aggregated transmission capacity of the wavelength division multiplexing system per a single-mode fiber has reached 115 Tbit/s over 240 km. It is time to turn our attention to data processing by photons from the data transport by photons. A photonic accelerator (PAXEL) is a special class of processor placed at the front end of a digital computer, which is optimized to perform a specific function but does so faster with less power consumption than an electronic general-purpose processor. It can process images or time-serial data either in an analog or digital fashion on a real-time basis. Having had maturing manufacturing technology of optoelectronic devices and a diverse array of computing architectures at hand, prototyping PAXEL becomes feasible by leveraging on, e.g., cutting-edge miniature and power-efficient nanostructured silicon photonic devices. In this article, first the bottleneck and the paradigm shift of digital computing are reviewed. Next, we review an array of PAXEL architectures and applications, including artificial neural networks, reservoir computing, pass-gate logic, decision making, and compressed sensing. We assess the potential advantages and challenges for each of these PAXEL approaches to highlight the scope for future work toward practical implementation.",project-academic
10.1364/OPTICA.6.001132,2019-04-24,a,,all optical neural network with nonlinear activation functions," Artificial neural networks (ANNs) have now been widely used for industry applications and also played more important roles in fundamental researches. Although most ANN hardware systems are electronically based, optical implementation is particularly attractive because of its intrinsic parallelism and low energy consumption. Here, we propose and demonstrate fully-functioned all optical neural networks (AONNs), in which linear operations are programmed by spatial light modulators and Fourier lenses, and optical nonlinear activation functions are realized with electromagnetically induced transparency in laser-cooled atoms. Moreover, all the errors from different optical neurons here are independent, thus the AONN could scale up to a larger system size with final error still maintaining in a similar level of a single neuron. We confirm its capability and feasibility in machine learning by successfully classifying the order and disorder phases of a typical statistic Ising model. The demonstrated AONN scheme can be used to construct various ANNs of different architectures with the intrinsic parallel computation at the speed of light.",project-academic
10.1201/B17223,2014-07-25,b,CRC Press,background modeling and foreground detection for video surveillance," Background modeling and foreground detection are important steps in video processing used to detect robustly moving objects in challenging environments. This requires effective methods for dealing with dynamic backgrounds and illumination changes as well as algorithms that must meet real-time and low memory requirements. Incorporating both established and new ideas, Background Modeling and Foreground Detection for Video Surveillance provides a complete overview of the concepts, algorithms, and applications related to background modeling and foreground detection. Leaders in the field address a wide range of challenges, including camera jitter and background subtraction. The book presents the top methods and algorithms for detecting moving objects in video surveillance. It covers statistical models, clustering models, neural networks, and fuzzy models. It also addresses sensors, hardware, and implementation issues and discusses the resources and datasets required for evaluating and comparing background subtraction algorithms. The datasets and codes used in the text, along with links to software demonstrations, are available on the books website. A one-stop resource on up-to-date models, algorithms, implementations, and benchmarking techniques, this book helps researchers and industry developers understand how to apply background models and foreground detection methods to video surveillance and related areas, such as optical motion capture, multimedia applications, teleconferencing, video editing, and humancomputer interfaces. It can also be used in graduate courses on computer vision, image processing, real-time architecture, machine learning, or data mining.",project-academic
10.1016/J.FUTURE.2018.08.006,2019-01-01,a,North-Holland,irobot factory an intelligent robot factory based on cognitive manufacturing and edge computing," Abstract None None The Internet of Things (IoT) and Artificial Intelligence (AI) have been driving forces in propelling the technical innovation of intelligent manufacturing, promoting economic growth, and improving the quality of people’s lives. In an intelligent factory, introducing edge computing is conducive to expanding the computing resources, the network bandwidth, and the storage capacity of the cloud platform to the IoT edge, as well as realizing the resource scheduling and data uplink and downlink processing during the manufacturing and production processes. Moreover, the emotion recognition and interaction of the Affective Interaction Intelligence Robot (iRobot), with the IoT cloud platform as the infrastructure and AI technology as the core competitiveness, can better solve the psychological problems of the user. Accordingly, this has become a hot research topic in the field of intelligent manufacturing. In this paper, we describe an intelligent robot factory (iRobot-Factory), adopt a highly interconnected and deeply integrated intelligent production line, and introduce the overall structure, composition, characteristics, and advantages of such a factory in details from the two aspects of cognitive manufacturing and edge computing. Then, we describe the implementation of the volume production of iRobot using iRobot-Factory and look at the system performance experimental results and analysis of the iRobot-Factory and a traditional factory. The experimental results show that our scheme significantly improved both the chip assembly and the production efficiency, while the number of system instructions also decreased significantly. In addition, we discuss some open issues relating to cloud-end fusion, load balancing, and personalized robots to make reference to promoting the emotion recognition and interaction experience of users.",project-academic
10.1109/ACCESS.2018.2846609,2018-06-12,a,IEEE,ubehealth a personalized ubiquitous cloud and edge enabled networked healthcare system for smart cities," Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients’ data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications’ communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.",project-academic
10.14778/3137628.3137633,2017-08-01,p,VLDB Endowment,towards linear algebra over normalized data," Providing machine learning (ML) over relational data is a mainstream requirement for data analytics systems. While almost all ML tools require the input data to be presented as a single table, many datasets are multi-table. This forces data scientists to join those tables first, which often leads to data redundancy and runtime waste. Recent works on ""factorized"" ML mitigate this issue for a few specific ML algorithms by pushing ML through joins. But their approaches require a manual rewrite of ML implementations. Such piecemeal methods create a massive development overhead when extending such ideas to other ML algorithms. In this paper, we show that it is possible to mitigate this overhead by leveraging a popular formal algebra to represent the computations of many ML algorithms: linear algebra. We introduce a new logical data type to represent normalized data and devise a framework of algebraic rewrite rules to convert a large set of linear algebra operations over denormalized data into operations over normalized data. We show how this enables us to automatically ""factorize"" several popular ML algorithms, thus unifying and generalizing several prior works. We prototype our framework in the popular ML environment R and an industrial R-over-RDBMS tool. Experiments with both synthetic and real normalized data show that our framework also yields significant speed-ups, up to 36x on real data.",project-academic
,2016-12-22,a,,towards linear algebra over normalized data," Providing machine learning (ML) over relational data is a mainstream requirement for data analytics systems. While almost all the ML tools require the input data to be presented as a single table, many datasets are multi-table, which forces data scientists to join those tables first, leading to data redundancy and runtime waste. Recent works on ""factorized"" ML mitigate this issue for a few specific ML algorithms by pushing ML through joins. But their approaches require a manual rewrite of ML implementations. Such piecemeal methods create a massive development overhead when extending such ideas to other ML algorithms. In this paper, we show that it is possible to mitigate this overhead by leveraging a popular formal algebra to represent the computations of many ML algorithms: linear algebra. We introduce a new logical data type to represent normalized data and devise a framework of algebraic rewrite rules to convert a large set of linear algebra operations over denormalized data into operations over normalized data. We show how this enables us to automatically ""factorize"" several popular ML algorithms, thus unifying and generalizing several prior works. We prototype our framework in the popular ML environment R and an industrial R-over-RDBMS tool. Experiments with both synthetic and real normalized data show that our framework also yields significant speed-ups, up to 36x on real data.",project-academic
10.1145/3097983.3098139,2017-08-13,p,ACM,extremely fast decision tree mining for evolving data streams," Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning. In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources.",project-academic
10.1016/J.JMSY.2017.02.007,2017-04-01,a,Elsevier,framework and development of fault detection classification using iot device and cloud environment," Abstract None None While Cyber-physical system (CPS) is considered as a key foundation for cyber manufacturing, many related frameworks and applications have been provided. This research suggests a new and effective CPS architecture for supporting multi-sites and multi-products manufacturing. As target processes, the manufacturing processes for vehicles’ High Intensity Discharge (HID) headlight and cable modules are considered. These modules are manufactured with several multi-manufacturing sites consisting of internal manufacturing tasks and intermediate outsourcing processes. In addition, they produce multiple types of HID cable modules with different components. These issues make it difficult to improve the qualities of the overall processes and to control those considering overall manufacturing plants and processes. In order to overcome these limitations, this research provides an Internet of Things (IoT) embedded cloud control architecture. The mixed flow issues are overcome with the cloud control server with the suggested framework. The developed IoT device detects several system status and transmits the signals. The data is analyzed for the fault detection classification (FDC) mechanism using deep learning based analytics. Then, the cyber manufacturing based simulation is executed using the provided multi-products queueing network model. The estimated simulation results is used for generating dynamic manufacturing decisions reflecting the real-time changes of the production environment. The suggested framework and its implementations can be used for various industrial processes and applications.",project-academic
10.1016/J.CIRP.2018.04.041,2018-01-01,a,Elsevier,reinforcement learning for adaptive order dispatching in the semiconductor industry," Abstract None None The digitalization of production systems tends to provide a huge amount of data from heterogeneous sources. This is particularly true for the semiconductor industry wherein real time process monitoring is inherently required to achieve a high yield of good parts. An application of data-driven algorithms in production planning to enhance operational excellence for complex semiconductor production systems is currently missing. This paper shows the successful implementation of a reinforcement learning-based adaptive control system for order dispatching in the semiconductor industry. Furthermore, a performance comparison of the learning-based control system with the traditionally used rule-based system shows remarkable results. Since a strict rulebook does not bind the learning-based control system, a flexible adaption to changes in the environment can be achieved through a combination of online and offline learning.",project-academic
10.1016/J.COMPIND.2018.07.004,2018-10-01,a,Elsevier,idarts towards intelligent data analysis and real time supervision for industry 4 0," Abstract None None The manufacturing industry represents a data rich environment, in which larger and larger volumes of data are constantly being generated by its processes. However, only a relatively small portion of it is actually taken advantage of by manufacturers. As such, the proposed Intelligent Data Analysis and Real-Time Supervision (IDARTS) framework presents the guidelines for the implementation of scalable, flexible and pluggable data analysis and real-time supervision systems for manufacturing environments. IDARTS is aligned with the current Industry 4.0 trend, being aimed at allowing manufacturers to translate their data into a business advantage through the integration of a Cyber-Physical System at the edge with cloud computing. It combines distributed data acquisition, machine learning and run-time reasoning to assist in fields such as predictive maintenance and quality control, reducing the impact of disruptive events in production.",project-academic
10.3390/S18051428,2018-05-04,a,Multidisciplinary Digital Publishing Institute,implementation of cyber physical production systems for quality prediction and operation control in metal casting," The prediction of internal defects of metal casting immediately after the casting process saves unnecessary time and money by reducing the amount of inputs into the next stage, such as the machining process, and enables flexible scheduling. Cyber-physical production systems (CPPS) perfectly fulfill the aforementioned requirements. This study deals with the implementation of CPPS in a real factory to predict the quality of metal casting and operation control. First, a CPPS architecture framework for quality prediction and operation control in metal-casting production was designed. The framework describes collaboration among internet of things (IoT), artificial intelligence, simulations, manufacturing execution systems, and advanced planning and scheduling systems. Subsequently, the implementation of the CPPS in actual plants is described. Temperature is a major factor that affects casting quality, and thus, temperature sensors and IoT communication devices were attached to casting machines. The well-known NoSQL database, HBase and the high-speed processing/analysis tool, Spark, are used for IoT repository and data pre-processing, respectively. Many machine learning algorithms such as decision tree, random forest, artificial neural network, and support vector machine were used for quality prediction and compared with R software. Finally, the operation of the entire system is demonstrated through a CPPS dashboard. In an era in which most CPPS-related studies are conducted on high-level abstract models, this study describes more specific architectural frameworks, use cases, usable software, and analytical methodologies. In addition, this study verifies the usefulness of CPPS by estimating quantitative effects. This is expected to contribute to the proliferation of CPPS in the industry.",project-academic
10.1109/TMECH.2013.2245337,2014-04-01,a,IEEE,image based visual servoing of a 7 dof robot manipulator using an adaptive distributed fuzzy pd controller," This paper is concerned with the design and implementation of a distributed proportional-derivative (PD) controller of a 7-degrees of freedom (DOF) robot manipulator using the Takagi-Sugeno (T-S) fuzzy framework. Existing machine learning approaches to visual servoing involve system identification of image and kinematic Jacobians. In contrast, the proposed approach actuates a control signal primarily as a function of the error and derivative of the error in the desired visual feature space. This approach leads to a significant reduction in the computational burden as compared to model-based approaches, as well as existing learning approaches to model inverse kinematics. The simplicity of the controller structure will make it attractive in industrial implementations where PD/PID type schemes are in common use. While the initial values of PD gain are learned with the help of model-based controller, an online adaptation scheme has been proposed that is capable of compensating for local uncertainties associated with the system and its environment. Rigorous experiments have been performed to show that visual servoing tasks such as reaching a static target and tracking of a moving target can be achieved using the proposed distributed PD controller. It is shown that the proposed adaptive scheme can dynamically tune the controller parameters during visual servoing, so as to improve its initial performance based on parameters obtained while mimicking the model-based controller. The proposed control scheme is applied and assessed in real-time experiments using an uncalibrated eye-in-hand robotic system with a 7-DOF PowerCube robot manipulator.",project-academic
10.1007/978-3-319-57870-5_1,2018-01-01,a,"Springer, Cham",a conceptual framework for industry 4 0," Industrial Revolution emerged many improvements in manufacturing and service systems. Because of remarkable and rapid changes appeared in manufacturing and information technology, synergy aroused from the integration of the advancements in information technology, services and manufacturing were realized. These advancements conduced to the increasing productivity both in service systems and manufacturing environment. In recent years, manufacturing companies and service systems have been faced substantial challenges due to the necessity in the coordination and connection of disruptive concepts such as communication and networking (Industrial Internet), embedded systems (Cyber Physical Systems), adaptive robotics, cyber security, data analytics and artificial intelligence, and additive manufacturing. These advancements caused the extension of the developments in manufacturing and information technology, and these coordinated and communicative technologies are constituted to the term, Industry 4.0 which was first announced from German government as one of the key initiatives and highlights a new industrial revolution. As a result, Industry 4.0 indicates more productive systems; companies have been searching the right adaptation of this term. On the other hand, the achievement criteria and performance measurements of the transformation to Industry 4.0 are still uncertain. Additionally, a structured and systematic implementation roadmap is still not clear. Thus, in this study, the fundamental relevance between design principles and technologies is given and conceptual framework for Industry 4.0 is proposed concerning fundamentals of smart products and smart processes development.",project-academic
10.1109/TASE.2017.2696748,2018-07-01,a,IEEE,automatic fabric defect detection using learning based local textural distributions in the contourlet domain," We propose a learning-based approach for automatic detection of fabric defects. Our approach is based on a statistical representation of fabric patterns using the redundant contourlet transform (RCT). The distribution of the RCT coefficients are modeled using a finite mixture of generalized Gaussians (MoGG), which constitute statistical signatures distinguishing between defective and defect-free fabrics. In addition to being compact and fast to compute, these signatures enable accurate localization of defects. Our defect detection system is based on three main steps. In the first step, a preprocessing is applied for detecting basic pattern size for image decomposition and signature calculation. In the second step, labeled fabric samples are used to train a Bayes classifier (BC) to discriminate between defect-free and defective fabrics. Finally, defects are detected during image inspection by testing local patches using the learned BC. Our approach can deal with multiple types of textile fabrics, from simple to more complex ones. Experiments on the TILDA database have demonstrated that our method yields better results compared with recent state-of-the-art methods. None None Note to Practitioners —Fabric defect detection is central to automated visual inspection and quality control in textile manufacturing. This paper deals with this problem through a learning-based approach. By opposite to several existing approaches for fabric defect detection, which are effective in only some types of fabrics and/or defects, our method can deal with almost all types of patterned fabric and defects. To enable both detection and localization of defects, a fabric image is first divided into local blocks, which are representative of the repetitive pattern structure of the fabric. Then, statistical signatures are calculated by modeling the distribution of coefficients of an RCT using the finite MoGG. The discrimination between defect-free and defective fabrics is then achieved through supervised classification of RCT-MoGG signatures based on expert-labeled examples of defective fabric images. Experiments have shown that our method yields very good performance in terms of defect detection and localization. In addition to its accuracy, inspection of images can be performed in a fully automatic fashion, whereas only labeled examples are initially required. Finally, our method can be easily adapted to a real-time scenario since defect detection on inspected images is performed at the block level, which can be easily parallelized through hardware implementation.",project-academic
10.1007/978-3-030-18732-3_1,2020-01-01,a,Springer Science and Business Media LLC,the convergence of digital twin iot and machine learning transforming data into action," Digital twins, Internet of Things (IoT), block chains, and Artificial Intelligence (AI) may redefine our imagination and future vision of globalization. Digital Twin will likely affect most of the enterprises worldwide as it duplicates the physical model for remote monitoring, viewing, and controlling based on the digital format. It is actually the living model of the physical system which continuously adapts to operational changes based on the real-time data from various IoT sensors and devices and forecasts the future of the corresponding physical counterparts with the help of machine learning/artificial intelligence. We have investigated the architecture, applications, and challenges in the implementation of digital twin with IoT capabilities. Some of the major research areas like big data and cloud, data fusion, and security in digital twins have been explored. AI facilitates the development of new models and technology systems in the domain of intelligent manufacturing.",project-academic
10.1007/978-981-33-4901-8_8,2021-01-01,a,"Springer, Singapore",digital twins based lca and iso 20140 for smart and sustainable manufacturing systems," Currently, several facilities around the world, as part of government’s sustainable development strategies, are turning to the development of smart value chains that can leverage efficiently all of countries available and local resources. Smart factories vision embracing the fourth industrial revolution of manufacturing plants introduced a completely renewed industrial organization based on collaboration between human intelligence and capabilities and machines intelligence and computing capacities, manufacturing plants horizontal and vertical integration, and with particular interest for our paper end-to-end engineering. This collaborative endeavour has been translated in the field by a set of technologies for instance advanced simulation tools through digital twins. The use of these new resources and productivity enhancement has not been without consequences on natural ecosystems, which are increasingly subject to industrial competitiveness pressure. To counter the adverse environmental effects of industrial and technological growth, some manufacturers are developing simulation-based life cycle assessment approaches. Over the last few years, several research communities have explored the potential of simulation-based LCA method for the optimization of the environmental impact of production systems through the application of advanced artificial intelligence algorithms. However, only a limited number of these attempts have seen their practical implementation. Currently, digital twins’ technologies are rapidly expanding due to the advantages they offer for real-time simulation, multidimensional replication of industrial systems and end-to-end engineering. Through this work, we propose a generic solution based on digital twins’ technologies and ISO 20140 for real-time life cycle assessment and manufacturing systems sustainable optimization.",project-academic
10.1109/TCSI.2017.2743004,2018-04-01,a,IEEE,current mirror array a novel circuit topology for combining physical unclonable function and machine learning," Edge analytics support industrial Internet of Things by pushing some data processing capacity to the edge of the network instead of sending the streaming data captured by the sensor nodes directly to the cloud. It is advantageous to endow machine learners for data reduction with suitable security primitives for privacy protection in edge computing devices to conserve area and power consumption. In this paper, we propose a novel physical unclonable function (PUF) based on current mirror array (CMA) circuits that reuses the circuit implementation of a machine learner–the extreme learning machine (ELM), which is a randomized neural network. Seven different challenge activation and response readout schemes are proposed to realize different weak and strong PUF functions from within the same CMA array. ELM endowed with such reconfigurable challenge-response mechanism is more robust and adaptable to different authentication protocols and security functions. Measurement results on None None None $0.35\mu m$ None None None test chips demonstrate that the proposed strong PUF outperforms other state-of-the-art designs with smaller area/bit of None None None $9\times 10^{-36} \mu m^{2}$ None None None and lower native bit error rate (BER) of 0.16% with an added overhead of less than 2.5% power and 2.9% area over the native ELM implementation.",project-academic
10.2139/SSRN.3845698,2021-05-13,a,,bbe simulating the microstructural dynamics of an in play betting exchange via agent based modelling," This paper describes the rationale for, and design of, an agent-based simulation model of a contemporary online sports-betting exchange: such exchanges, closely related to the exchange mechanisms at the heart of major financial markets, have revolutionized the gambling industry in the past 20 years, but gathering sufficiently large quantities of rich and temporally high-resolution (sub-second interval) data from real exchanges — i.e., the sort of data that is needed in large quantities for leading-edge machine learning techniques such as Deep Learning — is often very expensive, and sometimes simply impossible; this creates a need for a plausibly realistic synthetic data generator (SDG), which is what the simulation described here is intended to provide. The simulator, named the Bristol Betting Exchange (BBE), is intended to become a common platform, a data-source and experimental test-bed, for any/all researchers studying the application of artificial intelligence (AI) and machine learning (ML) techniques to issues arising in betting exchanges; and, as far as I have been able to determine, BBE is the first of its kind: a free open-source agent-based simulation model consisting not only of a sports-betting exchange, but also a minimal simulation model of racetrack sporting events (e.g., horse-races or car-races) about which bets may be made, and a population of simulated bettors who each form their own private evaluation of odds and place bets on the exchange before and — crucially — during the race itself (i.e., so-called ""in-play'' betting) and whose betting opinions change second-by-second as each race event unfolds. BBE is offered as a proof-of-concept simulator system that enables the generation of large high-resolution data-sets for automated discovery or improvement of profitable strategies for betting on sporting events via the application of AI/ML and advanced data analytics techniques. This paper offers an extensive survey of relevant literature and explains the motivation and design of BBE, and presents brief illustrative results. In a companion paper, I and my co-authors describe three independent implementations of BBE that each serve different audiences, for which the source-code is being released on GitHub, and show comparative results.",project-academic
10.1016/J.ASOC.2020.106208,2020-06-01,a,Elsevier,dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning," Abstract None None In modern manufacturing industry, dynamic scheduling methods are urgently needed with the sharp increase of uncertainty and complexity in production process. To this end, this paper addresses the dynamic flexible job shop scheduling problem (DFJSP) under new job insertions aiming at minimizing the total tardiness. Without lose of generality, the DFJSP can be modeled as a Markov decision process (MDP) where an intelligent agent should successively determine which operation to process next and which machine to assign it on according to the production status of current decision point, making it particularly feasible to be solved by reinforcement learning (RL) methods. In order to cope with continuous production states and learn the most suitable action (i.e. dispatching rule) at each rescheduling point, a deep Q-network (DQN) is developed to address this problem. Six composite dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine every time an operation is completed or a new job arrives. Seven generic state features are extracted to represent the production status at a rescheduling point. By taking the continuous state features as input to the DQN, the state–action value (Q-value) of each dispatching rule can be obtained. The proposed DQN is trained using deep Q-learning (DQL) enhanced by two improvements namely double DQN and soft target weight update. Moreover, a “softmax” action selection policy is utilized in real implementation of the trained DQN so as to promote the rules with higher Q-values while maintaining the policy entropy. Numerical experiments are conducted on a large number of instances with different production configurations. The results have confirmed both the superiority and generality of DQN compared to each composite rule, other well-known dispatching rules as well as the stand Q-learning-based agent.",project-academic
10.1016/J.IJPE.2016.10.021,2017-01-01,a,Elsevier,single hidden layer neural networks for forecasting intermittent demand," Abstract None None Managing intermittent demand is a vital task in several industrial contexts, and good forecasting ability is a fundamental prerequisite for an efficient inventory control system in stochastic environments. In recent years, research has been conducted on single-hidden layer feedforward neural networks, with promising results. In particular, back-propagation has been adopted as a gradient descent-based algorithm for training networks. However, when managing a large number of items, it is not feasible to optimize networks at item level, due to the effort required for tuning the parameters during the training stage. A simpler and faster learning algorithm, called the extreme learning machine, has been therefore proposed in the literature to address this issue, but it has never been tried for forecasting intermittent demand. On the one hand, an extensive comparison of single-hidden layer networks trained by back-propagation is required to improve our understanding of them as predictors of intermittent demand. On the other hand, it is also worth testing extreme learning machines in this context, because of their lower computational complexity and good generalisation ability. None In this paper, neural networks trained by back-propagation and extreme learning machines are compared with benchmark neural networks, as well as standard forecasting methods for intermittent demand on real-time series, by combining different input patterns and architectures. A statistical analysis is then conducted to validate the best performance through different aggregation levels. Finally, some insights for practitioners are presented to improve the potential of neural networks for implementation in real environments.",project-academic
10.1145/3316781.3317741,2019-06-02,p,ACM,a fast reliable and wide voltage range in memory computing architecture," As the computational complexity of applications on the consumer market, such as high-definition video encoding and deep neural networks, become ever more demanding, novel ways to efficiently compute data intensive workloads are being explored. In this context, In-Memory Computing (IMC) solutions, and particularly bitline computing in SRAM, appear promising as they mitigate one of the most energy consuming aspects in computation: data movement. While IMC architectural level characteristics have been defined by the research community, only a few works so far have explored the implementation of such memories at a low level. Furthermore, these proposed solutions are either slow $(\lt1$ GHz), area hungry (10T SRAM), or suffer from read disturb and corruption issues. Overall, there is no extensive design study considering realistic assumptions at the circuit level. In this work we propose a fast (up to 2.2Ghz), 6T SRAM-based, reliable (no read disturb issues), and wide voltage range (from 0.6 to 1V) IMC architecture using local bitlines. Beyond standard read and write, the proposed architecture can perform copy, addition and shift operations at the array level. As addition is the slowest operation, we propose a modified carry chain adder, providing a 2 × None carry propagation improvement. The proposed architecture is validated using a 28nm bulk high performances technology PDK with CMOS variability and post-layout simulations. High density SRAM bitcells $(0.127_{\mu }\mathrm{m})$ enable area efficiency of 59.7% for a $256_{\times} 128$ array, on par with current industrial standards.",project-academic
10.1016/J.PETROL.2019.106332,2019-12-01,a,Elsevier,machine learning methods applied to drilling rate of penetration prediction and optimization a review," Abstract None None Drilling wells in challenging oil/gas environments implies in large capital expenditure on wellbore's construction. In order to optimize the drilling related operation, real-time decisions making have been put in place, so that prediction of rate of penetration (ROP) with accuracy is essential. Despite many efforts (theoretical and experimental) throughout the years, modeling the ROP as a mathematical function of some key variables is not so trivial, due to the highly non-linearity behavior experienced. Therefore, several researches in the recent years have been proposing to use data-driven models from artificial intelligence field for ROP prediction and optimization. None This paper presents an extensive review of the literature on ROP prediction, especially, with machine learning techniques, as well as how these models can be used to optimize the drilling activities. The ROP models are classified as traditional models (based on physics-models), statistical models (e.g. multiple regression), or machine learning methods. This review enables to see that machine learning techniques can potentially outperform in terms of ROP-prediction accuracy on top of traditional or statistical models. Throughout this work, an extensive analysis of different ways of obtaining ROP models is carried out, concluding with different strategies adopted in literature to perform data-driven model optimization. None Despite the saving potential which can be achieved with real-time optimization based on data-driven ROP models, it is noticeable that there is a lack of implementation of those techniques in the industry, as per literature review. To take a step forward in real implementations, the petroleum industry must be aware that yet no rule of thumb already exists on this specific area, but still, good and very reasonable results can be achieved by following the best practices identified in this review. In addition, the modern practices of machine learning provide promising guidelines for implementing projects in oil and gas industry.",project-academic
10.1016/J.ENGAPPAI.2016.08.019,2017-06-01,a,Pergamon,gpu based parallel optimization of immune convolutional neural network and embedded system," Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",project-academic
,2018-11-06,,,supermarket commodity identification method based on deep learning," The invention puts forward a supermarket commodity identification method based on deep learning, and solves the problem in the prior art that an identification rate is low in a real supermarket scene.The implementation scheme of the method comprises the following steps that: 1) manufacturing a supermarket goods shelf commodity training set; 2) constructing a commodity detection network, and carrying out training on the manufactured training set; 3) inputting a goods shelf picture into a trained network model to obtain all commodity target areas in a picture; 4) taking the output of a last convolutional layer in the network model as the characteristics of a commodity target; 5) coding the commodity characteristic to obtain a commodity descriptor; 6) calculating a similarity of the commodity model descriptor in an existing model library and the commodity descriptor; and 7) taking the most similar commodity model as an identification result. By use of the method, a commodity target areain the goods shelf picture can be accurately detected, in addition, the commodity target can be correctly identified, and the method can be used for managing supermarket goods shelf commodities.",project-academic
10.1016/J.CSDA.2010.06.014,2011-01-01,a,Elsevier Science Publishers B. V.,robust weighted kernel logistic regression in imbalanced and rare events data," Recent developments in computing and technology, along with the availability of large amounts of raw data, have contributed to the creation of many effective techniques and algorithms in the fields of pattern recognition and machine learning. The main objectives for developing these algorithms include identifying patterns within the available data or making predictions, or both. Great success has been achieved with many classification techniques in real-life applications. With regard to binary data classification in particular, analysis of data containing rare events or disproportionate class distributions poses a great challenge to industry and to the machine learning community. This study examines rare events (REs) with binary dependent variables containing many more non-events (zeros) than events (ones). These variables are difficult to predict and to explain as has been evidenced in the literature. This research combines rare events corrections to Logistic Regression (LR) with truncated Newton methods and applies these techniques to Kernel Logistic Regression (KLR). The resulting model, Rare Event Weighted Kernel Logistic Regression (RE-WKLR), is a combination of weighting, regularization, approximate numerical methods, kernelization, bias correction, and efficient implementation, all of which are critical to enabling RE-WKLR to be an effective and powerful method for predicting rare events. Comparing RE-WKLR to SVM and TR-KLR, using non-linearly separable, small and large binary rare event datasets, we find that RE-WKLR is as fast as TR-KLR and much faster than SVM. In addition, according to the statistical significance test, RE-WKLR is more accurate than both SVM and TR-KLR.",project-academic
10.1016/J.MLWA.2020.100006,2020-12-15,a,Machine Learning with Applications,chatbots history technology and applications," Abstract None None This literature review presents the History, Technology, and Applications of Natural Dialog Systems or simply chatbots. It aims to organize critical information that is a necessary background for further research activity in the field of chatbots. More specifically, while giving the historical evolution, from the generative idea to the present day, we point out possible weaknesses of each stage. After we present a complete categorization system, we analyze the two essential implementation technologies, namely, the pattern matching approach and machine learning. Moreover, we compose a general architectural design that gathers critical details, and we highlight crucial issues to take into account before system design. Furthermore, we present chatbots applications and industrial use cases while we point out the risks of using chatbots and suggest ways to mitigate them. Finally, we conclude by stating our view regarding the direction of technology so that chatbots will become really smart.",project-academic
10.1155/2010/794891,2010-01-01,a,Hindawi,cordic architectures a survey," In the last decade, CORDIC algorithm has drawn wide attention from academia and industry for various applications such as DSP, biomedical signal processing, software defined radio, neural networks, and MIMO systems to mention just a few. It is an iterative algorithm, requiring simple shift and addition operations, for hardware realization of basic elementary functions. Since CORDIC is used as a building block in various single chip solutions, the critical aspects to be considered are high speed, low power, and low area, for achieving reasonable overall performance. In this paper, we first classify the CORDIC algorithm based on the number system and discuss its importance in the implementation of CORDIC algorithm. Then, we present systematic and comprehensive taxonomy of rotational CORDIC algorithms, which are subsequently discussed in depth. Special attention has been devoted to the higher radix and flat techniques proposed in the literature for reducing the latency. Finally, detailed comparison of various algorithms is presented, which can provide a first-order information to designers looking for either further improvement of performance or selection of rotational CORDIC for a specific application.",project-academic
10.1016/J.NEUCOM.2017.08.036,2018-01-31,a,Elsevier,data driven model free slip control of anti lock braking systems using reinforcement q learning," Abstract None None This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an e-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS.",project-academic
10.1039/D0LC00521E,2020-08-26,a,The Royal Society of Chemistry,ai on a chip," Artificial intelligence (AI) has dramatically changed the landscape of science, industry, defence, and medicine in the last several years. Supported by considerably enhanced computational power and cloud storage, the field of AI has shifted from mostly theoretical studies in the discipline of computer science to diverse real-life applications such as drug design, material discovery, speech recognition, self-driving cars, advertising, finance, medical imaging, and astronomical observation, where AI-produced outcomes have been proven to be comparable or even superior to the performance of human experts. In these applications, what is essentially important for the development of AI is the data needed for machine learning. Despite its prominent importance, the very first process of the AI development, namely data collection and data preparation, is typically the most laborious task and is often a limiting factor of constructing functional AI algorithms. Lab-on-a-chip technology, in particular microfluidics, is a powerful platform for both the construction and implementation of AI in a large-scale, cost-effective, high-throughput, automated, and multiplexed manner, thereby overcoming the above bottleneck. On this platform, high-throughput imaging is a critical tool as it can generate high-content information (e.g., size, shape, structure, composition, interaction) of objects on a large scale. High-throughput imaging can also be paired with sorting and DNA/RNA sequencing to conduct a massive survey of phenotype-genotype relations whose data is too complex to analyze with traditional computational tools, but is analyzable with the power of AI. In addition to its function as a data provider, lab-on-a-chip technology can also be employed to implement the developed AI for accurate identification, characterization, classification, and prediction of objects in mixed, heterogeneous, or unknown samples. In this review article, motivated by the excellent synergy between AI and lab-on-a-chip technology, we outline fundamental elements, recent advances, future challenges, and emerging opportunities of AI with lab-on-a-chip technology or ""AI on a chip"" for short.",project-academic
10.1007/S10845-016-1194-1,2018-10-01,a,Springer US,manifold learning based rescheduling decision mechanism for recessive disturbances in rfid driven job shops," In actual manufacturing processes, some unexpected disturbances, called as recessive disturbances (e.g., job set-up time variation and arrival time deviation), would gradually make the original production schedule obsolete. It is hard for production managers to perceive their presences. Thus, the impact of recessive disturbances can not be eliminated by rescheduling in time. On account of this, a rescheduling decision mechanism for recessive disturbances in RFID-driven job shops is proposed in this article, and a manifold learning method, which reduces the response time of manufacturing system, is applied in the mechanism to preprocess manufacturing data. The rescheduling decision mechanism is expected to answer the questions of whether to reschedule, when to reschedule, and which rescheduling method to be used. Firstly, RFID devices acquire the actual process completion time of all work in process (WIPs) at every WIP machining process completion time. Secondly, recessive disturbances are quantified to time accumulation error (TAE) which represents the difference between actual process completion time and planned process completion time. Lastly, according to the TAE and production managers’ experience, the rescheduling decision mechanism selects a proper rescheduling method to update or repair the original production schedule. The realization algorithms of rescheduling decision mechanism includes: (1) supervised locally linear embedding. (2) General regression neural network. (3) Least square-support vector Machine. Finally, a numerical experiment is used to demonstrate the implementation procedures of the rescheduling decision mechanism.",project-academic
10.1109/ICPP.2011.40,2011-09-13,p,IEEE,location aware mapreduce in virtual cloud," MapReduce is an important programming model for processing and generating large data sets in parallel. It is commonly applied in applications such as web indexing, data mining, machine learning, etc. As an open-source implementation of MapReduce, Hadoop is now widely used in industry. Virtualization, which is easy to configure and economical to use, shows great potential for cloud computing. With the increasing core number in a CPU and involving of virtualization technique, one physical machine can hosts more and more virtual machines, but I/O devices normally do not increase so rapidly. As MapReduce system is often used to running I/O intensive applications, decreasing of data redundancy and load unbalance, which increase I/O interference in virtual cloud, come to be serious problems. This paper builds a model and defines metrics to analyze the data allocation problem in virtual environment theoretically. And we design a location-aware file block allocation strategy that retains compatibility with the native Hadoop. Our model simulation and experiment in real system shows our new strategy can achieve better data redundancy and load balance to reduce I/O interference. Execution time of applications such as RandomWriter, Text Sort and Word Count are reduced by up to 33% and 10% on average.",project-academic
10.3390/S21020487,2021-01-12,a,Multidisciplinary Digital Publishing Institute,reliable industry 4 0 based on machine learning and iot for analyzing monitoring and securing smart meters," The modern control infrastructure that manages and monitors the communication between the smart machines represents the most effective way to increase the efficiency of the industrial environment, such as smart grids. The cyber-physical systems utilize the embedded software and internet to connect and control the smart machines that are addressed by the internet of things (IoT). These cyber-physical systems are the basis of the fourth industrial revolution which is indexed by industry 4.0. In particular, industry 4.0 relies heavily on the IoT and smart sensors such as smart energy meters. The reliability and security represent the main challenges that face the industry 4.0 implementation. This paper introduces a new infrastructure based on machine learning to analyze and monitor the output data of the smart meters to investigate if this data is real data or fake. The fake data are due to the hacking and the inefficient meters. The industrial environment affects the efficiency of the meters by temperature, humidity, and noise signals. Furthermore, the proposed infrastructure validates the amount of data loss via communication channels and the internet connection. The decision tree is utilized as an effective machine learning algorithm to carry out both regression and classification for the meters’ data. The data monitoring is carried based on the industrial digital twins’ platform. The proposed infrastructure results provide a reliable and effective industrial decision that enhances the investments in industry 4.0.",project-academic
10.1109/ACCESS.2018.2871724,2018-09-24,a,IEEE,artificial intelligence for cloud assisted smart factory," In the context of industry 4.0, the main way to realize the intelligent manufacturing is to build a smart factory integrated with the advanced technologies, such as the Internet of Things (IoT), cloud computing, and artificial intelligence (AI). With the aim to emphasize the role and potential of cloud computing and AI in improving the smart factories’ performances, such as system flexibility, efficiency, and intelligence, we comprehensively summarize and explain the AI application in a cloud-assisted smart factory (CaSF). In this paper, a vertically-integrated four-tier CaSF architecture is presented. Also, the key AI technologies involved in the CaSF are classified and described according to the logical relationships in the architecture hierarchy. Finally, the main issues and technical challenges of AI technologies in the CaSF systems are introduced, and some possible solutions are also given. The application of the AI in smart factories has accelerated the implementation of the industry 4.0 to the certain extent.",project-academic
10.3390/MACHINES7020042,2019-06-14,a,Multidisciplinary Digital Publishing Institute,unmanned ground vehicle modelling in gazebo ros based environments," The fusion of different technologies is the base of the fourth industrial revolution. Companies are encouraged to integrate new tools in their production processes in order to improve working conditions and increase productivity and production quality. The integration between information, communication technologies and industrial automation can create highly flexible production models for products and services that can be customized through real-time interactions between consumer, production and machinery throughout the production process. The future of production, therefore, depends on increasingly intelligent machinery through the use of digital systems. The key elements for future integrated devices are intelligent systems and machines, based on human–machine interaction and information sharing. To do so, the implementation of shared languages that allow different systems to dialogue in a simple way is necessary. In this perspective, the use of advanced prototyping tools like Open-Source programming systems, the development of more detailed multibody models through the use of CAD software and the use of self-learning techniques will allow for developing a new class of machines capable of revolutionizing our companies. The purpose of this paper is to present a waypoint navigation activity of a custom Wheeled Mobile Robot (WMR) in an available simulated 3D indoor environment by using the Gazebo simulator. Gazebo was developed in 2002 at the University of Southern California. The idea was to create a high-fidelity simulator that gave the possibility to simulate robots in outdoor environments under various conditions. In particular, we wanted to test the high-performance physics Open Dynamics Engine (ODE) and the sensors feature present in Gazebo for prototype development activities. This choice was made for the possibility of emulating not only the system under analysis, but also the world in which the robot will operate. Furthermore, the integration tools available with Solidworks and Matlab-Simulink, well known commercial platforms of modelling and robotics control respectively, are also explored.",project-academic
10.2118/89033-JPT,2005-04-01,a,Society of Petroleum Engineers,recent developments in application of artificial intelligence in petroleum engineering," With the recent interest and enthusiasm in the industry toward smart wells, intelligent fields, and real-time analysis and interpretation of large amounts of data for process optimization, our industry’s need for powerful, robust, and intelligent tools has significantly increased. Operations such as asset evaluation; 3D- and 4D-seismicdata interpretation; complex multilateral-drilling design and implementation; log interpretation; building of geologic models; well-test design, implementation, and interpretation; reservoir modeling; and simulation are being integrated to result in comprehensive reservoir management. In recent years, artificial intelligence (AI), in its many integrated flavors from neural networks to genetic optimization to fuzzy logic, has made solid steps toward becoming more accepted in the mainstream of the oil and gas industry. In a recent set of JPT articles,1‐3 fundamentals of these technologies were discussed. This article covers some of the most recent and advanced uses of intelligent systems in our industry and discusses their potential role in our industry’s future.",project-academic
,2020-08-10,a,,bilevel learning model towards industrial scheduling," Automatic industrial scheduling, aiming at optimizing the sequence of jobs over limited resources, is widely needed in manufacturing industries. However, existing scheduling systems heavily rely on heuristic algorithms, which either generate ineffective solutions or compute inefficiently when job scale increases. Thus, it is of great importance to develop new large-scale algorithms that are not only efficient and effective, but also capable of satisfying complex constraints in practice. In this paper, we propose a Bilevel Deep reinforcement learning Scheduler, \textit{BDS}, in which the higher level is responsible for exploring an initial global sequence, whereas the lower level is aiming at exploitation for partial sequence refinements, and the two levels are connected by a sliding-window sampling mechanism. In the implementation, a Double Deep Q Network (DDQN) is used in the upper level and Graph Pointer Network (GPN) lies within the lower level. After the theoretical guarantee for the convergence of BDS, we evaluate it in an industrial automatic warehouse scenario, with job number up to $5000$ in each production line. It is shown that our proposed BDS significantly outperforms two most used heuristics, three strong deep networks, and another bilevel baseline approach. In particular, compared with the most used greedy-based heuristic algorithm in real world which takes nearly an hour, our BDS can decrease the makespan by 27.5\%, 28.6\% and 22.1\% for 3 largest datasets respectively, with computational time less than 200 seconds.",project-academic
10.1016/J.PROCIR.2017.03.093,2017-01-01,a,Elsevier,cyber physical manufacturing metrology model cpm3 for sculptured surfaces turbine blade application," Abstract None None Cyber-Physical Manufacturing (CPM) and digital manufacturing represent the key elements for implementation of Industry 4.0 framework. Worldwide, Industry 4.0 becomes national research strategy in the field of engineering for the following ten years. The International Conference USA-EU-Far East-Serbia Manufacturing Summit was held from 31 st None May to 2 nd None June 2016 in Belgrade, Serbia. The result of the conference was the development of Industry 4.0 Model for Serbia as a framework for New Industrial Policy – Horizon 2020/2030. None Implementation of CPM in manufacturing systems generates “smart factory”. Products, resources, and processes within smart factory are realized and controlled through CPM model. This leads to significant advantages with respect to high product/process quality, real-time applications, savings in resources consumption, as well as, lower costs in comparison with classical manufacturing systems. Smart factory is designed in accordance with sustainable and service-oriented best business practices/models. It is based on optimization, flexibility, self-adaptability and learning, fault tolerance, and risk management. Complete manufacturing digitalization and digital factory are the key elements of Industry 4.0 Program. None In collaborative research, which we carry out in the field of quality control and manufacturing metrology at University of Belgrade, Faculty of Mechanical Engineering in Serbia and at Department of Mechanical Engineering, University of Texas, Austin in USA, three research areas are defined: (а) Digital manufacturing – towards Cloud Manufacturing Systems (as a basis for CPS), in which quality and metrology represent integral parts of process optimization based on Taguchi model, and (б) Cyber-Physical Quality Model (CPQM) – our approach, in which we have developed and tested intelligent model for prismatic parts inspection planning on CMM (Coordinate Measuring Machine). The third research area directs our efforts to the development of framework for Cyber-Physical Manufacturing Metrology Model (CPM 3 ). CPM 3 None framework will be based on integration of digital product metrology information through metrology features recognition, and generation of global/local inspection plan for free-form surfaces; we will illustrate our approach using turbine blade example. This paper will present recent results of our research on CPM 3 .",project-academic
10.1016/J.SFTR.2020.100023,2020-01-01,a,Elsevier,challenges opportunities and future directions of smart manufacturing a state of art review," Abstract None None Smart manufacturing is the technology utilizing the interconnected machines and tools for improving manufacturing performance and optimizing the energy and workforce required by the implementation of bigdata processing, artificial intelligence and advanced robotics technology and interconnectivity of them. This paper defines and discusses the smart manufacturing system and states it current implementation status and analyzes the gap between current manufacturing system and the predicted future smart manufacturing system, discusses the technologies associated with it and their contribution in smart manufacturing technology. Also, to realize this rapidly growing technology and cover its all dimensions a survey of the latest developments in this field and its impacts were analyzed and presented along with the challenges of implementation, opportunities and the future directions for smart manufacturing system.",project-academic
10.1145/3130859.3131439,2017-10-15,p,ACM,automated game testing with icarus intelligent completion of adventure riddles via unsupervised solving," With ICARUS, we introduce a framework for autonomous video game playing, testing, and bug reporting. We report on the design rationale, the practical implementation, and its use in game development industry projects. The underlying solving mechanic is based on discrete reinforcement learning in a dualistic fashion, encompassing volatile short-term memory as well as persistent long-term memory that spans across distinct game iterations. In combination with heuristics that reduce the search space and the possibility to employ pre-defined situation-dependent action choices, the system manages to traverse complete playthrough iterations in roughly the same amount of time that a professional game tester requires for a speedrun. The ICARUS project was developed at Daedalic Entertainment. The software can be used to generically run all adventure games built with the popular Visionaire Engine and is currently used for evaluating daily builds, for large-scale hardware compatibility and performance tests, as well as for semi-supervised quality assurance playthroughs. The supplementary video depicts real-time solving with active control and observation via a web control panel.",project-academic
10.12958/1817-3772-2019-3(57)-189-216,2019-01-01,a,State University Luhansk Taras Shevchenko National University,industry 4 0 the directions for attracting investment from the perspective of the interests of domestic producers," The modern world is changing rapidly, the convergence of globalization and localization processes, a combination of competition and cooperation, interaction and rivalry in the relations of economic agents are taking place. A new challenge for all countries of the world is becoming Industry 4.0, based on such important elements of the modern type of production as cyber-physical systems, big data, artificial intelligence, 3D printing, which can increase productivity, speed and flexibility of production, improve the quality of goods and, as a result, ensure industry competitiveness. For Ukraine, readiness for Industry 4.0 is an important aspect of its secure future. It is defined, that Ukraine has almost lost an industrial platform upon which an innovative economy can develop, and industrial regions have inherited its basic structural problems. The traditional production of Donbass also requires an innovative push, the current economic situation of which is defined as a disaster. It is proved that this catastrophe has social, economic, environmental and scientific-technical dimensions. The stages and system of measures for reloading the economy of Donbass and turning it from coal and metallurgical region into a region of a creative economy have been developed. It would be based on knowledge, new development models, new types of social relations, a new cultural paradigm. A vision of the strategic future of Donbass as a region of high technological culture, which should retain the role of the industrial center of Ukraine, is proposed. In doing so, the concept of the industry should be enriched with neo-industrial content based on the introduction of domestic and world achievements in science and technology. Special attention has been devoted to the problem of a high level of Ukraine’s illicit economy, which hampers the realization of structural and technological transformations in industrial regions and in the Donbass, in particular. It was evaluated the level of smartization of the domestic industry. It is established that Ukraine remains considerably behind the processes of smart industrialization that ensued in developed Western and Eastern countries because investment in machinery and equipment is not accompanied by adequate investment in creating a software environment for the smart industry. It is suggested to legislate on the ideology of industrial development at the state and regional level by developing and adopting the Law of Ukraine «On the modernization of industrial regions» which will create a favorable regulatory conditions for business entities and authorities in the implementation of structural and technological modernization of the economy and its industrial sector.",project-academic
,2021-03-02,a,,graph computing for financial crime and fraud detection trends challenges and outlook," The rise of digital payments has caused consequential changes in the financial crime landscape. As a result, traditional fraud detection approaches such as rule-based systems have largely become ineffective. AI and machine learning solutions using graph computing principles have gained significant interest in recent years. Graph-based techniques provide unique solution opportunities for financial crime detection. However, implementing such solutions at industrial-scale in real-time financial transaction processing systems has brought numerous application challenges to light. In this paper, we discuss the implementation difficulties current and next-generation graph solutions face. Furthermore, financial crime and digital payments trends indicate emerging challenges in the continued effectiveness of the detection techniques. We analyze the threat landscape and argue that it provides key insights for developing graph-based solutions.",project-academic
,2006-01-01,a,,a medical claim fraud abuse detection system based on data mining a case study in chile," This paper describes an effective medical claim fraud/abuse detection system based on data mining used by a Chilean private health insurance company. Fraud and abuse in medical claims have become a major concern within health insurance companies in Chile the last years due to the increasing losses in revenues. Processing medical claims is an exhausting manual task carried out by a few medical experts who have the responsibility of approving, modifying or rejecting the subsidies requested within a limited period from their reception. The proposed detection system uses one committee of multilayer perceptron neural networks (MLP) for each one of the entities involved in the fraud/abuse problem: medical claims, affiliates, medical professionals and employers. Results of the fraud detection system show a detection rate of approximately 75 fraudulent and abusive cases per month, making the detection 6.6 months earlier than without the system. The application of data mining to a real industrial problem through the implementation of an automatic fraud detection system changed the original non-standard medical claims checking process to a standardized process helping to fight against new, unusual and known fraudulent/abusive behaviors.",project-academic
10.1016/J.ASOC.2011.05.011,2011-12-01,p,Elsevier Science Publishers B. V.,credit risk evaluation using neural networks emotional versus conventional models," Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. Artificial neural networks (NNs) have been considered to be accurate tools for credit analysis among others in the credit industry. Lately, emotional neural networks (EmNNs) have been suggested and applied successfully for pattern recognition. In this paper we investigate the efficiency of EmNNs and compare their performance to conventional NNs when applied to credit risk evaluation. In total 12 neural networks; based equally on emotional and conventional neural models; are arbitrated under three learning schemes to classify whether a credit application is approved or declined. The learning schemes differ in the ratio of training-to-validation data used during training and testing the neural networks. The emotional and conventional neural models are trained using real world credit application cases from the Australian credit approval datasets which has 690 cases; each case with 14 numerical attributes; based on which an application is accepted or rejected. The performance of the 12 neural networks will be evaluated using certain criteria. Experimental results suggest that both emotional and conventional neural models can be used effectively for credit risk evaluations, however the emotional models outperform their conventional counterparts in decision making speed and accuracy, thus, making them ideal for implementation in fast automatic processing of credit applications.",project-academic
10.1109/TIFS.2019.2923577,2020-01-01,a,IEEE,anomaly detection in real time multi threaded processes using hardware performance counters," We propose a novel methodology for real-time monitoring of software running on embedded processors in cyber-physical systems (CPS). The approach uses real-time monitoring of hardware performance counters (HPC) and applies to multi-threaded and interrupt-driven processes typical in programmable logic controller (PLC) implementation of real-time controllers. The methodology uses a black-box approach to profile the target process using HPCs. The time series of HPC measurements over a time window under known-good operating conditions is used to train a machine learning classifier. At run-time, this trained classifier classifies the time series of HPC measurements as baseline (i.e., probabilistically corresponding to a model learned from the training data) or anomalous. The baseline versus anomalous labels over successive time windows offer robustness against the stochastic variability of code execution on the embedded processor and detect code modifications. We demonstrate effectiveness of the approach on an embedded PLC in a hardware-in-the-loop (HITL) testbed emulating a benchmark industrial process. In addition, to illustrate the scalability of the approach, we also apply the methodology to a second PLC platform running a representative embedded control process.",project-academic
10.1016/J.ORALONCOLOGY.2018.10.026,2018-12-01,a,Oral Oncol,the application of artificial intelligence in the imrt planning process for head and neck cancer," Artificial intelligence (AI) is beginning to transform IMRT treatment planning for head and neck patients. However, the complexity and novelty of AI algorithms make them susceptible to misuse by researchers and clinicians. Understanding nuances of new technologies could serve to mitigate potential clinical implementation pitfalls. This article is intended to facilitate integration of AI into the radiotherapy clinic by providing an overview of AI algorithms, including support vector machines (SVMs), random forests (RF), gradient boosting (GB), and several variations of deep learning. This document describes current AI algorithms that have been applied to head and neck IMRT planning and identifies rapidly growing branches of AI in industry that have potential applications to head and neck cancer patients receiving IMRT. AI algorithms have great clinical potential if used correctly but can also cause harm if misused, so it is important to raise the level of AI competence within radiation oncology so that the benefits can be realized in a controlled and safe manner.",project-academic
,2008-03-01,a,Institute of Electrical and Electronics Engineers,robotics software the future should be open," This column introduces a number of problem claims about the pitiful state of practice in software for robotics (and for all kinds of engineering domains in general). It also presents solution claims, whose realization can lead to a long-term, macroeconomically optimal solution, both for the industry and academia. Key problems in robotics software in the industrial and the academic practice are a chronic lack of standardization, interoperability and reuse of software libraries, both proprietary and open source. For example, we still have not standardized the Kalman filter or particle filter that everyone can and wants to use, and the same holds for many other mature robotics software components such as kinematics and dynamics, control laws, or planning algorithms. As a result, thousands of (Ph.D.) person months are lost worldwide every year in reimplementing these things for the zillionth time, without any new contribution to software reuse. This pitiful state of the practice is not unique to robotics, and only a few engineering domains do it right: numerical linear algebra (starting many decades ago already); the World Wide Web [with (X)HTML, cascading style sheets (CSS), scalable vector graphics (SVG), and other W3C standards as the fundamental enablers]; the Java middleware ecosystem [XML processing, open services gateway initiative (OSGi, now obsolete), Eclipse, mobile phone frameworks, etc.]; and tools around the Object Management Group (OMG) standards of UML, SysML, and modeldriven architecture. These examples are not tied to specific applications (this is not a coincidence but a very wise design decision about modularity and decoupling!), and they all have healthy commercial and open-source offerings, with real and rapid innovation taking place in both software development models. Every section in this article focuses on one of the fundamental issues that has led to the retarded state of software in robotics and suggests a concrete solution. Most neighboring scientific and technologic domains (computer vision, systems and control, cognitive science, artificial intelligence, etc.) suffer from exactly the same problems, such that cooperation with those domains can lead to faster implementation of the presented solutions.",project-academic
10.1109/IPDPSW.2017.44,2017-05-01,p,Institute of Electrical and Electronics Engineers Inc.,a pipelined and scalable dataflow implementation of convolutional neural networks on fpga," Convolutional Neural Network (CNN) is a deep learning algorithm extended from Artificial Neural Network (ANN) and widely used for image classification and recognition, thanks to its invariance to distortions. The recent rapid growth of applications based on deep learning algorithms, especially in the context of Big Data analytics, has dramatically improved both industrial and academic research and exploration of optimized implementations of CNNs on accelerators such as GPUs, FPGAs and ASICs, as general purpose processors can hardly meet the ever increasing performance and energy-efficiency requirements. FPGAs in particular are one of the most attractive alternative, as they allow the exploitation of the implicit parallelism of the algorithm and the acceleration of the different layers of a CNN with custom optimizations, while retaining extreme flexibility thanks to their reconfigurability. In this work, we propose a methodology to implement CNNs on FPGAs in a modular, scalable way. This is done by exploiting the dataflow pattern of convolutions, using an approach derived from previous work on the acceleration of Iterative Stencil Loops (ISLs), a computational pattern that shares some characteristics with convolutions. Furthermore, this approach allows the imple- mentation of a high-level pipeline between the different network layers, resulting in an increase of the overall performance when the CNN is employed to process batches of multiple images, as it would happen in real-life scenarios.",project-academic
10.2139/SSRN.3653544,2020-10-07,a,,regulating transformative technology in the quantum age intellectual property standardization sustainable innovation," The behavior of nature at the smallest scale can be strange and counterintuitive. In addition to unique physical characteristics, quantum technology has many legal aspects. In this article, we first explain what quantum technology entails. Next, we discuss implementation and areas of application, including quantum computing, quantum sensing and the quantum internet. Through an interdisciplinary lens, we then focus on intellectual property (‘IP’), standardization, ethical, legal & social aspects (‘ELSA’) as well as horizontal & industry-specific regulation of this transformative technology.

The Quantum Age raises many legal questions. For example, which existing legislation applies to quantum technology? What types of IP rights can be vested in the components of a scalable quantum computer? Are there sufficient market-set innovation incentives for the development and dissemination of quantum software and hardware structures? Or is there a need for open source ecosystems, enrichment of the public domain and even democratization of quantum technology? Should we create global quantum safety, security and interoperability standards and make them mandatory in each area of application? In what way can quantum technology enhance artificial intelligence (‘AI’) that is legal, ethical and technically robust? 

The article argues that the pervasiveness of quantum technology asks for a holistic view on a regulatory framework, that balances the interests of stakeholders and that of society at large. It demands for an agile legislative system that can adapt quickly to changing circumstances and societal needs. 

How can policy makers realize these objectives and regulate quantum computing, quantum sensing and the quantum internet in a socially responsible manner? Regulation that addresses risks in a proportional manner, whilst optimizing the benefits of this cutting edge technology? Without hindering sustainable innovation, including the apportionment of rights, responsibilities and duties of care? What are the effects of standardization and certification on innovation, intellectual property, competition and market-entrance of quantum-startups?

Moreover, which culturally sensitive ethical issues play a role in these regulations? Would it be a good first step to link the governance of quantum & AI hybrids to the Trustworthy AI principles? Do quantum’s different physical properties call for additional core rules? Is it wise to embed our democratic values into the architecture of quantum systems, by way of Trustworthy Quantum Technology by Design? The article explores possible answers to these tantalizing questions.

Particles and energy at the subatomic level do not follow the same rules as the objects we can detect around us in our everyday lives. In addition to universal, overarching guiding principles of Trustworthy & Responsible Quantum Technology that are in line with the unique physical characteristics of quantum mechanics, the article advocates a vertical, differentiated industry-specific legislative approach regarding innovation incentives (based on the innovation policy pluralism toolkit), externalities and risks (based on the pyramid of criticality, which should include a definition of high-risk quantum technology applications).

The article demonstrates that strategically using a mixture of IP rights to maximize the value of the IP portfolio of the quantum computer’s owner, potentially leads to IP protection in perpetuity. Overlapping IP protection regimes can result in unlimited duration of global exclusive exploitation rights for first movers, being a handful of universities and large corporations. The ensuing IP overprotection in the field of quantum computing leads to an unwanted concentration of market power. Overprotection of information causes market barriers and hinders both healthy competition and industry-specific innovation. In this particular case it slows down progress in an important application area of quantum technology, namely quantum computing. 

In general, our current intellectual property framework is not written with quantum technology in mind. Intellectual property should be an exception -limited in time and scope- to the rule that information goods can be used for the common good without restraint. Intellectual property cannot incentivize creation, prevent market failure, fix winner-takes-all effects, eliminate free riding and prohibit predatory market behavior at the same time. To encourage fair competition and correct market skewness, antitrust law is the instrument of choice. 

The article proposes a solution tailored to the exponential pace of innovation in The Quantum Age, by introducing shorter IP protection durations of 3 to 10 years for Quantum and AI infused creations and inventions. These shorter terms could be made applicable to both the software and the hardware side of things. Clarity about the recommended limited durations of exclusive rights -in combination with compulsory licenses or fixed prized statutory licenses- encourages legal certainty, knowledge dissemination and follow on innovation within the quantum domain. In this light, policy makers should build an innovation architecture that mixes freedom (e.g. access, public domain) and control (e.g. incentive & reward mechanisms).

Regulating transformative technology in The Quantum Age requires synergetic relationships between legislation, standardization, certification and government institutions. The article suggests that quantum products and services made within the EU or elsewhere in the world should adhere to EU safety and security benchmarks, including not limited to the high technical, legal and ethical standards that reflect Trustworthy quantum technology core values, before they qualify for a CE-marking and are eligible to enter the European markets.

The article concludes that anticipating spectacular advancements in quantum technology, the time is now ripe for governments, research institutions and the markets to prepare regulatory and intellectual property strategies that strike the right balance between safeguarding our fundamental rights & freedoms, our democratic norms & standards, and pursued policy goals that include rapid technology transfer, the free flow of information and the creation of a thriving global quantum ecosystem, whilst encouraging healthy competition and incentivizing sustainable innovation.",project-academic
10.1109/95.296402,1994-06-01,a,IEEE,artificial neural networks in manufacturing concepts applications and perspectives," New approaches and techniques are continuously and rapidly introduced and adopted in today's manufacturing environment. Recently, there has been an explosion of interest in applying artificial neural networks to manufacturing. Artificial neural networks have several advantages that are desired in manufacturing practice, including learning and adapting ability, parallel distributed computation, robustness, etc. There is an expectation that neural network techniques can lead to the realization of truly intelligent manufacturing systems. This paper introduces the basic concepts of neural networks and reviews the current application of neural networks in manufacturing. The problems with neural networks are also identified and some possible solutions are suggested. The aim of the authors is to provide useful guidelines and references for the research and implementation of artificial neural networks in the field of manufacturing. >",project-academic
10.1007/S12273-020-0711-5,2021-02-01,a,Tsinghua University Press,development of an ann based building energy model for information poor buildings using transfer learning," Accurate building energy prediction is vital to develop optimal control strategies to enhance building energy efficiency and energy flexibility. In recent years, the data-driven approach based on machine learning algorithms has been widely adopted for building energy prediction due to the availability of massive data in building automation systems (BASs), which automatically collect and store real-time building operational data. For new buildings and most existing buildings without installing advanced BASs, there is a lack of sufficient data to train data-driven predictive models. Transfer learning is a promising method to develop accurate and reliable data-driven building energy prediction models with limited training data by taking advantage of the rich data/knowledge obtained from other buildings. Few studies focused on the influences of source building datasets, pre-training data volume, and training data volume on the performance of the transfer learning method. The present study aims to develop a transfer learning-based ANN model for one-hour ahead building energy prediction to fill this research gap. Around 400 non-residential buildings’ data from the open-source Building Genome Project are used to test the proposed method. Extensive analysis demonstrates that transfer learning can effectively improve the accuracy of BPNN-based building energy models for information-poor buildings with very limited training data. The most influential building features which influence the effectiveness of transfer learning are found to be building usage and industry. The research outcomes can provide guidance for implementation of transfer learning, especially in selecting appropriate source buildings and datasets for developing accurate building energy prediction models.",project-academic
10.1109/81.747195,1999-02-01,a,IEEE,reaction diffusion cnn algorithms to generate and control artificial locomotion," In this paper a physiological-behavioral approach to neural processing is used to realize artificial locomotion in mechatronic devices. The task has been realized by using a particular model of reaction-diffusion cellular neural networks (RD-CNN's) generating autowave fronts as well as Turing patterns. Moreover a programmable hardware cellular neural network structure is presented in order to model, generate, and control in real time some biorobots. The programmable hardware implementation gives the possibility of generating locomotion in real time and also to control the transition among several types of locomotion, with particular attention to hexapodes. The approach proposed allows not only the design of walking robots, but also the ability to build structures able to efficiently solve typical problems in industrial automation, such as online routing of objects moved on conveyor belts.",project-academic
,2021-04-14,a,,design of an efficient ease of use and affordable artificial intelligence based nucleic acid amplification diagnosis technology for tuberculosis and multi drug resistant tuberculosis," Current technologies that facilitate diagnosis for simultaneous detection of Mycobacterium tuberculosis and its resistance to first-line anti-tuberculosis drugs (Isoniazid and Rifampicim) are designed for lab-based settings and are unaffordable for large scale testing implementations. The suitability of a TB diagnosis instrument, generally required in low-resource settings, to be implementable in point-of-care last mile public health centres depends on manufacturing cost, ease-of-use, automation and portability. This paper discusses a portable, low-cost, machine learning automated Nucleic acid amplification testing (NAAT) device that employs the use of a smartphone-based fluorescence detection using novel image processing and chromaticity detection algorithms. To test the instrument, real time polymerase chain reaction (qPCR) experiment on cDNA dilution spanning over two concentrations (40 ng/uL and 200 ng/uL) was performed and sensitive detection of multiplexed positive control assay was verified.",project-academic
10.1007/S00170-015-7807-6,2016-06-01,a,Springer London,adaptive control optimization in micro milling of hardened steels evaluation of optimization approaches," Nowadays, the miniaturization of many consumer products is extending the use of micro-milling operations with high-quality requirements. However, the impacts of cutting-tool wear on part dimensions, form and surface integrity are not negligible and part quality assurance for a minimum production cost is a challenging task. In fact, industrial practices usually set conservative cutting parameters and early cutting replacement policies in order to minimize the impact of cutting-tool wear on part quality. Although these practices may ensure part integrity, the production cost is far away to be minimized, especially in highly tool-consuming operations like mold and die micro-manufacturing. In this paper, an adaptive control optimization (ACO) system is proposed to estimate cutting-tool wear in terms of part quality and adapt the cutting conditions accordingly in order to minimize the production cost, ensuring quality specifications in hardened steel micro-parts. The ACO system is based on: (1) a monitoring sensor system composed of a dynamometer, (2) an estimation module with Artificial Neural Networks models, (3) an optimization module with evolutionary optimization algorithms, and (4) a CNC interface module. In order to operate in a nearly real-time basis and facilitate the implementation of the ACO system, different evolutionary optimization algorithms are evaluated such as particle swarm optimization (PSO), genetic algorithms (GA), and simulated annealing (SA) in terms of accuracy, precision, and robustness. The results for a given micro-milling operation showed that PSO algorithm performs better than GA and SA algorithms under computing time constraints. Furthermore, the implementation of the final ACO system reported a decrease in the production cost of 12.3 and 29 % in comparison with conservative and high-production strategies, respectively.",project-academic
10.1007/978-3-030-20704-5_15,2019-03-01,a,Elsevier,collaborative prognostics in social asset networks," With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the field of Asset Management has moved from fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must find a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, defined as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they are not aware of their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares fit of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet.",project-academic
10.1145/3218603.3218611,2018-07-23,p,ACM,dynamic bit width reconfiguration for energy efficient deep learning hardware," Deep learning models have reached state of the art performance in many machine learning tasks. Benefits in terms of energy, bandwidth, latency, etc., can be obtained by evaluating these models directly within Internet of Things end nodes, rather than in the cloud. This calls for implementations of deep learning tasks that can run in resource limited environments with low energy footprints. Research and industry have recently investigated these aspects, coming up with specialized hardware accelerators for low power deep learning. One effective technique adopted in these devices consists in reducing the bit-width of calculations, exploiting the error resilience of deep learning. However, bit-widths are tipically set statically for a given model, regardless of input data. Unless models are retrained, this solution invariably sacrifices accuracy for energy efficiency. In this paper, we propose a new approach for implementing input-dependant dynamic bit-width reconfiguration in deep learning accelerators. Our method is based on a fully automatic characterization phase, and can be applied to popular models without retraining. Using the energy data from a real deep learning accelerator chip, we show that 50% energy reduction can be achieved with respect to a static bit-width selection, with less than 1% accuracy loss.",project-academic
10.1007/S00253-019-09796-X,2019-05-02,a,Springer,accelerating the implementation of biocatalysis in industry," Despite enormous progress in protein engineering, complemented by bioprocess engineering, the revolution awaiting the application of biocatalysis in the fine chemical industry has still not been fully realized. In order to achieve that, further research is required on several topics, including (1) rapid methods for protein engineering using machine learning, (2) mathematical modelling of multi-enzyme cascade processes, (3) process standardization, (4) continuous process technology, (5) methods to identify improvements required to achieve industrial implementation, (6) downstream processing, (7) enzyme stability modelling and prediction, as well as (8) new reactor technology. In this brief mini-review, the status of each of these topics will be briefly discussed.",project-academic
10.1016/J.YMSSP.2012.01.021,2012-07-01,a,Academic Press,fpga based entropy neural processor for online detection of multiple combined faults on induction motors," Abstract None None For industry, a faulty induction motor signifies production reduction and cost increase. Real-world induction motors can have one or more faults present at the same time that can mislead to a wrong decision about its operational condition. The detection of multiple combined faults is a demanding task, difficult to accomplish even with computing intensive techniques. This work introduces information entropy and artificial neural networks for detecting multiple combined faults by analyzing the 3-axis startup vibration signals of the rotating machine. A field programmable gate array implementation is developed for automatic online detection of single and combined faults in real time.",project-academic
10.1016/J.ENGAPPAI.2004.03.001,2004-04-01,a,Pergamon,artificial neural networks and neuro fuzzy systems for modelling and controlling real systems a comparative study," Abstract None None This article presents a comparison of artificial neural networks and neuro-fuzzy systems applied for modelling and controlling a real system. The main objective is to model and control the temperature inside of a kiln for the ceramic industry. The details of all system components are described. The steps taken to arrive at the direct and inverse models using the two architectures: adaptive neuro fuzzy inference system and feedforward neural networks are described and compared. Finally, real-time control results using internal model control strategy are presented. None Using available None Matlab None software for both algorithms, the objective is to show the implementation steps for modelling and controlling a real system. Finally, the performances of the two solutions were compared through different parameters for a specific real didactic case.",project-academic
10.1007/S00521-007-0150-6,2007-09-19,a,Springer-Verlag,constrained multi variable generalized predictive control using a dual neural network," Multi-variable generalized predictive control algorithm has obtained great success in process industries. However, it suffers from a high computational cost because the multi-stage optimization approach in the algorithm is time-consuming when constraints of the control system are considered. In this paper, a dual neural network is employed to deal with the multi-stage optimization problem, and bounded constraints on the input and output signals of the control system are taken into account. The dual neural network has many favorable features such as simple structure, rapid execution, and easy implementation. Therefore, the computation efficiency, in comparison with the consecutive executions of numerical algorithms on digital computers, is increased dramatically. In addition, the dual network model can yield the exact optimum values of future control signals while many other neural networks only obtain the approximate optimal solutions. Hence the multi-variable generalized predictive control algorithm based on the dual neural network is suitable for industrial applications with the real-time computation requirement. Simulation examples are given to demonstrate the efficiency of the proposed approach.",project-academic
,2007-01-01,p,Springer,constrained multi variable generalized predictive control using a dual neural network," Multi-variable generalized predictive control algorithm has obtained great success in process industries. However, it suffers from a high computational cost because the multi-stage optimization approach in the algorithm is time-consuming when constraints of the control system are considered. In this paper, a dual neural network is employed to deal with the multi-stage optimization problem, and bounded constraints on the input and output signals of the control system are taken into account. The dual neural network has many favorable features such as simple structure, rapid execution, and easy implementation. Therefore, the computation efficiency, in comparison with the consecutive executions of numerical algorithms on digital computers, is increased dramatically. In addition, the dual network model can yield the exact optimum values of future control signals while many other neural networks only obtain the approximate optimal solutions. Hence the multi-variable generalized predictive control algorithm based on the dual neural network is suitable for industrial applications with the real-time computation requirement. Simulation examples are given to demonstrate the efficiency of the proposed approach.",project-academic
10.1023/A:1008818817588,1998-05-01,a,Kluwer Academic Publishers,a review of neural networks for statistical process control," This paper aims to take stock of the recent research literature on application of Neural Networks (NNs) to the analysis of Shewhart's traditional Statistical Process Control (SPC) charts. First appearing in the late 1980s, most of the literature claims success, great or small, in applying NNs for SPC (NNSPC). These efforts are viewed in this paper as useful steps towards automatic on-line SPC for continuous improvement of quality and for real-time manufacturing process control. A standard NN approach that can parallel the universality of the traditional Shewhart charts has not yet been developed or adopted, although knowledge in this area is rapidly increasing. This paper attempts to provide a practical insight into the issues involved in application of NNs to SPC with the hope of advancing the use of NN techniques and facilitating their adoption as a new and useful aspect of SPC. First, a brief review of control chart analysis prior to the introduction of NN technology is presented. This is followed by an examination and classification of the NNSPC existing literature. Next, an extensive discussion of implementation issues with reference to significant research papers is presented. Finally, after summarising the survey, a set of general guidelines for future applications of NNs to SPC is outlined.",project-academic
10.1007/S00521-020-05515-0,2021-05-01,a,Springer Science and Business Media LLC,adaptive control of manipulator based on neural network," With the development of economic science and technology, the development of computer vision has undergone rapid changes, and various products relying on computer vision are also more and more, such as smart home, robot technology, and so on. At present, robot technology has become a very important part of the development of human science and technology, and in the field of industrial robots, the most rapid development is the robot with robot arm adaptive motion. It is very necessary to study the adaptive motion control of the manipulator based on machine learning. The robot with the adaptive motion of the manipulator can carry out logistics express sorting, operate in the doors and windows outside the building, and pick fruits in the orchard, which can ensure the effective implementation of hard work. Therefore, this paper proposes a mechanical adaptive control method based on a neural network. According to the motion model of the manipulator, the RBF neural network model is used to judge the stability of the system according to the Lyapunov function. The related algorithms of machine learning and multi-degree of freedom manipulator are studied and improved. The RBF neural network model approximates the unknown function infinitely and then establishes the complex motion model. Aiming at the adaptive neural network of a manipulator, a network adaptive terminal control method is proposed. Firstly, a stable manipulator motion system is designed by using a neural network, and then the terminal synovial controller is designed by using backstepping control technology. The stability of the method is proved by using the approximation virtual control technology of the neural network. The adaptive control is realized by using the learning and self-adaptability of the neural network; thus, the stability analysis of the closed-loop system is realized.",project-academic
,2019-08-30,,,edge computing intelligent adapter for industrial manufacturing and implementation method," The invention discloses an edge computing intelligent gateway for industrial manufacturing and an implementation method. The gateway comprises an industrial control protocol analysis module, an edge calculation module and an output module, the industrial control protocol analysis module is connected to a plurality of communication protocol acquisition devices. The communication protocol data is parsed and stored in the edge calculation module; the edge calculation module obtains prediction information through a lightweight machine learning algorithm submodule; the data processing sub-module performs data processing according to a processing mode preset by the configuration sub-module; and equipment is diagnosed and predictively maintained by combining the prediction information, the processed data is stored in a database, an output module provides OPC UA server service, the data of a processing data area in the database is read, man-machine interaction operation is carried out, and meanwhile data transmission is reported. The method is applicable to access of various industrial control protocol devices, mutual recognition and interconnection of multi-dimensional heterogeneous data are realized, data can be processed at the edge of a network, and a production-oriented intelligent optimization management and control function is realized.",project-academic
10.1007/S40137-021-00297-3,2021-01-01,a,Springer US,the age of artificial intelligence use of digital technology in clinical nutrition," Computing advances over the decades have catalyzed the pervasive integration of digital technology in the medical industry, now followed by similar applications for clinical nutrition. This review discusses the implementation of such technologies for nutrition, ranging from the use of mobile apps and wearable technologies to the development of decision support tools for parenteral nutrition and use of telehealth for remote assessment of nutrition. Mobile applications and wearable technologies have provided opportunities for real-time collection of granular nutrition-related data. Machine learning has allowed for more complex analyses of the increasing volume of data collected. The combination of these tools has also translated into practical clinical applications, such as decision support tools, risk prediction, and diet optimization. The state of digital technology for clinical nutrition is still young, although there is much promise for growth and disruption in the future.",project-academic
,2018-10-12,,,stock prediction method which combines news corpus and stock market transaction data," The invention provides a stock prediction method which combines news corpus and stock market transaction data. The method makes full use of a large amount of corpus information of a network and breaksthe traditional boundary of a single analysis data source. Through a deep learning model, the stock market news corpus can be analyzed in batches, and the importance of corpus for prediction can be judged automatically, thus the automation and precision of network information analysis are realized. Modeling is performed on news corpus and transaction data through deep learning, and the relationship between different data is comprehensively analyzed according to different information from many aspects. The influence of stock market information on the stock price, the persistence of the stock market information and the investor's psychological factor are grasped, so that the stock market forecast accuracy is further improved; and the word vector, a GRU neural network, an attention mechanismand other in-depth learning cutting-edge technology are used, so that the implementation of science into the industry is realized, and scientific and technological innovation is achieved.",project-academic
10.1007/978-3-030-21290-2_39,2019-06-03,p,"Springer, Cham",solution patterns for machine learning," Despite the hype around machine learning (ML), many organizations are struggling to derive business value from ML capabilities. Design patterns have long been used in software engineering to enhance design effectiveness and to speed up the development process. The contribution of this paper is two-fold. First, it introduces solution patterns as an explicit way of representing generic and well-proven ML designs for commonly-known and recurring business analytics problems. Second, it reports on the feasibility, expressiveness, and usefulness of solution patterns for ML, in collaboration with an industry partner. It provides a prototype architecture for supporting the use of solution patterns in real world scenarios. It presents a proof-of-concept implementation of the architecture and illustrates its feasibility. Findings from the collaboration suggest that solution patterns can have a positive impact on ML design and development efforts.",project-academic
10.1109/IPDPS.2018.00068,2018-05-21,p,IEEE,do developers understand ieee floating point," Floating point arithmetic, as specified in the IEEE standard, is used extensively in programs for science and engineering. This use is expanding rapidly into other domains, for example with the growing application of machine learning everywhere. While floating point arithmetic often appears to be arithmetic using real numbers, or at least numbers in scientific notation, it actually has a wide range of gotchas. Compiler and hardware implementations of floating point inject additional surprises. This complexity is only increasing as different levels of precision are becoming more common and there are even proposals to automatically reduce program precision (reducing power/energy and increasing performance) when results are deemed """"good enough.'"""" Are software developers who depend on floating point aware of these issues? Do they understand how floating point can bite them? To find out, we conducted an anonymous study of different groups from academia, national labs, and industry. The participants in our sample did only slightly better than chance in correctly identifying key unusual behaviors of the floating point standard, and poorly understood which compiler and architectural optimizations were non-standard. These surprising results and others strongly suggest caution in the face of the expanding complexity and use of floating point arithmetic.",project-academic
10.3389/FROBT.2018.00025,2018-01-01,a,Frontiers Media SA,exploiting three dimensional gaze tracking for action recognition during bimanual manipulation to enhance human robot collaboration," Human-robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze-object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The ""gaze object sequence"" was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments.",project-academic
10.1109/INFOCOM.2019.8737649,2019-04-01,p,IEEE,reles a neural adaptive multipath scheduler based on deep reinforcement learning," The Multipath TCP (MPTCP) protocol, featured by its ability of capacity aggregation across multiple links and connectivity maintenance against single-path failure, has been attracting increasing attention from the industry and academy. Multipath packet scheduling is a unique and fundamental mechanism for the design and implementation of MPTCP, which is responsible for distributing the traffic over multiple subflows. The existing multipath schedulers are facing the challenges of network heterogeneities, comprehensive QoS goals, and dynamic environments, etc. To address these challenges, we propose ReLeS, a Reinforcement Learning based Scheduler for MPTCP. ReLeS uses modern deep reinforcement learning (DRL) techniques to learn a neural network to generate the control policy for packet scheduling. It adopts a comprehensive reward function that takes diverse QoS characteristics into consideration to optimize packet scheduling. To support real-time scheduling, we propose an asynchronous training algorithm that enables parallel execution of packet scheduling, data collecting, and neural network training. We implement ReLeS in the Linux kernel and evaluate it over both emulated and real network conditions. Extensive experiments show that ReLeS significantly outperforms the state-of-the-art schedulers.",project-academic
10.1016/J.KNOSYS.2015.04.002,2015-08-01,a,Elsevier,transforming expertise into knowledge based engineering tools," Research on Engineering Knowledge Management (EKM) has identified challenges with the systematic source of engineering knowledge for the design process optimisation. In this context, Knowledge-Based Engineering (KBE) is acknowledged as a key area within the EKM field and designated by the research community as a potential solution to carry out the effective capture and reuse of expert knowledge. However, papers on KBE for knowledge sourcing are not abundant in the literature and they are also dispersed. From this perspective, this research is an effort to further consolidate the learning gained on industrial practice on how engineering knowledge can be effectively sourced. This is achieved by realising a research survey, where using the resulting insights KBE practice reaching aerospace engineering offices shall be more efficiently delivered through fast and accurate knowledge extraction and encoding into usable methods and tools. The research findings provided by literature survey confirmed the existence of a research gap on knowledge sourcing; and more precisely they underlined the need for an extended KBE development process which integrates Artificial Intelligence (AI) tools and expert intervention to systematically manage the knowledge (using the KM methods and tools) efficiently captured and modelled (employing AI algorithms and expert involvement). Therefore, this paper concludes that there is a need for further research on the knowledge sourcing KBE aspect and presents the integration of KBE systems and AI implementations as a potential solution to develop the extended KBE development process requested by the industry.",project-academic
10.2139/SSRN.3840222,2021-04-26,a,,artificial intelligence in local government enabling artificial intelligence for good governance in uk local authorities," Local governments face increased challenges providing services to their communities, especially in light of austerity, shifting central government policies that impact local responsibilities, changing demographics, and diverse resident needs. UK local authorities are exploring the use of artificial intelligence (AI) to fully or partially automate tasks or support their frontline workers to deliver services more efficiently and effectively. While there have been a number of successful projects related to back-office automation, predictive analytics for decision support, or the use of chatbots for interactions with residents, little is known about the practical challenges that local authorities face in making these projects realities. This briefing note synthesizes academic, grey, and journalistic literature to identify the key practical challenges that local authorities face when collaborating with industry or striking out on their own.

Analysis based on this synthesis of the literature led to a set of findings indicating that local authorities are faced with three key challenges and three key enablers. In terms of challenges, local authorities need to both get their data in order and clearly define problems before seeking information technology (IT) solutions. The third challenge is when suppliers lack contextual knowledge about the local authority, its processes, its residents, and the way it carries out its services, which may require local authorities to take products that are not fit for purpose and modify them to align with their work. In terms of the three enablers, local authorities benefit from in-house capacity, opportunities for collaboration, and project transparency.

The findings suggest that some foundational governance arrangements need to be in place both locally and nationally before AI technologies can realise benefits for good governance. More specifically, this briefing note proposes that the following measures are necessary for the implementation of artificial intelligence for good governance in the UK:

▪ Minimum mandatory data standards and dedicated resources for the maintenance of data quality.

▪ Minimum mandatory guidance for problem definition and project progress monitoring.

▪ Minimum mandatory supplier standards and flexible procurement to avoid lock-in and align projects with local context.

▪ Dedicated resources to ensure that local authorities can be intelligent consumers and capable developers of AI.

▪ A formal mechanism for collaboration across all local authorities and with the third sector (e.g., universities and non-profit organisations).

▪ A platform to compile all relevant information about information technology projects in local authorities.",project-academic
10.1007/978-3-030-34387-3_64,2019-06-28,a,"Springer, Cham",research on digital twin technology for production line design and simulation," With the rapid development of big data, artificial intelligence and internet of things, digital twin technology becomes a new research hotspot in the field of intelligent manufacturing. In this paper, the digital twin technology for production line design and simulation is studied. Emphasis is laid on the building and fusion of production line model, virtual-real mapping and real-time interaction technology and virtual production line simulation and verification technology. The research content of this paper provides theoretical and technical reference for the application of digital twins in the design and implementation of manufacturing production line.",project-academic
10.1117/12.504838,2004-01-26,p,International Society for Optics and Photonics,neural networks for led color control," The design and implementation of an architectural dimming control for multicolor LED-based lighting fixtures is complicated by the need to maintain a consistent color balance under a wide variety of operating conditions. Factors to consider include nonlinear relationships between luminous flux intensity and drive current, junction temperature dependencies, LED manufacturing tolerances and binning parameters, device aging characteristics, variations in color sensor spectral responsitivities, and the approximations introduced by linear color space models. In this paper we formulate this problem as a nonlinear multidimensional function, where maintaining a consistent color balance is equivalent to determining the hyperplane representing constant chromaticity. To be useful for an architectural dimming control design, this determination must be made in real time as the lighting fixture intensity is adjusted. Further, the LED drive current must be continuously adjusted in response to color sensor inputs to maintain constant chromaticity for a given intensity setting. Neural networks are known to be universal approximators capable of representing any continuously differentiable bounded function. We therefore use a radial basis function neural network to represent the multidimensional function and provide the feedback signals needed to maintain constant chromaticity. The network can be trained on the factory floor using individual device measurements such as spectral radiant intensity and color sensor characteristics. This provides a flexible solution that is mostly independent of LED manufacturing tolerances and binning parameters.",project-academic
,2004-01-01,a,Society of Photo-Optical Instrumentation Engineers,neural networks for led color control," The design and implementation of an architectural dimming control for multicolor LED-based lighting fixtures is complicated by the need to maintain a consistent color balance under a wide variety of operating conditions. Factors to consider include nonlinear relationships between luminous flux intensity and drive current, junction temperature dependencies, LED manufacturing tolerances and binning parameters, device aging characteristics, variations in color sensor spectral responsivities, and the approximations introduced by linear color space models. In this paper we formulate this problem as a nonlinear multidimensional function, where maintaining a consistent color balance is equivalent to determining the hyperplane representing constant chromaticity. To be useful for an architectural dimming control design, this determination must be made in real time as the lighting fixture intensity is adjusted. Further, the LED drive current must be continuously adjusted in response to color sensor inputs to maintain constant chromaticity for a given intensity setting. Neural networks are known to be universal approximators capable of representing any continuously differentiable bounded function. We therefore use a radial basis function neural network to represent the multidimensional function and provide the feedback signals needed to maintain constant chromaticity. The network can be trained on the factory floor using individual device measurements such as spectral radiant intensity and color sensor characteristics. This provides a flexible solution that is mostly independent of LED manufacturing tolerances and binning parameters.",project-academic
10.1016/J.PROCIR.2019.03.212,2019-01-01,a,Elsevier,contribution to the development of a digital twin based on product lifecycle to support the manufacturing process," Abstract None None The current manufacture challenges are closely linked to the aim of digitalizing the product, the process and the means of production. In such aspects, information about the production processes is available in real-time, allowing managers to act on digital models and, through them, apply decisions in real systems. Thus, having a mirror model or a Digital Twin enables real-time absorption, simulation and implementation of manufacturing variations from the real environment, allowing faster detection of physical problems, and faster production response. The Digital Twin is a virtual representation of the physical system, which is equipped with sensors and actuators and feed the digital system, where the monitoring of data and simulation of variations, for instance, take place. From the synchronized interactions of both components, it is possible to deliver the mentioned faster production responses. Brazilian and German universities joined efforts to develop a Digital Twin based on product lifecycle to support the Manufacturing Process to address these challenges. The proposed Digital Twin seeks to integrate the product twin and the twin of its development process. It shall represent the manufacturing process, enabling the monitoring and optimization of the real production process. The Digital Twin itself is addressed as a product inside the production system and, therefore, its development process will follow the product lifecycle perspective, from the conception and planning to its implementation and usage. The Digital Twin will be further improved with the introduction of Artificial Intelligence tools, characterizing a Smart Digital Twin of the Manufacturing Process. Thus, this paper aims to present the concepts of a research project that is being developed in a joint Brazilian-German Cooperative Research.",project-academic
10.1109/PHM-PARIS.2019.00061,2019-05-02,p,IEEE,convolutional neural network based rolling element bearing fault diagnosis for naturally occurring and progressing defects using time frequency domain features," Convolutional Neural Networks (CNN) are becoming increasingly popular for bearing fault diagnosis due to their ability to automatically capture the sensitive fault information without the need for expert knowledge. Most of these applications are developed considering vibration data from artificially induced faults. However, bearing failure in real-life can show huge damage variations even within a single category of failure which artificially induced failures are unable to represent. Thus, in this paper, the performance of classical CNN is evaluated on bearings with naturally occurring and progressing defects from the Paderborn University Dataset. A three-class (Healthy, Inner Race Fault and Outer Race Fault) classification problem is solved considering five bearing conditions within each class. These conditions vary in terms of bearing operating hours, damage mode, damage repetition pattern, the extent of damage, etc. The classification accuracy is evaluated under two cases: 1) at least a portion of data from each bearing condition from all classes is used in training; 2) data from all available conditions are considered for training except from one condition which is used explicitly for testing. Within each case, the effect of changing the domain of the input data is evaluated on the achieved accuracy. Three input signals based on vibration data (raw time domain signal, envelope spectrum, and spectrogram) were explored for their representation effectiveness. The proposed CNN with a spectrogram of the vibration signal as input achieves better results than similar architectures. Finally, the potential challenges that come along with the implementation of Deep Learning technologies for industrial applications are discussed and future research directions are proposed.",project-academic
10.1109/JIOT.2019.2940131,2019-09-09,a,Institute of Electrical and Electronics Engineers (IEEE),a two stage transfer learning based deep learning approach for production progress prediction in iot enabled manufacturing," In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",project-academic
10.1109/MSMC.2015.2472915,2016-06-07,a,IEEE,an enhanced approach for parameter estimation using immune dynamic learning swarm optimization based on multicore architecture," The identification of physical parameters is crucial for control-system designs, condition monitoring, and fault diagnosis of industrial drive systems. This article brings multicorearchitecture-based parallel computing technology and bioinspired intelligent optimization algorithm insight into designing for system parameter estimation models. In this study, a parallel implementation using an immune-cooperative dynamic learning particle swarm optimization (PSO) algorithm with multicore computation architectures is presented for permanent magnet synchronous machine (PMSM) parameter estimations. Three novel strategies are discussed, all with the purpose of enhancing the dynamic response and fast convergence performance of the designed parameter estimator. The strategies include a dynamic velocity modification strategy, an immune-memory-based searched information preservation mechanism, and an immune-network-based learning operator for PSO. Finally, a proposed method is applied to the parameter estimations of PMSMs as well as parallel running on multicore central processing units (CPUs). The results illustrate that the proposed method can effectively estimate multiple parameters of PMSMs. Moreover, the computational efficiency of the proposed method is greatly enhanced by using multicore parallel computation techniques, which satisfies the required real-time response needed in drive control systems. The proposed method is a generic model that can be applied to other nonlinear parameter identification systems and can assist in operation prediction and state observation of a system.",project-academic
10.1007/978-3-540-85481-4_7,2008-09-01,p,"Springer, Berlin, Heidelberg",behavioural targeting in on line advertising an empirical study," On-line behavioural targeting is a dynamically evolving area of web mining concerning the applications of data analysis of on-line users' behaviour and machine learning in optimising web on-line advertising and constitutes a problem of high importance and complexity.

The paper reports on experimental work concerning testing various benchmark machine-learning algorithms and attribute preprocessing techniques in the context of behavioural targeting.

Our final goal is to build a system which automatically learns and subsequently decides which on-line advertisements to present to a user visiting a web page, based on his previous recorded behaviour, in order to maximise the revenue of the ad-network and the web site hosting the ad, and, at the same time, to minimise the user's annoyance caused by potentially inappropriate advertisements.

We present a general adaptive model which makes it possible to test various machine-learning algorithms in a plug-in mode and its implementation.

We also report our experimental work concerning comparison of the performance of various machine-learning algorithms and data preprocessing techniques on a real dataset.

The performance of our experiments is evaluated by some objective metrics such as the click-through rate(CTR) or related.

Our experimental results clearly indicate that the presented adaptive system can significantly increase the CTR metric by 40% for some settings.

All the experiments are performed on a real industrial dataset concerning on-line ads emitted in the Polish Web during a 1-month period in 2007.

Up to the authors' best knowledge this is the first and largest evaluation made on this kind of real industrial data in Poland, at the time of writing.",project-academic
10.3390/S20195670,2020-10-04,a,MDPI AG,real time fruit recognition and grasping estimation for robotic apple harvesting," Robotic harvesting shows a promising aspect in future development of agricultural industry. However, there are many challenges which are still presented in the development of a fully functional robotic harvesting system. Vision is one of the most important keys among these challenges. Traditional vision methods always suffer from defects in accuracy, robustness, and efficiency in real implementation environments. In this work, a fully deep learning-based vision method for autonomous apple harvesting is developed and evaluated. The developed method includes a light-weight one-stage detection and segmentation network for fruit recognition and a PointNet to process the point clouds and estimate a proper approach pose for each fruit before grasping. Fruit recognition network takes raw inputs from RGB-D camera and performs fruit detection and instance segmentation on RGB images. The PointNet grasping network combines depth information and results from the fruit recognition as input and outputs the approach pose of each fruit for robotic arm execution. The developed vision method is evaluated on RGB-D image data which are collected from both laboratory and orchard environments. Robotic harvesting experiments in both indoor and outdoor conditions are also included to validate the performance of the developed harvesting system. Experimental results show that the developed vision method can perform highly efficient and accurate to guide robotic harvesting. Overall, the developed robotic harvesting system achieves 0.8 on harvesting success rate and cycle time is 6.5 seconds.",project-academic
10.1109/DFT.2017.8244433,2017-10-01,p,IEEE Computer Society,realizing strong puf from weak puf via neural computing," Physically Unclonable Functions (PUFs) are hardware-based security primitives that promise to provide an advantage in terms of area and power compared to hardware implementations of standard cryptography algorithms. PUFs harness manufacturing process variations to realize binary keys (Weak PUFs) or binary functions (SStrong PUFs). An ideal Strong PUF realizes a binary function that maps an m-bit input challenge to a random n-bit output response and offers an exponential number of such unique challenge-response pairs (CRPs). Hence, it is attractive for authentication applications. Unfortunately, most Strong PUF implementations are non-ideal, where an adversary can build a machine-learning model by observing a relatively few CRPs, making it possible to predict the output response of a PUF to a future challenge. Existence of such a model, or clone, constitutes a breach of security. In this paper, we make two contributions: first, we demonstrate that by leveraging a Weightless Neural Network (WNN), we can realize a CMOS Strong PUF from a Weak PUF. Next, we demonstrate that WNN based Strong PUFs offer robust resistance to machine-learning, while also delivering on uniqueness and reliability metrics — bringing it closer to an ideal Strong PUF. Neural network hardware is gaining importance for pattern matching and classification. This work demonstrates how such a design may be re-purposed for security. In the rest of the paper, we present architecture, practical implementation and analysis of Neural Network based PUFs.",project-academic
10.1109/IPDPSW.2018.00207,2018-05-21,p,IEEE,custom machine learning architectures towards realtime anomaly detection for flight testing," Test flight of a new commercial aeroplane is crucial in validating the functionality, safety and performance of the new aeroplane design before its batch manufacturing can take place. Massive amounts of data streams are typically generated from thousands of sensors on an aeroplane during test flight, which require realtime processing to detect anomaly and to predict malfunctions for emergency response. This paper provides an overview of recent research in custom machine learning architectures which have shown promise for highspeed data processing, and proposes a time series learning model based on LSTM (Long Short Term Memory). This LSTM model is adopted for realtime data analysis used in anomaly detection for the COMAC C919 test flight. A custom architecture targeting FPGA (Field Programmable Gate Array) implementation for the proposed approach can be embedded into realtime data analysis and processing platforms for large commercial aircraft.",project-academic
10.1007/978-3-319-99707-0_39,2018-08-26,p,"Springer, Cham",a hybrid machine learning approach for predictive maintenance in smart factories of the future," Advanced technologies based on Internet of Things (IOT) are blazing a trail to effective and efficient management of an overall plant. In this context, manufacturing companies require an innovative strategy to survive in a competitive business environment, utilizing those technologies. Guided by these requirements, the so-called predictive maintenance is of paramount importance and offers a significant potential for innovation to overcome the limitations of traditional maintenance policies. However, real shop-floors often have obstacles in providing insights to facilitate the effective management of assets in smart factories. Even if a significant amount of machine and process data is available, one of the common problems of these data is the lack of annotations describing the machine status or maintenance history. For this reason, companies have limited options to analyse manufacturing data, despite the capability of advanced machine learning techniques in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations. Moreover, each machine generates highly heterogeneous data, making it difficult to integrate all the information to provide data-driven decision support for predictive maintenance. Inspired by these challenges, this research provides a hybrid machine learning approach combining unsupervised learning and semi-supervised learning. The approach and result in this article are based on the development and implementation in a large collaborative EU-funded H2020 research project entitled BOOST 4.0 i.e. Big Data Value Spaces for COmpetitiveness of European COnnected Smart FacTories.",project-academic
,2009-01-01,a,,use of a virtual prototyping in construction of a mining machine s control system," Purpose: Possibilities of state-of-the-art information and automation systems in the process of construction and testing the machine’s control system are the project objectives. The concept of virtual prototyping of a roadheader’s control system consisting in coupling the real PLC controller, made by WAGO, with the virtual machine with its surrounding was presented in the paper. Such a solution enables testing the functions of the control system before its installation on a real object, what can significantly reduce the time of the system implementation. Data concerning .NET simulation application made in the Visual Basic language as well as the method of communication between PLC controller and MODBUS TCP simulation application were given. The work also contains a description of basic functions of the roadheader control system as well as assumptions concerning construction of an adaptive control system – which uses the methods of artificial intelligence for analysis of conditions that are around the machine. Design/methodology/approach: Virtual prototyping of a roadheader’s control system consisting in coupling the real PLC controller, made by WAGO, with the virtual machine and with its surrounding. Findings: Trends in the world mining industry indicate for necessity of development roadheader control systems to improve operations associated with roadways driving. Use of a virtual prototyping method will enable to speed up implementation of the system. Research limitations/implications: Building the adaptive system for roadheader control using artificial intelligence will be the next step of the project. Practical implications: The solution presented in the paper is a part of the project aiming at a development of the system for visualization and control of roadheader. The system will be installed in a roadheader manufactured in Poland.",project-academic
10.1007/S10845-005-4823-7,2005-02-01,a,Kluwer Academic Publishers,the training of neural networks to model manufacturing processes," Neural networks have been increasingly used in various areas of manufacturing. Modelling of manufacturing processes, to allow experimentation on the model, is one of the areas in which successful applications have been reported. Most literature in this area is focused on network results. This paper concentrates on methods for training neural networks to model complex manufacturing processes. It summarises the use of neural network for process modelling in the past decade and provides some detailed guidelines for network training. A case study of a complex forming process is used to demonstrate a real implementation case in industry, and the issues arising from this case are discussed.",project-academic
10.1016/J.CVIU.2016.03.018,2016-07-01,a,Academic Press,wize mirror a smart multisensory cardio metabolic risk monitoring system," A multi-sensor device for health self-monitoring and assessment is proposed.A real-time head pose estimation and tracking method is introduced.An inexpensive 3D scanner facilitating facial morphology analysis is described.Face 3D shape analysis facilitates tracking changes in weight and BMI index.The evaluation of stress and anxiety seems possible using dynamic facial features. In the recent years personal health monitoring systems have been gaining popularity, both as a result of the pull from the general population, keen to improve well-being and early detection of possibly serious health conditions and the push from the industry eager to translate the current significant progress in computer vision and machine learning into commercial products. One of such systems is the Wize Mirror, built as a result of the FP7 funded SEMEOTICONS (SEMEiotic Oriented Technology for Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring) project. The project aims to translate the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, multispectral images, and 3D scans of the face. The multisensory platform, being developed as the result of that project, in the form of a smart mirror, looks for signs related to cardio-metabolic risks. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. This paper is focused on the description of the part of that system, utilising computer vision and machine learning techniques to perform 3D morphological analysis of the face and recognition of psycho-somatic status both linked with cardio-metabolic risks. The paper describes the concepts, methods and the developed implementations as well as reports on the results obtained on both real and synthetic datasets.",project-academic
10.1016/J.PROCIR.2013.09.042,2013-01-01,a,Elsevier,an enabling digital foundation towards smart machining," Abstract None None Today's major challenges for manufacturing companies in the aerospace and automotive industries are clear: global cooperation with multiple supply chain partners, production optimization, management and tracking of information so as to meet new requirements in terms of traceability, security and sustainability. The need for a data exchange standard that allows disparate entities and their associated devices in a manufacturing system to share data seamlessly is clearly obvious. And the first expected impact is the ‘next generation’ smart controller that could really enable an intelligent machining process based on real-time monitoring and diagnosis, self-learning decision and adaptive optimization. The four-year project titled FoFdation envisions a ‘Digital and Smart Factory’ architecture and implementation. This has the potential to achieve significant benefits in earlier visibility of manufacturing issues, faster production ramp-up time, faster time to volume production and subsequently shorter time to market, reduced manufacturing costs and improved product quality, as well as sustainability objectives like low energy consumption and waste reduction. The present paper describes the on-going work with specific focus on the definition and implementation of the FoFdation Smart Machine Controller (SMC) in an adaptable architecture that satisfies both commercial and open source CNC controllers. It highlights the project's end use validation framework as well as sets a strong Manufacturing Information System foundation on which process optimization and control as well as sustainable practices can be based. It presents the general vision of the target solution for the SMC developed in the FoFdation project. It is based on efforts past and present both by academia and industry in various capacities and proposes tentative implementations based on the STEP-NC standard to define the machine controller of the future.",project-academic
10.3390/SU11113173,2019-06-05,a,Multidisciplinary Digital Publishing Institute,monitoring potato waste in food manufacturing using image processing and internet of things approach," Approximately one-third of the food produced globally is spoiled or wasted in the food supply chain (FSC). Essentially, it is lost before it even reaches the end consumer. Conventional methods of food waste tracking relying on paper-based logs to collect and analyse the data are costly, laborious, and time-consuming. Hence, an automated and real-time system based on the Internet of Things (IoT) concepts is proposed to measure the overall amount of waste as well as the reasons for waste generation in real-time within the potato processing industry, by using modern image processing and load cell technologies. The images captured through a specially positioned camera are processed to identify the damaged, unusable potatoes, and a digital load cell is used to measure their weight. Subsequently, a deep learning architecture, specifically the Convolutional Neural Network (CNN), is utilised to determine a potential reason for the potato waste generation. An accuracy of 99.79% was achieved using a small set of samples during the training test. We were successful enough to achieve a training accuracy of 94.06%, a validation accuracy of 85%, and a test accuracy of 83.3% after parameter tuning. This still represents a significant improvement over manual monitoring and extraction of waste within a potato processing line. In addition, the real-time data generated by this system help actors in the production, transportation, and processing of potatoes to determine various causes of waste generation and aid in the implementation of corrective actions.",project-academic
10.1007/S10489-013-0501-1,2014-07-01,a,Springer US,hardware implementation methods in random vector functional link networks," Recently appeared a renewed interest for Single Layer Feedforward Neural Network (SLF-NN) models where the hidden layer coefficients are randomly assigned and the output coefficients are calculated by a least square algorithm. In addition to random coefficient initialization, the main advantages for these learning models are the speed of training (no multiple iterations required) and no initial coefficient definition (e.g. no adaptation constant as in multilayer perceptron). These features are adequate for real time operation since a fast online training can be achieved, benefiting to applications (industrial, automotive, portable systems) where other neural networks learning approaches could not be used due to large resource usage, low speed and lack of flexibility. Thus, targeting hardware implementation allows its use in embedded systems, expanding its application areas to real time systems and, in general, those applications where the use of desktop computers is not possible. Typically, RVFLN demands a wide number of resources and a high computational burden; high dimension matrices are involved, and computation intensive algorithms are required to obtain the output layer coefficient values for the neural network, especially matrix inversion. This work describes the algorithm implementation and optimization of these models to fit embedded hardware system requirements together with a parameterizable model, allowing different applications to benefit from it. The proposal includes the use of fuzzy activation functions in neurons to reduce computations. An exhaustive analysis of three proposed different computation architectures for the learning algorithm is done. Classification results for three standard datasets and fixed point arithmetic are compared to Matlab floating point results, together with hardware related analysis as speed of operation, bit-length accuracy in fixed point arithmetic and logic resource occupation.",project-academic
10.1145/3437963.3441657,2021-03-08,p,ACM,personalization in practice methods and applications," Personalization is one of the key applications in machine learning with widespread usage across e-commerce, entertainment, production, healthcare and many other industries. While various machine learning techniques present novel state-of-the-art advances and super-human performance year-over-year, personalization and recommender-systems applications are often late-adopters of novel solutions due to problem hardness and implementation complexity. This tutorial presents recent advances across the personalization industry and demonstrates their practical applications in real case-studies of world-leading online platforms. Key trends such as deep learning, causality and active exploration with bandits are depicted with real examples and demonstrated alongside their business considerations and implementation challenges.Rising topics like explainability, fairness, natural interfaces and content generation are covered, touching on aspects of both technology and user experience. Our tutorial relies on recent advances in the field and on work conducted at Booking.com, where we implement personalization models on one of the world's leading online travel platform.",project-academic
10.1108/JM2-02-2016-0015,2017-09-04,a,Emerald Publishing Limited,decision support systems in manufacturing a survey and future trends," Purpose




This paper aims to propose a theoretical decision support framework, which integrates artificial intelligence (AI), discrete-event simulation (DES) and database management technologies so as to determine the steady state flow of items (e.g. fixtures, jigs, tools, etc.) in manufacturing.




Design/methodology/approach




The existing literature was carefully reviewed to address the state of the arts in decision support systems (DSS), the shortcomings of pure simulation-based and pure AI-based DSS. A conceptual example is illustrated to show the integrated application of AI, simulation and database components of the proposed DSS framework.




Findings




Recent DSS studies have revealed the limitations of pure simulation-based and pure AI-based DSS. A new DSS framework is required in manufacturing to address these limitations, taking into account the problems of flowing items.




Research limitations/implications




The theoretical DSS framework is proposed using simple rules and equations. This implies that it is not complex for software development and implementation. Practical data are not presented in this paper. A real DSS will be developed using the proposed theoretical framework and realistic results will be presented in the near future.




Originality/value




The proposed theoretical framework reveals how the integrated components of DSS can work together in manufacturing in order to determine the stable flow of items in a specific production period. Especially, the integrated performance of case-based reasoning (CBR) and DES is conceptually illustrated.",project-academic
10.1109/ICASSP39728.2021.9414882,2021-06-06,p,IEEE,unsupervised clustering of time series signals using neuromorphic energy efficient temporal neural networks," Unsupervised time series clustering is a challenging problem with diverse industrial applications such as anomaly detection, bio-wearables, etc. These applications typically involve small, low-power devices on the edge that collect and process real-time sensory signals. State-of-the-art time-series clustering methods perform some form of loss minimization that is extremely computationally intensive from the perspective of edge devices. In this work, we propose a neuromorphic approach to unsupervised time series clustering based on Temporal Neural Networks that is capable of ultra low-power, continuous online learning. We demonstrate its clustering performance on a subset of UCR Time Series Archive datasets. Our results show that the proposed approach either outperforms or performs similarly to most of the existing algorithms while being far more amenable for efficient hardware implementation. Our hardware assessment analysis shows that in 7 nm CMOS the proposed architecture, on average, consumes only about 0.005 mm2 die area and 22 μW power and can process each signal with about 5 ns latency.",project-academic
10.1109/TCIAIG.2012.2212194,2012-08-07,a,IEEE,antbot ant colonies for video games," The video game industry is an emerging market which continues to expand. From its early beginning, developers have focused mainly on sound and graphical applications, paying less attention to developing game bots or other kinds of nonplayer characters (NPCs). However, recent advances in artificial intelligence offer the possibility of developing game bots which are dynamically adjustable to several difficulty levels as well as variable game environments. Previous works reveal a lack of swarm intelligence approaches to develop these kinds of agents. Considering the potential of particle swarm optimization due to its emerging properties and self-adaptation to dynamic environments, further investigation into this field must be undertaken. This research focuses on developing a generic framework based on swarm intelligence, and in particular on ant colony optimization, such as it allows general implementation of real-time bots that work over dynamic game environments. The framework has been adapted to allow the implementation of intelligent agents for the classical game Ms. Pac-Man. These were trialed at the Ms. Pac-Man competitions held during the 2011 International Congress on Evolutionary Computation.",project-academic
,2019-01-29,a,,structural material property tailoring using deep neural networks," Advances in robotics, artificial intelligence, and machine learning are ushering in a new age of automation, as machines match or outperform human performance. Machine intelligence can enable businesses to improve performance by reducing errors, improving sensitivity, quality and speed, and in some cases achieving outcomes that go beyond current resource capabilities. Relevant applications include new product architecture design, rapid material characterization, and life-cycle management tied with a digital strategy that will enable efficient development of products from cradle to grave. In addition, there are also challenges to overcome that must be addressed through a major, sustained research effort that is based solidly on both inferential and computational principles applied to design tailoring of functionally optimized structures. Current applications of structural materials in the aerospace industry demand the highest quality control of material microstructure, especially for advanced rotational turbomachinery in aircraft engines in order to have the best tailored material property. In this paper, deep convolutional neural networks were developed to accurately predict processing-structure-property relations from materials microstructures images, surpassing current best practices and modeling efforts. The models automatically learn critical features, without the need for manual specification and/or subjective and expensive image analysis. Further, in combination with generative deep learning models, a framework is proposed to enable rapid material design space exploration and property identification and optimization. The implementation must take account of real-time decision cycles and the trade-offs between speed and accuracy.",project-academic
10.2514/6.2019-1703,2019-01-07,p,American Institute of Aeronautics and Astronautics,structural material property tailoring using deep neural networks," Advances in robotics, artificial intelligence, and machine learning are ushering in a new age of automation, as machines match or outperform human performance. Machine intelligence can enable businesses to improve performance by reducing errors, improving sensitivity, quality and speed, and in some cases achieving outcomes that go beyond current resource capabilities. Relevant applications include new product architecture design, rapid material characterization, and life-cycle management tied with a digital strategy that will enable efficient development of products from cradle to grave. In addition, there are also challenges to overcome that must be addressed through a major, sustained research effort that is based solidly on both inferential and computational principles applied to design tailoring of functionally optimized structures. Current applications of structural materials in the aerospace industry demand the highest quality control of material microstructure, especially for advanced rotational turbomachinery in aircraft engines in order to have the best tailored material property. In this paper, deep convolutional neural networks were developed to accurately predict processing-structure-property relations from materials microstructures images, surpassing current best practices and modeling efforts. The models automatically learn critical features, without the need for manual specification and/or subjective and expensive image analysis. Further, in combination with generative deep learning models, a framework is proposed to enable rapid material design space exploration and property identification and optimization. The implementation must take account of real-time decision cycles and the trade-offs between speed and accuracy.",project-academic
10.1109/ACCESS.2021.3104472,2021-08-12,a,Institute of Electrical and Electronics Engineers (IEEE),transfer learning strategies for credit card fraud detection," Credit card fraud jeopardizes the trust of customers in e-commerce transactions. This led in recent years to major advances in the design of automatic Fraud Detection Systems (FDS) able to detect fraudulent transactions with short reaction time and high precision. Nevertheless, the heterogeneous nature of the fraud behavior makes it difficult to tailor existing systems to different contexts (e.g. new payment systems, different countries and/or population segments). Given the high cost (research, prototype development, and implementation in production) of designing data-driven FDSs, it is crucial for transactional companies to define procedures able to adapt existing pipelines to new challenges. From an AI/machine learning perspective, this is known as the problem of None transfer learning . This paper discusses the design and implementation of transfer learning approaches for e-commerce credit card fraud detection and their assessment in a real setting. The case study, based on a six-month dataset (more than 200 million e-commerce transactions) provided by the industrial partner, relates to the transfer of detection models developed for a European country to another country. In particular, we present and discuss 15 transfer learning techniques (ranging from naive baselines to state-of-the-art and new approaches), making a critical and quantitative comparison in terms of precision for different transfer scenarios. Our contributions are twofold: (i) we show that the accuracy of many transfer methods is strongly dependent on the number of labeled samples in the target domain and (ii) we propose an ensemble solution to this problem based on self-supervised and semi-supervised domain adaptation classifiers. The thorough experimental assessment shows that this solution is both highly accurate and hardly sensitive to the number of labeled samples.",project-academic
10.23919/DATE51398.2021.9474254,2021-02-01,p,Institute of Electrical and Electronics Engineers Inc.,printed stochastic computing neural networks," Printed electronics (PE) offers flexible, extremely low-cost, and on-demand hardware due to its additive manufacturing process, enabling emerging ultra-low-cost applications, including machine learning applications. However, large feature sizes in PE limit the complexity of a machine learning classifier (e.g., a neural network (NN)) in PE. Stochastic computing Neural Networks (SC-NNs) can reduce area in silicon technologies, but still require complex designs due to unique implementation tradeoffs in PE. In this paper, we propose a printed mixed-signal system, which substitutes complex and power-hungry conventional stochastic computing (SC) components by printed analog designs. The printed mixed-signal SC consumes only 35% of power consumption and requires only 25% of area compared to a conventional 4-bit NN implementation. We also show that the proposed mixed-signal SC-NN provides good accuracy for popular neural network classification problems. We consider this work as an important step towards the realization of printed SC-NN hardware for near-sensor-processing.",project-academic
10.4324/9780429021381,2020-04-27,a,Routledge,handbook of research on stem education," The Handbook of Research on STEM Education represents a groundbreaking and comprehensive synthesis of research and presentation of policy within the realm of science, technology, engineering, and mathematics (STEM) education. What distinguishes this Handbook from others is the nature of integration of the disciplines that is the founding premise for the work – all chapters in this book speak directly to the integration of STEM, rather than discussion of research within the individual content areas. None The Handbook of Research on STEM Education explores the most pressing areas of STEM within an international context. Divided into six sections, the authors cover topics including: the nature of STEM, STEM learning, STEM pedagogy, curriculum and assessment, critical issues in STEM, STEM teacher education, and STEM policy and reform. The Handbook utilizes the lens of equity and access by focusing on STEM literacy, early childhood STEM, learners with disabilities, informal STEM, socio-scientific issues, race-related factors, gender equity, cultural-relevancy, and parental involvement. Additionally, discussion of STEM education policy in a variety of countries is included, as well as a focus on engaging business/industry and teachers in advocacy for STEM education. None The Handbook’s 37 chapters provide a deep and meaningful landscape of the implementation of STEM over the past two decades. As such, the findings that are presented within provide the reader with clear directions for future research into effective practice and supports for integrated STEM, which are grounded in the literature to date.",project-academic
10.1142/S021821300900024X,2009-08-01,a,World Scientific Publishing Company,nature inspired intelligence a review of selected methods and applications," The successful handling of numerous real–world complex problems has increased the popularity of nature–inspired intelligent (NII) algorithms and techniques. Their successful implementation primarily on difficult and complicated optimization problems, stresses their upcoming importance in the broader area of artificial intelligence. NII techniques take advantage of the way that biological systems deal with real–world situations. Specifically, they simulate the way real biological systems, such as the human brain, ant colonies and human immune system work, when solving complex real–world situations. In this survey paper, we briefly present a number of selected NII approaches and we point particular suitable areas of application for each of them. Specifically, five major categories of nature inspired approaches are presented, namely, Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), DNA computing, artificial immune systems and membrane computing. Applications include problems related to optimization (financial, industrial and medical), task scheduling, system design (optimization of the system's parameters), image processing and data processing (feature selection and classification). We also refer to collaboration between NII techniques and classical AI methodologies, such as neural networks, genetic algorithms, fuzzy logic, etc. The current survey states that NII techniques are likely to become the next step in the rapid evolution of artificial intelligence tools.",project-academic
10.1039/D0EE03116J,2021-03-23,a,Royal Society of Chemistry (RSC),advancing photoreforming of organics highlights on photocatalyst and system designs for selective oxidation reactions," Photoreforming is a process that harnesses the redox ability of photocatalysts upon illumination, to simultaneously drive the reduction of H+ into hydrogen gas and oxidation of organic compounds. Over the past few decades, significant effort has been devoted to improving the photocatalytic hydrogen evolution efficiency, while substantially less focus has been directed towards the oxidation reactions. More recently, the realization of the potential for simultaneous hydrogen production with value-added organics has inspired researchers to use photooxidation pathways to tune the selectivity of oxidized products. As a distinct benefit, the less energetically demanding organic reforming is highly favorable when compared to the slow kinetics of oxygen evolution which negates the need for expensive and/or harmful hole scavengers. Photocatalyst modifications, such as secondary component deposition, doping, defect, phase and morphology engineering, have been the main strategies adopted to tune the photooxidation pathways and oxidation products. Direct control of the process conditions, including pH, temperature and reactant concentration, and favorable reactor designs can further improve the selectivity towards desired products. While other published reviews focus on the types of photocatalysts or feedstocks used to enhance the hydrogen evolution efficiency, this review highlights the importance of controlling the selectivity of the photoreforming reaction, particularly as an alternative path for waste abatement or valorization for industry. This review links the strategies used to improve the selectivity of photoreforming of organic waste into high-value and desirable chemicals, as well as offers an outlook on the future research direction required to deliver highly selective photocatalyst. A holistic strategy that comprises photocatalyst and system designs, appropriate characterizations and implementation of artificial intelligence has also been proposed and discussed to further aid establishment of the structure–mechanism–function relationship, thereby accelerating the discovery of optimum selective photoreforming systems.",project-academic
10.17853/1994-5639-2019-6-93-121,2019-07-03,a,Федеральное государственное автономное образовательное учреждение высшего образования «Российский государственный профессионально-педагогический университет»,стратегические ориентиры подготовки педагогических кадров для системы непрерывного профессионального образования," . Introduction. None Transition to a post-industrial socio-economic structure and information society caused education crisis, which is common for world space. Dynamic development of high technologies and rapid obsolescence of knowledge significantly reduced life cycle of professions and caused deep structural changes in the sphere of employment. The need of economics for workers with qualitatively new qualification characteristics - a flexible person with meta-professional competencies, capable to solve complex professional problems and ready for innovative activity not just for today, but with the future in mind - has necessitated a significant updating of the system of vocational training and its reorganisation facilitating the implementation of a formula “lifelong learning”. The solution of problems of education begins with qualitative training of teachers, since the qualities of future professionals - graduates of the educational organisation, primarily depend on the level of teachers’ competencies. None The None aim None of the present article is to determine the strategic directions of innovative development of vocational education and preparation of highly qualified pedagogical personnel. None None Methodology and research methods. None The hypothetico-inductive method, theoretic-methodological analysis and synthesis of the content of scientific literature were the major research methods. The research work was carried out on the basis of the principle of convergence, which determines cross-disciplinary and supra-professional communications, acts as a factor of design and a statement in professiology of a new phenomenon - transprofessionalism. The authors described the concept of transprofessionalism through the process- and project-based approaches. None None Results and scientific novelty. None The developmental trends and directions of vocational education are considered. The principal defining factor of vocational education is close integration of all its processes and subsystems (prevocational training - secondary vocational education - higher education institutions - postgraduate training), which integrity is provided by the continuing and advanced education. The concept of transprofessionalism and convergence in multidisciplinary training of specialists is proved. The innovative educational programmes based on this concept are characterised. The logical-semantic model of a modern specialist is designed. This model can form an empirical basis to design vocational education platform, which integrates socio-humanistic, natural-science disciplines and related innovative technologies for teachers’ training in the system of continuing vocational education. Strategic directions for realisation of such preparation are formulated: transprofessionalism, cooperation / collaboration, advanced and “high-speed” training, digital transformation. None None Practical significance . The research materials can be useful for specialists in the field of vocational education, heads of the educational organisations in order to develop and make managerial decisions and to effectively organise the process of vocational training of pedagogical personnel.",project-academic
10.1109/TNS.2014.2309254,2012-06-09,p,IEEE,implementation of the disruption predictor apodis in jet s real time network using the marte framework," The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution.",project-academic
,2018-09-07,,,method for constructing neural network model of super deep adversarial learning," The invention relates to a method for constructing a neural network model of super deep adversarial learning in the field of artificial intelligence. According to the invention, adversarial learning is unsupervised machine learning which is directly for small data and transfers towards a high probability direction in a self-disciplined manner on the basis of a multi-probability scale, a strict distance relationship of data traversing different spaces is obtained through learning, a fuzzy event probability measure theory is introduced, the microscopic fuzzy information and the probability information are fully utilized, and the most rigorous adversarial relationship for performing adversity between the data is further established, so that the best adversarial learning effect can be obtained. The implementation effects are that problems of the optimal classification with the maximum probability and highest reliability, optimal pattern recognition and optimal prediction of the data traversing different spaces can be solved, a real simulated brain neuron mechanism can be realized, and an optimal algorithm of machine learning is achieved, thereby being an epoch-making new model capableof enabling the artificial intelligence to be widely applied in industry.",project-academic
10.1109/08IAS.2008.164,2008-10-24,p,IEEE,real time implementation of intelligent modeling and control techniques on a plc platform," Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC's features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications.",project-academic
,2011-07-12,b,,sustainable development for engineers a handbook and resource guide," It is crucial that engineers – from students to those already practising – have a deep understanding of the environmental threats facing the world, if they are to become part of the solution and not the problem. Is there a way to reconcile modern lifestyles with the compelling need for change? Could new improved technologies play a key role? If great leaps in the environmental efficiency of technologies are needed, can they be produced? Engineers are in a privileged and hugely influential position to innovate, design and build a sustainable future. But are they engaged or uninterested? Are they knowledgeable or ignorant?  This book has been developed by a number of committed educators in European engineering departments under the leadership of Delft University of Technology and the Technical University of Catalunya to meet the perceived gap between what engineers know and what they should know in relation to sustainable development. The University of Delft decided as long ago as 1998 that all of its engineering graduates, working towards careers as designers, managers or researchers, should be prepared for the challenge of sustainable development and, as such, should leave university able to make sustainable development operational in their designs and daily practices. The huge amount of knowledge gathered on best-practice teaching for engineers is reflected in this book.  The aim is to give engineering students a grounding in the challenge that sustainable development poses to the engineering profession, the contribution the engineer can make to attaining some of the societal and environmental goals of sustainability, and the barriers and pitfalls engineers will likely need to confront in their professional lives.  Concise but comprehensive, the book examines the key tools, skills and techniques that can be used in engineering design and management to ensure that whole-life costs and impacts of engineering schemes are addressed at every stage of planning, implementation and disposal. The book also aims to demonstrate through real-life examples the tangible benefits that have already been achieved in many engineering projects, and to highlight how real improvements can be, and are being, made. Each chapter ends with a series of questions and exercises for the student to undertake. Sustainable Development for Engineers will be essential reading for all engineers and scientists concerned with sustainable development. In particular, it provides key reading and learning materials for undergraduate and postgraduate students reading environmental, chemical, civil or mechanical engineering, manufacturing and design, environmental science, green chemistry and environmental management.",project-academic
10.1016/J.AEI.2021.101246,2021-01-01,a,Elsevier,a systematic literature review on intelligent automation aligning concepts from theory practice and future perspectives," Abstract None None With the recent developments in robotic process automation (RPA) and artificial intelligence (AI), academics and industrial practitioners are now pursuing robust and adaptive decision making (DM) in real-life engineering applications and automated business workflows and processes to accommodate context awareness, adaptation to environment and customisation. The emerging research via RPA, AI and soft computing offers sophisticated decision analysis methods, data-driven DM and scenario analysis with regard to the consideration of decision choices and provides benefits in numerous engineering applications. The emerging intelligent automation (IA) – the combination of RPA, AI and soft computing – can further transcend traditional DM to achieve unprecedented levels of operational efficiency, decision quality and system reliability. RPA allows an intelligent agent to eliminate operational errors and mimic manual routine decisions, including rule-based, well-structured and repetitive decisions involving enormous data, in a digital system, while AI has the cognitive capabilities to emulate the actions of human behaviour and process unstructured data via machine learning, natural language processing and image processing. Insights from IA drive new opportunities in providing automated DM processes, fault diagnosis, knowledge elicitation and solutions under complex decision environments with the presence of context-aware data, uncertainty and customer preferences. This sophisticated review attempts to deliver the relevant research directions and applications from the selected literature to the readers and address the key contributions of the selected literature, IA’s benefits, implementation considerations, challenges and potential IA applications to foster the relevant research development in the domain.",project-academic
10.1016/J.COMPIND.2015.05.001,2015-12-01,a,Elsevier Science Publishers B. V.,artificial cognitive control with self x capabilities," This computational architecture is inspired and fed by recent progress in neuroscience.The design and implementation of self-learning and self-optimization capabilities.The implementation in a low-cost computational platform to facilitate technology transfer in industry. Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup.",project-academic
10.1016/J.JFOODENG.2010.02.027,2010-07-01,a,Elsevier,nonlinear predictive control based on artificial neural network model for industrial crystallization," This paper illustrates the benefits of a nonlinear model based predictive control (NMPC) strategy for setpoint tracking control of an industrial crystallization process. A neural networks model is used as internal model to predict process outputs. An optimization problem is solved to compute future control actions taking into account real-time control objectives. Furthermore, a more suitable output variable is used for process control: the mass of crystals in the solution is used instead of the traditional electrical conductivity. The performance of the NMPC implementation is assessed via simulation results based on industrial data.",project-academic
10.1109/MIPRO.2016.7522210,2016-05-01,p,,the challenge of cellular cooperative its services based on 5g communications technology," We live at a time when the automotive industry is going through a technological revolution with the development of vehicles changing into autonomous moving objects having the properties of artificial intelligence. Additionally, cellular communications networks introduce new technologies and concepts: SDN (Software-defined networking), NFV (Network Functions Virtualization). These advanced software-defined communications networks virtualize network functions, allowing a new way of configuration, control and management. Increasing the speed and automation are key requirements to support the most demanding services such as mobile payment of contextual services, as well as the introduction of new “Machine” users. SDN also assists in the implementation of new infrastructure for the dynamic services that are based on the concepts of IoT, Big Data and Everything-as-a-Service. Using NFV enables the Internet of Things services that provide a new way of connecting people, processes, data and devices. It is precisely these requirements that are essential for the introduction of C-ITS systems, i.e., the integration of passengers, drivers, vehicles and transport infrastructure, as well as information, statistics, predictive traffic analytics, all this in real-time. Due to this trend of development of cellular communications networks in the next 5G communications networks, automotive and ITS traffic systems are becoming the most important market for business expansion of telecom operators. Such revolutionary technological changes, together with the integration of various industries entails a series of challenges. The aim of this paper is to define important information, challenges and opportunities for the telecom industry to provide mobility for people and goods.",project-academic
10.3390/S21082689,2021-04-11,a,Multidisciplinary Digital Publishing Institute,soft grippers for automatic crop harvesting a review," Agriculture 4.0 is transforming farming livelihoods thanks to the development and adoption of technologies such as artificial intelligence, the Internet of Things and robotics, traditionally used in other productive sectors. Soft robotics and soft grippers in particular are promising approaches to lead to new solutions in this field due to the need to meet hygiene and manipulation requirements in unstructured environments and in operation with delicate products. This review aims to provide an in-depth look at soft end-effectors for agricultural applications, with a special emphasis on robotic harvesting. To that end, the current state of automatic picking tasks for several crops is analysed, identifying which of them lack automatic solutions, and which methods are commonly used based on the botanical characteristics of the fruits. The latest advances in the design and implementation of soft grippers are also presented and discussed, studying the properties of their materials, their manufacturing processes, the gripping technologies and the proposed control methods. Finally, the challenges that have to be overcome to boost its definitive implementation in the real world are highlighted. Therefore, this review intends to serve as a guide for those researchers working in the field of soft robotics for Agriculture 4.0, and more specifically, in the design of soft grippers for fruit harvesting robots.",project-academic
10.1016/J.PROMFG.2020.01.168,2019-01-01,a,Elsevier BV,robot assisted concept for assembling form coils in laminated stator cores of large electric motors," Abstract None None Manufacturers of large electric motors, whether for automation or traction purposes, are facing major challenges in high-wage countries. Due to the high proportion of manual activities, a conflict between ensuring profitability and increasing variant variety arises. In order to meet the high demand for large electric motors and to maintain the value added in high-wage countries, manual manufacturing processes must be automated, at least partially. For large electric motors with form coil technology, especially the assembly into the laminated stator core represents a tedious, cost-intensive manual activity. Therefore, this paper deals with the development of a hybrid, robot-assisted assembly system for inserting form coils in laminated stator cores. The combination of manual and automated assembly activities is intended to ensure high productivity while maintaining flexibility. Before realizing such a hybrid assembly system, all process steps are simulated first. In addition, additive manufacturing is used to quickly produce and practically validate different end effector geometries. As shown by the final prototypical implementation, a major portion of the assembly of form coils can be carried out fully automatically by a dual-arm robot. Remaining challenges could be addressed by adding a vision system and thereby making use of novel machine learning techniques.",project-academic
10.1007/978-981-15-2133-1_14,2020-01-01,a,"Springer, Singapore",evolutionary artificial neural networks comparative study on state of the art optimizers," Artificial neural networks (ANN) have a great impact on research in the field of artificial intelligence. It has great capability besides the easy implementation, and due to that, it has been widely used in a wide area of real-life and industrial applications. Today, we can see a variety of ANNs such as feed-forward ANN, Kohonen self-organizing ANN, radial basis function (RBF) ANN, spiking ANN, etc. This chapter focuses on evolutionary ANN wherein the learning process is by nature-inspired optimization techniques instead of the classic routine. The focus of this chapter is the neuro-evolution-based ANN techniques by different state-of-the-art nature-inspired meta-heuristic optimization techniques and comparison of them over a monitoring system to detect the oil filter condition in agricultural machines (Ag machines). In this comparative study, the fourteen state-of-art meta-heuristic optimizers are compared in the same regard.",project-academic
,2013-11-14,p,IEEE,utilisation of on line machine learning for scada system alarms forecasting," This paper describes a prototype design and implementation of a real-time (on-line) knowledge generation component which can be utilised in industrial Supervisory Control and Data Acquisition (SCADA) systems. The overall architecture of our SCADA scenario, which utilise proposed knowledge generation is based on a multi-agent approach. This design is different from what we can see in conventional commercial SCADA solutions. Nowadays, there is a big pressure on operators to precisely analyse a huge amount of data coming from technological processes and make right decisions in the right time. This is where a real-time knowledge generation can highly improve decision making strategies in complex industrial processes. Nevertheless, the actual state of the art solutions are usually not using the knowledge generation directly, or there are often restricted so called off-line learning approaches. The recent development in the area of machine learning lead to the creation of distributed solutions which could process real-time data and dynamically adapt the generated knowledge. We applied this on-line machine learning approach in our proposed prototype. The experimental agent is focused on the specific scenario of the process alarm forecasting, which is considered to be a binary classification problem. We describe our solution for useful classifier feature vector construction. The classifier itself is based on Passive-Aggressive algorithm. Furthermore, in order to evaluate a performance of the classification the results from knowledge generation experiments were provided in form of Matthews Correlation Coefficient (MCC) together with Receiver Operating Characteristic (ROC). The proposed prototype shows how to design and implement an on-line knowledge generation component for novel SCADA solutions.",project-academic
10.1016/J.PROMFG.2017.04.042,2017-01-01,a,Elsevier,enhancing learning experience in physical action orientated learning factories using a virtually extended environment and serious gaming approaches," Abstract None None Development and implementation of new paradigms such as cyber-physical as well as lean and green production systems lead to higher degrees of system dependencies and complexity. Learning factories are a possible solution to convey the knowledge for new paradigms. Since exclusively physical learning factories provide a realistic learning experience on a low level – including product, machine and process view – new paradigms require an extended system perspective. To enable learners to understand how changes to production systems might not only affect neighboring production systems, but also building services, maintenance activities, up- and downstream supply chains and even product life cycle, the authors propose a virtually extended environment tightly intertwined with the physical learning factory. Embedded in a business game, the learners interact within the hybrid physical-virtual learning factory. Technical and organizational changes can be applied to the learning factory to solve given tasks. To overcome physically limited possibilities, these changes might not only be physical, but also virtual. Subsequent long-term simulations ascertain how changes affect key figures on management and supply chain level.",project-academic
10.1109/IOLTS50870.2020.9159704,2020-07-13,p,IEEE,high level modeling of manufacturing faults in deep neural network accelerators," The advent of data-driven real-time applications requires the implementation of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google’s Tensor Processing Unit (TPU) is one such neural network accelerator that uses systolic array-based matrix multiplication hardware for computation in its crux. Manufacturing faults at any state element of the matrix multiplication unit can cause unexpected errors in these inference networks. In this paper, we propose a formal model of permanent faults and their propagation in a TPU using the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed using the probabilistic model checking technique to reason about the likelihood of faulty outputs. The obtained quantitative results show that the classification accuracy is sensitive to the type of permanent faults as well as their location, bit position and the number of layers in the neural network. The conclusions from our theoretical model have been validated using experiments on a digit recognition-based DNN.",project-academic
10.1016/J.PROMFG.2018.12.017,2019-01-01,a,Elsevier,ai based injection molding process for consistent product quality," Abstract None None In manufacturing processes, Injection Molding is widely used for producing plastic components with large lot size. So, continuous improvements in product quality consistency is crucial to maintaining a competitive edge in the injection molding industry. Various optimization techniques like ANN, GA, Iterative method, and simulation based are being used for optimization of Injection Molding process and obtaining optimal processing conditions. But still due to variation during molding cycles, quality failure occurs. As many constituents like process, Material, machine together yields product quality. This paper is focused on Real time AI based control of process parameters in injection molding cycle. Process parameters and their interrelationship with quality failure has been studied and later supposed to be used to generate algorithm for compensating the deviation of process parameters. Pressure and temperature sensor assisted monitoring system is used to collect data in real time and based on its comparison with the standard values an interrelationship is formed between parameters and plastic material properties. Algorithm generates new process parameter values to compensate the deviation and machine control follows the same. The entire process is supposed to be smart and automatic after being trained with AI and machine learning techniques. Simulation using Moldflow software and real industry collected data has been used for understanding whole molding process establishing relationship between failure and parameters. An automotive product in real industry is chosen for data acquisition, implementation and validation of entire AI based system.",project-academic
10.1109/TVT.2021.3084829,2021-07-08,a,Institute of Electrical and Electronics Engineers (IEEE),guest editorialintroduction to the special section on vehicular networks in the era of 6g end edge cloud orchestrated intelligence," The articles in this special section focus on vehicular networks in the era of 6G mobile communication. With the growth of the vehicle population, vehicular networks play a key role in building safe, efficient, and intelligent transport systems and has been attracting a lot of attention from both academic and industrial communities around the world. The rise of autonomous driving technology and the prosperity of mobile applications, e.g., real-time video analytic, image-aided navigation, natural language processing, and etc, have brought tremendous pressure on current vehicular networks, e.g., high bandwidth, ultra-low latency, high reliability, high security, powerful computation capability, and massive connections. It is necessary to continue to develop vehicular networks by combining the latest research intends in other fields to meet quickly rising communication and computation demands. The upcoming 6G technology, which provides Holographic and Artificial Intelligence (AI) enabled communications, together with the increasing implementation of artificial intelligence in mobile devices, will lead to a new research trend to end-edge-cloud orchestrated computing with intelligence. It means that, not only the intelligent communication protocols, but also the intelligent computing resource management and machine learning algorithms among the mobile vehicles, the edge and the cloud, should be redesigned to support the development of vehicular networks.",project-academic
10.1023/A:1015883728142,2002-07-01,a,Kluwer Academic Publishers,design and implementation of a flexible manufacturing control system using neural network," Design and implementation of a sequential controller based on the concept of artificial neural networks for a flexible manufacturing system are presented. The recurrent neural network (RNN) type is used for such a purpose. Contrary to the programmable controller, an RNN-based sequential controller is based on a definite mathematical model rather than depending on experience and trial and error techniques. The proposed controller is also more flexible because it is not limited by the restrictions of the finite state automata theory. Adequate guidelines of how to construct an RNN-based sequential controller are presented. These guidelines are applied to different case studies. The proposed controller is tested by simulations and real-time experiments. These tests prove the successfulness of the proposed controller performances. Theoretical as well as experimental results are presented and discussed indicating that the proposed design procedure using Elman's RNN can be effective in designing a sequential controller for event-based type manufacturing systems. In addition, the simulation results assure the effectiveness of the proposed controller to outperform the effect of noisy inputs.",project-academic
10.1080/09511929108944511,1991-11-01,a,Taylor & Francis Group,real time scheduling in computer integrated manufacturing a review of recent research," Abstract More people are realizing that the success of computer integrated manufacturing (CIM) systems depends on effective scheduling and control. Due to the dynamic nature of manufacturing processes, intelligent real-time scheduling has always been a desirable, but elusive, goal. However, with more computer power being installed on the factory floor and more emphasis placed on networked computer communications, there has been renewed industrial interest and increased academic research in the real-time scheduling area. This paper presents a working definition of real-time control, reviews the recent research on real-time control of CIM systems in the areas of general approaches, artificial intelligence techniques, and simulation methods, and discusses several implementation and feasibility issues.",project-academic
10.1109/SCC49971.2021.00021,2021-08-01,p,IEEE,improving dependability of onboard deep learning with resilient tensorflow," As the dawn of a new age in spaceflight approaches, the drive to equip future spacecraft with high-performance computing capabilities is increasing. Many within the industry are looking to leverage solutions enabled by machine learning (ML) and artificial intelligence to enhance mission efficiency. Tasks such as image processing and object tracking are desired for long-duration spaceflight and extravehicular activities. In order to realize these applications in practice, enhancements to onboard processing are needed. ML applications require state-of-the-art processors and hardware accelerators, such as GPUs. However, GPUs are heavily susceptible to radiation-induced single-event effects (SEEs). Additionally, missions require a level of safety-criticality, which is unable to be met by existing commercial-off-the-shelf (COTS) GPUs. In an effort to create an end-to-end solution, this work aims to bridge ML-application development with device-architectural awareness to deliver a fault-aware implementation of the TensorFlow framework called Resilient TensorFlow (RTF). By building customized operations into the TensorFlow framework and employing them within the graph of various models, RTF demonstrates an ability to mask faults that occur during processing while minimizing overhead. Reducing faults during the processing of deep-learning applications brings the space computing industry closer to realizing onboard high-performance computing.",project-academic
10.1016/J.NEUCOM.2016.09.005,2017-05-10,a,Elsevier,wood moisture content prediction using feature selection techniques and a kernel method," Wood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (MC) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting MC from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the MC of wood. The relationship between the reflection coefficient measurements and the MC is studied. Based upon this relationship, MC predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible. HighlightsThe prediction of moisture content for two wood chips species using the wood dielectric property is studied.Nonlinear models are built to predict the reflection coefficient values from frequencies.Those reflection coefficients are used as input variables of a moisture content predictive model designed using Least Squares Support Vector Machines (LS-SVM) technique and feature selection methods.Numerical experiments using real world data show the effectiveness of the proposed methodology that requires a limited computational power.",project-academic
,2018-12-17,a,Université Paris-Saclay,security in the cloud an anomaly based detection framework for the insider threats," Cloud Computing (CC) opens new possibilities for more flexible and efficient services for Cloud Service Clients (CSCs). However, one of the main issues while migrating to the cloud is that what once was a private domain for CSCs, now is handled by a third-party, hence subject to their security policies. Therefore, CSCs' confidentiality, integrity, and availability (CIA) should be ensured. In spite of the existence of protection mechanisms, such as encryption, the monitoring of the CIA properties becomes necessary. Additionally, new threats emerge every day, requiring more efficient detection techniques. The work presented in this document goes beyond the state of the art by treating the malicious insider threat, one of the least studied threats in CC. This is mainly due to the organizational and legal barriers from the industry, and therefore the lack of appropriate datasets for detecting it. We tackle this matter by addressing two challenges.First, the derivation of an extensible methodology for modeling the behavior of a user in a company. This abstraction of an employee includes intra psychological factors, contextual information and is based on a role-based approach. The behaviors follow a probabilistic procedure, where the malevolent motivations are considered to occur with a given probability in time.The main contribution, a design and implementation of an anomaly-based detection framework for the aforementioned threat. This implementation enriches itself by comparing two different observation points: a profile-based view from the local network of the company, and a cloud-end view that analyses data from the services with whom the clients interact. This allows the learning process of anomalies to benefit from two perspectives: (1) the study of both real and simulated traffic with respect to the cloud service's interaction, in favor of the characterization of anomalies; and (2) the analysis of the cloud service in order to aggregate data statistics that support the overall behavior characterization.The design of this framework empirically shows to detect a broader set of anomalies of the company's interaction with the cloud. This is possible due to the replicable and extensible nature of the mentioned insider model. Also, the proposed detection model takes advantage of the autonomic nature of a clustering machine learning technique, following an unsupervised, adaptive algorithm capable of characterizing the evolving behaviors of the users towards cloud assets. The solution efficiently tackles the detection of anomalies by showing high levels of clustering performance, while keeping a low False Positive Rate (FPR), ensuring the detection performance for threat scenarios where the threat comes from inside the enterprise",project-academic
10.1159/000504785,2019-12-13,a,S. Karger AG,artificial intelligence power for civilisation and for better healthcare," Artificial intelligence (AI) is changing the world we live in, and it has the potential to transform struggling healthcare systems with new efficiencies, new therapies, new diagnostics, and new economies. Already, AI is having an impact on healthcare, and new prospects of far greater advances open up daily. This paper sets out how AI can bring new precision to care, with benefits for patients and for society as a whole. But it also sets out the conditions for realizing the potential: key issues are ensuring adequate access to data, an appropriate regulatory environment, action to sustain innovation in research institutes and industry big and small, promotion of take-up of innovation by the healthcare establishment, and resolution of a range of vital legal and ethical questions centred on safeguarding patients and their rights. For Europe to fulfil the conditions for success, it will have to find a new spirit of cooperation that can overcome the handicaps of the continent's fragmented technical and legal landscape. The start the European Union has made shows some ambition, but a clearer strategic vision and firmer plans for implementation will be needed. The European Alliance for Personalised Medicine (EAPM) has listed its own priorities: data, integrating innovation into care, building trust, developing skills and constructing policy frameworks that guarantee infrastructure, equitable access, and legal clarity.",project-academic
10.1109/TII.2020.3002197,2021-02-01,a,Institute of Electrical and Electronics Engineers (IEEE),utilizing industry 4 0 on the construction site challenges and opportunities," In recent years, a step change has been seen in the rate of adoption of Industry 4.0 technologies by manufacturers and industrial organizations alike. This article discusses the current state of the art in the adoption of Industry 4.0 technologies within the construction industry. Increasing complexity in onsite construction projects coupled with the need for higher productivity is leading to increased interest in the potential use of Industry 4.0 technologies. This article discusses the relevance of the following key Industry 4.0 technologies to construction: data analytics and artificial intelligence, robotics and automation, building information management, sensors and wearables, digital twin, and industrial connectivity. Industrial connectivity is a key aspect as it ensures that all Industry 4.0 technologies are interconnected allowing the full benefits to be realized. This article also presents a research agenda for the adoption of Industry 4.0 technologies within the construction sector, a three-phase use of intelligent assets from the point of manufacture up to after build, and a four-staged R&D process for the implementation of smart wearables in a digital enhanced construction site.",project-academic
10.1109/ACC.2008.4587179,2008-06-11,p,IEEE,prototype design of a multi agent system for integrated control and asset management of petroleum production facilities," This paper addresses a practical intelligent multi- agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work and defined its autonomy, communications, and artificial intelligence (AI) requirements, we are proceeding to build a system prototype and simulate it in real time to validate its logical behavior in normal and abnormal process situations. We also conducted a thorough system performance analysis to detect any computational bottlenecks. Although the preliminary system prototype design has limitations, simulation results have demonstrated an effective system logical behavior and performance.",project-academic
10.1109/MEMCOD.2015.7340480,2015-12-03,p,IEEE,passive testing of production systems based on model inference," This paper tackles the problem of testing produc­tion systems, i.e. systems that run in industrial environments, and that are distributed over several devices and sensors. Usually, such systems lack of models, or are expressed with models that are not up to date. Without any model, the testing process is often done by hand, and tends to be an heavy and tedious task. This paper contributes to this issue by proposing a framework called Autofunk, which combines different fields such as model inference, expert systems, and machine learning. This framework, designed with the collaboration of our industrial partner Michelin, infers formal models that can be used as specifications to perform offline passive testing. Given a large set of production messages, it infers exact models that only capture the functional behaviours of a system under analysis. Thereafter, inferred models are used as input by a passive tester, which checks whether a system under test conforms to these models. Since inferred models do not express all the possible behaviours that should happen, we define conformance with two implementation relations. We evaluate our framework on real production systems and show that it can be used in practice.",project-academic
10.1038/S41598-020-68156-2,2020-07-08,a,Nature Publishing Group,a machine learning workflow for raw food spectroscopic classification in a future industry," Over the years, technology has changed the way we produce and have access to our food through the development of applications, robotics, data analysis, and processing techniques. The implementation of these approaches by the food industry ensure quality and affordability, reducing at the same time the costs of keeping the food fresh and increase productivity. A system, as the one presented herein, for raw food categorization is needed in future food industries to automate food classification according to type, the process of algorithm approaches that will be applied to every different food origin and also for serving disabled people. The purpose of this work was to develop a machine learning workflow based on supervised PLS regression and SVM classification, towards automated raw food categorization from FTIR. The system exhibited high efficiency in multi-class classification of 7 different types of raw food. The selected food samples, were diverse in terms of storage conditions (temperature, storage time and packaging), while the variability within each food was also taken into account by several different batches; leading in a classifier able to embed this variation towards increased robustness and efficiency, ready for real life applications targeting to the digital transformation of the food industry.",project-academic
10.1109/GIOTS49054.2020.9119497,2020-06-03,p,IEEE,industrial iot and digital twins for a smart factory an open source toolkit for application design and benchmarking," The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the ‘digital twin’ concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to ‘close the gap’ between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry ‘open data’, and is bench-marked with universal testing tools.",project-academic
10.1016/J.JMBBM.2021.104728,2021-11-01,a,Elsevier,what can artificial intelligence and machine learning tell us a review of applications to equine biomechanical research," Artificial intelligence (AI) and machine learning (ML) are fascinating interdisciplinary scientific domains where machines are provided with an approximation of human intelligence. The conjecture is that machines are able to learn from existing examples, and employ this accumulated knowledge to fulfil challenging tasks such as regression analysis, pattern classification, and prediction. The horse biomechanical models have been identified as an alternative tool to investigate the effects of mechanical loading and induced deformations on the tissues and structures in humans. Many reported investigations into bone fatigue, subchondral bone damage in the joints of both humans and animals, and identification of vital parameters responsible for retaining integrity of anatomical regions during normal activities in all species are heavily reliant on equine biomechanical research. Horse racing is a lucrative industry and injury prevention in expensive thoroughbreds has encouraged the implementation of various measurement techniques, which results in massive data generation. ML substantially accelerates analysis and interpretation of data and provides considerable advantages over traditional statistical tools historically adopted in biomechanical research. This paper provides the reader with: a brief introduction to AI, taxonomy and several types of ML algorithms, working principle of a feedforward artificial neural network (ANN), and, a detailed review of the applications of AI, ML, and ANN in equine biomechanical research (i.e. locomotory system function, gait analysis, joint and bone mechanics, and hoof function). Reviewing literature on the use of these data-driven tools is essential since their wider application has the potential to: improve clinical assessments enabling real-time simulations, avoid and/or minimize injuries, and encourage early detection of such injuries in the first place.",project-academic
10.1126/SCIENCE.AAX1566,2019-08-09,a,American Association for the Advancement of Science,a robotic platform for flow synthesis of organic compounds informed by ai planning," INTRODUCTION None The ability to synthesize complex organic molecules is essential to the discovery and manufacture of functional compounds, including small-molecule medicines. Despite advances in laboratory automation, the identification and development of synthetic routes remain a manual process and experimental synthesis platforms must be manually configured to suit the type of chemistry to be performed, requiring time and effort investment from expert chemists. The ideal automated synthesis platform would be capable of planning its own synthetic routes and executing them under conditions that facilitate scale-up to production goals. Individual elements of the chemical development process (design, route development, experimental configuration, and execution) have been streamlined in previous studies, but none has presented a path toward integration of computer-aided synthesis planning (CASP), expert refined chemical recipe generation, and robotically executed chemical synthesis. None RATIONALE None We describe an approach toward automated, scalable synthesis that combines techniques in artificial intelligence (AI) for planning and robotics for execution. Millions of previously published reactions inform the computational design of synthetic routes; expert-refined chemical recipe files (CRFs) are run on a robotic flow chemistry platform for scalable, reproducible synthesis. This development strategy augments a chemist’s ability to approach target-oriented flow synthesis while substantially reducing the necessary information gathering and manual effort. None RESULTS None We developed an open source software suite for CASP trained on millions of reactions from the Reaxys database and the U.S. Patent and Trademark Office. The software was designed to generalize known chemical reactions to new substrates by learning to apply retrosynthetic transformations, to identify suitable reaction conditions, and to evaluate whether reactions are likely to be successful when attempted experimentally. Suggested routes partially populate CRFs, which require additional details from chemist users to define residence times, stoichiometries, and concentrations that are compatible with continuous flow. To execute these syntheses, a robotic arm assembles modular process units (reactors and separators) into a continuous flow path according to the desired process configuration defined in the CRF. The robot also connects reagent lines and computer-controlled pumps to reactor inlets through a fluidic switchboard. When that is completed, the system primes the lines and starts the synthesis. After a specified synthesis time, the system flushes the lines with a cleaning solvent, and the robotic arm disconnects reagent lines and removes process modules to their appropriate storage locations. None This paradigm of flow chemistry development was demonstrated for a suite of 15 medicinally relevant small molecules. In order of increasing complexity, we investigated the synthesis of aspirin and secnidazole run back to back; lidocaine and diazepam run back to back to use a common feedstock; (S)-warfarin and safinamide to demonstrate the planning program’s stereochemical awareness; and two compound libraries: a family of five ACE inhibitors including quinapril and a family of four nonsteroidal anti-inflammatory drugs including celecoxib. These targets required a total of eight particular retrosynthetic routes and nine specific process configurations. None CONCLUSION None The software and platform herein represent a milestone on the path toward fully autonomous chemical synthesis, where routes still require human input and process development. Over time, the results generated by this and similar automated experimental platforms may reduce our reliance on historical reaction data, particularly in combination with smaller-scale flow-screening platforms. Increased availability of reaction data will further enable robotically realized syntheses based on AI recommendations, relieving expert chemists of manual tasks so that they may focus on new ideas.",project-academic
,2011-10-03,b,,computer aided reasoning an approach," From the Publisher:
An Approach
Computer-Aided Reasoning: An Approach is a textbook introduction to computer-aided reasoning. It can be used in graduate and upper-division undergraduate courses on software engineering or formal methods. It is also suitable in conjunction with other books in courses on hardware design, discrete mathematics, or theory, especially courses stressing formalism, rigor, or mechanized support. It is also appropriate for courses on artificial intelligence or automated reasoning and as a reference for business and industry. 
Current hardware and software systems are often very complex and the trend is towards increased complexity. Many of these systems are of critical importance; therefore making sure that they behave as expected is also of critical importance. By modeling computing systems mathematically, we obtain models that we can prove behave correctly. The complexity of computing systems makes such proofs very long, complicated, and error-prone. To further increase confidence in our reasoning, we can use a computer program to check our proofs and even to automate some of their construction. 
In this book we present:

A practical functional programming language closely related to Common Lisp which is used to define functions (which can model computing systems) and to make assertions about defined functions; A formal logic in which defined functions correspond to axioms; the logic is first-order, includes induction, and allows us to prove theorems about the functions; The computer-aided reasoning system ACL2, which includes the programming language, the logic, and mechanical support for the proof process.

The ACL2 system hasbeen successfully applied to projects of commercial interest, including microprocessor, modeling, hardware verification, microcode verification, and software verification. This book gives a methodology for modeling computing systems formally and for reasoning about those models with mechanized assistance. The practicality of computer-aided reasoning is further demonstrated in the companion book, Computer-Aided Reasoning: ACL2 Case Studies. 
Approximately 140 exercises are distributed throughout the book. Additional material is freely available from the ACL2 home page on the Web, http://www.cs.utexas.edu/users/moore/ac12, including solutions to the exercises, additional exercises, case studies from the companion book, research papers, and the ACL2 system with detailed documentation.

ACL2 Case Studies
Computer-Aided Reasoning: ACL2 Case Studies illustrates how the computer-aided reasoning system ACL2 can be used in productive and innovative ways to design, build, and maintain hardware and software systems. Included here are technical papers written by twenty-one contributors that report on self-contained case studies, some of which are sanitized industrial projects. The papers deal with a wide variety of ideas, including floating-point arithmetic, microprocessor simulation, model checking, symbolic trajectory evaluation, compilation, proof checking, real analysis, and several others. 
Computer-Aided Reasoning: ACL2 Case Studies is meant for two audiences: those looking for innovative ways to design, build, and maintain hardware and software systems faster and more reliably, and those wishing to learn how to do this. The former audience includes project managers and students in survey-oriented courses. The latter audience includes students and professionals pursuing rigorous approaches to hardware and software engineering or formal methods. Computer-Aided Reasoning: ACL2 Case Studies can be used in graduate and upper-division undergraduate courses on Software Engineering, Formal Methods, Hardware Design, Theory of Computation, Artificial Intelligence, and Automated Reasoning. 
The book is divided into two parts. Part I begins with a discussion of the effort involved in using ACL2. It also contains a brief introduction to the ACL2 logic and its mechanization, which is intended to give the reader sufficient background to read the case studies. A more thorough, textbook introduction to ACL2 may be found in the companion book, Computer-Aided Reasoning: An Approach. 
The heart of the book is Part II, where the case studies are presented. The case studies contain exercises whose solutions are on the Web. In addition, the complete ACL2 scripts necessary to formalize the models and prove all the properties discussed are on the Web. For example, when we say that one of the case studies formalizes a floating-point multiplier and proves it correct, we mean that not only can you read an English description of the model and how it was proved correct, but you can obtain the entire formal content of the project and replay the proofs, if you wish, with your copy of ACL2.
ACL2 may be obtained from its home page, http://www.cs.utexas.edu/users/moore/ac12. The results reported in each case study, as ACL2 input scripts, as well as exercise solutions for both books, are available from this page.",project-academic
10.1002/9780470569962,2010-03-22,b,Wiley-Blackwell,evolving intelligent systems methodology and applications," From theory to techniques, the first all-in-one resource for EIS There is a clear demand in advanced process industries, defense, and Internet and communication (VoIP) applications for intelligent yet adaptive/evolving systems. Evolving Intelligent Systems is the first self- contained volume that covers this newly established concept in its entirety, from a systematic methodology to case studies to industrial applications. Featuring chapters written by leading world experts, it addresses the progress, trends, and major achievements in this emerging research field, with a strong emphasis on the balance between novel theoretical results and solutions and practical real-life applications. Explains the following fundamental approaches for developing evolving intelligent systems (EIS): the Hierarchical Prioritized Structure the Participatory Learning Paradigm the Evolving Takagi-Sugeno fuzzy systems (eTS+) the evolving clustering algorithm that stems from the well-known Gustafson-Kessel offline clustering algorithm Emphasizes the importance and increased interest in online processing of data streams Outlines the general strategy of using the fuzzy dynamic clustering as a foundation for evolvable information granulation Presents a methodology for developing robust and interpretable evolving fuzzy rule-based systems Introduces an integrated approach to incremental (real-time) feature extraction and classification Proposes a study on the stability of evolving neuro-fuzzy recurrent networks Details methodologies for evolving clustering and classification Reveals different applications of EIS to address real problems in areas of: evolving inferential sensors in chemical and petrochemical industry learning and recognition in robotics Features downloadable software resources Evolving Intelligent Systems is the one-stop reference guide for both theoretical and practical issues for computer scientists, engineers, researchers, applied mathematicians, machine learning and data mining experts, graduate students, and professionals.",project-academic
,2013-11-01,b,,information and influence propagation in social networks," Research on social networks has exploded over the last decade. To a large extent, this has been fueled by the spectacular growth of social media and online social networking sites, which continue growing at a very fast pace, as well as by the increasing availability of very large social network datasets for purposes of research. A rich body of this research has been devoted to the analysis of the propagation of information, influence, innovations, infections, practices and customs through networks. Can we build models to explain the way these propagations occur? How can we validate our models against any available real datasets consisting of a social network and propagation traces that occurred in the past? These are just some questions studied by researchers in this area. Information propagation models find applications in viral marketing, outbreak detection, finding key blog posts to read in order to catch important stories, finding leaders or trendsetters, information feed ranking, etc. A number of algorithmic problems arising in these applications have been abstracted and studied extensively by researchers under the garb of influence maximization. This book starts with a detailed description of well-established diffusion models, including the independent cascade model and the linear threshold model, that have been successful at explaining propagation phenomena. We describe their properties as well as numerous extensions to them, introducing aspects such as competition, budget, and time-criticality, among many others. We delve deep into the key problem of influence maximization, which selects key individuals to activate in order to influence a large fraction of a network. Influence maximization in classic diffusion models including both the independent cascade and the linear threshold models is computationally intractable, more precisely #P-hard, and we describe several approximation algorithms and scalable heuristics that have been proposed in the literature. Finally, we also deal with key issues that need to be tackled in order to turn this research into practice, such as learning the strength with which individuals in a network influence each other, as well as the practical aspects of this research including the availability of datasets and software tools for facilitating research. We conclude with a discussion of various research problems that remain open, both from a technical perspective and from the viewpoint of transferring the results of research into industry strength applications. Table of Contents: Acknowledgments / Introduction / Stochastic Diffusion Models / Influence Maximization / Extensions to Diffusion Modeling and Influence Maximization / Learning Propagation Models / Data and Software for Information/Influence: Propagation Research / Conclusion and Challenges / Bibliography / Authors' Biographies / Index",project-academic
10.1145/2970276.2970311,2016-08-25,p,ACM,testing advanced driver assistance systems using multi objective search and neural networks," Recent years have seen a proliferation of complex Advanced Driver Assistance Systems (ADAS), in particular, for use in autonomous cars. These systems consist of sensors and cameras as well as image processing and decision support software components. They are meant to help drivers by providing proper warnings or by preventing dangerous situations. In this paper, we focus on the problem of design time testing of ADAS in a simulated environment. We provide a testing approach for ADAS by combining multi-objective search with surrogate models developed based on neural networks. We use multi-objective search to guide testing towards the most critical behaviors of ADAS. Surrogate modeling enables our testing approach to explore a larger part of the input search space within limited computational resources. We characterize the condition under which the multi-objective search algorithm behaves the same with and without surrogate modeling, thus showing the accuracy of our approach. We evaluate our approach by applying it to an industrial ADAS system. Our experiment shows that our approach automatically identifies test cases indicating critical ADAS behaviors. Further, we show that combining our search algorithm with surrogate modeling improves the quality of the generated test cases, especially under tight and realistic computational resources.",project-academic
,2004-01-01,b,,head first design patterns," You're not alone. At any given moment, somewhere in the world someone struggles with the same software design problems you have. You know you don't want to reinvent the wheel (or worse, a flat tire), so you look to Design Patterns--the lessons learned by those who've faced the same problems. With Design Patterns, you get to take advantage of the best practices and experience of others, so that you can spend your time on...something else. Something more challenging. Something more complex. Something more fun. You want to learn about the patterns that matter--why to use them, when to use them, how to use them (and when NOT to use them). But you don't just want to see how patterns look in a book, you want to know how they look ""in the wild"". In their native environment. In other words, in real world applications. You also want to learn how patterns are used in the Java API, and how to exploit Java's built-in pattern support in your own code. You want to learn the real OO design principles and why everything your boss told you about inheritance might be wrong (and what to do instead). You want to learn how those principles will help the next time you're up a creek without a design pattern. Most importantly, you want to learn the ""secret language"" of Design Patterns so that you can hold your own with your co-worker (and impress cocktail party guests) when he casually mentions his stunningly clever use of Command, Facade, Proxy, and Factory in between sips of a martini. You'll easily counter with your deep understanding of why Singleton isn't as simple as it sounds, how the Factory is so often misunderstood, or on the real relationship between Decorator, Facade and Adapter. With Head First Design Patterns, you'll avoid the embarrassment of thinking Decorator is something from the ""Trading Spaces"" show. Best of all, in a way that won't put you to sleep! We think your time is too important (and too short) to spend it struggling with academic texts. If you've read a Head First book, you know what to expect--a visually rich format designed for the way your brain works. Using the latest research in neurobiology, cognitive science, and learning theory, Head First Design Patterns will load patterns into your brain in a way that sticks. In a way that lets you put them to work immediately. In a way that makes you better at solving software design problems, and better at speaking the language of patterns with others on your team.",project-academic
,2018-05-03,a,,rf puf enhancing iot security through authentication of wireless nodes using in situ machine learning," Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key recovery attacks. State-of-the-art IoT networks such as Nest also use Open Authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUF), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF- PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. Simulation results involving the process variations in a standard 65 nm technology node, and features such as LO offset and I-Q imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 transmitters with an accuracy of 99.9% (~ 99% for 10,000 transmitters) under varying channel conditions, and without the need for traditional preambles.",project-academic
10.1109/JIOT.2018.2849324,2019-02-01,a,IEEE,rf puf enhancing iot security through authentication of wireless nodes using in situ machine learning," Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key-recovery attacks. State-of-the-art Internet of Things networks such as Nest also use open authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUFs), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF-PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through None in-situ None machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. The burden of device identification is completely shifted to the gateway Rx, similar to the operation of a human listener’s brain. Simulation results involving the process variations in a standard 65-nm technology node, and features such as local oscillator offset and None None None ${I}$ None None – None None ${Q}$ None None None imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 Tx(s) with an accuracy of 99.9% [≈99% for 10000 Tx(s)] under varying channel conditions, and without the need for traditional preambles. The proposed scheme can be used as a stand-alone security feature, or as a part of traditional multifactor authentication.",project-academic
10.1016/J.PROMFG.2017.04.039,2017-01-01,a,Elsevier,digital twin as enabler for an innovative digital shopfloor management system in the esb logistics learning factory at reutlingen university," Abstract None None Technologies for mapping the “digital twin” have been under development for approximately 20 years. Nowadays increasingly intelligent, individualized products encourages companies to respond innovatively to customer requirements and to handle the rising product variations quickly. None An integrated engineering network, spanning across the entire value chain, is operated to intelligently connect various company divisions, and to generate a business ecosystem for products, services and communities. The conditions for the digital twin are thereby determined in which the digital world can be fed into the real, and the real world back into the digital to deal such intelligent products with rising variations. None The term digital twin can be described as a digital copy of a real factory, machine, worker etc., that is created and can be independently expanded, automatically updated as well as being globally available in real time. Every real product and production site is permanently accompanied by a digital twin. First prototypes of such digital twins already exist in the ESB Logistics Learning Factory on a cloud- and app-based software that builds on a dynamic, multidimensional data and information model. A standardized language of the robot control systems via software agents and positioning systems has to be integrated. The aspect of the continuity of the real factory in the digital factory as an economical means of ensuring continuous actuality of digital models looks as the basis of changeability. None For the indoor localization sensor combinations that in addition to the hardware already contain the software required for the sensor data fusion should be used. Processing systems, scenario-live-simulations and digital shop floor management results in a mandatory procedural combination. Essential to the digital twin is the ability to consistently provide all subsystems with the latest state of all required information, methods and algorithms.",project-academic
10.1016/J.TRPRO.2016.05.236,2016-01-01,a,Elsevier,traffic models for self driving connected cars," Self-driving and connected vehicles, communicating with one another (V2V technology) and with the road infrastructure (V2I technology), are a subject of extensive research nowadays and are expected to revolutionize the automotive industry in the near future. The major goal of the authors' work is to design a microscopic traffic simulation model for such vehicles, including a robust protocol for exchanging information. The question arises as to whether such communication system may efficiently improve travel quality while reducing the risk of collisions. For the purpose of their research the authors created and developed a simulation software. Their tool visualizes traffic flow for custom but simplified road maps. The transport infrastructure includes multiple junctions, optionally equipped with traffic lights, and roads with varying number of travel lanes. Each vehicle is assigned a fixed route leading to a randomly chosen destination point. Any decisions made by autonomous cars (regarding acceleration or turning maneuvers) are preceded by communication stages (retrieving necessary data, negotiations). In the paper we present fundamental concepts, assumptions and design of our model and simulation software, the authors also discuss potential issues relevant to their approach. As for the future work, the authors plan to implement their model in a large-scale agent-based traffic simulation software, Traffic Simulation Framework, so that further examination will be carried out for realistic road networks taken from the OpenStreetMap project. The authors also plan to apply machine learning techniques, so that self-driving vehicles, as well as traffic light controllers, will be able to learn how to develop the best strategy and by this way improve traffic safety and efficiency in atypical cases.",project-academic
10.1016/J.MATDES.2011.01.058,2011-06-01,a,Elsevier,a hybrid of back propagation neural network and genetic algorithm for optimization of injection molding process parameters," Abstract None None This paper presents a hybrid optimization method for optimizing the process parameters during plastic injection molding (PIM). This proposed method combines a back propagation (BP) neural network method with an intelligence global optimization algorithm, i.e. genetic algorithm (GA). A multi-objective optimization model is established to optimize the process parameters during PIM on the basis of the finite element simulation software Moldflow, Orthogonal experiment method, BP neural network as well as Genetic algorithm. Optimization goals and design variables (process parameters during PIM) are specified by the requirement of manufacture. A BP artificial neural network model is developed to obtain the mathematical relationship between the optimization goals and process parameters. Genetic algorithm is applied to optimize the process parameters that would result in optimal solution of the optimization goals. A case study of a plastic article is presented. Warpage as well as clamp force during PIM are investigated as the optimization objectives. Mold temperature, melt temperature, packing pressure, packing time and cooling time are considered to be the design variables. The case study demonstrates that the proposed optimization method can adjust the process parameters accurately and effectively to satisfy the demand of real manufacture.",project-academic
10.1016/J.COR.2005.05.019,2007-04-01,a,Elsevier,using mega trend diffusion and artificial samples in small data set learning for early flexible manufacturing system scheduling knowledge," Abstract None None Neural networks are widely utilized to extract management knowledge from acquired data, but having enough real data is not always possible. In the early stages of dynamic flexible manufacturing system (FMS) environments, only a litter data is obtained, and this means that the scheduling knowledge is often unreliable. The purpose of this research is to utilize data expansion techniques for an obtained small data set to improve the accuracy of machine learning for FMS scheduling. This research proposes a mega-trend-diffusion technique to estimate the domain range of a small data set and produce artificial samples for training the modified backpropagation neural network (BPNN). The tool used is the Pythia software. The results of the FMS simulation model indicate that learning accuracy can be significantly improved when the proposed method is applied to a very small data set.",project-academic
,2019-02-24,p,,freeflow software based virtual rdma networking for containerized clouds," Many popular large-scale cloud applications are increasingly using containerization for high resource efficiency and lightweight isolation. In parallel, many data-intensive applications (e.g., data analytics and deep learning frameworks) are adopting or looking to adopt RDMA for high networking performance. Industry trends suggest that these two approaches are on an inevitable collision course. In this paper, we present FreeFlow, a software-based RDMA virtualization framework designed for containerized clouds. FreeFlow realizes virtual RDMA networking purely with a software-based approach using commodity RDMA NICs. Unlike existing RDMA virtualization solutions, FreeFlow fully satisfies the requirements from cloud environments, such as isolation for multi-tenancy, portability for container migrations, and controllability for control and data plane policies. FreeFlow is also transparent to applications and provides networking performance close to bare-metal RDMA with low CPU overhead. In our evaluations with TensorFlow and Spark, FreeFlow provides almost the same application performance as bare-metal RDMA.",project-academic
,2016-06-22,,,machine vision based automatic identification system and method for production line products," A machine vision-based automatic identification system and method for production line products are disclosed and relate to the field of automation sorting of industrial production line products and can be used for solving a technical problem that conventional automatic production line product identification technologies are low in accuracy. The machine vision-based automatic identification system comprises a machine vision identification algorithm module, a software interaction interface, a system communication module and an image acquisition module, wherein the machine vision identification algorithm module is used for comparing and identifying images of products to be identified and images of products of known types, and information of the types of the products to be identified can be provided; the software interaction interface is used for real time video display, algorithm operation result display and realization of algorithm operation parameter control; the system communication module is used for controlling an industrial camera to collect images via photoelectric sensor signals and is used for signal output of algorithm results; the image acquisition module is used for controlling image collecting environment and is used for collecting product images. The machine vision-based automatic identification system and method for the production line products are simple in operation, low in learning cost and high in accuracy and stability.",project-academic
10.1109/TPWRD.2007.899522,2007-07-02,a,IEEE,classification of electrical disturbances in real time using neural networks," Power-quality (PQ) monitoring is an essential service that many utilities perform for their industrial and larger commercial customers. Detecting and classifying the different electrical disturbances which can cause PQ problems is a difficult task that requires a high level of engineering knowledge. This paper presents a novel system based on neural networks for the classification of electrical disturbances in real time. In addition, an electrical pattern generator has been developed in order to generate common disturbances which can be found in the electrical grid. The classifier obtained excellent results (for both test patterns and field tests) thanks in part to the use of this generator as a training tool for the neural networks. The neural system is integrated on a software tool for a PC with hardware connected for signal acquisition. The tool makes it possible to monitor the acquired signal and the disturbances detected by the system.",project-academic
,2013-01-01,a,,comparative analysis of classification function techniques for heart disease prediction," 2 ABSTRACT: The data mining can be referred as discovery of relationships in large databases automatically and in some cases it is used for predicting relationships based on the results discovered. Data mining plays an important role in various applications such as business organizations, e-commerce, health care industry, scientific and engineering. In the health care industry, the data mining is mainly used for Disease Prediction. Various data mining techniques are available for predicting diseases namely clustering, classification, association rules, regression and etc. This paper analyses the performance of various classification function techniques in data mining for predicting the heart disease from the heart disease data set. The classification function algorithms used and tested in this work are Logistics, Multi Layer Perception and Sequential Minimal Optimization algorithms. Comparative analysis is done by using Waikato Environment for Knowledge Analysis or in short, WEKA. It is open source software which consists of a collection of machine learning algorithms for data mining tasks. The performance factors used for analysing the efficiency of algorithms are clustering accuracy and error rate. The result shows that logistics classification function efficiency is better than multi layer perception and sequential minimal optimization. Data mining can be defined as the extraction of useful knowledge from large data repositories. Compared with other data mining application fields, medical data mining plays a vital role and it has some unique characteristics. Data mining techniques are the result of a long process of research and product development. This evolution began when business data was first stored on computers, continued with improvements in data access, and more recently, generated technologies that allow users to navigate through their data in real time. Data mining takes this evolutionary process beyond retrospective data access and navigation to prospective and proactive information delivery. Data mining is ready for application in the business community because it is supported by three technologies that are now sufficiently mature: Massive data collection, Powerful multiprocessor computers and Data mining algorithms The medical data mining has the high potential in medical domain for extracting the hidden patterns in the datasets (3). These patterns are used for clinical diagnosis and prognosis. The medical data are widely distributed, heterogeneous, voluminous in nature. The data should be integrated and collected to provide a user oriented approach to novel and hidden patterns of the data. A major problem in medical science or bioinformatics analysis is in attaining the correct diagnosis of certain important information. For an ultimate diagnosis, normally, many tests generally involve the classification or clustering of large scale data. The test procedures are said to be necessary in order to reach the ultimate diagnosis. However, on the other hand, too many tests could complicate the main diagnosis process and lead to the difficulty in obtaining the end results, particularly in the case of finding disease many tests are should be performed. This kind of difficulty could be resolved with the aid of machine learning which could be used directly to obtain the end result with the aid of several artificial intelligent algorithms which perform the role as classifiers. Classification is one of the most important techniques in data mining. If a categorization process is to be done, the data is to be classified, and/or codified, and then it can be placed into chunks that are manageable by a human (12). This paper describes classification function algorithms and it also analyzes the performance of these algorithms. The performance factors used for analysis are accuracy and error measures. The accuracy measures are True Positive (TP) rate, F Measure, ROC area and Kappa Statistics. The error measures are Mean Absolute Error (M.A.E), Root Mean Squared Error (R.M.S.E), Relative Absolute Error (R.A.E) and Relative Root Squared Error (R.R.S.E).",project-academic
,2017-10-23,a,,benchip benchmarking intelligence processors," The increasing attention on deep learning has tremendously spurred the design of intelligence processing hardware. The variety of emerging intelligence processors requires standard benchmarks for fair comparison and system optimization (in both software and hardware). However, existing benchmarks are unsuitable for benchmarking intelligence processors due to their non-diversity and nonrepresentativeness. Also, the lack of a standard benchmarking methodology further exacerbates this problem. In this paper, we propose BENCHIP, a benchmark suite and benchmarking methodology for intelligence processors. The benchmark suite in BENCHIP consists of two sets of benchmarks: microbenchmarks and macrobenchmarks. The microbenchmarks consist of single-layer networks. They are mainly designed for bottleneck analysis and system optimization. The macrobenchmarks contain state-of-the-art industrial networks, so as to offer a realistic comparison of different platforms. We also propose a standard benchmarking methodology built upon an industrial software stack and evaluation metrics that comprehensively reflect the various characteristics of the evaluated intelligence processors. BENCHIP is utilized for evaluating various hardware platforms, including CPUs, GPUs, and accelerators. BENCHIP will be open-sourced soon.",project-academic
10.1007/S11390-018-1805-8,2018-01-26,a,Springer US,benchip benchmarking intelligence processors," The increasing attention on deep learning has tremendously spurred the design of intelligence processing hardware. The variety of emerging intelligence processors requires standard benchmarks for fair comparison and system optimization (in both software and hardware). However, existing benchmarks are unsuitable for benchmarking intelligence processors due to their non-diversity and nonrepresentativeness. Also, the lack of a standard benchmarking methodology further exacerbates this problem. In this paper, we propose BenchIP, a benchmark suite and benchmarking methodology for intelligence processors. The benchmark suite in BenchIP consists of two sets of benchmarks: microbenchmarks and macrobenchmarks. The microbenchmarks consist of single-layer networks. They are mainly designed for bottleneck analysis and system optimization. The macrobenchmarks contain state-of-the-art industrial networks, so as to offer a realistic comparison of different platforms. We also propose a standard benchmarking methodology built upon an industrial software stack and evaluation metrics that comprehensively reflect various characteristics of the evaluated intelligence processors. BenchIP is utilized for evaluating various hardware platforms, including CPUs, GPUs, and accelerators. BenchIP will be open-sourced soon.",project-academic
10.1145/2484838.2484884,2013-07-29,p,ACM,making sense of big data with the berkeley data analytics stack," The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications require a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowd-sourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. In this talk I'll describe the current state of BDAS with an emphasis on the key components that have been released to date. I'll then discuss ongoing efforts on machine learning scalability and ease of use, including the MLbase system, as our focus moves higher up the stack. Finally I will present our longer-term views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.",project-academic
10.1145/2684822.2685326,2015-02-02,p,ACM,making sense of big data with the berkeley data analytics stack," The Berkeley AMPLab is creating a new approach to data analytics. Launching in early 2011, the lab aims to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and in crowds). The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the four years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. BDAS features prominently in many industry discussions of the future of the Big Data analytics ecosystem -- a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving ""up the stack"" to better integrate and support advanced analytics and to make people a full-fledged resource for making sense of data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe the current state of BDAS with an emphasis on our newest efforts, including some or all of: the GraphX graph processing system, the Velox and MLBase machine learning platforms, and the SampleClean framework for hybrid human/computer data cleaning. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.",project-academic
10.1007/S10845-012-0633-X,2013-08-01,a,Springer US,feature based generation of machining process plans for optimised parts manufacture," Efficacious integration of such CAx technologies as CAD, CAM and CAPP still remains a problem in engineering practice which constantly attracts research attention. Design by feature model is assumed as a main factor in the integration effort in various engineering and manufacturing domains. It refers principally to feature clustering and consequently operation sequencing in elaborated process plan designs. The focus of this paper is on CAPP for parts manufacture in systems of definite processing capabilities, involving multi-axis machining centres. A methodical approach is proposed to optimally solve for process planning problems, which consists in the identification of process alternatives and sequencing adequate working steps. The approach involves the use of the branch and bound concept from the field of artificial intelligence. A conceptual scheme for generation of alternative process plans in the form of a network is developed. It is based on part design data modelling in terms of machining features. A relevant algorithm is proposed for creating such a network and searching for the optimal process plan solution from the viewpoint of its operational performance, under formulated process constraints. The feasibility of the approach and the algorithm are illustrated by a numerical case with regard to a real application and diverse machine tools with relevant tooling. Generated process alternatives for complex machining with given systems, are studied using models programmed in the environment of Matlab® software.",project-academic
10.1109/TII.2019.2945012,2020-02-01,a,Institute of Electrical and Electronics Engineers (IEEE),online gmm clustering and mini batch gradient descent based optimization for industrial iot 4 0," The future fifth-generation (5G) networks are expected to support a huge number of connected devices with various and multitude services having different quality of service (QoS) requirements. Communication in Industry 4.0 is one of the flagships and special applications of the 5G due to the specificity of the industrial environment as well as the variety of its services such as safety communication, robot's communications, and machine monitoring. In this context, we propose a new resource allocation for the future Industry 4.0 based on software-defined networking and network function virtualization technologies, machine learning tools and the slicing paradigm where each slice of the network is dedicated to a category of services having similar QoS requirement level. In this article, the proposed solution ensures the allocation of the resources to the slices depending on their requirements in terms of bandwidth, delay, and reliability. Toward this goal, our solution is performed in three main steps: first, Internet of Things (IoT) devices assignment to the slices step based on online Gaussian mixture model clustering algorithm, second, inter-slices resources reservations step based on mini-batch gradient descent, and third, intra-slices resources allocations based on the max-utility algorithm. We have performed extensive simulations in a realistic industrial scenario using NS3 simulator. Numerical results show the effectiveness of our proposed solution in terms of reducing packet error rate, energy consumption, and in terms of increasing the percentage of served devices in delay comparing to the traditional approaches.",project-academic
10.1007/S11663-016-0735-5,2016-09-13,a,Springer US,digitalizing the circular economy," Metallurgy is a key enabler of a circular economy (CE), its digitalization is the metallurgical Internet of Things (m-IoT). In short: Metallurgy is at the heart of a CE, as metals all have strong intrinsic recycling potentials. Process metallurgy, as a key enabler for a CE, will help much to deliver its goals. The first-principles models of process engineering help quantify the resource efficiency (RE) of the CE system, connecting all stakeholders via digitalization. This provides well-argued and first-principles environmental information to empower a tax paying consumer society, policy, legislators, and environmentalists. It provides the details of capital expenditure and operational expenditure estimates. Through this path, the opportunities and limits of a CE, recycling, and its technology can be estimated. The true boundaries of sustainability can be determined in addition to the techno-economic evaluation of RE. The integration of metallurgical reactor technology and systems digitally, not only on one site but linking different sites globally via hardware, is the basis for describing CE systems as dynamic feedback control loops, i.e., the m-IoT. It is the linkage of the global carrier metallurgical processing system infrastructure that maximizes the recovery of all minor and technology elements in its associated refining metallurgical infrastructure. This will be illustrated through the following: (1) System optimization models for multimetal metallurgical processing. These map large-scale m-IoT systems linked to computer-aided design tools of the original equipment manufacturers and then establish a recycling index through the quantification of RE. (2) Reactor optimization and industrial system solutions to realize the “CE (within a) Corporation—CEC,” realizing the CE of society. (3) Real-time measurement of ore and scrap properties in intelligent plant structures, linked to the modeling, simulation, and optimization of industrial extractive process metallurgical reactors and plants for both primary and secondary materials processing. (4) Big-data analysis and process control of industrial metallurgical systems, processes, and reactors by the application of, among others, artificial intelligence techniques and computer-aided engineering. (5) Minerals processing and process metallurgical theory, technology, simulation, and analytical tools, which are all key enablers of the CE. (6) Visualizing the results of all the tools used for estimating the RE of the CE system in a form that the consumer and general public can understand. (7) The smart integration of tools and methods that quantify RE and deliver sustainable solutions, named in this article as circular economy engineering. In view of space limitations, this message will be colored in by various publications also with students and colleagues, referring to (often commercial) software that acts as a conduit to capture and formalize the research of the large body of work in the literature by distinguished metallurgical engineers and researchers and realized in innovative industrial solutions. The author stands humbly on the shoulders of these developments and their distinguished developers. This award lecture article implicitly also refers to work done while working for Ausmelt (Australia), Outotec (Finland and Australia), Mintek (South Africa), and Anglo American Corporation (South Africa), honoring the many colleagues the author has worked with over the years.",project-academic
10.1109/SMARTCOMP.2017.7946997,2017-05-29,p,IEEE,phd forum deep learning based real time malware detection with multi stage analysis," Protecting computer systems is a critical and ongoing problem, given that real-time malware detection is hard. The state-of-the-art for defense cannot keep pace with the increasing level of sophistication of malware. The industry, for instance, relies heavily on anti-virus technology for threat, which is effective for malware with known signatures, but not sustainable given the massive amount of malware samples released daily, as well as and its inefficacy in dealing with zero-day and polymorphic/metamorphic malware (practical detection rates range from 25% to 50%). Behavior-based approaches attempt to identify malware behaviors using instruction sequences, computation trace logic, and system (or API) call sequences. These solutions have been mostly based on conventional machine learning (ML) models with hand-craft features, such as K-nearest neighbor, SVM, and decision tree algorithms. However, current solutions based on ML suffer from high false-positive rates, mainly because of (i) the complexity and diversity of current software and malware, which are hard to capture during the learning phase of thealgorithms, (ii) sub-optimal feature extraction, and (iii) limited/outdated dataset. Since malware has been continuously evolving, existing protection mechanisms do not cope well with the increasedsophistication and complexity of these attacks, especially those performed by advanced persistent threats (APT), which are multi-module, stealthy, and target- focused. Furthermore, malware campaigns are not homogeneous--malware sophistication varies depending on the target, the type of service exploited as part of the attack (e.g., Internet Banking, relationship sites), the attack spreading source (e.g., phishing, drive-by downloads), and the location of the target. The accuracy of malware classification depends on gaining sufficient context information and extracting meaningful abstraction of behaviors. In problems about detecting malicious behavior based on sequence of system calls, longer sequences likely contain more information. However, classical ML- based detectors (i.e., Random Forest, Naive Bayes) often use short windows of system calls during the decision process and may not be able to extract enough features for accurate detection in a long term window. Thus, the main drawback of such approaches is to accomplish accurate detection, since it is difficult to analyze complex and longer sequences of malicious behaviors with limited window sizes, especially when malicious and benign behaviors are interposed. In contrast, Deep Learning models are capable of analyzing longer sequences of system calls and making better decisions through higher level information extraction and semantic knowledge learning. However, Deep Learning requires more computation time to estimate the probability of detection when the model needs to be retrained incrementally, a common requirement for malware detection when new variants and samples are frequently added to the training set. The trade-off is challenging: fast and not-so-accurate (classical ML methods) versus time-consuming and accurate detection (emerging Deep Learning methods). Our proposal is to leverage the best of the two worlds with Spectrum, a practical multi-stage malware- detection system operating in collaboration with the operating system (OS).",project-academic
10.1016/J.FLOWMEASINST.2016.10.001,2017-04-01,a,Elsevier,intelligent recognition of gas oil water three phase flow regime and determination of volume fraction using radial basis function," Abstract None None The problem of how to accurately measure the flow rate of oil–gas–water mixtures in a pipeline remains one of the key challenges in the petroleum industry. This paper proposes a new methodology for identifying flow regimes and predicting volume fractions in gas-oil-water multiphase systems using dual energy fan-beam gamma-ray attenuation technique and artificial neural networks. The novelty of this study in comparison with previous works, is using just 4 extracted features (photo peaks of None 241 Am and None 137 Cs in 2 detectors) from the gamma ray spectrums instead of using the whole gamma ray spectrum, which reduces the undesired noises and also improves the speed of recognition in real situations. Radial basis function was used for developing the neural network model in MATLAB software in order to classify the flow patterns (annular, stratified and homogenous) and predict the value of volume fractions. The ideal and static theoretical models for flow regimes have been developed using MCNP-X code. The proposed networks could correctly recognize all the three different flow regimes and also determine volume fractions with mean absolute error of less than 5.68% according to the recognized regime.",project-academic
10.1109/ICECTECH.2011.5941592,2011-04-08,p,IEEE,real time simulation a novel approach in engineering education," Today, with the advent of computers and various easily accessible software packages, computer aided teaching tools have become an essential part of both classroom lectures and laboratory experiments in any kind of education curriculum. Real Time Simulation tools, as a part of Electrical Machine Drives laboratory experiments, enhance lab experience by providing students with the opportunity to judge the performance of Electrical Machines in real time situations. This interactive learning environment, consisting of simulations, demonstrations and exercises, can fulfill the role of a bridge from passive learning to active engagement and thus stimulate deeper thinking; grounding a problem based-learning environment. The applications are also very important for relating theory to practice, so that the students can develop engineering judgment and understand how process behavior can be captured using real time simulations. Due to advancement of the software tools like MATLAB/SEVIULINK with its Real Time Workshop(RTW) and Real Time Windows Target (RTWT), real time simulators are used extensively in many engineering fields, such as industry, education and research institutions. As a consequence, inclusion of the real time simulation applications in academic curriculum provides great learning value to the students. A case is made to present overview of the Real Time Simulations of Electrical Machines Drives possibility of including these techniques in modern engineering educational curriculum. This paper will review the various real time simulation techniques such as Real Time Laboratory (RT Lab), Rapid Control Prototyping (RCP) and Hardware in the Loop (HIL), which can be used in a modern engineering education.",project-academic
10.1016/J.RCIM.2019.101847,2020-02-01,a,Pergamon,trajectory smoothing method using reinforcement learning for computer numerical control machine tools," Abstract None None Tool-path codes output by computer-aided manufacturing software for high-speed machining are composed of discontinuous G01 line segments. The discontinuity of these tool movements causes computer numerical control (CNC) inefficiency. To achieve high-speed continuous motion, corner smoothing algorithms based on pre-planning methods are widely used. However, it is difficult to optimize smoothing trajectories in real-time systems. To obtain smooth trajectories efficiently, this paper proposes a neural network-based direct trajectory smoothing method. An intelligent neural network agent outputs servo commands directly based on the current tool path and running state in every cycle. To achieve direct control, motion feature and reward models were built, and reinforcement learning was used to train the neural network parameters without additional experimental data. The proposed method provides higher cutting efficiency than the local and global smoothing algorithms. Given its simple structure and low computational demands, it can easily be applied to real-time CNC systems.",project-academic
10.1016/J.PROENG.2016.07.416,2016-01-01,a,Elsevier,automated detection of faults in wastewater pipes from cctv footage by using random forests," Abstract None None Sewer systems require regular inspection in order to ensure their satisfactory condition. As most sewer networks consist of pipes too small for engineers to traverse, CCTV footage is used to record the interior of these pipes. This footage is manually analysed by qualified engineers, to determine the condition of the pipe and the presence of any faults. We propose a methodology, which automatically detects faults within the CCTV footage. This has the potential to dramatically reduce the time required to process the large volume of CCTV footage produced during a survey. The proposed methodology first characterises localised regions of each video frame using multiscale GIST features. Extremely randomised trees are then used to learn a classifier that distinguishes between frames showing a fault and normal frames. The technique is tested on 670 video segments from real sewer inspections of a variety of pipes, supplied by Wessex Water. Detection performance is assessed by plotting receiver operating characteristics and quantifying the area under the curve. Preliminary results indicate high detection accuracy of 88% and an area under the ROC curve of 96%. The machine learning used reduces the footage to a selection of frames containing faults, which can be quickly identified (whether by an engineer or another piece of software), showing promise for use in industrial wastewater network surveys.",project-academic
10.1109/HST.2018.8383920,2018-04-01,p,IEEE,sin 2 stealth infection on neural network a low cost agile neural trojan attack methodology," Deep Neural Network (DNN) has recently become the “de facto” technique to drive the artificial intelligence (AI) industry. However, there also emerges many security issues as the DNN based intelligent systems are being increasingly prevalent. Existing DNN security studies, such as adversarial attacks and poisoning attacks, are usually narrowly conducted at the software algorithm level, with the misclassification as their primary goal. The more realistic system-level attacks introduced by the emerging intelligent service supply chain, e.g. the third-party cloud based machine learning as a service (MLaaS) along with the portable DNN computing engine, have never been discussed. In this work, we propose a low-cost modular methodology-Stealth Infection on Neural Network, namely “SIN2”, to demonstrate the novel and practical intelligent supply chain triggered neural Trojan attacks. Our “SIN2” well leverages the attacking opportunities built upon the static neural network model and the underlying dynamic runtime system of neural computing framework through a bunch of neural Trojaning techniques. We implement a variety of neural Trojan attacks in Linux sandbox by following proposed “SIN2”. Experimental results show that our modular design can rapidly produce and trigger various Trojan attacks that can easily evade the existing defenses.",project-academic
10.1109/TII.2020.3023430,2021-08-01,a,IEEE,deepfed federated deep learning for intrusion detection in industrial cyber physical systems," The rapid convergence of legacy industrial infrastructures with intelligent networking and computing technologies (e.g., 5G, software-defined networking, and artificial intelligence), have dramatically increased the attack surface of industrial cyber–physical systems (CPSs). However, withstanding cyber threats to such large-scale, complex, and heterogeneous industrial CPSs has been extremely challenging, due to the insufficiency of high-quality attack examples. In this article, we propose a novel federated deep learning scheme, named DeepFed, to detect cyber threats against industrial CPSs. Specifically, we first design a new deep learning-based intrusion detection model for industrial CPSs, by making use of a convolutional neural network and a gated recurrent unit. Second, we develop a federated learning framework, allowing multiple industrial CPSs to collectively build a comprehensive intrusion detection model in a privacy-preserving way. Further, a Paillier cryptosystem-based secure communication protocol is crafted to preserve the security and privacy of model parameters through the training process. Extensive experiments on a real industrial CPS dataset demonstrate the high effectiveness of the proposed DeepFed scheme in detecting various types of cyber threats to industrial CPSs and the superiorities over state-of-the-art schemes.",project-academic
10.1145/3218603.3218616,2018-06-24,a,,in situ stochastic training of mtj crossbar based neural networks," Owing to high device density, scalability and non-volatility, Magnetic Tunnel Junction-based crossbars have garnered significant interest for implementing the weights of an artificial neural network. The existence of only two stable states in MTJs implies a high overhead of obtaining optimal binary weights in software. We illustrate that the inherent parallelism in the crossbar structure makes it highly appropriate for in-situ training, wherein the network is taught directly on the hardware. It leads to significantly smaller training overhead as the training time is independent of the size of the network, while also circumventing the effects of alternate current paths in the crossbar and accounting for manufacturing variations in the device. We show how the stochastic switching characteristics of MTJs can be leveraged to perform probabilistic weight updates using the gradient descent algorithm. We describe how the update operations can be performed on crossbars both with and without access transistors and perform simulations on them to demonstrate the effectiveness of our techniques. The results reveal that stochastically trained MTJ-crossbar NNs achieve a classification accuracy nearly same as that of real-valued-weight networks trained in software and exhibit immunity to device variations.",project-academic
10.2118/150314-MS,2012-03-27,p,OnePetro,state of the art of artificial intelligence and predictive analytics in the e p industry a technology survey," Artificial intelligence (AI) has been used for more than two decades as a development tool for solutions in several areas of the EP (b) approximately 50% of respondents declared they were somehow engaged in applying workflow automation, automatic process control, rule-based case reasoning, data mining, proxy models, and virtual environments; (c) production is the area most impacted by the applications of AI technologies; (d) the perceived level of available literature and public knowledge of AI technologies is generally low; and (e) although availability of information is generally low, it is not perceived equally among different roles. This work aims to be a guide for personnel responsible for production and asset management on how AI-based applications can add more value and improve their decision making. The results of the survey offer a guideline on which tools to consider for each particular oil and gas challenge. It also illustrates how AI techniques will play an important role in future developments of IT solutions in the E&P industry. Introduction While there is hardly a rigorous definition of the term artificial intelligence (AI) that is unequivocally accepted, the tools of AI and its intended uses have been well studied for decades and many applications have appeared. Loosely speaking, AI is the capability of machines (usually in the form of computer hardware and software) to mimic or exceed human intelligence in everyday engineering and scientific tasks associated with perceiving, reasoning, and acting. Since human intelligence is multifaceted, so is AI, comprising goals that range from knowledge representation and reasoning, to learning, to visual perception and language understanding (Winston 1992). AI techniques have been present in the E&P industry for many years. A quick literature search reveals application of AI in SPE scientific and engineering papers as early as in the 1970s. There are numerous references about the applications of neural networks, fuzzy logic, genetic algorithms, expert systems, and other artificial techniques in the resolution of problems in diverse areas, such as reservoir simulation, production optimization, process control, and fault detection and diagnosis, among many others. AI is an area of great interest in the E&P industry, mainly in applications related to production control and optimization, proxy model simulation, and virtual sensing. The most popular techniques are artificial neural networks, fuzzy logic, and genetic algorithms, with interesting developments in hybrid and nontraditional techniques. There has been recent increase in such AI-based commercial applications for production management. While the full impact of such applications is still being realized, there are already solutions in the market with a positive impact in the E&P industry.",project-academic
10.1016/J.JMSY.2014.06.004,2015-07-01,a,Elsevier,a toolbox for the design planning and operation of manufacturing networks in a mass customisation environment," Abstract None None The task of design, planning and operation of manufacturing networks is becoming more and more challenging for companies, as globalisation, mass customisation and the turbulent economic landscape create demand volatility, uncertainties and high complexity. In this context, this paper investigates the performance of decentralised manufacturing networks through a set of methods developed into a software framework in a toolbox approach. The Tabu Search and Simulated Annealing metaheuristic methods are used together with an Artificial Intelligence method, called Intelligent Search Algorithm. A multi-criteria decision making procedure is carried out for the evaluation of the quality of alternative manufacturing network configurations using multiple conflicting criteria including dynamic complexity, reliability, cost, time, quality and environmental footprint. A comparison of the performance of each method based on the quality of the solutions that it provided is carried out. The statistical design of experiments robust engineering technique is used for the calibration of the adjustable parameters of the methods. Moreover, the impact of demand fluctuation to the operational performance of the alternative networks, expressed thorough a dynamic complexity indicator, is investigated through simulation. The developed framework is validated through a real life case, with data coming from the CNC machine building industry.",project-academic
,2011-03-31,,,system and method for creating a graphical control programming environment," A system and method for generating modeling software for processing control, used in power plant control, cement plants or other industrial control applications, using models such as expert systems, fuzzy logic, genetic optimization algorithms, and neural networks to convert sensor data into actionable data, information and/or diagnostics. The present invention includes a graphical programming environment, graphical programming tools, graphical user interface (GUI), visual feedback, real-time refresh, run-time object swap, logic standby (safety recovery), modeling and optimization to allows a user to create a control system for an industrial process, and that allows the user to change the process without any manual compile, assemble or load steps other than a save and refresh pushbutton.",project-academic
10.1016/J.JPDC.2018.04.005,2018-10-01,a,Academic Press,a malicious threat detection model for cloud assisted internet of things cot based industrial control system ics networks using deep belief network," Abstract None None Internet of Things (IoT) devices are extensively used in modern industries combined with the conventional industrial control system (ICS) network through the industrial cloud to make the production data easily available to the corporate business management and easier control for highly profitable production systems. The different devices within the conventional ICS network originally manufactured to run on an isolated network and was not considered for the privacy and security of the control and production/architecture data being trafficked over the manufacturing plant to the corporate. Due to their extensive integration with the industrial cloud network over the internet, these ICS networks are exposed to a significant threat of malicious activities created by malicious software. Protecting ICS from such attacks requires continuous update of their database of anti-malware tools which requires efforts from manual experts on a regular basis. This limits real time protection of ICS. None Earlier work by Huda et al. (2017) based on a semi-supervised approach performed well. However training process of the semi-supervised-approach (Huda et al., 2017) is complex procedure which requires a hybridization of feature selection, unsupervised clustering and supervised training techniques. Therefore, it could be time consuming for ICS network for real time protection. In this paper, we propose an adaptive threat detection model for industrial cloud of things (CoT) based on deep learning. Deep learning has been used in many domain of pattern recognition and a popular approach for its simple training procedure. Most importantly, deep learning can learn the hidden patterns of the domain in an unsupervised manner which can avoid the requirements of huge expensive labeled data. We used this particular characteristic of deep learning to design our detection model. None Two different types of deep learning based detection models are proposed in this work. The first model uses a disjoint training and testing data for a deep belief network (DBN) and corresponding artificial neural network (ANN). In the second proposed detection model, DBN is trained using new unlabeled data to provide DBN with additional knowledge about the changes in the malicious attack patterns. Novelty of the proposed detection models is that the models are adaptive where training procedures is simpler than earlier work (Huda et al, 2017) and can adapt new malware behaviors from already available and cheap unlabeled data at the same time. This will avoid expensive manual labeling of new attacks and corresponding time complexity making it feasible for ICS networks. Performances of standard DBNs are sensitive to its configurations and values for the hyper-parameters including number of hidden nodes, learning rate and number epochs. Therefore proposed detection models find an optimal configuration by varying the structure of DBNs and other parameters. The proposed detection models are extensively tested on a real malware test bed. Experimental results show that the proposed approaches achieve higher accuracies than standard detection algorithms and obtain similar performances with earlier semi-supervised work (Huda et al., 2017) but provide a comparatively simplified training model.",project-academic
10.1145/3324884.3416584,2020-12-21,p,ACM,metamorphic object insertion for testing object detection systems," Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs --- similar to traditional software --- may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist. To fill this critical gap, we introduce the design and realization of MetaOD, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection failures. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.",project-academic
10.1016/J.JCLEPRO.2021.125834,2021-03-20,a,Elsevier,artificial intelligence in sustainable energy industry status quo challenges and opportunities," Abstract None None The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.",project-academic
10.1007/S40747-020-00155-2,2020-10-01,a,Springer International Publishing,machine learning based customer sentiment analysis for recommending shoppers shops based on customers review," Big data analytics plays a major role in various industries using computing applications such as E-commerce and real-time shopping. Big data are used for promoting products and provide better connectivity between retailers and shoppers. Nowadays, people always use online promotions to know about best shops for buying better products. This shopping experience and opinion about the shopper’s shop can be observed by the customer-experience shared across social media platforms. A new customer when searching a shop needs information about manufacturing date (MRD) and manufacturing price (MRP), offers, quality, and suggestions which can only be provided by the previous customer experience. The MRP and MRD are already available in the product cover or label. Several approaches have been used for predicting the product details but not providing accurate information. This paper is motivated towards applying Machine Learning algorithms for learning, analysing and classifying the product information and the shop information based on the customer experience. The product data with customer reviews is collected from benchmark Unified computing system (UCS) which is a server for data based computer product lined up for evaluating hardware, support to visualization, software management. From the results and comparison, it has been found that machine learning algorithms outperform than other approaches. The proposed HRS system has higher values of MAPE which is 96% and accuracy is nearly 98% when compared to other existing techniques. Mean absolute error of proposed HRS system is nearly 0.6 which states that the performance of the system is significantly effective.",project-academic
10.1016/J.COLA.2020.100970,2020-05-20,a,Elsevier,visual programming environments for end user development of intelligent and social robots a systematic review," Abstract None None Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working “in-the-wild” rather than only in laboratories and structured settings.",project-academic
,2010-01-01,a,,load frequency control in four area power systems using fuzzy logic pi controller," Abstract- This paper presents a load frequency control in four area power systems using fuzzy gain scheduling of PI controller is realized. The system simulation is realized by using Matlab/ Simulink software. System dynamic performance is observed for conventional PI, fuzzy PI and fuzzy logic controllers. I. I NTRODUCTION In recent years, power systems have more complicated and nonlinear configurations. Many industrial establishments are affected by operating point variations [1]. Electricity sector and end user are concerned about power quality reliability, efficiency and energy future. There are many reasons about increasing concerns on power quality. The microprocessor based equipments and power electronic devices are more sensitive to power quality. On the other hand, an electric network consists of many interconnected subsystems. If a fault occurs in a subsystem, disturbances and interruptions adversely affecting power quality take place in the power system. Any disharmonies between energy generation and demand cause frequency deviations. Thus, significant frequency deviations lead to system blackouts [2]. Power system loads are usually variable so that controller system must be designed to provide power system quality. Interconnected power systems regulate power flows and frequency by means of an automatic generation control (AGC). AGC is a feedback control system adjusting a generator output power to remain defined frequency [3]. AGC comprises a load frequency control (LFC) loop and an automatic voltage regulator (AVR) loop. LFC system provides generator load control via frequency [3]. Zero steady-state errors of frequency deviations and optimal transient behavior are objectives of the LFC in a multi-area interconnected power system [4]. So far there are many studies about load frequency control of interconnected power systems. The aim is a design of feedback controller to realize desired power flow and frequency in multi-area power system. In literature, control strategies based on conventional, fuzzy and neural network controller are proposed [5]. Several authors suggest variable-structure systems, various adaptive control techniques and Riccati equation approach for load a frequency controller design [6, 7]. There are many studies about different control strategies having advantages and disadvantages [1, 2, 5, 8-10]. In Reference [9], a load frequency control using a conventional PID controller is applied and it is emphasized that the controller performance is better that others. However, if a power system structure has nonlinear dynamics and parts, the system operating point varies and conventional controllers needing system model must not be used. In Reference [5], a modified dynamic neural networks controller is proposed. It is determined that the proposed controller offers better performance than conventional neural network controller. In Reference [2], for a single area system and two area interconnected power systems, artificial intelligence techniques are purposed for the automatic generation control and the comparison is performed between intelligent controllers and the conventional PI and PID controllers. In Reference [10], a robust decentralized control strategy is used for Load frequency control for four area power systems to obtain robust stability and better performances. In References [1, 8], power system load frequency control is realized by fuzzy gain scheduling of PI controller. II. F",project-academic
,2016-03-30,,,machine vision based intelligent artistic paint robot," The invention discloses a machine-vision-based intelligent artistic paint robot system that is applied to various souvenirs or ornaments. According to the intelligent artistic paint robot, a scenery or portrait image that is captured by a camera is processed into an image with high light-shade contrast by built-in programming software; an arm is controlled to hold a pen to paint a seen scenery or portrait based on a complex algorithm, so that the artistic painting becomes vivid; and then the paint amount of an inflator is controlled by a line segment interpolation algorithm and a running speed, thereby realizing a clear and coherent painting effect. According to the invention, the advanced computer software, computer vision system and the artificial intelligence are combined. When the provided robot is used for replacing the manual operation, time consumed for painting one picture by the robot is less than ten minutes and the speed is much faster than the street artist. Meanwhile, the interestingness of artistic ornament production is enhanced and the application market of the artistic decoration industry can be seized. Therefore, the provided robot has the wide application range and broad development prospects.",project-academic
10.1109/BIGDATA.2013.6691545,2013-12-23,p,IEEE,the berkeley data analytics stack present and future," The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications requires a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowdsourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark inmemory computation framework, and the Shark query processing system. BDAS shows up prominently in many industry discussions of the future of the Big Data analytics ecosystem - a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving ""up the stack"" to better integrate and support deep machine learning and to make people a full-fledged resource for making sense of Big Data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe the current state of BDAS with an emphasis on the key components listed above and will address our current efforts on machine learning scalability and ease of use, and hybrid human/computer processing. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.",project-academic
,2019-12-19,a,,metamorphic testing for object detection systems," Recent advances in deep neural networks (DNNs) have led to object detectors that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based object detection as a standard computer vision service, object detection systems - similar to traditional software - may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users of these object detection systems. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, principled, systematic methods for testing object detection systems do not yet exist, despite their importance. 
To fill this critical gap, we introduce the design and realization of MetaOD, the first metamorphic testing system for object detectors to effectively reveal erroneous detection results by commercial object detectors. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of object detection results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. Evaluated on four commercial object detection services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection defects in these object detectors. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is increased significantly, from an mAP score of 9.3 to an mAP score of 10.5.",project-academic
10.1016/J.DSS.2015.02.003,2015-04-01,p,North-Holland,cost sensitive and ensemble based prediction model for outsourced software project risk prediction," Nowadays software is mainly developed through outsourcing and it has become one of the most important business practice strategies for the software industry. However, outsourcing projects are often affiliated with high failure rate. Therefore to ensure success in outsourcing projects, past research has aimed to develop intelligent risk prediction models to evaluate the success rate and cost-effectiveness of software projects. In this study, we first summarized related work over the past 20years and observed that all existing prediction models assume equal misclassification costs, neglecting actual situations in the management of software projects. In fact, overlooking project failure is far more serious than the misclassification of a success-prone project as a failure. Moreover, ensemble learning, a technique well-recognized to improve prediction performance in other fields, has not yet been comprehensively studied in software project risk prediction. This study aims to close the research gaps by exploring cost-sensitive analysis and classifier ensemble methods. Comparative analysis with T-test on 60 different risk prediction models using 327 outsourced software project samples suggests that the ideal model is a homogeneous ensemble model of decision trees (DT) based on bagging. Interestingly, DT underperformed Support Vector Machine (SVM) in accuracy (i.e., assuming equal misclassification cost), but outperformed in cost-sensitive analysis under the proposed framework. In conclusion, this study proposes the first cost-sensitive and ensemble-based hybrid modeling framework (COSENS) for software project risk prediction. In addition, it establishes a new rigorous evaluation standard for assessing software risk prediction models by considering misclassification costs. Display Omitted The first cost-sensitive and ensemble framework to predict software project riskA comprehensive T-test method was used for rigorous performance comparison.A total of 60 models were built and compared based on 327 real project samples.Decision tree underperformed SVM in accuracy, but outperformed in cost analysis.A new rigorous model standard for software project risk analysis is established.",project-academic
,2011-04-18,b,,numerical analysis," Computational science is fundamentally changing how technological questions are addressed. The design of aircraft, automobiles, and even racing sailboats is now done by computational simulation. The mathematical foundation of this new approach is numerical analysis, which studies algorithms for computing expressions defined with real numbers. Emphasizing the theory behind the computation, this book provides a rigorous and self-contained introduction to numerical analysis and presents the advanced mathematics that underpin industrial software, including complete details that are missing from most textbooks.Using an inquiry-based learning approach, Numerical Analysis is written in a narrative style, provides historical background, and includes many of the proofs and technical details in exercises. Students will be able to go beyond an elementary understanding of numerical simulation and develop deep insights into the foundations of the subject. They will no longer have to accept the mathematical gaps that exist in current textbooks. For example, both necessary and sufficient conditions for convergence of basic iterative methods are covered, and proofs are given in full generality, not just based on special cases.The book is accessible to undergraduate mathematics majors as well as computational scientists wanting to learn the foundations of the subject.Presents the mathematical foundations of numerical analysis Explains the mathematical details behind simulation software Introduces many advanced concepts in modern analysis Self-contained and mathematically rigorous Contains problems and solutions in each chapter Excellent follow-up course to Principles of Mathematical Analysis by Rudin",project-academic
,2020-08-18,b,,climate econometrics an overview," Climate econometrics is a new sub-discipline that has grown rapidly over the last few years. As greenhouse gas emissions like carbon dioxide (CO2), nitrous oxide (N2O) and methane (CH4) are a major cause of climate change, and are generated by human activity, it is not surprising that the tool set designed to empirically investigate economic outcomes should be applicable to studying many empirical aspects of climate change. Economic and climate time series exhibit many commonalities. Both data are subject to non-stationarities in the form of evolving stochastic trends and sudden distributional shifts. Consequently, the well-developed machinery for modeling economic time series can be fruitfully applied to climate data. In both disciplines, we have imperfect and incomplete knowledge of the processes actually generating the data. As we donât know that data generating process (DGP), we must search for what we hope is a close approximation to it. The data modeling approach adopted at Climate Econometrics (http://www.climateeconometrics.org/) is based on a model selection methodology that has excellent properties for locating an unknown DGP nested within a large set of possible explanations, including dynamics, outliers, shifts, and non-linearities. The software we use is a variant of machine learning which implements multi-path block searches commencing from very general specifications to discover a well-specified and undominated model of the processes under analysis. To do so requires implementing indicator saturation estimators designed to match the problem faced, such as impulse indicators for outliers, step indicators for location shifts, trend indicators for trend breaks, multiplicative indicators for parameter changes, and indicators specifically designed for more complex phenomena that have a common reaction âshapeâ like the impacts of volcanic eruptions on temperature reconstructions. We also use combinations of these, inevitably entailing settings with more candidate variables than observations. Having described these econometric tools, we take a brief excursion into climate science to provide the background to the later applications. By noting the Earthâs available atmosphere and water resources, we establish that humanity really can alter the climate, and is doing so in myriad ways. Then we relate past climate changes to the âgreat extinctionsâ seen in the geological record. Following the Industrial Revolution in the mid-18th century, building on earlier advances in scientific, technological and medical knowledge, real income levels per capita have risen dramatically globally, many killer diseases have been tamed, and human longevity has approximately doubled. However, such beneficial developments have led to a global explosion in anthropogenic emissions of greenhouse gases. These are also subject to many relatively sudden shifts from major wars, crises, resource discoveries, technology and policy interventions. Consequently, stochastic trends, large shifts and numerous outliers must all be handled in practice to develop viable empirical models of climate phenomena. Additional advantages of our econometric methods for doing so are detecting the impacts of important policy interventions as well as improved forecasts. The econometric approach we outline can handle all these jointly, which is essential to accurately characterize non-stationary observational data. Few approaches in either climate or economic modeling consider all such effects jointly, but a failure to do so leads to mis-specified models and hence incorrect theory evaluation and policy analyses. We discuss the hazards of modeling wide-sense non-stationary data (namely data not just with stochastic trends but also distributional shifts), which also serves to describe our notation. The application of the methods is illustrated by two detailed modeling exercises. The first investigates the causal role of CO2 in Ice Ages, where a simultaneous-equations system is developed to characterize land ice volume, temperature and atmospheric CO2 levels as non-linear functions of measures of the Earthâs orbital path round the Sun. The second turns to analyze the United Kingdomâs highly non-stationary annual CO2 emissions over the last 150 years, walking through all the key modeling stages. As the first country into the Industrial Revolution, the UK is one of the first countries out, with per capita annual CO2 emissions now below 1860âs levels when our data series begin, a reduction achieved with little aggregate cost. However, very large decreases in all greenhouse gas emissions are still required to meet the UKâs 2050 target set by its Climate Change Act in 2008 of an 80% reduction from 1970 levels, since reduced to a net zero target by that date, as required globally to stabilize temperatures. The rapidly decreasing costs of renewable energy technologies offer hope of further rapid emission reductions in that area, illustrated by a dynamic scenario analysis.",project-academic
10.31548/ENERGIYA2018.05.034,2018-10-23,a,National University of Life and Environmental Sciences of Ukraine,визначення нештатних ситуацій на підприємствах харчової промисловості та розробка системи підтримки прийнятя рішень," Solving the problems of production management at food industry enterprises with modern and promising methods requires the use of fundamentally new approaches, which is caused by a sharp complication of both the management objects themselves and the more stringent requirements for the efficiency of management of technological objects. For food production is characterized by instability of the process due to changes in the influence of parameters on its course. There are a number of measures to stabilize the conditions of the technological process, but they can only reduce instability, so the control algorithm should change during the process depending on the current situation in production. Therefore, the management of the Electrotechnological complex of food production as a complex object should be systematically coordinated not only in accordance with the goals, objectives, resources and expected results, but also in the efficiency and effectiveness of interaction in the real world of an emergency situation. None The authors propose a systematic approach to the solvable problem, in which the entire cycle of the technological process passing from the input stream of raw materials, including the treatment of wastewater from production, to the decision-making process, is investigated. An important element of the system of forecasting and decision-making is a database in which knowledge is accumulated on the basis of consideration of unusual situations occurring at the enterprises of the food industry. In this case, the frequency of industrial accidents is not so great and the same type, so that there is a real opportunity to train and train the personnel responsible for making decisions, and the consequences of non-optimal solutions can be significant. In limited time, the use of standard instructions on personnel actions in emergency situations turns out to be ineffective, since only an intelligent system of forecasting and decision-making can, in view of the current state of the electrotechnological process and equipment, model and propose optimal solutions. Therefore, the task of creating a system for controlling the technological complex of food production, based on systems of artificial intelligence with prediction and support of decision-making in emergency situations, is relevant. The urgency of this work is also determined by the fact that efficient control systems of the Electrotechnological Complex give an opportunity in various branches of the food industry to ensure the production of high quality products with a significant reduction in the cost of its production. None Thus, despite the measures to maintain a given technological mode of operation and minimize the losses of the electrical technology complex of an enterprise, the human factor often interferes with the production process. The presence of automation and control systems at the control and operator points can lead to both positive and negative results. Any wrong decision of the dispatcher to control automation systems can lead to significant costs of raw materials, financial losses and environmental disasters. Especially critical is the human factor in an emergency situation, in which, in addition to the psychological component, the time factor acts. Since abnormal situations in food production occur frequently, and their consequences significantly affect the resource and energy efficiency of production, there is an urgent need to predict and recognize an abnormal situation, to provide information support to dispatch personnel, to take urgent and adequate measures to localize it. None The purpose of the research is to determine the classification of emergency situations resulting from the transition from the standard to the emergency mode of the electro-technological complex of food production and to develop a decision support system for monitoring in real time in order to localize the predicted or detected emergency situations. None As a result of the analysis of the functioning of the technological complex of food production, the following causes of the emergence of abnormal situations have been identified: None - termination of supply of energy - due to a sudden disconnection of electricity or reduction of gas pressure in the pipeline supply at the gas-separating point; None - structural violations - associated with any structural violation in the work (failure of electrotechnical equipment, pipeline impulse, etc.); None - parametric deviations - any violation of the given electrotechnical mode of operation, recorded as a deviation of the parameters of the operating modes of the control system. None The various situations of the non-regular regime indicate the practical necessity of rational actions in conditions of uncertainty of different nature and multi-factor risks. The main idea of the developed management strategy is to ensure the timely and reliable detection and recognition of the TcxV in the real conditions, assessment of risk factors, forecasting their development during a certain operating mode, and on this basis ensuring timely elimination of the causes of the risks before failures and other unwanted consequences. None In control activities, along with the basic function of control of the technological process, a significant role is taken by decision-making function. In this regard, the task is to develop and improve the information-algorithmic provision of the monitoring system of the enterprise's electrical processes in real time in order to localize predicted or detected by monitoring of unusual situations. None The software and hardware complex of the monitoring and decision support system (DSS) is developed, which for the normal functioning should receive data SCADA-system of automated production management. The DSS system predicts the possibility of an emergency situation, determines the existence of a non-emergency situation, is able to simulate the possible risks and consequences and issue recommendations to the dispatcher. The basic part of the DSS is the module for analyzing the impressions of process technology sensors, and the main part of the DSS dispatch should be the ability to predict the risks and consequences of the use of a decision of the controller. None The main task of the monitoring complex is to reduce unplanned production stoppages and simple equipment in case of emergency situations due to the forecasting of the operation of the electrotechnological complex and the increase in the speed of the controller's reaction. The use of traditional multicriteria optimization methods for making decisions on managing electrotechnological systems of food production is impossible, due to the need to process and analyze large volumes of poorly structured information. Thanks to the creation of decision support systems, it is possible to obtain effective methods of analysis and forecasting in the field of complex electrotechnological processes characterized by large volumes of information that is poorly formalized by logical conclusion procedures for decision-making. For effective functioning of the developed system it is necessary to fully implement automated workplaces of all the specialists of the enterprise. The developed decision support system will work only in the advisor-consultant mode, without any action, while the dispatcher can completely ignore the message system and act independently, but the recommendations of the DSS can significantly improve the quality of acceptable solutions.",project-academic
10.1016/J.KNOSYS.2017.07.007,2017-10-01,a,Elsevier,fit evaluation of virtual garment try on by learning from digital pressure data," A remote garment fit evaluation model using machine-learning technique is proposed to estimate garment fit without any real try-on.Digital clothing pressures, generated from a 3D garment CAD software, were taken into account during the remote garment fit evaluation.Our proposed model has significance in garment e-shopping. Presently, garment fit evaluation mainly focuses on real try-on, and rarely deals with virtual try-on. With the rapid development of E-commerce, there is a profound growth of garment purchases through the internet. In this context, fit evaluation of virtual garment try-on is vital in the clothing industry. In this paper, we propose a Naive Bayes-based model to evaluate garment fit. The inputs of the proposed model are digital clothing pressures of different body parts, generated from a 3D garment CAD software; while the output is the predicted result of garment fit (fit or unfit). To construct and train the proposed model, data on digital clothing pressures and garment real fit was collected for input and output learning data respectively. By learning from these data, our proposed model can predict garment fit rapidly and automatically without any real try-on; therefore, it can be applied to remote garment fit evaluation in the context of e-shopping. Finally, the effectiveness of our proposed method was validated using a set of test samples. Test results showed that digital clothing pressure is a better index than ease allowance to evaluate garment fit, and machine learning-based garment fit evaluation methods have higher prediction accuracies.",project-academic
10.1109/IECON.2018.8591641,2018-10-01,p,IEEE,integration patterns for interfacing software agents with industrial automation systems," Agent-based systems, an approach derived from distributed artificial intelligence, have been introduced for designing large complex systems. They are also suitable to solve challenging problems in industrial environments, being an appropriate technology for realizing cyber-physical systems. In such configuration, they need to interface with automation and control devices. However, until now there is no widely accepted practice nor pattern to interface the software agents with the automation functions. This work addresses this issue and introduces corresponding integration patterns in order to achieve full interoperability and reusability. This work, therefore, provides a methodology for mapping existing practices into a set of generic templates and also discusses the applicability of the proposed approach to different industrial application domains.",project-academic
,2019-01-15,,,robot obstacle avoidance trajectory planning method and system based on deep learning," The invention provides a robot obstacle avoidance trajectory planning method and system based on deep learning. The robot obstacle avoidance trajectory planning method based on the deep learning comprises the following steps: adding a camera to a simulation environment, taking images from multiple angles and simultaneously inputting the images into a convolutional neural network; obtaining information of a robot arm updated angle according to the input information, and calling a simulation software to update through an interface to obtain a posture; and performing convolutional neural networktraining by means of the deep learning, transferring an obtained characteristic pattern to a one-dimensional vector after convolution operation is performed on the input images, inputting the one-dimensional vector into a subsequent fully connected layer to obtain a q value corresponding to each action, selecting the action with the largest q value and updating the posture, and sending the updatedposture to the simulation environment to obtain a new image input, and executing circularly until the target point is reached. The invention can realize the autonomous obstacle avoidance of an industrial robot and improve the industrial automation production capacity.",project-academic
10.1109/IC-ETITE47903.2020.450,2020-02-01,p,IEEE,an iot based smart water quality monitoring system using cloud," The Internet of Things (IoT) is the network of physical devices, vehicles, home appliances, and other items embedded with electronics, software, sensors, actuators and connectivity which enables these things to connect and exchange data. The number of IoT devices has increased 31% year-over-year to 8.4 billion in 2017 and it is estimated that there will be 30 billion devices by 2020. Water pollution is a major environmental problem in India. The largest source of water pollution in India is untreated sewage. Other sources of pollution include agricultural runoff and unregulated small scale industry that results in polluting, most of the rivers, lakes and surface water in India. In this paper, An IoT Based Smart Water Quality Monitoring System using Cloud and Deep Learningis proposedto monitor the quality of the water in water-bodies. In conventional systems, the monitoring process involves the manual collection of sample water from various regions, followed by laboratory testing and analysis. This process is ineffective, as this process is arduous and time-consuming and it does not provide real-time results. The quality of water should be monitored continuously, to ensure the safe supply of water from any water bodies and water resources. Hence, the design and development of a low-cost system for real-time monitoring of water quality using the Internet of Things (IoT) is essential. Monitoring water quality in water bodies using Internet of Things (IoT) helps in combating environmental issues and improving the health and living standards of all living things. The proposed system monitors the quality of water relentlessly with the help of IoT devices, such as, NodeMCU. The in-built Wi-Fi module is attached in NodeMCU which enables internet connectivity transfers the measured data from sensors to the Cloud. The prototype is designed in such a way that it can monitor the number of pollutants in the water. Multiple sensors are used to measure various parameters to assess the quality of water from water bodies. The results are stored in the Cloud, deep learning techniques are used to predict whether the water suitable or not.",project-academic
10.1109/TC.2020.2968888,2020-06-01,a,IEEE,accurate cost estimation of memory systems utilizing machine learning and solutions from computer vision for design automation," Hardware/software co-designs are usually defined at high levels of abstractions at the beginning of the design process in order to provide a variety of options on how to realize a system. This allows for design exploration which relies on knowing the costs of different design configurations (with respect to hardware usage and firmware metrics). To this end, methods for cost estimation are frequently applied in industrial practice. However, currently used methods oversimplify the problem and ignore important features, leading to estimates which are far off from real values. In this article, we address this problem for memory systems. To this end, we borrow and re-adapt solutions based on None Machine Learning None (ML) which have been found suitable for problems from the domain of None Computer Vision None (CV). Based on that, an approach is proposed which outperforms existing methods for cost estimation. Experimental evaluations within an industrial context show that, while the accuracy of the state-of-the-art approach is frequently off by more than 20 percent for area estimation and more than 15 percent for firmware estimation, the method proposed in this article comes rather close to the actual values (just 5–7 percent off for both area and firmware). Furthermore, our approach outperforms existing methods for scalability, generalization, and decrease in manual effort.",project-academic
10.1109/TII.2021.3053128,2021-01-20,a,IEEE,a survey on deep learning for data driven soft sensors," Soft sensors are widely constructed in process industry to realize process monitoring, quality prediction, and many other important applications. With the development of hardware and software, industrial processes have embraced new characteristics, which lead to the poor performance of traditional soft sensor modeling methods. Deep learning, as a kind of data-driven approach, shows its great potential in many fields, as well as in soft sensing scenarios. After a period of development, especially in the last five years, many new issues have emerged that need to be investigated. Therefore, in this article, the necessity and significance of deep learning for soft sensor applications are demonstrated first by analyzing the merits of deep learning and the trends of industrial processes. Next, mainstream deep learning models, tricks, and frameworks/toolkits are summarized and discussed to help designers propel the developing progress of soft sensors. Then, existing works are reviewed and analyzed to discuss the demands and problems occurred in practical applications. Finally, outlook and conclusions are given.",project-academic
10.1109/ACCESS.2020.2979015,2020-03-06,a,IEEE,quantum gis based descriptive and predictive data analysis for effective planning of waste management," Waste has a direct impact on human health and the surrounding environment. Apart from the health aspect, many industries' growth is effected by waste material such as the food industry. Waste management authorities are interested in reducing the cost of waste management operations and searching for sustainable waste management solutions. For effective planning of waste management, reliable data analysis is required to produce results that can facilitate the planning process. Data mining and machine learning-based data analysis over the waste data can produce a more detailed, and in-time waste information generation, which can lead to effectively manage the waste amount of specific area. In this paper, a descriptive data analysis approach, along with predictive analysis, is used to produce in-time waste information. The performance of the proposed approach is evaluated using a real waste dataset of Jeju Island, South Korea. Waste bins are virtualized on its actual location on the Jeju map in Quantum Geographic Information Systems(QGIS) software. The performance results of the predictive analysis models are evaluated in terms of Mean Absolute Error(MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error(MAPE). Performance results indicate that predictive analysis models are reliable for the effective planning and optimization of waste management operations.",project-academic
10.1016/J.PROCS.2016.06.107,2016-01-01,a,Elsevier,optimizing basic cocomo model using simplified genetic algorithm," Abstract None None The estimation of software effort is an essential and crucial activity for the software development life cycle. In recent years, many researchers and software industries have given significant attention on the estimation of software effort. In industry, effort is used for planning, budgeting and development time calculation. Therefore a realistic effort estimation is required. Many researchers have proposed various models for software effort estimation, such as statistical models, algorithmic models, machine learning based models and nature inspired models in the past. In this research paper, a simplified genetic algorithm based model is proposed. A simplified genetic algorithm is used for optimizing the parameters of the basic COCOMO model. The proposed approach is applied on NASA software project dataset. Experimental results show better realistic estimation over the basic COCOMO.",project-academic
,2014-08-13,,,intelligent substation secondary equipment operation and maintenance decision support system and data analysis method," The invention relates to an intelligent substation secondary equipment operation and maintenance decision support system and a data analysis method. The system comprises a decision support center, a knowledge base, a data analysis manager, a massive data acquisition unit and an industrial Ethernet switch. For the system and the method, the computer software technology, the intelligent device communication interface technology, the data mining technology, the database technology, the artificial intelligence technology and the visualization technology are adopted to realize automatic analysis on massive operating data of digitized secondary equipment, the data analysis methods such as clustering analysis, classification analysis, correlation analysis and exception analysis are utilized to disclose the potential logical relationship from the massive non-logic relational data, so that the intelligent substation secondary equipment operation and maintenance operation is simplified, standardized and automated, the existing digitized secondary equipment operation and maintenance decision and data analysis modes are greatly improved, and a blank in the field of digitized secondary equipment operation and maintenance decision is filled up.",project-academic
10.1145/3358185,2019-10-07,a,"ACMPUB27New York, NY, USA",multi objective exploration for practical optimization decisions in binary translation," In the design of mobile systems, hardware/software (HW/SW) co-design has important advantages by creating specialized hardware for the performance or power optimizations. Dynamic binary translation (DBT) is a key component in co-design. During the translation, a dynamic optimizer in the DBT system applies various software optimizations to improve the quality of the translated code. With dynamic optimization, optimization time is an exposed run-time overhead and useful analyses are often restricted due to their high costs. Thus, a dynamic optimizer needs to make smart decisions with limited analysis information, which complicates the design of optimization decision models and often causes failures in human-made heuristics. In mobile systems, this problem is even more challenging because of strict constraints on computing capabilities and memory size.To overcome the challenge, we investigate an opportunity to build practical optimization decision models for DBT by using machine learning techniques. As the first step, loop unrolling is chosen as the representative optimization. We base our approach on the industrial strength DBT infrastructure and conduct evaluation with 17,116 unrollable loops collected from 200 benchmarks and real-life programs across various domains. By utilizing all available features that are potentially important for loop unrolling decision, we identify the best classification algorithm for our infrastructure with consideration for both prediction accuracy and cost. The greedy feature selection algorithm is then applied to the classification algorithm to distinguish its significant features and cut down the feature space. By maintaining significant features only, the best affordable classifier, which satisfies the budgets allocated to the decision process, shows 74.5% of prediction accuracy for the optimal unroll factor and realizes an average 20.9% reduction in dynamic instruction count during the steady-state translated code execution. For comparison, the best baseline heuristic achieves 46.0% prediction accuracy with an average 13.6% instruction count reduction. Given that the infrastructure is already highly optimized and the ideal upper bound for instruction reduction is observed at 23.8%, we believe this result is noteworthy.",project-academic
10.1088/1757-899X/114/1/012071,2016-02-01,a,IOP Publishing,e learning application for machine maintenance process using iterative method in xyz company," XYZ Company is a company based on manufacturing part for airplane, one of the machine that is categorized as key facility in the company is Millac 5H6P. As a key facility, the machines should be assured to work well and in peak condition, therefore, maintenance process is needed periodically. From the data gathering, it is known that there are lack of competency from the maintenance staff to maintain different type of machine which is not assigned by the supervisor, this indicate that knowledge which possessed by maintenance staff are uneven. The purpose of this research is to create knowledge-based e-learning application as a realization from externalization process in knowledge transfer process to maintain the machine. The application feature are adjusted for maintenance purpose using e-learning framework for maintenance process, the content of the application support multimedia for learning purpose. QFD is used in this research to understand the needs from user. The application is built using moodle with iterative method for software development cycle and UML Diagram. The result from this research is e-learning application as sharing knowledge media for maintenance staff in the company. From the test, it is known that the application make maintenance staff easy to understand the competencies.",project-academic
10.1007/BF00209184,1996-12-01,a,Kluwer Academic Publishers,classification techniques for metric based software development," Managing software development and maintenance projects requires predictions about components of the software system that are likely to have a high error rate or that need high development effort. The value of any classification is determined by the accuracy and cost of such predictions. The paper investigates the hypothesis whether fuzzy classification applied to criticality prediction provides better results than other classification techniques that have been introduced in this area. Five techniques for identifying error-prone software components are compared, namely Pareto classification, crisp classification trees, factor-based discriminant analysis, neural networks, and fuzzy classification. The comparison is illustrated with experimental results from the development of industrial real-time projects. A module quality model — with respect to changes — provides both quality of fit (according to past data) and predictive accuracy (according to ongoing projects). Fuzzy classification showed best results in terms of overall predictive accuracy.",project-academic
10.1109/ITHET.2012.6246058,2012-06-21,p,IEEE,virtual industrial training joining innovative interfaces with plant modeling," Training in industry is one of the most critical and expensive tasks to be faced by the management. Furthermore, in some cases, it is dangerous or even impossible to directly train operators on the real plants where security and safety problems may arise, making it very difficult to start training programs at low cost. For these reasons, the field of training in industry is rapidly developing using software or hardware solutions coming mainly from the following research areas: i) Human-Computer interaction, i.e., the use of complex and interactive human-machine interfaces, ii) plant simulators, i.e., software systems which are delivered with the plant itself to test and to learn complex tasks and processes, iii) Intelligent Training Systems, i.e., the availability of intelligent and personalized training systems where a virtual tutor guides users through a personalized learning path. In this paper we present the overall architecture of a system for industrial training, embedded into an Intelligent Tutoring System that can provide more effective and personalized training and learning in a context where working directly on real plants can be difficult and very expensive. In particular we present a simulator for training operators in using power plants, based on a multimedia and on interactive interface. This system is particularly suitable to be used for training in industrial electric and oil plants. Moreover, the system allows operators for collaborative problem solving. Currently the system is under delivery to an Italian Electric industry.",project-academic
,2018-09-01,a,National Science Foundation,inter disciplinary research challenges in computer systems for the 2020s," The broad landscape of new technologies currently being explored makes the current times very exciting for computer systems research. The community is actively researching an extensive set of topics, ranging from the small (e.g., energy-independent embedded devices) to the large (e.g., brain-scale deep learning), simultaneously addressing technology discontinuities (End of Moore's Law and EnergyWall), new challenges in security and privacy, and the rise of artificial intelligence (AI).

While industry is applying some of these technologies, its efforts are necessarily focused on only a few areas, and on relatively short-term horizons. This offers academic researchers the opportunity to attack the problems with a broader and longer-term view. Further, in recent times, the computer systems community has started to pay increasing attention to non-performance measures, such as security, complexity, and power. To make progress in this multi-objective world, the composition of research teams needs to change. Teams have to become inter-disciplinary, enabling the flow of ideas across computing fields.

While many research directions are interesting, this report outlines a few high-priority areas where inter-disciplinary research is likely to have a high payoff:

a) Developing the components for a usable planet-scale Internet of Things (IoT), with provably energy-efficient devices. This report envisions a highly-available, geographically distributed, heterogeneous large-scale IoT system with the same efficiency, maintainability, and usability as today's data centers. This planet-scale IoT will be populated by many computationally-sophisticated IoT devices that are ultra-low power and operate energy-independently.

b) Rethinking the hardware-software security contract in the age of AI. In light of the recent security vulnerabilities, this report argues for building hardware abstractions that communicate security guarantees, and for allowing software to communicate its security and privacy requirements to the hardware. Further, security and privacy mechanisms should be integrated into the disruptive emerging technologies that support AI.

c) Making AI a truly dependable technology that is usable by all the citizens in all settings. As AI frameworks automate an increasing number of critical operations, this report argues for end-to-end dependable AI, where both the hardware and the software are understood and verified. Further, AI needs to turn from a centralized tool into a capability easily usable by all the citizens in all settings to meet an ever expanding range of needs.

d) Developing solutions to tackle extreme complexity, possibly based on formal methods. This report argues for the need to tame the explosion of system complexity and heterogeneity by creating new abstractions and complexity-management solutions. Such solutions need to be accessible to domain experts. An important step towards this goal is to scale out and extend formal methods for the real world.

This report also describes other, related research challenges.",project-academic
10.1145/3359992.3366641,2019-12-09,p,ACM,unsupervised machine learning for network centric anomaly detection in iot," Industry 4.0 holds the promise of greater automation and productivity but also introduces new security risks to critical industrial control systems from unsecured devices and machines. Networks need to play a larger role in stopping attacks before they disrupt essential infrastructure as host-centric IT security solutions, such as anti-virus and software patching, have been ineffective in preventing IoT devices from getting compromised. We propose a network-centric, behavior-learning based, anomaly detection approach for securing such vulnerable environments. We demonstrate that the predictability of TCP traffic from IoT devices can be exploited to detect different types of DDoS attacks in real-time, using unsupervised machine learning (ML). From a small set of features, our ML classifier can separate normal and anomalous traffic. Our approach can be incorporated in a larger system for identifying compromised end-points despite IP spoofing, thus allowing the use of SDN-based mechanisms for blocking attack traffic close to the source. Compared to supervised ML methods, our unsupervised ML approaches are easier to instrument and are more effective in detecting new and unseen attacks.",project-academic
10.3390/SU10093142,2018-09-01,a,MDPI AG,a systematic review of smart real estate technology drivers of and barriers to the use of digital disruptive technologies and online platforms," Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology’s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders—consumers, agents and associations, government and regulatory authorities, and complementary industries—and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers’ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers’ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports.",project-academic
10.1007/S00170-021-08156-2,2021-11-01,a,Springer London,tool path continuity determination based on machine learning method," Computer-aided manufacturing (CAM) software outputs machining data by encoding a tool-path into a series of G-codes which are composed of various lengths of line segments. The discontinuities of these line segments may cause inefficiency for computer numerical control (CNC) system. To achieve high-speed continuous motions, corner smoothing algorithms based on look-ahead methods are widely used. However, it is difficult to meet smoothing trajectories in real-time requirements. Based on machine learning method, in this paper, a support vector machine (SVM) system is presented for directly outputting classification results of the various geometric continuities at the transition corners. The feature values used for generating continuity classification model are extracted from sampling paths of the previous publication work: the machining parameters, length, fairness criteria, the root mean square (RMS) contour errors, and dominant stage type of movement of each sampling path are calculated. The acceleration/deceleration (ACC/DEC) feedrate planning scheme is used to determine the feedrate at the transition corners. Simulations and experiments show that the proposed algorithm can realize accurately and efficiently continuity classification in real-time requirements under the conditions of machining accuracy.",project-academic
10.1007/S10664-013-9254-Z,2014-12-01,a,Springer US,software process evaluation a machine learning framework with application to defect management process," Software process evaluation is important to improve software development and the quality of software products in a software organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they usually suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In this study, we mainly focus on the procedure aspect of software processes, and formulate the problem as a sequence (with additional information, e.g., time, roles, etc.) classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to evaluate the execution of a software process more objectively. To validate the efficacy of our approach, we apply it to evaluate the execution of a defect management (DM) process in nine real industrial software projects. Our empirical results show that our approach is effective and promising in providing a more objective and quantitative measurement for the DM process evaluation task. Furthermore, we conduct a comprehensive empirical study to compare our proposed machine learning approach with an existing conventional approach (i.e., artifacts inspection). Finally, we analyze the advantages and disadvantages of both approaches in detail.",project-academic
10.1007/978-3-319-95270-3_20,2018-06-24,a,"Springer, Cham",virtual reality as a tool for the cascade control learning," This work presents the development of an interactive didactic system for the learning of the cascade control technique of industrial processes, which uses an immersive virtual environment for the emulation of a plant behavior which can be monitored and controlled through an HMI (Human-Machine Interface). The virtual environment has been created through Computer Aided Design software and a graphic engine and, in addition, peripheral devices are used to achieve the immersion and interaction with the environment. Finally, the experimental results that validate the system when it is tested with a real or simulated process are presented.",project-academic
10.1109/AINS47559.2019.8968698,2019-11-01,p,IEEE,cyber security risk assessment on industry 4 0 using ics testbed with ai and cloud," Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures.",project-academic
10.1109/59.589783,1997-05-01,a,IEEE,security boundary visualization for systems operation," This paper presents a security assessment approach for operational planning studies that provides the operator with accurate boundary visualization in terms of easily monitored precontingency information. The approach is modeled after traditional security assessment procedures which result in use of a nomogram for characterizing the security boundaries; these procedures are common among many North American utilities today. Therefore, the approach builds on what is already familiar in the industry, but it takes advantage of computer automation and neural networks for generating and understanding large databases. The appeal of the approach is threefold: it provides increased accuracy in boundary representation, it reduces the labor traditionally required in generating security boundaries, and the resulting boundaries, encoded in fast, flexible C subroutines, can be integrated into energy management system software to provide the operator with compact, understandable boundary illustration in real time. These improvements are of particular interest in securely operating transmission systems close to their limits so as to fully utilize existing facilities.",project-academic
10.1016/J.FLUID.2013.08.018,2013-11-25,a,Elsevier,utilization of support vector machine to calculate gas compressibility factor," Abstract None None The compressibility factor ( Z -factor) is considered as a very important parameter in the petroleum industry because of its broad applications in PVT characteristics. In this study, a meta-learning algorithm called Least Square Support Vector Machine (LSSVM) was developed to predict the compressibility factor. In addition, the proposed technique was examined with previous models, exhibiting an None R None 2 None and an None MSE None of 0.999 and 0.000014, respectively. A significant drawback in the conventional LSSVM is the determination of optimal parameters to attain desired output with a reasonable accuracy. To eliminate this problem, the current study introduced coupled simulated annealing (CSA) algorithm to develop a new model, known as CSA-LSSVM. The proposed algorithm included 4756 datasets to validate the effectiveness of the CSA-LSSVM model using statistical criteria. The new technique can be utilized in chemical and petroleum engineering software packages where the most accurate value of None Z -factor is required to predict the behavior of real gas, significantly affecting design aspects of equipment involved in gas processing plants.",project-academic
,2018-03-30,a,,learning beyond human expertise with generative models for dental restorations," Computer vision has advanced significantly that many discriminative approaches such as object recognition are now widely used in real applications. We present another exciting development that utilizes generative models for the mass customization of medical products such as dental crowns. In the dental industry, it takes a technician years of training to design synthetic crowns that restore the function and integrity of missing teeth. Each crown must be customized to individual patients, and it requires human expertise in a time-consuming and labor-intensive process, even with computer-assisted design software. We develop a fully automatic approach that learns not only from human designs of dental crowns, but also from natural spatial profiles between opposing teeth. The latter is hard to account for by technicians but important for proper biting and chewing functions. Built upon a Generative Adversar-ial Network architecture (GAN), our deep learning model predicts the customized crown-filled depth scan from the crown-missing depth scan and opposing depth scan. We propose to incorporate additional space constraints and statistical compatibility into learning. Our automatic designs exceed human technicians' standards for good morphology and functionality, and our algorithm is being tested for production use.",project-academic
10.1007/978-3-030-49556-5_6,2019-11-14,a,"Springer, Cham",exploring the performance bound of cambricon accelerator in end to end inference scenario," Deep learning algorithms have become pervasive in a broad range of industrial application scenarios. DianNao/Cambricon family is a set of energy-efficient hardware accelerators for machine learning, especially for deep learning, covering from edge embedded devices to cloud data centers. However, in the real application scenario, the complicated software stack and the extra overhead (memory copy) hinder the full exploitation of the accelerator performance. In this paper, we try to explore the performance bound of Cambricon accelerator MLU100 in end-to-end deep learning inference scenarios (from data/model load to inference results store). We leverage the offline model to bypass the general deep learning framework, use the multiple threads programming to fully exploit the parallelism of the multi-core accelerator and apply specific data structure to decrease the memory copy overhead. The evaluation results show that, for RetNet-50 on CIFAR-10 dataset, our optimization methods are 32.09\(\times \) faster than the baseline of the optimized batch size (64), and achieve \(85\%\) of the performance upper-bound on the Cambricon MLU100 board.",project-academic
10.3390/SU11226527,2019-11-19,a,Multidisciplinary Digital Publishing Institute,new artificial neural networks model for predicting rate of penetration in deep shale formation," Rate of penetration (ROP) means how fast the drilling bit is drilling through the formations. It is known that in the petroleum industry, most of the well cost is taken by the drilling operations. Therefore, it is very crucial to drill carefully and improve drilling processes. Nevertheless, it is challenging to predict the influence of every single parameter because most of the drilling parameters depend on each other and altering an individual parameter will have an impact on the rest. Due to the complexity of the drilling operations, up to the present time, there is no reliable model that can adequately estimate the ROP. Artificial intelligence (AI) might be capable of building a predictive model from a number of input parameters that correlate to the output parameter. A real field dataset, of shale formation, that contains records of both drilling parameters such as, rotation per minute (RPM), weight on bit (WOB), drilling torque (τ), standpipe pressure (SPP) and flow pump (Q) and mud properties such as, mud weight (MW), funnel and plastic viscosities (FV) (PV), solid (%) and yield point (YP) were used to predict ROP using artificial neural network (ANN). A comparison between the developed ANN-ROP model and the number of selected published ROP models were performed. A novel empirical equation of ROP using the above-mentioned parameters was derived based on ANN technique which is able to estimate ROP with excellent precision (correlation coefficient (R) of 0.996 and average absolute percentage error (AAPE) of 5.776%). The novel ANN-based correlation outperformed three published empirical models and it can be used to predict the ROP without the need for artificial intelligence software.",project-academic
,2021-06-10,a,,ai enabled automation for completeness checking of privacy policies," Technological advances in information sharing have raised concerns about data protection. Privacy policies contain privacy-related requirements about how the personal data of individuals will be handled by an organization or a software system (e.g., a web service or an app). In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). A prerequisite for GDPR compliance checking is to verify whether the content of a privacy policy is complete according to the provisions of GDPR. Incomplete privacy policies might result in large fines on violating organization as well as incomplete privacy-related software specifications. Manual completeness checking is both time-consuming and error-prone. In this paper, we propose AI-based automation for the completeness checking of privacy policies. Through systematic qualitative methods, we first build two artifacts to characterize the privacy-related provisions of GDPR, namely a conceptual model and a set of completeness criteria. Then, we develop an automated solution on top of these artifacts by leveraging a combination of natural language processing and supervised machine learning. Specifically, we identify the GDPR-relevant information content in privacy policies and subsequently check them against the completeness criteria. To evaluate our approach, we collected 234 real privacy policies from the fund industry. Over a set of 48 unseen privacy policies, our approach detected 300 of the total of 334 violations of some completeness criteria correctly, while producing 23 false positives. The approach thus has a precision of 92.9% and recall of 89.8%. Compared to a baseline that applies keyword search only, our approach results in an improvement of 24.5% in precision and 38% in recall.",project-academic
10.1109/TSE.2021.3124332,2021-06-10,a,Institute of Electrical and Electronics Engineers (IEEE),ai enabled automation for completeness checking of privacy policies," Technological advances in information sharing have raised concerns about data protection. Privacy policies contain privacy-related requirements about how the personal data of individuals will be handled by an organization or a software system (e.g., a web service or an app). In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). A prerequisite for GDPR compliance checking is to verify whether the content of a privacy policy is complete according to the provisions of GDPR. Incomplete privacy policies might result in large fines on violating organization as well as incomplete privacy-related software specifications. Manual completeness checking is both time-consuming and error-prone. In this paper, we propose AI-based automation for the completeness checking of privacy policies. Through systematic qualitative methods, we first build two artifacts to characterize the privacy-related provisions of GDPR, namely a conceptual model and a set of completeness criteria. Then, we develop an automated solution on top of these artifacts by leveraging a combination of natural language processing and supervised machine learning. Specifically, we identify the GDPR-relevant information content in privacy policies and subsequently check them against the completeness criteria. To evaluate our approach, we collected 234 real privacy policies from the fund industry. Over a set of 48 unseen privacy policies, our approach detected 300 of the total of 334 violations of some completeness criteria correctly, while producing 23 false positives. The approach thus has a precision of 92.9% and recall of 89.8%. Compared to a baseline that applies keyword search only, our approach results in an improvement of 24.5% in precision and 38% in recall.",project-academic
10.23919/DATE.2019.8714961,2019-03-01,p,IEEE,accurate cost estimation of memory systems inspired by machine learning for computer vision," Hardware/software co-designs are usually defined at high levels of abstractions at the beginning of the design process in order to allow plenty of options how to eventually realize a system. This allows for design exploration which in turn heavily relies on knowing the costs of different design configurations (with respect to hardware usage as well as firmware metrics). To this end, methods for cost estimation are frequently applied in industrial practice. However, currently used methods for cost estimation oversimplify the problem and ignore important features – leading to estimates which are far off from the real values. In this work, we address this problem for memory systems. To this end, we borrow and re-adapt solutions based on Machine Learning (ML) which have been found suitable for problems from the domain of Computer Vision (CV) – in particular age determination of persons depicted in images. We show that, for an ML approach, age determination from the CV domain is actually very similar to cost estimation of a memory system.",project-academic
10.1145/1536274.1536292,2009-05-01,p,ACM,sw development projects in academia," A significant amount of current software engineering research is conducted within the context of computer science and computing departments or colleges. Similarly, software engineering degree programs are being developed by such academic units as well as within engineering colleges [1]. However, every computer or computing department has its own experiences, successes or pitfalls in software engineering (SE) and software development (SD) teaching, which would be useful to share and discuss with the education community. In this paper we discuss the experiences and results from four years of teaching ""Projects in Computer Science"" in Computer Information Systems (CIS) Diploma and ""Software Engineering"" in Bachelor of Computer Information Systems (BCIS) Degree programs [2] at Okanagan College. Also we provide analysis and evaluation for several already finished and current projects. The class teaching in both programs was organized into two parts. The learning of SD in the CIS as well as SE in the BCIS programs were synchronized with the practical SD and SE projects with real sponsors from industry and academia in small and medium size groups of students (3--6 members in SD and 5--11 members in SE projects). Students developed their final projects incorporating four main Rational Unified Process phases [3] and 4--7 short iterations typical to the Agile SD process. Additionally, in the SE projects students learned and used Extreme Programming (XP) iterative process [4]. In several projects students successfully combined Software prototyping [5] with Agile SD and Rapid Application Development (RAD) tools [6]. Instructors supervised and supported students in the role of sponsors or mediators. Many student groups were able to develop impressive, high quality final project applications and systems. The sponsors provided very positive feedback and references for most of the projects.",project-academic
10.1007/S10664-021-09993-1,2021-09-01,a,Springer US,ai lifecycle models need to be revised an exploratory study in fintech," Tech-leading organizations are embracing the forthcoming artificial intelligence revolution. Intelligent systems are replacing and cooperating with traditional software components. Thus, the same development processes and standards in software engineering ought to be complied in artificial intelligence systems. This study aims to understand the processes by which artificial intelligence-based systems are developed and how state-of-the-art lifecycle models fit the current needs of the industry. We conducted an exploratory case study at ING, a global bank with a strong European base. We interviewed 17 people with different roles and from different departments within the organization. We have found that the following stages have been overlooked by previous lifecycle models: data collection, feasibility study, documentation, model monitoring, and model risk assessment. Our work shows that the real challenges of applying Machine Learning go much beyond sophisticated learning algorithms – more focus is needed on the entire lifecycle. In particular, regardless of the existing development tools for Machine Learning, we observe that they are still not meeting the particularities of this field.",project-academic
,2020-10-03,a,,ai lifecycle models need to be revised an exploratory study in fintech," Tech-leading organizations are embracing the forthcoming artificial intelligence revolution. Intelligent systems are replacing and cooperating with traditional software components. Thus, the same development processes and standards in software engineering ought to be complied in artificial intelligence systems. This study aims to understand the processes by which artificial intelligence-based systems are developed and how state-of-the-art lifecycle models fit the current needs of the industry. We conducted an exploratory case study at ING, a global bank with a strong European base. We interviewed 17 people with different roles and from different departments within the organization. We have found that the following stages have been overlooked by previous lifecycle models: data collection, feasibility study, documentation, model monitoring, and model risk assessment. Our work shows that the real challenges of applying Machine Learning go much beyond sophisticated learning algorithms - more focus is needed on the entire lifecycle. In particular, regardless of the existing development tools for Machine Learning, we observe that they are still not meeting the particularities of this field.",project-academic
,2019-03-14,a,,gym gazebo2 a toolkit for reinforcement learning using ros 2 and gazebo," This paper presents an upgraded, real world application oriented version of gym-gazebo, the Robot Operating System (ROS) and Gazebo based Reinforcement Learning (RL) toolkit, which complies with OpenAI Gym. The content discusses the new ROS 2 based software architecture and summarizes the results obtained using Proximal Policy Optimization (PPO). Ultimately, the output of this work presents a benchmarking system for robotics that allows different techniques and algorithms to be compared using the same virtual conditions. We have evaluated environments with different levels of complexity of the Modular Articulated Robotic Arm (MARA), reaching accuracies in the millimeter scale. The converged results show the feasibility and usefulness of the gym-gazebo 2 toolkit, its potential and applicability in industrial use cases, using modular robots.",project-academic
10.1109/ICPC.2019.00018,2019-05-25,p,IEEE,measuring the cognitive load of software developers a systematic mapping study," Context: In recent years, several studies explored different facets of the developers' cognitive load while executing tasks related to software engineering. Researchers have proposed and assessed different ways to measure developers' cognitive load at work and some studies have evaluated the interplay between developers' cognitive load and other attributes such as productivity and software quality. Problem: However, the body of knowledge about developers' cognitive load measurement is still dispersed. That hinders the effective use of developers' cognitive load measurements by industry practitioners and makes it difficult for researchers to build new scientific knowledge upon existing results. Objective: This work aims to pinpoint gaps providing a classification and a thematic analysis of studies on the measurement of cognitive load in the context of software engineering. Method: We carried out a Systematic Mapping Study (SMS) based on well-established guidelines to investigate nine research questions. In total, 33 articles (out of 2,612) were selected from 11 search engines after a careful filtering process. Results: The main findings are that (1) 55% of the studies adopted electroencephalogram (EEG) technology for monitoring the cognitive load; (2) 51% of the studies applied machine-learning classification algorithms for predicting cognitive load; and (3) 48% of the studies measured cognitive load in the context of programming tasks. Moreover, a taxonomy was derived from the answers of research questions. Conclusion: This SMS highlighted that the precision of machine learning techniques is low for realistic scenarios, despite the combination of a set of features related to developers' cognitive load used on these techniques. Thus, this gap makes the effective integration of the measure of developers' cognitive load in industry still a relevant challenge.",project-academic
10.15406/IRATJ.2019.05.00182,2019-05-16,p,"MedCrave Group, LLC",system retraining to professional competences of cognitive robots on basis of communicative associative logic of technological thinking, There are two main approaches to hardware software realization of imitative thinking of cognitive robots First approach is machine learning Such cognitive robots are used in services industry for the commercial and entertaining purposes In article approach to creation of cognitive robots on the basis of modeling of communicative and associative logic of imitative thinking of the person is considered Cognitive robots on the basis of modeling of communicative and associative logic of imitative thinking of the person are used as lecturers and consultants Also in professional activity when concepts and competences are strictly defined The communicative and associative logic of thinking allows to create the symbolical conceiving robot capable to study realize information requirements tasks to train subject areas to communicate with the help of the speech to read and write in various languages The robot on the basis of symbolical language communicative logic solves a problem of the automated imitation of imitative thinking with associative and communicative symbolical language elements of knowledge The main practical objectives of imitation of imitative thinking are drawing up the intrinsic focused dictionaries of the developed subject domains of knowledge and standard information requirements drawing up standard procedures of realization of information requirements formation of networks from communicative and associative symbolical language elements of knowledge of subject domains expansion of a natural language to functional creation of systems of speech and text communication in a natural language and recognitions of the speech of interlocutors In article the system of retraining to professional competences of robots androids is considered on the basis of communicative associative logic of technological thinking by cognitive methods,project-academic
,2011-01-01,a,,soft computing techniques for software project effort estimation," The effort invested in a software project is probably one of the most important and most analyzed variables in recent years in the process of project management. The limitation of algorithmic effort prediction models is their inability to cope with uncertainties and imprecision surrounding software projects at the early development stage. More recently attention has turned to a variety of machine learning methods, and soft computing in particular to predict software development effort. Soft computing is a consortium of methodologies centering in fuzzy logic, artificial neural networks, and evolutionary computation. It is important, to mention here, that these methodologies are complementary and synergistic, rather than competitive. They provide in one form or another flexible information processing capability for handling real life ambiguous situations. These methodologies are currently used for reliable and accurate estimate of software development effort, which has always been a challenge for both the software industry and academia. The aim of this study is to analyze soft computing techniques in the existing models and to provide in depth review of software and project estimation techniques existing in industry and literature based on the different test datasets along with their strength and weaknesses.",project-academic
10.1145/3394885.3431620,2021-01-18,p,ACM,efficient computing platform design for autonomous driving systems," Autonomous driving is becoming a hot topic in both academic and industrial communities. Traditional algorithms can hardly achieve the complex tasks and meet the high safety criteria. Recent research on deep learning shows significant performance improvement over traditional algorithms and is believed to be a strong candidate in autonomous driving system. Despite the attractive performance, deep learning does not solve the problem totally. The application scenario requires that an autonomous driving system must work in real-time to keep safety. But the high computation complexity of neural network model, together with complicated pre-process and post-process, brings great challenges. System designers need to do dedicated optimizations to make a practical computing platform for autonomous driving. In this paper, we introduce our work on efficient computing platform design for autonomous driving systems. In the software level, we introduce neural network compression and hardware-aware architecture search to reduce the workload. In the hardware level, we propose customized hardware accelerators for pre- and post-process of deep learning algorithms. Finally, we introduce the hardware platform design, NOVA-30, and our on-vehicle evaluation project.",project-academic
10.1108/PM-06-2016-0027,2017-10-12,a,Emerald Publishing Limited,artificial neural network in property valuation application framework and research trend," Purpose




The predictive accuracy and reliability of artificial intelligence models, such as the artificial neural network (ANN), has led to its application in property valuation studies. However, a large percentage of such previous studies have focused on the property markets in developed economies, and at the same time, effort has not been put into documenting its research trend in the real estate domain. The purpose of this paper is to critically review the studies that adopted ANN for property valuation in order to present an application guide for researchers and practitioners, and also establish the trend in this research area.




Design/methodology/approach




Relevant articles were retrieved from online databases and search engines and were systematically analyzed. First, the background, the construction and the strengths and weaknesses of the technique were highlighted. In addition, the trend in this research area was established in terms of the country of origin of the articles, the year of publication, the affiliations of the authors, the sample size of the data, the number of the variables used to develop the models, the training and testing ratio, the model architecture and the software used to develop the models.




Findings




The analysis of the retrieved articles shows that the first study that applied ANN in property valuation was published in 1991. Thereafter, the technique received more attention from 2000. While a quarter of the articles reviewed emanated from the USA, the rest were conducted in mostly developed countries. Most of the studies were conducted by universities scholars, while very few industry practitioners participated in the research works. Also, the predictive accuracy of the ANN technique was reported in most of the papers reviewed, but a few reported otherwise.




Research limitations/implications




The articles that are not indexed in the search engines and databases searched and also not available in the public domain might not have been captured in this study.




Practical implications




The findings of this study reveal a gap between the valuation practice in developed and developing property markets and also the contributions of real estate practitioners and universities scholars to real estate research. A paradigm shift in the valuation practice in developing nations could lead to achieving a sustainable international valuation practice.




Originality/value




This paper presents the trend in this research area that could be useful to real estate researchers and practitioners in different property markets around the world. The findings of this study could also encourage collaboration between industry professionals and researchers domiciled in both developed and developing countries.",project-academic
10.1109/ICST.2019.00029,2019-04-22,p,IEEE,directing a search towards execution properties with a learned fitness function," Search based software testing is a popular and successful approach both in academia and industry. SBST methods typically aim to increase coverage whereas searching for executions with specific properties is largely unresearched. Fitness functions for execution properties often possess search landscapes that are difficult or intractable. We demonstrate how machine learning techniques can convert a property that is not searchable, in this case crashes, into one that is. Through experimentation on 6000 C programs drawn from the Codeflaws repository, we demonstrate a strong, program independent correlation between crashing executions and library function call patterns within those executions as discovered by a neural net. We then exploit the correlation to produce a searchable fitness landscape to modify American Fuzzy Lop, a widely used fuzz testing tool. On a test set of previously unseen programs drawn from Codeflaws, a search strategy based on a crash targeting fitness function outperformed a baseline in 80.1% of cases. The experiments were then repeated on three real world programs: the VLC media player, and the libjpeg and mpg321 libraries. The correlation between library call traces and crashes generalises as indicated by ROC AUC scores of 0.91, 0.88 and 0.61. The produced search landscape however is not convenient due to plateaus. This is likely because these programs do not use standard C libraries as often as do those in Codeflaws. This limitation can be overcome by considering a more powerful observation domain and a broader training corpus in future work. Despite limited generalisability of the experimental setup, this research opens new possibilities in the intersection of machine learning, fitness functions, and search based testing in general.",project-academic
10.1016/J.COMPSTRUCT.2020.112514,2020-10-15,a,Elsevier,machine learning in composites manufacturing a case study of automated fiber placement inspection," Abstract None None The large-scale adoption of composite materials in industry has allowed for a greater freedom in design and function of structures and their respective components. However, the freedom of material choice has resulted in increased complexity in manufacturing. Machine learning (ML) and Artificial Intelligence (AI) are currently being explored for a number of advanced manufacturing applications, and their applicability has begun to extend into the composites manufacturing realm. In this document, a comprehensive overview of machine learning applications in composites manufacturing will be presented with discussions on a novel inspection software developed for the Automated Fiber Placement (AFP) process at the University of South Carolina utilizing an ML vision system. This vision system allows for defect data to be fully integrated into the manufacturing process, allowing for the ML inspection system to influence several chains in the composites product lifecycle management.",project-academic
10.1109/ISSREW.2016.23,2016-10-01,p,IEEE,programming the network application software faults in software defined networks," Software-defined networking (SDN) is a key new paradigm emerging in the industry, in which networks can be dynamically reconfigured in real-time through software. SDN networks are also being used in conjunction with cloud computing to extend virtualization and elasticity to the network level and as a foundation for the Internet of Things (IoT). A key concept in SDN is the separation of the network control and data planes, together with an application plane that supports the programming of network applications in general-purpose languages such as Java and Python. These network applications can be developed by an enterprise, service provider or vendor, or purchased from third-parties through SDN application stores. While the programmability of SDN provides tremendous flexibility and adaptability to changing network conditions and demands, it also exposes networks to significant vulnerabilities through software faults in network applications, as well as in the control and data planes. In this paper, we demonstrate how faulty SDN applications can compromise other SDN applications or even crash an entire SDN network, and describe relationships between software faults in SDN applications and design faults in SDN controllers. We also show how machine-learning based anomaly detection and analytics can be used to identify SDN software faults and help guide real-time network response, through a proof-of-concept case study.",project-academic
10.1016/J.JSS.2019.05.026,2019-09-01,a,Elsevier,sentiment based approval prediction for enhancement reports," Abstract None None The maintenance and evolution of the software application is a continuous phase in the industry. Users are frequently proposing enhancement requests for further functionalities. However, although only a small part of these requests are finally adopted, developers have to go through all of such requests manually, which is tedious and time consuming. To this end, in this paper we propose a sentiment based approach to predict how likely enhancement reports would be approved or rejected so that developers can first handle likely-to-be-approved requests. This could help the software applications to compete in the industry by upgrading their features in time as per user’s requirements. First, we preprocess enhancement reports using natural language preprocessing techniques. Second, we identify the words having positive and negative sentiments in the summary attribute of the enhancements reports and calculate the sentiment of each enhancement report. Finally, with the history data of real software application, we train a machine learning based classifier to predict whether a given enhancement report would be approved. The proposed approach has been evaluated with the history data from real software applications. The cross-application validation suggests that the proposed approach outperforms the state-of-the-art. The evaluation results suggest that the proposed approach increases the accuracy from 70.94% to 77.90% and improves the F-measure significantly from 48.50% to 74.53%.",project-academic
10.1016/J.IFACOL.2016.11.160,2016-01-01,a,Elsevier,neural networks as a diagnosing tool for industrial level measurement through non contacting radar type and support to the decision for its better application," Abstract: None None The aim of this study was to develop an analysis tool based on artificial neural networks (ANN) to detect level measurement problems with free wave propagation radars. The trend of using this type of radar has been growing in the last ten years mainly because of its easy installation on the top of tanks and reservoirs, and for its low rate maintenance comparing to other level measurement technologies. For the experiments, a Rosemount radar was used and the training of the neural network was based on the data from the software Radar Master. Therefore, some network topologies in different scenarios were tested and it was possible to demonstrate the efficiency of the ANN with accuracy rate between 94.44 to 100% for the first experiment with networks using 10, 20 or 50 neurons in the hidden layer. This technique was applied in a real industrial application, a sugar and ethanol mill, and accuracy rate was about 87,0 to 96,1%. This methodology can be applied to asset management software for diagnosis report or troubleshooting which would increase the level measurement reliability and plant safety.",project-academic
10.1109/INFOCOMWKSHPS50562.2020.9162720,2020-02-11,p,IEEE,reinforcement learning for scalable and reliable power allocation in sdn based backscatter heterogeneous network," Backscatter heterogeneous networks are expected to usher a new era of massive connectivity of low-powered devices. With the integration of software-defined networking (SDN), such networks hold the promise to be a key enabling technology for massive Internet-of-things (IoT) due to myriad applications in industrial automation, healthcare, and logistics management. However, there are many aspects of SDN-based backscatter heterogeneous networks that need further development before practical realization. One of the challenging aspects is the high level of interference due to the reuse of spectral resources for backscatter communications. To partly address this issue, this article provides a reinforcement learning-based solution for effective interference management when backscatter tags coexist with other legacy devices in a heterogeneous network. Specifically, using reinforcement learning, the agents are trained to minimize the interference for macro-cell (legacy users) and small-cell (backscatter tags). Novel reward functions for both macro- and small-cells have been designed that help in controlling the transmission power levels of users. The results show that the proposed framework not only improves the performance of macro-cell users but also fulfills the quality of service requirements of backscatter tags by optimizing the long-term rewards.",project-academic
10.1109/PHM-PARIS.2019.00052,2019-05-01,p,IEEE,a common service middleware for intelligent complex software system," With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technology, various intelligent complex software systems (i-CSS) are increasingly popular, becoming one of the most important software system development paradigms. Its inherent growth construction and adaptive evolution properties pose new challenges to existing software design and development methods. Especially, how to achieve growth construction by quickly reusing existing excellent software resources, and how to establish data flow across system boundaries around the business flow to achieve adaptive evolution based on data intelligence. Facing the above challenges, this paper proposes novel data-oriented analysis and design method (DOAD), microservice and container-based mashup development method (SCMD). On this basis, the paper implements i-CSS common service middleware to support the above methods in engineering. In a real cloud-based PHM system and the other three industry projects, the proposed methods and middleware are used for application verification, the results show that they can greatly reduce the complexity of i-CSS design and development, reduce the ability threshold of the i-CSS development team, improve the development efficiency of the development team, reduce the team development workload by 31.5% on average, and help the i-CSS team effectively cope with the challenges of growth construction and adaptive evolution.",project-academic
10.2139/SSRN.3682048,2019-04-01,a,,optimized for addiction extending product liability concepts to defectively designed social media algorithms and overcoming the communications decency act," Over the past decade, social media has gained an ever more pervasive presence in our lives. But the social connection, real-time dissemination of information, and creative outlet it provides come with a cost that is invisible to most users: in order to increase site value, many platforms are designed to addict users and trap them in a cycle of dependence that can ravage their mental health and well-being. One of the most effective ways for platforms to maximize their value is by using recommendation systems, sophisticated software systems that utilize artificial intelligence to learn about the user and predict what kinds of content will keep them scrolling, clicking, and liking. This Comment sketches out a legal solution for a subset of the harms caused by these algorithms by arguing that product liability concepts should extend to social media platforms, who have managed to achieve a nearly impenetrable form of immunity due to § 230 of the Communications Decency Act (CDA). The first Part of this Comment provides an overview of relevant concepts, including the recommendation engines that drive social media, the mental health effects of social media use, and the origins and purpose of the CDA. Next, this Comment demonstrates that social media platforms qualify as products for the purposes of strict product liability and walks the reader through a hypothetical example of the risk-utility test. Finally, this Comment considers § 230 of the CDA and offers two avenues for circumventing its broad grant of immunity. Instead of joining the extensive body of literature lamenting the woes of the CDA, this Comment proposes an affirmative solution that enables victims to recover while holding the multi-billion-dollar social media industry to a higher standard and incentivizing greater levels of caution in its development process.",project-academic
,2019-12-10,a,,datamorphic testing a methodology for testing ai applications," With the rapid growth of the applications of machine learning (ML) and other artificial intelligence (AI) techniques, adequate testing has become a necessity to ensure their quality. This paper identifies the characteristics of AI applications that distinguish them from traditional software, and analyses the main difficulties in applying existing testing methods. Based on this analysis, we propose a new method called datamorphic testing and illustrate the method with an example of testing face recognition applications. We also report an experiment with four real industrial application systems of face recognition to validate the proposed approach.",project-academic
,2021-01-14,a,,ajalon simplifying the authoring of wearable cognitive assistants," Wearable Cognitive Assistance (WCA) amplifies human cognition in real time through a wearable device and low-latency wireless access to edge computing infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation tools that provide real-time step-by-step guidance, with prompt error detection and correction. WCA applications are likely to be transformative in education, health care, industrial troubleshooting, manufacturing, and many other areas. Today, WCA application development is difficult and slow, requiring skills in areas such as machine learning and computer vision that are not widespread among software developers. This paper describes Ajalon, an authoring toolchain for WCA applications that reduces the skill and effort needed at each step of the development pipeline. Our evaluation shows that Ajalon significantly reduces the effort needed to create new WCA applications.",project-academic
10.1145/3308560.3317711,2019-05-13,p,ACM,panel knowledge graph industry applications," This panel will focus on industry applications related to knowledge graph and showcase how knowledge graph transforms the conventional and unconventional industries to the new era of AI, ranging from innovations in medicine and healthcare, literature search, e-commerce, professional connections, to getting a ride. Panelists are: Senior Software Engineer/Research Scientist at Uber, Co-founder of Tinkerpop, specialized on real-time semantics, RDF streams and graph databases Head of AI at Genentech, passionate about modeling, and currently developing a general medical inference engine that can be applied to a wide variety of areas, from point of care decision support, triage, insurance risk managment to name a few A senior staff engineer/director at Business Platform Unit, Alibaba, leading product knowledge graph (PKG) team and Business Platform AI team. He and his team have built a huge PKG with 10 billion of entities. Data Scientist at Numedii, previously postdoctoral research fellow at Stanford University School of Medicine, developing novel methods to integrate and explore a broad set of biological and clinical data for scientific reproducibility and biomedical discovery. An accomplished technical scientist, innovator and R&D leader in cutting-edge technology research and product development. Proven track record of success (20+ years of successful professional career in leading R&D organizations) in leading rapid technological advancement, innovation, and highly competitive environments. Broad range of skills from initiating research breakthroughs to achieving marketable product development. Renowned expert and technological visionary in the fields of enterprise middleware, cloud computing, data centric computing, workload optimized systems and appliances, business analytics, big data, social media and multimedia, speech and natural language processing. Extended technical leadership in systems design for LinkedIn Economic Graph, Google Search and Google Research. Breadth and depth of expertise in building data systems and platforms. Technical leadership and management of software development in both start-up and large companies. A principal research staff member, and a senior manager at IBM Almaden Research Center. I manage the information management department, working on HTAP (hybrid transactional and analytical processing) systems, large scale machine learning, and natural language querying of data. A team lead and senior scientist at Bloomberg. He holds a PhD in computer science from the University of Amsterdam and has an extensive track record in artificial intelligence, information retrieval, knowledge graphs, natural language processing, and machine learning. Before joining Bloomberg he worked at Yahoo Labs on semantic search at web scale using the Yahoo Knowledge Graph. At Bloomberg he leads the team that is responsible for leveraging knowledge graph technology to drive advanced financial insights.",project-academic
10.1016/J.ADHOC.2020.102305,2021-02-01,p,Elsevier,a city wide experimental testbed for the next generation wireless networks," Abstract None None To facilitate research in dynamic spectrum access, 5G, vehicular networks, underground wireless communications, and radio frequency machine learning, a city-wide experimental testbed is developed to provide realistic radio environment, standardized experimental configurations, reusable datasets, and advanced computational resources. The testbed contains 5 cognitive radio sites, and covers 1.1 square miles across two campuses of the University of Nebraska-Lincoln and a public street in the city of Lincoln, Nebraska. Each site is equipped with a 4x4 MIMO software-defined radio transceiver with 20Gbps fronthaul connectivity. Additional cognitive radio transceivers with an underground 2x2 MIMO antenna are included in a site. High speed fronthaul network based on dedicated fiber connects the 5 sites to a cloud-based central unit for data processing and storage. The testbed provides researchers rich computational resources such as arrays of CPUs and GPUs at the cloud and FPGAs at both the edge and fronthaul network. Developed via the collaboration of the university, city, and industrial partners, this testbed will facilitate education and researches in academic and industrial communities.",project-academic
10.1016/J.COMMATSCI.2019.05.022,2019-05-18,a,Elsevier,high throughput thermodynamic calculations of phase equilibria in solidified 6016 al alloys," Abstract None None In the present work, high-throughput calculation (HTC) method is performed to obtain the phase equilibria of solidified 6016 Al-alloys. The calculations of primary phase fraction, precipitates fraction and phase composition are realized based on the Scheil-Gulliver solidification model of Thermo-Calc software, and the entire composition ranges of standard 6016 Al-alloy is taken into account. A Python-based program called Automatic Execution and Extraction Tasks (AEET) is developed, it automatically generate the commands of calculations, execute the Thermo-Calc software and then extract the key data of output files. The obtained results are listed in an Excel file, which is convenient for the subsequent visualization analysis and machine learning. Several criteria are combined to filter the appropriate compositions of industrial 6016 Al-alloys, providing a valuable guidance to the experimentalists and avoiding unnecessary trial-and-error tests. This HTC approach is not limited to the solidification modelings; it can be extended to any kinds of thermodynamic and kinetic calculations.",project-academic
10.1007/978-3-540-77465-5,2008-03-20,b,"Springer Publishing Company, Incorporated",soft computing applications in industry," Softcomputing techniques play a vital role in the industry. This book presents several important papers presented by some of the well-known scientists from all over the globe. The application domains discussed in this book include: agroecology, bioinformatics, branched fluid-transport network layout design, dam scheduling, data analysis and exploration, detection of phishing attacks, distributed terrestrial transportation, fault detection of motors, fault diagnosis of electronic circuits, fault diagnosis of power distribution systems, flood routing, hazard sensing, health care, industrial chemical processes, knowledge management in software development, local multipoint distribution systems, missing data estimation, parameter calibration of rainfall intensity models, parameter identification for systems engineering, petroleum vessel mooring, query answering in P2P systems, real-time strategy games, robot control, satellite heat pipe design, monsoon rainfall forecasting, structural design, tool condition monitoring, vehicle routing, water network design, etc. The softcomputing techniques presented in this book are on (or closely related to): ant-colony optimization, artificial immune systems, artificial neural networks, Bayesian models, case-based reasoning, clustering techniques, differential evolution, fuzzy classification, fuzzy neural networks, genetic algorithms, harmony search, hidden Markov models, locally weighted regression analysis, probabilistic principal component analysis, relevance vector machines, self-organizing maps, other machine learning and statistical techniques, and the combinations of the above techniques.",project-academic
10.1016/J.ESWA.2008.02.053,2009-03-01,a,"Pergamon Press, Inc.",software reliability identification using functional networks a comparative study," Software engineering development has gradually become essential element in different aspects of the daily life and an important factor in numerous critical real-industry applications, such as, nuclear plants, medical monitoring control, real-time military, bioinformatics, oil and gas industry, and air traffic control. This paper proposes a functional network as a novel computational intelligence scheme for tracking and predicting the software reliability. Several applications are presented to illustrate this new intelligent system framework models. To demonstrate the usefulness of functional networks and the existing data mining schemes, we briefly describe the learning algorithm of functional networks associativity model in predicting the software reliability. Comparative studies will be carried out to compare the performance of functional networks with the most popular existing data mining techniques, such as, statistical regression multilayer feed forward neural networks, and support vector machines. The results show that the performance of functional networks is more reliable, stable, accurate, and outperforms other techniques.",project-academic
10.1145/1989622.1989637,2011-05-06,p,ACM,international collaboration in sw engineering research projects," Many software engineering research projects are conducted within university computer science and computing departments or colleges. Every computer or computing department has its own experiences, successes or pitfalls in software engineering and software development teaching, which would be useful to share and discuss with the education community. In this paper we discuss the international software engineering research project experiences and results from five years of teaching ""Projects in Computer Science"" in Computer Information Systems Diploma and ""Software Engineering"" in Bachelor of Computer Information Systems Degree programs at Okanagan College. The learning of software development in the Diploma as well as software engineering in the Degree programs were synchronized with the practical software development and software engineering projects with real sponsors from industry and academia in small and medium size groups of students (3-6 members in Diploma and 5-11 members in Degree projects). Additionally to industrial projects we introduced a number of software engineering research projects from academia and industry, located worldwide in the last 5 years. Instructors supervised and supported students in the role of sponsors or mediators. Many student groups were able to develop impressive, high quality final engineering and research project applications and systems. The sponsors provided very positive feedback and references for most of the projects. We offered a greater number of real projects for students to select from, including international research projects from the United States, France, Ukraine and Canada. Several student teams chose research projects (as opposed to industrial projects), which will be briefly discussed in this paper.",project-academic
10.1007/978-3-319-26187-4_12,2015-11-01,p,"Springer, Cham",file relation graph based malware detection using label propagation," The rapid development of malicious software programs has posed severe threats to Computer and Internet security. Therefore, it motivates anti-malware industry to develop novel methods which are capable of protecting users against new threats. Existing malware detectors mostly treat the file samples separately using supervised learning algorithms. However, ignoring of relationship among file samples limits the capability of malware detectors. In this paper, we present a new malware detection method based on file relation graph to detect newly developed malware samples. When constructing file relation graph, k-nearest neighbors are chosen as adjacent nodes for each file node. Files are connected with edges which represent the similarity between the corresponding nodes. Label propagation algorithm, which propagates label information from labeled file samples to unlabeled files, is used to learn the probability that one unknown file is classified as malicious or benign. We evaluate the effectiveness of our proposed method on a real and large dataset. Experimental results demonstrate that the accuracy of our method outperforms other existing detection approaches in classifying file samples.",project-academic
,2018-11-16,,,robot industrial sorting method based on cloud terminal deep learning," The invention discloses a robot industrial sorting method based on cloud terminal deep learning, and relates to the field of robot industrial automation. Relatively simple item refining and sorting can be realized. The robot industrial sorting method includes an industrial camera, a computer, a UR5 robot (mechanical arm bodies and a control box). The Gige industrial camera, the computer, and the UR5 robot are connected by using a PCi-E interface, wherein a Halcon software is further installed in the computer, and the UR5 robot is connected by using a socket communication mode. A sorting program is written on a demonstrator of the UR5 robot, and the programming language on the demonstrator is a UR-specific UR script. The robot industrial sorting method is suitable for automatic sorting of robots.",project-academic
10.1109/METROIND4.0IOT48571.2020.9138288,2020-06-03,p,IEEE,introducing a cloud based architecture for the distributed analysis of real time ethernet traffic," The use of industrial communication protocols based on Real-Time Ethernet (RTE) standards is completely replacing traditional industrial fieldbuses. As usual, when a technology becomes mature, the need of efficient diagnostic and maintenance tools quickly raises. Very often, following the paradigm of Industry 4.0, the most effective diagnostic systems are today based on distributed, cloud-centric, architectures and artificial intelligence. However, the distributed analysis of RTE systems is challenging, considering the plurality of protocols and the stringent cost constrains which are common in industry. In this paper, a new architecture for the distributed analysis of RTE networks is proposed, leveraging on distributed probes that send traffic samples to Matlab cloud for remote analysis. The paper also proposes a software conversion tool to adapt general PCAP files captured by popular sniffers (e.g. Wireshark) into MAT file for easier Matlab elaboration. Last, a test bench for characterization (in terms of transfer delays) of the first part of the chain for RTE traffic sampling is described. The results show that in less than 10 seconds it is possible to transfer chunks of RTE traffic data (captured on industrial networks with hundreds of real-time devices) directly to the cloud and to have them converted in Matlab format.",project-academic
10.1109/ICSE-SEET.2019.00014,2019-05-27,p,IEEE,teaching internet of things iot literacy a systems engineering approach," The Internet of Things (IoT) invades our world with billions of smart, interconnected devices, all programmed to make our lives easier. For educators, teaching such a vast and dynamic field is both a necessity and a challenge. IoT-relevant topics such as programming, hardware, networking and artificial intelligence are already covered in core computing curricula. Does this mean that fresh graduates are well prepared to tackle complex IoT problems? Unfortunately, nothing could be further from the truth. The problem is that IoT devices are complex systems, where software, hardware, and humans interact with each other. From this interaction, unique behavior and hazardous situations can emerge that might easily stay undetected, unless systems are analyzed as a whole. This paper presents two differently flavored courses that teach IoT using a holistic, system-centric approach. The first is a broad introduction to Pervasive Computing, focused on the intelligence of ""Things"". The second is an advanced course that zooms on the process of testing a software-intensive system. The key characteristics of our approach are : (1) teaching only the bare essentials (topics needed for end-to-end engineering of a smart system), (2) a strong, hands-on project component, using microcontroller-based miniature systems, inspired by real-life, and (3) a rich partnership with industry and academic idea incubators. Positive student evaluations gathered during the last five years demonstrate that such an approach brings engagement, self-confidence and realism in IoT classrooms. We believe that this success can be replicated in other courses, by shifting the focus on different IoT-relevant aspects.",project-academic
10.1109/BIGDATA.2016.7840859,2016-12-01,p,IEEE Computer Society,building a research data science platform from industrial machines," Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions.",project-academic
10.1145/2608253,2014-06-01,a,ACM,performance portability across heterogeneous socs using a generalized library based approach," Because of tight power and energy constraints, industry is progressively shifting toward heterogeneous system-on-chip (SoC) architectures composed of a mix of general-purpose cores along with a number of accelerators. However, such SoC architectures can be very challenging to efficiently program for the vast majority of programmers, due to numerous programming approaches and languages. Libraries, on the other hand, provide a simple way to let programmers take advantage of complex architectures, which does not require programmers to acquire new accelerator-specific or domain-specific languages. Increasingly, library-based, also called algorithm-centric, programming approaches propose to generalize the usage of libraries and to compose programs around these libraries, instead of using libraries as mere complements.In this article, we present a software framework for achieving performance portability by leveraging a generalized library-based approach. Inspired by the notion of a component, as employed in software engineering and HW/SW codesign, we advocate nonexpert programmers to write simple wrapper code around existing libraries to provide simple but necessary semantic information to the runtime. To achieve performance portability, the runtime employs machine learning (simulated annealing) to select the most appropriate accelerator and its parameters for a given algorithm. This selection factors in the possibly complex composition of algorithms used in the application, the communication among the various accelerators, and the tradeoff between different objectives (i.e., accuracy, performance, and energy).Using a set of benchmarks run on a real heterogeneous SoC composed of a multicore processor and a GPU, we show that the runtime overhead is fairly small at 5.1p for the GPU and 6.4p for the multi-core. We then apply our accelerator selection approach to a simulated SoC platform containing multiple inexact accelerators. We show that accelerator selection together with hardware parameter tuning achieves an average 46.2p energy reduction and a speedup of 2.1× while meeting the desired application error target.",project-academic
10.1016/J.ENG.2019.02.013,2019-12-01,a,Elsevier,artificial intelligence in steam cracking modeling a deep learning algorithm for detailed effluent prediction," Abstract None None Chemical processes can benefit tremendously from fast and accurate effluent composition prediction for plant design, control, and optimization. The Industry 4.0 revolution claims that by introducing machine learning into these fields, substantial economic and environmental gains can be achieved. The bottleneck for high-frequency optimization and process control is often the time necessary to perform the required detailed analyses of, for example, feed and product. To resolve these issues, a framework of four deep learning artificial neural networks (DL ANNs) has been developed for the largest chemicals production process—steam cracking. The proposed methodology allows both a detailed characterization of a naphtha feedstock and a detailed composition of the steam cracker effluent to be determined, based on a limited number of commercial naphtha indices and rapidly accessible process characteristics. The detailed characterization of a naphtha is predicted from three points on the boiling curve and paraffins, iso-paraffins, olefins, naphthenes, and aronatics (PIONA) characterization. If unavailable, the boiling points are also estimated. Even with estimated boiling points, the developed DL ANN outperforms several established methods such as maximization of Shannon entropy and traditional ANNs. For feedstock reconstruction, a mean absolute error (MAE) of 0.3 wt% is achieved on the test set, while the MAE of the effluent prediction is 0.1 wt%. When combining all networks—using the output of the previous as input to the next—the effluent MAE increases to 0.19 wt%. In addition to the high accuracy of the networks, a major benefit is the negligible computational cost required to obtain the predictions. On a standard Intel i7 processor, predictions are made in the order of milliseconds. Commercial software such as COILSIM1D performs slightly better in terms of accuracy, but the required central processing unit time per reaction is in the order of seconds. This tremendous speed-up and minimal accuracy loss make the presented framework highly suitable for the continuous monitoring of difficult-to-access process parameters and for the envisioned, high-frequency real-time optimization (RTO) strategy or process control. Nevertheless, the lack of a fundamental basis implies that fundamental understanding is almost completely lost, which is not always well-accepted by the engineering community. In addition, the performance of the developed networks drops significantly for naphthas that are highly dissimilar to those in the training set.",project-academic
,2015-01-01,a,,use of artificial intelligence in software development life cycle a state of the art review," Abstract — Artificial Intelligence (AI) is the younger field in computer science ready to accept challenges. Software engineering (SE) is the dominating industrial field. So, automating SE is the most relevant challenge today. AI has the capacity to empower SE in that way. Here in this paper we present a state of the art literature review which reveals the past and present work done for automating Software Development Life Cycle (SDLC) using AI. Keywords — Artificial Intelligence, Code Generation, Requirements Engineering, SDLC, Software Design, Software Estimation, Software Testing. I. I NTRODUCTION The disciplines of artificial intelligence and software engineering have developed separately. There is not much exchange of research results between them. AI research techniques make it possible to perceive, reason and act. Research in software engineering is concerned with supporting engineers to developed better software in less period. Rech and Altoff(2008) say ""The disciplines of artificial intelligences and software engineering have many commonalities. Both deals with modeling real world objects from the real world like business process, expert knowledge, or process models."" Now a day's several research directions of both disciplines come closer together and are beginning to build new research areas. Software agents play an important role as research objects in distributed AI(DAI) as well as in Agent Oriented Software Engineering(AOSE). Knowledge-based System(KBS) are being examine for Learning Software Organizations (LSO) as well as Knowledge Engineering(KE). Ambient intelligence(AmI) a new research area for distributed, non-intrusive, and intelligent software system both from the direction of how to build these system as well as how to designed the collaboration between system. Lastly computational intelligence(CI) plays an important role in research about software analysis or project management as well as knowledge discovery in machine learning or databases.[1] Artificial Intelligence techniques, which aim to create software systems that exhibit some form of human intelligence, have been employed to assist or automate the activities in software engineering. Software inspections are been applied with great success to detect defects in different kinds of software documents such as specifications, design, test plans, or source code by many researchers.[2] Automated software engineering is a research area which is constantly developing new methodologies and technologies. It includes toolsets and frameworks based on mathematical models (theorem provers and model checkers), requirements-driven developments and reverse engineering (design, coding, verification validation), software management (configurations and projects), and code drivers (generators, analyzers, and visualizers). In the following sections we have tried to review some research techniques to automate each phase of software development life cycle using artificial intelligence.",project-academic
10.1016/J.ENGAPPAI.2017.06.014,2017-11-01,a,Pergamon,a configurable partial order planning approach for field level operation strategies of plc based industry 4 0 automated manufacturing systems," The machine and plant automation domain is faced with an ever increasing demand for ensuring the adaptability of manufacturing facilities in context of Industry 4.0. Field level automation software plays a dominant role in strengthening the overall flexibility of manufacturing resources. Classical programming approaches based typically on signal-oriented languages result in disproportionate effort for ensuring necessary flexibility. To address this challenge, a novel approach based on artificial intelligence planning techniques is presented which is able to handle domain specific requirements while facilitating efficient, scalable problem solving. Throughout this article, a discussion of specific requirements on automated planning techniques for field level automation software in the machine and plant automation domain with respect to Industry 4.0 is provided. An intensive study on existing works and their drawbacks towards addressing these requirements is presented. The proposed configurable partial-order planning approach is based upon a combination of an adapted goal-based planning formulation and its reformulation by means of linear programming techniques. It is shown that the proposed approach is able to efficiently solve large planning problems by exhibiting positive scalability characteristics which indicates its applicability for real-size plants.",project-academic
,2021-10-26,a,,deep dic deep learning based digital image correlation for end to end displacement and strain measurement," Digital image correlation (DIC) has become an industry standard to retrieve accurate displacement and strain measurement in tensile testing and other material characterization. Though traditional DIC offers a high precision estimation of deformation for general tensile testing cases, the prediction becomes unstable at large deformation or when the speckle patterns start to tear. In addition, traditional DIC requires a long computation time and often produces a low spatial resolution output affected by filtering and speckle pattern quality. To address these challenges, we propose a new deep learning-based DIC approach -- Deep DIC, in which two convolutional neural networks, DisplacementNet and StrainNet, are designed to work together for end-to-end prediction of displacements and strains. DisplacementNet predicts the displacement field and adaptively tracks the change of a region of interest. StrainNet predicts the strain field directly from the image input without relying on the displacement prediction, which significantly improves the strain prediction accuracy. A new dataset generation method is proposed to synthesize a realistic and comprehensive dataset including artificial speckle patterns, randomly generated displacement and strain fields, and deformed images based on the given deformation. Proposed Deep DIC is trained purely on a synthetic dataset, but designed to perform both on simulated and experimental data. Its performance is systematically evaluated and compared with commercial DIC software. Deep DIC gives highly consistent and comparable predictions of displacement and strain with those obtained from commercial DIC software, while it outperforms commercial software with very robust strain prediction even with large and localized deformation and varied pattern qualities.",project-academic
10.1016/J.JMSY.2020.06.012,2021-01-01,a,Elsevier,a digital twin to train deep reinforcement learning agent for smart manufacturing plants environment interfaces and intelligence," Abstract None None Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.",project-academic
10.3390/ELECTRONICS9030511,2020-01-01,a,Multidisciplinary Digital Publishing Institute,detecting sensor faults anomalies and outliers in the internet of things a survey on the challenges and solutions," The Internet of Things (IoT) has gained significant recognition to become a novel sensing paradigm to interact with the physical world in this Industry 4.0 era. The IoTs are being used in many diverse applications that are part of our life and is growing to become the global digital nervous systems. It is quite evident that in the near future, hundreds of millions of individuals and businesses with billions will have smart-sensors and advanced communication technology, and these things will expand the boundaries of current systems. This will result in a potential change in the way we work, learn, innovate, live and entertain. The heterogeneous smart sensors within the Internet of Things are indispensable parts, which capture the raw data from the physical world by being the first port of contact. Often the sensors within the IoT are deployed or installed in harsh environments. This inevitably means that the sensors are prone to failure, malfunction, rapid attrition, malicious attacks, theft and tampering. All of these conditions cause the sensors within the IoT to produce unusual and erroneous readings, often known as outliers. Much of the current research has been done in developing the sensor outlier and fault detection models exclusively for the Wireless Sensor Networks (WSN), and adequate research has not been done so far in the context of the IoT. Wireless sensor network’s operational framework differ greatly when compared to IoT’s operational framework, using some of the existing models developed for WSN cannot be used on IoT’s for detecting outliers and faults. Sensor faults and outlier detection is very crucial in the IoT to detect the high probability of erroneous reading or data corruption, thereby ensuring the quality of the data collected by sensors. The data collected by sensors are initially pre-processed to be transformed into information and when Artificially Intelligent (AI), Machine Learning (ML) models are further used by the IoT, the information is further processed into applications and processes. Any faulty, erroneous, corrupted sensor readings corrupt the trained models, which thereby produces abnormal processes or outliers that are significantly distinct from the normal behavioural processes of a system. In this paper, we present a comprehensive review of the detecting sensor faults, anomalies, outliers in the Internet of Things and the challenges. A comprehensive guideline to select an adequate outlier detection model for the sensors in the IoT context for various applications is discussed.",project-academic
10.1109/ACCESS.2019.2958284,2019-12-06,a,IEEE,on the generation of anomaly detection datasets in industrial control systems," In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people’s safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",project-academic
10.1016/J.SCS.2020.102252,2020-09-01,a,Elsevier,a deep learning based iot oriented infrastructure for secure smart city," Abstract None None In recent years, the Internet of Things (IoT) infrastructures are developing in various industrial applications in sustainable smart cities and societies such as smart manufacturing, smart industries. The Cyber-Physical System (CPS) is also part of IoT-oriented infrastructure. CPS has gained considerable success in industrial applications and critical infrastructure with a distributed environment. This system aims to integrate the physical world to computational facilities as cyberspace. However, there are many challenges, such as security and privacy, centralization, communication latency, scalability in such an environment. To mitigate these challenges, we propose a Deep Learning-based IoT-oriented infrastructure for a secure smart city where Blockchain provides a distributed environment at the communication phase of CPS, and Software-Defined Networking (SDN) establishes the protocols for data forwarding in the network. A deep learning-based cloud is utilized at the application layer of the proposed infrastructure to resolve communication latency and centralization, scalability. It enables cost-effective, high-performance computing resources for smart city applications such as the smart industry, smart transportation. Finally, we evaluated the performance of our proposed infrastructure. We compared it with existing methods using quantitative analysis and security and privacy analysis with different measures such as scalability and latency. The evaluation of our implementation results shows that performance is improved.",project-academic
