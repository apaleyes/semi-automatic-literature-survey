id,updated,published,title,summary,database
http://arxiv.org/abs/2202.10075v1,2022-02-21T09:37:28Z,2022-02-21T09:37:28Z,"ICSML: Industrial Control Systems Machine Learning inference framework
  natively executing on IEC 61131-3 languages","Industrial Control Systems (ICS) have played a catalytic role in enabling the
4th Industrial Revolution. ICS devices like Programmable Logic Controllers
(PLCs), automate, monitor and control critical processes in industrial, energy
and commercial environments. The convergence of traditional Operational
Technology (OT) with Information Technology (IT) has opened a new and unique
threat landscape. This has inspired defense research that focuses heavily on
Machine Learning (ML) based anomaly detection methods that run on external IT
hardware which means an increase in costs and the further expansion of the
threat landscape. To remove this requirement, we introduce the ICS Machine
Learning inference framework (ICSML) which enables the execution of ML models
natively on the PLC. ICSML is implemented in IEC 61131-3 code and works around
the limitations imposed by the domain-specific languages, providing a complete
set of components for the creation of fully fledged ML models in a way similar
to established ML frameworks. We then demonstrate a complete end-to-end
methodology for creating ICS ML models using an external framework for training
and ICSML for the PLC implementation. To evaluate our contributions we run a
series of benchmarks studying memory and performance and compare our solution
to the TFLite inference framework. Finally, to demonstrate the abilities of
ICSML and to verify its non-intrusive nature, we develop and evaluate a case
study of a real defense for process aware attacks against a Multi Stage Flash
(MSF) desalination plant.",arxiv
http://arxiv.org/abs/2202.09549v1,2022-02-19T08:21:56Z,2022-02-19T08:21:56Z,"Learning to Detect Slip with Barometric Tactile Sensors and a Temporal
  Convolutional Neural Network","The ability to perceive object slip via tactile feedback enables humans to
accomplish complex manipulation tasks including maintaining a stable grasp.
Despite the utility of tactile information for many applications, tactile
sensors have yet to be widely deployed in industrial robotics settings; part of
the challenge lies in identifying slip and other events from the tactile data
stream. In this paper, we present a learning-based method to detect slip using
barometric tactile sensors. These sensors have many desirable properties
including high durability and reliability, and are built from inexpensive,
off-the-shelf components. We train a temporal convolution neural network to
detect slip, achieving high detection accuracies while displaying robustness to
the speed and direction of the slip motion. Further, we test our detector on
two manipulation tasks involving a variety of common objects and demonstrate
successful generalization to real-world scenarios not seen during training. We
argue that barometric tactile sensing technology, combined with data-driven
learning, is suitable for many manipulation tasks such as slip compensation.",arxiv
http://arxiv.org/abs/2202.09113v1,2022-02-18T10:36:11Z,2022-02-18T10:36:11Z,How to Manage Tiny Machine Learning at Scale: An Industrial Perspective,"Tiny machine learning (TinyML) has gained widespread popularity where machine
learning (ML) is democratized on ubiquitous microcontrollers, processing sensor
data everywhere in real-time. To manage TinyML in the industry, where mass
deployment happens, we consider the hardware and software constraints, ranging
from available onboard sensors and memory size to ML-model architectures and
runtime platforms. However, Internet of Things (IoT) devices are typically
tailored to specific tasks and are subject to heterogeneity and limited
resources. Moreover, TinyML models have been developed with different
structures and are often distributed without a clear understanding of their
working principles, leading to a fragmented ecosystem. Considering these
challenges, we propose a framework using Semantic Web technologies to enable
the joint management of TinyML models and IoT devices at scale, from modeling
information to discovering possible combinations and benchmarking, and
eventually facilitate TinyML component exchange and reuse. We present an
ontology (semantic schema) for neural network models aligned with the World
Wide Web Consortium (W3C) Thing Description, which semantically describes IoT
devices. Furthermore, a Knowledge Graph of 23 publicly available ML models and
six IoT devices were used to demonstrate our concept in three case studies, and
we shared the code and examples to enhance reproducibility:
https://github.com/Haoyu-R/How-to-Manage-TinyML-at-Scale",arxiv
http://arxiv.org/abs/2202.08897v1,2022-02-17T21:00:59Z,2022-02-17T21:00:59Z,"Implementing Spiking Neural Networks on Neuromorphic Architectures: A
  Review","Recently, both industry and academia have proposed several different
neuromorphic systems to execute machine learning applications that are designed
using Spiking Neural Networks (SNNs). With the growing complexity on design and
technology fronts, programming such systems to admit and execute a machine
learning application is becoming increasingly challenging. Additionally,
neuromorphic systems are required to guarantee real-time performance, consume
lower energy, and provide tolerance to logic and memory failures. Consequently,
there is a clear need for system software frameworks that can implement machine
learning applications on current and emerging neuromorphic systems, and
simultaneously address performance, energy, and reliability. Here, we provide a
comprehensive overview of such frameworks proposed for both, platform-based
design and hardware-software co-design. We highlight challenges and
opportunities that the future holds in the area of system software technology
for neuromorphic computing.",arxiv
http://arxiv.org/abs/2202.10336v1,2022-02-15T03:34:56Z,2022-02-15T03:34:56Z,Artificial Intelligence for the Metaverse: A Survey,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.",arxiv
http://arxiv.org/abs/2202.06149v1,2022-02-12T21:43:08Z,2022-02-12T21:43:08Z,"Automatic Issue Classifier: A Transfer Learning Framework for
  Classifying Issue Reports","Issue tracking systems are used in the software industry for the facilitation
of maintenance activities that keep the software robust and up to date with
ever-changing industry requirements. Usually, users report issues that can be
categorized into different labels such as bug reports, enhancement requests,
and questions related to the software. Most of the issue tracking systems make
the labelling of these issue reports optional for the issue submitter, which
leads to a large number of unlabeled issue reports. In this paper, we present a
state-of-the-art method to classify the issue reports into their respective
categories i.e. bug, enhancement, and question. This is a challenging task
because of the common use of informal language in the issue reports. Existing
studies use traditional natural language processing approaches adopting
key-word based features, which fail to incorporate the contextual relationship
between words and therefore result in a high rate of false positives and false
negatives. Moreover, previous works utilize a uni-label approach to classify
the issue reports however, in reality, an issue-submitter can tag one issue
report with more than one label at a time. This paper presents our approach to
classify the issue reports in a multi-label setting. We use an off-the-shelf
neural network called RoBERTa and fine-tune it to classify the issue reports.
We validate our approach on issue reports belonging to numerous industrial
projects from GitHub. We were able to achieve promising F-1 scores of 81%, 74%,
and 80% for bug reports, enhancements, and questions, respectively. We also
develop an industry tool called Automatic Issue Classifier (AIC), which
automatically assigns labels to newly reported issues on GitHub repositories
with high accuracy.",arxiv
http://arxiv.org/abs/2202.04834v1,2022-02-10T04:47:47Z,2022-02-10T04:47:47Z,"Geometric Digital Twinning of Industrial Facilities: Retrieval of
  Industrial Shapes","This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",arxiv
http://arxiv.org/abs/2202.03028v1,2022-02-07T09:41:24Z,2022-02-07T09:41:24Z,QUARK: A Framework for Quantum Computing Application Benchmarking,"Quantum computing (QC) is anticipated to provide a speedup over classical HPC
approaches for specific problems in optimization, simulation, and machine
learning. With the advances in quantum computing toward practical applications,
the need to analyze and compare different quantum solutions increases. While
different low-level benchmarks for QC exist, these benchmarks do not provide
sufficient insights into real-world application-level performance. We propose
an application-centric benchmark method and the QUantum computing Application
benchmaRK (QUARK) framework to foster the investigation and creation of
application benchmarks for QC. This paper establishes three significant
contributions: (1) it makes a case for application-level benchmarks and
provides an in-depth ""pen and paper"" benchmark formulation of two reference
problems: robot path and vehicle option optimization from the industrial
domain; (2) it proposes the open-source QUARK framework for designing,
implementing, executing, and analyzing benchmarks; (3) it provides multiple
reference implementations for these two reference problems based on different
known, and where needed, extended, classical and quantum algorithmic approaches
and analyzes their performance on different types of infrastructures.",arxiv
http://arxiv.org/abs/2202.02813v2,2022-02-19T12:11:48Z,2022-02-06T16:29:15Z,A Coding Framework and Benchmark towards Compressed Video Understanding,"Most video understanding methods are learned on high-quality videos. However,
in real-world scenarios, the videos are first compressed before the
transportation and then decompressed for understanding. The decompressed videos
may have lost the critical information to the downstream tasks. To address this
issue, we propose the first coding framework for compressed video
understanding, where another learnable analytic bitstream is simultaneously
transported with the original video bitstream. With the dedicatedly designed
self-supervised optimization target and dynamic network architectures, this new
stream largely boosts the downstream tasks yet with a small bit cost. By only
one-time training, our framework can be deployed for multiple downstream tasks.
Our framework also enjoys the best of both two worlds, (1) high efficiency of
industrial video codec and (2) flexible coding capability of neural networks
(NNs). Finally, we build a rigorous benchmark for compressed video
understanding on three popular tasks over seven large-scale datasets and four
different compression levels. The proposed Understanding oriented Video Coding
framework UVC consistently demonstrates significantly stronger performances
than the baseline industrial codec.",arxiv
http://arxiv.org/abs/2201.12170v3,2022-02-16T07:33:20Z,2022-01-28T15:11:34Z,"Unsupervised Single-shot Depth Estimation using Perceptual
  Reconstruction","Real-time estimation of actual object depth is a module that is essential to
performing various autonomous system tasks such as 3D reconstruction, scene
understanding and condition assessment of machinery parts. During the last
decade of machine learning, extensive deployment of deep learning methods to
computer vision tasks has yielded approaches that succeed in achieving
realistic depth synthesis out of a simple RGB modality. While most of these
models are based on paired depth data or availability of video sequences and
stereo images, methods for single-view depth synthesis in a fully unsupervised
setting have hardly been explored. This study presents the most recent advances
in the field of generative neural networks, leveraging them to perform fully
unsupervised single-shot depth synthesis. Two generators for RGB-to-depth and
depth-to-RGB transfer are implemented and simultaneously optimized using the
Wasserstein-1 distance and a novel perceptual reconstruction term. To ensure
that the proposed method is plausible, we comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database and the SURREAL dataset that records body depth. The success observed
in this study suggests the great potential for unsupervised single-shot depth
estimation in real-world applications.",arxiv
http://arxiv.org/abs/2201.06735v1,2022-01-18T04:20:37Z,2022-01-18T04:20:37Z,AI Augmented Digital Metal Component,"The aim of this work is to propose a new paradigm that imparts intelligence
to metal parts with the fusion of metal additive manufacturing and artificial
intelligence (AI). Our digital metal part classifies the status with real time
data processing with convolutional neural network (CNN). The training data for
the CNN is collected from a strain gauge embedded in metal parts by laser
powder bed fusion process. We implement this approach using additive
manufacturing, demonstrate a self-cognitive metal part recognizing partial
screw loosening, malfunctioning, and external impacting object. The results
indicate that metal part can recognize subtle change of multiple fixation state
under repetitive compression with 89.1% accuracy with test sets. The proposed
strategy showed promising potential in contributing to the hyper-connectivity
for next generation of digital metal based mechanical systems",arxiv
http://arxiv.org/abs/2201.06616v2,2022-01-20T15:00:50Z,2022-01-17T20:15:37Z,Improving the quality control of seismic data through active learning,"In image denoising problems, the increasing density of available images makes
an exhaustive visual inspection impossible and therefore automated methods
based on machine-learning must be deployed for this purpose. This is
particulary the case in seismic signal processing. Engineers/geophysicists have
to deal with millions of seismic time series. Finding the sub-surface
properties useful for the oil industry may take up to a year and is very costly
in terms of computing/human resources. In particular, the data must go through
different steps of noise attenuation. Each denoise step is then ideally
followed by a quality control (QC) stage performed by means of human expertise.
To learn a quality control classifier in a supervised manner, labeled training
data must be available, but collecting the labels from human experts is
extremely time-consuming. We therefore propose a novel active learning
methodology to sequentially select the most relevant data, which are then given
back to a human expert for labeling. Beyond the application in geophysics, the
technique we promote in this paper, based on estimates of the local error and
its uncertainty, is generic. Its performance is supported by strong empirical
evidence, as illustrated by the numerical experiments presented in this
article, where it is compared to alternative active learning strategies both on
synthetic and real seismic datasets.",arxiv
http://arxiv.org/abs/2201.06599v1,2022-01-17T19:25:33Z,2022-01-17T19:25:33Z,"Who supervises the supervisor? Model monitoring in production using deep
  feature embeddings with applications to workpiece inspection","The automation of condition monitoring and workpiece inspection plays an
essential role in maintaining high quality as well as high throughput of the
manufacturing process. To this end, the recent rise of developments in machine
learning has lead to vast improvements in the area of autonomous process
supervision. However, the more complex and powerful these models become, the
less transparent and explainable they generally are as well. One of the main
challenges is the monitoring of live deployments of these machine learning
systems and raising alerts when encountering events that might impact model
performance. In particular, supervised classifiers are typically build under
the assumption of stationarity in the underlying data distribution. For
example, a visual inspection system trained on a set of material surface
defects generally does not adapt or even recognize gradual changes in the data
distribution - an issue known as ""data drift"" - such as the emergence of new
types of surface defects. This, in turn, may lead to detrimental
mispredictions, e.g. samples from new defect classes being classified as
non-defective. To this end, it is desirable to provide real-time tracking of a
classifier's performance to inform about the putative onset of additional error
classes and the necessity for manual intervention with respect to classifier
re-training. Here, we propose an unsupervised framework that acts on top of a
supervised classification system, thereby harnessing its internal deep feature
representations as a proxy to track changes in the data distribution during
deployment and, hence, to anticipate classifier performance degradation.",arxiv
http://arxiv.org/abs/2201.05115v1,2022-01-13T18:20:32Z,2022-01-13T18:20:32Z,Functional Anomaly Detection: a Benchmark Study,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",arxiv
http://arxiv.org/abs/2201.04263v1,2022-01-12T01:22:26Z,2022-01-12T01:22:26Z,The Human Factor in AI Safety,"AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.",arxiv
http://arxiv.org/abs/2201.02028v1,2022-01-06T12:36:35Z,2022-01-06T12:36:35Z,"A Light in the Dark: Deep Learning Practices for Industrial Computer
  Vision","In recent years, large pre-trained deep neural networks (DNNs) have
revolutionized the field of computer vision (CV). Although these DNNs have been
shown to be very well suited for general image recognition tasks, application
in industry is often precluded for three reasons: 1) large pre-trained DNNs are
built on hundreds of millions of parameters, making deployment on many devices
impossible, 2) the underlying dataset for pre-training consists of general
objects, while industrial cases often consist of very specific objects, such as
structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal
issues for companies. As a remedy, we study neural networks for CV that we
train from scratch. For this purpose, we use a real-world case from a solar
wafer manufacturer. We find that our neural networks achieve similar
performances as pre-trained DNNs, even though they consist of far fewer
parameters and do not rely on third-party datasets.",arxiv
