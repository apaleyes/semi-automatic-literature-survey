id,datePublished,description,title,downloadUrl,publisher,journals,doi,database
387604654,2021-03-01T00:00:00,"The recent progress of artificial intelligence(AI) has shown great potentials
for alleviating human burden in various complex tasks. From the view of
software engineering, AI techniques can be seen in many fundamental aspects of
development, such as source code comprehension, in which state-of-the-art
models are implemented to extract and express the meaning of code snippets
automatically. However, such technologies are still struggling to tackle and
comprehend the complex structures within industrial code, thus far from
real-world applications. In the present work, we built an innovative and
systematical framework, emphasizing the problem of complexity in code
comprehension and further software engineering. Upon automatic data collection
from the latest Linux kernel source code, we modeled code structures as complex
networks through token extraction and relation parsing. Comprehensive analysis
of complexity further revealed the density and scale of network-based code
representations. Our work constructed the first large-scale dataset from
industrial-strength software code for downstream software engineering tasks
including code comprehension, and incorporated complex network theory into
code-level investigations of software development for the first time. In the
longer term, the proposed methodology could play significant roles in the
entire software engineering process, powering software design, coding,
debugging, testing, and sustaining by redefining and embracing complexity","Rethinking complexity for software code structures: A pioneering study
  on Linux kernel code repository",http://arxiv.org/abs/2103.00821,,,,core
14702701,2051-09-28T00:00:00,"Questo lavoro di tesi è il risultato di un periodo di stage svolto presso Navicelli S.p.A di Pisa, finalizzato allo sviluppo di un modello di ottimizzazione di un sistema gestionale CRM esistente di supporto al settore della cantieristica nautica. L’attività si colloca nell’ambito del progetto Mistral, che prevede la realizzazione di una piattaforma tecnologica collaborativa web based ed open source rivolta alla Nautica da Diporto dell'Alto Mediterraneo. L'obiettivo è quello di offrire ai cantieri una soluzione software che permetta loro di migliorare il processo di gestione delle relazioni con i loro clienti reali e potenziali. Con la collaborazione del cantiere Società Navale Pisa il lavoro svolto si è articolato principalmente in tre fasi: analisi della filiera nautica e rappresentazione dei processi riguardanti la gestione di una commessa navale semi-custom; analisi dei bisogni informativi del cantiere e studio delle problematiche legate allo strumento di gestione reso disponibile da Navicelli S.p.A nei confronti delle aziende sul canale dei Navicelli; traduzione dei bisogni individuati in requisiti funzionali attraverso lo sviluppo di un modello concettuale di reingegnerizzazione del sistema creato utilizzando lo standard internazionale UML. Le specifiche elaborate con UML saranno consegnate ad una softwarehouse per l'implementazione delle modifiche attuate al CRM.
This thesis is the result of an internship at the Navicelli S.p.A company in Pisa, aimed at developing an optimization model of a CRM (Customer Relationship Management) management system which exists in order to support the shipbuilding industry. The activity is part of the Mistral project that involves the realization of a web based and open source collaborative technological platform directed towards Recreational Boating of the North Mediterranean. The goal is to offer the shipyards a software solution allowing them to improve the relationship management process with their real and potential customers. In collaboration with the Pisa Naval Society the work performed was mainly divided into three phases: shipyard's organizational structure analysis and processes representation regarding the management of a semi-custom naval job order, individuation of the shipyard's informative needs and study of the problematics tied to the management tool made available by Navicelli S.p.A for the companies on the Navicelli canal; translation of the needs identified as functional requirements through the development of a conceptual model of system reengineering created by utilizing the international standard UML (Unified Modeling Language). The specifications elaborated through UML will be delivered to a softwarehouse for the implementation of the modifications effectuated to the CRM",Sviluppo di un modello di ottimizzazione di un CRM nell'ambito della cantieristica navale,,'Pisa University Press',,,core
337301137,2021-06-17T00:00:00,"Deep learning-based sequential recommender systems have recently attracted
increasing attention from both academia and industry. Most of industrial
Embedding-Based Retrieval (EBR) system for recommendation share the similar
ideas with sequential recommenders. Among them, how to comprehensively capture
sequential user interest is a fundamental problem. However, most existing
sequential recommendation models take as input clicked or purchased behavior
sequences from user-item interactions. This leads to incomprehensive user
representation and sub-optimal model performance, since they ignore the
complete user behavior exposure data, i.e., items impressed yet unclicked by
users. In this work, we attempt to incorporate and model those unclicked item
sequences using a new learning approach in order to explore better sequential
recommendation technique. An efficient triplet metric learning algorithm is
proposed to appropriately learn the representation of unclicked items. Our
method can be simply integrated with existing sequential recommendation models
by a confidence fusion network and further gain better user representation. The
offline experimental results based on real-world E-commerce data demonstrate
the effectiveness and verify the importance of unclicked items in sequential
recommendation. Moreover we deploy our new model (named XDM) into EBR of
recommender system at Taobao, outperforming the deployed previous generation
SDM.Comment: 10 page","XDM: Improving Sequential Deep Matching with Unclicked User Behaviors
  for Recommender System",http://arxiv.org/abs/2010.12837,,,,core
480043809,2021-01-01T00:00:00,"What we did This report assesses the potential of data-driven approaches to improving transport infrastructure maintenance. It looks at trends in maintenance strategies, explores how the targeted use of data could make them more effective for different types of transport infrastructure, and looks into implications for policy. The report builds on discussions held during workshops with members of the International Transport Forum’s Corporate Partnership Board. What we found Maintenance constitutes an inevitable, albeit often invisible, part of countries’ transport policies. Increased demand for transport infrastructure accelerates infrastructure’s ageing. The effects of climate change further aggravate this. Unsurprisingly, many governments look for transport infrastructure maintenance policies that provide better value for money than current practices offer. Infrastructure maintenance strategies are gradually shifting towards data-driven approaches. They exploit the power of digital technologies, Big Data analytics and advanced forecasting methodologies. Data-driven approaches have gained momentum in transport infrastructure maintenance as a result of four simultaneous technological innovations. First, the development of digital technologies has resulted in the digitalisation of society, industry and transport, which facilitates data sharing. Second, computing technologies have provided the necessary horsepower for running the digital infrastructure. Third, the Internet of Things and sensor technology have increased the potential for automating reporting from sensors that capture and measure new phenomena and provide data sets that flow through digital infrastructures. Fourth, artificial intelligence (AI) has helped to extract information from vast amounts of data, recognising patterns beyond the capacity of individual observation and exploiting digital infrastructure and computing power. Policy makers are beginning to leverage these developments in various ways. Data-driven maintenance is becoming common in many parts of the transport industry. Railroads collect massive amounts of inspection data from different sources using various methods, such as track inspection cars and drones that gather data to model track degradation. However, the rail sector faces numerous challenges for applying Big Data analysis: a lack of specific data analysis tools, high cost of involving stakeholders and heterogeneous data sources. Also, the algorithms currently used to predict the wear of rail infrastructure only work under lab conditions. For road infrastructure, various automated inspection methods exist. These include vision-based methods, laser scanning, ground penetration radar and a combination of these. All are accurate and effective but usually costly. As a result, the coverage and collection frequency can prove insufficient for detectingchanging road conditions. Several pilot studies have tried to use smartphones to collect data on the state of roads to reduce deployment costs for data-driven maintenance. At airports, the demand for accurate real-time data has spawned systems that automatically acquire and process infrastructure data. Advanced technologies now register when deformities develop on runways. They accurately measure moisture levels, temperature, strain and other factors relevant to wear and degradation. Several airports have built, or plan to build, concrete pavements with embedded strain gauges and other sensors to monitor the stress in the material caused by aircraft. Overall, data-driven approaches to infrastructure maintenance promise to enhance fact-based decision making and capabilities to predict the remaining useful life of assets. They can also improve cost efficiency and environmental sustainability. However, some new challenges need to be addressed, notably for the use of AI. AI predicts future behaviour based on historical data. Yet all predictions can prove incorrect where events do not follow past trends. What we recommendScale up and speed up the deployment of data-driven approaches to transport infrastructure maintenance Transport infrastructure maintenance could benefit from a broader and accelerated roll-out of data-driven approaches. These could improve the quality of assets, enhance the life cycles and save costs - especially when the relevant technologies are well-known, such as sensor technologies. In some cases, more tests and pilot projects will be useful, notably where leveraging data technologies for more effective maintenance policies poses specific challenges, as is the case of artificial intelligence in the railway sector. Update regulation and guidelines for transport infrastructure maintenance to facilitate the introduction of more data-driven approaches Current regulations and guidelines apply to condition-based maintenance strategies. These may set requirements that are ill-adapted to data-driven approaches to maintenance and may hamper their roll-out. Policy makers should ensure that the policies applied to data-driven approaches do not stifle their potential benefits. Ensure data-driven infrastructure maintenance approaches follow good practices in data governance The use of data in infrastructure maintenance must be in line with privacy protection laws and regulations. All data should be anonymised and encrypted. Location and trajectory data should be covered by the most robust protection methods, as they create the severest vulnerabilities for citizens. Tools to limit privacy risks include non-disclosure agreements between data users and providers, the involvement of trusted third parties to conduct the data collection and the development of “safe answers” approaches, in which only query results are exchanged instead of raw data. Governments could also broker data-sharing partnerships for the purpose of data-driven maintenance, for instance, between data providers and infrastructure managers. However, it may want to limit such partnerships to data of public interest and require purpose specificity and data minimisation",Data-driven Transport Infrastructure Maintenance,,,,,core
479336202,2021-05-25T00:00:00,"Recently, the Internet of Things (IoT) has an important role in the growth and development of digitalized electric power stations while offering ambitious opportunities, specifically real-time monitoring and cybersecurity. In this regard, this paper introduces a novel IoT architecture for the online monitoring of the gas-insulated switchgear (GIS) status instead of the traditional observation methods. The proposed IoT architecture is derived from the concept of the cyber-physic system (CPS) in Industry 4.0. However, the cyber-attacks and the classification of the GIS insulation defects represent the main challenges against the implementation of IoT topology for the online monitoring and tracking of the GIS status. For this purpose, advanced machine learning techniques are utilized to detect cyber-attacks to conduct the paradigm and verification. Different test scenarios on various defects in GIS are performed to demonstrate the effectiveness of the proposed IoT architecture. Partial discharge pulse sequence features are extracted for each defect to represent the inputs for IoT architecture. The results confirm that the proposed IoT architecture based on the machine learning technique, that is the extreme gradient boosting (XGBoost), can visualize all defects in the GIS with different alarms, besides showing the cyber-attacks on the networks effectively. Furthermore, the defects of GIS and the fake data due to the cyber-attacks are recognized and presented on the dashboard of the proposed IoT platform with high accuracy and more clarified visualization to enhance the decision-making about the GIS status.Peer reviewe",Towards Secured Online Monitoring for Digitalized GIS Against Cyber-Attacks Based on IoT and Machine Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",10.1109/ACCESS.2021.3083499,core
479510128,2021-04-23T00:00:00,"Federated Learning (FL) is one of the leading learning paradigms for enabling a more significant presence of intelligent applications in networked and Internet of Things (IoT) systems. It consists of individual user devices performing machine learning (ML) models training locally, so that only trained models due to privacy concerns, but not raw data, is transferred through the network for aggregation at the edge or cloud data centers [Li et al. 2019]. Due to the pervasive presence of connected devices such as smart phones and IoT devices in peoples lives, there is a growing concern about how we can preserve and secure users’ information. FL reduces the risk of exposing user information to attackers during transmission over networks or information leakages at the central data centers. Another advantage of FL is scalability and maintainability of intelligent applications in networked and IoT systems. Considering highly distributed environments in which such systems are deployed, collecting and transmitting raw user data for training of ML models at central data centers is a challenging task as it imposes huge workload on the networks and consumes high bandwidth. Training of ML models is distributed over locations and transmitting the trained models for aggregation alleviates these challenges.



Among others, distributed and federated learning have applications in smart healthcare systems, where very sensitive user data is involved, and industrial IoT applications, where the amount of data for training may be too large and cumbersome to transport to central data centers. However, FL has the significant shortcoming of requiring user data to be Independent Identically Distributed (IID) (i.e., users which have similar data statistical distributions and are not mutually dependent) and make reliable predictions for a given group of users aggregated into a single model. IID users have similar statistical features, and thus can be aggregated into the same ML models. Since raw data is not available at the model aggregator, it is necessary to find IID users based solely on their trained machine learning models.



We present a Neural Network-based Federated Clustering mechanism capable of clustering IID with no access to their raw data called Neural-network SIMilarity estimator, NSIM. Such mechanism performs significantly better than competing techniques for neural-network clustering [Pacheco et al. 2021]. We also present an alternative to the FedAvg aggregation algorithm used in traditional FL, which significantly increases the aggregated models’ reliability in terms of Mean Square Error by creating several training models over IID users in a real-world mobility prediction dataset. We observe improvements of up to 97.52% in terms of Pearson correlation between the similarity estimation by NSIM and ground truth based on the LCSS (Longest Common Sub-Sequence) similarity metric, in comparison with other state-of-the-art approaches. Federated Clustering of IID data in different geographical locations can improve performance of early warning applications such as flood prediction [Samikwa et al. 2020], where the data for some locations may have more statistical similarities. We further present a technique for accelerating ML inference in resource-constrained devices through distributed computation of ML models over IoT networks, while preserving privacy. This has the potential to improve the performance of time sensitive ML applications",Distributed and Federated Learning Optimization with Federated Clustering of IID-users,https://core.ac.uk/download/479510128.pdf,,,,core
291716187,10000-01-01,"This thesis addresses the task of distributed production scheduling, especially for flexible manufacturing systems (FMS). Current mathematical programming and model-based approaches are considered, but are shown to inadequately manage the dynamic behavior, uncertainty, and large scale typical of such systems. Thus, a new unified framework for modelling and control is proposed that incorporates insights from Operations Research, Control Theory, Artificial Intelligence, and Software Engineering.In this research, production scheduling is addressed in terms of the allocation of resources. The method of ""resource allocation policy"" scheduling is proposed and compared with the ""event-time"" schedules, which result from typical formulations of the production scheduling problem. Next, the technique of real-time simulation is introduced and applied to study the Automated Manufacturing Research Facility (AMRF). The proposed solution methodology for on-line production scheduling centers on a generic controller, which employs real-time simulation to evaluate the trade-offs existing among alternative production policies. The generic controller establishes a standard, parallel-processing architecture in which to perform the decision-making at any level in a hierarchically decomposed scheduling problem. The specification of the functional elements of the generic controller (GC), conceptualized by Davis, Jones, and Saleh (1991), has been further developed in this research work. Likewise, precise definitions have been enumerated for the types of data utilized in the GC, as well as the manner in which they are transmitted between the GC's elements.The essential structure and functional content of the generic controller as well as the proposed resource-based, discrete-event model for production systems have been incorporated into the development of the Flexible Manufacturing Simulation Library (FMSL). The FMSL software provides a set of routines to model hierarchical production systems and perform the tasks required for on-line scheduling. Based on the preliminary studies with the AMRF model and the application of the FMSL for the modelling and simulation of the Motorola Grafted Multilayer process, conclusions are drawn regarding the importance of critical resources. Furthermore, the possibility to apply the proposed approach for true real-time monitoring and control in production systems is likewise considered.U of I OnlyETDs are only available to UIUC Users without author permissio","Real-time, distributed scheduling of production resources",,,,,core
334946296,2021-06-02T00:00:00,"Tech-leading organizations are embracing the forthcoming artificial
intelligence revolution. Intelligent systems are replacing and cooperating with
traditional software components. Thus, the same development processes and
standards in software engineering ought to be complied in artificial
intelligence systems. This study aims to understand the processes by which
artificial intelligence-based systems are developed and how state-of-the-art
lifecycle models fit the current needs of the industry. We conducted an
exploratory case study at ING, a global bank with a strong European base. We
interviewed 17 people with different roles and from different departments
within the organization. We have found that the following stages have been
overlooked by previous lifecycle models: data collection, feasibility study,
documentation, model monitoring, and model risk assessment. Our work shows that
the real challenges of applying Machine Learning go much beyond sophisticated
learning algorithms - more focus is needed on the entire lifecycle. In
particular, regardless of the existing development tools for Machine Learning,
we observe that they are still not meeting the particularities of this field.Comment: Accepted in Empirical Software Engineering in April, 202",AI Lifecycle Models Need To Be Revised. An Exploratory Study in Fintech,http://arxiv.org/abs/2010.02716,,,,core
387323254,2021-05-31T00:00:00,"While data sharing is crucial for knowledge development, privacy concerns and
strict regulation (e.g., European General Data Protection Regulation (GDPR))
unfortunately limit its full effectiveness. Synthetic tabular data emerges as
an alternative to enable data sharing while fulfilling regulatory and privacy
constraints. The state-of-the-art tabular data synthesizers draw methodologies
from generative Adversarial Networks (GAN) and address two main data types in
the industry, i.e., continuous and categorical. In this paper, we develop
CTAB-GAN, a novel conditional table GAN architecture that can effectively model
diverse data types, including a mix of continuous and categorical variables.
Moreover, we address data imbalance and long-tail issues, i.e., certain
variables have drastic frequency differences across large values. To achieve
those aims, we first introduce the information loss and classification loss to
the conditional GAN. Secondly, we design a novel conditional vector, which
efficiently encodes the mixed data type and skewed distribution of data
variable. We extensively evaluate CTAB-GAN with the state of the art GANs that
generate synthetic tables, in terms of data similarity and analysis utility.
The results on five datasets show that the synthetic data of CTAB-GAN
remarkably resembles the real data for all three types of variables and results
into higher accuracy for five machine learning algorithms, by up to 17%.Comment: This paper consists of 11 pages which contain 8 figures, 5 tables and
  an appendix with a user manual for our software applicatio",CTAB-GAN: Effective Table Data Synthesizing,http://arxiv.org/abs/2102.08369,,,,core
459156956,2021-11-01T00:00:00,"Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model",A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand,,'Elsevier BV',,10.1016/j.renene.2021.05.155,core
459154453,2021-10-26T00:00:00,"Of all the causes of accidents to ships, 14% pertains to damage due to ship equipment. Accordingly, the maritime industry is currently considering state-of-the-art maintenance and inspection processes, an example of which is Condition-Based Maintenance (CBM). This is a strategy that hinges on the condition monitoring of assets. Condition Monitoring (CM) has proven to increase efficiency, reliability, profitability, and performance of vessel, and thus facilitate emissions reduction during its operational lifetime. To enable this maintenance strategy, sensors need to be installed along the most critical ship components and around the environment where these assets are operating through the appliance of Internet of Ships (IoS). IoS has demonstrated to be effective for collecting data in real time as well as performing diagnosis and prognosis to assess the current and future health of machinery to assist instant decision-making processes. The employment of IoS presents several challenges, an example of which is the imputation of missing values. Data imputation is a compelling pre-processing step, the aim of this is to estimate identified missing values to avoid under-utilisation of data. This data preparation step has gained popularity over the last few years due to its importance when dealing with Industrial Internet of Things (IIoT) sensor data. Nonetheless, very few publications presently refer to this aspect in the maritime industry. Although some articles presented new methodologies to impute missing values from sensor data of marine machinery based on machine learning methodologies, deep learning models have not yet been considered. For this reason, variational autoencoders for imputing missing values from sensor data of marine systems are analysed in this study. To assess the performance of variational autoencoders as imputation methods, a comparative study is performed with widely implemented imputation techniques. Mean imputation, Forward Fill and Backward Fill, and k-Nearest Neighbors are considered. To that end, a case study on marine machinery system parameters obtained from sensors installed on a diesel generator is performed. Results demonstrate the applicability of variational autoencoders when dealing with missing values from marine machinery systems sensor data, achieving a coefficient of determination of 0.99 when imputing missing values of the diesel generator power parameter",Analysis of variational autoencoders for imputing missing values from sensor data of marine systems,,,,,core
478717116,2021-05-12T07:00:00,"Achievements in all fields of engineering and fabrication methods have led towards optimization and integration of multiple sensing devices into a concise system. These advances have caused significant innovation in various commercial, industrial, and research efforts. Integrations of subsystems have important applications for sensor systems in particular. The need for reporting and real time awareness of a device’s condition and surroundings have led to sensor systems being implemented in a wide variety of fields. From environmental sensors for agriculture, to object characterization and biomedical sensing, the application for sensor systems has impacted all modern facets of innovation. With these innovations, however, additional sources of errors can occur, that can cause new but exciting challenges for such integrated devices. Such challenges range from error correction and accuracy to power optimization. Researchers have invested significant time and effort to improve the applicability and accuracy of sensors and sensor systems. Efforts to reduce inherent and external noise of sensors can range from hardware to software solutions, focusing on signal processing and exploiting the integration of multiple signals and/or sensor types. My research work throughout my career has been focused on deployable and integrated sensor systems. Their integration not only in hardware and components but also in software, machine learning, pattern recognition, and overall signal processing algorithms to aid in error correction and noise tailoring in all their hardware and software components",Error Prevention in Sensors and Sensor Systems,,LSU Digital Commons,,,core
479917899,2021-06-22T09:30:10,"The subject matter of this research paper discusses the nature of the relationship between law and artificial intelligence. The paper presents the nature of this relationship according to a critical study comparing the French legislative reality with its Qatari counterpart, in light of the European civil law rules on robotics of 2017, and the Comprehensive European industrial policy on artificial intelligence and robotics of 2019. The study is carried out in two main axes that formed the two main sections of this study. The first path dealt with the protection of  artificial intelligence , by protecting the results of this intelligence at various levels; in its concrete form  Robot  and intangible  Logarithms and Software.  The second path addressed the protection of the  society in its various components  in terms of ensuring that our basic rights and freedoms are not violated, in addition to determining the nature of legal liability resulting from the illegal use of this intelligence. The research concluded that there is a clear deficiency of legislation - albeit with some variation - in the nature of the legal treatment of many points raised by artificial intelligence in both the French and the Qatari civil legislations, whether in relation to the legal rules concerning the protection of intellectual property rights, especially those related to the protection of intellectual property rights of artificial intelligence, or to the recognition of the legal personality of the tangible form of artificial intelligence represented by robotics. On the other hand, the research stressed the need to codify the process of using artificial intelligence in order to ensure respect for the latter and those responsible for the basic rights and freedoms of individuals. The research also showed that, in the absence of the cognitive aspect of artificial intelligence, the main pillar of the idea of its legal liability in both the French and Qatari legislations, the error remains a human error of unconventional concept, which requires both legislators to reconsider the legal rules governing their notion of legal liability, consistent with the specificity and nature of this liability. Therefore, the research recommended the need to ensure the ethics of using artificial intelligence in both legislations, through the development of a code of ethical work outlining the various aspects of its work, as well as the rights and duties ensuing, in order to ensure the re-harmonization of various related legal texts within the spirit and philosophy of the relationship between the artificial intelligence and the law, and the nature of the legal system for both the French and Qatari legislators",الذكاء الاصطناعي والقانون-دراسة نقدية مقارنة في التشريعين المدني الفرنسي و القطري -في ضوء القواعد الأوربية في القانون المدني للإنسآلة لعام 2017 والسياسة الصناعية الأوربية للذكاء الاصطناعي والإنسآلات لعام 2019,https://core.ac.uk/download/479917899.pdf,Digital Commons @ BAU,,,core
478906450,2021-08-01T07:00:00,"The Citrus fruit industry in the United States has been affected during the last two decades because of the outbreak of the citrus greening disease, also known as Huanglongbing (HLB). Many people and organizations are working in therapeutics to help mitigate the impact of this disease, unfortunately there is not a cure yet. There are many mechanisms that needs to be understood about this disease, especially at the molecular level. Not only HLB, but many other infectious diseases are controlled by the interaction of proteins from the host (Citrus in this case) and from the pathogen that causes the disease. When these interactions occur, the normal activity of the plant host is altered and then the plant start to develop more and more ill severe symptoms.
The purpose of this research was to implement diverse computational methods to predict those interactions accurately. We applied several methods successfully used in other infectious diseases, and developed novel techniques including the Artificial Intelligence models. After a comprehensive analysis of our results, we identified some candidate genes that we believe are key HLB players which could be genetically modified to enhance resistance in Citrus plants.
The models developed from this study were implemented as web-based tools for real time use by the biologists and other stakeholders. The knowledge gained from this study can further be used to understand plant-pathogen interactions in other agricultural systems of economic importance",Computational Techniques for Elucidating Plant-Pathogen Interactions: A Case Study on Citrus-HLB Interactome,,DigitalCommons@USU,,,core
387306946,2021-01-12T00:00:00,"Leveraging machine-learning (ML) techniques for compiler optimizations has
been widely studied and explored in academia. However, the adoption of ML in
general-purpose, industry strength compilers has yet to happen. We propose
MLGO, a framework for integrating ML techniques systematically in an industrial
compiler -- LLVM. As a case study, we present the details and results of
replacing the heuristics-based inlining-for-size optimization in LLVM with
machine learned models. To the best of our knowledge, this work is the first
full integration of ML in a complex compiler pass in a real-world setting. It
is available in the main LLVM repository. We use two different ML algorithms:
Policy Gradient and Evolution Strategies, to train the inlining-for-size model,
and achieve up to 7\% size reduction, when compared to state of the art LLVM
-Oz. The same model, trained on one corpus, generalizes well to a diversity of
real-world targets, as well as to the same set of targets after months of
active development. This property of the trained models is beneficial to deploy
ML techniques in real-world settings.Comment: First two authors are equal contributor",MLGO: a Machine Learning Guided Compiler Optimizations Framework,http://arxiv.org/abs/2101.04808,,,,core
421613443,2021-04-01T00:00:00,"International audienceIn the aeronautics sector, aircraft parts are inspected during manufacture, assembly and service, to detect defects eventually present. Defects can be of different types, sizes and orientations, appearing in materials presenting a complex structure. Among the different inspection techniques, Non Destructive Testing (NDT) presents several advantages as they are noninvasive and cost effective. Within the NDT methods, Ultrasonic (US) waves are widely used to detect and characterize defects. However, due the so-called blind zone, they cannot be easily employed for defects close to the surface being inspected. On the other hand, another NDT technique such Eddy Current (EC) can be used only for detecting flaws close to the surface, due to the presence of the EC skin effect. The work presented in this article aims to combine the use of these two NDT methods, exploiting their complementary advantages. To reach this goal, a data fusion method is developed, by using Machine Learning techniques such as Artificial Neural Networks (ANNs). A simulated training database involving simulations of US and EC signals propagating in an Aluminum block in the presence of Side Drill Holes (SDHs) has been implemented, to train the ANNs. Measurements have been then performed on an Aluminum block, presenting tree different SDHs at specific depths. The trained ANNs were used to characterize the different real SDHs, providing an experimental validation. Eventually, particular attention has been addressed to the estimation errors corresponding to each flaw. Experimental results will show that depths and radii estimations error were confined on average within a range of 4%, recording a peak of 11% for the second SDHs",A Data Fusion Method for Non-Destructive Testing by Means of Artificial Neural Networks,,'MDPI AG',,10.3390/s21082598,core
402257131,2021-07-01T00:00:00,"The challenge of providing training and education in healthcare has never been greater. The COVID-19 pandemic has highlighted the weaknesses of methods that rely on master–apprentice models, face-to-face delivery and patient access. The emergence of a new generation of ‘immersive technologies’ (eg, augmented and virtual reality) presents an opportunity to overcome existing weaknesses and radically transform the healthcare education landscape. While digital simulations have been available for decades, recent large-scale investments coupled with breakthroughs in low-cost computing and artificial intelligence make this feel like a watershed moment for immersive simulation technologies. Yet, improper implementation and poorly designed evaluation could risk future growth and place a considerable burden on the healthcare system. With this in mind, we recently brought together a collection of clinicians, researchers and industry under the banner of the ‘Immersive Healthcare Collaboration’. We sought to generate a set of guiding principles to maximise the utility of these technologies for training and education. The result of this cross-disciplinary effort is the creation of a report laying out three evidence-based principles for safe, efficient and effective progress for immersive technologies in healthcare training and education. To understand the rationale and evidence behind each principles, we refer readers to the full report. Here, we provide a summary to encourage the broader immersive healthcare community to implement in their own work and practice. We believe adoption of these principles will help realise the enormous potential of these technologies and in turn, benefit the healthcare community and ultimately, patient care",Three principles for the progress of immersive technologies in healthcare training and education,,'BMJ',,10.1136/bmjstel-2021-000881,core
389180549,2021-01-13T00:00:00,"Research background: Rapid technical progress supports the development of advanced technologies, which increases the requirements of customers who emphasize the quality of products. We can expect revolutionary changes in every business area, but especially in production and logistics, as they will be most subject to the implementation of artificial intelligence, robotics, augmented reality, 3D printing and other technologies related to Industry 4.0 as a representative of The Fourth Industrial Revolution. Based on these facts, every enterprise is looking for new ways to reduce costs not only in production but also logistics, speed up and simplify the production and logistics process. One of the effective ways to achieve this is to use and implement progressive technologies in logistics. The aim of the paper was to examine progressive technologies in logistics that are used in Slovak automotive enterprises and then analyse selected technologies. The paper is divided into two parts. The first part of the paper is devoted to a literary view of progressive technologies in logistics. The second part provides an insight into a comparative analysis of selected technologies in logistics of 65 Slovak automotive enterprises. The paper used several research methods, which include: literature search, system analysis, comparative analysis, induction, deduction and visualization method. Progressive technologies in today’s conditions are a prerequisite for effective results in every enterprise in the form of economy and efficiency. The use and continuous improvement of progressive technologies in automotive enterprises must be one of the necessary activities in the management of the enterprise in order to maintain competitiveness in the market.
Purpose of the article: The aim of the paper was to examine progressive technologies in logistics that are used in Slovak automotive enterprises and then analyse selected technologies. The paper is divided into two parts. The first part of the paper is devoted to a literary view of progressive technologies in logistics. The second part provides an insight into a comparative analysis of selected technologies in logistics of 65 Slovak automobile enterprises.
Methods: The paper used several research methods, which include: literature search, system analysis, comparative analysis, induction, deduction and visualization method.
Findings & Value added: Progressive technologies in today’s conditions are a prerequisite for effective results in every enterprise in the form of economy and efficiency. The use and continuous improvement of advanced technologies in automotive enterprises must be one of the necessary activities in the management of the enterprise in order to maintain competitiveness in the market",The Use of Progressive Technologies in Logistics of Slovak Automotive Enterprises,,'EDP Sciences',,10.1051/shsconf/20219204022,core
387323821,2021-04-02T00:00:00,"The number of connected Internet of Things (IoT) devices within
cyber-physical infrastructure systems grows at an increasing rate. This poses
significant device management and security challenges to current IoT networks.
Among several approaches to cope with these challenges, data-based methods
rooted in deep learning (DL) are receiving an increased interest. In this
paper, motivated by the upcoming surge of 5G IoT connectivity in industrial
environments, we propose to integrate a DL-based anomaly detection (AD) as a
service into the 3GPP mobile cellular IoT architecture. The proposed
architecture embeds autoencoder based anomaly detection modules both at the IoT
devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing
between the system responsiveness and accuracy. We design, integrate,
demonstrate and evaluate a testbed that implements the above service in a
real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT)
mobile operator network.Comment: 12 pages, 9 figures, revised version, submitted to IEEE journal for
  possible publicatio","Deep Learning Anomaly Detection for Cellular IoT with Applications in
  Smart Logistics",http://arxiv.org/abs/2102.08936,,,,core
480038968,2021-01-01T00:00:00,"The evaluation of groundwater quality in the Dammam formation, Faddak farm, Karbala Governorate, Iraq proved that the sulfate (SO42−) concentrations have high values; so, this water is not suitable for livestock, poultry and irrigation purposes. For reclamation of this water, manufacturing of new sorbent for permeable reactive barrier was required through precipitation of Mg and Fe hydroxides nanoparticles on the activated carbon (AC) surface with best Mg/Fe molar ratio of 7.5/2.5. Mixture of 50% coated AC and 50% scrap iron was applied to eliminate SO42− from contaminated water with efficiency of 59% and maximum capacity of adsorption equals to 9.5 mg/g for a time period of 1 h, sorbent dosage 40 g/L, and initial pH = 5 at 50 mg/L initial SO42− concentration and 200 rpm shaking speed. Characterization analyses certified that the plantation of Mg and Fe nanoparticles onto AC was achieved. Continuous tests showed that the longevity of composite sorbent is increased with thicker bed and lower influent concentration and flow rate. Computer solution (COMSOL) software was well simulated for continuous measurements. The reclamation of real contaminated groundwater was achieved in column set-up with efficiency of 70% when flow rate was 5 mL/min, bed depth was 50 cm and inlet SO42− concentration was 2301 mg/L.Validerad;2021;Nivå 2;2021-07-29 (beamah);Forskningsfinansiär: Taif University (TURSP-2020/49)</p",New Composite Sorbent for Removal of Sulfate Ions from Simulated and Real Groundwater in the Batch and Continuous Tests,,'MDPI AG',,10.3390/molecules26144356,core
479999354,2021-01-01T00:00:00,"The Software Defined Networking (SDN) paradigm enables the development of systems that centrally monitor and manage network traffic, providing support for the deployment of machine learning-based systems that automatically detect and mitigate network intrusions. This paper presents an intelligent system capable of deciding which countermeasures to take in order to mitigate an intrusion in a software defined network. The interaction between the intruder and the defender is posed as a Markov game and MuZero algorithm is used to train the model through self-play. Once trained, the model is integrated with an SDN controller, so that it is able to apply the countermeasures of the game in a real network. To measure the performance of the model, attackers and defenders with different training steps have been confronted and the scores obtained by each of them, the duration of the games and the ratio of games won have been collected. The results show that the defender is capable of deciding which measures minimize the impact of the intrusion, isolating the attacker and preventing it from compromising key machines in the network.This work was supported in part by the Spanish Centre for the Development of Industrial Technology (CDTI) through the Project EGIDA-RED DE EXCELENCIA EN TECNOLOGIAS DE SEGURIDAD Y PRIVACIDAD under Grant CER20191012, in part by the Spanish Ministry of Science and Innovation under Grant PID2019-104966GB-I00, in part by the Basque Business Development Agency (SPRI)-Basque Country Government ELKARTEK Program through the projects TRUSTIND under Grant KK-2020/00054 and 3KIA under Grant KK-2020/00049, and in part by the Basque Country Program of Grants for Research Groups under Grant IT-1244-19",Towards Autonomous Defense of SDN Networks Using MuZero Based Intelligent Agents,https://core.ac.uk/download/479999354.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",10.1109/ACCESS.2021.3100706,core
428287406,2021-05-31T00:00:00,"The article introduces iDesign, a novel full-body immersive 3D digital set modelling system currently in joint development by the Sydney Theatre Company, University of New South Wales’ iCinema Centre, University of Technology Sydney, and the National Institute for Dramatic Art (Australia; 2018-21). Deploying a visualisation pipeline that utilises a 3D digital environment that remotely located creatives can share, iDesign enables collaborative ideation, testing and iteration of production design concepts in real time, to be supported by an artificially intelligent (AI) system. The article contextualises iDesign’s underpinning aesthetic rationale by reviewing the functionalities and ontological differences of physical and virtual models as epistemological conduits and representative objects. It outlines the system’s key features and capabilities, and discusses them in relation to other visualisation approaches currently trialled across the entertainment industry, such as Virtual Reality (VR), 3D modelling suites and pre-visualisation software. Considering the system’s archival capabilities, the text further sketches iDesign’s value as an educational platform and closes with a reflection on the impacts of digital technologies on scenographic practice",The iDesign platform – immersive intelligent aesthetics for scenographic modelling,,'Informa UK Limited',,10.1080/23322551.2021.1919488,core
387884113,2021-01-01T00:00:00,"We quantify the reductions in primary emissions due to the COVID-19 lockdowns in Europe. Our estimates are provided in the form of a dataset of reduction factors varying per country and day that will allow the modelling and identification of the associated impacts upon air quality. The country- and daily-resolved reduction factors are provided for each of the following source categories: energy industry (power plants), manufacturing industry, road traffic and aviation (landing and take-off cycle). We computed the reduction factors based on open-access and near-real-time measured activity data from a wide range of information sources. We also trained a machine learning model with meteorological data to derive weather-normalized electricity consumption reductions. The time period covered is from 21 February, when the first European localized lockdown was implemented in the region of Lombardy (Italy), until 26 April 2020. This period includes 5 weeks (23 March until 26 April) with the most severe and relatively unchanged restrictions upon mobility and socio-economic activities across Europe. The computed reduction factors were combined with the Copernicus Atmosphere Monitoring Service's European emission inventory using adjusted temporal emission profiles in order to derive time-resolved emission reductions per country and pollutant sector. During the most severe lockdown period, we estimate the average emission reductions to be −33 % for NOx, −8 % for non-methane volatile organic compounds (NMVOCs), −7 % for SOx and −7 % for PM2.5 at the EU-30 level (EU-28 plus Norway and Switzerland). For all pollutants more than 85 % of the total reduction is attributable to road transport, except SOx. The reductions reached −50 % (NOx), −14 % (NMVOCs), −12 % (SOx) and −15 % (PM2.5) in countries where the lockdown restrictions were more severe such as Italy, France or Spain. To show the potential for air quality modelling, we simulated and evaluated NO2 concentration decreases in rural and urban background regions across Europe (Italy, Spain, France, Germany, United-Kingdom and Sweden). We found the lockdown measures to be responsible for NO2 reductions of up to −58 % at urban background locations (Madrid, Spain) and −44 % at rural background areas (France), with an average contribution of the traffic sector to total reductions of 86 % and 93 %, respectively. A clear improvement of the modelled results was found when considering the emission reduction factors, especially in Madrid, Paris and London where the bias is reduced by more than 90 %. Future updates will include the extension of the COVID-19 lockdown period covered, the addition of other pollutant sectors potentially affected by the restrictions (commercial and residential combustion and shipping) and the evaluation of other air quality pollutants such as O3 and PM2.5. All the emission reduction factors are provided in the Supplement.The research leading to these results has received funding from the Copernicus Atmosphere Monitoring Service (CAMS), which is implemented by the European Centre for Medium-Range Weather Forecasts (ECMWF) on behalf of the European Commission. We acknowledge support from the Ministerio de Ciencia, Innovación y Universidades (MICINN) as part of the BROWNING project RTI2018-099894-B-I00 and NUTRIENT project CGL2017-88911-R, the Agencia Estatal de Investigacion (AEI) as part of the VITALISE project (PID2019-108086RA-I00/AEI/0.13039/501100011033), the AXA Research Fund, and the European Research Council (grant no. 773051, FRAGMENT). We also acknowledge PRACE and RES for awarding access to Marenostrum4 based in Spain at the Barcelona Supercomputing Center through the eFRAGMENT2 and AECT-2020-1-0007 projects. This project has also received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement H2020-MSCA-COFUND-2016-754433. Carlos Pérez García-Pando also acknowledges support received through the Ramón y Cajal programme (grant RYC-2015-18690) of the MICINN.Peer ReviewedPostprint (published version",Time-resolved emission reductions for atmospheric chemistry modelling in Europe during the COVID-19 lockdowns,,'Copernicus GmbH',"[{'title': None, 'identifiers': ['1680-7316', 'issn:1680-7316']}]",10.5194/acp-21-773-2021,core
429964430,2021-06-10T00:00:00,"Technological advances in information sharing have raised concerns about data
protection. Privacy policies contain privacy-related requirements about how the
personal data of individuals will be handled by an organization or a software
system (e.g., a web service or an app). In Europe, privacy policies are subject
to compliance with the General Data Protection Regulation (GDPR). A
prerequisite for GDPR compliance checking is to verify whether the content of a
privacy policy is complete according to the provisions of GDPR. Incomplete
privacy policies might result in large fines on violating organization as well
as incomplete privacy-related software specifications. Manual completeness
checking is both time-consuming and error-prone. In this paper, we propose
AI-based automation for the completeness checking of privacy policies. Through
systematic qualitative methods, we first build two artifacts to characterize
the privacy-related provisions of GDPR, namely a conceptual model and a set of
completeness criteria. Then, we develop an automated solution on top of these
artifacts by leveraging a combination of natural language processing and
supervised machine learning. Specifically, we identify the GDPR-relevant
information content in privacy policies and subsequently check them against the
completeness criteria. To evaluate our approach, we collected 234 real privacy
policies from the fund industry. Over a set of 48 unseen privacy policies, our
approach detected 300 of the total of 334 violations of some completeness
criteria correctly, while producing 23 false positives. The approach thus has a
precision of 92.9% and recall of 89.8%. Compared to a baseline that applies
keyword search only, our approach results in an improvement of 24.5% in
precision and 38% in recall",AI-enabled Automation for Completeness Checking of Privacy Policies,http://arxiv.org/abs/2106.05688,,,,core
436601343,2021-06-18T00:00:00,"This paper addresses the problem of policy selection in domains with abundant
logged data, but with a very restricted interaction budget. Solving this
problem would enable safe evaluation and deployment of offline reinforcement
learning policies in industry, robotics, and healthcare domain among others.
Several off-policy evaluation (OPE) techniques have been proposed to assess the
value of policies using only logged data. However, there is still a big gap
between the evaluation by OPE and the full online evaluation in the real
environment. To reduce this gap, we introduce a novel \emph{active offline
policy selection} problem formulation, which combined logged data and limited
online interactions to identify the best policy. We rely on the advances in OPE
to warm start the evaluation. We build upon Bayesian optimization to
iteratively decide which policies to evaluate in order to utilize the limited
environment interactions wisely. Many candidate policies could be proposed,
thus, we focus on making our approach scalable and introduce a kernel function
to model similarity between policies. We use several benchmark environments to
show that the proposed approach improves upon state-of-the-art OPE estimates
and fully online policy evaluation with limited budget. Additionally, we show
that each component of the proposed method is important, it works well with
various number and quality of OPE estimates and even with a large number of
candidate policies",Active Offline Policy Selection,http://arxiv.org/abs/2106.10251,,,,core
334931203,2021-01-21T00:00:00,"Most recently, with the proliferation of IoT devices, computational nodes in
manufacturing systems IIoT(Industrial-Internet-of-things) and the lunch of 5G
networks, there will be millions of connected devices generating a massive
amount of data. In such an environment, the controlling systems need to be
intelligent enough to deal with a vast amount of data to detect defects in a
real-time process. Driven by such a need, artificial intelligence models such
as deep learning have to be deployed into IIoT systems. However, learning and
using deep learning models are computationally expensive, so an IoT device with
limited computational power could not run such models. To tackle this issue,
edge intelligence had emerged as a new paradigm towards running Artificial
Intelligence models on edge devices. Although a considerable amount of studies
have been proposed in this area, the research is still in the early stages. In
this paper, we propose a novel edge-based multi-phase pruning pipelines to
ensemble learning on IIoT devices. In the first phase, we generate a diverse
ensemble of pruned models, then we apply integer quantisation, next we prune
the generated ensemble using a clustering-based technique. Finally, we choose
the best representative from each generated cluster to be deployed to a
distributed IoT environment. On CIFAR-100 and CIFAR-10, our proposed approach
was able to outperform the predictability levels of a baseline model (up to
7%), more importantly, the generated learners have small sizes (up to 90%
reduction in the model size) that minimise the required computational
capabilities to make an inference on the resource-constraint devices.Comment: a revised version is going to be submitted to a journal soo","Prune2Edge: A Multi-Phase Pruning Pipelines to Deep Ensemble Learning in
  IIoT",http://arxiv.org/abs/2004.04710,,,,core
479718027,2021-01-01T00:00:00,"The evaluation of groundwater quality in the Dammam formation, Faddak farm, Karbala Governorate, Iraq proved that the sulfate (SO42−) concentrations have high values; so, this water is not suitable for livestock, poultry and irrigation purposes. For reclamation of this water, manufacturing of new sorbent for permeable reactive barrier was required through precipitation of Mg and Fe hydroxides nanoparticles on the activated carbon (AC) surface with best Mg/Fe molar ratio of 7.5/2.5. Mixture of 50% coated AC and 50% scrap iron was applied to eliminate SO42− from contaminated water with efficiency of 59% and maximum capacity of adsorption equals to 9.5 mg/g for a time period of 1 h, sorbent dosage 40 g/L, and initial pH = 5 at 50 mg/L initial SO42− concentration and 200 rpm shaking speed. Characterization analyses certified that the plantation of Mg and Fe nanoparticles onto AC was achieved. Continuous tests showed that the longevity of composite sorbent is increased with thicker bed and lower influent concentration and flow rate. Computer solution (COMSOL) software was well simulated for continuous measurements. The reclamation of real contaminated groundwater was achieved in column set-up with efficiency of 70% when flow rate was 5 mL/min, bed depth was 50 cm and inlet SO42− concentration was 2301 mg/L.Validerad;2021;Nivå 2;2021-07-29 (beamah);Forskningsfinansiär: Taif University (TURSP-2020/49)</p",New Composite Sorbent for Removal of Sulfate Ions from Simulated and Real Groundwater in the Batch and Continuous Tests,,'MDPI AG',,10.3390/molecules26144356,core
475061243,2021-07-01T00:00:00,"The food industry has improved product quality while reducing production time and cost by automating production using programmable logic controllers (PLC) over the last several decades. However, many production plants still require some level of manual expert interaction, mainly because the production processes are not 100% under control. Operators are often still present to take quality samples, re-tune unit operation controls or resolve failures.
The use of a physics-based “Digital Twin” is getting more and more traction to develop the equipment virtually due to the improvements in prediction accuracy and speed of computation. Digital twins allow engineers to find the optimal design before the unit goes into production. However, these digital twins can’t be deployed at the operational level because they can be complex or too slow to react at the speed of operation.
In this contribution a new set of solutions that lowers the barrier in executing the digital twins on the production floor is explain based on a few examples. This will deliver substantial return on investment (ROI) for the food production industry. They include technologies such as:A machine learning based methodology to perform Model Order Reduction (MOR) on the digital twin in order to get real time response based on production information.
A machine learning based methodology to convert the reduced model into a virtual sensor for online quality predictions or predictive maintenance scheduling as well as to use it for creating an optimal controller of the unit based on the product requirements.
Fast edge computing hardware that can collect data from sensors and run the Executable Digital Twin (xDT) to suggest corrective action to the operator, in real time, or ultimately run in closed loop control",Increase Food Production Efficiency Using the Executable Digital Twin (xdt),https://core.ac.uk/download/475061243.pdf,AIDIC Servizi S.r.l.,"[{'title': None, 'identifiers': ['issn:2283-9216', '2283-9216']}]",10.3303/CET2187007,core
475040205,2021-07-28T00:00:00,"Recent advancements in predictive machine learning has led to its application
in various use cases in manufacturing. Most research focused on maximising
predictive accuracy without addressing the uncertainty associated with it.
While accuracy is important, focusing primarily on it poses an overfitting
danger, exposing manufacturers to risk, ultimately hindering the adoption of
these techniques. In this paper, we determine the sources of uncertainty in
machine learning and establish the success criteria of a machine learning
system to function well under uncertainty in a cyber-physical manufacturing
system (CPMS) scenario. Then, we propose a multi-agent system architecture
which leverages probabilistic machine learning as a means of achieving such
criteria. We propose possible scenarios for which our proposed architecture is
useful and discuss future work. Experimentally, we implement Bayesian Neural
Networks for multi-tasks classification on a public dataset for the real-time
condition monitoring of a hydraulic system and demonstrate the usefulness of
the system by evaluating the probability of a prediction being accurate given
its uncertainty. We deploy these models using our proposed agent-based
framework and integrate web visualisation to demonstrate its real-time
feasibility.Comment: International Workshop on Service Orientation in Holonic and
  Multi-Agent Manufacturin","Multi Agent System for Machine Learning Under Uncertainty in Cyber
  Physical Manufacturing System",http://arxiv.org/abs/2107.13252,'Organisation for Economic Co-Operation and Development  (OECD)',,10.17863/CAM.51696,core
386209409,2021-02-11T00:00:00,"Wireless sensor networks (WSN) prove to be an enabling technology for Industry 4.0 for their ability to perform in autonomous manner even in regions of extreme conditions. Autonomy brings in independent decision making and exerting controls without manual intervention and frequent maintenance. This paper aims to inculcate intelligence to the WSN exploiting the merits of Artificial Intelligence (AI) algorithms in cheap and most preferred ESP8266 and ESP32 based nodes. Autonomy is brought in by means of optimal data transmission, compressive sensing fault detection and network reconfiguration and energy efficiency. Optimal data transmission is achieved using Q-learning based exploration exploitation algorithm. Compressive sensing performed using Autoencoders ensure reduction in transmission overhead. Fault detection is done using Binary SVM classifier and the net- work re-configures based on physical redundancy. This paper high- lights the implementation of such autonomous WSN in real time along with their performance statistics",Realization of Autonomous Sensor Networks with AI based Self-reconfiguration and Optimal Data Transmission Algorithms in resource constrained nodes,https://core.ac.uk/download/386209409.pdf,Journal of Scientific and Industrial Research (JSIR),,,core
402908136,2021-03-04T00:19:41,"© 2020 IEEE. Scratches on the optical surface can directly affect the reliability of the optical system. Machine vision-based methods have been widely applied in various industrial surface defect inspection scenarios. Since weak scratches imaging in the dark field has an ambiguous edge and low contrast, which brings difficulty in automatic defect detection. Recently, many existing visual inspection methods based on deep learning cannot effectively inspect weak scratches due to the lack of attention-aware features. To address the problems arising from industry-specific characteristics, this paper proposes 'Attention Fusion Network;', a convolutional neural network using attention mechanism built by hard and soft attention modules to generate attention-aware features. The hard attention module is implemented by integrating the brightness adjustment operation in the network, and the soft attention module is composed of scale attention and channel attention. The proposed model is trained on a real-world industrial scratch dataset and compared with other defect inspection methods. The proposed method can achieve the best performance to detect the weak scratch inspection of optical components compared to the traditional scratch detection methods and other deep learning-based methods",Weak Scratch Detection of Optical Components Using Attention Fusion Network,http://hdl.handle.net/10453/146738,'Institute of Electrical and Electronics Engineers (IEEE)',"[{'title': None, 'identifiers': ['issn:2161-8089', '2161-8089', 'issn:2161-8070', '2161-8070']}]",10.1109/CASE48305.2020.9216781,core
475286399,2021-06-08T07:00:00,"Purpose of review
Computing advances over the decades have catalyzed the pervasive integration of digital technology in the medical industry, now followed by similar applications for clinical nutrition. This review discusses the implementation of such technologies for nutrition, ranging from the use of mobile apps and wearable technologies to the development of decision support tools for parenteral nutrition and use of telehealth for remote assessment of nutrition.
Recent findings
Mobile applications and wearable technologies have provided opportunities for real-time collection of granular nutrition-related data. Machine learning has allowed for more complex analyses of the increasing volume of data collected. The combination of these tools has also translated into practical clinical applications, such as decision support tools, risk prediction, and diet optimization.
Summary
The state of digital technology for clinical nutrition is still young, although there is much promise for growth and disruption in the future",The Age of Artificial Intelligence: Use of Digital Technology in Clinical Nutrition,https://core.ac.uk/download/475286399.pdf,SJSU ScholarWorks,,,core
475498831,2021-04-01T00:00:00,"RÉSUMÉ: La demande d’un client peut être mesurée à plusieurs endroits le long de la chaîne logistique.

Lorsque les mesures sont prises au point de vente d’un client, un arrangement collaboratif de type Vendor Managed Inventory (VMI) est possible. Dans ce type d’arrangement, le fournisseur est uniquement responsable de la gestion du stock de ses client. Cependant, ceci nécessite la confiance entre les partenaires de la chaîne logistique, la volonté de partager l’information, ou la mise en application d’un système de télémesure des données qui peuvent tous être une source de risque ou

prohibitive en matière de coût. Ainsi en pratique, un fournisseur exploitant un arrangement VMI peut ne pas être en possession des données du point de vente pour certains clients. Ce sujet a généré quelques travaux dans la littérature et plusieurs améliorations sont possibles pour la gestion d’une chaîne logistique sous ces conditions. L’objectif général de notre recherche est d’améliorer la gestion de la chaîne logistique pour un

fournisseur dans un arrangement collaboratif où il collecte des données par télémesure de la demande au point de vente de ses clients, dans un contexte de demande intermittente. Nous proposons trois contributions sur la prévision, la mesure d’erreur et l’évaluation de la chaîne logistique, tous pour une demande intermittente et en exploitant les données de télémesure. En premier lieu, lorsque les données de demande du point de vente sont absentes, un fournisseur doit se servir d’autres moyens pour déterminer la demande de ses clients. Pour cela, l’historique des livraisons effectuées chez un client peut être agrégé et transformé en série temporelle de la demande, mais ces séries peuvent être bruitées et avoir un comportement intermittent. Des modèles ont été proposés, pour ces données de la demande prises en amont dans la chaîne logistique, qui déduisent la demande d’un client à son point de vente et ainsi améliorent les prévisions de la demande. Cependant, ces modèles n’ont pas pu être évalués à cause d’un manque de vraies données

du point de vente. De plus, aucun modèle de prévision de la demande existant ne considère le contexte d’une demande connue partiellement lorsque les données du point de vente manquent pour un petit groupe de clients. Dans cette situation, nous proposons comme première contribution un modèle d’apprentissage supervisé pour la prévision de la demande. Ce modèle détermine une

relation entre les données de demande du point de vente et les données de demande des livraisons. Cette relation permet d’améliorer les prévisions de la demande pour des clients pour lesquelles les données de point de vente sont inconnues. Nos résultats montrent une amélioration de 10 % en précision comparée aux modèles traditionnels de prévision de la demande. En deuxième lieu, une mesure d’erreur précise est requise pour la validation du choix d’un modèle de prévision. Plusieurs mesures d’erreurs ont été proposées dans la littérature où l’erreur est

calculée sur la partie hors échantillon de la série temporelle. Cependant, ces mesures d’erreurs ne sont pas intuitives ou robustes lorsque les données sont intermittentes. De nouvelles mesures d’erreurs ont été proposées pour rectifier certains de ces problèmes en inférant le mouvement des stocks sous-jacents à la prévision de la demande. Dans un arrangement collaboratif dans lequel les données de la demande du point de vente des clients sont connues, ces données peuvent être

utilisées pour améliorer ces mesures d’erreurs en permettant de véritablement calculer les mouvements du stock chez les clients. Au lieu de comparer les prévisions de la demande unitaires au vrai stock, nous proposons comme deuxième contribution une mesure d’erreur basée sur la capacité à prédire le temps avant d’atteindre une situation de rupture de stock chez les clients. Cette nouvelle mesure d’erreur est intuitive et robuste et est très utile dans un arrangement de type VMI. Nous comparons la mesure d’erreur proposée à celles retrouvées dans la littérature pour pouvoir la

remplacer lorsque les données du point de vente sont absentes, mais l’efficacité est réduite. En troisième lieu, quoique l’amélioration du modèle de prévision et de la mesure d’erreur va bien sûr permettre l’amélioration de la prévision de la demande d’un client, les données utilisées pour la prévision demeurent l’aspect le plus crucial. La littérature promeut le partage des données ou

leur collection par des moyens technologiques comme la télémesure, mais les analyses de type coût-avantage sont rares. Ces analyses permettent de guider un individu lorsqu’il doit décider de mettre en œuvre ou non ces types d’arrangement collaboratif. Nous proposons comme troisième contribution une simulation qui minimise le niveau de stock de sécurité et le nombre de livraisons

à effectuer pour maintenir un niveau de service ciblé. L’accès à deux sources de données de la demande (point de vente des clients et l’historique de livraison du fournisseur) permet de comparer ces deux stratégies informationnelles dans la gestion de la chaîne logistique. Nos résultats montrent que les données du point de vente permettent de diminuer le nombre de livraisons à effectuer de 16 % et la quantité de stock à maintenir chez le client de 43 % tout en garantissant un niveau de service de 100 %. Les trois contributions présentées ci-dessus sont des avancées scientifiques importantes tout en étant des solutions pratiques pour l’industrie. Pour évaluer nos contributions, nous utilisons des données réelles provenant d’un vrai partenaire industriel. Nous prenons soin de bien décrire le partenaire industriel selon les caractéristiques retrouvées dans la littérature sur la chaîne logistique.

Au cœur de notre travail est l’accès fourni par le partenaire à ses données de télémesure de la demande mesurées au point de vente de ses clients. Ces données de télémesure permettent de valider nos contributions. La méthodologie employée est comparative. Nous cherchons à isoler l’effet des données de la demande dans la prévision, la validation des modèles et l’analyse des

avantages dans la gestion de la chaîne logistique.

Finalement, comme perspective de recherche, nous proposons de poursuivre la méthode de caractérisation de série temporelle de la demande intermittente pour permettre la recommandation de décisions logistiques selon les caractéristiques intrinsèques de la demande. Nous proposons l’étude de modèles de prévision de la demande qui intègrent les différentes agrégations temporelles disponibles dans les données recueillies et les différentes agrégations temporelles nécessaires pour

les besoins industriels. Nous proposons, comme dernière perspective, l’étude de l’interdépendance de la prévision, sa mesure d’erreur et les avantages logistiques, soit nos trois contributions, dans

l’analyse complète de la chaîne logistique.----------ABSTRACT: A customer’s demand can be measured at multiple locations along a supply chain. When observations are collected at a customer’s point-of-sale, collaborative Vendor Managed Inventory (VMI) arrangements are possible. However, this requires either trust between supply chain

partners, the willingness to share the information, or the implementation of telemetry, all of which are unreliable and prohibitive. This means that in practice, suppliers operating a VMI arrangement may have missing point-of-sale demand data for some customers. This topic has generated only few works in the literature and numerous improvements can be made for managing a supply chain under these conditions. The main objective of our research is improving supply chain management for a supplier in a collaborative arrangement in which he collects demand data from a customer’s point of sale using telemetry in an intermittent demand context. We propose trois contributions on forecasting, error

measurement and supply chain evaluation, all for an intermittent demand while utilizing telemetry

data. Firstly, when point-of-sale demand information is missing, suppliers turn to other methods of determining their customer’s demand. Historical delivery data can be aggregated to produce demand time series, but these can contain noise and display intermittent behavior. Models have been proposed for use on this upstream demand data which infer a customer’s point-of-sale demand

data to improve the subsequent demand forecast. However, these models are poorly evaluated in the literature due to a lack of access to real point-of-sale data. Furthermore, no current demand forecasting model considers the practical situation where data is partially known—i.e., point-ofsale data is missing for a small subset of customers. In this situation, we propose as our first contribution a supervised learning model to produce a mapping between known point-of-sale demand data and historical delivery demand data, that is then leveraged to improve future demand forecasts for customers with missing point-of-sale data. Our results show up to a 10% accuracy improvement over traditional demand forecasting methods. Secondly, accurate error measurements are required to validate the choice of forecasting models.

Numerous error measurements have been proposed in the literature which calculate the error on out-of-sample time series observations. However, these error measurements are unintuitive and not very robust to intermittent behavior. Newer error measurements have remedied some of these issues by intuiting the underlying stock behavior of forecasted demand. In a collaborative

arrangement for which customers’ point-of-sale demand data is known, point-of-sale data can be used to improve upon these error measurements by calculating the error directly on a customer’s real stock behavior. Instead of comparing the point forecasts results to the real stock, we propose as our second contribution an error measurement for forecasting models based on their accuracy at forecasting stock-out situations. This is both a robust and useful error measurement over the other

alternatives and of preeminent use in VMI arrangements. We further compare our proposed error measurement to those found in the literature which allows them to be used as a replacement to our proposed error measurement when the point-of-sale data is unavailable but with reduced efficacy. Thirdly, although improving the forecasting model and error measurement are both critical for improving a customer’s demand forecast, the underlying data used for demand forecasting is the

foremost constraint—garbage in, garbage out. This is made even more important under a VMI arrangement where suppliers assume the entire responsibility of its customers’ stock. The literature encourages data sharing and collection using technological means such as telemetry, but costbenefit analyses are rare. These analyses can guide practitioners when making decision to

implement these types of arrangements. We propose a simulation-based framework which minimizes the inventory’s safety stock and the number of deliveries required to maintain a specified service level. Access to both point-of-sale demand data and the supplier’s historical delivery demand data allows for comparison between information strategies in supply chain management. Our results show that collecting a customer’s point-of-sale demand data offers more savings in terms of customer inventory stock (16%) than in terms of the number of deliveries (43%) for a targeted service level of 100%. These three proposals presented above both fill a gap in the literature and represent real practical

applications. To evaluate our contributions, we rely on real data provided by a real industrial partner. Care is taken to adequately describe this industrial partner in relation to current supply chain literature. Core to our work is the access the industrial partner provides to telemetric pointof-sale demand data which it collects for its customers. This telemetric point-of-sale data is used

to validate our proposals. The methodology employed to identify the improvements brought by this telemetric demand data is comparative in its nature whereby we isolate the impact of the data in the forecasting, model validation, and supply chain cost benefit analyses. Finally, in terms of perspectives to the work found herein, we propose continuing research in intermittent demand time series characterization methods for logistic recommendations as a function of demand characteristics. We further suggest the study of forecasting models which

integrate the multiple temporalities available in the collected data and the multiple temporalities required for the industry. Lastly, we propose the study of the interdependence between forecasts, error measurements, and logistic advantages—i.e., our three contributions—in the holistic analysis of supply chains","Prévision de la demande intermittente avec ou sans données

de télémesure",,,,,core
475672735,2021-07-08T00:00:00,"Quantificar a qualidade de um modelo é um problema existente na área de Identificação de Sistemas. Esta atividade, também conhecida como validação, é fundamental nas aplicações onde se utilizam Controladores Preditivos baseados em  Modelos, porque estes precisam de um modelo adequado para seu bom funcionamento. Baseado nisto, nesta dissertação são implementados três algoritmos de Inteligência Artificial capazes de predizer, de forma autônoma, quão adequado pode ser um modelo para este tipo de aplicação. Os algoritmos são Árvores de Decisão, Máquina de Suporte de Vetores e Redes Neurais Artificiais. Eles predizem a qualidade do modelo a partir de resultados de outras métricas de validação não existentes. As plantas para a implementação destes algoritmos são: (i) Planta de Clarke (simulada) e (ii) Planta Piloto de Neutralização de pH (real) do Laboratório de Controle de Processos Industriais da  Escola Politécnica da Universidade de São Paulo. Em ambos os casos se usa um  algoritmo Dynamic Matrix Control - DMC ou sua variante Quadratic Dynamic Matrix  Control - QDMC (em caso de se ter restrições) para executar o controle. Como resultado deste trabalho obtiveram-se algoritmos capazes de predizer a qualidade do modelo com uma acurácia de 84,1%, 91,5% e 91,0% para a malha de controle da Planta de Clarke, e de Nível e de pH para a Planta Piloto de Neutralização de pH, respectivamente.Quantifying the quality of a model is an existing problem in the Systems Identification  area. This task, also known as validation, is fundamental in applications where Model  Predictive Control is used, because they need an adequate model for their proper  operation. Based on this need, in this dissertation, the author implements three Artificial Intelligence algorithms that are capable of autonomously predicting how suitable a model can be for this type of application. The algorithms are Decision Trees, Support Vector Machine and Artificial Neural Networks. They predict the quality of the model from the results of other non-existent validation metrics. The plants for the implementation of these algorithms are the Clarke Plant (simulated) and the pH Neutralization Pilot Plant (real) of the Industrial Process Control Laboratory of the Polytechnic School of the University of  São Paulo. In both cases, a Dynamic Matrix Control - DMC algorithm or its Quadratic  Dynamic Matrix Control - QDMC variant (in case of constrained problems) is used to  perform the control. This work results are algorithms capable of predicting the model  quality with an accuracy of 84.1%, 91.5%, and 91.0% for the Clarke Plant, and the Level  and pH control loops pH of the Neutralization Pilot Plant, respectively",Model validation tool for model predictive control applications based on artificial intelligence.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,10.11606/D.3.2020.tde-08072021-140955,core
475663426,2021-01-01T00:00:00,"The progress in technology development over the past decades, both with respect to software and hardware, offers the vision of automated vehicles as means of achieving zero fatalities in traffic. However, the promises of this new technology – an increase in road safety, traffic efficiency, and user comfort – can only be realized if this technology is smoothly introduced into the existing traffic system with all its complexities, constraints, and requirements. SHAPE- IT will contribute to this major undertaking by addressing research questions relevant for the development and introduction of automated vehicles in urban traffic scenarios. Previous research has pointed out several research areas that need more attention for a successful implementation and deployment of human-centred vehicle automation in urban environments. In SHAPE-IT, for example, a better understanding of human behaviour and the underlying psychological mechanisms will lead to improved models of human behaviour that can help to predict the effects of automated systems on human behaviour already during system development. Such models can also be integrated into the algorithms of automated vehicles, enabling them to better understand the human interaction partners’ behaviours. Further, the development of vehicle automation is much about technology (software and hardware), but the users will be humans and they will interact with humans both inside and outside of the vehicle. To be successful in the development of automated vehicles functionalities, research must be performed on a variety of aspects. Actually, a highly interdisciplinary team of researchers, bringing together expertise and background from various scientific fields related to traffic safety, human factors, human-machine interaction design and evaluation, automation, computational modelling, and artificial intelligence, is likely needed to consider the human-technology aspects of vehicle automation. Accordingly, SHAPE-IT has recruited fifteen PhD candidates (Early Stage Researchers – ESRs), that work together to facilitate this integration of automated vehicles into complex urban traffic by performing research to support the development of transparent, cooperative, accepted, trustworthy, and safe automated vehicles. With their (and their supervisors’) different scientific background, the candidates bring different theoretical concepts and methodological approaches to the project. This interdisciplinarity of the project team offers the unique possibility for each PhD candidate to address research questions from a broad perspective – including theories and methodological approaches of other interrelated disciplines. This is the main reason why SHAPE-IT has been funded by the European Commission’s Marie Skłodowska-Curie Innovative Training Network (ITN) program that is aimed to train early state researchers in multidisciplinary aspects of research including transferable skills. With the unique scope of SHAPE-IT, including the human-vehicle perspective, considering different road-users (inside and outside of the vehicle), addressing for example trust, transparency, and safety, and including a wide range of methodological approaches, the project members can substantially contribute to the development and deployment of safe and appreciated vehicle automation in the cities of the future. To achieve the goal of interdisciplinary research, it is necessary to provide the individual PhD candidate with a starting point, especially on the different and diverse methodological approaches of the different disciplines. The empirical, user-centred approach for the development and evaluation of innovative automated vehicle concepts is central to SHAPE- IT. This deliverable (D1.1 “Methodological Framework for Modelling and Empirical Approaches”) provides this starting point. That is, this document provides a broad overview of approaches and methodologies used and developed by the SHAPE-IT ESRs during their research. The SHAPE-IT PhD candidates, as well as other researchers and developers outside of SHAPE-IT, can use this document when searching for appropriate methodological approaches, or simply get a brief overview of research methodologies often employed in automated vehicle research. The first chapter of the deliverable shortly describes the major methodological approaches to collect data relevant for investigating road user behaviour. Each subchapter describes one approach, ranging from naturalistic driving studies to controlled experiments in driving simulators, with the goal to provide the unfamiliar reader with a broad overview of the approach, including its scope, the type of data collected, and its limitations. Each subchapter ends with recommendations for further reading – literature that provide much more detail and examples. The second chapter explains four different highly relevant tools for data collection, such as interviews, questionnaires, physiological measures, and as other current tools (the Wizard of Oz paradigm and Augmented and Virtual Reality). As in the first chapter this chapter provides the reader with information about advantages and disadvantages of the different tools and with proposed further readings. The third chapter deals with computational models of human/agent interaction and presents in four subchapters different modelling approaches, ranging from models based on psychological mechanisms, rule-based and artificial intelligence models to simulation models of traffic interaction. The fourth chapter is devoted to Requirements Engineering and the challenge of communicating knowledge (e.g., human factors) to developers of automated vehicles. When forming the SHAPE-IT proposal it was identified that there is a lack of communication of human factors knowledge about the highly technical development of automated vehicles. This is why it is highly important that the SHAPE-IT ESRs get training in requirement engineering. Regardless of the ESRs working in academia or industry after their studies it is important to learn how to communicate and disseminate the findings to engineers. The deliverable ends with the chapter “Method Champions”. Here the expertise and association of the different PhD candidates with the different topics are made explicit to facilitate and encourage networking between PhDs with special expertise and those seeking support, especially with regards to methodological questions",Methodological Framework for Modelling and Empirical Approaches (Deliverable D1.1 in the H2020 MSCA ITN project SHAPE-IT),,,,10.17196/shape-it/2021/02/D1.1,core
478235725,2021-09-28T00:00:00,"In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios","SafetyNet: Safe planning for real-world self-driving vehicles using
  machine-learned policies",http://arxiv.org/abs/2109.13602,,,,core
437429008,2021-06-22T00:00:00,"The Internet of Things (IoT), in combination with advancements in Big Data, communications and networked systems, offers a positive impact across a range of sectors including health, energy, manufacturing and transport. By virtue of current business models adopted by manufacturers and ICT operators, IoT devices are deployed over various networked infrastructures with minimal security, opening up a range of new attack vectors. Conventional rule-based intrusion detection mechanisms used by network management solutions rely on pre-defined attack signatures and hence are unable to identify new attacks. In parallel, anomaly detection solutions tend to suffer from high false positive rates due to the limited statistical validation of ground truth data, which is used for profiling normal network behaviour. In this work we go beyond current solutions and leverage the coupling of anomaly detection and Cyber Threat Intelligence (CTI) with parallel processing for the profiling and detection of emerging cyber attacks. We demonstrate the design, implementation, and evaluation of Citrus: a novel intrusion detection framework which is adept at tackling emerging threats through the collection and labelling of live attack data by utilising diverse Internet vantage points in order to detect and classify malicious behaviour using graph-based metrics as well as a range of machine learning (ML) algorithms. Citrus considers the importance of ground truth data validation and its flexible software architecture enables both the real-time and offline profiling, detection and classification of emerging cyber-attacks under optimal computational costs. Thus, establishing it as a viable and practical solution for next generation network defence and resilience strategies",Practical Intrusion Detection of Emerging Threats,https://core.ac.uk/download/437429008.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/TNSM.2021.3091517,core
395065592,2021-09-01T00:00:00,"Time series modelling is gaining spectacular popularity in the prediction process of decision making, with applications including real-world management and engineering. However, for short time series, prediction has to face unavoidable limitation for modelling extremely complex systems. It has to apply inadequate and incomplete data from short time to predict unknown observations. With such limited data source, existing techniques, such as statistical modelling or machine learning methods, fail to predict short time series effectively. To address this problem, this paper provides a global framework for short time series modelling predictions, integrating the rolling mechanism, grey model, and meta-heuristic optimization algorithms. In addition, dragonfly algorithm and whale optimization algorithm are investigated and deployed to optimize the framework by enhancing its predicting accuracy with less computational costs. To verify the performance of the proposed framework, three industrial cases are introduced as simulation experiments in this paper. Experimental results confirm that the framework solves corresponding short time series modelling predictions with greater accuracy and speed than the standard GM(1,1) models. The source codes of this framework are available at: https://github.com/zhesencui/HybridRollingGreyFramework.git.</p",A hybrid rolling grey framework for short time series modelling,,'Springer Science and Business Media LLC',,10.1007/s00521-020-05658-0,core
429874403,2021-01-01T00:00:00,"Real-time monitoring of multiphase fluid flows with distributed fibre optic sensing has the potential to play a major role in industrial flow measurement applications. One such application is the optimization of hydrocarbon production to maximize short-term income, and prolong the operational lifetime of production wells and the reservoir. While the measurement technology itself is well understood and developed, a key remaining challenge is the establishment of robust data analysis tools that are capable of providing real-time conversion of enormous data quantities into actionable process indicators. This paper provides a comprehensive technical review of the data analysis techniques for distributed fibre optic technologies, with a particular focus on characterizing fluid flow in pipes. The review encompasses classical methods, such as the speed of sound estimation and Joule-Thomson coefficient, as well as their data-driven machine learning counterparts, such as Convolutional Neural Network (CNN), Support Vector Machine (SVM), and Ensemble Kalman Filter (EnKF) algorithms. The study aims to help end-users establish reliable, robust, and accurate solutions that can be deployed in a timely and effective way, and pave the wave for future developments in the field.publishedVersio",A Survey on Distributed Fibre Optic Sensor Data Modelling Techniques and Machine Learning Algorithms for Multiphase Fluid Flow Estimation,https://core.ac.uk/download/429874403.pdf,'MDPI AG',,10.3390/s21082801,core
384308353,2021-01-25T00:00:00,"Today’s factories are considered as smart ecosystems with humans, machines and devices interacting with each other for efficient manufacturing of products. Industry 4.0 is a suite of enabler technologies for such smart ecosystems that allow transformation of industrial processes. When implemented, Industry 4.0 technologies have a huge impact on efficiency, productivity and profitability of businesses. The adoption and implementation of Industry 4.0, however, require to overcome a number of practical challenges, in most cases, due to the lack of modernisation and automation in place with traditional manufacturers. This paper presents a first of its kind case study for moving a traditional food manufacturer, still using the machinery more than one hundred years old, a common occurrence for small- and medium-sized businesses, to adopt the Industry 4.0 technologies. The paper reports the challenges we have encountered during the transformation process and in the development stage. The paper also presents a smart production control system that we have developed by utilising AI, machine learning, Internet of things, big data analytics, cyber-physical systems and cloud computing technologies. The system provides novel data collection, information extraction and intelligent monitoring services, enabling improved efficiency and consistency as well as reduced operational cost. The platform has been developed in real-world settings offered by an Innovate UK-funded project and has been integrated into the company’s existing production facilities. In this way, the company has not been required to replace old machinery outright, but rather adapted the existing machinery to an entirely new way of operating. The proposed approach and the lessons outlined can benefit similar food manufacturing industries and other SME industries",Towards design and implementation of Industry 4.0 for food manufacturing,https://core.ac.uk/download/384308353.pdf,'Springer Science and Business Media LLC',,10.1007/s00521-021-05726-z,core
479718118,2021-01-01T00:00:00,"What we did This report assesses the potential of data-driven approaches to improving transport infrastructure maintenance. It looks at trends in maintenance strategies, explores how the targeted use of data could make them more effective for different types of transport infrastructure, and looks into implications for policy. The report builds on discussions held during workshops with members of the International Transport Forum’s Corporate Partnership Board. What we found Maintenance constitutes an inevitable, albeit often invisible, part of countries’ transport policies. Increased demand for transport infrastructure accelerates infrastructure’s ageing. The effects of climate change further aggravate this. Unsurprisingly, many governments look for transport infrastructure maintenance policies that provide better value for money than current practices offer. Infrastructure maintenance strategies are gradually shifting towards data-driven approaches. They exploit the power of digital technologies, Big Data analytics and advanced forecasting methodologies. Data-driven approaches have gained momentum in transport infrastructure maintenance as a result of four simultaneous technological innovations. First, the development of digital technologies has resulted in the digitalisation of society, industry and transport, which facilitates data sharing. Second, computing technologies have provided the necessary horsepower for running the digital infrastructure. Third, the Internet of Things and sensor technology have increased the potential for automating reporting from sensors that capture and measure new phenomena and provide data sets that flow through digital infrastructures. Fourth, artificial intelligence (AI) has helped to extract information from vast amounts of data, recognising patterns beyond the capacity of individual observation and exploiting digital infrastructure and computing power. Policy makers are beginning to leverage these developments in various ways. Data-driven maintenance is becoming common in many parts of the transport industry. Railroads collect massive amounts of inspection data from different sources using various methods, such as track inspection cars and drones that gather data to model track degradation. However, the rail sector faces numerous challenges for applying Big Data analysis: a lack of specific data analysis tools, high cost of involving stakeholders and heterogeneous data sources. Also, the algorithms currently used to predict the wear of rail infrastructure only work under lab conditions. For road infrastructure, various automated inspection methods exist. These include vision-based methods, laser scanning, ground penetration radar and a combination of these. All are accurate and effective but usually costly. As a result, the coverage and collection frequency can prove insufficient for detectingchanging road conditions. Several pilot studies have tried to use smartphones to collect data on the state of roads to reduce deployment costs for data-driven maintenance. At airports, the demand for accurate real-time data has spawned systems that automatically acquire and process infrastructure data. Advanced technologies now register when deformities develop on runways. They accurately measure moisture levels, temperature, strain and other factors relevant to wear and degradation. Several airports have built, or plan to build, concrete pavements with embedded strain gauges and other sensors to monitor the stress in the material caused by aircraft. Overall, data-driven approaches to infrastructure maintenance promise to enhance fact-based decision making and capabilities to predict the remaining useful life of assets. They can also improve cost efficiency and environmental sustainability. However, some new challenges need to be addressed, notably for the use of AI. AI predicts future behaviour based on historical data. Yet all predictions can prove incorrect where events do not follow past trends. What we recommendScale up and speed up the deployment of data-driven approaches to transport infrastructure maintenance Transport infrastructure maintenance could benefit from a broader and accelerated roll-out of data-driven approaches. These could improve the quality of assets, enhance the life cycles and save costs - especially when the relevant technologies are well-known, such as sensor technologies. In some cases, more tests and pilot projects will be useful, notably where leveraging data technologies for more effective maintenance policies poses specific challenges, as is the case of artificial intelligence in the railway sector. Update regulation and guidelines for transport infrastructure maintenance to facilitate the introduction of more data-driven approaches Current regulations and guidelines apply to condition-based maintenance strategies. These may set requirements that are ill-adapted to data-driven approaches to maintenance and may hamper their roll-out. Policy makers should ensure that the policies applied to data-driven approaches do not stifle their potential benefits. Ensure data-driven infrastructure maintenance approaches follow good practices in data governance The use of data in infrastructure maintenance must be in line with privacy protection laws and regulations. All data should be anonymised and encrypted. Location and trajectory data should be covered by the most robust protection methods, as they create the severest vulnerabilities for citizens. Tools to limit privacy risks include non-disclosure agreements between data users and providers, the involvement of trusted third parties to conduct the data collection and the development of “safe answers” approaches, in which only query results are exchanged instead of raw data. Governments could also broker data-sharing partnerships for the purpose of data-driven maintenance, for instance, between data providers and infrastructure managers. However, it may want to limit such partnerships to data of public interest and require purpose specificity and data minimisation",Data-driven Transport Infrastructure Maintenance,,,,,core
479991097,2021-01-01T08:00:00,The electronics industry is one of the largest global industries and its growing rapidly as a result of increasing demand from emerging market economies. Many international electronic companies are increasingly producing more electronics in fastest time as possible to meet the high demand and fast changing evolution of electronics. All of the electronics company goal is to produce zero defect and deliver the product on-time to meet the global high demand for electronics. This study seeks to help the manufacturing to determine/predict each production lot on its corresponding electrical test result. One production lot will be subjected to data gathering. Corresponding wire bond ultrasonic signal will be gathered using Arduino Microcontroller and fed on machine learning algorithm to detect/predict if the gathered signal will be rejected and will become possible failure at later electrical testing. This will give a heads-up to the manufacturing if the lot will be to replaced immediately to still meet the committed schedule of delivery. A key point for this study is the usage of a low-cost hardware for signal acquisition and processing. Its embedded design approach lend itself well to real-time operation. More importantly this study would provide a way for legacy manufacturing equipment to be upgraded and thus be integrated into other system that are designed for “Industry 4.0” implementation,Monitoring of the Semiconductor Wirebond Ultrasonic Signal for Prediction of Corresponding Electrical Test Result,,Archīum Ateneo,,,core
427423986,2021-05-08T00:00:00,"[EN] Industrial effluents contain a wide range of organic pollutants that present harmful effects on the environment and deprived communities with no access to clean water. As this organic matter is resistant to conventional treatments, Advanced Oxidation Processes (AOPs) have emerged as a suitable option to counteract these envi-ronmental challenges. Engineered iron oxide nanoparticles have been widely tested in AOPs catalysis, but their full potential as magnetic induction self-heating catalysts has not been studied yet on real and highly contam-inated industrial wastewaters. In this study we have designed a self-heating catalyst with a finely tuned structure of small cores (10 nm) aggregates to develop multicore particles (40 nm) with high magnetic moment and high colloidal stability. This nanocatalyst, that can be separated by magnetic harvesting, is able to increase reaction temperatures (up to 90 ◦C at 1 mg/mL suspension in 5 min) under the action of alternating magnetic fields. This efficient heating was tested in the degradation of a model compound (methyl orange) and real wastewaters, such as leachate from a solid landfill (LIX) and colored wastewater from a textile industry (TIW). It was possible to increase reaction rates leading to a reduction of the chemical oxygen demand of 50 and 90%, for TIW and LIX. These high removal and degradation ability of the magnetic nanocatalyst was sustained with the formation of strong reactive oxygen species by a Fenton-like mechanism as proved by electron paramagnetic resonance. These findings represent an important advance for the industrial implementation of a scalable, non-toxic, self-heating catalysts that can certainly enhance AOP for wastewater treatment in a more sustainable and efficient way.This research was funded by the Spanish Ministry of Economy and Competitiveness under grant MAT2017-88148-R (AEI/FEDER, UE) and Consejo Superior de Invetigaciones Científícas PIE- 201960E062. This study was also supported by the USFQ Collaboration Grant 2018 Nº 11197 and the  USFQ PoliGrants 2018–2019 Nº 12501. E.W., E.L.Jr and R.D.Z. acknowledge Argentine governmental agency ANPCyT (Project No. PICT-2016-0288 and PICT-2018-02565) and UNCuyo (Project No.06/ C527 and  06/C528) for  the  financial support.Peer reviewe",Improving degradation of real wastewaters with self-heating magnetic nanocatalysts,,'Japanese Society of Applied Entomology & Zoology',"[{'title': 'Journal of Cleaner Production', 'identifiers': ['0959-6526', 'issn:0959-6526']}]",10.13039/501100003074,core
387326377,2021-02-23T00:00:00,"Thermal power generation plays a dominant role in the world's electricity
supply. It consumes large amounts of coal worldwide, and causes serious air
pollution. Optimizing the combustion efficiency of a thermal power generating
unit (TPGU) is a highly challenging and critical task in the energy industry.
We develop a new data-driven AI system, namely DeepThermal, to optimize the
combustion control strategy for TPGUs. At its core, is a new model-based
offline reinforcement learning (RL) framework, called MORE, which leverages
logged historical operational data of a TPGU to solve a highly complex
constrained Markov decision process problem via purely offline training. MORE
aims at simultaneously improving the long-term reward (increase combustion
efficiency and reduce pollutant emission) and controlling operational risks
(safety constraints satisfaction). In DeepThermal, we first learn a data-driven
combustion process simulator from the offline dataset. The RL agent of MORE is
then trained by combining real historical data as well as carefully filtered
and processed simulation data through a novel restrictive exploration scheme.
DeepThermal has been successfully deployed in four large coal-fired thermal
power plants in China. Real-world experiments show that DeepThermal effectively
improves the combustion efficiency of a TPGU. We also report and demonstrate
the superior performance of MORE by comparing with the state-of-the-art
algorithms on the standard offline RL benchmarks. To the best knowledge of the
authors, DeepThermal is the first AI application that has been used to solve
real-world complex mission-critical control tasks using the offline RL
approach","DeepThermal: Combustion Optimization for Thermal Power Generating Units
  Using Offline Reinforcement Learning",http://arxiv.org/abs/2102.11492,,,,core
475378248,2021-08-14T00:00:00,"Nowadays, open innovations such as intelligent automation and digitalization are being adopted by every industry with the help of powerful technology such as Artificial Intelligence (AI). This evolution drives systematic running processes, involves less overhead of managerial activities and increased production rate. However, it also gave birth to different kinds of attacks and security issues at the data storage level and process level. The real-life implementation of such AI-enabled intelligent systems is currently plagued by the lack of security and trust levels in system predictions. Blockchain is a prevailing technology that can help to alleviate the security risks of AI applications. These two technologies are complementing each other as Blockchain can mitigate vulnerabilities in AI, and AI can improve the performance of Blockchain. Many studies are currently being conducted on the applicability of Blockchains for securing intelligent applications in various crucial domains such as healthcare, finance, energy, government, and defense. However, this domain lacks a systematic study that can offer an overarching view of research activities currently going on in applying Blockchains for securing AI-based systems and improving their robustness. This paper presents a bibliometric and literature analysis of how Blockchain provides a security blanket to AI-based systems. Two well-known research databases (Scopus and Web of Science) have been examined for this analytical study and review. The research uncovered that idea proposals in conferences and some articles published in journals make a major contribution. However, there is still a lot of research work to be done to implement real and stable Blockchain-based AI systems",Blockchain for securing AI applications and open innovations,,,,,core
387280891,2021-09-29T00:00:00,"Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates","Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space",http://arxiv.org/abs/2011.09747,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/JIOT.2021.3114102,core
477732650,2021-07-11T00:00:00,"International audienceHybrid optimisation methods using machine learning tools are a hot topic in combinatorial optimization.The AI promise is to learn from past solutions or in real time what are the markers of good solutions and then to guide the resolution of the problem.However, for many hard problems such as VRP, a wide range of powerful solvers have already proven their efficiency whereas the most recent successes in machine learning (e.g deep learning) need a huge amount of training data before reaching a satisfactory performance level.In this paper, we study the efficiency of a features-guided multiple-neighborhood search (FG-MNS) for realistic instances of HFVRP.The solver is based on two steps.In the learning step, a powerful solver (RADOS) is used to generate a set of solutions, which are characterized by a set of features described in previous work (Lucas et al., 2019, Lucas et. al, 2020).Then, a decision tree is built on this training set to determine where are the promising areas in the features spaces.In a second step, called exploitation step, the solver uses some rules extracted from the decision tree to guide the solution toward a promising area.We present a wide range of experimentations with different variants of the FG-MNS solver (offline, online).While the results are promising for a better understanding of what makes a good solution, the real benefits of machine learning in an industrial implementation are questionable and discussed in this work",Some insights about the use of machine learning for solving VRP,,HAL CCSD,,,core
440947125,2021-01-01T00:00:00,"Estimation of the quality of food products is vital in determining the properties and validity of the food concerning the baking and other manufacturing processes. This article considers the quality estimation of the wheat bread that is baked under standard conditions. The sensory data are collected in real-time, and the obtained data are analysed using the efficient data analytics to predict the quality of the product. The dataset obtained consists of 300 bread samples prepared in 15 days whose vital physical, chemical, and rheological measures are sensed. The measures of the read are obtained through sensory tools and are gathered as a dataset. The obtained data are generally raw, and hence, the required features are obtained through dimensionality reduction using the Linear Discriminant Analysis (LDA). The processed data and the attributes are given as input to the classifier to obtain final estimation results. The efficient Fuzzy Weighted Relevance Vector Machine (FWRVM) classifier model is developed for this achieving this objective. The proposed quality estimation model is implemented using the MATLAB programming environment with the required setting for the FWRVM classifier. The model is trained and tested with the input dataset with data analysis steps. Some state-of-the-art classifiers are also implemented to compare the evaluated performance of the proposed model. The estimation accuracy is obtained by comparing the number of correctly detected bread classes with the wrongly classified breads. The results indicate that the proposed FWRVM-based classifier estimates the quality of the breads with 96.67% accuracy, 96.687% precision, 96.6% recall, and 96.6% F-measure within 8.96726 seconds processing time which is better than the compared Support vector machine (SVM), RVM, and Deep Neural Networks (DNN) classifiers",Estimation Model for Bread Quality Proficiency Using Fuzzy Weighted Relevance Vector Machine Classifier,,'Hindawi Limited',"[{'title': 'Applied Bionics and Biomechanics', 'identifiers': ['1754-2103', 'issn:1754-2103']}]",10.1155/2021/6670316,core
479101368,2021-01-01T08:00:00,"Industrial Control Systems (ICS) are rapidly shifting from closed local networks, to remotely accessible networks. This shift has created a need for strong cybersecurity anomaly and intrusion detection for these systems; however, due to the complexity and diversity of ICSs, well defined and reliable anomaly and intrusion detection systems are still being developed. Machine learning approaches for anomaly and intrusion detection on the network level may provide general protection that can be applied to any ICS. This paper explores two machine learning applications for classifying the attack label of the UNSW-NB15 dataset. The UNSW-NB15 is a benchmark dataset that was created off general network communications and includes labels for normal behavior and attack vectors. A baseline was created using K-Nearest Neighbors (kNN) due to its mathematical simplicity. Once the baseline was created a feed forward artificial neural network known as a Multi-Layer Perceptron (MLP), was implemented for comparison due to its ease of reuse for running in a production environment. The experimental results show that both kNN and MLPs are effective approaches for identifying malicious network traffic; although, both still need to be further refined and improved before implementation on a real-world production scale",Intrusion detection for industrial control systems,https://core.ac.uk/download/479101368.pdf,EWU Digital Commons,,,core
429098591,2021-12-01T00:00:00,"BACKGROUND: Nutrition labels show potential in increasing healthy food and beverage purchases, but their effectiveness seems to depend on the type of label, the targeted food category and the setting, and evidence on their impact in real-world settings is limited. The aim of this study was to evaluate the effectiveness of an industry-designed on-shelf sugar label on the sales of beverages with no, low, medium and high sugar content implemented within a real-world supermarket. METHODS: In week 17 of 2019, on-shelf sugar labels were implemented by a Dutch supermarket chain. Non-alcoholic beverages were classified using a traffic-light labeling system and included the beverage categories ""green"" for sugar free ( 13.5 g/250 ml). Store-level data on beverage sales and revenue from 41 randomly selected supermarkets for 13 weeks pre-implementation and 21 weeks post-implementation were used for analysis. In total, 30 stores implemented the on-shelf sugar labels by week 17, and the 11 stores that had not were used as comparisons. Outcome measures were differences in the number of beverages sold in the four label categories and the total revenue from beverage sales in implementation stores relative to comparison stores. Analyses were conducted using a multiple-group Interrupted Time Series Approach. Results of individual store data were combined using random effect meta-analyses. RESULTS: At the end of the intervention period, the changes in sales of beverages with green (B 3.4, 95%CI -0.3; 7.0), blue (B 0.0, 95%CI -0.6; 0.7), yellow (B 1.3, 95%CI -0.9; 3.5), and amber (B 0.9, 95%CI -5.5; 7.3) labels were not significantly different between intervention and comparison stores. The changes in total revenues for beverages at the end of the intervention period were also not significantly different between intervention and comparison stores. CONCLUSION: The implementation of an on-shelf sugar labeling system did not significantly decrease unhealthy beverage sales or significantly increase healthier beverage sales. Nutrition labeling initiatives combined with complementary strategies, such as pricing strategies or other healthy food nudging approaches, should be considered to promote healthier beverage purchases",The effect of on-shelf sugar labeling on beverage sales in the supermarket: a comparative interrupted time series analysis of a natural experiment,,'Springer Science and Business Media LLC',,10.1186/s12966-021-01114-x,core
430154594,2021-06-10T00:00:00,"Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.Comment: The paper is published in KDD 202",Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs,http://arxiv.org/abs/2106.06150,,,,core
479018341,2021-05-01T07:00:00,"Advances in artificial intelligence and machine learning have begun a revolution in the understanding and analysis of data across nearly every industry. AI and ML methods (particularly deep neural models) have been successfully scaled to fit the massive datasets available today, especially in image- and text-based tasks. However, in many settings, the application of these advanced methods is held back by underlying data issues that hamstring the models’ generalization performance.
In this work, two such challenges have been considered. The first is data-dependent uncertainty in ground-truth labels. This uncertainty can arise from ambiguity in the labeling process - e.g., whether a certain song should be labeled ‘folk’ or ‘country’ may be answered differently by different annotators - or low data quality that induces annotation mistakes. In this work, a new neighborhood-based scoring system is introduced to identify the data examples that may have suspect labels. A means for translating those uncertainty scores to sample weights is then provided so that the influence of label mistakes on a model’s decision boundary can be reduced.
The second challenge is uneven generalization performance across individuals leading to unfair deployed models. When a deep neural network is deployed to predict data from unseen users, some of those new users could experience poor performance if their data is not typical of that in the training set. To improve model fairness over individuals in a deep learning setting, we use mode connectivity, a technique from the study of neural network loss landscapes, to explore the region around a trained network in parameter space to identify a feasible set of weight configurations with similar overall performance but different distributions of performance over individuals. Multi-objective optimization over that feasible set can then be used to select the best model by observed fairness, a process we call Fairness Maximization via Mode Connectivity (FMMC).
These methods have been validated in real-world settings on time-distributed data, including two human activity recognition datasets and a music genre classification task. Our fairness approach is further validated on a Tamil handwriting classification dataset. Each is shown to surpass the performance of current baseline approaches",Effects of Real-World Data Challenges on Generalization in Applied Machine Learning and Time Series Modeling,,ScholarWorks at UMass Boston,,,core
427129268,2021-04-28T00:00:00,"Corn kernels detection can be implemented in industry area. This can be implemented in the selection and packaging the corn kernels before it is distributed. This technique can be implemented in the selection and packaging machine to detect corn kernels accurately. Corn kernel images was used before it is implemented in real-time. The objective of this research was corn kernel detection using Convolutional Neural Network (CNN) deep learning. This technique consists of 3 main stages, the first preprocessing or normalizing the input of corn kernels image data by wrapping and cropping, both modeling and training the system, and testing. The experiment used CNN method to classify images of dry corn kernels and to determine the accuracy value. This research used 20 dry corn kernels images as testing from 80 dry corn kernels images which used in training dataset. The accuracy of detection was dependent from the size of image and position when the image was taken. The accuracy is around 80% - 100% by using 7 convolutional layers and the average of accuracy for testing data was 0,90296. The convolutional layer which implemented in CNN has the strength to detect features in the input image. 
&nbsp;Pendeteksian bji jagung kering dapat diimplementasikan pada dunia industri. Khususnya ketika pemilahan dan pembungkusan biji jagung kering dilakukan sebelum dipasarkan. Saat ini pemilahan dan pembungkusan belum mengimplementasikan deteksi biji jagung kering sehingga terkadang di dalam kemasan biji jagung kering sering terdapat biji lainnya. Metode pendeteksian ini dapat diintegrasikan dengan mesin pemilah dan pembungkus di industri biji-bijian salah satunya untuk mendeteksi biji jagung kering. Untuk mendapatkan proses pendeteksian yang akurat, citra biji jagung kering digunakan sebagai data pada metode deep learning sebelum implementasi secara real-time. Tujuan dari penelitian ini adalah mendeteksi citra biji jagung kering dengan menerapkan metode Convolutional Neural Network (CNN) deep learning. Teknik ini terdiri dari 3 tahap utama, pertama preprocessing atau menormalkan data input citra biji jagung dengan melakukan wrapping dan cropping. Kedua, pembentukan model dan pelatihan sistem, yang terakhir adalah melakukan untuk pengujian sistem. Penelitian menggunakan CNN untuk mengenali citra biji jagung kering dan menentukan nilai akurasinya. Pada penelitian ini digunakan 20 citra biji jagung yang digunakan sebagai testing data dari 80 citra biji jagung yang digunakan pada training dataset. Nilai akurasi pendeteksian biji jagung kering dipengaruhi oleh ukuran citra dan posisi pengambilan citra dari kamera smartphone. Penggunaan 7 convolutional layer memberikan nilai akurasi berkisar antara 80% - 100% sehingga nilai rata-rata akurasi testing data sebesar 0,90296. Penggunaan convolutional layer mampu mendeteksi kekuatan bentuk dari suatu citra",Penerapan Convolutional Neural Network Deep Learning dalam Pendeteksian Citra Biji Jagung Kering,,'Ikatan Ahli Informatika Indonesia (IAII)',,10.29207/resti.v5i2.3040,core
479313872,2021-07-01T00:00:00,"Abstract Rapid advancements of artificial intelligence of things (AIoT) technology pave the way for developing a digital‐twin‐based remote interactive system for advanced robotic‐enabled industrial automation and virtual shopping. The embedded multifunctional perception system is urged for better interaction and user experience. To realize such a system, a smart soft robotic manipulator is presented that consists of a triboelectric nanogenerator tactile (T‐TENG) and length (L‐TENG) sensor, as well as a poly(vinylidene fluoride) (PVDF) pyroelectric temperature sensor. With the aid of machine learning (ML) for data processing, the fusion of the T‐TENG and L‐TENG sensors can realize the automatic recognition of the grasped objects with the accuracy of 97.143% for 28 different shapes of objects, while the temperature distribution can also be obtained through the pyroelectric sensor. By leveraging the IoT and artificial intelligence (AI) analytics, a digital‐twin‐based virtual shop is successfully implemented to provide the users with real‐time feedback about the details of the product. In general, by offering a more immersive experience in human–machine interactions, the proposed remote interactive system shows the great potential of being the advanced human–machine interface for the applications of the unmanned working space",Artificial Intelligence of Things (AIoT) Enabled Virtual Shop Applications Using Self‐Powered Sensor Enhanced Soft Robotic Manipulator,,'Wiley',"[{'title': 'Advanced Science', 'identifiers': ['2198-3844', 'issn:2198-3844']}]",10.1002/advs.202100230,core
475218111,2021-07-01T00:00:00,"Abstract The convergence of Artificial Intelligence (AI) and the Internet of Things (IoT), or AIoT, has breathed a new life into IoT operations and human-machine interactions. Currently, resource-constrained IoT devices usually cannot provide sufficient capability for data storage and processing so as to support building modern AI models. An intuitive solution is to integrate cloud computing technology into AIoT and exploit the powerful and elastic computing as well as the storage capacity of the servers on the cloud end. Nevertheless, the network bandwidth and communication latency increasingly become serious bottlenecks. The emerging edge computing can complement the cloud-based AIoT in terms of communication latency, and hence attracts more and more attention from the AIoT area. In this paper, we present an industrial edge-cloud collaborative computing platform, namely Sophon Edge, that helps to build and deploy AIoT applications efficiently. As an enterprise-level solution for the AIoT computing paradigm, Sophon Edge adopts a pipeline-based computing model for streaming data from IoT devices. Besides, this platform supports an iterative way for model evolution and updating so as to enable the AIoT applications agile and data-driven. Through a real-world example, we demonstrate the effectiveness and efficiency of building an AIoT application based on the Sophon Edge platform",An edge-cloud collaborative computing platform for building AIoT applications efficiently,,'Springer Science and Business Media LLC',"[{'title': 'Journal of Cloud Computing Advances Systems and Applications', 'identifiers': ['issn:2192-113X', '2192-113x']}]",10.1186/s13677-021-00250-w,core
388364882,2021-03-04T00:00:00,"In the last years, scientific and industrial research has experienced a
growing interest in acquiring large annotated data sets to train artificial
intelligence algorithms for tackling problems in different domains. In this
context, we have observed that even the market for football data has
substantially grown. The analysis of football matches relies on the annotation
of both individual players' and team actions, as well as the athletic
performance of players. Consequently, annotating football events at a
fine-grained level is a very expensive and error-prone task. Most existing
semi-automatic tools for football match annotation rely on cameras and computer
vision. However, those tools fall short in capturing team dynamics, and in
extracting data of players who are not visible in the camera frame. To address
these issues, in this manuscript we present FootApp, an AI-based system for
football match annotation. First, our system relies on an advanced and mixed
user interface that exploits both vocal and touch interaction. Second, the
motor performance of players is captured and processed by applying machine
learning algorithms to data collected from inertial sensors worn by players.
Artificial intelligence techniques are then used to check the consistency of
generated labels, including those regarding the physical activity of players,
to automatically recognize annotation errors. Notably, we implemented a full
prototype of the proposed system, performing experiments to show its
effectiveness in a real-world adoption scenario",FootApp: an AI-Powered System for Football Match Annotation,http://arxiv.org/abs/2103.02938,,,,core
328022848,2021-06-13T00:00:00,"In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the 'deep learning for NLP' community in the past fewyears and
presents it as a coherent story.Comment: Accepted at TKDD for publication. 53 page",Compression of Deep Learning Models for Text: A Survey,http://arxiv.org/abs/2008.05221,,,,core
395604827,2021-01-01T00:00:00,"Tensor Cores are specialized hardware units added to recent NVIDIA GPUs to speed up matrix multiplication-related tasks, such as convolutions and densely connected layers in neural networks. Due to their specific hardware implementation and programming model, Tensor Cores cannot be straightforwardly applied to other applications outside machine learning. In this paper, we demonstrate the feasibility of using NVIDIA Tensor Cores for the acceleration of a non-machine learning application: iterative Computed Tomography (CT) reconstruction. For large CT images and real-time CT scanning, the reconstruction time for many existing iterative reconstruction methods is relatively high, ranging from seconds to minutes, depending on the size of the image. Therefore, CT reconstruction is an application area that could potentially benefit from Tensor Core hardware acceleration. We first studied the reconstruction algorithm's performance as a function of the hardware related parameters and proposed an approach to accelerate reconstruction on Tensor Cores. The results show that the proposed method provides about 5 x increase in speed and energy saving using the NVIDIA RTX 2080 Ti GPU for the parallel projection of 32 images of size 512 x 512. The relative reconstruction error due to the mixed-precision computations was almost equal to the error of single-precision (32-bit) floating- point computations. We then presented an approach for real-time and memory-limited applications by exploiting the symmetry of the system (i.e., the acquisition geometry). As the proposed approach is based on the conjugate gradient method, it can be generalized to extend its application to many research and industrial fields",Accelerating iterative CT reconstruction algorithms using Tensor Cores,https://core.ac.uk/download/395604827.pdf,'Springer Science and Business Media LLC',,10.1007/s11554-020-01069-5,core
478777365,2021-01-01T08:00:00,"Alloying has been used to confer desirable properties to materials. It typically involves the addition of small amounts of secondary elements to a primary element. In the past decade, however, a new alloying strategy that involves the combination of multiple principal elements in high concentrations to create new materials called high- entropy alloys (HEAs) has been in vogue. In the first part, the investigation focused on the fabrication process and property assessment of the additive manufactured HEA to broaden its engineering applications. Additive manufacturing (AM) is based on manufacturing philosophy through the layer-by-layer method and accomplish the near net-shaped components fabrication. Attempt was made to coat AlCoCrFeNi HEA on an AISI 304 stainless steel substrate to integrate their properties, however, it failed due to the cracks at the interface. The implementation of an intermediate layer improved the bond and eliminated the cracks. Next, an AlCoCrFeNiTi0.5 HEA coating was fabricated on the Ti6Al4V substrate, and its isothermal oxidation behavior was studied. The HEA coating effectively improved the Ti6Al4V substrate\u27s oxidation resistance due to the formation of continuous protective oxides. In the second part, research efforts were made on the deep learning-based quality inspection of additive manufactured products. The traditional inspection process has relied on manual recognition, which could suffer from low efficiency and potential bias. A neural-network approach was developed toward robust real-world AM anomaly detection. The results indicate the promising application of the neural network in the AM industry --Abstract, page iv","Fabrication, characterization of high-entropy alloys and deep learning-based inspection in metal additive manufacturing",https://core.ac.uk/download/478777365.pdf,Scholars\u27 Mine,,,core
479163802,2091-04-28T00:00:00,"Questo lavoro di tesi è il risultato del tirocinio svolto presso la società di consulenza Accenture S.p.A. in ambito Enterprise Technology Services & Solution per il settore farmaceutico. L’obiettivo dell’elaborato è quello di descrivere un caso concreto di implementazione SAP in una società farmaceutica con focus sulla contabilità aziendale, Financial Accounting. In un primo momento vengono definiti i sistemi Enterprise Resource Planning, delineando la loro implementazione ed introduzione in azienda con particolare attenzione al software gestionale SAP e ai suoi moduli operativi. In seguito sono analizzati il contesto di lavoro e gli obiettivi di progetto. È descritto, inoltre, lo svolgimento dettagliato del progetto lungo tutte le fasi operative con particolare riferimento alla fase di Design & Build, fulcro dell’esperienza professionale intrapresa, analizzandone la metodologia Hybrid Agile, un approccio ibrido che combina la metodologia Agile e quella Waterfall. Nello specifico, l’elaborato tratta l’implementazione del modulo Financial Accounting di SAP illustrando lo sviluppo delle principali soluzioni progettuali accolte a livello funzionale e tecnico. Infine sono riportati i risultati raggiunti e gli sviluppi futuri del progetto.
This thesis work refers to an internship carried out at consulting company Accenture S.p.A. in Enterprise Technology Services & Solution for pharmaceutical industry. The objective of the paper is to describe a real case of SAP implementation in a pharmaceutical company with a focus on Financial Accounting. Firstly, a brief definition of Enterprise Resource Planning systems and their implementation in a company are reported with particular attention to SAP management software and its operating modules. Then, the working context and the project objectives are analyzed. A detailed project presentation is shown along all the operational phases with particular reference to Design & Build phase involved in internship and the Hybrid Agile methodology which combine Agile and Waterfall approach. In particular, the paper deals with the implementation of SAP Financial Accounting module, illustrating the main design solutions developed at functional and technical level. Finally, the results achieved and the future developments of the project are reported",Sviluppo con Hybrid Agile Methodology e introduzione di SAP S/4HANA - modulo Financial Accounting presso un?azienda farmaceutica,,'Pisa University Press',,,core
389476431,2021-01-01T00:00:00,"This article belongs to the Special Issue The Artificial Intelligence Technologies for Electric Power SystemsThe scheduling of tasks in a production line is a complex problem that needs to take into account several constraints, such as product deadlines and machine limitations. With innovative focus, the main constraint that will be addressed in this paper, and that usually is not considered, is the energy consumption cost in the production line. For that, an approach based on genetic algorithms is proposed and implemented. The use of local energy generation, especially from renewable sources, and the possibility of having multiple energy providers allow the user to manage its consumption according to energy prices and energy availability. The proposed solution takes into account the energy availability of renewable sources and energy prices to optimize the scheduling of a production line using a genetic algorithm with multiple constraints. The proposed algorithm also enables a production line to participate in demand response events by shifting its production, by using the flexibility of production lines. A case study using real production data that represents a textile industry is presented, where the tasks for six days are scheduled. During the week, a demand response event is launched, and the proposed algorithm shifts the consumption by changing task orders and machine usage.This work has received funding from Portugal 2020 under SPEAR project (NORTE-01-0247-FEDER-040224) and from FEDER Funds through COMPETE program and from National Funds through (FCT) under the project UIDB/00760/2020, and CEECIND/02887/2017.info:eu-repo/semantics/publishedVersio",Production Line Optimization to Minimize Energy Cost and Participate in Demand Response Events,https://core.ac.uk/download/389476431.pdf,'MDPI AG',"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",10.3390/en14020462,core
291715296,10000-01-01,"Planning is the task of finding a set of operators whose executive transforms the current world state into a world state which satisfies some goal criterion. Because many tasks involve focussed change of a world state, planning techniques are relevant to a wide variety of important AI tasks such as automatic programming, process design and control, and manufacturing engineering.However, when planning in complex, real-world domains, large amounts of knowledge are needed to adequately describe world behavior. With a large domain theory, complete reasoning can become a computationally intractable task. Consequently, even if a system has a complete and correct domain theory the computational demands of exhaustive reasoning may prevent successful planning.This thesis describes incremental reasoning and learning techniques to reduce the cost of planning in computationally intractable domains. In this approach plans are initially constructed using inference limiting simplifications. Because limiting inference implies not exhaustively checking all possible inferences, resulting plans may make incorrect predictions. In order to deal with this difficulty the system uses these incorrect goal predictions to direct a refinement process which expands the limited inference of the initial plan, thus preventing recurrence of the incorrect goal prediction. By using executive feedback to focus attention upon parts of the plan requiring further inference, the system avoids the computationally intractable blind search of potentially relevant inferences required by exhaustive reasoning. The class of limited inference simplifications and refinement techniques described in this thesis have been shown to have the properties of convergence upon soundness (i.e. a plan will eventually be refined to make the same goal predictions as a plan developed using exhaustive reasoning) and completeness (i.e. the simplifications will not cause the planner to overlook any potential solutions considered by an exhaustive planner).This incremental reasoning approach has been validated in two ways. First, a complexity analysis of the computational savings of the incremental reasoning approach has been performed. Second, this approach has been fully implemented and this implementation has been used to empirically compare the cost of the incremental reasoning approach and the exhaustive reasoning approach in two domains.U of I OnlyETDs are only available to UIUC Users without author permissio",An explanation-based learning approach to incremental planning,,,,,core
323317621,2023-05-11T00:00:00,"IL mio elaborato pone le sue basi sull'innovazione tecnologica, che viene definita come quell'insieme di attività che le imprese e le istituzioni adottano al fine di introdurre nuovi prodotti e servizi, per renderli disponibili e fruibili ai consumatori. L’innovazione tecnologica ha influenzato e condizionato le varie epoche, difatti, nel 1784 , con la prima rivoluzione industriale, si è assistito all'invenzione del telaio e del motore a vapore, per poi arrivare al 1890 dove l’invenzione dell’energia elettrica è stata sicuramente una delle scoperte più importanti, fino al 1970 in cui si è assistito al sopravvento dell’automazione e quindi alla costruzione dei primi computer. Arriviamo poi ai primi anni del 2000, dove in occasione della fiera di Hannover, per descrivere l’avvento della quarta rivoluzione industriale, fu coniato il termine “industria 4.0”, ad evidenziare il passaggio da un sistema basato unicamente sulle interazioni fisiche tra uomo e macchina ad uno in cui l’interazione tra dati e macchine costituisce la maggior parte delle operazioni produttive. Nel settore food, sia le ricerche che le innovazioni tecnologiche sono state condotte non solo da enti di ricerca ed università ma anche dalle start-up, al fine di ottenere miglioramenti tecnologici e prodotti finora sconosciuti.
Il termine start-up (dall'inglese “inizio”), indica l’avvio di una nuova impresa con tutto ciò che ne comporta sia dal punto di vista strutturale che organizzativo e quest’elaborato infatti, si basa sull'utilizzo del software SCM (Safeaty Content Management), nato dallo Spin-Off della Università di Pisa, ed applicato ad un servizio di catering aereo al fine di integrarsi nella gestione delle fasi che caratterizzano l’azienda stessa evidenziandone i vantaggi e le potenzialità.
La missione principale del software è quella di aiutare le aziende alimentari a redigere un piano di autocontrollo basato sul principio dell’ HACCP, senza incappare in errori o dimenticanze, il tutto a portata di un click. Il sistema HACCP sta alla base della piramide della sicurezza alimentare, ma prima ancora incontriamo il regolamento 178/02 e il pacchetto igiene. Il regolamento 178/02, costituisce la fonte primaria del moderno diritto alimentare europeo e racchiude in un unico testo i diritti ed i doveri di tutti gli operatori del settore alimentare, quindi sia quelli attribuiti agli OSA che quelli designati alle Autorità Competenti che effettuano i controlli in materia. Stabilisce inoltre i principi e i requisiti generali della legislazione alimentare segnando il passaggio da una moltitudine di normative nazionali, in attuazione delle direttive comunitarie ad un insieme razionale di regole comuni testualmente applicate nell'intero Mercato Unico. Con l’avvento di questa normativa poi, si è attribuita una responsabilità primaria del raggiungimento degli obiettivi, agli operatori del settore alimentare; il compito di verifica invece, alle autorità di controllo. Si arriva poi al pacchetto igiene, un insieme di quattro testi legislativi, pubblicati per la prima volta nel 2004 ma entrati in vigore il 1° gennaio del 2006, e nominati: Regolamento 852/04, Regolamento 853/04, Regolamento 854/04 e regolamento 882/04 (gli ultimi due regolamenti sono stati abrogati e sostituiti dal Regolamento 625/2017).
I primi due regolamenti, sono indirizzati alle responsabilità che gli OSA devono ottemperare per il raggiungimento degli obiettivi di sicurezza; gli ultimi, invece, sono destinati alle autorità di controllo. Arriviamo così all’HACCP, acronimo di “Hazard Analysis and Critical Control Points”, ovvero “Analisi del pericolo e punti critici di controllo” che costituisce un valido sistema, organizzato e razionale oltre che condiviso a livello europeo, per l’applicazione dell’autocontrollo degli operatori che operano nelle industrie che trattano, manipolano o distribuiscono alimenti.
L’applicazione del sistema HACCP nasce proprio dall'esigenza di garantire la salubrità delle produzioni alimentari cambiando approccio, cioè passando da un controllo a valle del processo produttivo, che esisteva in passato ed effettuava il controllo sul prodotto finito, ad un controllo del processo produttivo in ogni sua fase, identificando quindi tutti i rischi che possono influire sulla sicurezza degli alimenti e attuando tutte le misure preventive necessarie a tenerli sotto controllo. Per cui, da un controllo di prodotto di tipo tradizionale, si è passati ad un controllo di processo, cioè il controllo di tipo moderno. In Italia è stato introdotto nel 1997, dal D.lgs 155 del ’97. Attualmente è in vigore poiché contemplato dal Regolamento CE 852 del 2004, il quale ha abrogato il precedente D.lgs 155 del ’97. Lo sviluppo del metodo HACCP prevede che debbano essere attuati e rispettati i cosiddetti “programmi prerequisito” o PRPs, cioè delle condizioni di base ed attività necessarie a mantenere un ambiente in condizioni consone per la preparazione di prodotti sani e sicuri per il consumo umano.
L’efficacia di questo metodo però si completa pienamente solo con l’ottemperanza a 5 fasi preliminari (costituzione dell’HACCP team, descrizione del prodotto, descrizione dei possibili usi, realizzazione di un diagramma di flusso, verifica in campo) e 7 principi (analisi dei pericoli, individuazione dei CCP, definizione dei limiti critici, definizione dei monitoraggi, individuazione delle azioni correttive, verifica dell’efficacia del sistema e verifica della documentazione).
Il software utilizzato per questa tesi, permette di redigere un piano di gestione ed autocontrollo aziendale, basato proprio sui dettami dell’HACCP ma il tutto a portata di un click. Nel caso specifico, lo abbiamo utilizzato per un’azienda londinese, la Niche free-from kitchen, che opera nel settore del catering aereo. Dopo una prima analisi, abbiamo riscontrato che il software, semplifica le operazioni di gestione: difatti la visione d’insieme delle produzioni aiuta l’operatore a tenere sotto controllo in tempo reale eventuali non conformità per una risposta immediata; la dematerializzazione dei documenti porta benefici in termini di precisione del lavoro, risparmio economico e riduzione di ingombro volumetrico in ufficio nonché un impatto ambientale inferiore; per ultimo, ma non meno importante, essendo la gestione totalmente digitalizzata, è senz'altro d’aiuto in un momento come quello che stiamo vivendo, in cui lo smart working ha assunto un’importanza fondamentale.
My thesis its foundations on technological innovation, which is described as the set of activities that businesses and institutions adopt at the end of new products and services, to make them available and usable to consumers. Technological innovation has influenced and conditioned the various eras, in fact, in 1784, with the first industrial revolution, we witnessed the invention of the frame and the steam engine, and then went on to 1890 where the electrical invention was certainly one of the most important discoveries, until 1970 in which he witnessed the upper hand of automation and therefore the construction of the first computers. We come then to the early 2000s, where on the occasion of the Hannover fair, to describe the advent of the fourth industrial revolution, the term ""industry 4.0"" was coined, to highlight the transition from a system based on physical interactions between man and machine to one in which the interaction between the data and the machines carry out most of the production operations. In the food sector, both research and technological innovations have been conducted not only by research entities and universities but also by start-ups, in order to obtain technological improvements and unknown products.
The term start-up (from English ""start""), indicates an intervention by a new company with all that is necessary both from a structural and organizational point of view and this elaborate, in fact, is based on the use of the SCM software (Safeaty Content Management), born from the Spin-Off of the University of Pisa, and applied to an air catering service in order to integrate itself into the management of the phases that carried out the same highlighting its advantages and potential.
The main mission of the software is to help food companies to draw up a self-control plan based on the HACCP principle, without running into errors or forgetfulness, all just a click away. The HACCP system is at the base of the food safety pyramid, but before that we meet regulation 178/02 and the hygiene package. Regulation 178/02, constitutes the primary source of modern European food law and contains in a single text the rights and duties of all operators in the food sector, therefore both those attributed to the FBOs and those designated to the Competent Authorities that carry out the controls in the field. It also establishes the general principles and requirements of food law, marking the transition from a multitude of national regulations, in implementation of the EU directives to a rational set of common rules verbatim applied in the entire Single Market. With the advent of this legislation, then, primary responsibility for achieving the objectives has been attributed to operators in the food sector; the task of verification, however, to the supervisory authorities. Then comes the hygiene package, a set of four legislative texts, published for the first time in 2004 but which entered into force on 1st January 2006, and named: Regulation 852/04, Regulation 853/04, Regulation 854/04 and regulation 882/04 (the last two regulations have been repealed and replaced by regulation 625/2017).
The first two regulations are addressed to the responsibilities that the FBOs must comply with in order to achieve the safety objectives; the latter, however, are intended for supervisory authorities. Thus we come to HACCP, an acronym for ""Hazard Analysis and Critical Control Points"", or ""Hazard Analysis and Critical Control Points"" which constitutes a valid system, organized and rational as well as shared at European level, for the application of the self-control of operators operating in the industries that treat, manipulate or distribute food.
The application of the HACCP system arises precisely from the need to guarantee the healthiness of food production by changing the approach, that is, passing from a control downstream of the production process, which existed in the past and carried out the control on the finished product, to a control of the production process at each stage, therefore identifying all the risks that can affect the safety of food and implementing all the preventive measures necessary to keep them under control. So, from a traditional product control, we have passed to a process control, that is the modern control. In Italy it was introduced in 1997, by Legislative Decree 155 of 1997. It is currently in force since it is covered by EC Regulation 852 of 2004, which repealed the previous Legislative Decree 155 of 1997. The development of the HACCP method requires that the so-called ""prerequisite programs"" or PRPs, that is the basic conditions and activities necessary to maintain an environment in a suitable condition for the preparation of healthy and safe products for human consumption, must be implemented and respected.
The effectiveness of this method, however, is only fully completed with compliance with 5 preliminary phases (establishment of the HACCP team, description of the product, description of possible uses, creation of a flowchart, field verification) and 7 principles ( hazard analysis, identification of CCPs, definition of critical limits, definition of monitoring, identification of corrective actions, verification of the effectiveness of the system and verification of documentation).
The software used for this thesis allows you to draw up a business management and self-control plan, based precisely on the dictates of HACCP but all within a click. In the specific case, we used it for a London-based company, the Niche free-from kitchen, which operates in the airline catering sector.
After a first analysis, we found that the software simplifies management operations: in fact, the overall view of the productions helps the operator to monitor any non-compliance in real time for an immediate response; the dematerialisation of documents brings benefits in terms of precision of work, economic savings and reduction of volumetric dimensions in the office as well as a lower environmental impact; last, but not least, being fully digitalized management, it is certainly helpful at a time like the one we are experiencing, in which smart working has taken on a fundamental importance",Informatizzazione dei sistemi di gestione della sicurezza alimentare: valutazione ed applicabilità in un sito di produzione pasti per catering aereo.,,'Pisa University Press',,,core
324071199,2021-02-13T00:00:00,"Numerous Machine Learning (ML) bias-related failures in recent years have led
to scrutiny of how companies incorporate aspects of transparency and
accountability in their ML lifecycles. Companies have a responsibility to
monitor ML processes for bias and mitigate any bias detected, ensure business
product integrity, preserve customer loyalty, and protect brand image.
Challenges specific to industry ML projects can be broadly categorized into
principled documentation, human oversight, and need for mechanisms that enable
information reuse and improve cost efficiency. We highlight specific roadblocks
and propose conceptual solutions on a per-category basis for ML practitioners
and organizational subject matter experts. Our systematic approach tackles
these challenges by integrating mechanized and human-in-the-loop components in
bias detection, mitigation, and documentation of projects at various stages of
the ML lifecycle. To motivate the implementation of our system -- SIFT (System
to Integrate Fairness Transparently) -- we present its structural primitives
with an example real-world use case on how it can be used to identify potential
biases and determine appropriate mitigation strategies in a participatory
manner.Comment: 14 pages, 4 figure",Towards Integrating Fairness Transparently in Industrial Applications,http://arxiv.org/abs/2006.06082,,,,core
388998882,2021-02-01T00:00:00,"149-158Wireless sensor networks (WSN) prove to be an enabling technology for Industry 4.0 for their ability to perform in autonomous manner even in regions of extreme conditions. Autonomy brings in independent decision making and exerting controls without manual intervention and frequent maintenance. This paper aims to inculcate intelligence to the WSN exploiting the merits of Artificial Intelligence (AI) algorithms in cheap and most preferred ESP8266 and ESP32 based nodes. Autonomy is brought in by means of optimal data transmission, compressive sensing fault detection and network reconfiguration and energy efficiency. Optimal data transmission is achieved using Q-learning based exploration exploitation algorithm. Compressive sensing performed using Autoencoders ensure reduction in transmission overhead. Fault detection is done using Binary SVM classifier and the network re-configures based on physical redundancy. This paper highlights the implementation of such autonomous WSN in real time along with their performance statistics",Realization of Autonomous Sensor Networks with AI based Self-reconfiguration and Optimal Data Transmission Algorithms in Resource Constrained Nodes,https://core.ac.uk/download/388998882.pdf,"NISCAIR-CSIR, India",,,core
477983070,2021-01-01T00:00:00,"The progress in technology development over the past decades, both with respect to software and hardware, offers the vision of automated vehicles as means of achieving zero fatalities in traffic. However, the promises of this new technology – an increase in road safety, traffic efficiency, and user comfort – can only be realized if this technology is smoothly introduced into the existing traffic system with all its complexities, constraints, and requirements. SHAPE- IT will contribute to this major undertaking by addressing research questions relevant for the development and introduction of automated vehicles in urban traffic scenarios. Previous research has pointed out several research areas that need more attention for a successful implementation and deployment of human-centred vehicle automation in urban environments.In SHAPE-IT, for example, a better understanding of human behaviour and the underlying psychological mechanisms will lead to improved models of human behaviour that can help to predict the effects of automated systems on human behaviour already during system development. Such models can also be integrated into the algorithms of automated vehicles, enabling them to better understand the human interaction partners’ behaviours.Further, the development of vehicle automation is much about technology (software and hardware), but the users will be humans and they will interact with humans both inside and outside of the vehicle. To be successful in the development of automated vehicles functionalities, research must be performed on a variety of aspects. Actually, a highly interdisciplinary team of researchers, bringing together expertise and background from various scientific fields related to traffic safety, human factors, human-machine interaction design and evaluation, automation, computational modelling, and artificial intelligence, is likely needed to consider the human-technology aspects of vehicle automation.Accordingly, SHAPE-IT has recruited fifteen PhD candidates (Early Stage Researchers – ESRs), that work together to facilitate this integration of automated vehicles into complex urban traffic by performing research to support the development of transparent, cooperative, accepted, trustworthy, and safe automated vehicles. With their (and their supervisors’) different scientific background, the candidates bring different theoretical concepts and methodological approaches to the project. This interdisciplinarity of the project team offers the unique possibility for each PhD candidate to address research questions from a broad perspective – including theories and methodological approaches of other interrelated disciplines. This is the main reason why SHAPE-IT has been funded by the European Commission’s Marie Skłodowska-Curie Innovative Training Network (ITN) program that is aimed to train early state researchers in multidisciplinary aspects of research including transferable skills. With the unique scope of SHAPE-IT, including the human-vehicle perspective, considering different road-users (inside and outside of the vehicle), addressing for example trust, transparency, and safety, and including a wide range of methodological approaches, the project members can substantially contribute to the development and deployment of safe and appreciated vehicle automation in the cities of the future.To achieve the goal of interdisciplinary research, it is necessary to provide the individual PhD candidate with a starting point, especially on the different and diverse methodological approaches of the different disciplines. The empirical, user-centred approach for the development and evaluation of innovative automated vehicle concepts is central to SHAPE- IT. This deliverable (D1.1 “Methodological Framework for Modelling and Empirical Approaches”) provides this starting point. That is, this document provides a broad overview of approaches and methodologies used and developed by the SHAPE-IT ESRs during their research. The SHAPE-IT PhD candidates, as well as other researchers and developers outside of SHAPE-IT, can use this document when searching for appropriate methodological approaches, or simply get a brief overview of research methodologies often employed in automated vehicle research.The first chapter of the deliverable shortly describes the major methodological approaches to collect data relevant for investigating road user behaviour. Each subchapter describes one approach, ranging from naturalistic driving studies to controlled experiments in driving simulators, with the goal to provide the unfamiliar reader with a broad overview of the approach, including its scope, the type of data collected, and its limitations. Each subchapter ends with recommendations for further reading – literature that provide much more detail and examples.The second chapter explains four different highly relevant tools for data collection, such as interviews, questionnaires, physiological measures, and as other current tools (the Wizard of Oz paradigm and Augmented and Virtual Reality). As in the first chapter this chapter provides the reader with information about advantages and disadvantages of the different tools and with proposed further readings.The third chapter deals with computational models of human/agent interaction and presents in four subchapters different modelling approaches, ranging from models based on psychological mechanisms, rule-based and artificial intelligence models to simulation models of traffic interaction.The fourth chapter is devoted to Requirements Engineering and the challenge of communicating knowledge (e.g., human factors) to developers of automated vehicles. When forming the SHAPE-IT proposal it was identified that there is a lack of communication of human factors knowledge about the highly technical development of automated vehicles. This is why it is highly important that the SHAPE-IT ESRs get training in requirement engineering. Regardless of the ESRs working in academia or industry after their studies it is important to learn how to communicate and disseminate the findings to engineers.The deliverable ends with the chapter “Method Champions”. Here the expertise and association of the different PhD candidates with the different topics are made explicit to facilitate and encourage networking between PhDs with special expertise and those seeking support, especially with regards to methodological questions.Transport and Plannin",Methodological Framework for Modelling and Empirical Approaches (Deliverable D1.1 in the H2020 MSCA ITN project SHAPE-IT),,SHAPE-IT Consortium,,10.17196/shape-it/2021/02/D1.1,core
478235498,2021-09-27T00:00:00,"The construction industry is one of the main producers of greenhouse gasses
(GHG). Quantifying the amount of air pollutants including GHG emissions during
a construction project has become an additional project objective to
traditional metrics such as time, cost, and safety in many parts of the world.
A major contributor to air pollution during construction is the use of heavy
equipment and thus their efficient operation and management can substantially
reduce the harm to the environment. Although the on-road vehicle emission
prediction is a widely researched topic, construction equipment emission
measurement and reduction have received very little attention. This paper
describes the development and deployment of a novel framework that uses machine
learning (ML) methods to predict the level of emissions from heavy construction
equipment monitored via an Internet of Things (IoT) system comprised of
accelerometer and gyroscope sensors. The developed framework was validated
using an excavator performing real-world construction work. A portable emission
measurement system (PEMS) was employed along with the inertial sensors to
record data including the amount of CO, NOX, CO2, SO2, and CH4 pollutions
emitted by the equipment. Different ML algorithms were developed and compared
to identify the best model to predict emission levels from inertial sensors
data. The results showed that Random Forest with the coefficient of
determination (R2) of 0.94, 0.91 and 0.94 for CO, NOX, CO2, respectively was
the best algorithm among different models evaluated in this study","Automated Estimation of Construction Equipment Emission using Inertial
  Sensors and Machine Learning Models",http://arxiv.org/abs/2109.13375,,,,core
443944421,2021-06-01T07:00:00,"The advent of Chip Multiprocessor (CMP) with high performance, compact size and power efficiency has made many engineering marvel possible. CMPs has played great role in industrial automation, autonomous vehicle, embedded AI, and medical prognosis. In industrial autonomy or in autonomous vehicle there are many critical task which has to be run in isolation without any interference and delay. Virtualization software (Hypervisors) are being used for application isolation in CMPs. Hypervisors such as XEN, KVM are fully fledged hypervisor with many features and have their own scheduling scheme thus, scheduling overhead. In this thesis we used light-weight partitioning hypervisor known as Jailhouse in order to provide isolation to critical task. From our experiment we see that Jailhouse provide better isolation without any scheduling overhead which is suitable for real time application. As Jailhouse partition available resources among cells without any emulation, the number of cell we can create is limited. Also, the resources from root cell (which runs Linux) get divided and application running on it may suffer from resource constraints. We purpose adaptive offloading in order to address this issue which shows performance and quality improvement.  We also explore deep learning and its implementation in edge computing device. The availability of GPUs and large data set made it possible to use deep learning state-of-art in many fields computer vision, medical diagnosis, image processing, surveillance, etc. It is evident that deep learning consists of two parts training and inferencing, both of these are power and compute intensive.  We implemented YOLOV3 object detection state-of-art  algorithm in NVIDIA AGX Xavier. We utilized Tensor and NVDLA cores in the Xavier using NVIDIA TensorRT and CUDA library. This has resulted more than 100% improvement in performance and significant decrease in power consumption from original YOLOV3 in FP16 precision. We explorer FP16 and INT8 precision with TensorRT, and DLA. INT8 precision further optimizes the performance and power with some compromise in accuracy. Our results shows, we can optimize inference engine by using TensorRT and DLA in edge computing device like Jetson Xavier",EFFICIENT RESOURCE MANAGEMENT ON EMBEDDED DEVICES VIA ISOLATION AND ADAPTIVE RESOURCE ALLOCATION,,OpenSIUC,,,core
401949336,2021-05-19T00:00:00,"Target settings. This study examines the possibilities of modern geographic information and cloud technologies and prospects for their use for administrative and economic management of an airport. The study is related to the implementation of the State Target Program for Airport Development until 2023 and the Aviation Transport Strategy of Ukraine until 2030, which aims to develop the aviation industry in Ukraine, bringing airport infrastructure to the requirements of the European Union. The Law of Ukraine «On the National Infrastructure of Geospatial Data» and «Consolidated Concept of VIM Implementation in Ukraine» has a great influence on the formation of geospatial data of airports.
Actual scientific researches and issues analysis. The paper analyzes and summarizes publications on methods of obtaining geospatial data, implementation of geographic information technologies, virtual, augmented and mixed reality technologies, artificial intelligence and the concept of «smart» city for administrative and economic management of airports.
Uninvestigated parts of general matters defining. Analysis of recent research and publications has shown that the prospects for the introduction of geographic information technology for administrative and economic asset management of Ukrainian airports need further research, as these issues are very important and relevant, given the rapid growth of digital society, environment and infrastructure.
The research objective. The purpose of this study is to analyze the possibilities and prospects for the introduction of modern technologies for processing and visualization of geospatial data for administrative and economic management of the airport and the development of a conceptual model. The task of the research is to analyze the methods of obtaining geospatial data of the airport, the use of geographic information systems in airports, artificial intelligence technologies, virtual, augmented and mixed reality, the Internet of Things, digital duplicates, implementation of the concept of «smart» city, etc.
The statement of basic materials. Geospatial data is created digitally using modern information and cloud technologies that offer a wide range of equipment, software, methods and technologies for working with geospatial information. Every year, new technologies that are used in the administrative and economic management of airports appear: cloud data acquisition methods, geographic information systems, artificial intelligence technologies, virtual reality, the Internet of Things, digital counterparts, «smart» cities, and more. Successful integration and use of existing capabilities for the collection, storage, processing and visualization of geospatial data of airports will ensure their effective management and economic growth.
Conclusions. Based on the analysis of the possibilities of using virtual, augmented and mixed reality technologies, artificial intelligence, digital duplicates and the concept of «smart» cities in airports, a conceptual model of prospects for using geospatial data of the airport to address administrative and economic management of the property complex was developed.Актуальність теми дослідження. Останніми роками в аеропортах світу активно впроваджуються хмарні технології збору, опрацювання та візуалізації геопросторових даних: лазерне та лідарне сканування, інтеграція ВІМ/GIS моделей, застосування штучного інтелекту, технологій віртуальної та доповненої реальності, цифрових двійників та «розумних міст». Для України, яка активно йде по шляху цифровізації та впровадження сучасних геоінформаційних технологій у багатьох сферах діяльності, розробка нових методів та підходів для адміністративно-господарського управління аеропортовими комплексами є актуальним та перспективним напрямом.
Постановка проблеми. У цьому дослідженні розглянуто можливості сучасних геоінформаційних та хмарних технологій і перспективи їх використання для адміністративно-господарського управління територією аеропорту. Дослідження пов’язано з реалізацією Державної цільової програми розвитку аеропортів на період до 2023 року та Авіаційної транспортної стратегії України до 2030 року, метою яких є розвиток авіаційної галузі в Україні, приведення інфраструктури аеропортів до вимог Європейського Союзу. Також великий вплив на формування геопросторових даних аеропортів має Закон України «Про національну інфраструктуру геопросторових даних» та Консолідована Концепція впровадження ВІМ в Україні.
Аналіз останніх досліджень і публікацій. У роботі були проаналізовані та узагальнені публікації, що присвячені методам отримання геопросторових даних, впровадження геоінформаційних технологій, технологій віртуальної, доповненої та змішаної реальності, штучного інтелекту та концепції «розумного міста» для адміністративно-господарського управління аеропортами.
Виділення недосліджених частин загальної проблеми. Аналіз останніх досліджень і публікацій показав, що питання перспектив впровадження геоінформаційних технологій для адміністративно-господарського управління активами аеропортів України потребує додаткового дослідження, оскільки ці питання дуже важливі та актуальні, враховуючи стрімке зростання цифровізації суспільства, навколишнього середовища та інфраструктурних об’єктів.
Постановка завдання. Метою цього дослідження є аналіз можливостей та перспектив впровадження сучасних технологій опрацювання та візуалізації геопросторових даних для адміністративно-господарського управління територією аеропорту та розробка концептуальної моделі. Завданням дослідження є аналіз методів отримання геопросторових даних території аеропорту, застосування в аеропортах геоінформаційних систем, технологій штучного інтелекту, віртуальної, доповненої та змішаної реальності, інтернету речей, цифрових двійників, реалізації концепції «розумне місто», тощо.
Виклад основного матеріалу. Геопросторові дані створюються в цифровій формі з використанням сучасних інформаційних та хмарних технологій, які пропонують широкий спектр обладнання, програмного забезпечення, методів і технологій роботи з геопросторовою інформацією. З кожним роком з’являються все нові технології, які знаходять застосування в адміністративно-господарському управлінні аеропортів: хмарні методи отримання даних, геоінформаційні системи, технології штучного інтелекту, віртуальної реальності, інтернету речей, цифрові двійники, «розумні міста» тощо. Вдала інтеграція та використання наявних можливостей щодо збору, зберігання, опрацювання та візуалізації геопросторових даних аеропортів забезпечить їх ефективне управління та економічне зростання. 
Висновки відповідно до статті. За результатами проведеного аналізу можливостей використання в аеропортах технологій віртуальної, доповненої та змішаної реальності, штучного інтелекту, цифрових двійників та концепції «розумних міст», було розроблено концептуальну модель перспектив використання геопросторових даних території аеропорту для вирішення питань адміністративно-господарського управління майновим комплексом",ПЕРСПЕКТИВИ ВИКОРИСТАННЯ ГЕОІНФОРМАЦІЙНИХ ТЕХНОЛОГІЙ В АЕРОПОРТАХ УКРАЇНИ ДЛЯ АДМІНІСТРАТИВНО-ГОСПОДАРСЬКОГО УПРАВЛІННЯ,,Національний університет «Чернігівська політехніка»,,,core
478151049,2021-06-01T00:00:00,"Decision making is a central activity in all clinical professions. Clinical decisions bear wellbeing and economic risks and consequences for patients, families, employers, and national economies. Thus, clinicians should employ sound scientific knowledge to promote optimum decision outcomes. Evidence-based medicine organizes clinical decision-making activities with philosophical, ethical, and methodological foundations to ensure accessibility to the best scientific knowledge to inform clinical decision-making. But high-quality evidence could be lacking or methodically, ethically, or economically unfeasible, compelling clinicians to make risk-bearing decisions without an ideal evidence-base. In uncertain clinical conditions, decisions` outcomes are probabilistic and the clinicians' 'knowledge about outcomes are limited which make the clinical decision in significant need for aid. The advent of electronic health records and the data warehouse technology boosted healthcare industry capacity to generate and store vast amounts of diverse data types. Advanced analytical and computational techniques in artificial intelligence and medical informatics provide unprecedented opportunities to galvanize knowledge deployment in clinical decision, bridging gaps in clinical knowledge. The ability of machine learning approaches to handle the large-scale and diverse data addresses some evidence-based medicine challenges, providing real-time, cost-effective evidence. Nevertheless, despite our beliefs that artificial intelligence and data science may have the potentials to transform the clinical practice, the utilization of artificial intelligence in healthcare is still poor, and the full benefits are not reaped. The adoption of machine learning in healthcare faces several epistemological, methodological, and ethical challenges that make the integration between the machine learning and the evidence-based medicine a hard mission. This dissertation adopted a literature-based reconceptualization to help comprehensively understand the two paradigms, to guide the paradigms reconciliation agenda and to determine the reasons behind the slow adoption of machine learning in healthcare industry. Secondly, we followed an interpretive research design in order to propose a roadmap that aims to enhance the adoption of machine learning in clinical decision support. To validate the theoretical work, we conducted five empirical studies in collaboration with trauma surgery section at Hamad Medical Corporation. In these studies, we developed several ML predictive algorithms to address real-life clinical issues that are faced by the trauma surgery clinicians and predict the prognosis of patients who suffered from Traumatic Brain Injury which include mortality, prolonged mechanical ventilation, ventilator associated pneumonia and prolonged in-hospital length of stay. Subsequently, this dissertation determines how machine learning and evidence-based medicine can be reconciled through proposing a novel pragmatic reconciliation framework that guides the clinicians and the scholars on how to benefit from the synergistic effect of both paradigms. In addition, this dissertation determines the factors that negatively affect the adoption of machine learning in clinical decision support and proposes an original theoretical framework that draws a strategic road map towards the effective adoption of machine learning in healthcare. Furthermore, the dissertation sheds the light on the future research directives that may enhance the compatibility between the data science and the evidence-based medicine paradigms in order to augment the clinicians' capacity to make high-quality informed decisions",HARNESSING MACHINE LEARNING IN CLINICAL DECISION SUPPORT: THEORY AND PRACTICE,,,,,core
475037719,2021-07-22T00:00:00,"Autonomous vehicles (AV) are expected to revolutionize transportation and
improve road safety significantly. However, these benefits do not come without
cost; AVs require large Deep-Learning (DL) models and powerful hardware
platforms to operate reliably in real-time, requiring between several hundred
watts to one kilowatt of power. This power consumption can dramatically reduce
vehicles' driving range and affect emissions. To address this problem, we
propose SAGE: a methodology for selectively offloading the key energy-consuming
modules of DL architectures to the cloud to optimize edge energy usage while
meeting real-time latency constraints. Furthermore, we leverage Head Network
Distillation (HND) to introduce efficient bottlenecks within the DL
architecture in order to minimize the network overhead costs of offloading with
almost no degradation in the model's performance. We evaluate SAGE using an
Nvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge
devices and demonstrate that our offloading strategy is practical for a wide
range of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi
technologies. Compared to edge-only computation, SAGE reduces energy
consumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one
low-resolution camera, one high-resolution camera, and three high-resolution
cameras, respectively. SAGE also reduces upload data size by up to 98.40%
compared to direct camera offloading.Comment: This article appears as part of the ESWEEK-TECS special issue and was
  presented in the International Conference on Hardware/Software Codesign and
  System Synthesis (CODES+ISSS), 202","SAGE: A Split-Architecture Methodology for Efficient End-to-End
  Autonomous Vehicle Control",http://arxiv.org/abs/2107.10895,'Association for Computing Machinery (ACM)',,10.1145/3477006,core
478671740,2021-01-01T00:00:00,"Real-time monitoring of multiphase fluid flows with distributed fibre optic sensing has the potential to play a major role in industrial flow measurement applications. One such application is the optimization of hydrocarbon production to maximize short-term income, and prolong the operational lifetime of production wells and the reservoir. While the measurement technology itself is well understood and developed, a key remaining challenge is the establishment of robust data analysis tools that are capable of providing real-time conversion of enormous data quantities into actionable process indicators. This paper provides a comprehensive technical review of the data analysis techniques for distributed fibre optic technologies, with a particular focus on characterizing fluid flow in pipes. The review encompasses classical methods, such as the speed of sound estimation and Joule-Thomson coefficient, as well as their data-driven machine learning counterparts, such as Convolutional Neural Network (CNN), Support Vector Machine (SVM), and Ensemble Kalman Filter (EnKF) algorithms. The study aims to help end-users establish reliable, robust, and accurate solutions that can be deployed in a timely and effective way, and pave the wave for future developments in the field",A Survey on Distributed Fibre Optic Sensor Data Modelling Techniques and Machine Learning Algorithms for Multiphase Fluid Flow Estimation,,,,,core
365264877,2023-12-09T00:00:00,"La finalità del presente elaborato di tesi è di valutare l’applicabilità del software SCM (Safeaty Content Management), fornito dallo spin-off di ateneo Safeaty srl, in quanto strumento informatizzato utile per la realizzazione, gestione e implementazione dei sistemi di gestione del rischio allergeni predisposti a livello aziendali. Nel presente lavoro è stata condotta un’attenta analisi degli strumenti e delle funzionalità che la piattaforma Safeaty predispone di default, andandone a valutare l’applicabilità nel processo di gestione del rischio allergeni. Dopodiché, sono state individuate le lacune presenti nel sistema e le necessarie implementazioni da apportare. Dall’analisi effettuata è emerso che la piattaforma Safeaty non solo possiede già configurati di default molti degli strumenti utili nel processo di controllo degli allergeni, ma consente anche di introdurre nuove funzionalità in maniera semplice e rapida. L’adozione del software SCM per la gestione dei piani di sicurezza alimentare a livello aziendale ha dimostrato apportare numerosi vantaggi. Innanzitutto, la dematerializzazione della documentazione consente il superamento dell’ancora attuale modalità cartacea con benefici in termini di sostenibilità ambientale. Proprio grazie alla digitalizzazione delle procedure, la piattaforma può essere gestita in remoto da differenti devices. Altro punto di forza è la sua adattabilità, che consente di customizzare gli strumenti e le funzionalità presenti in base alla realtà e alle necessità dell’azienda. L’adozione del software SCM consente una coerente e precisa gestione in real-time dell’autocontrollo, in grado di semplificare e rendere più affidabile tale processo. Di conseguenza, è possibile raggiungere più alti standard di sicurezza alimentare del prodotto e ridurre gli errori da parte degli OSA. Infine, l’archiviazione dei dati tramite Cloud favorisce maggiore trasparenza e rintracciabilità. Altrettanto interessanti sono le prospettive future. Il software SCM, infatti, consente di avvicinarsi a un modello industriale 4.0, è abilitante alla condivisione delle informazioni in blockchain e può potenzialmente essere impiegato per semplificare le attività connesse ai controlli ufficiali.
The aim of this thesis is to assess the applicability of the SCM (Safety Content Management) software, provided by university spin-off Safeaty srl, as a computerized system for the creation, management and implementation of allergens control plans in food industries. In the present work, a close examination of the tools set up by default in the Safety platform was carried out together with an assessment of its applicability in the allergen risk management processes. This evaluation has allowed to identify the gaps in the system and the related improvements to be made. The outcomes of this analysis show that many of the tools necessary in the allergens control process are already set up by default, but also the software allows to introduce new features quickly and easily. The Safeaty system offers many advantages. First, the software makes it possible to overcome all the paper-based checklists, thus ensuring greater environmental sustainability. Thanks to this, the platform allows remote control from different devices. Key strengths of the Safeaty system are its adaptability and customization options as needed and the Cloud data storage, that promotes greater transparency and traceability. In conclusion, the use of SCM software permit a precise and real-time management of the food safety plans, which can simplify and make the process more reliable. As a result, it is possible to achieve higher food safety standards and minimize the amount of errors during the manufacturing process. Also, the Safeaty system offers many interesting prospects for the future. The SCM software is suitable for Industry 4.0 development, it can allow a blockchain-based data sharing and it can potentially be used to facilitate the official controls activities",Informatizzazione dei sistemi di gestione della sicurezza alimentare: la gestione del rischio allergeni.,,'Pisa University Press',,,core
479151683,2021-06-13T00:00:00,"The ongoing deployment of Distributed Energy Resources (DERs), while bringing benefits, introduces significant challenges to the electric utility industry, especially in the distribution grid. These challenges call for closer monitoring through state estimation, where real-time topology recovery is the basis for accurate modeling. With the dramatic increase of the residential photovoltaic (PV) systems (i.e., DER), utilities need to know the locations of these new assets to manage the unconventional two-way power flow for sustainable management of distribution grids. Previous methods to maintain the system connectivity are either based on outdated maps or an ideal assumption of an isolated sub-network for topology recovery, e.g., within one transformer. This requires field engineers to identify the association, which is costly and may contain errors. As it has been shown that, historical records are not always up-to-date. 



To solve these problems, a density-based clustering method is proposed that leverage both voltage domain data from the Advanced Measurement Infrastructure (AMI) and the geographical space information. The goal of such a method is to efficiently segment data sets from a large utility customer pool, after which other topology reconstruction methods can carry over. Specifically, it is shown how to use the voltage data and GIS information to refine the connectivity within one transformer. To give a guarantee, a theoretic bound for the proposed clustering method is shown, providing the ability to explain the performance of the machine learning method. Numerical results on both IEEE test systems and utility networks show the outstanding performance of the new method. An implementation is also demonstrated in the field.

In this dissertation, we consider the rich potential of large utility datasets, in which physical laws are inherently embedded, to identify system information and utilization by using machine learning algorithms. In order to provide situational awareness and tackle practical issues such as limited measurements and un-scalability, we start with proposing a customized data-driven approach to provide an accurate model for distribution grid control and planning",Utilizing AMI Interval Data and Machine Learning Algorithms to IdentifyDistribution System Topology and DER Connectivity,,,,,core
479346073,2021-09-15T05:06:39,"Background: Implementation research has delved into barriers to implementing change and interventions for the implementation of innovation in practice. There remains a gap, however, that fails to connect implementation barriers to the most effective implementation strategies and provide a more tailored approach during implementation. This study aimed to explore barriers for the implementation of professional services in community pharmacies and to predict the effectiveness of facilitation strategies to overcome implementation barriers using machine learning techniques. Methods: Six change facilitators facilitated a 2-year change programme aimed at implementing professional services across community pharmacies in Australia. A mixed methods approach was used where barriers were identified by change facilitators during the implementation study. Change facilitators trialled and recorded tailored facilitation strategies delivered to overcome identified barriers. Barriers were coded according to implementation factors derived from the Consolidated Framework for Implementation Research and the Theoretical Domains Framework. Tailored facilitation strategies were coded into 16 facilitation categories. To predict the effectiveness of these strategies, data mining with random forest was used to provide the highest level of accuracy. A predictive resolution percentage was established for each implementation strategy in relation to the barriers that were resolved by that particular strategy. Results: During the 2-year programme, 1131 barriers and facilitation strategies were recorded by change facilitators. The most frequently identified barriers were a ‘lack of ability to plan for change’, ‘lack of internal supporters for the change’, ‘lack of knowledge and experience’, ‘lack of monitoring and feedback’, ‘lack of individual alignment with the change’, ‘undefined change objectives’, ‘lack of objective feedback’ and ‘lack of time’. The random forest algorithm used was able to provide 96.9% prediction accuracy. The strategy category with the highest predicted resolution rate across the most number of implementation barriers was ‘to empower stakeholders to develop objectives and solve problems’. Conclusions: Results from this study have provided a better understanding of implementation barriers in community pharmacy and how data-driven approaches can be used to predict the effectiveness of facilitation strategies to overcome implementation barriers. Tailored facilitation strategies such as these can increase the rate of real-time implementation of innovations in healthcare, leading to an industry that can confidently and efficiently adapt to continuous change",Data-driven approach for tailoring facilitation strategies to overcome implementation barriers in community pharmacy,https://opus.lib.uts.edu.au/bitstream/10453/150560/2/Data-driven%20approach%20for%20tailoring%20facilitation%20strategies%20to%20overcome%20implementation%20barriers%20in%20community%20pharmacy.pdf,'Springer Science and Business Media LLC',"[{'title': 'Implementation Science', 'identifiers': ['issn:1748-5908', '1748-5908']}]",10.1186/s13012-021-01138-8,core
374287583,2021-01-25T00:00:00,"YesToday’s factories are considered as smart ecosystems with humans, machines and devices interacting with each other for efficient manufacturing of products. Industry 4.0 is a suite of enabler technologies for such smart ecosystems that allow transformation of industrial processes. When implemented, Industry 4.0 technologies have a huge impact on efficiency, productivity and profitability of businesses. The adoption and implementation of Industry 4.0, however, require to overcome a number of practical challenges, in most cases, due to the lack of modernisation and automation in place with traditional manufacturers. This paper presents a first of its kind case study for moving a traditional food manufacturer, still using the machinery more than one hundred years old, a common occurrence for small- and medium-sized businesses, to adopt the Industry 4.0 technologies. The paper reports the challenges we have encountered during the transformation process and in the development stage. The paper also presents a smart production control system that we have developed by utilising AI, machine learning, Internet of things, big data analytics, cyber-physical systems and cloud computing technologies. The system provides novel data collection, information extraction and intelligent monitoring services, enabling improved efficiency and consistency as well as reduced operational cost. The platform has been developed in real-world settings offered by an Innovate UK-funded project and has been integrated into the company’s existing production facilities. In this way, the company has not been required to replace old machinery outright, but rather adapted the existing machinery to an entirely new way of operating. The proposed approach and the lessons outlined can benefit similar food manufacturing industries and other SME industries.Innovate UK—Knowledge Transfer Partnerships (KTP010551",Towards design and implementation of Industry 4.0 for food manufacturing,,'Springer Science and Business Media LLC',,10.1007/s00521-021-05726-z,core
4828828,,"This thesis addresses the task of distributed production scheduling, especially for flexible manufacturing systems (FMS). Current mathematical programming and model-based approaches are considered, but are shown to inadequately manage the dynamic behavior, uncertainty, and large scale typical of such systems. Thus, a new unified framework for modelling and control is proposed that incorporates insights from Operations Research, Control Theory, Artificial Intelligence, and Software Engineering.In this research, production scheduling is addressed in terms of the allocation of resources. The method of ""resource allocation policy"" scheduling is proposed and compared with the ""event-time"" schedules, which result from typical formulations of the production scheduling problem. Next, the technique of real-time simulation is introduced and applied to study the Automated Manufacturing Research Facility (AMRF). The proposed solution methodology for on-line production scheduling centers on a generic controller, which employs real-time simulation to evaluate the trade-offs existing among alternative production policies. The generic controller establishes a standard, parallel-processing architecture in which to perform the decision-making at any level in a hierarchically decomposed scheduling problem. The specification of the functional elements of the generic controller (GC), conceptualized by Davis, Jones, and Saleh (1991), has been further developed in this research work. Likewise, precise definitions have been enumerated for the types of data utilized in the GC, as well as the manner in which they are transmitted between the GC's elements.The essential structure and functional content of the generic controller as well as the proposed resource-based, discrete-event model for production systems have been incorporated into the development of the Flexible Manufacturing Simulation Library (FMSL). The FMSL software provides a set of routines to model hierarchical production systems and perform the tasks required for on-line scheduling. Based on the preliminary studies with the AMRF model and the application of the FMSL for the modelling and simulation of the Motorola Grafted Multilayer process, conclusions are drawn regarding the importance of critical resources. Furthermore, the possibility to apply the proposed approach for true real-time monitoring and control in production systems is likewise considered.U of I OnlyETDs are only available to UIUC Users without author permissio","Real-time, distributed scheduling of production resources",,,,,core
387324085,2021-02-18T00:00:00,"Unsupervised time series clustering is a challenging problem with diverse
industrial applications such as anomaly detection, bio-wearables, etc. These
applications typically involve small, low-power devices on the edge that
collect and process real-time sensory signals. State-of-the-art time-series
clustering methods perform some form of loss minimization that is extremely
computationally intensive from the perspective of edge devices. In this work,
we propose a neuromorphic approach to unsupervised time series clustering based
on Temporal Neural Networks that is capable of ultra low-power, continuous
online learning. We demonstrate its clustering performance on a subset of UCR
Time Series Archive datasets. Our results show that the proposed approach
either outperforms or performs similarly to most of the existing algorithms
while being far more amenable for efficient hardware implementation. Our
hardware assessment analysis shows that in 7 nm CMOS the proposed architecture,
on average, consumes only about 0.005 mm^2 die area and 22 uW power and can
process each signal with about 5 ns latency.Comment: Accepted for publication at ICASSP 202","Unsupervised Clustering of Time Series Signals using Neuromorphic
  Energy-Efficient Temporal Neural Networks",http://arxiv.org/abs/2102.09200,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/ICASSP39728.2021.9414882,core
388365302,2021-03-10T00:00:00,"AI-manipulated videos, commonly known as deepfakes, are an emerging problem.
Recently, researchers in academia and industry have contributed several
(self-created) benchmark deepfake datasets, and deepfake detection algorithms.
However, little effort has gone towards understanding deepfake videos in the
wild, leading to a limited understanding of the real-world applicability of
research contributions in this space. Even if detection schemes are shown to
perform well on existing datasets, it is unclear how well the methods
generalize to real-world deepfakes. To bridge this gap in knowledge, we make
the following contributions: First, we collect and present the largest dataset
of deepfake videos in the wild, containing 1,869 videos from YouTube and
Bilibili, and extract over 4.8M frames of content. Second, we present a
comprehensive analysis of the growth patterns, popularity, creators,
manipulation strategies, and production methods of deepfake content in the
real-world. Third, we systematically evaluate existing defenses using our new
dataset, and observe that they are not ready for deployment in the real-world.
Fourth, we explore the potential for transfer learning schemes and
competition-winning techniques to improve defenses.Comment: Accepted to The Web Conference 2021; First two authors contributed
  equally to this work; 12 pages, 6 table",Deepfake Videos in the Wild: Analysis and Detection,http://arxiv.org/abs/2103.04263,,,,core
199172230,2088-05-07T00:00:00,"The introduction of robots and automation in industrial processes brought many benefits to manufacturing industries, by improving both quality and quantity of production. Automation-assisted production is however mostly open-loop and requires checkpoints to perform product quality analysis. Early vision-based systems were therefore developed since the nineties. This approach worked very well for those activities in which the critical issues can be formally expressed by means of geometrical properties (e.g. measures) or the presence/absence of well-known features on the inspected objects.
However, to date, there are still many quality-control activities that cannot be performed by automated vision inspection machines. Their use is limited by the need to express the checks to be performed as a predefined sequence of actions in which each of them must be carefully designed to fit the specific production requirements. Nevertheless, many of the checks which can be easily learned by humans, are extremely difficult to be formally described using a set of static rules. The major difference relies on the fact that humans can exploit their experience as a relevant evaluation criterion while assessing the quality of a product. On the other hand, implementing such feature on an automated vision inspection machine requires the development of novel algorithms which can be trained and improved with time and experience. Early attempts using traditional Artificial Neural Networks (ANNs) and other learning tools dramatically failed due to the complexity of modeling in rigid mathematical structures the vast set of rules required to model the expertise.
In the last few years, the research on Deep Neural Network (DNN) brought new interest in the field of machine learning based on ANNs. In particular, it has been shown that, given enough data, a DNN can be successfully used to perform complex computer vision tasks such as object classification, object detection and image segmentation. Those networks showed several interesting features, such as experience-based learning, scalability, and performances similar (and in some cases even superior) to human beings. Moreover, with respect to previous ANN approaches, these networks have the advantage that they do not require anymore explicit efforts to describe optimized features extraction algorithms.
This Thesis work has the aim to map the human learning ability from experience into an equivalent learning-from-data capability of a DNN. The effectiveness of this approach has been proved in a real application for the inspection of welding defects on the assembly line of fuel injectors. We designed and assessed an automated defect analysis system based on deep learning. The Thesis covered all the development steps from early concept design, specification design, development of a prototype, software design and finally the development of a complete system which will be integrated into the assembly line. The hardware and software have been designed for all the system components, from the acquisition of the images to the communication of the analysis results to the enterprise PLC. Most of the efforts have been put in the design of the network architecture and on the definition of the training procedure. Starting from state-of-the-art deep architectures and using the transfer learning technique, it has been possible to train a network with about 7 millions parameters using a reduced number of injectors images, obtaining an accuracy of 97.22%.
The realization of this Thesis has been made possible thanks to the collaboration with Continental Automotive Italy S.p.A and their interest in the Industry 4.0 research topics to bring intelligence in the manufacturing processes.
So far, the system described in this work has been successfully tested on the assembly line and will enter into production starting from June 2018. We already provided intelligence in the system so that during the early phases of operation, it will collect new images to extend the existing dataset and to improve further its performance. With this Thesis work, we showed that deep neural networks can successfully perform quality inspection tasks which are actually done by humans. The approach, the system and the methodology described here can be also easily extended and applied to other applications",Automated Defect Analysis of Mechanical Components using Deep Learning-Based Computer Vision,,'Pisa University Press',,,core
427483031,2021-04-30T00:00:00,"The object of research is the process of using information technology in the construction industry. One of the most problematic areas is increasing the efficiency of the construction industry through the introduction of digital technologies. The research carried out is based on the application of an approach that is implemented using artificial intelligence. The study used machine learning and fuzzy logic methods to mark visual data and analyze it for potential threats, as well as to reduce all possible risks. The main feature of this approach is that using machine learning technology, it is possible to reduce the risks of a project before they affect its profit. So, using artificial intelligence in combination with BIM technologies, it is possible to predict work on construction projects based on real-time data, past activities and other factors in such a way as to optimize construction processes. The benefits to be gained from implementing digital processes will become even more evident in future projects as AI continues to analyze company data. This is due to the fact that the proposed approach using fuzzy logic has a number of features, in particular, the more information machine learning algorithms process, the more complex they become. As a result, they provide even more useful information and allow to make even better decisions. This provides an opportunity to minimize risks and efficiently allocate resources when working on projects. Compared to conventional information technology, artificial intelligence can be used to build a knowledge-based security management system and combine statistical probabilities to help mitigate security risks in construction projects.Объектом исследования является процесс использования информационных технологий в строительной отрасли. Одним из самых проблемных мест является повышение эффективности работы строительной отрасли за счет внедрения цифровых технологий. Проведенные исследования базируются на применении подхода, который реализуется с помощью использования искусственного интеллекта. В ходе исследования использовались методы машинного обучения и нечеткой логики, позволяющие отмечать визуальные данные и анализировать их на предмет потенциальных угроз, а также для сокращения всех возможных рисков. Главная особенность данного подхода заключается в том, что с помощью технологии машинного обучения можно сокращать риски проекта до того, как они повлияют на его прибыль. Так, используя искусственный интеллект в сочетании с BIM-технологией, можно на основе данных в режиме реального времени, прошедшей деятельности и других факторов спрогнозировать работу над строительными проектами таким образом, чтобы оптимизировать строительные процессы. Преимущества, которые можно получить в результате внедрения цифровых процессов, будут еще более очевидными при работе над проектами в будущем по мере того, как искусственный интеллект продолжит анализировать данные компаний. Это связано с тем, что предложенный подход с использованием нечеткой логики имеет ряд особенностей, в частности, чем больше информации обрабатывают алгоритмы машинного обучения, тем сложнее они становятся. А в результате они предоставляют еще больше полезной информации и позволяют принимать еще более грамотные ришення. Благодаря этому обеспечивается возможность максимально снизить риски и эффективно распределить ресурсы при работе над проектами. По сравнению с обычными информационными технологиями, искусственный интеллект можно использовать для создания системы управления безопасностью, основанной на имеющихся знаниях, и объединить статистические вероятности для помощи в снижении рисков безопасности строительных проектов.Об'єктом дослідження є процес використання інформаційної технології в будівельній галузі. Одним з найбільш проблемних місць є підвищення ефективності роботи будівельної галузі за рахунок впровадження цифрових технологій. Проведені дослідження базуються на застосуванні підходу, який реалізується за допомогою використання штучного інтелекту. В ході дослідження використовувалися методи машинного навчання та нечіткої логіки, що дозволяють відзначати візуальні дані та аналізувати їх на предмет потенційних загроз, а також для скорочення всіх можливих ризиків. Головна особливість даного підходу полягає в тому, що за допомогою технології машинного навчання можна скорочувати ризики проєкту до того, як вони вплинуть на його прибуток. Так, використовуючи штучний інтелект у поєднанні з BIM-технологіями, можна на основі даних в режимі реального часу, минулої діяльності та інших факторів спрогнозувати роботу над будівельними проєктами таким чином, щоб оптимізувати будівельні процеси. Переваги, які можна отримати в результаті впровадження цифрових процесів, будуть ще більш очевидними при роботі над проєктами в майбутньому в міру того, як штучний інтелект продовжить аналізувати дані компаній. Це пов'язано з тим, що запропонований підхід з використанням нечіткої логіки має ряд особливостей, зокрема чим більше інформації обробляють алгоритми машинного навчання, тим складнішими вони стають. А в результаті вони надають ще більше корисної інформації та дозволяють приймати ще більш грамотні рішення. Завдяки цьому забезпечується можливість максимально знизити ризики та ефективно розподілити ресурси при роботі над проєктами. У порівнянні зі звичайними інформаційними технологіями, штучний інтелект можна використовувати для створення системи управління безпекою, що базується на наявних знаннях, і об'єднати статистичні ймовірності для допомоги в зниженні ризиків безпеки будівельних проєктів",Впровадження штучного інтелекту в будівельну галузь та аналіз існуючих технологій,,'Private Company Technology Center',,10.15587/2706-5448.2021.229532,core
478595370,2021-10-01T00:00:00,"One source of learning in universities is a digital library. In the era of industry 4.0, most universities have implemented digital libraries in supporting the learning process. However, the reality shows that digital library management is still ineffective. Therefore, the implementation of digital libraries needs to be evaluated for determining the digital library effectiveness used as learning resources in supporting the learning process in universities. Many evaluation tools are used to evaluate the effectiveness of digital libraries but have not provided accurate recommendation results to support decision-making. This research presents an innovation in the form of an evaluation tool that can be used to evaluate the digital library effectiveness in universities. That evaluation tool is called the Alkin-WP-based digital library evaluation software. This software is a desktop platform that contains aspects of measuring the digital library effectiveness by referring to the components of the Alkin evaluation model and the WP (Weighted Product) method. This research aimed to show the effectiveness level of the utilization of Alkin-WP-based digital library evaluation software. This research method was R & D (Research & Development) which refers to the ten development stages of the Borg and Gall model. In this research, development was focused only on a few stages, included: usage trials, final product revision, dissemination, and implementation. The subjects involved in assessing the implementation/utilization of the Alkin-WP-based digital library evaluation software were 35 people, in the usage trials were six people, in product revision were three people, and at the stage of dissemination were 15 people. The tools used to collect data were questionnaires and interview guidelines. The data analysis technique used was descriptive quantitative. The effectiveness level of utilizing the Alkin-WP-based digital library evaluation software was 88.34%. It showed that the evaluation software had effective. The impact of this research results on the scientific field of educational evaluation is being able to show the existence of a new evaluation tool based on educational evaluation and artificial intelligence. That evaluation tool can easier for library heads to make policies for revamping digital library services based on accurate recommendations. Doi: 10.28991/esj-2021-01308 Full Text: PD",Utilization of Alkin-WP-Based Digital Library Evaluation Software as Evaluation Tool of Digital Library Effectiveness,https://core.ac.uk/download/478595370.pdf,'Ital Publication',,10.28991/esj-2021-01308,core
327254342,2021-01-13T00:00:00,"E-commerce business is revolutionizing our shopping experiences by providing
convenient and straightforward services. One of the most fundamental problems
is how to balance the demand and supply in market segments to build an
efficient platform. While conventional machine learning models have achieved
great success on data-sufficient segments, it may fail in a large-portion of
segments in E-commerce platforms, where there are not sufficient records to
learn well-trained models. In this paper, we tackle this problem in the context
of market segment demand prediction. The goal is to facilitate the learning
process in the target segments by leveraging the learned knowledge from
data-sufficient source segments. Specifically, we propose a novel algorithm,
RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a
meta-learning paradigm. The multi-pattern fusion network considers both local
and seasonal temporal patterns for segment demand prediction. In the
meta-learning paradigm, transferable knowledge is regarded as the model
parameter initialization of MPFN, which are learned from diverse source
segments. Furthermore, we capture the segment relations by combining
data-driven segment representation and segment knowledge graph representation
and tailor the segment-specific relations to customize transferable model
parameter initialization. Thus, even with limited data, the target segment can
quickly find the most relevant transferred knowledge and adapt to the optimal
parameters. We conduct extensive experiments on two large-scale industrial
datasets. The results justify that our RMLDP outperforms a set of
state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a
real-world E-commerce platform. The online A/B testing results further
demonstrate the practicality of RMLDP.Comment: First two authors contributed equally; Accepted by WSDM 202","Relation-aware Meta-learning for Market Segment Demand Prediction with
  Limited Records",http://arxiv.org/abs/2008.00181,,,,core
443971503,2021-06-30T00:00:00,"Robot programming typically makes use of a set of mechanical skills that is
acquired by machine learning. Because there is in general no guarantee that
machine learning produces robot programs that are free of surprising behavior,
the safe execution of a robot program must utilize monitoring modules that take
sensor data as inputs in real time to ensure the correctness of the skill
execution. Owing to the fact that sensors and monitoring algorithms are usually
subject to physical restrictions and that effective robot programming is
sensitive to the selection of skill parameters, these considerations may lead
to different sensor input qualities such as the view coverage of a vision
system that determines whether a skill can be successfully deployed in
performing a task. Choosing improper skill parameters may cause the monitoring
modules to delay or miss the detection of important events such as a mechanical
failure. These failures may reduce the throughput in robotic manufacturing and
could even cause a destructive system crash. To address above issues, we
propose a sensing quality-aware robot programming system that automatically
computes the sensing qualities as a function of the robot's environment and
uses the information to guide non-expert users to select proper skill
parameters in the programming phase. We demonstrate our system framework on a
6DOF robot arm for an object pick-up task.Comment: 7 pages, 9 figures, 1 table; accepted for presentation in IEEE ICRA
  2021(IEEE International Conference on Robotics and Automation","SQRP: Sensing Quality-aware Robot Programming System for Non-expert
  Programmers",http://arxiv.org/abs/2107.00127,,,,core
4827933,,"Planning is the task of finding a set of operators whose executive transforms the current world state into a world state which satisfies some goal criterion. Because many tasks involve focussed change of a world state, planning techniques are relevant to a wide variety of important AI tasks such as automatic programming, process design and control, and manufacturing engineering.However, when planning in complex, real-world domains, large amounts of knowledge are needed to adequately describe world behavior. With a large domain theory, complete reasoning can become a computationally intractable task. Consequently, even if a system has a complete and correct domain theory the computational demands of exhaustive reasoning may prevent successful planning.This thesis describes incremental reasoning and learning techniques to reduce the cost of planning in computationally intractable domains. In this approach plans are initially constructed using inference limiting simplifications. Because limiting inference implies not exhaustively checking all possible inferences, resulting plans may make incorrect predictions. In order to deal with this difficulty the system uses these incorrect goal predictions to direct a refinement process which expands the limited inference of the initial plan, thus preventing recurrence of the incorrect goal prediction. By using executive feedback to focus attention upon parts of the plan requiring further inference, the system avoids the computationally intractable blind search of potentially relevant inferences required by exhaustive reasoning. The class of limited inference simplifications and refinement techniques described in this thesis have been shown to have the properties of convergence upon soundness (i.e. a plan will eventually be refined to make the same goal predictions as a plan developed using exhaustive reasoning) and completeness (i.e. the simplifications will not cause the planner to overlook any potential solutions considered by an exhaustive planner).This incremental reasoning approach has been validated in two ways. First, a complexity analysis of the computational savings of the incremental reasoning approach has been performed. Second, this approach has been fully implemented and this implementation has been used to empirically compare the cost of the incremental reasoning approach and the exhaustive reasoning approach in two domains.U of I OnlyETDs are only available to UIUC Users without author permissio",An explanation-based learning approach to incremental planning,,,,,core
477954974,2021-01-01T00:00:00,"International audienceRecent development in smart devices has lead us to an explosion in data generation and heterogeneity, which requires new network solutions for better analysing and understanding traffic. These solutions should be intelligent and scalable in order to handle the huge amount of data automatically. With the progress of high-performance computing (HPC), it becomes feasible easily to deploy machine learning (ML) to solve complex problems and its efficiency has been validated in several domains (e.g., healthcare or computer vision). At the same time, network slicing (NS) has drawn significant attention from both industry and academia as it is essential to address the diversity of service requirements. Therefore, the adoption of ML within NS management is an interesting issue. In this paper, we have focused on analyzing network data with the objective of defining network slices according to traffic flow behaviors. For dimensionality reduction, the feature selection has been applied to select the most relevant features (15 out of 87 features) from a real dataset of more than 3 million instances. Then, a K-Means clustering is applied to better understand and distinguish behaviors of traffic. The results demonstrated a good correlation among instances in the same cluster generated by the unsupervised learning. This solution can be further integrated in a real environment using network function virtualization",Network Traffic Analysis using Machine Learning: an unsupervised approach to understand and slice your network,,'Springer Fachmedien Wiesbaden GmbH',,,core
395656985,2021-01-01T00:00:00,"Proceedings of the 10th edition of the International Workshop “Service Orientation in Holonic and Multi-agent Manufacturing – SOHOMA 2020”, Paris, France, October 1-2, 2020, Studies in Computational Intelligence, Vol. 852, Springer International Publishing 2021International audienceThe scientific theme of the book concerns “Manufacturing as a Service (MaaS)” which is developed in a layered cloud networked manufacturing perspective, from the shop floor resource sharing model to the virtual enterprise collaborative model, by distributing the cost of the manufacturing infrastructure - equipment, software, maintenance, networking - across all customers.MaaS is approached in terms of new models of service-oriented, knowledge-based manufacturing systems optimized and reality-aware, that deliver value to customer and manufacturer via Big data analytics, Internet of Things communications, Machine learning and Digital twins embedded in Cyber-Physical System frameworks. From product design to after-sales services, MaaS relies on the servitization of manufacturing operations such as: Design as a Service, Predict as a Service or Maintain as a service.The general scope of the book is to foster innovation in smart and sustainable manufacturing and logistics systems and in this context to promote concepts, methods and solutions for the digital transformation of manufacturing through service orientation in holonic and agent-based control with distributed intelligence.The book’s readership is comprised by researchers and engineers working in the manufacturing value chain area who develop and use digital control solutions in the ‘Industry of the Future’ vision. The book also addresses to master and Ph.D. students enrolled in Engineering Sciences programs","Service Oriented, Holonic and Multi-Agent Manufacturing Systems for Industry of the Future: Proceedings of SOHOMA 2020",,'Springer Science and Business Media LLC',,10.1007/978-3-030-69373-2,core
387313014,2021-01-26T00:00:00,"In this paper, we demonstrate how deep autoencoders can be generalized to the
case of inpainting and denoising, even when no clean training data is
available. In particular, we show how neural networks can be trained to perform
all of these tasks simultaneously. While, deep autoencoders implemented by way
of neural networks have demonstrated potential for denoising and anomaly
detection, standard autoencoders have the drawback that they require access to
clean data for training. However, recent work in Robust Deep Autoencoders
(RDAEs) shows how autoencoders can be trained to eliminate outliers and noise
in a dataset without access to any clean training data. Inspired by this work,
we extend RDAEs to the case where data are not only noisy and have outliers,
but also only partially observed. Moreover, the dataset we train the neural
network on has the properties that all entries have noise, some entries are
corrupted by large mistakes, and many entries are not even known. Given such an
algorithm, many standard tasks, such as denoising, image inpainting, and
unobserved entry imputation can all be accomplished simultaneously within the
same framework. Herein we demonstrate these techniques on standard machine
learning tasks, such as image inpainting and denoising for the MNIST and
CIFAR10 datasets. However, these approaches are not only applicable to image
processing problems, but also have wide ranging impacts on datasets arising
from real-world problems, such as manufacturing and network processing, where
noisy, partially observed data naturally arise",Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders,http://arxiv.org/abs/2101.10876,,,,core
426963251,2021-04-21T10:05:33,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search",Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,,'Institute of Electrical and Electronics Engineers (IEEE)',,10.23919/DATE.2019.8714959,core
395002393,2021-02-22T00:00:00,"Presented online February 22, 2021, 10:00 a.m.-10:30 a.m.National Symposium on Predicting Emergence of Virulent Entities by Novel Technologies (PREVENT) : What Advances In Science, Technology, And Human Behavior Will Enable Prediction And Prevention Of Future Pandemics?The Honorable Sethuraman Panchanathan is a computer scientist and engineer and the 15th director of the U.S. National Science Foundation (NSF). Panchanathan was nominated to this position by the President of the United States in 2019 and subsequently unanimously confirmed by the U.S. Senate on June 18, 2020. NSF is an $8.5B independent federal agency and the only government agency charged with advancing all fields of scientific discovery, technological innovation and STEM education. Panchanathan is a leader in science, engineering and education with more than three decades of experience. He has a distinguished career in both higher education and government, where he has designed and built knowledge enterprises, which advance research innovation, strategic partnerships, entrepreneurship, global development and economic growth. Panchanathan previously served as the executive vice president of the Arizona State University (ASU) Knowledge Enterprise, where he was also chief research and innovation officer. He was also the founder and director of the Center for Cognitive Ubiquitous Computing at ASU. Under his leadership, ASU increased research performance fivefold, earning recognition as the fastest growing and most innovative research university in the U.S. Prior to joining NSF, Panchanathan served on the National Science Board as chair of the Committee on Strategy and as a member of the External Engagement and National Science and Engineering Policy committees. Additionally, he served on the National Advisory Council on Innovation and Entrepreneurship. He was chair of the Council on Research of the Association of Public and Land-grant Universities and co-chair of the Extreme Innovation Taskforce of the Global Federation of Competitiveness Councils. Arizona's Governor appointed Panchanathan as senior advisor for science and technology in 2018. He was the editor-in-chief of the IEEE Multimedia Magazine and editor/associate editor of several international journals. Panchanathan's scientific contributions have advanced the areas of human-centered multimedia computing, haptic user interfaces, person-centered tools and ubiquitous computing technologies for enhancing the quality of life for individuals with different abilities; machine learning for multimedia applications; medical image processing; and media processor designs. He has published close to 500 articles in refereed journals and conference proceedings, and has mentored more than 150 graduate students, postdocs, research engineers and research scientists, many now occupy leading positions in academia and industry. For his scientific contributions, Panchanathan has received numerous awards, such as Distinguished Alumnus Awards and the Governor's Innovator of the Year for Academia Award for his development of information technology centric assistive and rehabilitative environments to assist individuals with visual impairments. Panchanathan is a fellow of the National Academy of Inventors, where he also served as vice president for strategic initiatives. He is also a fellow of the American Association for the Advancement of Science, the Canadian Academy of Engineering, the Association for Computing Machinery, the Institute of Electrical and Electronics Engineers and the Society of Optical Engineering. Panchanathan is married to Sarada ""Soumya"" Panchanathan, an academic pediatrician and informatician, who has taught medical students, pediatric residents and informatics fellows. They have two adult children, Amritha and Roshan.Dr. Mitra Basu joined NSF’s Computer Information Science and Engineering (CISE) Directorate in 2008 as the Lead Program Director for the Expeditions in Computing program – the Center scale program in CISE. During her tenure at NSF, she has served as acting Deputy Division Director in CISE. Mitra is a co-Lead for NSF PIPP Working Group.Dr. Paul M. Torrens is a Professor in Tandon's Department of Computer Science and Engineering and the Center for Urban Science + Progress at New York University. His work centers on the development and application of modeling and simulation tools for exploring and explaining complex urban systems, intricacies of behavior at the interface between cities and people, and emerging cyberinfrastructure for urban spaces and places. Torrens was the recipient of a Faculty Early Career Development Award from the National Science Foundation in 2007, and in 2008 George W. Bush presented him with the Presidential Early Career Award for Scientists and Engineers.Dr. Krista Rule Wigginton is an associate professor of Civil and Environmental Engineering at the University of Michigan. Prior to joining the faculty at UM, she was an assistant professor at the University of Maryland, College Park from 2011-2012. Her research focuses on applications of environmental biotechnology in drinking water and wastewater treatment. In particular, her research group develops new methods to detect and analyze the fate of emerging pollutants in the environment. Dr. Wigginton received her B.S. degree in Chemistry from the University of Idaho, and her M.S. and Ph.D. degrees in Environmental Engineering from Virginia Tech. After completing her Ph.D. degree, she worked as a postdoctoral researcher at École Polytechnique Fédérale de Lausanne (EPFL) in Switzerland from 2008-2011. Dr. Wigginton is a Co-Principal Investigator for the NSF grant award: Advancing Technologies and Improving Communication of Urine-Derived Fertilizers for Food Production within a Risk-Based Framework.Dr. John Yin is Professor of Chemical & Biological Engineering at the University of Wisconsin-Madison. He trained at Columbia University, earning dual bachelor’s degrees in the liberal arts and chemical engineering, while pursing cello studies at Juilliard School and piano studies at Columbia. Yin earned his PhD in chemical engineering at UC-Berkeley and pursued post-doctoral research as an Alexander von Humboldt Fellow working under Nobel laureate Manfred Eigen at the Max-Planck Institute for Biophysical Chemistry in Göttingen, Germany.  He started his academic career as an assistant professor at the Thayer School of Engineering, Dartmouth College, where he was awarded an NSF Young Investigator Award and Presidential Early Career Award in Science and Engineering (PECASE). He moved to UW-Madison as associate professor with tenure in 1998 and was promoted to full professor in 2004.  In his research Yin develops new experimental measures and computational models that aim to elucidate how viruses grow and how their infections spread.  Yin has co-authored more than 70 publications in areas of computational and experimental molecular and cell biology, with an emphasis on virus-host interactions.  He has further served on the NIH study section for Modeling and Analysis of Biological Systems (MABS).  In 2009 Yin was selected to lead the Systems Biology Theme of the Wisconsin Institutes for Discovery, a joint public-private venture to promote cross-disciplinary research, education and outreach at the UW-Madison, and he was recently recognized with a Vilas Distinguished Achievement Professorship for 2015-2020. Yin plays a 1918 Andre Bernard cello, he earned a 1725 rating at the US Table Tennis Association Open in Las Vegas, and he is an enthusiastic convert to sous vide quantitative cooking.Runtime: 39:09 minutesDr. B. Aditya Prakash is an Associate Professor in the College of Computing at the Georgia Institute of Technology (“Georgia Tech”). He received a Ph.D. from the Computer Science Department at Carnegie Mellon University in 2012, and a B.Tech (in CS) from the Indian Institute of Technology (IIT) -- Bombay in 2007. He has published one book, more than 80 papers in major venues, holds two U.S. patents and has given several tutorials at leading conferences. His work has also received multiple best-paper/best-of-conference selections and travel awards. His research interests include Data Science, Machine Learning and AI, with emphasis on big-data problems in large real-world networks and time-series, with applications to epidemiology, health, urban computing, security and the Web. His work has been supported by the National Science Foundation (NSF), the Centers for Disease Control (CDC), the Department of Energy (DoE), the National Security Agency (NSA), the National Endowment for Humanities (NEH) and various companies. Tools developed by his group have been in use in many places including ORNL, the CDC, Walmart and Facebook. He received a Facebook Faculty Award in 2015, was named as one of ‘AI Ten to Watch’ 2017 by IEEE, and received the NSF CAREER award in 2018. His work has also been highlighted by many media outlets and popular press. He was previously on the faculty of Computer Science at Virginia Tech. He is a member of the infectious diseases modeling MIDAS network and core-faculty at the Center for Machine Learning (ML@GT) and the Institute for Data Engineering and Science (IDEaS) at Georgia Tech. Aditya’s Twitter handle is @badityap.In the last year, the ongoing COVID-19 pandemic has severely disrupted the livelihoods of our planet’s human inhabitants, infecting over 85 million individuals, and causing nearly 2 million deaths. What actions should have been taken to minimize the severity of this pandemic (and others before it in the past decades such as Zika, SARS and Ebola)? In retrospect, many actions could have played key roles: environmental monitoring for potential animal-to-human infection spillovers, establishment of pipelines for rapid vaccine development and optimal deployment and distribution, designing data science tools to accurately forecast trajectories, fast and adaptive syndromic surveillance and behavior tracking, designing and timing effective interventions, training susceptible individuals for measures needed to inhibit the spread of infectious agents, and others. What lessons have been learned and what gaps in our knowledge, methodologies, technologies, and policies remain?National Science Foundation (U.S.","PRedicting Emergence Of Virulent Entities By Novel Technologies (PREVENT) Symposium - Opening Remarks, Welcome Statement, and Technical Background",,Georgia Institute of Technology,,,core
429831722,2021-06-09T00:00:00,"Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.Comment: Understanding the underpinnings of the TinyML edX course series:
  https://www.edx.org/professional-certificate/harvardx-tiny-machine-learnin",Widening Access to Applied Machine Learning with TinyML,http://arxiv.org/abs/2106.04008,,,,core
385962907,2021-01-29T00:00:00,"Mass-customization is related to optimizing the balance between flexibility, strongly required by the customer-focused industries and manufacturing efficiency, which is critical for market competitiveness. In the conventional industries, the process of designing, validating and manufacturing a product is long and expensive. Some of the common approaches for addressing those issues are parametric product modeling and Finite Element Analysis (FEA). However, the costs involved are still relatively high because of the very special expertise needed and the cost of the specialized software. Also, the specific design of the product cannot be validated in a real-time, which often leads to making hard compromises between the specific customer requirements and the structural properties of the product in its exploitation. In this paper, we propose the novel methodology for real-time structural analysis assistance for custom product design. We introduce the concept of so-called compiled FEA model, a Machine Learning (ML) model, consisting of dataset of characteristic product parameters and associated physical quantities and properties, selected ML algorithms and the sets of associated hyperparameters. A case study of creating a compiled FEA model for the case of internal orthopedic fixator is provided",NOVEL METHODOLOGY FOR REAL-TIME STRUCTURAL ANALYSIS ASSISTANCE IN CUSTOM PRODUCT DESIGN,https://core.ac.uk/download/385962907.pdf,'University of Nis - Faculty of Philosophy',,,core
477729056,2021-09-06T00:00:00,"International audienceWith the Industry 4.0 revolution currently underway, manufacturing companies are massively adopting new technologies to achieve the virtualization of their shop floor and the collaboration of their information systems. This process often leads to the construction of a real-time, collaborative, and intelligent virtual factory of their physical factory (so-called digital twin). The application of digital twins and frontier technologies in production planning still faces many challenges. But the research is still limited about how these frontier technologies can be applied to enhance production planning. This paper introduces how to enhance material resource planning (MRP) with digital twins and other frontier technologies, and presents a framework for the integration of MRP software with digital twin technologies. Indeed, the data collected from the shop floor can improve the accuracy of the optimization models used in the MRP software. First, several MRP parameters are unknown when planning, and some of these parameters may be accurately forecasted from the data with machine learning. Nevertheless, the forecast will never be perfect, and the variability of some parameters may have a critical impact on the resulting plan. Therefore, the optimization approach must properly account for these uncertainties, and some methods must allow building probability distribution from the data. Second, as the optimization models in MRP are based on aggregated data, the resulting plans are usually not implementable in practice. The capacity constraints may be acquired by communication with an accurate simulation of the execution of the plan on the shop floor",A Digital Twin-Driven Methodology for Material Resource Planning Under Uncertainties,,'Springer Science and Business Media LLC',,10.1007/978-3-030-85874-2_34,core
392381428,2021-03-15T07:00:00,"As machine learning becomes ubiquitous, the need to deploy models on real-time, embedded systems will become increasingly critical. This is especially true for deep learning solutions, whose large models pose interesting challenges for target architectures at the “edge” that are resource-constrained. The realization of machine learning, and deep learning, is being driven by the availability of specialized hardware, such as system-on-chip solutions, which provide some alleviation of constraints. Equally important, however, are the operating systems that run on this hardware, and specifically the ability to leverage commercial real-time operating systems which, unlike general purpose operating systems such as Linux, can provide the low-latency, deterministic execution required for embedded, and potentially safety-critical, applications at the edge. Despite this, studies considering the integration of real-time operating systems, specialized hardware, and machine learning/deep learning algorithms remain limited. In particular, better mechanisms for real-time scheduling in the context of machine learning applications will prove to be critical as these technologies move to the edge. In order to address some of these challenges, we present a resource management framework designed to provide a dynamic on-device approach to the allocation and scheduling of limited resources in a real-time processing environment. These types of mechanisms are necessary to support the deterministic behavior required by the control components contained in the edge nodes. To validate the effectiveness of our approach, we applied rigorous schedulability analysis to a large set of randomly generated simulated task sets and then verified the most time critical applications, such as the control tasks which maintained low-latency deterministic behavior even during off-nominal conditions. The practicality of our scheduling framework was demonstrated by integrating it into a commercial real-time operating system (VxWorks) then running a typical deep learning image processing application to perform simple object detection. The results indicate that our proposed resource management framework can be leveraged to facilitate integration of machine learning algorithms with real-time operating systems and embedded platforms, including widely-used, industry-standard real-time operating systems",On-Device Deep Learning Inference for System-on-Chip (SoC) Architectures,https://core.ac.uk/download/392381428.pdf,Chapman University Digital Commons,,,core
372716697,2021-01-05T00:00:00,"Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efficient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily influenced by changing demand pattern over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results",Switching Scheme: A Novel Approach for Handling Incremental Concept Drift in Real-World Data Sets,,'HICSS Conference Office',,10.24251/HICSS.2021.120,core
388359974,2021-03-05T00:00:00,"After more than a decade of intense focus on automated vehicles, we are still
facing huge challenges for the vision of fully autonomous driving to become a
reality. The same ""disillusionment"" is true in many other domains, in which
autonomous Cyber-Physical Systems (CPS) could considerably help to overcome
societal challenges and be highly beneficial to society and individuals. Taking
the automotive domain, i.e. highly automated vehicles (HAV), as an example,
this paper sets out to summarize the major challenges that are still to
overcome for achieving safe, secure, reliable and trustworthy highly automated
resp. autonomous CPS. We constrain ourselves to technical challenges,
acknowledging the importance of (legal) regulations, certification,
standardization, ethics, and societal acceptance, to name but a few, without
delving deeper into them as this is beyond the scope of this paper. Four
challenges have been identified as being the main obstacles to realizing HAV:
Realization of continuous, post-deployment systems improvement, handling of
uncertainties and incomplete information, verification of HAV with machine
learning components, and prediction. Each of these challenges is described in
detail, including sub-challenges and, where appropriate, possible approaches to
overcome them. By working together in a common effort between industry and
academy and focusing on these challenges, the authors hope to contribute to
overcome the ""disillusionment"" for realizing HAV.Comment: 13 pages, 2 figure",Challenges of engineering safe and secure highly automated vehicles,http://arxiv.org/abs/2103.03544,,,,core
387288572,2021-06-15T00:00:00,"Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.Comment: SIGKDD 202","Exploration in Online Advertising Systems with Deep Uncertainty-Aware
  Learning",http://arxiv.org/abs/2012.02298,'Association for Computing Machinery (ACM)',,10.1145/3447548.3467089,core
475540576,2021-08-01T00:00:00,"International audienceDuring the last decade, Deep Neural Networks (DNN) have progressively been integrated on all types of platforms, from data centers to embedded systems including low-power processors and, recently, FPGAs. Neural Networks (NN) are expected to become ubiquitous in IoT systems by transforming all sorts of real-world applications, including applications in the safety-critical and security-sensitive domains. However, the underlying hardware security vulnerabilities of embedded NN implementations remain unaddressed. In particular, embedded DNN implementations are vulnerable to Side-Channel Analysis (SCA) attacks, which are especially important in the IoT and edge computing contexts where an attacker can usually gain physical access to the targeted device. A research field has therefore emerged and is rapidly growing in terms of the use of SCA including timing, electromagnetic attacks and power attacks to target NN embedded implementations. Since 2018, research papers have shown that SCA enables an attacker to recover inference models architectures and parameters, to expose industrial IP and endangers data confidentiality and privacy. Without a complete review of this emerging field in the literature so far, this paper surveys state-of-the-art physical SCA attacks relative to the implementation of embedded DNNs on micro-controllers and FPGAs in order to provide a thorough analysis on the current landscape. It provides a taxonomy and a detailed classification of current attacks. It first discusses mitigation techniques and then provides insights for future research leads",Physical Side-Channel Attacks on Embedded Neural Networks: A Survey,,'MDPI AG',,10.3390/app11156790,core
387325315,2021-02-20T00:00:00,"Over the last years, the number of cyber-attacks on industrial control
systems has been steadily increasing. Among several factors, proper software
development plays a vital role in keeping these systems secure. To achieve
secure software, developers need to be aware of secure coding guidelines and
secure coding best practices. This work presents a platform geared towards
software developers in the industry that aims to increase awareness of secure
software development. The authors also introduce an interactive game component,
a virtual coach, which implements a simple artificial intelligence engine based
on the laddering technique for interviews. Through a survey, a preliminary
evaluation of the implemented artifact with real-world players (from academia
and industry) shows a positive acceptance of the developed platform.
Furthermore, the players agree that the platform is adequate for training their
secure coding skills. The impact of our work is to introduce a new automatic
challenge evaluation method together with a virtual coach to improve existing
cybersecurity awareness training programs. These training workshops can be
easily held remotely or off-line.Comment: Preprint accepted for publication at the 6th Workshop On The Security
  Of Industrial Control Systems & Of Cyber-Physical Systems (CyberICPS 2020","Cybersecurity Awareness Platform with Virtual Coach and Automated
  Challenge Assessment",http://arxiv.org/abs/2102.10430,,,,core
387586080,2021-01-01T08:00:00,"This paper serves as a literature review of artificial intelligence (AI) in the hiring process to examine the extent to which AI tools reliably, accurately, and fairly select talent. Today’s organizations are under immense pressure to develop innovative hiring strategies to identify and secure talent for their workforce. As technological advances arise, companies are increasingly turning to AI to aid in their hiring efforts. Hirers are enticed by AI because there is increased systematicity in determining the most qualified workers with AI as opposed to current selection methods that rely on intuitively combining and weighting applicant information. In an experimental study included in this paper, the researchers showed that increasing systematicity in decision-making processes leads to higher quality decisions. I then delve into specific use cases of AI such as processing applicants’ narrative responses and scraping applicants’ social media accounts. While it is possible for AI to execute these endeavors, researchers investigated whether AI could do so consistently and accurately. In one study, researchers demonstrated that the computer program they had trained exhibited a level of reliability comparable to that of a human rater and that the computer scores also exhibited construct validity. While establishing reliability and validity is crucial, it is also important to consider applicants’ perceptions towards AI. In one study, applicants generally held negative perceptions toward AI in the hiring process, but interestingly, their distrust was reduced when they possessed more knowledge and experience with AI. Lastly, this paper assesses whether AI is indeed as objective as it seems. Due to the nature of how AI programs are trained, it is likely that human biases will emerge in the seemingly objective process. Multiple real-world examples revealed that AI has led to biased and discriminatory outputs, which is a major concern since the implementation of AI in the hiring process would directly impact workforce diversity and individuals’ well-being at a greater scale than its human counterparts. It is possible that if the implementation of AI in the hiring process goes unchecked, socio-economic divides will be exacerbated. I therefore suggest that Industrial/Organizational psychologists be included in the development of AI hiring tools and that the companies developing and using AI tools for hiring purposes should be held accountable, specifically through publicly accessible algorithmic reviews",AI in Personnel Selection: Computing Talent or Augmenting Bias?,,Scholarship @ Claremont,,,core
289160157,2020-01-01T00:00:00,"Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the exponential Monte Carlo with counter (EMCQ). Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not (historically) be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to show the degree of interaction required to cover the code as well as finding faults. We applied Q-EMCQ on 37 real-world industrial programs written in Function Block Diagram (FBD) language, which is used for developing a train control management system at Bombardier Transportation Sweden AB. The results show that Q-EMCQ is an efficient technique for test case generation. Addition- ally, unlike the t-wise test suite generation, which deals with the minimization problem, we have also subjected Q-EMCQ to a maximization problem involving the general module clustering to demonstrate the effectiveness of our approach. The results show the Q-EMCQ is also capable of outperforming the original EMCQ as well as several recent meta/hyper-heuristic including modified choice function, Tabu high-level hyper-heuristic, teaching learning-based optimization, sine cosine algorithm, and symbiotic optimization search in clustering quality within comparable execution time",'Springer Science and Business Media LLC',An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications,10.1007/s00500-020-04769-z,,,core
322567711,2020-05-25T00:00:00,"The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.Comment: 6 pages, 7 figure, 1 tabl",,"Using Computer Vision to enhance Safety of Workforce in Manufacturing in
  a Post COVID World",,http://arxiv.org/abs/2005.05287,,core
323323388,2020-08-13T00:00:00,"The use of Reinforcement Learning (RL) is still restricted to simulation or
to enhance human-operated systems through recommendations. Real-world
environments (e.g. industrial robots or power grids) are generally designed
with safety constraints in mind implemented in the shape of valid actions masks
or contingency controllers. For example, the range of motion and the angles of
the motors of a robot can be limited to physical boundaries. Violating
constraints thus results in rejected actions or entering in a safe mode driven
by an external controller, making RL agents incapable of learning from their
mistakes. In this paper, we propose a simple modification of a state-of-the-art
deep RL algorithm (DQN), enabling learning from forbidden actions. To do so,
the standard Q-learning update is enhanced with an extra safety loss inspired
by structured classification. We empirically show that it reduces the number of
hit constraints during the learning phase and accelerates convergence to
near-optimal policies compared to using standard DQN. Experiments are done on a
Visual Grid World Environment and Text-World domain.Comment: Accepted at Internationnal Joint Conference on Neural Networks
  (IJCNN'2020",,"I'm sorry Dave, I'm afraid I can't do that, Deep Q-learning from
  forbidden action",,http://arxiv.org/abs/1910.02078,,core
356674913,2020-10-13T00:00:00,"In pharmaceutical manufacturing, the idea of non-destructive and real-time testing of raw materials and drugs is receiving increasing attention due to the implementation of continuous manufacturing. There is ample evidence in the literature that the particle size of an active ingredient significantly affects its dissolution rate. However, first-principles-based models have been used so far, instead of including robust, data-rich empirical models. Although first-principles models can provide adequate prediction for simple problems, the empirical approach could have several benefits when complex formulations are studied. In my work, I developed an empirical model that aimed to predict the dissolution of acetylsalicylic acid from the particle size distribution (PSD). For this purpose, the PSD of different acetylsalicylic acid powder fractions and mixtures was measured, and the dissolution curve of the powder mixture and capsules was studied. I compared and optimized different methods (Artificial Neural Networks, PLS regression) depending on the bin number and type of the PSD (i.e. number- and volume-based). The results could contribute to establish a good modeling approach when the effect of PSD on the dissolution of more complex formulations (e.g. tablets) needs to be modeled.
Kivonat
A gyógyszergyártásban egyre nagyobb figyelmet kap az alapanyag és a készítmények roncsolásmentes és valós-idejű vizsgálata a folyamatos technológiák bevezetése miatt. A szakirodalomban számtalan bizonyítékot találni arra, hogy a hatóanyag szemcsemérete jelentősen befolyásolja a tabletták kioldódását. Azonban, ennek modellezésére leginkább fizikai összefüggéseket alkalmaztak eddig, robusztus, sok adattal dolgozó empirikus modelleket nem. Bár az előbbiek egyszerű összefüggések modellezésére alkalmasak, az utóbbi komplex formulák tanulmányozása során jelentős előnyokkel szolgálhat. Munkám során egy empirikus modellt optimalizáltam, amely acetil-szalicilsav kioldódását célzott prediktálni a szemcseméret-eloszlásból. Ehhez különböző szemcseméretű acetil-szalicilsav porok frakcióinak és a keverékeinek szemcseméret-eloszlását mértem, majd felvettem a porkeverék és tiszta hatóanyagot tartalmazó kapszulák kioldódásgörbéjét. Ezután különböző módszereket (mesterséges neurális hálózat, részleges legkisebb négyzetek módszere) hasonlítottam össze és optimalizáltam a szemcseméret-eloszlás osztályszámainak és típusának (szám- vagy térfogatalapú) függvényében. Az eredmények jó kiindulópontként szolgálhatnak a szemcseméret-eloszlás komplex készítmények (pl. tabletták) kioldódására gyakorolt hatásának tanulmányozásához",Erdélyi Magyar Műszaki Tudományos Társaság (EMT) - Kémia Szakosztálya,Acetil-szalicilsav porok és tiszta hatóanyagot tartalmazó kapszulák kioldódásának jóslása a szemcseméret-eloszlás ismeretében: Predicting the Dissolution of Acetylsalicylic Acid Powder and Capsules with Pure Active Ingredients depending on the Particle Size Distribution,,https://core.ac.uk/download/356674913.pdf,,core
345076769,2020-05-19T00:00:00,"Nowadays, digitization is transforming the way businesses work. Recently, Artificial Intelligence (AI) techniques became an essential part of the automation of business processes: In addition to cost advantages, these techniques offer fast processing times and higher customer satisfaction rates, thus ultimately increasing sales. One of the intelligent approaches for accelerating digital transformation in companies is the Robotic Process Automation (RPA). 

An RPA-system is a software tool that robotizes routine and time-consuming responsibilities such as email assessment, various calculations, or creation of documents and reports (Mohanty and Vyas, 2018). Its main objective is to organize a smart workflow and therethrough to assist employees by offering them more scope for cognitively demanding and engaging work.



Intelligent Process Automation (IPA) offers all these advantages as well; however, it goes beyond the RPA by adding AI components such as Machine- and Deep Learning techniques to conventional automation solutions. Previously, IPA approaches were primarily employed within the computer vision domain. However, in recent times, Natural Language Processing (NLP) became one of the potential applications for IPA as well due to its ability to understand and interpret human language. Usually, NLP methods are used to analyze large amounts of unstructured textual data and to respond to various inquiries. However, one of the central applications of NLP within the IPA domain – are conversational interfaces (e.g., chatbots, virtual agents) that are used to enable human-to-machine communication. Nowadays, conversational agents gain enormous demand due to their ability to support a large number of users simultaneously while communicating in a natural language. The implementation of a conversational agent comprises multiple stages and involves diverse types of NLP sub-tasks, starting with natural language understanding (e.g., intent recognition, named entity extraction) and going towards dialogue management (i.e., determining the next possible bots action) and response generation. Typical dialogue system for IPA purposes undertakes straightforward customer support requests (e.g., FAQs), allowing human workers to focus on more complicated inquiries.



In this thesis, we are addressing two potential Intelligent Process Automation (IPA) applications and employing statistical Natural Language Processing (NLP) methods for their implementation.



The first block of this thesis (Chapter 2 – Chapter 4) deals with the development of a conversational agent for IPA purposes within the e-learning domain. As already mentioned, chatbots are one of the central applications for the IPA domain since they can effectively perform time-consuming tasks while communicating in a natural language. Within this thesis, we realized the IPA conversational bot that takes care of routine and time-consuming tasks regularly performed by human tutors of an online mathematical course. This bot is deployed in a real-world setting within the OMB+ mathematical platform. Conducting experiments for this part, we observed two possibilities to build the conversational agent in industrial settings – first, with purely rule-based methods, considering the missing training data and individual aspects of the target domain (i.e., e-learning). Second, we re-implemented two of the main system components (i.e., Natural Language Understanding (NLU) and Dialogue Manager (DM) units) using the current state-of-the-art deep-learning architecture (i.e., Bidirectional Encoder Representations from Transformers (BERT)) and investigated their performance and potential use as a part of a hybrid model (i.e., containing both rule-based and machine learning methods).



The second part of the thesis (Chapter 5 – Chapter 6) considers an IPA subproblem within the predictive analytics domain and addresses the task of scientific trend forecasting. Predictive analytics forecasts future outcomes based on historical and current data. Therefore, using the benefits of advanced analytics models, an organization can, for instance, reliably determine trends and emerging topics and then manipulate it while making significant business decisions (i.e., investments). In this work, we dealt with the trend detection task – specifically, we addressed the lack of publicly available benchmarks for evaluating trend detection algorithms. We assembled the benchmark for the detection of both scientific trends and downtrends (i.e., topics that become less frequent overtime). To the best of our knowledge, the task of downtrend detection has not been addressed before. The resulting benchmark is based on a collection of more than one million documents, which is among the largest that has been used for trend detection before, and therefore, offers a realistic setting for the development of trend detection algorithms.Robotergesteuerte Prozessautomatisierung (RPA) ist eine Art von Software-Bots, die manuelle menschliche Tätigkeiten wie die Eingabe von Daten in das System, die Anmeldung in Benutzerkonten oder die Ausführung einfacher, aber sich wiederholender Arbeitsabläufe nachahmt (Mohanty and Vyas, 2018). Einer der Hauptvorteile und gleichzeitig Nachteil der RPA-bots ist jedoch deren Fähigkeit, die gestellte Aufgabe punktgenau zu erfüllen. Einerseits ist ein solches System in der Lage, die Aufgabe akkurat, sorgfältig und schnell auszuführen. Andererseits ist es sehr anfällig für Veränderungen in definierten Szenarien. Da der RPA-Bot für eine bestimmte Aufgabe konzipiert ist, ist es oft nicht möglich, ihn an andere Domänen oder sogar für einfache Änderungen in einem Arbeitsablauf anzupassen (Mohanty and Vyas, 2018). Diese Unfähigkeit, sich an veränderte Bedingungen anzupassen, führte zu einem weiteren Verbesserungsbereich für RPAbots – den Intelligenten Prozessautomatisierungssystemen (IPA).



IPA-Bots kombinieren RPA mit Künstlicher Intelligenz (AI) und können komplexe und kognitiv anspruchsvollere Aufgaben erfüllen, die u.A. Schlussfolgerungen und natürliches Sprachverständnis erfordern. Diese Systeme übernehmen zeitaufwändige und routinemäßige Aufgaben, ermöglichen somit einen intelligenten Arbeitsablauf und befreien Fachkräfte für die Durchführung komplizierterer Aufgaben.  Bisher wurden die IPA-Techniken hauptsächlich im Bereich der Bildverarbeitung eingesetzt. In der letzten Zeit wurde die natürliche

Sprachverarbeitung (NLP) jedoch auch zu einem der potenziellen Anwendungen für IPA, und zwar aufgrund von der Fähigkeit, die menschliche Sprache zu interpretieren. NLP-Methoden werden eingesetzt, um große Mengen an Textdaten zu analysieren und auf verschiedene

Anfragen zu reagieren. Auch wenn die verfügbaren Daten unstrukturiert sind oder kein vordefiniertes Format haben (z.B. E-Mails), oder wenn die in einem variablen Format vorliegen (z.B. Rechnungen, juristische Dokumente), dann werden ebenfalls die NLP Techniken angewendet, um die relevanten Informationen zu extrahieren, die dann zur Lösung verschiedener Probleme verwendet werden können.



NLP im Rahmen von IPA beschränkt sich jedoch nicht auf die Extraktion relevanter Daten aus Textdokumenten. Eine der zentralen Anwendungen von IPA sind Konversationsagenten, die zur Interaktion zwischen Mensch und Maschine eingesetzt werden. Konversationsagenten erfahren enorme Nachfrage, da sie in der Lage sind, eine große Anzahl von Benutzern gleichzeitig zu unterstützen, und dabei in einer natürlichen Sprache kommunizieren. Die Implementierung

eines Chatsystems umfasst verschiedene Arten von NLP-Teilaufgaben, beginnend mit dem Verständnis der natürlichen Sprache (z.B. Absichtserkennung, Extraktion von Entitäten) über das Dialogmanagement (z.B. Festlegung der nächstmöglichen Bot-Aktion) bis hin zur Response-Generierung. Ein typisches Dialogsystem für IPA-Zwecke übernimmt in der Regel unkomplizierte Kundendienstanfragen (z.B. Beantwortung von FAQs), so dass sich die Mitarbeiter auf komplexere Anfragen konzentrieren können.



Diese Dissertation umfasst zwei Bereiche, die durch das breitere Thema vereint sind, nämlich die Intelligente Prozessautomatisierung (IPA) unter Verwendung statistischer Methoden der natürlichen Sprachverarbeitung (NLP).



Der erste Block dieser Arbeit (Kapitel 2 – Kapitel 4) befasst sich mit der Impementierung eines Konversationsagenten für IPA-Zwecke innerhalb der E-Learning-Domäne. Wie bereits erwähnt, sind Chatbots eine der zentralen Anwendungen für die IPA-Domäne, da sie zeitaufwändige Aufgaben in einer natürlichen Sprache effektiv ausführen können. Der IPA-Kommunikationsbot, der in dieser Arbeit realisiert wurde, kümmert sich ebenfalls um routinemäßige und zeitaufwändige Aufgaben, die sonst von Tutoren in einem Online-Mathematikkurs in deutscher Sprache durchgeführt werden. Dieser Bot ist in der täglichen Anwendung innerhalb der mathematischen Plattform OMB+ eingesetzt. Bei der Durchführung von Experimenten beobachteten wir zwei Möglichkeiten, den Konversationsagenten im industriellen Umfeld zu entwickeln – zunächst mit rein regelbasierten Methoden, unter Bedingungen der fehlenden Trainingsdaten und besonderer Aspekte der Zieldomäne (d.h. E-Learning). Zweitens haben wir zwei der Hauptsystemkomponenten (Sprachverständnismodul, Dialog-Manager) mit dem derzeit fortschrittlichsten Deep Learning Algorithmus reimplementiert und die Performanz dieser Komponenten untersucht.



Der zweite Teil der Doktorarbeit (Kapitel 5 – Kapitel 6) betrachtet ein IPA-Problem innerhalb des Vorhersageanalytik-Bereichs. Vorhersageanalytik zielt darauf ab, Prognosen über zukünftige Ergebnisse auf der Grundlage von historischen und aktuellen Daten zu erstellen. Daher kann ein Unternehmen mit Hilfe der Vorhersagesysteme z.B. die Trends oder neu entstehende Themen zuverlässig bestimmen und diese Informationen dann bei wichtigen Geschäftsentscheidungen (z.B. Investitionen) einsetzen. In diesem Teil der Arbeit beschäftigen wir uns mit dem Teilproblem der Trendprognose – insbesondere mit dem Fehlen öffentlich zugänglicher Benchmarks für die Evaluierung von Trenderkennungsalgorithmen. Wir haben den Benchmark zusammengestellt und veröffentlicht, um sowohl Trends als auch Abwärtstrends zu erkennen. Nach unserem besten Wissen ist die Aufgabe der Abwärtstrenderkennung bisher nicht adressiert worden. Der resultierende Benchmark basiert auf einer Sammlung von mehr als einer Million Dokumente, der zu den größten gehört, die bisher für die Trenderkennung verwendet wurden, und somit einen realistischen Rahmen für die Entwicklung von Trenddetektionsalgorithmen bietet",Ludwig-Maximilians-Universität München,Statistical natural language processing methods for intelligent process automation,,,,core
326905755,2020-04-23T00:00:00,"The smart grid control applications necessitate real-time communication systems with time efficiency for real-time monitoring, measurement, and control. Time-efficient communication systems should have the ability to function in severe propagation conditions in smart grid applications. The data/packet communications need to be maintained by synchronized timing and reliability through equally considering the signal deterioration occurrences, which are propagation delay, phase errors and channel conditions. Phase synchronization plays a vital part in the digital smart grid to get precise and real-time control measurement information. IEEE C37.118 and IEC 61850 had implemented for the synchronization communication to measure as well as control the smart grid applications. Both IEEE C37.118 and IEC 61850 experienced a huge propagation and packet delays due to synchronization precision issues. Because of these delays and errors, measurement and monitoring of the smart grid application in real-time is not accurate. Therefore, it has been investigated that the time synchronization in real-time is a critical challenge in smart grid applications, and for this issue, other errors raised consequently. The existing communication systems are designed with the phasor measurement unit (PMU) along with communication protocol IEEE C37.118 and uses the GPS timestamps as the reference clock stamps. The absence of GPS increases the clock offsets, which surely can hamper the synchronization process and the full control measurement system that can be imprecise. Therefore, to reduce this clock offsets, a new algorithm is needed which may consider any alternative reference timestamps rather than GPS. The revolutionary Artificial Intelligence (AI) enables the industrial revolution to provide a significant performance to engineering solutions. Therefore, this article proposed the AI-based Synchronization scheme to mitigate smart grid timing issues. The backpropagation neural network is applied as the AI method that employs the timing estimations and error corrections for the precise performances. The novel AIFS scheme is considered the radio communication functionalities in order to connect the external timing server. The performance of the proposed AIFS scheme is evaluated using a MATLAB-based simulation approach. Simulation results show that the proposed scheme performs better than the existing system",'Springer Science and Business Media LLC',A Novel Artificial Intelligence Based Timing Synchronization Scheme for Smart Grid Applications,10.1007/s11277-020-07408-w,,,core
322738580,2020-01-01T00:00:00,"An important research problem in artificial intelligence is how to organize multiple agents, and coordinate them, so that they can work together to solve problems. Coordinating agents in a multi-agent system can significantly affect the systems performance-the agents can, in many instances, be organized so that they can solve tasks more efficiently, and consequently benefit collectively and individually. Central to this endeavor is coalition formation-the process by which heterogeneous agents organize and form disjoint groups (coalitions). Coalition formation often involves finding a coalition structure (an exhaustive set of disjoint coalitions) that maximizes the systems potential performance (e.g., social welfare) through coalition structure generation. However, coalition structure generation typically has no notion of goals. In cooperative settings, where coordination of multiple coalitions is important, this may generate suboptimal teams for achieving and accomplishing the tasks and goals at hand. With this in mind, we consider simultaneously generating coalitions of agents and assigning the coalitions to independent alternatives (e.g., tasks/goals), and present an anytime algorithm for the simultaneous coalition structure generation and assignment problem. This combinatorial optimization problem hasmany real-world applications, including forming goal-oriented teams. To evaluate the presented algorithms performance, we present five methods for synthetic problem set generation, and benchmark the algorithm against the industry-grade solver CPLEXusing randomized data sets of varying distribution and complexity. To test its anytime-performance, we compare the quality of its interim solutions against those generated by a greedy algorithm and pure random search. Finally, we also apply the algorithm to solve the problem of assigning agents to regions in a major commercial strategy game, and show that it can be used in game-playing to coordinate smaller sets of agents in real-time.Funding Agencies|Linkoping University; Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation</p",'Springer Science and Business Media LLC',An anytime algorithm for optimal simultaneous coalition structure generation and assignment,10.1007/s10458-020-09450-1,,,core
334990296,2020-08-01T07:00:00,"The increasing number of spacefaring nations and agendas, miniaturization of subsystems, and trend toward integrated systems are no doubt influencing the evolution of space systems. The diversification of space architectures has surged at an unprecedented rate in recent history with initial deployments of planned mega-constellations. This paper explores HIVE-a reconfigurable small satellite system primed to revolutionize the concept of modular space systems and future space architectures.
Based on a mass producible functioning unit consisting of nested rings, HIVE is a comprehensive satellite design harnessing advancement in robotics, software and machine learning, precision scale manufacturing, and novel materials with multifunctional properties. HIVE is addressing solutions for detailed design of interconnected hardware, engineering analysis for multi-payload applications, and policy to accomplish modularized, in-space deployment and reconfiguration.
The HIVE unit design lends itself to the “infinite possibilities” of space mission architectures and presents a revolutionary way to design, integrate, and operate missions from space. This paper provides and overview of the HIVE concept development and provides examples of applications for HIVE to showcase the range of possible systems and architectural advantages; such as space domain awareness, large service structure, and planetary surface infrastructure. Finally, we will discuss technology transfer and possible pathways to making a resilient, adaptable, and continually upgradable space infrastructure a reality",DigitalCommons@USU,HIVE: A Space Architecture Concept,,https://core.ac.uk/download/334990296.pdf,,core
389431508,2020-10-16T00:00:00,"Intelligent assistive robots can potentially support elderly persons and caregivers in their everyday
lives and facilitate a closer man and machine collaboration as an essential part of the yet to come 5-th industrial revolution. In contrast to classical robotic applications where robots were mostly designed for repetitive tasks, assistive robots will face a variety of different tasks in close contact with everyday users. In particular, it is difficult to foresee the variety of applications beforehand since they depend on a person's individual needs and preferences. This renders preprogramming of all tasks for assistive robots difficult and gives need to explore methods of how robots can learn new tasks at hand during deployment time. Learning from and during direct interaction with humans provides hereby a potentially powerful tool for an assistive robot to acquire new skills and incorporate prior human knowledge during the exploration of novel tasks. Such an interactive learning process can not only help the robot to acquire new skills or profit from human prior knowledge but also facilitates the participation of inexperienced users or coworkers which can lead to a higher acceptance of the robot. However, while on the one hand human presence and assistance can be beneficial during the learning process, on the other hand, close contact with inexperienced users also imposes challenges. In shared workspaces or in close contact with everyday users a robot should be able to adapt learned skills to achieve as little disturbance of humans as possible. It becomes also important to evaluate human preferences about such adaptation strategies, their understanding of interactive learning processes and different ways for human input into learning. To come closer to the goal of intelligent assistive robots is therefore important to develop novel interactive learning methods and evaluate them in different robotic applications. 
This thesis focusses on three main challenges related to the development of assistive intelligent robots and their interaction with everyday users. The different parts of the thesis contribute not only novel theoretical methods but additionally also evaluations on different robotic tasks with users, that had zero or only little prior experience with robots. 
The first challenge is to enable robots to learn cooperative skills from a potentially open-ended stream of human demonstrations in an incremental fashion. While learning new skills from human demonstrations has already been exploited in the literature it remains challenging to learn skill libraries from incrementally incoming demonstrations and when the total number of skills is not known beforehand. Therefore, in the first part of the thesis, we introduce an approach for online and incremental learning of a library for collaborative skills. Here, we follow a Mixture of Experts based approach and incrementally learn a library of collaborative skills and a gating model from coupled human-robot trajectories. Once trained, the gating model can decide which skill to choose as an appropriate response to a human motion, based on prior demonstrations and activate the corresponding robot skill. In contrast to existing batch learning methods, our method does not require the total number of skills to be known a priori and can learn new skills as well as update existing skills from multiple human demonstrations. The cooperative skills are represented as Probabilistic Interaction Primitives which can capture variance and inherent correlations in the demonstrations. We evaluate our method with different human subjects in a task where a robot assists the subjects in making a salad. We also evaluate hereby how learned skills transfer between different subjects.
Second, intelligent assistive robots should be able to adapt learned skills to humans when working in close contact or shared workspaces. For Probabilistic Movement Primitives (ProMPs), which were chosen as a skill representation in this thesis, such methods for online adaptation were missing in the literature so far. Hereby, it is in particular important to also evaluate the perceived level of safety and comfort of humans according to different adaptation strategies. To this end, we present two methods for online adaptation of learned skills in a shared workspace setting. Here, we introduce two novel online adaptation methods for ProMPs, namely spatial deformation and temporal scaling. Spatial deformation avoids collisions by dynamically changing the shape of the movement primitive, while at the same time staying close to the demonstrated motions. In temporal scaling, we adapt the ProMP's velocity profile to avoid time-dependent collisions. To achieve intention aware adaptation in shared workspaces we combine both methods with a goal-directed prediction model for human motions. This prediction model can also be learned online from human motions. We conducted experiments for both novel adaptation methods in comparison to non-adaptive behavior with inexperienced users and evaluated influences on task performance as well as subjective metrics such as comfort and perceived level of safety.
The third challenge that we consider in this thesis is how a library of learned skills can be used in practice to solve sequential robotic tasks. While hereby reinforcement learning offers a powerful tool for reward-driven learning and self-improvement, in real robotic applications it often suffers from costly and time-consuming sample collection. Here, human input might be beneficial to speed up and guide the learning. Therefore, it is important to enable and compare different ways how human input can be incorporated in reinforcement learning algorithms. 
In this thesis, we present an approach, which incorporates multiple forms of human input into reinforcement learning for sequential tasks. Since depending on the task human input might not always be correct, we additionally introduce the concept of self-confidence for the robot, such that it becomes able to question human input. We evaluate which input channels humans prefer during interaction and how well they accept suggestions or rejections of the robot if the robot becomes confident in its own decisions.
To summarize, the different parts of the thesis contribute to the development of intelligent assistive robots that can learn from imitating humans, adapt the learned skills dynamically to humans in shared workspaces and profit and learn from human input during self-driven learning of how to sequence skills into more complex tasks. The three main contributions to the state of the art are hereby: First, a novel approach to incrementally learn a library for collaborative skills when the total number of skills is not known a priori. Second, two novel methods for online adaptation of ProMPs and their combination with a goal-directed prediction model to enable intention aware online adaptation in shared workspaces. And third, an approach that combines multiple forms of human input with a reinforcement learning algorithm and a novel concept of self-confidence to learn and improve the sequencing of skills into more complex tasks",,Interactive Machine Learning for Assistive Robots,10.25534/tuprints-00014184,,,core
441356408,2020-07-01T00:00:00,"Abstract Over the years, technology has changed the way we produce and have access to our food through the development of applications, robotics, data analysis, and processing techniques. The implementation of these approaches by the food industry ensure quality and affordability, reducing at the same time the costs of keeping the food fresh and increase productivity. A system, as the one presented herein, for raw food categorization is needed in future food industries to automate food classification according to type, the process of algorithm approaches that will be applied to every different food origin and also for serving disabled people. The purpose of this work was to develop a machine learning workflow based on supervised PLS regression and SVM classification, towards automated raw food categorization from FTIR. The system exhibited high efficiency in multi-class classification of 7 different types of raw food. The selected food samples, were diverse in terms of storage conditions (temperature, storage time and packaging), while the variability within each food was also taken into account by several different batches; leading in a classifier able to embed this variation towards increased robustness and efficiency, ready for real life applications targeting to the digital transformation of the food industry",'Springer Science and Business Media LLC',A machine learning workflow for raw food spectroscopic classification in a future industry,10.1038/s41598-020-68156-2,,"[{'title': 'Scientific Reports', 'identifiers': ['2045-2322', 'issn:2045-2322']}]",core
424061923,2020-01-01T00:00:00,"Optimised Asset Management in recent years has embraced a rapid diffusion of innovation and disruptive technology, especially with reference to infrared thermography. The integration of this technology and associated communication technologies are improving operational aspects of industry, including the prediction of machine and component failures and as a diagnostic tool in medicine. Most machines in the future will be connected to the Internet of Things (IoT) which will be the gateway to communicating with intelligent assets with self-diagnosing capabilities and expert systems. Asset connectivity and predictive analytics will discern patterns and algorithms leading optimised plant production, and enhanced energy efficiency particularly with reference to machines. Machine learning models will indicate future operation on a real-time basis using big data libraries, tabular databases with particular reference to condition monitoring. This paper concentrates on the application of qualitative and quantitative portable infrared thermography. Successful implementation of an intelligent portable infrared thermography system requires an understanding of the industrial process; the machine operation, its surroundings, and the dynamics of infrared radiation. Optimising and integrating infrared into an asset management subsystem requires correct monitoring equipment selection and accurate data collection including; Optimum radiometer wavelength, background characterization, spatial and thermal resolution and emissivity. This paper concludes with an industrial atlas of infrared normal and abnormal images, which are a useful reference in determining the various conditions in medical diagnostics with the primary intention to identify early onset of problems as part of an optimised asset management system",'Springer Science and Business Media LLC',Achieving Optimised Infrared Thermography in Innovative Asset Management,10.1007/978-3-030-57745-2_126,,,core
348664332,2020-01-01T00:00:00,"Background: Eggs have acquired a greater importance as an inexpensive and high-quality protein. The Brazilian egg industry has been characterized by a constant production expansion in the last decade, increasing the number of housed animals and facilitating the spread of many diseases. In order to reduce the sanitary and financial risks, decisions regard¬ing the production and the health status of the flock must be made based on objective criteria. The use of Artificial Neural Networks (ANN) is a valuable tool to reduce the subjectivity of the analysis. In this context, the aim of this study was at validating the ANNs as viable tool to be employed in the prediction and management of commercial egg production flocks. Materials, Methods & Results: Data from 42 flocks of commercial layer hens from a poultry company were selected. The data refer to the period between 2010 and 2018 and it represents a total of 600,000 layers. Six parameters were selected as “output” data (number of dead birds per week, feed consumption, number of eggs, weekly weight, weekly egg produc¬tion and flock uniformity) and a total of 13 parameters were selected as “input” data (flock age, flock identification, total hens in the flock, weekly weight, flock uniformity, lineage, weekly mortality, absolute number of dead birds, eggs/hen, weekly egg production, feed consumption, flock location, creation phase). ANNs were elaborated by software programs NeuroShell Predictor and NeuroShell Classifier. The programs identified input variables for the assembly of the networks seeking the prediction of the variables called outgoing that are subsequently validated. This validation goes through the comparison between the predictions and the real data present in the database that was the basis for the work. Validation of each ANN is expressed by the specific statistical parameters multiple determination (R2) and Mean Squared Error (MSE). For instance, R2 above 0.70 expresses a good validation. ANN developed for the output variable “number of dead birds per week” presented R2= 0.9533 and MSE= 256.88. For “feed consumption”, the results were R2= 0.7382 and MSE= 274.56. For “number of eggs (eggs/hen)”, the results were R2= 0.9901 and MSE= 172.26. For “weekly weight”, R2= 0.9712 and MSE= 11154.41. For “weekly egg production”, R2= 0.8015 and MSE= 72.60. For “flock uniformity”, R2= -2.9955 and MSE= 431.82. Discussion: From the six ANN designed in this study, in five it was possible to validate the predictions by comparing predictions with the real data. In one output parameter (“flock uniformity”), it was not possible to have adequate validation due to insufficient data in our database. For “number of dead birds per week”, “feed consumption”, “weekly weight” and “uniformity”, the most important variable was “flock age” (27.5%, 52.5%, 55.2% and 37.9%, respectively). For “number of eggs (eggs/hen)”, “uniformity” (52.1%) was the most relevant variable for prediction. For “weekly egg production”, “flock age” and “number of eggs (eggs/hen)” were the most important zootechnical parameters, both with a relative contribution of 38.2%. The results showed that even with the use of a robust tool such as ANNs, it is necessary to have well-noted and clear information that expresses the reality of the flocks. In any case, the results presented allow us to state that ANNs are capable for the management of data generated in a commercial egg production facility. The process of evaluation of these data would be improved if ANNs were routinely used by the professionals linked to this activity",,Artificial neural networks on eggs production data management,,,"[{'title': None, 'identifiers': ['1678-0345', 'issn:1678-0345']}]",core
387287427,2020-12-02T00:00:00,"Next-generation wireless networks are getting significant attention because
they promise 10-factor enhancement in mobile broadband along with the potential
to enable new heterogeneous services. Services include massive machine type
communications desired for Industrial 4.0 along with ultra-reliable low latency
services for remote healthcare and vehicular communications. In this paper, we
present the design of an intelligent and reconfigurable physical layer (PHY) to
bring these services to reality. First, we design and implement the
reconfigurable PHY via a hardware-software co-design approach on system-on-chip
consisting of the ARM processor and field-programmable gate array (FPGA). The
reconfigurable PHY is then made intelligent by augmenting it with online
machine learning (OML) based decision-making algorithm. Such PHY can learn the
environment (for example, wireless channel) and dynamically adapt the
transceivers' configuration (i.e., modulation scheme, word-length) and select
the wireless channel on-the-fly. Since the environment is unknown and changes
with time, we make the OML architecture reconfigurable to enable dynamic switch
between various OML algorithms on-the-fly. We have demonstrated the functional
correctness of the proposed architecture for different environments and
word-lengths. The detailed throughput, latency, and complexity analysis
validate the feasibility and importance of the proposed intelligent and
reconfigurable PHY in next-generation networks",,Towards Intelligent Reconfigurable Wireless Physical Layer (PHY),,http://arxiv.org/abs/2012.01153,,core
328196703,2020-08-16T00:00:00,"We present a holistic data-driven approach to the problem of productivity
increase on the example of a metallurgical pickling line. The proposed approach
combines mathematical modeling as a base algorithm and a cooperative
Multi-Agent Reinforcement Learning (MARL) system implemented such as to enhance
the performance by multiple criteria while also meeting safety and reliability
requirements and taking into account the unexpected volatility of certain
technological processes. We demonstrate how Deep Q-Learning can be applied to a
real-life task in a heavy industry, resulting in significant improvement of
previously existing automation systems.The problem of input data scarcity is
solved by a two-step combination of LSTM and CGAN, which helps to embrace both
the tabular representation of the data and its sequential properties. Offline
RL training, a necessity in this setting, has become possible through the
sophisticated probabilistic kinematic environment.Comment: 8 pages, 6 figure",,"The reinforcement learning-based multi-agent cooperative approach for
  the adaptive speed regulation on a metallurgical pickling line",,http://arxiv.org/abs/2008.06933,,core
334935031,2020-04-21T00:00:00,"Universal grasping of a diverse range of previously unseen objects from heaps
is a grand challenge in e-commerce order fulfillment, manufacturing, and home
service robotics. Recently, deep learning based grasping approaches have
demonstrated results that make them increasingly interesting for industrial
deployments. This paper explores the problem from an automation systems
point-of-view. We develop a robotics grasping system using Dex-Net, which is
fully integrated at the controller level. Two neural networks are deployed on a
novel industrial AI hardware acceleration module close to a PLC with a power
footprint of less than 10 W for the overall system. The software is tightly
integrated with the hardware allowing for fast and efficient data processing
and real-time communication. The success rate of grasping an object form a bin
is up to 95 percent with more than 350 picks per hour, if object and receptive
bins are in close proximity. The system was presented at the Hannover Fair 2019
(world s largest industrial trade fair) and other events, where it performed
over 5,000 grasps per event",,"Industrial Robot Grasping with Deep Learning using a Programmable Logic
  Controller (PLC)",,http://arxiv.org/abs/2004.10251,,core
334935886,2020-04-24T00:00:00,"Over the past decade, deep learning (DL) has been successfully applied to
many industrial domain-specific tasks. However, the current state-of-the-art DL
software still suffers from quality issues, which raises great concern
especially in the context of safety- and security-critical scenarios.
Adversarial examples (AEs) represent a typical and important type of defects
needed to be urgently addressed, on which a DL software makes incorrect
decisions. Such defects occur through either intentional attack or
physical-world noise perceived by input sensors, potentially hindering further
industry deployment. The intrinsic uncertainty nature of deep learning
decisions can be a fundamental reason for its incorrect behavior. Although some
testing, adversarial attack and defense techniques have been recently proposed,
it still lacks a systematic study to uncover the relationship between AEs and
DL uncertainty. In this paper, we conduct a large-scale study towards bridging
this gap. We first investigate the capability of multiple uncertainty metrics
in differentiating benign examples (BEs) and AEs, which enables to characterize
the uncertainty patterns of input data. Then, we identify and categorize the
uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated
by existing methods do follow common uncertainty patterns, some other
uncertainty patterns are largely missed. Based on this, we propose an automated
testing technique to generate multiple types of uncommon AEs and BEs that are
largely missed by existing techniques. Our further evaluation reveals that the
uncommon data generated by our method is hard to be defended by the existing
defense techniques with the average defense success rate reduced by 35\%. Our
results call for attention and necessity to generate more diverse data for
evaluating quality assurance solutions of DL software.Comment: 13 page",,"Towards Characterizing Adversarial Defects of Deep Learning Software
  from the Lens of Uncertainty",,http://arxiv.org/abs/2004.11573,,core
357337238,2020-04-03T00:00:00,"This paper investigates the use of an automatic system for preparation of gas mixtures in a multivariate calibration problem involving near-infrared (NIR) spectrometric analysis of natural gas. The automatic system is used to prepare calibration mixtures according to a Brereton experimental design, in order to exploit a suitable range of gas concentrations and thus avoid extrapolations in the predictions. These mixtures were employed to build partial-least-squares models for NIR determination of methane, ethane and propane, which are the major components of natural gas. Prediction performance was evaluated by using a separate set of prepared mixtures and natural gas samples with composition analyzed by gas chromatography, as well as a group of certified mixtures. The resulting root-mean-square errors of prediction (RMSEP) values for methane, ethane and propane (3.0, 0.9 and 1.2% mol mol -1 , respectively) were approximately 10 times smaller than the corresponding calibration ranges, with correlations of 0.91, 0.96 and 0.86 between the predicted and reference values. Keywords: natural gas analysis, automatic system for preparation of gas mixtures, multivariate calibration, NIR spectrometry, gas chromatography Introduction The analysis of chemical composition of gas samples is usually carried out by using gas chromatography (GC), which allows accurate determinations of individual gas components even in complex matrices. The widespread use of GC is motivated by the minimization of interference effects as the result of the separation in the chromatographic column. 1 However, the operational costs related to the use of consumables and the low sample throughput associated to the time required by the separation process are inconveniences that should be taken into account. In this context, spectrometric techniques have been proposed as a faster and less costly alternative for gas analysis, 2-4 provided that multivariate calibration is used to compensate for the absence of a separation process. 5 Multivariate calibration methods are aimed at obtaining a mathematical model that relates the instrumental measurements with the chemical composition of the sample. For this purpose, the analyst must gather a representative set of calibration samples with known composition. In the case of gas analysis, calibration mixtures with certified composition can be acquired from specialized suppliers. However, the acquisition of these mixtures can be expensive, which escapes the purpose of using a less costly alternative to GC. Alternatively, real samples with composition determined by GC can be used to build the multivariate calibration model, but the variability in the composition of these samples may not be large enough to build an appropriate model.  In this context, the present work investigates the use of an automatic system for accurate preparation of gas mixtures, which was proposed in a recent paper 7 as an improvement on a simpler architecture which had been developed for non-quantitative screening applications.  Use of an Automatic System in the Preparation of Gas Mixtures for Multivariate Calibration J. Braz. Chem. Soc.  2030    system comprises a set of gas admission valves which are controlled in an automatic manner to achieve the desired partial pressures for each component of the mixture. A piston-driven diaphragm pump is used to circulate the mixture within the system in order to obtain an appropriate homogenization. In Dantas et al.,   7 the operation of the system was validated by preparing binary mixtures of nitrogen with methane, ethane or propane. As a result, the programmed molar fractions of the component gases in the prepared mixtures were found to be in good agreement with the results of GC analysis. However, the system was not tested in an actual application involving the preparation of gas mixtures for multivariate calibration. Within this scope, the present investigation is aimed at demonstrating the applicability of this automatic system in an actual analytical problem involving the simultaneous determination of the major components in natural gas samples by using nearinfrared (NIR) spectrometry and multivariate calibration. Natural gas (NG) is mainly composed by methane (CH 4 ) and heavier hydrocarbons, especially ethane (C 2 H 6 ) and propane (C 3 H 8 ). 9 The development of analytical methods for quality control of this fuel has become an important issue, 10 in view of the growing demand for domestic, commercial, industrial, utility and vehicular use of NG, motivated by both economic gains and environmental impact. 11,12 Within this scope, NIR spectrometry has been proposed as an attractive alternative to the use of GC, with advantages including reduced analysis time and little sample preparation 2,13 in addition to the possibility of deploying portable field instruments. 5 More specifically, the use of NIR spectrometry has been reported for screening analysis 8 and determination of the calorific value of NG. 14 In a broader scope, applications have also been reported in the context of screening analysis of liquefied petroleum gas 15    and quantitative analysis of gases in hydrocarbon mixtures.  18 The prediction performance of the resulting model was evaluated by using a separate set of prepared mixtures, as well as three gas mixtures with certified composition and eight actual NG samples for vehicular use. Experimental Samples Methane (99.9%), ethane (99.0%), propane (99.5%), nitrogen (99.9%) and three mixtures of these gases, with certified composition, were acquired from Linde Gas. The certified mixtures were designed in order to simulate the composition of natural gas samples. All gas contents indicated herein are expressed in % mol mol -1 . In addition, eight real NG samples were acquired at 220 bar from vehicle fuelling stations in the city of João Pessoa (Paraíba, Brazil). These samples were collected by using a lab-made sampling cylinder described elsewhere.  After the NIR spectra of the 67 prepared mixtures were recorded, the Kennard-Stone algorithm 21 was employed to select 45 of these mixtures for use in the calibration of the PLS model. This algorithm is aimed at choosing a representative subset of samples in a near-uniform manner in the space of spectral variables, by avoiding the selection of samples with similar spectra. The remaining 22 mixtures were used as a separate prediction set, together with the 3 mixtures of certified composition and the 8 real NG samples. The composition of these 33 prediction samples was analyzed by GC, in order to evaluate the predictive ability of the PLS model.  Apparatus 8 In addition, the system was connected to a gas chromatograph for the analysis of the prediction samples. The NIR spectra of the samples were acquired by using an FTIR Analyzer (AIT, Analect Diamond 20) in the range 4,000-12,000 cm -1 as the average of 16 scans with a resolution of 2.0 cm -1 . The samples were introduced in Barbosa et al.   2031  Vol. 26, No. 10, 2015 the NIR flow cell at a pressure of 1.5 bar. The overall time required by the NIR analysis was one minute per sample. The experimental procedures were carried out in a laboratory environment with air conditioning (split configuration) and dehumidifier units for temperature and humidity control. The temperature and relative humidity were controlled during the analyses in order to remain within the ranges of 23 ± 1 °C and 55 ± 1%, respectively. The gas mixing system is not fitted with internal temperature sensors. However, the internal pressure is controlled by using a digital manometer with precision of ± 0.001 bar. The pressure measurements provided by the digital manometer are employed by the system software to control the admission of the components of the gas mixture, in order to achieve partial pressures corresponding to the desired molar fractions (% mol mol -1 ). Changes in the internal temperature of the system will not affect the results in a significant manner, because the preparation of the gas mixtures is based on the actual pressure values. The GC analyses were carried out by using a gas chromatograph (GC-2014, Shimadzu) using a 30-meter capillary column (GC-GASPRO) with internal diameter of 0.32 mm. The GC injections were performed in split mode (1:100) at a temperature of 240 °C by using a sampling valve (Valco E60) with a 25 microliter loop. Helium was used as carrier gas with a flow rate of 1.4 mL min -1 . All analyses were carried out in isothermal mode with the column temperature at 90 °C. A flame ionization detector (FID) was employed with temperature set at 250 °C. The total analysis time per run was 10 min. Software Spectral preprocessing, principal component analysis and PLS modelling were carried out by using The Unscrambler 9.7 (CAMO S.A.). The optimal number of factors for each PLS model was determined by using cross-validation with the default settings of the software package. The Kennard-Stone algorithm was implemented in Matlab R2010b. Results and Discussion After a preliminary inspection of the NIR spectra, the range 4,000-6,500 cm -1 was selected in view of its large signal-to-noise ratio compared to other spectral regions. The intervals 4,000-4,600 cm -1 and 5,500-6,500 cm -1 correspond to combination bands and first overtones of CH, CH 2 , CH 3 related to the main hydrocarbons (methane, ethane, propane) of the gas samples. 22,23 Figure 2a presents the NIR spectra of three mixtures prepared in this study. In order to remove the baseline features, first-derivative spectra were obtained by using the Savitzky-Golay method with a 2 nd order polynomial and a 3-point window.  An exploratory analysis of the spectral data was carried out by using principal component analysis (PCA). As can be seen in  The PLS models for methane, ethane and propane were built by using 1, 4 and 5 factors, respectively. The three elliptical joint confidence regions (EJCRs) (obtained on the basis of a linear regression between the reference and predicted gas concentrations) are presented in  As can be seen in  Conclusions This paper investigated the use of an automatic system for preparation of gas mixtures in a multivariate calibration problem involving NIR spectrometric analysis of natural gas. The use of prepared calibration mixtures is of value to form an adequate envelope around the samples to be analyzed, which is convenient to avoid extrapolations in the model predictions. For this purpose, the automatic system is convenient to reduce the manual workload in the preparation of the mixtures and to minimize the possibility of human errors. The NIR spectra of 45 prepared mixtures in the range 4,000-6,500 cm -1 was employed to build PLS models for determination of methane, ethane and propane, which are the major components of natural gas. The prediction performance of the resulting models was evaluated by using a separate set of 22 prepared mixtures and 8 natural gas samples, with composition analyzed by gas chromatography, as well as 3 certified mixtures. Only the results associated to reference values larger than the limit of quantification were considered.The resulting RMSEP values for methane, ethane and propane (3.0, 0.9 and 1.2% mol mol -1 , respectively) were approximately 10 times smaller than the corresponding calibration ranges, with correlations of 0.91, 0.96 and 0.86 between the predicted and reference values. No systematic error was observed. In addition, the prediction errors for the certified mixtures and real NG samples were comparable to the errors obtained for the prepared mixtures. The results of this investigation reveal that the automatic system for preparation of gas mixtures is indeed of value for use in multivariate calibration applications. Supplementary Information Supplementary data (tables of concentrations of the components in the mixtures) are available free of charge at http://jbcs.sbq.org.br as PDF file",,Use of an Automatic System in the Preparation of Gas Mixtures for Multivariate Calibration: A Case Study Involving NIR Analysis of Natural Gas,,,,core
347169206,2020-09-01T00:00:00,"International audienceSuccessful development of a micro-total-analysis system (μTAS, lab-on-a-chip) is strictly related to the degree of miniaturization, integration, autonomy, sensitivity, selectivity, and repeatability of its detector. Fluorescence sensing is an optical detection method used for a large variety of biological and chemical assays, and its full integration within lab-on-a-chip devices remains a challenge. Important achievements were reported during the last few years, including improvements of previously reported methodologies, as well as new integration strategies. However, a universal paradigm remains elusive. This review considers achievements in the field of fluorescence sensing miniaturization, starting from off-chip approaches, representing miniaturized versions of their lab counter-parts, continuing gradually with strategies that aim to fully integrate fluorescence detection on-chip, and reporting the results around integration strategies based on optical-fiber-based designs,optical layer integrated designs, CMOS-based fluorescence sensing, and organic electronics. Further successful development in this field would enable the implementation of sensing networks in specific environments that, when coupled to Internet of-Things (IoT) and artificial intelligence (AI), could provide real-time data collection and, therefore, revolutionize fields like health, environmental, and industrial sensing",'Springer Science and Business Media LLC',Miniaturization of fluorescence sensing in optofluidic devices,10.1007/s10404-020-02371-1,,,core
347158571,2020-09-04T00:00:00,"Reinforcement learning (RL) offers promising opportunities to handle the ever-increasing complexity in managing modern production systems. We apply a Q-learning algorithm in combination with a process-based discrete-event simulation in order to train a self-learning, intelligent, and autonomous agent for the decision problem of order dispatching in a complex job shop with strict time constraints. For the first time, we combine RL in production control with strict time constraints. The simulation represents the characteristics of complex job shops typically found in semiconductor manufacturing. A real-world use case from a wafer fab is addressed with a developed and implemented framework. The performance of an RL approach and benchmark heuristics are compared. It is shown that RL can be successfully applied to manage order dispatching in a complex environment including time constraints. An RL-agent with a gain function rewarding the selection of the least critical order with respect to time-constraints beats heuristic rules strictly by picking the most critical lot first. Hence, this work demonstrates that a self-learning agent can successfully manage time constraints with the agent performing better than the traditional benchmark, a time-constraint heuristic combining due date deviations and a classical first-in-first-out approach",Springer,Reinforcement learning for an intelligent and autonomous production control of complex job-shops under time constraints,10.5445/IR/1000123326,https://core.ac.uk/download/347158571.pdf,,core
373000478,2020-01-01T00:00:00,"This work aimed to develop pressure and temperature sensor based on piezo-resistive and thermoelectric phenomena, using porous polyurethane and melamine foams impregnated with a solution of Poly (3,4-ethylenedioxythiophene)-poly(styrenesulfonate) (PEDOT:PSS). This device allows the simultaneous measurement of temperature and pressure through the transduction of external stimuli into separate electrical signals, with a promising resolution of temperature and pressure detection for artificial intelligence and health application systems. It was possible to manufacture individual sensors, which achieved a maximum performance of 90% of the resistance variation, with the ability to detect, from 3 to 30 kPa, between a range of 10 and 50% of deformation, respectively. These devices generated an average thermal voltage of 2.8 mV under elevated temperature stimulus. Considering the manufacturing parameters and the performance obtained for individual sensors, a 4x4 matrix was manufactured, using an innovative approach, not yet reported in the literature. A single piece of melamine impregnated with diluted PEDOT: PSS was used to construct the device. The sensor was integrated with the Arduino microcontroller and an interface was implemented with visual interface software for real-time monitoring of pressure position and intensity. The 4x4 matrix sensor device showed promising results, with the need, however, to improve the contacts between the electrodes and the sponge, since they presented degradation with intensive use.Este trabalho teve como objetivo desenvolver um sensor de pressão e temperatura baseado em fenômenos piezo-resistivo e termoelétrico, usando espuma de poliuretana porosa e melamina impregnadas em solução de Poly(3,4-ethylenedioxythiophene)-poly(styrenesulfonate) (PEDOT:PSS). Este dispositivo permite a medida simultânea de temperatura e pressão através da transdução de estímulos externos em sinais elétricos separados, com resolução promissora de detecção de temperatura e pressão para inteligência artificial e sistemas de aplicação em saúde. Foi possível fabricar sensores individuais, que atingiram um desempenho máximo de 90% da variação da resistência, com capacidade de detectar, de 3 a 30 kPa, entre uma faixa de 10 e 50% de deformação, respectivamente. Esses dispositivos geraram uma tensão térmica média de 2,8 mV sob estímulo de alta temperatura. Levando em consideração os parâmetros de fabricação e a performance obtida para sensores individuais, foi fabricada uma matriz 4x4, utilizando uma abordagem nova, ainda não relatada na literatura. Utilizou-se uma peça única de melamina impregnada em PEDOT:PSS diluído para a construção do dispositivo. O sensor foi integrado ao microcontrolador Arduino e uma interface foi implementada com software de interface visual para monitoramento em tempo real de posição e intensidade de pressão. O dispositivo sensor matricial 4x4 apresentou resultados promissores, havendo a necessidade, contudo, de se aprimorar os contatos entre os eletrodos e a esponja, visto que estes apresentavam degradação com o uso intensivo",,Flexible temperature pressure organic sensor,,,,core
289145261,2020-02-17T00:00:00,"The correct handling and gripping of objects depend on the precise control of the
applied force, together with the slip detection of the object to be held. The implementation
of tactile sensing for slip detection in the control of the gripping force can reduce the costs
of handling objects in the industry and bring more safety to those who operate with these
machines. Thus, an object gripping system was developed that acts in only one direction, it
receives signals from two pressure sensors and an acoustic sensor, and it has a stepper motor
that controls the gripping force exerted on the object. The developed system has its operating
principle based on the slip detection, in real-time, on the contact surface of the object, through
the analysis of the signal from the acoustic sensor, positioned on the surface, in addition to
the detection of the grip strength. A supervised learning algorithm was used as a classifier
and performed the function of detecting the presence of slipping on the device’s surface. Thus,
the control system of the grip strength on the object to be held is based on the classifier’s
response to the presence of slipping. Its answer, if positive, implies a fixed incremental gain in
the force exerted during the grip. In the end, the performance of the online grip control system
is analyzed for different objects and it is observed that the system can maintain the grip stable
for objects with greater mass and high rigidity.CAPES - Coordenação de Aperfeiçoamento de Pessoal de Nível SuperiorDissertação (Mestrado)O manuseio e preensão de objetos de forma correta depende do controle preciso da
força aplicada, juntamente com a detecção de escorregamento do objeto a ser segurado. A
implementação de sensoriamento tátil para a detecção de escorregamento no controle da força
de preensão pode reduzir os custos na manipulação de objetos na indústria e trazer mais
segurança para aqueles que operam junto a estas máquinas. Assim, foi desenvolvido um sistema
de preensão de objetos que atua em apenas uma direção, recebe os sinais de dois sensores de
pressão e um sensor acústico, e tem como atuador um motor de passo, que controla a força
de preensão exercida no objeto. O sistema desenvolvido teve seu princípio de funcionamento
baseado na detecção, em tempo real, de escorregamento na superfície de contato do objeto,
por meio da análise do sinal do sensor acústico, posicionado na superfície, além da detecção da
força de preensão. Um algoritmo de treinamento supervisionado foi usado como classificador e
desempenhou a função de detectar a presença de escorregamento na superfície do dispositivo.
Assim, o sistema de controle da força de preensão sobre o objeto a ser segurado é baseado
na resposta do classificador frente à presença de escorregamento. Sua resposta, caso positiva,
implica um ganho incremental fixo da força exercida durante a preensão. Ao final, é analisada
a performance do sistema online de controle da preensão para diferentes objetos e observa-se
que o sistema é capaz de manter a preensão estável para objetos com maior massa e rigidez
elevada",'EDUFU - Editora da Universidade Federal de Uberlandia',"Real-time slip detection to control a robotic grasp, using Machine Learning",10.14393/ufu.di.2020.231.,,,core
344929710,2020-08-01T00:00:00,"Exploiting available condition monitoring data of industrial machines for intelligent maintenance purposes has been attracting attention in various application fields. Machine learning algorithms for fault detection, diagnosis and prognosis are popular and easily accessible. However, our experience in working at the intersection of academia and industry showed that the major challenges of building an end-to-end system in a real-world industrial setting go beyond the design of machine learning algorithms. One of the major challenges is the design of an end-to-end data management solution that is able to efficiently store and process large amounts of heterogeneous data streams resulting from a variety of physical machines. In this paper we present the design of an end-to-end Big Data architecture that enables intelligent maintenance in a real-world industrial setting. In particular, we will discuss various physical design choices for optimizing high-dimensional queries, such as partitioning and Z-ordering, that serve as the basis for health analytics. Finally, we describe a concrete fault detection use case with two different health monitoring algorithms based on machine learning and classical statistics and discuss their advantages and disadvantages. The paper covers some of the most important aspects of the practical implementation of such an end-to-end solution and demonstrates the challenges and their mitigation for the specific application of laser cutting machines",'Springer Science and Business Media LLC',Big data architecture for intelligent maintenance : a focus on query processing and machine learning algorithms,10.1186/s40537-020-00340-7,https://core.ac.uk/download/344929710.pdf,,core
326703392,2020-06-24T00:00:00,"5G and AI (Artificial Intelligence) are changing industrial production and offer great potential for manufacturing enterprises. One of the effects resulting from the increasing quantity of production data is the increasing demands of transmission of large amounts of data, fast transmission speed, and rapid data analysis. However, merely relying on traditional communication technology and manual data processing does not lead to high transmission performance and low analysis time. It is essential to integrate 5G and AI technology to flexibly transmit large amounts of data and real-time data. To demonstrate the feasibility and potential of these two technologies, a concept was developed at the Advanced Manufacturing Technology Center (AMTC) at the Tongji University (Shanghai, China) and further implemented in the AMTC learning factory in cooperation with wbk of Karlsruhe Institute of Technology (Karlsruhe, Germany) and Ruhr-University Bochum (Bochum, Germany). This paper presents the learning factory design in detail, describing the concept design, training environment and training phases in the AMTC learning factory. It is followed by a case study consisting of specific examples of 5G and AI, implemented in the AMTC learning factory. The importance of integrated 5G and AI applications is pointed out and discussed",Elsevier,5G and AI technology application in the AMTC learning factory,10.5445/IR/1000120493,https://core.ac.uk/download/326703392.pdf,,core
390009408,2020-02-20T00:00:00,"Methods of machine intelligence with training contribute their specifics to the creation and commissioning of a gaming system.One of the main problems is the need to anticipate the entire set of input situations and possible answers at the time of design and the impossibility of expanding their list withoutretraining. This leads to a narrowing of the possibility of their use in real gaming systems.  The object of research is the processes of creating and training game agents based on the evolutionary approaches of artificial intelligence. The purpose of the work is the development and justification of a formal model of a game agent based on machine learning methods, software implementation of the game process using neural networks and evolutionary optimization methods for multiple generations of game agent populations. To achieve this goal, the theoretical base, the existing research and development in the industry; a game scenario was designed, the main agents were identified, their main capabilities and expected behavior; training of game agents by various types of neural networks; development testing, quality assessment of behavior and decision-making by artificial intelligence using neural networks were performed; a comparative analysis of various types of neural networks, the proposed recommendations for their use for given conditions. To display the artificial neural network, the template components of the layer, neuron, and bridge were used. The created software module will allow game agents built using various neural networks to compete with each other (and not with a person, as in the normal game mode), and will reveal more prepared ones.  The scientific novelty of the study lies in the fact that a model of a game agent is formalized, based on machine learning methods. The results obtained in this work can be used in the development of video games built on the basis of artificial intelligence of game agents based on machine learning methods, as well as in other scientific studies.Методы машинного интеллекта с обучением вносят свою специфику к созданию и наладке игровой системы.Одна из главных проблем — это необходимость предвидения всего набора входных ситуаций и возможных ответов на моменте проектирования и невозможность расширения их списка без переобучения. Это приводит к сужению возможности их использования в реальных игровых системах.Объектом исследования являются процессы создания и обучения игровых агентов на основе эволюционных подходов искусственного интеллекта. Цель работы - разработка и обоснование формальной модели игрового агента, основанного на методах машинного обучения, программная реализация игрового процесса сприменением нейронных сетей и эволюционных методов оптимизации при множественных поколениях популяций игровых агентов.Для осуществления поставленной цели исследована теоретическая база, существующие исследования и разработки в отрасли; спроектирован игровой сценарий, определены основные агенты, их основные возможности и ожидаемое поведение;реализовано обучение игровых агентов различными типами нейронных сетей; выполнено тестирование разработок, оценка качества поведения и принятия решений искусственныминтеллектом с использованием нейронных сетей; проведен сравнительный анализ различных типов нейронных сетей, предлагаемых рекомендаций по их использованию для заданныхусловий. Для отображения искусственной нейронной сети использованы шаблонные компоненты слоя, нейрона и моста.Созданный программный модуль позволит игровым агентам, построенным с использованием различных нейронных сетей, соперничать друг с другом (а не с человеком,как в обычном режиме игры), позволит выявить более подготовленного из них. Научная новизна исследования заключается в том, что формализована модель игрового агента, в основе которой методы машинного обучения. Полученные в работе результаты могут быть использованы при разработке видеоигр, построенных на базе искусственного интеллекта игровых агентов, в основекоторых методы машинного обучения, а также в рамках других научных исследований.Методи машинного інтелекту з навчанням привносять свою специфіку до створення і налагодження ігрової системи.  Одна з головних проблем - це необхідність передбачення усього набору вхідних ситуацій і можливих відповідей на моменті проектування і неможливість розширення їх списку без перенавчання. Це призводить до звуження можливості їх використання у реальних ігрових системах. Об'єктом дослідження є процеси створення і навчання ігрових агентів на основі еволюційних підходів штучного інтелекту. Мета роботи - розробка і обґрунтування формальної моделі ігрового агенту, заснованої на методах машинного навчання, програмна реалізація ігрового процесу з застосуванням нейронних мереж та еволюційних методів оптимізації при множинних генераціях популяцій ігрових агентів. Для здійснення поставленої мети досліджена теоретична база, існуюча дослідження та розробки в галузі; спроектовано ігровий сценарій, визначені основні агенти, їх основні можливості та очікувана поведінка; реалізоване навчання ігрових агентів різними типами нейронних мереж; виконане тестування розробок, оцінка якості поведінки й прийняття рішень штучним інтелектом з використанням нейронних мереж; проведений порівняльний аналіз різних типів нейронних мереж, запропонування рекомендацій щодо їх використання для заданих умов. Для відображення штучної нейронної мережі використані шаблоні компоненти слою, нейрону і мосту. Створений програмний модуль дозволить ігровим агентам, побудованим з використанням різних нейронних мереж, суперничати один з іншим (а не з людиною, як у звичайному режимі гри), дозволить виявити більш підготовленого з них. Наукова новизна одержаних результатів полягає у тому, що формалізовано модель ігрового агенту, в основі якої методи машинного навчання. Отримані в роботі результати можуть бути використані при розробці відеоігор, побудованих на базі штучного інтелекту ігрових агентів, що гуртуються на методах машинного навчання, а також в рамках інших наукових досліджень",ДВНЗ «Приазовський державний технічний університет»,ДОСЛІДЖЕННЯ ТА МОДЕЛЮВАННЯ ІГРОВОГО ПРОЦЕСУ НА БАЗІ МЕТОДІВ МАШИННОГО НАВЧАННЯ,,,,core
385429504,2020-08-01T00:00:00,"Exploiting available condition monitoring data of industrial machines for intelligent maintenance purposes has been attracting attention in various application fields. Machine learning algorithms for fault detection, diagnosis and prognosis are popular and easily accessible. However, our experience in working at the intersection of academia and industry showed that the major challenges of building an end-to-end system in a real-world industrial setting go beyond the design of machine learning algorithms. One of the major challenges is the design of an end-to-end data management solution that is able to efficiently store and process large amounts of heterogeneous data streams resulting from a variety of physical machines. In this paper we present the design of an end-to-end Big Data architecture that enables intelligent maintenance in a real-world industrial setting. In particular, we will discuss various physical design choices for optimizing high-dimensional queries, such as partitioning and Z-ordering, that serve as the basis for health analytics. Finally, we describe a concrete fault detection use case with two different health monitoring algorithms based on machine learning and classical statistics and discuss their advantages and disadvantages. The paper covers some of the most important aspects of the practical implementation of such an end-to-end solution and demonstrates the challenges and their mitigation for the specific application of laser cutting machines",'Springer Science and Business Media LLC',Big data architecture for intelligent maintenance : a focus on query processing and machine learning algorithms,10.1186/s40537-020-00340-7,https://core.ac.uk/download/385429504.pdf,,core
294759563,2020-03-31T00:00:00,"У статті досліджено характерні особливості та розкрито зміст проривних технологій на основі реалізації моделі “інвестиції-вплив”. Проаналізовано показники України за світовим рейтингом цифрової конкурентоспроможності станом на 01.01.2016 р. та 01.01.2019 р.. В структурі показника знання простежується погіршення за інститутом навчання, освіта та наукова концентрація; за показником готовності до майбутнього, ситуація незмінна; показник технології, що розкривається через критерії нормативної бази, капіталу та технологічної основи, те ж є сталим.

Вказано компетенції, якими оволодіває індивідуум в ході навчання і роботи на цифровій платформі інноваційно-підприємницького університету, маючи такі фундаментальні знання: критичне мислення, кмітливість, вибір пріоритетів, фільтрація інформації, робота в команді, системне мислення, спілкування.

Аргументовано, що формування ефективно працюючої цифрової економіки можливе за умов напрацювання урядом інструментів наступних інноваційних досягнень: ріст використання Коботів, входження в промисловість штучного інтелекту, автономних речей, входження у промисловість технологій блокчейн.

Запропоновано авторське осмислення цифрової трансформації економіки України крізь зріз віртуально-реального кубічного простору. Визначено важливу роль становлення цифрової економіки, що формує принципово нові бізнес-моделі та постійно удосконалюється, впроваджуючи хмарні технології, штучний інтелект, нову віртуальну реальність, накопичує величезні обсяги даних (Big Data), які при досягненні критичної маси стають важливим її капіталом.

Авторами висловлено думку про те, що пріоритетними напрямами у розвитку цифрової економіки та інноваційно-підприємницьких університетів в ході становлення Індустрії 4.0 і надалі залишаються роботизація виробничих процесів, штучний інтелект, відцифрування інституту освіти та науки, запровадження цифрових технологій на всіх рівнях економічної агрегації, впровадження у промисловість технології блокчейн.The article investigates features and content of breakthrough technologies based on the implementation of the investment-impact model. Indicators of Ukraine according to global digital competitiveness rating as of 01.01.2016 and 01.01.2019 are analyzed. In the structure of knowledge index, the deterioration of the Institute of Education, Education and Scientific Concentration is traced; in terms of readiness for future, the situation is unchanged; the indicator of technology disclosed through the criteria of regulatory framework, capital and technological basis is same.

The competencies that an individual possesses in the course of training and work on digital platform of innovation-entrepreneurial university are indicated, having the following basic knowledge: critical thinking, ingenuity, choice of priorities, filtering information, teamwork, systemic thinking, communication.

It is argued that formation of an effective digital economy is possible under the conditions of government’s development of instruments of the following innovative achievements: growth of the use of Cobots, entry into the artificial intelligence industry, autonomous things, entry into blockchain technology industry.

Author’s understanding of digital transformation of Ukrainian economy through the section of a virtual-real cubic space is offered. The important role of becoming digital economy is defined, which forms fundamentally new business models and is constantly being improved by introducing cloud technologies, artificial intelligence, new virtual reality, and accumulating huge amounts of data (Big Data), which become critical capital when it reaches critical mass.

Authors argue that the priority areas in the development of digital economy and innovation-entrepreneurial universities in the process of becoming Industry 4.0 continue to be the robotization of production processes, artificial intelligence, digitization of the Institute of Education and Science, the introduction of digital technologies at all levels of economic aggregation, blockchain technology industry",Дніпропетровський державний аграрно-економічний університет,Digital economy and innovation-entrepreneurial university in the light of competitiveness,,https://core.ac.uk/download/294759563.pdf,,core
327015348,2020-01-01T00:00:00,"Recent advancement in predictive machine learning has led to its application in various use cases in manufacturing. Most research focused on maximising predictive accuracy without addressing the uncertainty associated with it. While accuracy is important, focusing primarily on it poses an overfitting danger, exposing manufacturers to risk, ultimately hindering the adoption of these techniques. In this paper, we determine the sources of uncertainty in machine learning and establish the success criteria of a machine learning system to function well under uncertainty in a cyber-physical manufacturing system (CPMS) scenario. Then, we propose a multi-agent system architecture which leverages probabilistic machine learning as a means of achieving such criteria. We propose possible scenarios for which our architecture is useful and discuss future work. Experimentally, we implement Bayesian Neural Networks for multi-tasks classification on a public dataset for the real-time condition monitoring of a hydraulic system and demonstrate the usefulness of the system by evaluating the probability of a prediction being accurate given its uncertainty. We deploy these models using our proposed agent-based framework and integrate web visualisation to demonstrate its real-time feasibility",,Multi agent system for machine learning under uncertainty in cyber physical manufacturing system,,,,core
323179714,2020-01-01T00:00:00,"An important research problem in artificial intelligence is how to organize multiple agents, and coordinate them, so that they can work together to solve problems. Coordinating agents in a multi-agent system can significantly affect the systems performance-the agents can, in many instances, be organized so that they can solve tasks more efficiently, and consequently benefit collectively and individually. Central to this endeavor is coalition formation-the process by which heterogeneous agents organize and form disjoint groups (coalitions). Coalition formation often involves finding a coalition structure (an exhaustive set of disjoint coalitions) that maximizes the systems potential performance (e.g., social welfare) through coalition structure generation. However, coalition structure generation typically has no notion of goals. In cooperative settings, where coordination of multiple coalitions is important, this may generate suboptimal teams for achieving and accomplishing the tasks and goals at hand. With this in mind, we consider simultaneously generating coalitions of agents and assigning the coalitions to independent alternatives (e.g., tasks/goals), and present an anytime algorithm for the simultaneous coalition structure generation and assignment problem. This combinatorial optimization problem hasmany real-world applications, including forming goal-oriented teams. To evaluate the presented algorithms performance, we present five methods for synthetic problem set generation, and benchmark the algorithm against the industry-grade solver CPLEXusing randomized data sets of varying distribution and complexity. To test its anytime-performance, we compare the quality of its interim solutions against those generated by a greedy algorithm and pure random search. Finally, we also apply the algorithm to solve the problem of assigning agents to regions in a major commercial strategy game, and show that it can be used in game-playing to coordinate smaller sets of agents in real-time.Funding Agencies|Linkoping University; Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation</p",'Springer Science and Business Media LLC',An anytime algorithm for optimal simultaneous coalition structure generation and assignment,10.1007/s10458-020-09450-1,,,core
478636725,2020-01-01T00:00:00,": S’il y a bien un domaine où les annonces pleuvent
en matière de développement de l’intelligence artificielle
(IA), c’est le secteur de l’imagerie médicale au sens large
du terme (regroupant la radiologie, la médecine nucléaire
et la radiothérapie). Les applications, encore souvent utilisées dans des niches précises, ont tendance à devenir
beaucoup plus transversales. De multiples acteurs industriels, en partenariat avec les utilisateurs, s’évertuent à
construire de réelles plateformes qui offrent aux cliniciens
une multitude d’applications utilisables pour combler plusieurs types de demandes et besoins (détection, diagnostic et prédiction). Il est indéniable que la capacité de l’IA
dépasse largement nos capacités humaines en matière de
résolution de l’image, de rapidité et d’efficience de lecture
et d’analyse. Une attitude de négation ou de scepticisme
de la part des professionnels du secteur n’est plus de mise.
Ils doivent, sans attendre, collaborer avec les spécialistes
data et les ingénieurs au développement à large échelle
de l’IA en imagerie médicale et ce, au profit des patients
et des payeurs.: Nowadays, we are facing an overwhelming
amount of public announcements concerning the rise of
artificial intelligence (AI) in the world of medical imaging
(including radiology, nuclear medicine and radiotherapy).
While most of the applications are still limited to specific
niches, there is a general trend to build real transversal
platforms. Multiple industrial players, in collaboration with
the clinicians in the field, are striving to build those platforms in order to offer plenty of use cases of AI for several
purposes and needs (screening/detection, diagnosis and
prediction). It is already undeniable that AI far exceeds
human capabilities in terms of resolution, speed of image
analysis and efficiency. Negative attitudes and skepticism
from concerned professionals should be banned. Collaboration with data scientists and engineers for the large
scale development and implementation should be pushed
forward for the benefit of both patients and payers.Peer reviewe",,Medical imaging professionals and related specialties : a questioning is essential!,,https://orbi.uliege.be/bitstream/2268/257226/1/La%20m%c3%a9decine%20du%20futur%20-%20une%20remise%20en%20question%20s%27impose.pdf,,core
387289905,2020-12-07T00:00:00,"Proliferation of 5G devices and services has driven the demand for wide-scale
enhancements ranging from data rate, reliability, and compatibility to sustain
the ever increasing growth of the telecommunication industry. In this regard,
this work investigates how machine learning technology can improve the
performance of 5G cell and beam index search in practice. The cell search is an
essential function for a User Equipment (UE) to be initially associated with a
base station, and is also important to further maintain the wireless
connection. Unlike the former generation cellular systems, the 5G UE faces with
an additional challenge to detect suitable beams as well as the cell identities
in the cell search procedures. Herein, we propose and implement new
channel-learning schemes to enhance the performance of 5G beam index detection.
The salient point lies in the use of machine learning models and softwarization
for practical implementations in a system level. We develop the proposed
channel-learning scheme including algorithmic procedures and corroborative
system structure for efficient beam index detection. We also implement a
real-time operating 5G testbed based on the off-the-shelf Software Defined
Radio (SDR) platform and conduct intensive experiments with commercial 5G base
stations. The experimental results indicate that the proposed channel-learning
schemes outperform the conventional correlation-based scheme in real 5G channel
environments.Comment: submitted to IEEE ACCES",,"Exploitation of Channel-Learning for Enhancing 5G Blind Beam Index
  Detection",,http://arxiv.org/abs/2012.03631,,core
325933976,2020-06-17T00:00:00,"Payment card fraud causes multibillion dollar losses for banks and merchants
worldwide, often fueling complex criminal activities. To address this, many
real-time fraud detection systems use tree-based models, demanding complex
feature engineering systems to efficiently enrich transactions with historical
data while complying with millisecond-level latencies.
  In this work, we do not require those expensive features by using recurrent
neural networks and treating payments as an interleaved sequence, where the
history of each card is an unbounded, irregular sub-sequence. We present a
complete RNN framework to detect fraud in real-time, proposing an efficient ML
pipeline from preprocessing to deployment.
  We show that these feature-free, multi-sequence RNNs outperform
state-of-the-art models saving millions of dollars in fraud detection and using
fewer computational resources.Comment: 9 pages, 4 figures, to appear in SIGKDD'20 Industry Trac",'Association for Computing Machinery (ACM)',Interleaved Sequence RNNs for Fraud Detection,10.1145/3394486.3403361,http://arxiv.org/abs/2002.05988,,core
427635765,2020-06-15T00:00:00,"Part 4: Machine Learning for SystemsInternational audienceThe mobile ecosystem is witnessing an unprecedented increase in the number of malware in the wild. To fight this threat, actors from both research and industry are constantly innovating to bring concrete solutions to improve security and malware protection. Traditional solutions such as signature-based anti viruses have shown their limits in front of massive proliferation of new malware, which are most often only variants specifically designed to bypass signature-based detection. Accordingly, it paves the way to the emergence of new approaches based on Machine Learning (ML) technics to boost the detection of unknown malware variants. Unfortunately, these solutions are most often underexploited due to the time and resource costs required to adequately fine tune machine learning algorithms. In reality, in the Android community, state-of-the-art studies do not focus on model training, and most often go through an empirical study with a manual process to choose the learning strategy, and/or use default values as parameters to configure ML algorithms. However, in the ML domain, it is well known admitted that to solve efficiently a ML problem, the tunability of hyper-parameters is of the utmost importance. Nevertheless, as soon as the targeted ML problem involves a massive amount of data, there is a strong tension between feasibility of exploring all combinations and accuracy. This tension imposes to automate the search for optimal hyper-parameters applied to ML algorithms, that is not anymore possible to achieve manually. To this end, we propose a generic and scalable solution to automatically both configure and evaluate ML algorithms to efficiently detect Android malware detection systems. Our approach is based on devOps principles and a microservice architecture deployed over a set of nodes to scale and exhaustively test a large number of ML algorithms and hyper-parameters combinations. With our approach, we are able to systematically find the best fit to increase up to 11% the accuracy of two state-of-the-art Android malware detection systems",'Springer Science and Business Media LLC',DroidAutoML: A Microservice Architecture to Automate the Evaluation of Android Machine Learning Detection Systems,10.1007/978-3-030-50323-9_10,,,core
475498807,2020-12-01T00:00:00,"RÉSUMÉ: Une des activités centrales de la chaîne logistique est la gestion de l’inventaire puisqu’elle

implique de larges sommes d’argent investies dans du matériel, ce qui a un impact important sur la profitabilité d’une entreprise. La gestion de l’inventaire vise en particulier à déterminer quand et en quelle quantité il faut commander du matériel. La démocratisation de l’accessibilité à des outils de prévision performants, automatisés et en amélioration continue modifie les problèmes importants que les chercheurs et développeurs de systèmes de gestion de l’inventaire doivent surmonter. De plus en plus, ces problèmes se focalisent autour de l’évaluation, du suivi des performances, de la sélection des modèles de prévision et de la prise de décision. Ces sujets sont abordés dans cette thèse qui propose une approche pour la

conception et le développement d’un système de gestion de l’inventaire pour un portefeuille de produits à profil de demande mixte. Les profils de demande sont une caractéristique des séries temporelle et il s’avère que les traitements et méthodes de prévisions à utiliser varient selon le profil de demande des séries. Plusieurs défis et questions demeurent sans réponse en présence de séries à profil mixtes. Notre état de l’art a soulevé quelques-uns des principaux défis. Les métriques de précision sont souvent à la base des décisions de sélection de modèles de prévision pour la gestion de

l’inventaire. Or, comment faire la sélection de modèles lorsque les métriques de performance n’atteignent pas un consensus ? Une méthode pour quantifier la précision et la fiabilité des métriques de performance des modèles de prévision est développée afin de répondre à cette

question. Nos résultats ont permis de mesurer la sensibilité et la fiabilité de plusieurs métriques de performance populaires et donne quelques recommandations sur quelles métriques utiliser selon diérentes circonstances. Avec cette méthode, la sélection de modèles de prévision devient plus aisée. Cependant, il

n’existe toujours pas de consensus sur le lien qui existe entre la précision des modèles de prévision et les performances associées en gestion d’inventaire. La littérature ne présente pas non plus de consensus sur l’impact de la sélection multiple et les études menées jusqu’à maintenant sont basées sur la précision des résultats plutôt que sur leur utilité en mesurant les performances en gestion d’inventaire. Des connaissances additionnelles sont requises et sont fournies par nos travaux de recherche. Nos résultats ont montré que la sélection multiple donne de meilleurs résultats que la sélection globale. Les résultats ont également montré que

la sélection individuelle basée sur des métriques de précision permet d’obtenir des résultats en inventaire performants comparables à une sélection basée sur les coûts. Toutefois, le lien entre la précision et la sélection globale demeure à éclaircir. Finalement, la prise de décision de réapprovisionnement demeure un aspect clé dont l’optimisation peut avoir un impact significatif sur la profitabilité d’une entreprise. Or, l’impact des politiques de réapprovisionnement dynamique demeure peu étudié. On propose donc d’étudier ce facteur tout en proposant une méthodologie pour optimiser une politique dynamique basée sur le cadre conceptuel de l’apprentissage par imitation. Ce cadre conceptuel permet

de tirer parti de l’apprentissage automatique, une méthode ayant connu un important succès dans divers domaines d’application, pour la résolution de problème de décision. Les résultats ont montré une amélioration importante des performances en gestion d’inventaire en utilisant une politique d’approvisionnement basée sur l’apprentissage par imitation versus une politique dynamique (st, Q) ou statique (s, Q) classique. Les résultats ont également montré que la méthode proposée permet de générer des politiques plus robustes aux changements de performance qu’un modèle de prévision de la demande. La thèse dans son ensemble fournit plusieurs recommandations et méthodologies pour faire la

conception d’un système de gestion de l’inventaire. Une méthodologie pour mener un tel système vers l’autonomie est également présentée. Les résultats cumulés des trois contributions ont ainsi permis d’accumuler de nouvelles connaissances sur le domaine et de proposer de nouvelles méthodes pour la résolution de problèmes d’inventaire. L’accumulation des résultats sur la relation complexe entre les performances en inventaire et la précision des prévisions de la demande a mené à l’explication de la nature de la complexité observée. Ces résultats pourront ainsi conduire au développement de nouvelles métriques de performance basées sur l’erreur et fortement corrélées avec les performances d’inventaire. Ceci permettra de faire la sélection et l’optimisation des paramètres de modèles de prévision de la demande pour la gestion d’inventaire sans avoir recours à la simulation. Les méthodologies proposées ont été validées à l’aide de données réelles provenant de notre partenaire industriel. Les conclusions, outils et méthodes développées dans cette recherche sont en cours d’implantation chez le partenaire industriel à des fins d’utilisation en production.----------ABSTRACT: One of the central activities of the logistics chain is inventory management since it involves large sums of money invested in stock, which has a significant impact on a company’s profitability. Inventory management is concerned with determining when and in what quantity to order material. The democratization of the accessibility to high-performance automated and continuously improving forecasting tools is changing the important problems that researchers and developers of inventory management systems must overcome. Increasingly, these issues focus around evaluation, performance monitoring, selection of forecasting models, and decision-making. These topics are addressed in this thesis which proposes an approach for

the design and development of an inventory management system for a portfolio of products with a mixed demand profile. Demand profiles are a characteristic of time series and it turns out that the treatments and forecasting methods to be used vary depending on the demand profile of the series. Several challenges and questions remain unanswered in the presence of series with mixed profiles. Our state of the art has raised some of the main challenges. Precision metrics are often

the basis for decisions on the selection of forecasting models for inventory management. However, how do you select models when performance metrics do not reach consensus? A method to quantify the precision and reliability of performance metrics of forecasting models

is developed to answer this question. Our results have measured the sensitivity and reliability of several popular performance metrics and make some recommendations on which metrics to use under dierent circumstances.

With this method, the selection of forecast models becomes easier. However, there is still no consensus on the relation between the accuracy of forecasting models and the associated performance in inventory management. There is also no consensus in the literature on the

impact of multiple selection and the studies conducted to date are based on the accuracy of the results rather than their usefulness in measuring inventory management performance. Additional knowledge is required and is provided by our research. Our results showed that

multiple selection gives better results than overall selection. The results also showed that individual selection based on precision metrics achieves ecient inventory results comparable to selection based on cost. However, the relation between precision and overall selection remains to be clarified. Finally, the decision-making of replenishment remains a key aspect whose optimization can have a significant impact on the profitability of a company. However, the impact of dynamic replenishment policies remains little studied. We therefore propose to study this factor while proposing a methodology to optimize a dynamic policy based on the conceptual framework Of learning by imitation. This conceptual framework makes it possible to take advantage of machine learning, a method that has been very successful in various application areas, for decision problems. The results showed a significant improvement in inventory management performance using an inventory policy based on imitation learning versus a dynamic (st, Q) or static (s, Q) policy. The results also showed that the proposed method makes it possible to generate policies that are more robust to changes in performance of a demand forecasting model. The thesis provides several recommendations and methodologies for making the design of an inventory management system. A methodology for leading such a system towards autonomy is also presented. The cumulative results of the three contributions have thus made it possible to accumulate new knowledge in the field and to propose new methods for solving inventory problems. The accumulation of results on the complex relationship between inventory performance and the accuracy of demand forecasts has led to the explanation of the nature of the observed complexity. These results may thus lead to the development of new

performance metrics based on error and strongly correlated with inventory performance. This will allow selection and optimization of demand forecasting model parameters for inventory management without resorting to simulations. The proposed methodologies have been validated using real data from our industrial partner.

The conclusions, tools and methods developed in this research are being implemented by the industrial partner for use in production",,Conception d’un système de gestion de l’inventaire pour un portefeuille de produits à profil de demande mixte,,,,core
334912309,2020-04-26T00:00:00,"In addition to their security properties, adversarial machine-learning
attacks and defenses have political dimensions. They enable or foreclose
certain options for both the subjects of the machine learning systems and for
those who deploy them, creating risks for civil liberties and human rights. In
this paper, we draw on insights from science and technology studies,
anthropology, and human rights literature, to inform how defenses against
adversarial attacks can be used to suppress dissent and limit attempts to
investigate machine learning systems. To make this concrete, we use real-world
examples of how attacks such as perturbation, model inversion, or membership
inference can be used for socially desirable ends. Although the predictions of
this analysis may seem dire, there is hope. Efforts to address human rights
concerns in the commercial spyware industry provide guidance for similar
measures to ensure ML systems serve democratic, not authoritarian endsComment: Authors ordered alphabetically; 4 page",,Politics of Adversarial Machine Learning,,http://arxiv.org/abs/2002.05648,,core
334927025,2020-03-29T00:00:00,"This paper introduces the MIP Platform architecture model, a novel AI-based
cognitive computing platform architecture. The goal of the proposed application
of MIP is to reduce the implementation burden for the usage of AI algorithms
applied to cognitive computing and fluent HMI interactions within the
manufacturing process in a cyber-physical production system. The cognitive
inferencing engine of MIP is a deterministic cognitive module that processes
declarative goals, identifies Intents and Entities, selects suitable actions
and associated algorithms, and invokes for the execution a processing logic
(Function) configured in the internal Function-as-aService or Connectivity
Engine. Constant observation and evaluation against performance criteria assess
the performance of Lambda(s) for many and varying scenarios. The modular design
with well-defined interfaces enables the reusability and extensibility of FaaS
components. An integrated BigData platform implements this modular design
supported by technologies such as Docker, Kubernetes for virtualization and
orchestration of the individual components and their communication. The
implementation of the architecture is evaluated using a real-world use case
later discussed in this paper.Comment: 7 page",,"MIP An AI Distributed Architectural Model to Introduce Cognitive
  computing capabilities in Cyber Physical Systems (CPS)",,http://arxiv.org/abs/2003.13174,,core
357291093,2020-04-02T00:00:00,". Abstract Design for manufacturing is often difficult for mechanical parts since significant manufacturing knowledge is required to adjust part designs for manufacturability. The traditional trial and error approach usually leads to expensive iterations and compromises the quality of the final design. The authors believe the appropriate way to handle product design for manufacturing problems is not to formulate a large design problem that exhaustively incorporates design and manufacturing issues, but to separate the design and manufacturing activities and provide support for collaboration between engineering teams. In this paper, the Collaborative Multidisciplinary Decision-making Methodology (CMDM) is used to solve a product design and manufacturing problem. First, the compromise Decision Support Problem is used as a mathematical model of each engineering teams&apos; design decisions and as a medium for information exchange. Second, game theoretic principles are employed to resolve couplings or interactions between the teams&apos; decisions. Third, design capability indices are used to maintain design freedom at the early stages of product realization in order to accommodate unexpected downstream design changes. A plastic robot arm design and manufacturing scenario is presented to demonstrate the application of this methodology and its effectiveness for solving a complex design for manufacturing problem in a streamlined manner, with minimal expensive iterations. Keywords: Collaborative Design, Design for Manufacturing, Game Theory, and Multidisciplinary Decision Making 2 Design for manufacturing Concurrent engineering involves separating product realization activities so that design activities can be executed independently while simultaneously incorporating relevant information from downstream domains such as manufacturing, assembly, or recycling  Alternatively, a manufacturing team that understands the purpose of a design and its functional requirements may be more capable of adjusting it to facilitate manufacturing. The authors believe the appropriate way to handle complex product design problems such as DfM is not to formulate large design problems but to support cooperation and collaboration between multidisciplinary engineering teams. Towards this end, the Collaborative Multidisciplinary Decision-making Methodology (CMDM) is established (Xiao et al. 2005)  to enable collaborative decision making between design and manufacturing teams through &apos;collaboration by separation&apos;. Separation signifies that the responsibility for DfM is transferred from the design team to the downstream manufacturing team; whereas, collaboration signifies that satisfactory systems-level solutions are coordinated with minimal information exchange and iteration. Unlike many mathematical multidisciplinary optimization (MDO) approaches (Balling and SobieszczanskiSobieski 1996 1. Exchanging Information. The information required for decision-making in an activity must be transferred completely from one team to another, and the recipient teams should be able to understand the team&apos;s intentions without requiring additional information flows or causing iterations. The compromise Decision Support Problem (DSP)  2. Accommodating interactions between activities. Some activities in a DfM process may be coupled, such that each design team makes decisions that affect the decisions of other teams. Game theory is used in the CMDM to model different degrees of collaboration and manage interactions between engineering teams, with little or no expensive, systems-level iterations. 3. Maintaining feasible and satisfactory overall designs. When design activities are separated, design teams must make decisions without full knowledge of their impact on downstream activities. If single point solutions are exchanged, downstream designers are prevented from adjusting designs for feasibility or satisfactory local performance, and iterations often ensue. With set-based approaches, however, ranges or sets of solutions are shared and gradually narrowed during the design process, thereby reducing or eliminating the need for global, systems-level iterations  In a product realization problem, the dependencies between any two activities, such as designmanufacturing, design-design, or manufacturing-manufacturing, may be interactive or sequential. Game theory is used to resolve the interactive couplings and design capability indices are used to handle the sequential relationships. The authors believe the sequential relationships are more significant in DfM problems, mostly due to the upstream/downstream nature of the designmanufacturing relationship. As shown in  &lt;Figure 1 goes about here&gt; Collaborative multidisciplinary decision making methodology The CMDM is implemented in three steps: Step 1 Representing decision making information in a compromise DSP which serves as an information medium to eliminate iterations caused by information exchange and communication; Step 2 Representing cooperation styles among engineering teams with game theoretic protocols to eliminate iterations caused by interdisciplinary interactions; and Step 3 Reformulating the compromise DSPs using design capability indices for finding superior ranged set of solutions that eliminate or reduce costly iterations caused by unexpected downstream requirements and constraints. Step 1, modeling product realization activities using compromise DSPs In order to resolve the first challenge, that of exchanging information, Section 1, a compromise Decision Support Problem, DSP, is used. A compromise DSP is a multi-objective decision modela hybrid formulation based on mathematical programming and goal programming -that is used to find the values of design variables that satisfy a set of constraints and achieve a set of conflicting goals as closely as possible  For a given product realization activity, a compromise DSP is capable of representing a team&apos;s decision-making knowledge, as well as the design rationale underlying its decision. A team&apos;s decision is represented with a feasible design space, a set of design objectives, and a tradeoff strategy between these design objectives. As shown in  The compromise DSP resolves the first challenge of exchanging information, but it does not address the second challenge of enabling the separation of activities. There are three possible relationships between any two compromise DSPs; they may be solved concurrently, sequentially, or as coupled problems. Given the disk brakes in a passenger vehicle as an example, there is no direct information exchange between the brake design and exhaust system design. From a decisionmaking perspective, the two compromise DSPs (brake and exhaust system) do not share any unknown variables. They can be solved concurrently, and the solution remains the same regardless of the teams&apos; cooperation styles. Meanwhile, the brake pad cannot be designed without knowledge of the rotor design team&apos;s results, whereas the rotor has to be designed with knowledge of the geometric shape, surface finish, and other details of the brake pad. This situation is reflected as shared variables between the rotor and brake pad design compromise DSPs. Neither compromise 6 DSP can be solved independently, and the result is always affected by the two design teams&apos; cooperation styles, namely, which team solves its compromise DSP first. Game theoretic protocols are used to address the second challenge of separating the coupled activities. In addition, a manufacturing team must design the fixtures, determining the processing parameters based on the final rotor and brake pad designs. The manufacturing compromise DSP includes variables that are determined only by solving the design compromise DSPs. This is a sequential process and the teams&apos; cooperation styles do not affect the solution. However, the downstream manufacturing team may need to modify the design, causing potential iterations. Design capability indices are used to address this third challenge. Since design and manufacturing activities are separated and the responsibility for DfM is transferred from the upstream design team to the downstream manufacturing team, the third challenge becomes more significant in this study. Step 2, representing cooperation styles using game theory The second challenge is resolving couplings between activities. Traditionally, a trial and error approach is used to solve coupled compromise DSPs. Since a team has to make assumptions about another team&apos;s decisions to initiate the trial and error process, this traditional approach may not guarantee consensus (convergence) and usually fails to achieve superior results. Game theory facilitates interaction among multiple engineers without integrating a product realization process into a single large optimization problem or causing iterations. There are three game protocols representing different types of interactions between teams (or players in game theory terminology): cooperative, noncooperative, and leader/follower. Rao and colleagues  In a leader/follower game, the leader makes a set of rational decisions by predicting the A is a subset of X A which must be determined using information from team B, and x B B must be determined using  The feasibility of the solution is ensured by using a BRC to predict the follower&apos;s behavior. Generally, a leader-follower game protocol facilitates collaborative decision making without requiring iteration, hence the coupled activities can be accomplished separately. This solves the second challenge. Step 3, maintaining design freedom using design capability indices The third challenge is to eliminate, or at least reduce, costly iterations between upstream and downstream activities by having the upstream team identify ranges of design variables, rather than single point values, that are as broad as possible without deviating from a desired range of 8 performance, as shown in  &lt;Figure 3 goes about here&gt; As presented in  In the design variable set, X, if any design variable is discrete, say x j , the location and deviation of the performance measures have to be conservatively estimated using: In many cases, calculating the min/max values of a performance measure requires exhaustive search, but the performance range estimated in this manner will cover all the possible values even though they are not continuous. If a performance variable is discrete, the design capability indices are not applicable. Given that all the performance variables are continuous, design capability indices are embedded into the compromise DSP by formulating the design goals using C dk , adding constraints C dk ≥1, and formulating the deviation function to maximize the overachievements of C dk . Moreover, the constraints are re-formulated using Equation  Clearly, constraint g k (X) must be differentiable. If any design variable is discrete, the constraints can be calculated using Equation (4). The bounds of design variables are still formulated using constant values. The resulting compromise DSP is shown in  &lt;Figure 4 goes about here&gt; In  Step 1, the challenge is to provide a method for exchanging information, and it is solved by representing the decision making information in compromise DSPs. In Step 2, the challenge is accommodating interactions between activities, and it is addressed with game theory. In Step 3, the challenge is to maintain feasible and satisfactory overall designs, and it is addressed by reformulating the compromise DSPs using design capability indices. The CMDM provides a normative framework that facilitates collaborative product realization by separating the decision making activities. A robot arm design and manufacturing scenario The authors have developed a distributed product realization environment called the Rapid Tooling  &lt;Figure 5 goes about here&gt; A design team, a rapid tooling team, and an injection molding team are assigned to this task. Correspondingly, the product realization process is partitioned into three activities, as shown in  In this scenario, DfM includes not only adjusting the geometric shape, but also the entire rapid tooling activity. Designing the mold pairs requires knowledge about rapid tooling; hence, it is very difficult for the design team. Since the robot arm is designed without knowledge of the downstream manufacturing process, the design team will have to modify its design based on feedback from manufacturing experts. For a simple product realization process like this one, it is possible to collect all the manufacturing related information and formulate a large design problem to solve all the design variables, i.e., robot arm geometry shape, mold geometry shape, and some manufacturing parameters, like that in Concurrent Engineering. For complex real-world problems, this implies a design problem containing large numbers of design variables and complex analyses; therefore it is not practical to solve as a single problem. The traditional approach to solving a DfM problem is a trial and error approach, which will cause extensive information exchange and iteration. In this example, all three of the challenges addressed by the CMDM exist. Information exchange is required between these activities (challenge 1). Rapid tooling and injection molding are coupled activities; thus iterations exist between them (challenge 2). The geometric shape of the 12 robot arm may have to be modified in order to fabricate the batch with given time and cost; this forces the upstream design team to redesign the geometry (challenge 3). Engineering teams&apos; compromise DSPs The first step of the CMDM is modeling each activity as a standard compromise DSP as shown in  Three design goals are determined based on the customer requirements: (i) the maximum deformation under working load should be as close to 0.5mm as possible, (ii) the maximum von Mises stress under working load should be as close to 6MPa as possible, and (iii) the weight of the robot arm should be as close to 3.5g as possible. The design compromise DSP is shown in  Please note at this step, no design capability indices or game protocol is yet involved. Mathematical equations for deformation, stress, and weight can be found in (Xiao 2003). &lt;Figure 6 goes about here&gt; The rapid tooling team designs the injection mold halves,  On the other hand, the mold life determines the number of mold halves, N m , that must be built in the rapid tooling activity, which thus affects all of the process parameters in this activity. Therefore, injection molding and rapid tooling are coupled activities. As shown in  If we combine all compromise DSPs into one problem with a variable set that includes all of the variables of design, rapid tooling, and injection molding and an objective set that includes all of the objectives with the same weights, the results are shown in  = 9.25mm, t = 3.10mm. In this case, all design goals achieve their target values and the overall deviation is 0. Then, the rapid tooling and injection molding teams make decisions based on this result. Since these two activities are coupled, the rapid tooling team assumes a value of ML, and expects to acquire converged results after several iterations. Unfortunately in this case, rapid tooling and injection molding teams&apos; solutions do not converge. The reason is the design team makes decisions only considering its own design goals; hence the thickness of robot arm t is too large and the mold life becomes so short that the rapid tooling team must build 10 pairs of molds. This violates the constraints of time and cost. Therefore, the product design must be modified. For simplicity, the intermediate results are not listed here. After several rounds of iterations, the converged results from the traditional approach are as shown in  In the trial and error process, iterations happen not only between the coupled rapid tooling and injection molding activities, but also with the upstream design activity. Furthermore, the number and styles of iterations are affected by some unpredictable or uncontrollable factors, such as the teams&apos; experience, and the initial values the teams choose to start the iterative process. So far, this case has demonstrated the difficulties of DfM, and why the traditional approach cannot guarantee the superiority of the final result. By using the CMDM to separate the activities, we expect to change the process shown in  &lt;Table 1 goes about here&gt; &lt;Table 2 goes about here&gt; Resolving couplings using Leader/Follower protocol Since the design team&apos;s decision is not coupled with the decisions of either of the manufacturing teams, as shown in  when LT = 2 mils (7) ML=1696. 10-135.46d-311.63t+423.03Θ +61.17(d-8.11  et al. 1996), which is fundamentally different from using them to approximate the BRCs. Beyond predicting a player&apos;s behavior using its BRC, game protocols also govern issues such as the sequence of the players&apos; decision making activities and control over specific variables. All of these factors are determined by the players&apos; cooperation styles. If the injection molding team is selected as the leader, the BRC T is: When 150&gt; ML ≥75, two pairs of molds have to be build, thus LT = 8 mils has to be selected in order to meet the time constraint. Here, we do not consider the situation that several pairs of molds can be built simultaneously in an SLA3500 machine. It can also be observed from BRC T that Θ remains 0 because the rapid tooling team does not know how the draft angle will affect mold life, and strives to reduce surface roughness, SF, of the robot arm which is achieved with Θ = 0. Obviously, the injection molding team will be unable to eject the parts with a zero draft angle. This is the main reason why the traditional trial and error approach does not converge between the rapid tooling and injection molding teams. Compromise DSP for a ranged set of decisions As described in Section 1, the third step is reformulating the compromise DSPs using design capability indices. In the design activity, all the design variables are continuous; therefore the 16 locations and deviations of the performance variables are calculated using Equation (2). The design team&apos;s compromise DSP for a ranged set of decisions is shown in  &lt;Table 5 goes about here&gt; &lt;Figure 14 goes about here&gt; In  The CMDM is especially useful in the early stages of product design, when little is known about the product and approximating a set of correct designs is much more efficient than conducting rounds and rounds of guessing and correcting. The advantage of the CMDM in DfM is that design and manufacturing activities are separated systematically; hence the product realization activities are accomplished in a more streamlined process as shown in  18 Design freedom in the process The reason the CMDM results in more superior results than the traditional trial and error approach is the design freedom in the product realization process. A design freedom metric is presented in  where n is the number of performance measures of the system. For the i th performance measure, TR i is the target range, PR i is the feasible performance range, and PR i,initial is the initial feasible performance range. In  At the second row, the performance ranges and design freedom at the initial state of this product realization process are listed. The initial design freedom is 0.734, which is smaller than 1 due to the natural limitation of this process and the couplings between activities. In the trial and error process, when the design team makes a specific decision, design freedom is quantified as 0.368, shown at the third row of  Step 3. The injection molding team finally makes its decision at Step 4. It is observed that when the design team makes a ranged set of decisions, design freedom is 0.504 at Step 2. At this moment of the product realization process, the rapid tooling and injection molding teams can  Closure In this paper, the idea of collaboration by separation is tested in product DfM problems. The CMDM is used to enable the separation without causing costly information exchange and iterations. Generally, the compromise DSP is used as an information medium to separate the activities at the information communication level, game theoretical principles separate the coupled activities, and design capability indices separate upstream and downstream activities. The robot arm design and manufacturing process demonstrates that by using the CMDM, a complex product realization process is implemented in a streamlined manner, with each engineering team focusing on its areas of expertise. With the CMDM, final results are obtained with fewer iterations between design teams and significantly less deviation from target performance, relative to using the traditional trial and error approach",,Sobieszczanski-Sobieski and Haftka,,,,core
344912436,2020-01-01T00:00:00,"Abstract

Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing",'Institute of Electrical and Electronics Engineers (IEEE)',A joint decentralized federated learning and communications framework for industrial networks,,,,core
287721910,2020-02-17T00:00:00,"In the era of the fourth industrial revolution (Industry 4.0), many Management Information Systems (MIS) integrate real-time data collection and use technologies such as big data, machine learning, and cloud computing, to foster a wide range of creative innovations, business improvements, and new business models and processes. However, the integration of blockchain with MIS offers the blockchain trilemma of security, decentralisation and scalability. MIS are usually Web 2.0 clientserver applications that include the front end web systems and back end databases; while blockchain systems are Web 3.0 decentralised applications. MIS are usually private systems that a single party controls and manages; while blockchain systems are usually public, and any party can join and participate. This paper clariﬁes the key concepts and illustrates with ﬁgures, the implementation of public, private and consortium blockchains on the Ethereum platform. Ultimately, the paper presents a framework for building a private blockchain system on the public Ethereum blockchain. Then,integrating the Web 2.0 client-server applications that are commonly used in MIS with Web 3.0 decentralised blockchain applications",'Institute of Electrical and Electronics Engineers (IEEE)',Integration of blockchains with management information systems,10.1109/MoRSE48060.2019.8998694,https://core.ac.uk/download/287721910.pdf,,core
427073590,2020-11-04T08:00:00,"In the industrial field, the need has arisen to use more efficient and robust controllers using artificial intelligence techniques that optimize the operation of processes within the industry. In this way, the need arises to employ adaptive controllers such as the BFOA and implement it in real systems in which its functionality can be analyzed. This article presents the implementation and analysis in a fully instrumented functional prototype with industrial sensors. The work methodology is documented from the acquisition of the physical variables through the OPC client-server communication; the synchronization of the excitation of the input variable (variable speed drive) and obtaining the evolution of the flow in time; with the experimental data, the identification methodology by relative least squares was used to obtain the transfer function. Later, the BFOA algorithm was implemented to adjust the constants of a PI controller (Kp and Ki) and analyze the response through simulation using Matlab software, in which satisfactory results were observed based on the analysis of response to disturbances and as an end final part, the controller and the BFOA algorithm were implemented in a PLC-S7-1500 controller in SCL language, and the functionality was validated with the functional prototype, changing the flow setpoints at certain times, observing a behavior according to the simulations carried out. with a minimum overshoot of approximately 5 % and an establishment time of 20s",Ciencia Unisalle,PI tuning based on Bacterial Foraging Algorithm for flow control,,,,core
326250378,2020-12-15T00:00:00,"In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem",'Organisation for Economic Co-Operation and Development  (OECD)',Cost-sensitive learning classification strategy for predicting product failures,10.17863/CAM.53829,,,core
297704935,2020-01-01T00:00:00,"Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the exponential Monte Carlo with counter (EMCQ). Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not (historically) be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to show the degree of interaction required to cover the code as well as finding faults. We applied Q-EMCQ on 37 real-world industrial programs written in Function Block Diagram (FBD) language, which is used for developing a train control management system at Bombardier Transportation Sweden AB. The results show that Q-EMCQ is an efficient technique for test case generation. Addition- ally, unlike the t-wise test suite generation, which deals with the minimization problem, we have also subjected Q-EMCQ to a maximization problem involving the general module clustering to demonstrate the effectiveness of our approach. The results show the Q-EMCQ is also capable of outperforming the original EMCQ as well as several recent meta/hyper-heuristic including modified choice function, Tabu high-level hyper-heuristic, teaching learning-based optimization, sine cosine algorithm, and symbiotic optimization search in clustering quality within comparable execution time",'Springer Science and Business Media LLC',An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications,10.1007/s00500-020-04769-z,,,core
345160048,2020-01-01T00:00:00,"Background: Eggs have acquired a greater importance as an inexpensive and high-quality protein. The Brazilian egg industry has been characterized by a constant production expansion in the last decade, increasing the number of housed animals and facilitating the spread of many diseases. In order to reduce the sanitary and financial risks, decisions regard¬ing the production and the health status of the flock must be made based on objective criteria. The use of Artificial Neural Networks (ANN) is a valuable tool to reduce the subjectivity of the analysis. In this context, the aim of this study was at validating the ANNs as viable tool to be employed in the prediction and management of commercial egg production flocks. Materials, Methods & Results: Data from 42 flocks of commercial layer hens from a poultry company were selected. The data refer to the period between 2010 and 2018 and it represents a total of 600,000 layers. Six parameters were selected as “output” data (number of dead birds per week, feed consumption, number of eggs, weekly weight, weekly egg produc¬tion and flock uniformity) and a total of 13 parameters were selected as “input” data (flock age, flock identification, total hens in the flock, weekly weight, flock uniformity, lineage, weekly mortality, absolute number of dead birds, eggs/hen, weekly egg production, feed consumption, flock location, creation phase). ANNs were elaborated by software programs NeuroShell Predictor and NeuroShell Classifier. The programs identified input variables for the assembly of the networks seeking the prediction of the variables called outgoing that are subsequently validated. This validation goes through the comparison between the predictions and the real data present in the database that was the basis for the work. Validation of each ANN is expressed by the specific statistical parameters multiple determination (R2) and Mean Squared Error (MSE). For instance, R2 above 0.70 expresses a good validation. ANN developed for the output variable “number of dead birds per week” presented R2= 0.9533 and MSE= 256.88. For “feed consumption”, the results were R2= 0.7382 and MSE= 274.56. For “number of eggs (eggs/hen)”, the results were R2= 0.9901 and MSE= 172.26. For “weekly weight”, R2= 0.9712 and MSE= 11154.41. For “weekly egg production”, R2= 0.8015 and MSE= 72.60. For “flock uniformity”, R2= -2.9955 and MSE= 431.82. Discussion: From the six ANN designed in this study, in five it was possible to validate the predictions by comparing predictions with the real data. In one output parameter (“flock uniformity”), it was not possible to have adequate validation due to insufficient data in our database. For “number of dead birds per week”, “feed consumption”, “weekly weight” and “uniformity”, the most important variable was “flock age” (27.5%, 52.5%, 55.2% and 37.9%, respectively). For “number of eggs (eggs/hen)”, “uniformity” (52.1%) was the most relevant variable for prediction. For “weekly egg production”, “flock age” and “number of eggs (eggs/hen)” were the most important zootechnical parameters, both with a relative contribution of 38.2%. The results showed that even with the use of a robust tool such as ANNs, it is necessary to have well-noted and clear information that expresses the reality of the flocks. In any case, the results presented allow us to state that ANNs are capable for the management of data generated in a commercial egg production facility. The process of evaluation of these data would be improved if ANNs were routinely used by the professionals linked to this activity",,Artificial neural networks on eggs production data management,,,"[{'title': None, 'identifiers': ['1678-0345', 'issn:1678-0345']}]",core
357544706,2020-04-23T00:00:00,"Abstract Games technology has undergone tremendous development. In this article, the authors report the rapid advancement that has been observed in the way games software is being developed, as well as in the development of games content using game engines. One area that has gained special attention is modeling the game environment such as terrain and buildings. This article presents the continuous level of detail terrain modeling techniques that can help generate and render realistic terrain in real time. Deployment of characters in the environment is increasingly common. This requires strategies to map scalable behavior characteristics for characters as well. The authors present two important aspects of crowd simulation: the realism of the crowd behavior and the computational overhead involved. A good simulation of crowd behavior requires delicate balance between these aspects. The focus in this article is on human behavior representation for crowd simulation. To enhance the player experience, the authors present the concept of player adaptive entertainment computing, which provides a personalized experience for each individual when interacting with the game. The current state of game development involves using very small percentage (typically 4% to 12%) of CPU time for game artificial intelligence (AI). Future game AI requires developing computational strategies that have little involvement of CPU for online play, while using CPU&apos;s idle capacity when the game is not being played, thereby emphasizing the construction of complex game AI models offline. A framework of such nonconventional game AI models is introduced.  753 Keywords artificial intelligence, behavior modeling, board game, character modeling, conversational avatars, environment modeling, game AI, game development, game editor, game engine, game software, gardening game, genetic algorithms, hybrid soft computing, memetic algorithms, neural network optimization, programmable tournament, racing game, small world models, terrain modeling This article is organized as follows. First, we present some simple and effective topics in software games engineering. Then we look at some of the state-of-the-art game engines available for game development. A survey of the major issues that have occupied game programmers, in relation to the problems of terrain generation, over the past 10 years, is presented, and then we present current trends as a prelude to the development of a general purpose reusable terrain function library. We then describe how to generate realistic human behaviors. This is a challenging and interesting problem in many applications such as computer games, military simulations, and crowd animation systems. First, we propose a generic behavior modeling framework that reflects the major observations on human behavior in daily-life situations. Next, we describe a case study to illustrate how this modeling framework can be used in various crowd simulations. Personalization in games can be in terms of difficulty levels, game resources, emotion, characters, and so forth. In game design, the first area can be easily identified and addressed. Normally, during this first stage, the genre of the game will be determined. However, the second area of personalization is sometimes difficult. This is because of the differences of players in terms of their personality, background, culture, skill, and learning ability. Currently, game artificial intelligence (AI) is used for modeling various aspects of nonplayer characters. Although such applications (e.g., behavior modeling) have been very successful, the deployment of game AI for serious games having learning as main component poses basic questions, such as the representation of an explanation capability as perceived by the human player. In this article, we propose a general framework for the development of such an explanation capability. Various soft-computing models have complementary capabilities. This motivates us to propose a framework for their integration. We identify the need for future game AI engines with such capabilities. Simple and Effective Topics in Software Games Engineering Over the past few years, we have introduced several simple but effective new topics into the teaching of software programming and other similar courses. These new topics are industry case study, open-source game editors, programmable tournament, and project-based development. The industry case study allows the students to understand the myriad factors in the technical and business sides of a real games company; the case study we use is id Software, a standout small company that is one of the true leaders in the games industry. The open-source game editor abstracts the programming at AJOU UNIV on March 1, 2010 http://sag.sagepub.com Downloaded from 754 Simulation &amp; Gaming 40 (6)    details of the platform away from the game, and provides an easy and fast way of creating the domain and objectives of game. Thus, the students can focus their learning on the story line, game boarding, and psychologically manipulating the player. The programmable tournament allows the students to devise and build game strategy in a multiplayer arena. This is quite effective as a tutorial for assessing the students&apos; problem solving and programming skills. Finally, the project demands that the student study and report on the approach to or method of developing games. These new topics are excellent for marrying games development with software engineering, and provide for a wide range of learning and criteria for assessment of the students. This section discusses the new topics introduced into the game programming and other similar courses in a computer science/engineering program. The topics are simple and easily adapted for most forms of games courses, yet they are also extremely effective in presenting meaningful depth and breadth to any software games engineering course",,"Advances in Games Technology: Software, Models, and Intelligence Advances in Games Technology: Software, Models, and Intelligence",,,,core
322567150,2020-05-10T00:00:00,"Intelligent systems designed using machine learning algorithms require a
large number of labeled data. Background knowledge provides complementary, real
world factual information that can augment the limited labeled data to train a
machine learning algorithm. The term Knowledge Graph (KG) is in vogue as for
many practical applications, it is convenient and useful to organize this
background knowledge in the form of a graph. Recent academic research and
implemented industrial intelligent systems have shown promising performance for
machine learning algorithms that combine training data with a knowledge graph.
In this article, we discuss the use of relevant KGs to enhance input data for
two applications that use machine learning -- recommendation and community
detection. The KG improves both accuracy and explainability",'Institute of Electrical and Electronics Engineers (IEEE)',Knowledge Graph semantic enhancement of input data for improving AI,10.1109/MIC.2020.2979620,http://arxiv.org/abs/2005.04726,,core
337894537,2020-01-01T00:00:00,"Big data is considered to play an important role in the fourth industrial revolution, which requires engineers and computers to fully utilize data to make smart decisions and improve the performance of industrial processes and of their control and safety systems. Traditionally, industrial process control systems rely on a (usually linear) data-driven model with parameters that are identified from industrial/simulation data, and in certain cases, for example, in profit-critical control loops, on first-principles models (with data-determined model parameters) that describe the underlying physico-chemical phenomena. However, modeling large-scale, complex nonlinear processes continues to be a major challenge in process systems engineering. Modeling is particularly important now and into the future, as process models are key elements of advanced model-based control systems, e.g., model predictive control (MPC) and economic MPC (EMPC).Due to the wide variety of applications, machine learning models have great potential, yet, the development of rigorous and systematic methods for incorporating machine learning techniques in nonlinear process control and operational safety is in its infancy. Traditionally, operational safety of chemical processes has been addressed through process design considerations and through a hierarchical, independent design of control and safety systems. However, the consistent accidents throughout chemical process plant history (including several high profile disasters in the last decade) have motivated researchers to design control systems that explicitly account for process operational safety considerations. In particular, a new design of control systems such as model predictive controllers (MPC) that incorporate safety considerations and can be coordinated with safety systems has the potential to significantly improve process operational safety and avoid unnecessary triggering of alarms systems, where machine learning techniques can be utilized to derive dynamic process models. However, the rigorous design of safety-based control systems poses new challenges that cannot be addressed with traditional process control methods, including, for example, proving simultaneous closed-loop stability and safety. On the other hand, cybersecurity has become increasingly important in chemical process industries in recent years as cyber-attacks that have grown in sophistication and frequency have become another leading cause of process safety incidents. While the traditional methods of handling cyber-attacks in control systems still rely partly on human analysis and mainly fall into the area of fault diagnosis, the intelligence of cyber-attacks and their accessibility to control system information has recently motivated researchers to develop cyber-attack detection and resilient operation control strategies to address directly cybersecurity concerns.Motivated by the above considerations, this dissertation presents the use of machine learning techniques in model predictive control, operational safety and cybersecurity for chemical processes described by nonlinear dynamic models. The motivation and organization of this dissertation are first presented. Then, the use of machine learning techniques to develop data-driven nonlinear dynamic process models to be used in model predictive controllers is presented, followed by the discussion of real-time implementation with online learning of machine leaning models and of physics-based machine learning modeling methods. Subsequently, the MPC and economic MPC schemes that use control Lyapunov-barrier functions (CLBF) are presented in detail with rigorous analysis provided on their closed-loop stability, operational safety and recursive feasibility properties. Next, the development of machine-learning-based CLBF-MPC schemes is presented with process stability and safety analysis. Finally, the development of an integrated detection and control system for process cybersecurity is developed, in which several types of intelligent cyber-attacks, machine learning detection methods and resilient control strategies are presented. Throughout the dissertation, the control methods are applied to numerical simulations of nonlinear chemical process examples to demonstrate their effectiveness and performance","eScholarship, University of California","Machine Learning in Model Predictive Control, Operational Safety and Cybersecurity",,,,core
335614288,2020-10-01T00:00:00,"Continuous delivery has gained increased popularity in industry as a development approach to develop, test, and deploy enhancements to software components in short development cycles. In order for continuous delivery to be effectively adopted, the services that a component depends upon must be readily available to software engineers in order to systematically apply quality assurance techniques. However, this may not always be possible as (i) these requisite services may have limited access and (ii) defects that are introduced in a component under development may cause ripple effects in real deployment environments. Service virtualisation (SV) has been introduced as an approach to address these challenges, but existing approaches to SV still fall short of delivering the required accuracy and/or ease-of-use to virtualise services for adoption in continuous delivery. In this work, we propose a novel machine learning based approach to predict numeric fields in virtualised responses, extending existing research that has provided a way to produce values for categorical fields. The SV approach introduced here uses machine learning techniques to derive values of numeric fields that are based on a variable number of pertinent historic messages. Our empirical evaluation demonstrates that the Cognitive SV approach can produce responses with the appropriate fields and accurately predict values of numeric fields across three data sets, some of them based on stateful protocols",'MDPI AG',Cognitive service virtualisation: A new machine learning-based virtualisation to generate numeric values,10.3390/s20195664,http://hdl.handle.net/10536/DRO/DU:30143885,,core
334907890,2020-01-30T00:00:00,"Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has
significantly advanced in recent years, mainly due to their relevance in a
large variety of commercial, industrial, and military applications. However,
UAV navigation in GPS-denied environments continues to be a challenging problem
that has been tackled in recent research through sensor-based approaches. This
paper presents a novel offline, portable, real-time in-door UAV localization
technique that relies on macro-feature detection and matching. The proposed
system leverages the support of machine learning, traditional computer vision
techniques, and pre-existing knowledge of the environment. The main
contribution of this work is the real-time creation of a macro-feature
description vector from the UAV captured images which are simultaneously
matched with an offline pre-existing vector from a Computer-Aided Design (CAD)
model. This results in a quick UAV localization within the CAD model. The
effectiveness and accuracy of the proposed system were evaluated through
simulations and experimental prototype implementation. Final results reveal the
algorithm's low computational burden as well as its ease of deployment in
GPS-denied environments",'MDPI AG',"UAV Autonomous Localization using Macro-Features Matching with a CAD
  Model",10.3390/s20030743,http://arxiv.org/abs/2001.11610,,core
334913630,2020-02-18T00:00:00,"Hyper-heuristic is a new methodology for the adaptive hybridization of
meta-heuristic algorithms to derive a general algorithm for solving
optimization problems. This work focuses on the selection type of
hyper-heuristic, called the Exponential Monte Carlo with Counter (EMCQ).
Current implementations rely on the memory-less selection that can be
counterproductive as the selected search operator may not (historically) be the
best performing operator for the current search instance. Addressing this
issue, we propose to integrate the memory into EMCQ for combinatorial t-wise
test suite generation using reinforcement learning based on the Q-learning
mechanism, called Q-EMCQ. The limited application of combinatorial test
generation on industrial programs can impact the use of such techniques as
Q-EMCQ. Thus, there is a need to evaluate this kind of approach against
relevant industrial software, with a purpose to show the degree of interaction
required to cover the code as well as finding faults. We applied Q-EMCQ on 37
real-world industrial programs written in Function Block Diagram (FBD)
language, which is used for developing a train control management system at
Bombardier Transportation Sweden AB. The results of this study show that Q-EMCQ
is an efficient technique for test case generation. Additionally, unlike the
t-wise test suite generation, which deals with the minimization problem, we
have also subjected Q-EMCQ to a maximization problem involving the general
module clustering to demonstrate the effectiveness of our approach.Comment: 37 Page",'Springer Science and Business Media LLC',"An Evaluation of Monte Carlo-Based Hyper-Heuristic for Interaction
  Testing of Industrial Embedded Software Applications",10.1007/s00500-020-04769-z,http://arxiv.org/abs/2002.07443,,core
304996217,2020-03-05T00:00:00,"Presented on March 5, 2020 at 12:00 p.m. in the Technology Square Research Building, Banquet Hall.David Anderson is currently a professor in the School of Electrical and Computer Engineering at Georgia Tech. Dr. Anderson's research interests include audio and psycho-acoustics, machine learning and signal processing in the context of human auditory characteristics, and the real-time application of such techniques.Adam Betuel is the Conservation Director at Atlanta Audubon Society.Brooke Bosley is a second-year Ph.D. student in Digital Media, specializing in Black Media, at the Georgia Institute of Technology. Her focus is on using principles of Afrofuturism and Black Feminism to rethink traditional methods of human-centered design.Lelia Glass is an Assistant Professor of Linguistics in the School of Modern Languages at Georgia Tech.  

She works on lexical semantics (word meaning), compositional semantics (sentence meaning), and pragmatics (inferences drawn in context) from an empirically rich perspective.  She is particularly interested in how our knowledge of the (physical, social) world affects our interpretation of language.Susana M. Morris is a scholar of Black Feminism, Black Digital Media, and Afrofuturism and is an associate professor in the School of Literature, Media, and Communication at Geogia Tech.Sean Mulvanity is a senior research associate and STEM Curriculum Development Lead with the Georgia Tech Research Institute.Andrew Partridge is a Research Associate with the Georgia Tech Research Institute.In 2017, Dr. Ploetz joined the School of Interactive Computing at the Georgia Institute of Technology where he works as an associate professor. His research agenda focuses on applied machine learning, that is developing systems and innovative sensor data analysis methods for real world applications.Mark Riedl is an associate professor in the College of Computing, School of Interactive Computing.  As director of the Entertainment Intelligence Lab, Dr. Riedl's research focuses on the study of artificial intelligence and storytelling for entertainment (e.g., computer games).  Narrative is a cognitive tool used by humans for communication, sense-making, entertainment, education, and training.Jon A. Sanford is a professor in the School of Industrial Design and director of the Center for Assistive Technology and Environmental Access at Georgia Tech. He is internationally-recognized for his expertise in universal design, accessible design, and design for aging, and is the director of the Rehabilitation Engineering Research Center on Technologies for Successful Aging with Disability, supported by DHHS.Dr. Anne Sullivan is an Assistant Professor in Digital Media and head of the Experimental Game Lab at Georgia Tech. Her current research focuses predominantly at computational craft through craft-based games, STEM education through the lens of crafting, and studying how narratives are told in crafting communities.Matthew Swarts is a member of the research faculty in the College of Architecture. His research focuses on the translation of human behavioral patterns and perceptions within real and virtual environments into computer models and simulations to inform the design decision making process.Emily Weigel is an Academic Professional in the School of Biological Sciences at the Georgia Institute of Technology (Georgia Tech).Runtime: 58:42 minutesThe GVU Center and the Institute for People and Technology (IPaT), with additional support from GTRI support two separate types of grant proposals. Research Grants provide seed funding for new research collaborations, and Engagement Grants provide support for new forms of internal and external community engagement and collaboration.

We’re pleased to announce that the following projects were selected for 2019-2020:

1) ""From #hashtags to Movements: Performance, Collective Narrative, and Erasure, a Black Feminist Perspective"", by Brooke Bosley and Susana Morris (Digital Media); 2) ""Workshop on Language, Technology, and Society"", by Lelia Glass (Modern Languages); 3) ""Getting Good: Using esports to inspire students in developing STEM skills"", by Laura Levy (IMTC), Andrew Partridge (GTRI), and Sean Mulvanity (GTRI); 4) ""Detecting and Measuring the Impact of Food Insecurity at Georgia Tech"", by Jon Sanford (Industrial Design) and Thomas Ploetz (Interactive Computing); 5) ""Artificial Intelligence and Interactive Digital Entertainment 2019"", by Anne Sullivan (Literature, Media, and Communication) and Mark Riedl (Interactive Computing); 6) ""Acoustic Sensor Deployment in the EcoCommons"", by Emily Weigel (Biological Sciences), Adam Beteul (Atlanta Audobon Society), David Anderson (ECE), and Matthew Swarts (GTRI)",,GVU/IPaT Research and Engagement Grants,,,,core
425952607,2020-01-01T00:00:00,"У статті проаналізовані існуючі підходи до розуміння поняття управлінських рішень. Реалізація
механізму управлінських рішень здійснюється на основі використання нової інформаційної технології,
адаптованої для оперативної розбудови та апробації сценаріїв швидкої оцінки проблемної ситуації на
основі використання сучасних методів обробки та аналізу даних та знань. Раніше при аналізі даних
підприємства використовували експертну оцінку, однак використання штучних нейронних мереж при
роботі промислового підприємства дають набагато більше можливостей для аналізу та прогнозування,
тому що нейронні мережі є гарними апроксиматорами. Тобто виходить, що запропонована модель більш
універсальна в порівнянні з відомими моделями. В статті показана та обґрунтована необхідність
удосконалення існуючих наукових підходів та організації механізмів підготовки і прийняття управлінських
рішень на промислових підприємствах. Запропоновано створення строгого математичного апарату, що
дозволяє проводити аналіз та оптимізацію систем прийняття рішень на промисловому підприємстві де
використовуються різні технології прийняття управлінських рішень.The subject of research is to develop a mechanism for making effective management decisions in the
liberalization of the engineering market.
The purpose of the study is to Identifying the main trends and contradictions that arise in the operation of
machine-building industrial enterprises.
The methodological basis of the study The article shows and substantiates the need to improve existing scientific
approaches and the organization of mechanisms for training and decision-making in industrial enterprises.
Results. The article analyzes the existing approaches to understanding the concept of ""mechanism of making
effective management"" in the industrial enterprises.
On the basis of the conducted research the author's approach to formation and realization of model of
acceptance of administrative decisions at the enterprise with use of artificial neural networks is proved. It is
proposed to use existing software products and international standards for analysis and forecasting to develop the
company's strategy, without which it is impossible to build an adequate mathematical model with given parameters,
which in turn will help to describe each process in production. The use of created elements of mathematical and
software allows to save time to obtain adequate mathematical models to reduce energy and computational costs for
the study of real objects, to improve the quality of management and prediction of their behavior.
Conclusions. The proposed scientific and practical developments can be used by industrial enterprises, and the
study of the functioning of both individual enterprises and industry complexes, requires a systematic approach that
focuses not only on the enterprise itself but also on its environmental factors. In this case, the object of study is not
considered as an isolated and independent structure, but as an open socio-economic system",'Sumy State University',Management decisions and the efficiency of the industrial enterprise,10.21272/1817-9215.2020.2-22,,,core
427319835,2020-05-30T00:00:00,"Electric machines and motors have been the subject of enormous development. New concepts in design and control allow expanding their applications in different fields. The vast amount of data have been collected almost in any domain of interest. They can be static; that is to say, they represent real-world processes at a fixed point of time. Vibration analysis and vibration monitoring, including how to detect and monitor anomalies in vibration data are widely used techniques for predictive maintenance in high-speed rotating machines. However, accurately identifying the presence of a bearing fault can be challenging in practice, especially when the failure is still at its incipient stage, and the signal-to-noise ratio of the monitored signal is small. The main objective of this work is to design a system that will analyze the vibration signals of a rotating machine, based on recorded data from sensors, in the time/frequency domain. As a consequence of such substantial interest, there has been a dramatic increase of interest in applying Machine Learning (ML) algorithms to this task. An ML system will be used to classify and detect abnormal behavior and recognize the different levels of machine operation modes (normal, degraded, and faulty). The proposed solution can be deployed as predictive maintenance for Industry 4.0",'Springer Science and Business Media LLC',Automatic classification of rotating machinery defects using Machine Learning (ML) algorithms,10.1007/978-981-15-5784-2_16,,,core
343446702,2020-02-20T00:00:00,"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence demonstrated by humans. Examples of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects, which is usually regarded as intelligent control. In recent years, the applications of AI to robotics have experimented with exponential growth. AI plays a crucial role in the path planning of robots, allowing fast responses to changes in complex environments. It also plays a leading role in modeling and intelligent control of robots by allowing a more complex feedback analysis, self-tuning applications, and on-the-fly adaptation to environmental changes.



Changing industrial environments like flexible manufacturing facilities and automated warehouses where robots are intended to work side by side with humans are benefiting directly from advancements in complex path planning and autonomous decision making based on AI-powered algorithms. On the consumer side, applications like cleaning robots and delivery robots are also becoming part of our daily lives. The implementation of AI-powered path planning and control algorithms drastically improves the efficiency and practicality of these robots, as the environments in which these robots must operate is highly dynamic and needs constant adaptation.



This Research Topic is organized under the section “Robotic Control Systems” within Frontiers in Robotics and AI. The first article by Tan et al. is focused on designing mechanisms and algorithms for robotics, which serves as a platform for path planning and control. Current robot designs have been taking inspiration from games and entertainment artifacts (GEAs). However, there is a lack of systematic and general processes for implementing a GEA-inspired design in robotics. In this article, a systematic robot design paradigm is proposed based on the inspiration of GEAs. Both problem-driven and solution-driven processes can be followed to make use of analogies of GEAs so that robotic solutions can be obtained for real-world problems. The application of the design paradigm is demonstrated by using a reconfigurable floor cleaning robot and its path planning algorithm.



Due to the capacity of reasoning, AI plays a crucial role in achieving safe human-robot interaction (HRI) for collaborative robots. The article by Du et al. combines different AI technologies to achieve active collision avoidance for safe HRI. A Microsoft's motion sensing input device named Kinect is employed to detect anyone who enters the workspace of the robot so that the skeleton data of the human can be calculated in real-time. An expert system with collision avoidance knowledge is employed to analyze the behavior of the human for active collision avoidance. An artificial potential field method is adopted to plan a new path for the robot such that it can bypass the human in real-time. Experiments show that by applying these AI-powered algorithms, the proposed system can safeguard the human by detecting the human and analyzing the motion of the human.



An important issue for collaborative robots is to learn the compositionality of human activities, i.e., to recognize both activities and their comprising actions. Even a small set of actions and objects can create a large combination of possible activities. Most existing approaches in this topic address action and activity recognitions separately. The article by Mici et al. suggests learning human activities concurrently on two levels of semantic and temporal complexity: Transitive actions such as reaching and opening a cereal box, and high-level activities such as having breakfast. The learning model consists of a hierarchy of growing-when-required (GWR) networks which can process and learn inherent spatiotemporal dependencies of multiple visual cues abstracted from human body skeletal representation and interaction with objects. GWR means that new network nodes are added only when the number of iterations is an integer multiple of some pre-defined constant. The proposed architecture semantically segments input RGB-D sequences of high-level activities into their component actions without supervision. Experiments show that the proposed approach possesses a superior ability regarding the classification of high-level activities.



AI is also useful for collaborative robots to assist humans in co-manipulation and teleoperation tasks under demonstrated trajectories. Most existing approaches in this topic are not applicable when the solutions of demonstrations are suboptimal or when the generalization capabilities of the learned models cannot cope with changing environments. The article by Ewerton et al. presents a reinforcement learning-based approach to solve the problem above. The proposed approach makes use of the concept of relevance functions and is initialized by a probability distribution of demonstrated trajectories. Gaussian Process regression is applied to cope with the changes in dynamic environments. Experiments demonstrate that the proposed algorithm embedded in a 7-degree of Freedom (DoF) robot arm performs well under a dynamic environment.



The last article by Galati and Giulio serves more as an outreach of this Research Topic, where a physical model-based approach for terrain characterization is presented for a tracked skid-steer vehicle. A set of physics-based parameters, including the equivalent track, the power spectral density for the vertical accelerations, drive motor electrical currents, and motor currents, is employed to characterize the terrain properties. The proposed algorithm predicts the type of terrain that the robot traverses based on the parameter set. Experiments under various surfaces verify the effectiveness of the proposed approach for autonomous robots. The results of this article also indicate that the intelligent integration of model-based and AI-based techniques will be promising in robotic applications.YP was funded by the Fundamental Research Funds for the

Central Universities of China (Grant No. 19lgzd40). CY was

partially supported by the Engineering and Physical Sciences

Research Council (Grant EP/S001913).Peer reviewe",'Frontiers Media SA',"Editorial: AI for Robot Modeling, Path Planning, and Intelligent Control",10.3389/frobt.2020.00019,https://core.ac.uk/download/343446702.pdf,"[{'title': 'Frontiers in Robotics and AI', 'identifiers': ['issn:2296-9144', '2296-9144']}]",core
345051037,2020-01-01T00:00:00,"To identify quality issues within the production and prevent defect products to be delivered to customers is critical for most manufacturing companies, and usually performed both within and at the end of each production section. In this paper we investigate the use of deep neural networks for performing automatic quality inspections based on image processing, with the aim of eliminating today’s manual inspection processes. A deep neural network is implemented on a real-world industrial case study and its performance is evaluated and analyzed when it comes to detecting quality problems in produced products. The results show that the network has an accuracy of 94.5% which is considered good in comparison to the 70-80% accuracy that a trained human inspector can achieve.CC BY-NC-ND 4.0</p",'Elsevier BV',Image Processing based on Deep Neural Networks for Detecting Quality Problems in Paper Bag Production,10.1016/j.procir.2020.04.158,,,core
334841807,2020-01-18T00:00:00,"Driverless vehicles promise a host of societal benefits including
dramatically improved safety, increased accessibility, greater productivity,
and higher quality of life. As this new technology approaches widespread
deployment, both industry and government are making provisions for
teleoperations systems, in which remote human agents provide assistance to
driverless vehicles. This assistance can involve real-time remote operation and
even ahead-of-time input via human-in-the-loop artificial intelligence systems.
In this paper, we address the problem of staffing such a remote support center.
Our analysis focuses on the tradeoffs between the total number of remote
agents, the reliability of the remote support system, and the resulting safety
of the driverless vehicles. By establishing a novel connection between queues
with large batch arrivals and storage processes, we determine the probability
of the system exceeding its service capacity. This connection drives our
staffing methodology. We also develop a numerical method to compute the exact
staffing level needed to achieve various performance measures. This moment
generating function based technique may be of independent interest, and our
overall staffing analysis may be of use in other applications that combine
human expertise and automated systems",,"Beyond Safety Drivers: Staffing a Teleoperations System for Autonomous
  Vehicles",,http://arxiv.org/abs/1907.12650,,core
395676456,2020-06-15T00:00:00,"International audienceThe mobile ecosystem is witnessing an unprecedented increase in the number of malware in the wild. To fight this threat, actors from both research and industry are constantly innovating to bring concrete solutions to improve security and malware protection. Traditional solutions such as signature-based anti viruses have shown their limits in front of massive proliferation of new malware, which are most often only variants specifically designed to bypass signature-based detection. Accordingly, it paves the way to the emergence of new approaches based on Machine Learning (ML) technics to boost the detection of unknown malware variants. Unfortunately, these solutions are most often underexploited due to the time and resource costs required to adequately fine tune machine learning algorithms. In reality, in the Android community, state-of-the-art studies do not focus on model training, and most often go through an empirical study with a manual process to choose the learning strategy, and/or use default values as parameters to configure ML algorithms. However, in the ML domain, it is well known admitted that to solve efficiently a ML problem, the tunability of hyper-parameters is of the utmost importance. Nevertheless, as soon as the targeted ML problem involves a massive amount of data, there is a strong tension between feasibility of exploring all combinations and accuracy. This tension imposes to automate the search for optimal hyper-parameters applied to ML algorithms, that is not anymore possible to achieve manually. To this end, we propose a generic and scalable solution to automatically both configure and evaluate ML algorithms to efficiently detect Android malware detection systems. Our approach is based on devOps principles and a microservice architecture deployed over a set of nodes to scale and exhaustively test a large number of ML algorithms and hyper-parameters combinations. With our approach, we are able to systematically find the best fit to increase up to 15% the accuracy of two state-of-the-art Android malware detection systems",'Springer Fachmedien Wiesbaden GmbH',DroidAutoML: A microservice architecture to automate the evaluation of Android machine learning detection systems,,https://core.ac.uk/download/395676456.pdf,,core
333580898,2020-01-01T00:00:00,"The design work-flow of machine learning techniques for continuous monitoring or predictive maintenance in an industrial context is usually a two step procedure: the selection of features to be computed from the observed signals and training of a suitable algorithm with real-life meaningful data, that will be next deployed in the second step. Feature selection is a relevant task since it provides a powerful optimisation of the deployed algorithm performance, for the given training data-set. The paper provides a method for feature ranking and selection that embeds constraints coming from real-life applications, including sensing device specifications, environmental noise, available processing resources, being all these latter aspects not considered in the currently available literature methods for feature selection. A practical case-study in the field on anomaly detection of machines is reported and discussed, in order to show the good properties of the provided method",'Institute of Electrical and Electronics Engineers (IEEE)',Feature Ranking under Industrial Constraints in Continuous Monitoring Applications based on Machine Learning Techniques,10.1109/i2mtc43012.2020.9129595,,,core
333595489,2020-08-27T00:00:00,"Indukční stroje jsou jedním z nejpoužívanějších elektrických strojů v mnoha průmyslových a dopravních aplikacích. Je proto důležité mít zařízení pro jejich ovládání. Naproti tomu, DC stroje je možné řídit pomocí mnohem jednodušších schémat, ale kvůli jejich technickým nevýhodám se v současnosti nejeví jako nejlepší volba pro průmyslové použití. Pro řízení rychlosti a točivého momentu indukčních strojů bylo vyvinuto mnoho řídicích strategií a stále probíhá další výzkum pomocí modernějších technik, jako například „Fuzzy logic“ nebo neuronové sítě. Tématem práce je polem orientované vektorové řízení, které je jedním z nejpoužívanějších způsobů řízení. Implementovali jsme schéma s polem orientovaným řízením v softwaru Simulink na indukční stroj se skutečnými parametry. Pro zkoumaný řadič jsme vyzkoušeli různé scénáře s cílem zjistit jeho fungovaní a možná vylepšení. Rovněž jsme implementovali bezsenzorové řízení stroje s odhadem otáček.Induction machines are one of the most widely used electric machines on many industrial and transportation applications. It is, therefore, important to have a facility to control these machines for the wide range of applications. On the other hand, it is possible to control DC machines with much simpler control schemes, but their technical drawbacks do not make them the preferable choice for the industry nowadays. Many control strategies have been developed for the speed and torque control of induction machines and more research is still ongoing using more modern techniques, such as “fuzzy logic” and “neural networks”. One of the most widely used control techniques is the “field-oriented control”, which is the topic of this thesis. With the parameters of a real induction machine, we implemented a control scheme based on the field-oriented control in the Simulink software. Various scenarios were applied to the controller in order to study its functioning and possible improvements. Furthermore, a speed estimation part was implemented for a sensorless control of the machine",Czech Technical University in Prague. Computing and Information Centre.,Implementation of Field Oriented Control in Simulink,,,,core
322450777,2020-05-02T00:00:00,"The integration of communication networks and the Internet of Things (IoT) in
Industrial Control Systems (ICSs) increases their vulnerability towards
cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection
Systems (IDSs), which are mainly developed to support Information Technology
(IT) systems, count vastly on predefined models and are trained mostly on
specific cyber-attacks. Besides, most IDSs do not consider the imbalanced
nature of ICS datasets, thereby suffering from low accuracy and high false
positive on real datasets. In this paper, we propose a deep representation
learning model to construct new balanced representations of the imbalanced
dataset. The new representations are fed into an ensemble deep learning attack
detection model specifically designed for an ICS environment. The proposed
attack detection model leverages Deep Neural Network (DNN) and Decision Tree
(DT) classifiers to detect cyber-attacks from the new representations. The
performance of the proposed model is evaluated based on 10-fold
cross-validation on two real ICS datasets. The results show that the proposed
method outperforms conventional classifiers, including Random Forest (RF), DNN,
and AdaBoost, as well as recent existing models in the literature. The proposed
approach is a generalized technique, which can be implemented in existing ICS
infrastructures with minimum changes",,"An Ensemble Deep Learning-based Cyber-Attack Detection in Industrial
  Control System",,http://arxiv.org/abs/2005.00936,,core
357372811,2020-04-11T00:00:00,"Abstract The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach. Real-world goals We consider a broad set of design goals important for making classifiers work well in real-world applications, and discuss how metrics quantifying many of these goals can be represented in a particular optimization framework. The key theme is that these metrics, which range from the standard precision and recall, to less well-known examples such as coverage and fairness  Coverage: One may wish to control how often a classifier predicts the positive (or negative) class. For example, one may want to ensure that only 10% of customers are selected to receive a printed catalog due to budget constraints, or perhaps to compensate for a biased training set. In practice, constraining the &quot;coverage rate&quot; (the expected proportion of positive predictions) is often easier than measuring e.g. accuracy or precision because coverage can be computed on unlabeled data-labeling data can be expensive, but acquiring a large number of unlabeled examples is often very easy. Coverage was also considered by Mann and McCallum [17], who proposed what they call &quot;label regularization&quot;, in which one adds a regularizer penalizing the relative entropy between the mean score for each class and the desired distribution, with an additional correction to avoid degeneracies. Churn: Work does not stop once a machine learning model has been adopted. There will be new training data, improved features, and potentially new model structures. Hence, in practice, one will deploy a series of models, each improving slightly upon the last. In this setting, determining whether each candidate should be deployed is surprisingly challenging: if we evaluate on the same held-out testing set every time a new candidate is proposed, and deploy it if it outperforms its predecessor, then every compare-and-deploy decision will increase the statistical dependence between the deployed model and the testing dataset, causing the model sequence to fit the originally-independent testing data. This problem is magnified if, as is typical, the candidate models tend to disagree only on a relatively small number of examples near the true decision boundary",,Satisfying Real-world Goals with Dataset Constraints,,,,core
478159166,2021-05-01T00:00:00,"The real estate sector brings a fortune to the global economy. But, presently, this sector is regressive and uses traditional methods and approaches. Therefore, it needs a technological transformation and innovation in line with the Industry 4.0 requirements to transform into smart real estate. However, it faces the barriers of disruptive digital technology (DDT) adoption and innovation that need effective management to enable such transformation. These barriers present managerial challenges that affect DDT adoption and innovation in smart real estate. The current study assesses these DDTs adoption and innovation barriers facing the Australian real estate sector from a managerial perspective. Based on a comprehensive review of 72 systematically retrieved and shortlisted articles, we identify 21 key barriers to digitalisation and innovation. The barriers are grouped into the technology-organisation-external environment (TOE) categories using a Fault tree. Data is collected from 102 real estate and property managers to rate and rank the identified barriers. The results show that most of the respondents are aware of the DDTs and reported AI (22.5% of respondents), big data (12.75%) and VR (12.75%) as the most critical technologies not adopted so far due to costs, organisation policies, awareness, reluctance, user demand, tech integration, government support and funding. Overall, the highest barrier (risk) scores are observed for high costs of software and hardware (T1), high complexity of the selected technology dissemination system (T2) and lack of government incentives, R&D support, policies, regulations and standards (E1). Among the TOE categories, as evident from the fault tree analysis, the highest percentage of failure to adopt the DDT is attributed to E1 in the environmental group. For the technological group, the highest failure reason is attributed to T2. And for the organisational group, the barrier with the highest failure chances for DDT adoption is the lack of organisational willingness to invest in digital marketing (O4). These barriers must be addressed to pave the way for DDT adoption and innovation in the Australian real estate sector and move towards smart real estate",'Elsevier BV',Barriers to the digitalisation and innovation of Australian Smart Real Estate: A managerial perspective on the technology non-adoption,10.1016/j.eti.2021.101527,,,core
479062550,2021-09-09T00:00:00,"Solar energy is the most abundant, inexhaustible, and environmentally friendly of all renewable energy sources. Because of its advantages, interest in electrical solar PV power generation has grown in recent years. This extensive spread of electrical phenomenon panel manufacture was not accompanied by services such as monitoring, fault detection, and identification to confirm greater gain. This methodology proposes a method for real- time monitoring and fault diagnosis in electrical solar PV systems. This method is based on a comparison of the performance of a defective electrical solar PV module with its right model, which is done by measuring the precise differential residue that is associated with it. The deformations generated on the I-V and PV curves are used to determine the electrical signature of each default. Module to module faults, short circuit faults, open circuit faults, cell to ground faults, and various shading patterns are all taken into account. The projected method is frequently adapted and applied to a wider range of faults. Back- propagation based Artificial Neural Networks were used to assess the faults situation (ANN). The MATLAB simulation model's simulation results indicate satisfactory outcomes for various fault conditions as a function of solar irradiation. The entire system is designed and tested in the MATLAB 2015a software environment for various failure conditions. The system's definition and configuration are based on the basic paper system",Journal of Electrical and Power System Engineering (e-ISSN: 2582-5712),Solar PV Array System Fault Location and Classification using Wavelet and Artificial Neural Network,,,,core
390059886,2021-06-16T00:00:00,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model",'Elsevier BV',Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,10.1016/j.ymssp.2020.107510,,,core
429987298,2021-06-01T00:00:00,"Abstract Current Additive Manufacturing machines have limited techniques to observe process conditions and to decrease process errors. In order to overcome these limitations and increase the level and accuracy of machine intelligence, machine conditions need to be monitored more meticulously. A novel method for the condition monitoring of a 3D‐printer is proposed in this paper. Quantum support vector machine (QSVM) is compiled for recognizing the health condition of the 3D‐printer. The proposed quantum machine learning approach helps in monitoring the health state of the machine and classifies the same as healthy or aberrant. Classical machine learning approaches are inefficient to process the large amount of experimental data in real time. For better decision‐making on such big data, quantum machine learning approaches are deployed which are much more efficient due to their exponential speed and parallel operation on complex sensor data, they show speedups in both the dimensionality and number of experimental data deployed to train the algorithm. The simulation results show that the proposed method has higher accuracy in fault diagnosis than the traditional Support Vector Machine. All the numerical simulations and experiments have been carried out on a real quantum hardware provided by the IBM Quantum computing over the cloud",'Institution of Engineering and Technology (IET)',A quantum‐based diagnostics approach for additive manufacturing machine,10.1049/cim2.12022,,"[{'title': 'IET Collaborative Intelligent Manufacturing', 'identifiers': ['2516-8398', 'issn:2516-8398']}]",core
387307904,2021-01-14T00:00:00,"Wearable Cognitive Assistance (WCA) amplifies human cognition in real time
through a wearable device and low-latency wireless access to edge computing
infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation
tools that provide real-time step-by-step guidance, with prompt error detection
and correction. WCA applications are likely to be transformative in education,
health care, industrial troubleshooting, manufacturing, and many other areas.
Today, WCA application development is difficult and slow, requiring skills in
areas such as machine learning and computer vision that are not widespread
among software developers. This paper describes Ajalon, an authoring toolchain
for WCA applications that reduces the skill and effort needed at each step of
the development pipeline. Our evaluation shows that Ajalon significantly
reduces the effort needed to create new WCA applications",,Ajalon: Simplifying the Authoring of Wearable Cognitive Assistants,,http://arxiv.org/abs/2101.05766,,core
433806836,2021-04-30T00:00:00,"The object of research is the process of using information technology in the construction industry. One of the most problematic areas is increasing the efficiency of the construction industry through the introduction of digital technologies. The research carried out is based on the application of an approach that is implemented using artificial intelligence. The study used machine learning and fuzzy logic methods to mark visual data and analyze it for potential threats, as well as to reduce all possible risks. The main feature of this approach is that using machine learning technology, it is possible to reduce the risks of a project before they affect its profit. So, using artificial intelligence in combination with BIM technologies, it is possible to predict work on construction projects based on real-time data, past activities and other factors in such a way as to optimize construction processes. The benefits to be gained from implementing digital processes will become even more evident in future projects as AI continues to analyze company data. This is due to the fact that the proposed approach using fuzzy logic has a number of features, in particular, the more information machine learning algorithms process, the more complex they become. As a result, they provide even more useful information and allow to make even better decisions. This provides an opportunity to minimize risks and efficiently allocate resources when working on projects. Compared to conventional information technology, artificial intelligence can be used to build a knowledge-based security management system and combine statistical probabilities to help mitigate security risks in construction projects.Объектом исследования является процесс использования информационных технологий в строительной отрасли. Одним из самых проблемных мест является повышение эффективности работы строительной отрасли за счет внедрения цифровых технологий. Проведенные исследования базируются на применении подхода, который реализуется с помощью использования искусственного интеллекта. В ходе исследования использовались методы машинного обучения и нечеткой логики, позволяющие отмечать визуальные данные и анализировать их на предмет потенциальных угроз, а также для сокращения всех возможных рисков. Главная особенность данного подхода заключается в том, что с помощью технологии машинного обучения можно сокращать риски проекта до того, как они повлияют на его прибыль. Так, используя искусственный интеллект в сочетании с BIM-технологией, можно на основе данных в режиме реального времени, прошедшей деятельности и других факторов спрогнозировать работу над строительными проектами таким образом, чтобы оптимизировать строительные процессы. Преимущества, которые можно получить в результате внедрения цифровых процессов, будут еще более очевидными при работе над проектами в будущем по мере того, как искусственный интеллект продолжит анализировать данные компаний. Это связано с тем, что предложенный подход с использованием нечеткой логики имеет ряд особенностей, в частности, чем больше информации обрабатывают алгоритмы машинного обучения, тем сложнее они становятся. А в результате они предоставляют еще больше полезной информации и позволяют принимать еще более грамотные ришення. Благодаря этому обеспечивается возможность максимально снизить риски и эффективно распределить ресурсы при работе над проектами. По сравнению с обычными информационными технологиями, искусственный интеллект можно использовать для создания системы управления безопасностью, основанной на имеющихся знаниях, и объединить статистические вероятности для помощи в снижении рисков безопасности строительных проектов.Об'єктом дослідження є процес використання інформаційної технології в будівельній галузі. Одним з найбільш проблемних місць є підвищення ефективності роботи будівельної галузі за рахунок впровадження цифрових технологій. Проведені дослідження базуються на застосуванні підходу, який реалізується за допомогою використання штучного інтелекту. В ході дослідження використовувалися методи машинного навчання та нечіткої логіки, що дозволяють відзначати візуальні дані та аналізувати їх на предмет потенційних загроз, а також для скорочення всіх можливих ризиків. Головна особливість даного підходу полягає в тому, що за допомогою технології машинного навчання можна скорочувати ризики проєкту до того, як вони вплинуть на його прибуток. Так, використовуючи штучний інтелект у поєднанні з BIM-технологіями, можна на основі даних в режимі реального часу, минулої діяльності та інших факторів спрогнозувати роботу над будівельними проєктами таким чином, щоб оптимізувати будівельні процеси. Переваги, які можна отримати в результаті впровадження цифрових процесів, будуть ще більш очевидними при роботі над проєктами в майбутньому в міру того, як штучний інтелект продовжить аналізувати дані компаній. Це пов'язано з тим, що запропонований підхід з використанням нечіткої логіки має ряд особливостей, зокрема чим більше інформації обробляють алгоритми машинного навчання, тим складнішими вони стають. А в результаті вони надають ще більше корисної інформації та дозволяють приймати ще більш грамотні рішення. Завдяки цьому забезпечується можливість максимально знизити ризики та ефективно розподілити ресурси при роботі над проєктами. У порівнянні зі звичайними інформаційними технологіями, штучний інтелект можна використовувати для створення системи управління безпекою, що базується на наявних знаннях, і об'єднати статистичні ймовірності для допомоги в зниженні ризиків безпеки будівельних проєктів",РС ТЕСHNOLOGY СЕNTЕR,Впровадження штучного інтелекту в будівельну галузь та аналіз існуючих технологій,,,,core
430679651,2021-06-15T00:00:00,"The connectivity and resource-constrained nature of IoT, and in particular
single-board devices, opens up to cybersecurity concerns affecting the
Industrial Internet of Things (IIoT). One of the most important is the presence
of evil IoT twins. Evil IoT twins are malicious devices, with identical
hardware and software configurations to authorized ones, that can provoke
sensitive information leakages, data poisoning, or privilege escalation in
industrial scenarios. Combining behavioral fingerprinting and Machine/Deep
Learning (ML/DL) techniques is a promising solution to identify evil IoT twins
by detecting minor performance differences generated by imperfections in
manufacturing. However, existing solutions are not suitable for single-board
devices because they do not consider their hardware and software limitations,
underestimate critical aspects during the identification performance
evaluation, and do not explore the potential of ML/DL techniques. Moreover,
there is a dramatic lack of work explaining essential aspects to considering
during the identification of identical devices. This work proposes an
ML/DL-oriented methodology that uses behavioral fingerprinting to identify
identical single-board devices. The methodology leverages the different
built-in components of the system, comparing their internal behavior with each
other to detect variations that occurred in manufacturing processes. The
validation has been performed in a real environment composed of identical
Raspberry Pi 4 Model B devices, achieving the identification for all devices by
setting a 50% threshold in the evaluation process. Finally, a discussion
compares the proposed solution with related work and provides important lessons
learned and limitations",,"Can Evil IoT Twins Be Identified? Now Yes, a Hardware Behavioral
  Fingerprinting Methodology",,http://arxiv.org/abs/2106.08209,,core
475535546,2021-01-01T00:00:00,"Grafické karty stojí za úspěchem neuronových sítí v posledních letech a jejich rozšíření do běžného života. Dalším úspěšným oborem umělé inteligence jsou evoluční algoritmy. Jejich schopnost paralelizace je známá již řadu let a byla prakticky aplikována na řadu problémů. Paralelizace evolučních algoritmů se ovšem zpravidla zaměřuje na vícejádrové nebo víceprocesorové stroje, popřípadě na cluster takových strojů. Cílem této práce je prozkoumat možnost paralelizace evolučních algoritmů na grafic- kých kartách. V rámci práce představuji jejich implementaci v knihovně PyTorch, která umožňuje spuštění stejného zdrojového kódu jak na CPU, tak na GPU. Implemen- tace poskytuje nejběžnější evoluční operátory pro genetické algoritmy, algoritmy s reál- ným kódováním a algoritmy optimalizace hejnem částic. Implementace je podrobena intenzivnímu testování a ukazuji, že jejich spuštěním na GPU lze dosáhnout několika- násobnému zrychlení pro středně velké až velké problémy a populace. 1Graphical Processing Units stand for the success of Artificial Neural Networks over the past decade and their broader application in the industry. Another promising field of Artificial Intelligence is Evolutionary Algorithms. Their parallelization ability is well known and has been successfully applied in practice. However, these attempts focused on multi-core and multi-machine parallelization rather than on the GPU. This work explores the possibilities of Evolutionary Algorithms parallelization on GPU. I propose implementation in PyTorch library, allowing to execute EA on both CPU and GPU. The proposed implementation provides the most common evolutionary operators for Genetic Algorithms, Real-Coded Evolutionary Algorithms, and Particle Swarm Op- timization Algorithms. Finally, I show the performance is an order of magnitude faster on GPU for medium and big-sized problems and populations. 1Department of Theoretical Computer Science and Mathematical LogicKatedra teoretické informatiky a matematické logikyMatematicko-fyzikální fakultaFaculty of Mathematics and Physic","Univerzita Karlova, Matematicko-fyzikální fakulta",Paralelizace evolučních algoritmů pomocí GPU,,,,core
403683488,2021-05-01T00:00:00,"abstract: Every communication system has a receiver and a transmitter. Irrespective if it is wired or wireless.The future of wireless communication consists of a massive number of transmitters and receivers. The question arises, can we use computer vision to help wireless communication? To satisfy the high data requirement, a large number of antennas are required. The devices that employ large-antenna arrays have other sensors such as RGB camera, depth camera, or LiDAR sensors.These vision sensors help us overcome the non-trivial wireless communication challenges, such as beam blockage prediction and hand-over prediction.This is further motivated by the recent advances in deep learning and computer vision that can extract high-level semantics from complex visual scenes, and the increasing interest of leveraging machine/deep learning tools in wireless communication problems.[1] 

The research was focused solely based on technology like 3D cameras,object detection and object tracking using Computer vision and compression techniques. The main objective of using computer vision was to make Milli-meter Wave communication more robust, and to collect more data for the machine learning algorithms. Pre-build lossless and lossy compression algorithms, such as FFMPEG, were used in the research. An algorithm was developed that could use 3D cameras and machine learning models such as YOLOV3, to track moving objects using servo motors and low powered computers like the raspberry pi or the Jetson Nano. In other words, the receiver could track the highly mobile transmitter in 1 dimension using a 3D camera. Not only that, during the research, the transmitter was loaded on a DJI M600 pro drone, and then machine learning and object tracking was used to track the highly mobile drone. In order to build this machine learning model and object tracker, collecting data like depth, RGB images and position coordinates were the first yet the most important step. GPS coordinates from the DJI M600 were also pulled and were successfully plotted on google earth. This proved to be very useful during data collection using a drone and for the future applications of position estimation for a drone using machine learning. 

Initially, images were taken from transmitter camera every second,and those frames were then converted to a text file containing hex-decimal values. Each text file was then transmitted from the transmitter to receiver, and on the receiver side, a python code converted the hex-decimal to JPG. This would give an efect of real time video transmission. However, towards the end of the research, an industry standard, real time video was streamed using pre-built FFMPEG modules, GNU radio and Universal Software Radio Peripheral (USRP). The transmitter camera was a PI-camera. More details will be discussed as we further dive deep into this research report. (abstract",,Computer Vision in Milli-Meter Wave Communication,,,,core
395445125,2021-01-01T00:00:00,"A significant amount of research effort is put into studying machine learning (ML) and deep learning (DL) technologies. Real-world ML applications help companies to improve products and automate tasks such as classification, image recognition and automation. However, a traditional “fixed” approach where the system is frozen before deployment leads to a sub-optimal system performance. Systems autonomously experimenting with and improving their own behavior and performance could improve business outcomes but we need to know how this could actually work in practice. While there is some research on autonomously improving systems, the focus on the concepts and theoretical algorithms. However, less research is focused on empirical industry validation of the proposed theory. Empirical validations are usually done through simulations or by using synthetic or manually alteration of datasets. The contribution of this paper is twofold. First, we conduct a systematic literature review in which we focus on papers describing industrial deployments of autonomously improving systems and their real-world applications. Secondly, we identify open research questions and derive a model that classifies the level of autonomy based on our findings in the literature review",'Springer Science and Business Media LLC',Autonomously Improving Systems in Industry: A Systematic Literature Review,10.1007/978-3-030-67292-8_3,,,core
478201354,2021-01-01T00:00:00,"Public healthcare has a history of cautious adoption for artificial intelligence (AI) systems. The rapid growth of data collection and linking capabilities combined with the increasing diversity of the data-driven AI techniques, including machine learning (ML), has brought both ubiquitous opportunities for data analytics projects and increased demands for the regulation and accountability of the outcomes of these projects. As a result, the area of interpretability and explainability of ML is gaining significant research momentum. While there has been some progress in the development of ML methods, the methodological side has shown limited progress. This limits the practicality of using ML in the health domain: the issues with explaining the outcomes of ML algorithms to medical practitioners and policy makers in public health has been a recognized obstacle to the broader adoption of data science approaches in this domain. This study builds on the earlier work which introduced CRISP-ML, a methodology that determines the interpretability level required by stakeholders for a successful real-world solution and then helps in achieving it. CRISP-ML was built on the strengths of CRISP-DM, addressing the gaps in handling interpretability. Its application in the Public Healthcare sector follows its successful deployment in a number of recent real-world projects across several industries and fields, including credit risk, insurance, utilities, and sport. This study elaborates on the CRISP-ML methodology on the determination, measurement, and achievement of the necessary level of interpretability of ML solutions in the Public Healthcare sector. It demonstrates how CRISP-ML addressed the problems with data diversity, the unstructured nature of data, and relatively low linkage between diverse data sets in the healthcare domain. The characteristics of the case study, used in the study, are typical for healthcare data, and CRISP-ML managed to deliver on these issues, ensuring the required level of interpretability of the ML solutions discussed in the project. The approach used ensured that interpretability requirements were met, taking into account public healthcare specifics, regulatory requirements, project stakeholders, project objectives, and data characteristics. The study concludes with the three main directions for the development of the presented cross-industry standard process",'Frontiers Media SA',Interpretability of machine learning solutions in public healthcare : the CRISP-ML approach,10.3389/fdata.2021.660206,https://core.ac.uk/download/478201354.pdf,,core
478097952,2021-02-10T13:01:42,"Beginning and experienced programmers will use this comprehensive guide to persistent memory programming. You will understand how persistent memory brings together several new software/hardware requirements, and offers great promise for better performance and faster application startup times—a huge leap forward in byte-addressable capacity compared with current DRAM offerings. This revolutionary new technology gives applications significant performance and capacity improvements over existing technologies. It requires a new way of thinking and developing, which makes this highly disruptive to the IT/computing industry. The full spectrum of industry sectors that will benefit from this technology include, but are not limited to, in-memory and traditional databases, AI, analytics, HPC, virtualization, and big data. Programming Persistent Memory describes the technology and why it is exciting the industry. It covers the operating system and hardware requirements as well as how to create development environments using emulated or real persistent memory hardware. The book explains fundamental concepts; provides an introduction to persistent memory programming APIs for C, C++, JavaScript, and other languages; discusses RMDA with persistent memory; reviews security features; and presents many examples. Source code and examples that you can run on your own systems are included. What You’ll Learn Understand what persistent memory is, what it does, and the value it brings to the industry Become familiar with the operating system and hardware requirements to use persistent memory Know the fundamentals of persistent memory programming: why it is different from current programming methods, and what developers need to keep in mind when programming for persistence Look at persistent memory application development by example using the Persistent Memory Development Kit (PMDK) Design and optimize data structures for persistent memory Study how real-world applications are modified to leverage persistent memory Utilize the tools available for persistent memory programming, application performance profiling, and debugging Who This Book Is For C, C++, Java, and Python developers, but will also be useful to software, cloud, and hardware architects across a broad spectrum of sectors, including cloud service providers, independent software vendors, high performance compute, artificial intelligence, data analytics, big data, etc",'Springer Science and Business Media LLC',Programming Persistent Memory,10.1007/978-1-4842-4932-1,https://core.ac.uk/download/478097952.pdf,,core
478035639,2021-07-16T06:55:09,"The energy industry is going through challenging times of disruptive changes caused by decarbonization, decentralization, and digitalization. As the energy value chain is restructuring itself to accommodate the growing penetration of renewables, increasing number of independent power producers, and augmented self-consumption, new energy management approaches are required to accomplish energy transition. With the Fourth Industrial Revolution underway, it becomes evident that digitalization is the key to increase energy efficiency and ensure stable, reliable, and secure operations of the electric grid. Due to the energy industry's massive inertia, most energy utilities are missing the real momentum for unleashing large-scale digitalization enabled by Information and Communication Technology (ICT). This thesis proposes a set of ICT-based software applications, models, and tools aiming to bridge the gap between strategic roadmaps focused on the energy industry's digitalization and their actual implementation in real-world scenarios through digital energy services. The research is conducted within the designed ICT-based smart building and smart community frameworks, the modular structure and scalability of which can serve as a backbone for future digital energy management solutions. The introduction of a novel unsupervised load disaggregation approach helps raise awareness of one's energy-related behavior and understand what drives the energy usage in residential households without compromising privacy and security. Showcasing algorithm's performance on real-world datasets from Norway and Germany highlights compliance with state-of-the-art disaggregation accuracy and reduced computational costs. The development of machine learning-based supervised and unsupervised building occupancy forecasting algorithms with prediction accuracies beyond 97% helps identify best-suited windows for energy-saving opportunities and deliver insights into one's presence and absence patterns. Built on top of that occupancy-centric rule-based heating and air conditioning automation algorithm strives to unlock the buildings' massive potential for energy savings without compromising the occupants' thermal comfort. Simulations on real-world datasets collected in Portugal demonstrate more than 15% potential energy savings. Zooming out from smart buildings towards smart communities, we focus on the important role of intelligent green mobility in supporting further digitalization of the electric power sector. To overcome the inconveniences posed by the sparsity of charging infrastructure and facilitate the adoption of Electric Vehicles (EVs), we present a reinforcement learning (RL)-based EV-specific routing method that guarantees paths' energy feasibility in a graph-theoretical context. Consequently, we propose several deep RL algorithms to control EV charging with the aim to increase renewables' self-consumption and EV drivers' satisfaction. Benchmarking against rule-based and model predictive control demonstrates RL's superior computational performance and better fitness for future mobility systems. Finally, we introduce an innovative decentralized blockchain-supported framework that enables secure and reliable accounting of energy exchanges within the smart community. Implementing it in a demonstration site in Switzerland shows blockchain's potential to reduce EV charging costs, transform the market's business model, and facilitate the large-scale deployment of EVs","Lausanne, EPFL",The digitalization of energy systems: towards higher energy efficiency,10.5075/epfl-thesis-9328,,,core
401545191,2021-05-01T07:00:00,"Network Intrusion Detection System (IDS) devices play a crucial role in the realm of network security. These systems generate alerts for security analysts by performing signature-based and anomaly-based detection on malicious network traffic. However, there are several challenges when configuring and fine-tuning these IDS devices for high accuracy and precision. Machine learning utilizes a variety of algorithms and unique dataset input to generate models for effective classification. These machine learning techniques can be applied to IDS devices to classify and filter anomalous network traffic. This combination of machine learning and network security provides improved automated network defense by developing highly-optimized IDS models that utilize unique algorithms for enhanced intrusion detection. Machine learning models can be trained using a combination of machine learning algorithms, network intrusion datasets, and optimization techniques. This study sought to identify which variation of these parameters yielded the best-performing network intrusion detection models, measured by their accuracy, precision, recall, and F1 score metrics. Additionally, this research aimed to validate theoretical models’ metrics by applying them in a real-world environment to see if they perform as expected. This research utilized a quantitative experimental study design to organize a two-phase approach to train and test a series of machine learning models for network intrusion detection by utilizing Python scripting, the scikit-learn library, and Zeek IDS software. The first phase involved optimizing and training 105 machine learning models by testing a combination of seven machine learning algorithms, five network intrusion datasets, and three optimization methods. These 105 models were then fed into the second phase, where the models were applied in a machine learning IDS pipeline to observe how the models performed in an implemented environment. The results of this study identify which algorithms, datasets, and optimization methods generate the best-performing models for network intrusion detection. This research also showcases the need to utilize various algorithms and datasets since no individual algorithm or dataset consistently achieved high metric scores independent of other training variables. Additionally, this research also indicates that optimization during model development is highly recommended; however, there may not be a need to test for multiple optimization methods since they did not typically impact the yielded models’ overall categorization of v success or failure. Lastly, this study’s results strongly indicate that theoretical machine learning models will most likely perform significantly worse when applied in an implemented IDS ML pipeline environment. This study can be utilized by other industry professionals and research academics in the fields of information security and machine learning to generate better highly-optimized models for their work environments or experimental research",Beadle Scholar,Analysis of Theoretical and Applied Machine Learning Models for Network Intrusion Detection,,https://core.ac.uk/download/401545191.pdf,,core
388358393,2021-03-02T00:00:00,"The rise of digital payments has caused consequential changes in the
financial crime landscape. As a result, traditional fraud detection approaches
such as rule-based systems have largely become ineffective. AI and machine
learning solutions using graph computing principles have gained significant
interest in recent years. Graph-based techniques provide unique solution
opportunities for financial crime detection. However, implementing such
solutions at industrial-scale in real-time financial transaction processing
systems has brought numerous application challenges to light. In this paper, we
discuss the implementation difficulties current and next-generation graph
solutions face. Furthermore, financial crime and digital payments trends
indicate emerging challenges in the continued effectiveness of the detection
techniques. We analyze the threat landscape and argue that it provides key
insights for developing graph-based solutions.Comment: arXiv admin note: substantial text overlap with arXiv:2103.0185",,"Graph Computing for Financial Crime and Fraud Detection: Trends,
  Challenges and Outlook",,http://arxiv.org/abs/2103.03227,,core
475699989,2021-06-14T00:00:00,"The chemical engineering sector faces the challenge of meeting the continuously growing demand for their products and services while at the same time ensuring that the industry fully integrates the concepts of sustainable manufacturing. Industry 4.0 provides immense opportunities for the realisation of sustainable manufacturing. Industry 4.0 is a concept that represents the adoption of techniques and processes by industry to gain competitive advantages in domestic and global markets, and Pharma 4.0 is an iteration of Industry 4.0 that relates specifically to the pharmaceutical and biopharmaceutical sectors. The emerging technologies encountered in Pharma and Industry 4.0 facilitate sustainable value creation, through the implementation of agile and smart technologies leading to highly efficient automated processes driven by an integrated manufacturing control strategy. The objective of this research is to quantify the industrial opportunities for enhanced sustainable manufacturing and in parallel evaluate the status of Industry 4.0 within 3rd level chemical engineering education and training establishments such as the National Institute for Bioprocessing Research and Training (NIBRT). The research focuses on cross-linking, as well as the implementation of Industry 4.0 with curriculum development and examines critical aspects of industrial application such as production efficiencies, eco-friendly production, and end-of-life products disposal, providing new educational sustainability benchmarks. Preliminary findings indicate that students have fundamental knowledge regarding core Pharma 4.0 concepts such as Augmented Reality, Cloud Computing and Artificial Intelligence; however, the structure of the 3rd level engineering education needs to adapt, incorporating a more connected curriculum in order to ensure new graduates can successfully engage with a rapidly developing industry and related Pharma 4.0 concepts. Similarly, training institutes indicate an increasing requirement to re-train staff associated with pharmaceutical and biopharmaceutical industries, to upskill 4.0 concepts among the existing workforce",'University College Cork',Industry 4.0 as an enabler for sustainable manufacturing: An educational perspective,,,,core
429740636,2021-05-19T00:00:00,"Ковшова І.О., Бабич Ю.В. СТРАТЕГІЧНІ НАПРЯМИ РОЗВИТКУ ІННОВАЦІЙНОГО МАРКЕТИНГУ НА РИНКУ ВИСОКИХ ТЕХНОЛОГІЙМета. Визначення стратегічних напрямів розвитку інноваційного маркетингу на ринку високих технологій задля збільшення обсягів продажів товарів чи послуг, підвищення ефективності діяльності підприємств та аналізу потенційних ринків збуту.Методика дослідження. В процесі дослідження використано наступні методи: абстрактно-логічний - для формулювання понять інноваційного маркетингу, високих технологій та стратегій інноваційного маркетингу; аналізу та синтезу - для виділення окремих частин інноваційної маркетингової стратегії та правильне застосування їх на ринку високих технологій; економіко-статистичний - задля вивчення стратегічних напрямів інноваційного маркетингу та встановлення кількісного впливу на ринок високих технологій; порівняльний - для співставлення тенденції розвитку стратегічних напрямків інноваційного маркетингу у 2016 та 2020 роках.Результати дослідження. Поглиблено сутність характеристики інноваційного маркетингу на ринку високих технологій та визначено найефективніші стратегічні напрями розвитку інноваційного маркетингу сьогодення: програмне просування, штучний інтелект, відеомаркетинг, персоналізацію та розмовний маркетинг. Обґрунтовано, що для забезпечення інноваційної маркетингової діяльності підприємству важливо постійно проводити моніторинг ринку та нових інструментів, які дозволять вивести товари чи послуги на новий рівень. Встановлено, що виділені п’ять напрямів розвитку інноваційного маркетингу значно впливають на комплекс маркетингових інструментів високотехнологічних підприємств, програми штучного інтелекту включені у всі сфери маркетингового комплексу і активно використовуються маркетологами для обробки, розпізнавання і аналізу голосу, тексту, зображень і процесу прийняття рішень. Обґрунтовано варіанти застосування штучного інтелекту у маркетинговій діяльності підприємств та сформовано основні тренди подальшого розвитку.Наукова новизна результатів дослідження. Обґрунтовано, що зазначені стратегічні напрями розвитку інноваційного маркетингу сьогодення (програмне просування, штучний інтелект, відеомаркетинг, персоналізація та розмовний маркетинг), на відміну від існуючих стратегічних напрямків розвитку, збільшують продажі товарів і послуг підприємств, підвищують поінформованість про бренд, формують довіру та покращують процес комунікації з потенційними і реальними клієнтами.Практична значущість результатів дослідження. Результати проведеного дослідження можуть будуть застосовані високотехнологічними підприємствами в Україні та світі задля підвищення конкурентоспроможності на ринку та виведення продукту чи послуги на новий рівень. Також це дасть змогу збільшити обсягів продажів товарів чи послуг та підвищити ефективність діяльності підприємств.Ключові слова: інноваційний маркетинг, підприємство, напрями розвитку, ринок технологій, відеомаркетинг, штучний інтелект, маркетинг.Kovshova I.O., Babych Yu.V. STRATEGIC DIRECTIONS OF DEVELOPMENT OF INNOVATIVE MARKETING IN THE MARKET OF HIGH TECHNOLOGIESPurpose. The aim of the article is defining strategic directions for the development of innovative marketing in the high-tech market in order to increase sales of goods or services, increase the efficiency of enterprises and analyse potential markets.Methodology of research. The following methods were used in the research process: abstract and logical - to formulate the concepts of innovative marketing, high technologies and strategies of innovative marketing; analysis and synthesis - to identify certain parts of the innovative marketing strategy and their proper application in the high technology market; economic and statistical - to study the strategic directions of innovative marketing and to establish a quantitative impact on the high technology market; comparative - to compare the development trends of strategic areas of innovative marketing in 2016 and 2020.Findings. The essence of the characteristics of innovative marketing in the high-tech market is deepened and the most effective strategic directions of innovative marketing development of today are identified: software promotion, artificial intelligence, video marketing, personalization and conversational marketing. It is substantiated that to ensure innovative marketing activities, it is important for the company to constantly monitor the market and new tools that will bring goods or services to a new level. It is established that the five areas of innovative marketing have been identified as significantly influencing the marketing tools of high-tech enterprises, artificial intelligence programs are included in all areas of the marketing industry and are actively used by marketers to process, recognize and analyse voice, text, images and decision making. Options for the use of artificial intelligence in the marketing activities of enterprises are substantiated and the main trends of further development are formed.Originality. It is substantiated that these strategic directions of development of innovative marketing today (software promotion, artificial intelligence, video marketing, personalization and conversational marketing), in contrast to existing strategic directions of development, increase sales of goods and services, increase brand awareness, build trust and improve communication with potential and real customers.Practical value. The results of the conducted study can be used by high-tech companies in Ukraine and around the world to increase market competitiveness and bring the product or service to a new level. It will also increase sales of goods or services and increase the efficiency of enterprises.Key words: innovative marketing, enterprise, directions of development, technology market, video marketing, artificial intelligence, marketing","'Institute of Economics, Technologies and Entrepreneurship'",СТРАТЕГІЧНІ НАПРЯМИ РОЗВИТКУ ІННОВАЦІЙНОГО МАРКЕТИНГУ НА РИНКУ ВИСОКИХ ТЕХНОЛОГІЙ,10.37332/2309-1533.2021.1-2.17,https://core.ac.uk/download/429740636.pdf,,core
478617377,2021-01-01T00:00:00,"Autonomy plays a key role for more scalable, precise and economic future robotic space missions. Teleoperated space robotic tasks are affected by the communication delay between the spacecraft and the ground station. In the context of robotics in-space assembly and the PULSAR project, a technical demonstrator of the autonomous assembly of a telescope's primary mirror, a learning-based method for such operation is proposed in this work. Conventional robotics assembly methods usually rely on pre-defined motions and strategies, and are engineered for a single use case. Learning-based approaches allow to use the same method for different geometries with little efforts. In this work, a reinforcement learning environment where an industrial robotic arm performs the assembly operation is modelled. Then, open source software components are used to implement the proposed design and validate it in a simulated environment with a precise physics engine. The experiments in the simulator show that the training converges and the trained reinforcement learning agents are able to successfully perform the assemblies of different peg-in-hole geometries and the parts designed for the PULSAR project. These results make reinforcement learning methods worth considering for future real-world experiments and potential in-space assembly missions",,Learning Robust Strategies For In-Space Autonomous Assembly,,,,core
475060069,2021-07-01T00:00:00,"Tea (Camellia sinensis) is one of the most consumed drinks across the world. Based on processing techniques, there are more than 15 000 categories of tea, but the main categories include yellow tea, Oolong tea, Illex tea, black tea, matcha tea, green tea, and sencha tea, among others. Black tea is the most popular among the categories worldwide. During black tea processing, the following stages occur: plucking, withering, cutting, tearing, curling, fermentation, drying, and sorting. Although all these stages affect the quality of the processed tea, fermentation is the most vital as it directly defines the quality. Fermentation is a time-bound process, and its optimum is currently manually detected by tea tasters monitoring colour change, smelling the tea, and tasting the tea as fermentation progresses. This paper explores the use of the internet of things (IoT), deep convolutional neural networks, and image processing with majority voting techniques in detecting the optimum fermentation of black tea. The prototype was made up of Raspberry Pi 3 models  with a Pi camera to take real-time images of tea as fermentation progresses.  We  deployed the prototype in the Sisibo Tea Factory for training, validation, and evaluation.  When the deep learner was evaluated on offline images, it had a perfect precision and accuracy of 1.0 each. The deep learner recorded the highest precision and accuracy of 0.9589 and 0.8646, respectively, when evaluated on real-time images. Additionally, the deep learner recorded an average precision and accuracy of 0.9737 and 0.8953, respectively, when a majority voting technique was applied in decision-making. From the results, it is evident that the prototype can be used to monitor the fermentation of various categories of tea that undergo fermentation, including Oolong and black tea, among others. Additionally, the prototype can also be scaled up by retraining it for use in monitoring the fermentation of other crops, including coffee and cocoa.</p",'Copernicus GmbH',An internet of things (IoT)-based optimum tea fermentation detection model using convolutional neural networks (CNNs) and majority voting techniques,10.5194/jsss-10-153-2021,https://core.ac.uk/download/475060069.pdf,"[{'title': 'Journal of Sensors and Sensor Systems', 'identifiers': ['issn:2194-8771', '2194-8771', '2194-878x', 'issn:2194-878X']}]",core
479487476,2021-10-30T00:00:00,"The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a
mechanism that can evaluate the operational behaviour and security without
negatively affecting the operation of live systems. In this regard, Digital
Twins (DTs) are revolutionizing the CPSs. DTs strengthen the security of CPSs
throughout the product lifecycle, while assuming that the DT data is trusted,
providing agility to predict and respond to real-time changes. However,
existing DTs solutions in CPS are constrained with untrustworthy data
dissemination among multiple stakeholders and timely course correction. Such
limitations reinforce the significance of designing trustworthy distributed
solutions with the ability to create actionable insights in real-time. To do
so, we propose a framework that focuses on trusted and intelligent DT by
integrating blockchain and Artificial Intelligence (AI). Following a hybrid
approach, the proposed framework not only acquires process knowledge from the
specifications of the CPS, but also relies on AI to learn security threats
based on sensor data. Furthermore, we integrate blockchain to safeguard product
lifecycle data. We discuss the applicability of the proposed framework for the
automotive industry as a CPS use case. Finally, we identify the open challenges
that impede the implementation of intelligence-driven architectures in CPSs.Comment: 10 pages, 6 figure",,"Towards Trusted and Intelligent Cyber-Physical Systems: A
  Security-by-Design Approach",,http://arxiv.org/abs/2105.08886,,core
385613724,2021-01-01T00:00:00,"This article belongs to the Special Issue The Artificial Intelligence Technologies for Electric Power SystemsThe scheduling of tasks in a production line is a complex problem that needs to take into account several constraints, such as product deadlines and machine limitations. With innovative focus, the main constraint that will be addressed in this paper, and that usually is not considered, is the energy consumption cost in the production line. For that, an approach based on genetic algorithms is proposed and implemented. The use of local energy generation, especially from renewable sources, and the possibility of having multiple energy providers allow the user to manage its consumption according to energy prices and energy availability. The proposed solution takes into account the energy availability of renewable sources and energy prices to optimize the scheduling of a production line using a genetic algorithm with multiple constraints. The proposed algorithm also enables a production line to participate in demand response events by shifting its production, by using the flexibility of production lines. A case study using real production data that represents a textile industry is presented, where the tasks for six days are scheduled. During the week, a demand response event is launched, and the proposed algorithm shifts the consumption by changing task orders and machine usage.This work has received funding from Portugal 2020 under SPEAR project (NORTE-01-0247-FEDER-040224) and from FEDER Funds through COMPETE program and from National Funds through (FCT) under the project UIDB/00760/2020, and CEECIND/02887/2017.info:eu-repo/semantics/publishedVersio",'MDPI AG',Production Line Optimization to Minimize Energy Cost and Participate in Demand Response Events,10.3390/en14020462,,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core
387304782,2021-01-08T00:00:00,"Recommender systems play a crucial role in helping users to find their
interested information in various web services such as Amazon, YouTube, and
Google News. Various recommender systems, ranging from neighborhood-based,
association-rule-based, matrix-factorization-based, to deep learning based,
have been developed and deployed in industry. Among them, deep learning based
recommender systems become increasingly popular due to their superior
performance.
  In this work, we conduct the first systematic study on data poisoning attacks
to deep learning based recommender systems. An attacker's goal is to manipulate
a recommender system such that the attacker-chosen target items are recommended
to many users. To achieve this goal, our attack injects fake users with
carefully crafted ratings to a recommender system. Specifically, we formulate
our attack as an optimization problem, such that the injected ratings would
maximize the number of normal users to whom the target items are recommended.
However, it is challenging to solve the optimization problem because it is a
non-convex integer programming problem. To address the challenge, we develop
multiple techniques to approximately solve the optimization problem. Our
experimental results on three real-world datasets, including small and large
datasets, show that our attack is effective and outperforms existing attacks.
Moreover, we attempt to detect fake users via statistical analysis of the
rating patterns of normal and fake users. Our results show that our attack is
still effective and outperforms existing attacks even if such a detector is
deployed.Comment: To appear in NDSS 202",'Internet Society',Data Poisoning Attacks to Deep Learning Based Recommender Systems,10.14722/ndss.2021.24525,http://arxiv.org/abs/2101.02644,,core
475648276,2021-08-01T00:00:00,"Nanomaterials are at the core of many scientific discoveries in catalysis, energy and healthcare to name a few. However, their deployment is limited by the lack of reproducible and precise manufacturing technologies on-demand. In this work, a precision automated technology is demonstrated for nanoparticles synthesis with wide-range tunable sizes (≈4–100 nm). Dial-a-particle capabilities are achieved by a combination of a fast integrated multipoint particle sizing combined with a “plug-n-play” modular platform with reactors in series, distributed feed and in situ multipoint analysis. Real-time early growth information accurately predicts the resulting particle properties. Such real-time simple feedback control can overcome repeatability and stability issues associated with controllable (e.g., conditions) and uncontrollable (e.g., fouling, ageing, and impurities) variations leading to self-regulated, highly stable multistage systems with no human intervention even with long residence times (from a few minutes to hours). This is a paradigm shift from machine learning (ML) methodologies, which are restricted to trained networks with rich data sets, impractical in non-reproducible processes and limited to short residence times (e.g., within few minutes). The approach is demonstrated for plasmonic silver and gold nanoparticles showing agile control within minutes, opening the door for automation of more complex multistage procedures such as composites, multielement materials, and particle functionalization",'Organisation for Economic Co-Operation and Development  (OECD)',Dial-A-Particle: Precise Manufacturing of Plasmonic Nanoparticles Based on Early Growth Information—Redefining Automation for Slow Material Synthesis,10.17863/CAM.73868,,,core
478844722,2021-06-21T00:00:00,"Penelitian ini bertujuan untuk menguji dan menganalisis pengaruh financial distress, growth opportunities, debt covenant dan ukuran perusahaan terhadap konservatisme akuntansi pada sektor industri barang konsumsi yang terdaftar di Bursa Efek Indonesia (BEI) periode 2016-2019. Penelitian ini menggunakan explanatory research dengan pendekatan kuantitatif. Sampel dipilih menggunakan metode purposive sampling dan diperoleh 39 perusahaan selama 4 tahun masa penelitian. Metode analisis data yang digunakan dalam penelitian ini adalah analisis regresi linier berganda dengan menggunakan software SPSS 25. Hasil penelitian menunjukkan bahwa financial distress dan debt covenant berpengaruh negatif signifikan terhadap konservatisme akuntansi, kemudian growth opportunities dan ukuran perusahaan berpengaruh positif signifikan terhadap konservatisme akuntansi.DAFTAR PUSTAKAAgustina, Rice, & Stephen. (2016). Akuntansi Konservatisme Pada Perusahaan Manufaktur yang Terdaftar di Bursa Efek Indonesia. Jurnal Dinamika Akuntansi dan Bisnis, 3(1), 1–16. https://doi.org/10.24815/jdab.v3i1.4392Budiandru, Habsari, S. P., & Safuan. (2019). Debt Covenant, Investment Opportunity Set, dan Kepemilikan Manajerial Terhadap Konservatisme Akuntansi Pada Perusahaan Jasa Sub Sektor Property dan Real Estate yang Terdaftar di Bursa Efek Indonesia. Jurnal Ilmiah MEA (Manajemen, Ekonomi, dan Akuntansi), 3(3), 232–247. https://doi.org/10.31955/mea.vol3.iss3.pp232-247Daryatno, A. B., & Santioso, L. (2020). Faktor-Faktor yang Mempengaruhi Penerapan Konservatisme Akuntansi Pada Perusahaan Manufaktur yang Terdaftar di BEI. Jurnal Muara Ilmu Ekonomi dan Bisnis, 4(1), 126–136.Dayyanah, M., & Suryandari, D. (2019). Determinan Konservatisme Akuntansi Perusahaan: Peran Moderasi Financial Distress. SAR (Soedirman Accounting Review) : Journal of Accounting and Business, 4(2), 127–141. https://doi.org/10.20884/1.sar.2019.4.2.2464Dewi, L. P. K., Herawati, N. T., & Sinarwati, N. K. (2014). Faktor-Faktor yang Berpengaruh Terhadap Konservatisme Akuntansi Pada Perusahaan Manufaktur di BEI. E-Journal S1 Ak Universitas Pendidikan Ganesha, 2(1), 63–79.Dewi, N. K. S. L., & Suryanawa, I. K. (2014). Pengaruh Struktur Kepemilikan Manajerial, Leverage, dan Financial Distress Terhadap Konservatisme Akuntansi. E-Jurnal Akuntansi Universitas Udayana, 7(1), 223–234.El-Haq, Z. N. S., Zulpahmi, & Sumardi. (2019). Pengaruh Kepemilikan Manajerial, Kepemilikan Institusional, Growth Opportunities, dan Profitabilitas Terhadap Konservatisme Akuntansi. Jurnal ASET (Akuntansi Riset), 11(2), 315–328. https://doi.org/10.17509/jaset.v11i2.19940Firmasari, D. (2016). Pengaruh Leverage, Ukuran Perusahaan, dan Financial Distress Terhadap Konservatisme Akuntansi (Skripsi), Universitas Airlangga Surabaya, Indonesia.Fitriani, A., & Ruchjana, E. T. (2020). Pengaruh Financial Distress dan Leverage Terhadap Konservatisme Akuntansi pada Perusahaan Retail di Indonesia. Equilibrium: Jurnal Ekonomi-Manajemen-Akuntansi, 16(2), 82–93. https://doi.org/10.24843/eja.2020.v30.i07.p05Ghozali, I. (2018). Aplikasi Analisis Multivariate dengan Program IBM SPSS 25 Edisi 9. Semarang: Badan Penerbit Universitas Diponegoro.Gusyanto, A. (2020). Analisis Faktor-Faktor yang Mempengaruhi Penerapan Konservatisme Akuntansi ( Studi Empiris Pada Perusahaan Manufaktur yang Terdaftar di Bursa Efek Indonesia Tahun 2016-2018 ) (Skripsi), Universitas Tanjungpura Pontianak, IndonesiaHakim, M. Z. (2017). Determinan Konservatisme Akuntansi Pada Industri Dasar dan Kimia Periode 2012 - 2014. Competitive Jurnal Akuntansi dan Keuangan, 1(1), 111–135. https://doi.org/10.31000/competitive.v1i1.110Haryadi, E., Sumiati, T., & Umdiana, N. (2020). Financial Distress, Leverage, Persistensi Laba dan Ukuran Perusahaan Terhadap Konservatisme Akuntansi. Competitive Jurnal Akuntansi dan Keuangan, 4(2), 66–78.Iskandar, O. R., & Sparta. (2019). Pengaruh Debt Covenant, dan Political Cost Terhadap Konservatisme Akuntansi. Equity: Jurnal Ekonomi, Manajemen, Akuntansi, 22(1), 47–62. https://doi.org/10.34209/equ.v22i1.896Karantika, M. D., & Sulistyawati, A. I. (2018). Konservatisme Akuntansi dan Determinasinya. Fokus Ekonomi : Jurnal Ilmiah Ekonomi, 13(2), 163–185. https://doi.org/10.34152/fe.13.2.163-185Kodriyah, & Framita, D. S. (2019). Menguji Dampak Financial Distress dan Leverage Terhadap Konservatisme Akuntansi. Akuntoteknologi : Jurnal Ilmiah Akuntansi Dan Teknologi, 11(2), 1–8.Lestari, A. (2019). Pengaruh Ukuran Perusahaan, Potensi Kesulitan Keuangan dan Kesempatan Bertumbuh Terhadap Konservatisme Akuntansi yang dimoderasi Oleh Leverage (Skripsi), Universitas Islam Negeri Syarif Hidayatullah, Indonesia.Noviantari, N. W., & Ratnadi, N. M. D. (2015). Pengaruh Financial Distress, Ukuran Perusahaan, dan Leverage Pada Konservatisme Akuntansi. E-Jurnal Akuntansi, 11(3), 646–660.Oktomegah, C. (2012). Faktor-Faktor yang Mempengaruhi Penerapan Konservatisme Pada Perusahaan Manufaktur di BEI. Jurnal Ilmiah Mahasiswa Akuntansi, 1(1), 36–42.Pambudi, J. E. (2017). Pengaruh Kepemilikan Manajerial dan Debt Covenant Terhadap Konservatisme Akuntansi. Competitive Jurnal Akuntansi dan Keuangan, 1(1), 87–110. https://doi.org/10.31000/competitive.v1i1.109Rahardja, C., & Herawaty, V. (2019). Pengaruh Manajemen Laba, Sales Growth, Profitabilitas, Leverage, dan Ukuran Perusahaan Terhadap Prudence dengan Kepemilikan Manajerial sebagai Variabel Moderasi, Prosiding Seminar Online Cendekiawan (hlm. 2.27.1-2.27.6). Indonesia: Universitas Trisakti.Rivandi, M. (2019). Pengaruh Debt Covenant dan Growth Opportunity Terhadap Konservatisme Akuntansi. Economac, 3(5).Rivandi, M., & Ariska, S. (2019). Pengaruh Intensitas Modal, Dividend Payout Ratio dan Financial Distress Terhadap Konservatisme Akuntansi. Jurnal Benefita, 4(1), 104–114. https://doi.org/10.22216/jbe.v1i1.3850Sari, W. P. (2020). The Effect of Financial Distress and Growth Opportunities on Accounting Conservatism with Litigation Risk as Moderated Variables in Manufacturing Companies Listed on BEI. Budapest International Research and Critics Institute-Journal (BIRCI-Journal) : Humanities and Social Sciences, 3(1), 588–597. https://doi.org/10.33258/birci.v3i1.812Savitri, E. (2016a). Konservatisme Akuntansi. Yogyakarta: Pustaka Saliha.Savitri, E. (2016b). Pengaruh Struktur Kepemilikan Institusional, Debt Covenant dan Growth Opportunities Terhadap Konservatisme Akuntansi. Jurnal Al-Iqtishad, 1(12), 39–54. https://doi.org/10.24014/jiq.v12i1.4444Sinambela, M. O. E., & Almilia, L. S. (2018). Faktor-Faktor yang Mempengaruhi Konservatisme Akuntansi. Jurnal Ekonomi dan Bisnis, 21(2), 289–312. https://doi.org/10.24914/jeb.v21i2.1788Sudaryono. (2017). Metodologi Penelitian. Depok: PT. Raja Grafindo Persada.Sugiarto, N., & Nurhayati, I. (2017). Faktor-Faktor yang Mempengaruhi Konservatisme Akuntansi Pada Perusahaan Manufaktur yang Terdaftar di Bursa Efek Indonesia Tahun 2014-2016. Jurnal Dinamika Akuntansi, Keuangan dan Perbankan, 6(2), 102–116.Sulastri, A., Mulyati, S., & Icih. (2018). Analisis Pengaruh ASEAN Corporate Governance Scorecard, Leverage, Size, Growth Opportunities, dan Earnings Pressure Terhadap Konservatisme Akuntansi (Studi Kasus Pada Perusahaan Top Rank 50 ASEAN Corporate Governance Scorecard di Indonesia yang Terdaftar di Bursa Efek Indonesia Pada Tahun 2013-2015. Accruals (Accounting Research Journal of Sutaatmadja), 2(1), 41–67. https://doi.org/10.35310/accruals.v2i1.6Sulastri, S., & Anna, Y. D. (2018). Pengaruh Financial Distress dan Leverage Terhadap Konservatisme Akuntansi. Akuisisi: Jurnal Akuntansi, 14(1), 59–69. https://doi.org/10.24127/akuisisi.v14i1.251Suryandari, E., & Priyanto, R. E. (2012). Pengaruh Risiko Litigasi dan Tingkat Kesulitan Keuangan Perusahaan Terhadap Hubungan Antara Konflik Kepentingan dan Konservatisme Akuntansi. Jurnal Akuntansi dan Investasi, 12(2), 161–174. http://journal.umy.ac.id/index.php/ai/article/view/681Susanto, B., & Ramadhani, T. (2016). Faktor-Faktor yang Memengaruhi Konservatisme (Studi pada Perusahaan Manufaktur yang Terdaftar di BEI 2010-2014). Jurnal Bisnis dan Ekonomi (JBE), 23(2), 142–151. https://spcom.upc.edu/documents/file_1749.pdfSuwardjono. (2016). Teori Akuntansi Perekayasaan Pelaporan Keuangan Edisi Ketiga. Yogyakarta: BPFE-Yogyakarta.Syifa, H. M., Kristanti, F. T., & Dillak, V. J. (2017). Financial Distress, Kepemilikan Institusional, Profitabilitas Terhadap Konservatisme Akuntansi. Jurnal Riset Akuntansi Kontemporer (JRAK), 9(1), 1–6. https://doi.org/10.23969/jrak.v9i1.361Tista, K. W. N., & Suryanawa, I. K. (2017). Pengaruh Ukuran Perusahaan dan Potensi Kesulitan Keuangan Pada Konservatisme Akuntansi dengan Leverage sebagai Pemoderasi. E-Jurnal Akuntansi, 18(3), 2477–2504.Ursula, E. A., & Adhivinna, V. V. (2018). Pengaruh Kepemilikan Manajerial, Ukuran Perusahaan, Leverage, dan Growth opportunities Terhadap Konservatisme Akuntansi. Jurnal Akuntansi, 6(2), 194–206. https://doi.org/10.24964/ja.v6i2.643Viola, & Diana, P. (2016). Pengaruh Kepemilikan Managerial, Leverage, Financial Distress dan Kepemilikan Publik Terhadap Konservatisme Akuntansi. ULTIMA Accounting, 8(1), 22–36. https://doi.org/10.31937/akuntansi.v8i1.575Wanialisa, M., & Alam, I. K. (2021). Determinan Laporan Keuangan Terhadap Financial Distress Pada Perusahaan Manufaktur Sektor Barang Konsumsi yang Terdaftar di Bursa Efek Indonesia Periode 2014-2018. Jurnal IKRA-ITH Ekonomika, 4(1), 19–29.Wisuandari, N. K. P., & Putra, I. N. W. A. (2018). Pengaruh Tingkat Kesulitan Keuangan dan Konflik Kepentingan pada Konservatisme Akuntansi dengan Risiko Litigasi Sebagai Pemoderasi. Jurnal Akuntansi Universitas Udayana, 23(2), 1521–1547",Jurnal Kajian Ilmiah Akuntansi Fakultas Ekonomi UNTAN (KIAFE),"Pengaruh Financial Distress, Growth Opportunities, Debt Covenant dan Ukuran Perusahaan Terhadap Konservatisme Akuntansi (Studi Empiris Pada Perusahaan Sektor Industri Barang Konsumsi yang Terdaftar di Bursa Efek Indonesia Periode 2016-2019)",,,,core
479420270,2021-01-01T00:00:00,"In livestock operations, systematically monitoring animal body weight, bio-metric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves mas-sive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phe-notypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and its respective deep learning algorithms implemented in Animal Science studies. Second, we reviewed the published research studies in Animal Science, which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an33improved predictive ability in livestock applications and a faster inference.34However, only a few articles fully described the deep learning algorithms and its implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missed. We summarized peer-reviewed articles by computer vision tasks38(image classification, object detection, and object segmentation), deep learn-39ing algorithms, species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions.bitstream/item/226878/1/Review-deep.pd","Livestock Science, v. 253, 104700, 2021.",A review of deep learning algorithms for computer vision systems in livestock.,,https://core.ac.uk/download/479420270.pdf,,core
479679302,2021-01-01T00:00:00,"Liquid analysis is key to track conformity with the strict process quality standards of sectors like

food, beverage, and chemical manufacturing. In order to analyse product qualities online and at the very point of interest, automated monitoring systems must satisfy strong requirements in terms of miniaturization, energy autonomy, and real time operation. Towards this goal, we present the first implementation of artificial taste running on neuromorphic hardware for continuous edge monitoring applications. We used a solid-state electrochemical microsensor array to acquire multivariate, time-varying chemical measurements, employed temporal filtering to enhance sensor readout dynamics, and deployed a rate-based, deep convolutional spiking neural network to efficiently fuse the electrochemical sensor data. To evaluate performance we created MicroBeTa (Microsensor Beverage Tasting), a new dataset for beverage classification incorporating seven hours of temporal recordings performed over three days, including sensor drifts and sensor replacements. Our implementation of artificial taste is 15x more energy efficient on inference tasks than similar convolutional architectures running on other commercial, low power edge-AI inference devices, achieving over 178x lower latencies than the sampling period of the sensor readout, and high accuracy (>97%) on a single Intel Loihi neuromorphic research processor included in a USB stick form factor.N",'Frontiers Media SA',Real-time Edge Neuromorphic Tasting from Chemical Microsensor Arrays,,,"[{'title': 'Frontiers in Neuroscience', 'identifiers': ['issn:1662-4548', '1662-4548']}]",core
479906933,2021-11-15T12:07:35,"The presented paper study using augmented reality for converting industrial chemical spraying robot into soap bubbles robot as an interactive game augmented robot. Also, a fuzzy logic control system (FLCS) designed and structed as an artificial intelligence tool for soap bubbles robot, and it’s implemented to programming logic control (automated operating system). Fuzzy system used for predicting level of soap-water mixture inside the robot to conform best robot performance",Arab Journals Platform,Using Fuzzy Logic Control System as an Artificial Intelligence Tool to Design Soap Bubbles Robot as a Type of Interactive Games,,https://core.ac.uk/download/479906933.pdf,,core
389671201,2021-01-01T00:00:00,"How important is the human vision? Simply speaking, it is central for domain\ua0related users to understand a design, a framework, a process, or an application\ua0in terms of human-centered cognition. This thesis focuses on facilitating visual\ua0comprehension for users working with specific industrial processes characterized\ua0by tomography. The thesis illustrates work that was done during the past two\ua0years within three application areas: real-time condition monitoring, tomographic\ua0image segmentation, and affective colormap design, featuring four research papers\ua0of which three published and one under review.The first paper provides effective deep learning algorithms accompanied by\ua0comparative studies to support real-time condition monitoring for a specialized\ua0microwave drying process for porous foams being taken place in a confined chamber.\ua0The tools provided give its users a capability to gain visually-based insights\ua0and understanding for specific processes. We verify that our state-of-the-art\ua0deep learning techniques based on infrared (IR) images significantly benefit condition\ua0monitoring, providing an increase in fault finding accuracy over conventional\ua0methods. Nevertheless, we note that transfer learning and deep residual network\ua0techniques do not yield increased performance over normal convolutional neural\ua0networks in our case.After a drying process, there will be some outputted images which are reconstructed\ua0by sensor data, such as microwave tomography (MWT) sensor. Hence,\ua0how to make users visually judge the success of the process by referring to the\ua0outputted MWT images becomes the core task. The second paper proposes an\ua0automatic segmentation algorithm named MWTS-KM to visualize the desired low\ua0moisture areas of the foam used in the whole process on the MWT images, effectively\ua0enhance users\u27understanding of tomographic image data. We also prove its\ua0performance is superior to two other preeminent methods through a comparative\ua0study.To better boost human comprehension among the reconstructed MWT image,\ua0a colormap deisgn research based on the same segmentation task as in the second\ua0paper is fully elaborated in the third and the fourth papers. A quantitative\ua0evaluation implemented in the third paper shows that different colormaps can\ua0influence the task accuracy in MWT related analytics, and that schemes autumn,\ua0virids, and parula can provide the best performance. As the full extension of\ua0the third paper, the fourth paper introduces a systematic crowdsourced study,\ua0verifying our prior hypothesis that the colormaps triggering affect in the positiveexciting\ua0quadrant in the valence-arousal model are able to facilitate more precise\ua0visual comprehension in the context of MWT than the other three quadrants.\ua0Interestingly, we also discover the counter-finding that colormaps resulting in\ua0affect in the negative-calm quadrant are undesirable. A synthetic colormap design\ua0guideline is brought up to benefit domain related users.In the end, we re-emphasize the importance of making humans beneficial in every\ua0context. Also, we start walking down the future path of focusing on humancentered\ua0machine learning(HCML), which is an emerging subfield of computer\ua0science which combines theexpertise of data-driven ML with the domain knowledge\ua0of HCI. This novel interdisciplinary research field is being explored to support\ua0developing the real-time industrial decision-support system",,The Magic of Vision: Understanding What Happens in the Process,,https://core.ac.uk/download/389671201.pdf,,core
401623799,2021-03-29T00:00:00,"Purpose: This paper discusses the impact of COVID-19 on the Visual Arts industry in Malaysia. In general, this pandemic has affected various forms of artistic activities and the income of visual arts artists and galleries. The cancellation of art projects and exhibitions has greatly affected the artist's source of income as well as disrupted the sale of works and forms of art appreciation. The crisis has also opened up a new form to the visual arts industry by looking at alternative approaches to the continuity of the arts field by switching to virtual or online methods. This emerging crisis of COVID-19 might be the starting point for all art practitioners including artists, art critics, galleries/museums, collectors, and curators in using the online space to continue to capitalize on and expand the Visual Arts industry.
Design/methodology/approach: Review approach.
Findings: The COVID-19 pandemic has made a huge impact on the country's Visual Arts industry where a wide range of art activities cannot be implemented and opened up opportunities for online activities
Practical implications: Exhibition and sale of works through online approach has become one of the main methods that support the Visual Arts industry with the application of a combination of the latest technologies such as VR and AI that enable the representation of real experiences in the context of art appreciation.
Originality/value: This paper is original.
Paper type: This paper can be categorized as a viewpoin",'Narotama University',Covid -19: The Impact On Malaysian Visual Arts Scene,10.29138/ijebd.v4i2.1117,https://core.ac.uk/download/401623799.pdf,,core
478585512,2021-10-07T00:00:00,"Active Learning (AL) is a powerful tool to address modern machine learning
problems with significantly fewer labeled training instances. However,
implementation of traditional AL methodologies in practical scenarios is
accompanied by multiple challenges due to the inherent assumptions. There are
several hindrances, such as unavailability of labels for the AL algorithm at
the beginning; unreliable external source of labels during the querying
process; or incompatible mechanisms to evaluate the performance of Active
Learner. Inspired by these practical challenges, we present a hybrid query
strategy-based AL framework that addresses three practical challenges
simultaneously: cold-start, oracle uncertainty and performance evaluation of
Active Learner in the absence of ground truth. While a pre-clustering approach
is employed to address the cold-start problem, the uncertainty surrounding the
expertise of labeler and confidence in the given labels is incorporated to
handle oracle uncertainty. The heuristics obtained during the querying process
serve as the fundamental premise for accessing the performance of Active
Learner. The robustness of the proposed AL framework is evaluated across three
different environments and industrial settings. The results demonstrate the
capability of the proposed framework to tackle practical challenges during AL
implementation in real-world scenarios.Comment: 15 pages, 4 figures, 6 table",,"Addressing practical challenges in Active Learning via a hybrid query
  strategy",,http://arxiv.org/abs/2110.03785,,core
479305301,2021-07-01T00:00:00,"The publication presents a picture of modern steelworks that is evolving from steelworks 3.0 to steelworks 4.0. The paper was created on the basis of secondary sources of information (desk research). The entire publication concerns the emerging opportunities for the development of the steel producers to Industry 4.0 and the changes already implemented in the steel plants. The collected information shows the support environment for changes in the steel sector (EU programs), the levels of evolution of steel mills, along with the areas of change in the steel industry and implemented investment projects. The work consists of a theoretical part based on a literature review and a practical part based on case studies. The work ends with a discussion in which the staged and segmented nature of the changes introduced in the analyzed sector is emphasized. Based on the three case studies described in the paper, a comparative analysis was conducted between them. When we tried to compare methods used in the three analyzed steel producers (capital groups): ArcelorMittal, Thyssenkrupp, and Tata Steel Group, it can be seen that in all organizations, the main problem connected with steelworks 4.0 transition is the digitalization of all processes within an organization and in the entire supply chain. This is realized using various tools and methods but they are concentrated on using technologies and methods such as artificial intelligence, drones, virtual reality, full automatization, and industrial robots. The effects are connected to better relations with customers, which leads to an increase in customer satisfaction and the organizations’ profit. The steel industry will undergo further strong changes, bringing it closer to Industry 4.0 because it occupies an important place in the economies of many countries due to the strong dependence of steel producers on the markets of the recipients (steel consumers). Steel is the basic material needed to make many products, and its properties have been valued for centuries. In addition, steel mills with positive economic, social, and environmental aspects are part of the concept of sustainability for industries and economies",'MDPI AG',Transitioning of Steel Producers to the Steelworks 4.0—Literature Review with Case Studies,10.3390/en14144109,,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core
477674348,2021-01-01T00:00:00,"The progress in technology development over the past decades, both with respect to software and hardware, offers the vision of automated vehicles as means of achieving zero fatalities in traffic. However, the promises of this new technology – an increase in road safety, traffic efficiency, and user comfort – can only be realized if this technology is smoothly introduced into the existing traffic system with all its complexities, constraints, and requirements. SHAPE- IT will contribute to this major undertaking by addressing research questions relevant for the development and introduction of automated vehicles in urban traffic scenarios. Previous research has pointed out several research areas that need more attention for a successful implementation and deployment of human-centred vehicle automation in urban environments.In SHAPE-IT, for example, a better understanding of human behaviour and the underlying psychological mechanisms will lead to improved models of human behaviour that can help to predict the effects of automated systems on human behaviour already during system development. Such models can also be integrated into the algorithms of automated vehicles, enabling them to better understand the human interaction partners’ behaviours.Further, the development of vehicle automation is much about technology (software and hardware), but the users will be humans and they will interact with humans both inside and outside of the vehicle. To be successful in the development of automated vehicles functionalities, research must be performed on a variety of aspects. Actually, a highly interdisciplinary team of researchers, bringing together expertise and background from various scientific fields related to traffic safety, human factors, human-machine interaction design and evaluation, automation, computational modelling, and artificial intelligence, is likely needed to consider the human-technology aspects of vehicle automation.Accordingly, SHAPE-IT has recruited fifteen PhD candidates (Early Stage Researchers – ESRs), that work together to facilitate this integration of automated vehicles into complex urban traffic by performing research to support the development of transparent, cooperative, accepted, trustworthy, and safe automated vehicles. With their (and their supervisors’) different scientific background, the candidates bring different theoretical concepts and methodological approaches to the project. This interdisciplinarity of the project team offers the unique possibility for each PhD candidate to address research questions from a broad perspective – including theories and methodological approaches of other interrelated disciplines. This is the main reason why SHAPE-IT has been funded by the European Commission’s Marie Skłodowska-Curie Innovative Training Network (ITN) program that is aimed to train early state researchers in multidisciplinary aspects of research including transferable skills. With the unique scope of SHAPE-IT, including the human-vehicle perspective, considering different road-users (inside and outside of the vehicle), addressing for example trust, transparency, and safety, and including a wide range of methodological approaches, the project members can substantially contribute to the development and deployment of safe and appreciated vehicle automation in the cities of the future.To achieve the goal of interdisciplinary research, it is necessary to provide the individual PhD candidate with a starting point, especially on the different and diverse methodological approaches of the different disciplines. The empirical, user-centred approach for the development and evaluation of innovative automated vehicle concepts is central to SHAPE- IT. This deliverable (D1.1 “Methodological Framework for Modelling and Empirical Approaches”) provides this starting point. That is, this document provides a broad overview of approaches and methodologies used and developed by the SHAPE-IT ESRs during their research. The SHAPE-IT PhD candidates, as well as other researchers and developers outside of SHAPE-IT, can use this document when searching for appropriate methodological approaches, or simply get a brief overview of research methodologies often employed in automated vehicle research.The first chapter of the deliverable shortly describes the major methodological approaches to collect data relevant for investigating road user behaviour. Each subchapter describes one approach, ranging from naturalistic driving studies to controlled experiments in driving simulators, with the goal to provide the unfamiliar reader with a broad overview of the approach, including its scope, the type of data collected, and its limitations. Each subchapter ends with recommendations for further reading – literature that provide much more detail and examples.The second chapter explains four different highly relevant tools for data collection, such as interviews, questionnaires, physiological measures, and as other current tools (the Wizard of Oz paradigm and Augmented and Virtual Reality). As in the first chapter this chapter provides the reader with information about advantages and disadvantages of the different tools and with proposed further readings.The third chapter deals with computational models of human/agent interaction and presents in four subchapters different modelling approaches, ranging from models based on psychological mechanisms, rule-based and artificial intelligence models to simulation models of traffic interaction.The fourth chapter is devoted to Requirements Engineering and the challenge of communicating knowledge (e.g., human factors) to developers of automated vehicles. When forming the SHAPE-IT proposal it was identified that there is a lack of communication of human factors knowledge about the highly technical development of automated vehicles. This is why it is highly important that the SHAPE-IT ESRs get training in requirement engineering. Regardless of the ESRs working in academia or industry after their studies it is important to learn how to communicate and disseminate the findings to engineers.The deliverable ends with the chapter “Method Champions”. Here the expertise and association of the different PhD candidates with the different topics are made explicit to facilitate and encourage networking between PhDs with special expertise and those seeking support, especially with regards to methodological questions",SHAPE-IT Consortium,Methodological Framework for Modelling and Empirical Approaches (Deliverable D1.1 in the H2020 MSCA ITN project SHAPE-IT),10.17196/shape-it/2021/02/d1.1,,,core
430687431,2021-09-01T00:00:00,"Readmission of patients within a specific period after their discharge from a hospital is a cause of concern for the healthcare industry due to the cost involved. Most of the work done for predicting such readmissions using machine learning (ML) have been based on EHR, claims or authorization data from specific sources, which are mostly snapshot data at one static point in time and hence delayed. ADT being dynamic as the data is available instantaneous on occurrence of a medical event/visit adds value. Our goal is to utilize machine learning on unlabeled ADT data to identify patients who are at a high risk of being readmitted. We approached the problem in three parts. First, we labeled patient events using logical rules and finalized one of many readmission definitions that was more encapsulating of varied scenarios. Second, feature engineering was done which encapsulates the longitudinal timeline of each patient in a representative way considering all the contextual information. Third, we developed an automated machine learning pipeline which takes modeling inputs from the user, runs various models to generate readmission prediction, does a cross validation and returns the best model. We tried multiple combinations of models and cross-validation strategies and decided on a random forest model with specific hyper-parameter values and to be the most effective method to classify high risk patients. It had a test AUC-ROC of 72% which is better than quite a few industry standards. The model currently implemented in the client environment identifies the high-risk patients in real-time to care nurses who in turn take proper interventions to reduce their chances of readmission",'Elsevier BV',"Using hospital Admission, Discharge &amp; Transfer (ADT) data for predicting readmissions",10.1016/j.mlwa.2021.100055,,"[{'title': None, 'identifiers': ['issn:2666-8270', '2666-8270']}]",core
401630469,2021-04-28T00:00:00,"In Cloud Computing, the cloud serves as a central data hub for the Industrial Internet of Things' (IIoT) data and is deployed in diverse application fields, e.g., Smart Grid or Smart Manufacturing. Therefore, the aggregated and contextualized data is bundled in a central data hub, bringing tremendous cybersecurity advantages. Given the threat landscape in IIoT systems, especially SMEs (small and medium-sized enterprises)  need to be prepared regarding their cybersecurity, react quickly, and strengthen their overall cybersecurity.  For instance, with the application of machine learning algorithms,  security-related data can be analyzed predictively in order to be able to ward off a potential attack at an early stage. Since modern reference architectures for IIoT systems, such as RAMI 4.0 or IIRA, consider cybersecurity approaches on a high level and SMEs lack financial funds and knowledge, this paper conceptualizes a security analytics service used as a security add-on to these reference architectures.  Thus, this paper conceptualizes a flexible security analytics service that implements security capabilities with flexible analytical techniques that fit specific SMEs' needs. The security analytics service is also evaluated with a real-world use case",'Association for Computing Machinery (ACM)',A Flexible Security Analytics Service for the Industrial IoT,10.1145/3445969.3450427,,,core
427134564,,"Due to the massive developments in information technology, the world as we know it has been massively changing and evolving. This revolution has been named Industrial Revolution 4.0. IR4.0 shrinks the gap between the digital, more technical world and the physical world. The higher education institutes are now being forced to move into a more digital form of education because if they are not, they are found institutes that are too stubborn to move on from the traditional method of education. The more modern technologies such as data analysis, artificial intelligence and other technologies such as cloud computing need to be concentrated on while going through digital transformation. This paper includes an outlook on digitization, benefits of digital transformation, challenges of implementation, how to have a smooth transition to a more digital environment and the different mechanisms that are in place, that could be improved. Because of the fact that as the generations pass, more and more of the students are getting tech savvy, so, if there is any time to implement digital transformation, it is now. This study will be focused on the Maldives and as it is a small island nation, it comes with a set of challenges that other countries most probably do not have","Journal of Applied Technology and Innovation, APU Press",Digital transformation of the higher education sector in Maldives,,,,core
387306224,2021-01-11T00:00:00,"The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community",,"System Design for a Data-driven and Explainable Customer Sentiment
  Monitor",,http://arxiv.org/abs/2101.04086,,core
389870030,2021-01-01T00:00:00,"One of the most actual and consistent drivers for the industry is sustainability, which includes three main pillars: environment, economics, and society. While numerous methods for environmental and economic sustainability assessment have been proposed, social sustainability assessment is still lacking in structured methods and tools, although human has always played a key role. Moreover, technological development is pushing the industrial world toward a new paradigm, the \u201cIndustry 4.0,\u201d which embeds topics such as data digitalization, cyber-physical systems, and machine learning. It entails significant changes in human resources management, without reducing their importance. Humans were part of the manufacturing system from the first industrial revolution, and no automation or digitalization can be possible without humans. The industry can no longer underestimate the reasonable application of human factors and ergonomics principles to the workplace. For this purpose, the paper provides a novel transdisciplinary engineering method to measure and promote social sustainability on production sites. It exploits Internet of Things technology to support the (re)design of manufacturing processes and plants toward human-centered connected factories. To improve the workers' well-being has positive effects on their health, satisfaction, and performance. The method has been implemented in a real industrial case study within the footwear industry. The sole finishing process has been analyzed from different perspectives to solve ergonomics-related problems and implement effective improvement strategies",'Oxford University Press (OUP)',A method to improve workers' well-being toward human-centered connected factories,10.1093/jcde/qwaa047,,,core
475647455,2021-07-28T09:01:56,"Abstract: Nanomaterials are at the core of many scientific discoveries in catalysis, energy and healthcare to name a few. However, their deployment is limited by the lack of reproducible and precise manufacturing technologies on‐demand. In this work, a precision automated technology is demonstrated for nanoparticles synthesis with wide‐range tunable sizes (≈4–100 nm). Dial‐a‐particle capabilities are achieved by a combination of a fast integrated multipoint particle sizing combined with a “plug‐n‐play” modular platform with reactors in series, distributed feed and in situ multipoint analysis. Real‐time early growth information accurately predicts the resulting particle properties. Such real‐time simple feedback control can overcome repeatability and stability issues associated with controllable (e.g., conditions) and uncontrollable (e.g., fouling, ageing, and impurities) variations leading to self‐regulated, highly stable multistage systems with no human intervention even with long residence times (from a few minutes to hours). This is a paradigm shift from machine learning (ML) methodologies, which are restricted to trained networks with rich data sets, impractical in non‐reproducible processes and limited to short residence times (e.g., within few minutes). The approach is demonstrated for plasmonic silver and gold nanoparticles showing agile control within minutes, opening the door for automation of more complex multistage procedures such as composites, multielement materials, and particle functionalization",'Organisation for Economic Co-Operation and Development  (OECD)',Dial‐A‐Particle: Precise Manufacturing of Plasmonic Nanoparticles Based on Early Growth Information—Redefining Automation for Slow Material Synthesis,10.17863/CAM.72988,https://core.ac.uk/download/475647455.pdf,,core
387302273,2021-01-05T00:00:00,"A growing demand is witnessed in both industry and academia for employing
Deep Learning (DL) in various domains to solve real-world problems. Deep
Reinforcement Learning (DRL) is the application of DL in the domain of
Reinforcement Learning (RL). Like any software systems, DRL applications can
fail because of faults in their programs. In this paper, we present the first
attempt to categorize faults occurring in DRL programs. We manually analyzed
761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues)
developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl,
Tensorforce) and identified faults reported by developers/users. We labeled and
taxonomized the identified faults through several rounds of discussions. The
resulting taxonomy is validated using an online survey with 19
developers/researchers. To allow for the automatic detection of faults in DRL
programs, we have defined a meta-model of DRL programs and developed DRLinter,
a model-based fault detection approach that leverages static analysis and graph
transformations. The execution flow of DRLinter consists in parsing a DRL
program to generate a model conforming to our meta-model and applying detection
rules on the model to identify faults occurrences. The effectiveness of
DRLinter is evaluated using 15 synthetic DRLprograms in which we injected
faults observed in the analyzed artifacts of the taxonomy. The results show
that DRLinter can successfully detect faults in all synthetic faulty programs",,"Faults in Deep Reinforcement Learning Programs: A Taxonomy and A
  Detection Approach",,http://arxiv.org/abs/2101.00135,,core
475068163,2021-06-01T00:00:00,"In modern production environments, advanced and intelligent process monitoring strategies are required to enable an unambiguous diagnosis of the process situation and thus of the final component quality. In addition, the ability to recognize the current state of product quality in real-time is an important prerequisite for autonomous and self-improving manufacturing systems. To address these needs, this study investigates a novel ensemble deep learning architecture based on convolutional neural networks (CNN), gated recurrent units (GRU) combined with high-performance classification algorithms such as k-nearest neighbors (kNN) and support vector machines (SVM). The architecture uses spatio-temporal features extracted from infrared image sequences to locate critical welding defects including lack of fusion (false friends), sagging, lack of penetration, and geometric deviations of the weld seam. In order to evaluate the proposed architecture, this study investigates a comprehensive scheme based on classical machine learning methods using manual feature extraction and state-of-the-art deep learning algorithms. Optimal hyperparameters for each algorithm are determined by an extensive grid search. Additional work is conducted to investigate the significance of various geometrical, statistical and spatio-temporal features extracted from the keyhole and weld pool regions. The proposed method is finally validated on previously unknown welding trials, achieving the highest detection rates and the most robust weld defect recognition among all classification methods investigated in this work. Ultimately, the ensemble deep neural network is implemented and optimized to operate on low-power embedded computing devices with low latency (1.1 ms), demonstrating sufficient performance for real-time applications",'MDPI AG',A Spatio-Temporal Ensemble Deep Learning Architecture for Real-Time Defect Detection during Laser Welding on Low Power Embedded Computing Boards,10.3390/s21124205,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
479369487,2021-07-01T00:00:00,"Abstract Background Implementation research has delved into barriers to implementing change and interventions for the implementation of innovation in practice. There remains a gap, however, that fails to connect implementation barriers to the most effective implementation strategies and provide a more tailored approach during implementation. This study aimed to explore barriers for the implementation of professional services in community pharmacies and to predict the effectiveness of facilitation strategies to overcome implementation barriers using machine learning techniques. Methods Six change facilitators facilitated a 2-year change programme aimed at implementing professional services across community pharmacies in Australia. A mixed methods approach was used where barriers were identified by change facilitators during the implementation study. Change facilitators trialled and recorded tailored facilitation strategies delivered to overcome identified barriers. Barriers were coded according to implementation factors derived from the Consolidated Framework for Implementation Research and the Theoretical Domains Framework. Tailored facilitation strategies were coded into 16 facilitation categories. To predict the effectiveness of these strategies, data mining with random forest was used to provide the highest level of accuracy. A predictive resolution percentage was established for each implementation strategy in relation to the barriers that were resolved by that particular strategy. Results During the 2-year programme, 1131 barriers and facilitation strategies were recorded by change facilitators. The most frequently identified barriers were a ‘lack of ability to plan for change’, ‘lack of internal supporters for the change’, ‘lack of knowledge and experience’, ‘lack of monitoring and feedback’, ‘lack of individual alignment with the change’, ‘undefined change objectives’, ‘lack of objective feedback’ and ‘lack of time’. The random forest algorithm used was able to provide 96.9% prediction accuracy. The strategy category with the highest predicted resolution rate across the most number of implementation barriers was ‘to empower stakeholders to develop objectives and solve problems’. Conclusions Results from this study have provided a better understanding of implementation barriers in community pharmacy and how data-driven approaches can be used to predict the effectiveness of facilitation strategies to overcome implementation barriers. Tailored facilitation strategies such as these can increase the rate of real-time implementation of innovations in healthcare, leading to an industry that can confidently and efficiently adapt to continuous change",'Springer Science and Business Media LLC',Data-driven approach for tailoring facilitation strategies to overcome implementation barriers in community pharmacy,10.1186/s13012-021-01138-8,,"[{'title': 'Implementation Science', 'identifiers': ['issn:1748-5908', '1748-5908']}]",core
480019905,2020-01-01T00:00:00,"The paper presents the implementation of an intelligent decision support system (IDSS) to solve a real manufacturing problem at JSC “Savushkin Product”. The proposed system is intended to control the quality of product labeling, based on neuro-symbolic artificial intelligence, namely integrating deep neural networks and semantic models. The system perform localization and recognition of images from a high-speed video stream and is based on several deep neural networks. Semantic networks fulfill intelligent processing of recognition results in order to generate final decision as regards the state of the production conveyor. We demonstrate the performance of 
the proposed technique in the real production process. The main contribution of this paper is a novel view at the creation of a real intelligent decision support system, which combines bio inspired approach, namely neural networks and conventional technique, based on a knowledge base",'Springer Science and Business Media LLC',Intelligent Voice Assistant Based on Open Semantic Technology,10.1007/978-3-030-60447-9_8.,https://core.ac.uk/download/480019905.pdf,,core
296874516,2020-07-16T19:28:03,"Orientador: Janito Vaqueiro FerreiraDissertação (mestrado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: Na indústria do petróleo é comum à utilização de bombas centrífugas submersas (BCS) operando em escoamento multifásico líquido-gás. A presença de elevadas vazões de gás causam uma degradação severa no desempenho da bomba, gerando instabilidades nas curvas de pressão-vazão, como o `surging¿ e o `gas locking¿. Portanto o conhecimento destas instabilidades é fundamental para a adequada operação da bomba e assim evitar falhas prematuras no equipamento. Na atualidade não existem modelos matemáticos que representem de forma adequada o comportamento da BCS na região de `surging¿ e no `gas locking¿, gerando a necessidade de empregar circuitos de testes para fazer o levantamento das curvas de desempenho das bombas. A maioria dos circuitos de testes é operada de forma manual para obter às condições de operação da bomba, tornando os ensaios repetitivos, cansativos e trabalhosos. Por isto nasce a necessidade de automatizar estas bancadas com a finalidade de facilitar o processo do levantamento das curvas de desempenho das bombas. Este trabalho apresenta o projeto e simulação de um controle robusto tipo H_? que permita manter o escoamento multifásico na entrada de uma BCS em diferentes condições de operação da bomba. Este controlador é projetado a partir de um circuito de testes para BCS virtual que é modelado empregando formulações físicas e modelos ajustados mediante dados experimentais usando algoritmos de aprendizagem de máquinas baseados em máquinas de suporte vetorial para regressão (SVMr). Após o projeto de controle, o controlador projetado é testado no circuito de testes virtual mediante simulações em tempo real `software in the loop¿ (SIL)Abstract: In the oil industry, it is common to use electrical submersible pumps (ESP) operating with gas -liquid multiphase flow. The presence of high gas flows cause severe degradation in performance of the pump, generating instabilities in the flow-pressure curves, as ""surging"" and ""gas locking"". Therefore knowledge of these instabilities is essential for the proper functioning of the pump and thereby prevents premature failure of the equipment. Currently there are no mathematical models that adequately represent the behavior of the EPS in the region of ""surging"" and ""gas locking"", creating the need to use test circuits to make a study of the performance curves of the pumps. Most test circuits are operated manually to reach the operating conditions of the pump, making repetitive, tedious and laborious trials. Therefore there is a need to automate these circuits in order to facilitate the process of obtaining the performance curve of the pump. In this paper the project and simulation of a robust control type H_? for keeping the multiphase flow in the entrance of a EPS operating at different conditions is performed. This controller is designed based on a test circuit virtual for EPS which is modeled using physical formulations and adjusted models obtained by experimental data using machine learning algorithms based on support vector machines for regression (SVMR). After the controller design, the control is tested in the virtual test circuits using simulations in real time ""software in the loop"" (SIL)MestradoMecanica dos Sólidos e Projeto MecanicoMestre em Engenharia Mecânic",[s.n.],Simulation control of multiphase flow an electrical submersible pump - EPS,,https://core.ac.uk/download/296874516.pdf,,core
237690053,2020-01-01T00:00:00,"Industry 4.0 is nowadays the reference paradigm for production system implementation. The reasons lay in several motivations, among which the product/process data availability. This is paramount in supporting product tracking and tracing, feeding optimization applications, enabling sophisticated maintenance approaches and in monitoring resources and energy consumption in a sustainability perspective. While the setup of “green field” implementations is usually easier and well defined, problems arise when there is an existing system with physical shop-floor devices, applications and on-going production processes that cannot be disturbed or interrupted, and need to be interfaced. This paper is aiming to define an implementation strategy and a system architecture able to upgrade an existing production system to “Industry 4.0 compliant” status, keeping into account features and characteristics of said system, and applications without direct intervention (software or hardware) on the system and without perturbations of any on-going business. The proposed AI40A (Additive I40 Architecture) is structured on three basic components of: Data collection, Data transfer and Condition detection and trend forecasting. Each component and sub-module can be relocated individually on physical servers, cloud or edge computing virtual machines, based on availability or resources, computational needs or security reasons. As proof of concept, a prototype of the Additive Industry 4.0 Architecture that is implemented in Industry 4.0 Lab (I4.0Lab) of the School of Management of Politecnico di Milano in Bovisa (Milano) Campus will be shown. Two industrial applications are currently deployed on top of it: Production Rescheduling on Condition and Prognostic Maintenance based on Condition Regression with AR (Augmented Reality",'Springer Science and Business Media LLC',An “Additive” Architecture for Industry 4.0 Transition of Existing Production Systems,10.1007/978-3-030-27477-1,,,core
387291684,2020-12-09T00:00:00,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.Comment: A Computing Community Consortium (CCC) white paper, 4 page",,Artificial Intelligence at the Edge,,http://arxiv.org/abs/2012.05410,,core
347168399,2020-11-18T00:00:00,"International audienceComplex  industrial  systems  are  increasingly  software  driven,  rapidly  evolving  into autonomous,  self-adaptive  processing  at  industrial  field.  Artificial  intelligence technologies are spreading fast at industrial Edge, improving the industrial operations efficiency. However Edge computing systems involving artificial intelligence must also continuously ensure the safe operations. This paper assesses the state of the art of cognitive technologies with their relevance for artificial intelligence implementation at industrial  safety  critical  systems.  It  introduces  then  a  state of  the  art  for  artificial intelligence  safety  assurance  practices.  Implementation  at  the industrial  application stack is illustrated with the edge virtual operating system that operates the industrial fog. Edge operating system is evolving fast as kind of an AI intensive software. Industrial developments are illustrated with Slap OS, real case examples of swarm computing and advanced robotic",HAL CCSD,Industrial field autonomous systems: AI-assisted distributed applications at Edge,,,,core
333950166,2020-03-01T00:00:00,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy",'Institute of Electrical and Electronics Engineers (IEEE)',Federated Tensor Mining for Secure Industrial Internet of Things,10.1109/TII.2019.2937876,,,core
338815138,2020-10-01T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and analysis in order to improve the process representation. This paper presents the development and implementation of quality monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality variable, such as surface roughness. Firstly, the Hybrid Incremental Modelling strategy is applied. Secondly, a Generalized Fuzzy Clustering C-Means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing is performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to manufacturing industry",'Techno-Press',Quality Monitoring of Complex Manufacturing Systems on the basis of Model Driven Approach,10.12989/sss.2020.26.4.495,"http://www.techno-press.org/content/?page=article&journal=sss&volume=26&num=4&ordernum=7,",,core
333817724,2020-08-17T00:00:00,"It is said that Data and Information are the new oil. One, who handles the data, handles the emerging future of the global economy. Complex algorithms and intelligence-based filter programs are utilized to manage, store, handle, and maneuver vast amounts of data for the fulfillment of specific purposes. This paper seeks to find the bridge between artificial intelligence and its impact on international policy implementation in the light of geopolitical influence, the global economy, and the future of labor markets. We hypothesize that the distortion in the labor markets caused by artificial intelligence can be mitigated by a collaborative international foreign policy on the deployment of AI in the industrial circles. We, in this paper, then proceed to propose a disposition forth essentials of AI-based foreign policy and implementation, while asking questions such as: could AI become the real ‘invisible hand’ discussed by economists","Journal of Liberty and International Affairs, Institute for Research and European Studies - Bitola",TURBULENCE ON THE GLOBAL ECONOMY INFLUENCED BY ARTIFICIAL INTELLIGENCE AND FOREIGN POLICY INEFFICIENCIES,10.47305/JLIA2020113ob,https://core.ac.uk/download/333817724.pdf,,core
357408618,2020-04-11T00:00:00,"ABSTRACT Big data analytics is a critical and unavoidable process in any business and industrial environment. Nowadays, companies that do exploit big data&apos;s inner value get more economic revenue than the ones which do not. Once companies have determined their big data strategy, they face another serious problem: in-house designing and building of a scalable system that runs their business intelligence is difficult. The PROTEUS project aims to design, develop, and provide an open ready-to-use big data software architecture which is able to handle extremely large historical data and data streams and supports online machine learning predictive analytics and real-time interactive visualization. The overall evaluation of PROTEUS is carried out using a real industrial scenario. PROJECT DESCRIPTION PROTEUS 1 is an EU Horizon2020 2 funded research project, which has the goal to investigate and develop ready-to-use, scalable online machine learning algorithms and real-time interactive visual analytics, taking care of scalability, usability, and effectiveness. In particular, PROTEUS aims to solve the following big data challenges by surpassing the current state-of-art technologies with original contributions: 1. Handling extremely large historical data and data streams 2. Analytics on massive, high-rate, and complex data streams 3. Real-time interactive visual analytics of massive datasets, continuous unbounded streams, and learned models PROTEUS&apos;s solutions for the challenges above are: 1) a real-time hybrid processing system built on top of Apache Flink 3 (formerly Stratosphere 4 [1]) with optimized relational algebra and linear algebra operations support through LARA declarative language  PROTEUS faces an additional challenge which deals with cor",,PROTEUS: Scalable Online Machine Learning for Predictive Analytics and Real-Time Interactive Visualization,,https://core.ac.uk/download/357408618.pdf,,core
345810861,2020-03-01T00:00:00,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy",'Institute of Electrical and Electronics Engineers (IEEE)',Federated Tensor Mining for Secure Industrial Internet of Things,10.1109/TII.2019.2937876,,,core
387887208,2020-01-01T00:00:00,"As the development progresses, software projects tend to accumulate Technical Debt and become harder to maintain. Multiple tools exist with the mission to help practitioners to better manage Technical Debt. Despite this progress, there is a lack of tools providing actionable and self-learned suggestions to practitioners aimed at mitigating the impact of Technical Debt in real projects. We aim to create a data-driven, lightweight, and self-learning tool positioning highly impactful refactoring proposals on a Jira backlog. Bearing this goal in mind, the first two authors have founded a startup, called Skuld.ai, with the vision of becoming the go-to software renovation company. In this tool paper, we present the software architecture and demonstrate the main functionalities of our tool. It has been showcased to practitioners, receiving positive feedback. Currently, its release to the market is underway thanks to an industry-research institute collaboration with Fraunhofer IESE to incorporate self-learning technical debt capabilities.Peer ReviewedPostprint (author's final draft",'Association for Computing Machinery (ACM)',Skuld: a self-learning tool for impact-driven technical debt management,10.1145/3387906.3388626.,,,core
373457422,2020-01-01T00:00:00,"This paper proposes using reinforcement learning to solve scheduling problems where
two types of resources of limited availability must be allocated. The goal is to minimize the
makespan of a dual-resource constrained flexible job shop scheduling problem. Efficient practical
implementation is very valuable to industry, yet it is often only solved combining heuristics
and expert knowledge. A framework for training a reinforcement learning agent to schedule
diverse dual-resource constrained job shops is presented. Comparison with other state-of-theart approaches is done on both simpler and more complex instances that the ones used for
training. Results show the agent produces competitive solutions for small instances that can
outperform the implemented heuristic if given enough time. Other extensions are needed before
real-world deployment, such as deadlines and constraining resources to work shifts","21st IFAC World Congress, Berlin",Reinforcement Learning for Dual-Resource Constrained Scheduling,,https://core.ac.uk/download/373457422.pdf,,core
328880990,2020-06-08T00:00:00,"Lack of standardization is highly visible while we use historical data sets or compare our model with others that use IoMT devices from different vendors. The problem also concerns the trust in highly decentralized and anonymous environments where sensitive data are transferred through the Internet and then are analyzed by third-party companies. In our research we propose a standard that has been implemented in the form of framework that allows describing requirements for methods and platforms that collect, manage, share, and perform data analysis form the Internet of Medical Things in order to increase trust. Further, we can distinguish two types of IoMT devices: passive and active. Passive devices measure some parameters of the body and save them in databases. Active devices have the functionality of passive devices and moreover, they can act in a defined way, eg.: inject directly into the patient’s body some elements such as a medicament, electric signals to the nervous system, stimulus pacemaker, etc. Nevertheless how to create a safe and transparent environment for using data active sensors, developing safe ML models, performing medical decisions based on the created models and finally deploy this decision to the specified device. While the IoMT devices are used in real-life, professional healthcare the control system should offer tools for backtracking decisions, allowing e.g. to find who made a mistake, or which event caused a particular decision. Our framework provides backtracking in the IoMT environment in which for each medical decision supported by ML models we can prove which sensor sends the data, which data was used to create prediction/recommendation, what prediction was produced, who and when use it, what medical decision was made by who. We propose a vendor transparency framework for each IoMT devices and ML models that will process the medical data in order to increase patient’s privacy and prevent for eventual data leaking.This work was supported in part by the Department of Computer Architecture, Gdansk University of Technology, in part by the Spanish Research Agency (AEI) and the European Regional Development Fund (ERDF) under Project CloudDriver4Industry TIN2017-89266-R, and in part by Grant RTI2018-094283-B-C32, ECLIPSE-UA (Spanish Ministry of Education and Science)",'Institute of Electrical and Electronics Engineers (IEEE)',Framework for Integration Decentralized and Untrusted Multi-Vendor IoMT Environments,10.1109/ACCESS.2020.3000636,,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
327132315,2020-07-29T00:00:00,"With the growth of Internet of Things (IoT) and mo-bile edge computing,
billions of smart devices are interconnected to develop applications used in
various domains including smart homes, healthcare and smart manufacturing. Deep
learning has been extensively utilized in various IoT applications which
require huge amount of data for model training. Due to privacy requirements,
smart IoT devices do not release data to a remote third party for their use. To
overcome this problem, collaborative approach to deep learning, also known as
Collaborative DeepLearning (CDL) has been largely employed in data-driven
applications. This approach enables multiple edge IoT devices to train their
models locally on mobile edge devices. In this paper,we address IoT device
training problem in CDL by analyzing the behavior of mobile edge devices using
a game-theoretic model,where each mobile edge device aims at maximizing the
accuracy of its local model at the same time limiting the overhead of
participating in CDL. We analyze the Nash Equilibrium in anN-player static game
model. We further present a novel cluster-based fair strategy to approximately
solve the CDL game to enforce mobile edge devices for cooperation. Our
experimental results and evaluation analysis in a real-world smart home
deployment show that 80% mobile edge devices are ready to cooperate in CDL,
while 20% of them do not train their local models collaboratively",,"Learner's Dilemma: IoT Devices Training Strategies in Collaborative Deep
  Learning",,http://arxiv.org/abs/2007.15215,,core
333949819,2020-01-01T00:00:00,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory&#39;s data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.&nbsp;</p",,Federated Tensor Mining for Secure Industrial Internet of Things,,,,core
429074282,2020-11-19T00:00:00,"Millions of battery-powered sensors deployed for monitoring purposes in a multitude of scenarios, e.g., agriculture, smart cities, industry, etc., require energy-efficient solutions to prolong their lifetime. When these sensors observe a phenomenon distributed in space and evolving in time, it is expected that collected observations will be correlated in time and space. In this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling mechanism capable of taking advantage of correlated information. We design our solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The proposed mechanism is capable of determining the frequency with which sensors should transmit their updates, to ensure accurate collection of observations, while simultaneously considering the energy available. To evaluate our scheduling mechanism, we use multiple datasets containing environmental observations obtained in multiple real deployments. The real observations enable us to model the environment with which the mechanism interacts as realistically as possible. We show that our solution can significantly extend the sensors' lifetime. We compare our mechanism to an idealized, all-knowing scheduler to demonstrate that its performance is near-optimal. Additionally, we highlight the unique feature of our design, energy-awareness, by displaying the impact of sensors' energy levels on the frequency of updates",arXiv.org,Energy Aware Deep Reinforcement Learning Scheduling for Sensors Correlated in Time and Space,,,,core
334914254,2020-02-11T00:00:00,"Individualized manufacturing is becoming an important approach as a means to
fulfill increasingly diverse and specific consumer requirements and
expectations. While there are various solutions to the implementation of the
manufacturing process, such as additive manufacturing, the subsequent automated
assembly remains a challenging task. As an approach to this problem, we aim to
teach a collaborative robot to successfully perform pick and place tasks by
implementing reinforcement learning. For the assembly of an individualized
product in a constantly changing manufacturing environment, the simulated
geometric and dynamic parameters will be varied. Using reinforcement learning
algorithms capable of meta-learning, the tasks will first be trained in
simulation. They will then be performed in a real-world environment where new
factors are introduced that were not simulated in training to confirm the
robustness of the algorithms. The robot will gain its input data from tactile
sensors, area scan cameras, and 3D cameras used to generate heightmaps of the
environment and the objects. The selection of machine learning algorithms and
hardware components as well as further research questions to realize the
outlined production scenario are the results of the presented work",,"Towards Intelligent Pick and Place Assembly of Individualized Products
  Using Reinforcement Learning",,http://arxiv.org/abs/2002.08333,,core
334925912,2020-02-24T00:00:00,"In today's day and time solving real-world complex problems has become
fundamentally vital and critical task. Many of these are combinatorial
problems, where optimal solutions are sought rather than exact solutions.
Traditional optimization methods are found to be effective for small scale
problems. However, for real-world large scale problems, traditional methods
either do not scale up or fail to obtain optimal solutions or they end-up
giving solutions after a long running time. Even earlier artificial
intelligence based techniques used to solve these problems could not give
acceptable results. However, last two decades have seen many new methods in AI
based on the characteristics and behaviors of the living organisms in the
nature which are categorized as bio-inspired or nature inspired optimization
algorithms. These methods, are also termed meta-heuristic optimization methods,
have been proved theoretically and implemented using simulation as well used to
create many useful applications. They have been used extensively to solve many
industrial and engineering complex problems due to being easy to understand,
flexible, simple to adapt to the problem at hand and most importantly their
ability to come out of local optima traps. This local optima avoidance property
helps in finding global optimal solutions. This paper is aimed at understanding
how nature has inspired many optimization algorithms, basic categorization of
them, major bio-inspired optimization algorithms invented in recent time with
their applications",,Bio-inspired Optimization: metaheuristic algorithms for optimization,,http://arxiv.org/abs/2003.11637,,core
354485601,2020-01-01T00:00:00,"Includes bibliographical references.2020 Summer.The American Society of Civil Engineers estimates that by 2040, water and wastewater infrastructure (e.g., treatment facilities, conveyance systems) in the United States will require $144 billion to repair, replace, and upgrade. As water demand increases and stringent nutrient regulations are implemented, facilities will be forced to intensify existing operations or face costly process additions. Data-driven process monitoring and control is an underdeveloped paradigm in the treatment industry that could help address the growing infrastructure investment gap by improving the accuracy and precision with which water and wastewater are treated. Conventional wastewater treatment plants (WWTP) use relatively simple control systems compared to other process control industries (e.g., energy, manufacturing). “Normal” is demarcated by operator-determined thresholds on individual process variables; the upper and lower limits span a sufficiently large range to allow for some operational and environmental variability to water quality and quantity. Often these thresholds are so generous, process efficiency, and in some cases quality, is negatively impacted before an operator can respond to a fault or failure. Automated process adjustments are also limited by simple control logic, and process models lack the accuracy for real-time control. This frequently results in excessive energy and chemical use to safely operate in the event of a rapid and unexpected process upset. To improve the accuracy of monitoring and control, control strategies must consider the real-time multivariate and dynamic features of water treatment. This dissertation investigates multiple statistical and machine learning approaches to address control challenges faced by full-scale WWTP. First, a literature review details and compares state-of-the-art and state-of-the-industry WWTP monitoring and control methods. Second, multivariate statistical process control methods and adaptations are compared to identify faults in a decentralized WWTP. Third, a full-scale biological treatment process is (i) quantitatively assessed for stability and (ii) modeled to forecast ammonia for advanced control. Fourth, disinfection performance is modeled to adapt to changing water quality at a full-scale WWTP. Each chapter considers the realities of designing real-time monitoring and control using nonstationary, autocorrelated, and missing data, and solutions are proposed that use traditional and novel data science tools",Colorado School of Mines. Arthur Lakes Library,Data-driven process monitoring and control in municipal wastewater treatment,,,,core
350019993,2020-08-14T00:00:00,"The major advantages of spot and seam welding are high speed and adaptability primarily for high-volume and/or high-rate manufacturing. However, this paradigm fails to meet the principles laid down by Industry 4.0 for real-time control towards Zero Defect Manufacturing for each individual product and intuitive technical assistance on the process parameters. In this paper, a Robust Software Platform oriented for a CPS-based Quality Assessment system for Welding is presented based on data derived from IR cameras. Imaging data are pre – processed in real-time and streamed into a module which utilizes Machine Learning algorithms to perform quality assessment. A database enables data archiving and post processing tasks along with an intuitive User Interface which provide visualization capabilities and Decision Support on the welding process parameters. The modules’ IoT-based communication is performed with 5C architecture and is in line with Web Services",'EDP Sciences',A CPS platform oriented for Quality Assessment in welding,10.1051/matecconf/202031801030,,,core
326824501,2020-04-23T00:00:00,"The smart grid control applications necessitate real-time communication systems with time efficiency for real-time monitoring, measurement, and control. Time-efficient communication systems should have the ability to function in severe propagation conditions in smart grid applications. The data/packet communications need to be maintained by synchronized timing and reliability through equally considering the signal deterioration occurrences, which are propagation delay, phase errors and channel conditions. Phase synchronization plays a vital part in the digital smart grid to get precise and real-time control measurement information. IEEE C37.118 and IEC 61850 had implemented for the synchronization communication to measure as well as control the smart grid applications. Both IEEE C37.118 and IEC 61850 experienced a huge propagation and packet delays due to synchronization precision issues. Because of these delays and errors, measurement and monitoring of the smart grid application in real-time is not accurate. Therefore, it has been investigated that the time synchronization in real-time is a critical challenge in smart grid applications, and for this issue, other errors raised consequently. The existing communication systems are designed with the phasor measurement unit (PMU) along with communication protocol IEEE C37.118 and uses the GPS timestamps as the reference clock stamps. The absence of GPS increases the clock offsets, which surely can hamper the synchronization process and the full control measurement system that can be imprecise. Therefore, to reduce this clock offsets, a new algorithm is needed which may consider any alternative reference timestamps rather than GPS. The revolutionary Artificial Intelligence (AI) enables the industrial revolution to provide a significant performance to engineering solutions. Therefore, this article proposed the AI-based Synchronization scheme to mitigate smart grid timing issues. The backpropagation neural network is applied as the AI method that employs the timing estimations and error corrections for the precise performances. The novel AIFS scheme is considered the radio communication functionalities in order to connect the external timing server. The performance of the proposed AIFS scheme is evaluated using a MATLABbased simulation approach. Simulation results show that the proposed scheme performs better than the existing system",'Springer Science and Business Media LLC',A novel artificial intelligence based timing synchronization scheme for smart grid applications,10.1007/s11277-020-07408-w,https://core.ac.uk/download/326824501.pdf,,core
348688962,2020-01-01T00:00:00,"Hardware-assisted security solutions, and the isolation guarantees they provide, constitute the basis for the protection of modern software systems. Hardware-enforced isolation of individual components reduces complexity of the overall software as well as the size and complexity of the individual components. The basic idea is that a reduction in complexity minimizes the probability of vulnerabilities in the software, thus strengthening the system's security.
In classical system architectures, an application's security depends on the security of all privileged system entities, for example the Operating System. The Trusted Execution Environment (TEE) concept overcomes the dependence of security critical components on the systems overall security. TEEs provide isolated compartments within a single system, allowing isolated operation of a system's individual components and applications. The enclave computing paradigm enhances the TEE concept by enabling self-contained isolation of system components and applications, fulfilling the needs of modern software. It enables novel use cases by providing many parallel mutually isolated TEE-instances without the need to rely on complex privileged entities.
The TEE solutions developed by industry and deployed in today's systems follow distinct design approaches and come with various limitations. ARM TrustZone, which is widely available in mobile devices, is fundamentally limited to a single isolation domain. Intel's TEE solution Software Guard Extensions (SGX) provides multiple mutually isolated execution environments, called enclaves. However, SGX enclaves face severe threats, in particular side-channel leakage, that can void its security guarantees. Preventing side-channel leakage from enclaves in a universal and efficient way is a non-trivial problem. Nevertheless, these deployed TEE solutions enable various novel applications. However, different TEE architectures come with diverse properties and features that require special consideration in the design of TEE applications. 
Security architectures for embedded systems face additional challenges that have not been solved, neither by industry nor by academic research. These security architectures need to be compliant with and need to preserve all functional requirements of an embedded system. Since network-connected embedded devices are increasingly used in safety critical systems, such as industrial control systems or automotive scenarios, security architectures that combine safety and security aspects are vitally needed. 
Remote Attestation (RA) is a security service that relies on the isolation guarantees of TEEs. It is of particularly high relevance for connected embedded systems. It allows trust establishment between these devices enabling their reliable collaboration in large connected systems. However, many aspects of RA, such as its scalability in large networks or its applicability in autonomous connected systems, are unexplored. 
In this dissertation, we present novel isolation architectures that bring the enclave computing paradigm to mobile and embedded platforms. We present the first security architecture for small embedded systems that provides isolated execution enclaves and real-time guarantees. Moreover, we present a novel multi-TEE security architecture for TrustZone-systems bringing the enclave computing paradigm to mobile systems, overcoming TrustZone's fundamental limitation. 
Furthermore, we deal with Intel SGX's vulnerability to side-channel attacks. We demonstrate the severity of side-channel leakage due to observable memory access patterns of SGX enclaves. To counter side-channel attacks, we present solutions that hide memory access patterns of enclaves for both accesses to enclave-external memory as well as access patterns within enclaves' private memory. 
We present two TEE-applications that follow different design approaches, leveraging the specific capabilities of Intel SGX and ARM TrustZone, respectively. We introduce a cloud-based machine learning solution that enables privacy-preserving speech recognition utilizing isolated execution enclaves. We also demonstrate the limitations of the enclave computing paradigm and show a (remote) policy enforcement solution for mobile devices, which requires an isolated execution environment with elevated privileges. 
Additionally, we investigate novel RA schemes, which tackle many important aspects of RA that are highly relevant in emerging connected systems. We develop solutions to prevent the misuse of remote attestation for Denial-of-Service (DoS) attacks and present the first efficient multi-prover attestation scheme. Furthermore, we introduce the concept of data integrity attestation, which allows the efficient and reliable collaboration of autonomous connected devices",,Enclave Computing Paradigm: Hardware-assisted Security Architectures & Applications,10.25534/tuprints-00011912,,,core
337876230,2020-12-15T00:00:00,"In the maritime industry, sensors are utilised to implement condition-based maintenance (CBM) to assist decision-making processes for energy efficient operations of marine machinery. However, the employment of sensors presents several challenges including the imputation of missing values. Data imputation is a crucial pre-processing step, the aim of which is the estimation of identified missing values to avoid under-utilisation of data that can lead to biased results. Although various studies have been developed on this topic, none of the studies so far have considered the option of imputing incomplete values in real-time to assist instant data-driven decision-making strategies. Hence, a methodological comparative study has been developed that examines a total of 20 widely implemented machine learning and time series forecasting algorithms. Moreover, a case study on a total of 7 machinery system parameters obtained from sensors installed on a cargo vessel is utilised to highlight the implementation of the proposed methodology. To assess the models’ performance seven metrics are estimated (Execution time, MSE, MSLE, RMSE, MAPE, MedAE, Max Error). In all cases, ARIMA outperforms the remaining models, yielding a MedAE of 0.08 r/min and a Max Error of 2.4 r/min regarding the main engine rotational speed paramete",'Elsevier BV',Real-time data-driven missing data imputation for short-term sensor data of marine systems. A comparative study,10.1016/j.oceaneng.2020.108261,,,core
287860049,2020-01-01T00:00:00,"Underground fluid injection and extraction is able to change pore fluid pressure at depth and make faults unstable, due to friction-force reduction, with an increased possibility of triggering earthquakes. Studying the local seismicity, down to microearthquakes, and stress field in areas where such activities are developed are essential steps to discriminate between natural and induced events. In this context, the moment magnitude (MW) is a key-parameter to both evaluate the energy balance and the stress involved in earthquake rupture process and assess seismic hazard accurately. Here, we focus on the fast MW estimation of microearthquakes recorded around the underground gas storage of Collalto (Northeastern Italy) by a dedicated seismic monitoring network. The area of Montello-Collalto, where this industrial activity is carried out, is densely populated and characterized by relevant seismic hazard. We compute MW from the response spectra (SA) calculated at fixed periods (i.e., 1.0 and 0.3 s); we show that log (SA) and MW scale as 2/3 and extend our method to microseismicity by using response spectra at 0.1 s. We eventually estimate MW for 1659 events (0.4  64 MW  64 3.5) and find that ML and MW scale as 2/3 too. The discrepancy between these two magnitude scales affects both the Gutenberg-Richter parameters and completeness magnitude estimations; therefore, it has consequences when those quantities are used for physical interpretation. Our procedure shows to be efficient and suitable to be implemented within standard routine analyses of real-time monitoring and feed decision-making processes about plant management, such as the traffic light protocols",'Springer Science and Business Media LLC',"Fast MW estimation of microearthquakes recorded around the underground gas storage in the Montello-Collalto area (Southeastern Alps, Italy)",10.1007/s10950-019-09889-0,https://core.ac.uk/download/287860049.pdf,,core
391324127,2020-06-15T00:00:00,"International audienceThe mobile ecosystem is witnessing an unprecedented increase in the number of malware in the wild. To fight this threat, actors from both research and industry are constantly innovating to bring concrete solutions to improve security and malware protection. Traditional solutions such as signature-based anti viruses have shown their limits in front of massive proliferation of new malware, which are most often only variants specifically designed to bypass signature-based detection. Accordingly, it paves the way to the emergence of new approaches based on Machine Learning (ML) technics to boost the detection of unknown malware variants. Unfortunately, these solutions are most often underexploited due to the time and resource costs required to adequately fine tune machine learning algorithms. In reality, in the Android community, state-of-the-art studies do not focus on model training, and most often go through an empirical study with a manual process to choose the learning strategy, and/or use default values as parameters to configure ML algorithms. However, in the ML domain, it is well known admitted that to solve efficiently a ML problem, the tunability of hyper-parameters is of the utmost importance. Nevertheless, as soon as the targeted ML problem involves a massive amount of data, there is a strong tension between feasibility of exploring all combinations and accuracy. This tension imposes to automate the search for optimal hyper-parameters applied to ML algorithms, that is not anymore possible to achieve manually. To this end, we propose a generic and scalable solution to automatically both configure and evaluate ML algorithms to efficiently detect Android malware detection systems. Our approach is based on devOps principles and a microservice architecture deployed over a set of nodes to scale and exhaustively test a large number of ML algorithms and hyper-parameters combinations. With our approach, we are able to systematically find the best fit to increase up to 15% the accuracy of two state-of-the-art Android malware detection systems",'Springer Fachmedien Wiesbaden GmbH',DroidAutoML: A microservice architecture to automate the evaluation of Android machine learning detection systems,,,,core
328795470,2020-03-25T00:00:00,"Human-Machine Interaction (HMI) systems, once used for clinical applications, have recently reached a broader set of scenarios, such as industrial, gaming, learning, and health tracking thanks to advancements in Digital Signal Processing (DSP) and Machine Learning (ML) techniques. A growing trend is to integrate computational capabilities into wearable devices to reduce power consumption associated with wireless data transfer while providing a natural and unobtrusive way of interaction. However, current platforms can barely cope with the computational complexity introduced by the required feature extraction and classification algorithms without compromising the battery life and the overall intrusiveness of the system. Thus, highly-wearable and real-time HMIs are yet to be introduced. 

 

Designing and implementing highly energy-efficient biosignal devices demands a fine-tuning to meet the constraints typically required in everyday scenarios. This thesis work tackles these challenges in specific case studies, devising solutions based on bioelectrical signals, namely EEG and EMG, for advanced hand gesture recognition. 

 

The implementation of these systems followed a complete analysis to reduce the overall intrusiveness of the system through sensor design and miniaturization of the hardware implementation. Several solutions have been studied to cope with the computational complexity of the DSP algorithms, including commercial single-core and open-source Parallel Ultra Low Power architectures, that have been selected accordingly also to reduce the overall system power consumption. By further adding energy harvesting techniques combined with the firmware and hardware optimization, the systems achieved self-sustainable operation or a significant boost in battery life.



The HMI platforms presented are entirely programmable and provide computational power to satisfy the requirements of the studies applications while employing only a fraction of the CPU resources, giving the perspective of further application more advanced paradigms for the next generation of real-time embedded biosignal processing",,Low-Power Human-Machine Interfaces: Analysis And Design,10.6092/unibo/amsdottorato/9396.,,,core
329132270,2020-08-26T00:00:00,"With advances in the field of machine learning, precisely algorithms for
recommendation systems, robot assistants are envisioned to become more present
in the hospitality industry. Additionally, the COVID-19 pandemic has also
highlighted the need to have more service robots in our everyday lives, to
minimise the risk of human to-human transmission. One such example would be
coffee shops, which have become intrinsic to our everyday lives. However,
serving an excellent cup of coffee is not a trivial feat as a coffee blend
typically comprises rich aromas, indulgent and unique flavours and a lingering
aftertaste. Our work addresses this by proposing a computational model which
recommends optimal coffee beans resulting from the user's preferences.
Specifically, given a set of coffee bean properties (objective features), we
apply different supervised learning techniques to predict coffee qualities
(subjective features). We then consider an unsupervised learning method to
analyse the relationship between coffee beans in the subjective feature space.
Evaluated on a real coffee beans dataset based on digitised reviews, our
results illustrate that the proposed computational model gives up to 92.7
percent recommendation accuracy for coffee beans prediction. From this, we
propose how this computational model can be deployed on a service robot to
reliably predict customers' coffee bean preferences, starting from the user
inputting their coffee preferences to the robot recommending the coffee beans
that best meet the user's likings.Comment: Extended version of submission to ACM HAI 202",,At Your Service: Coffee Beans Recommendation From a Robot Assistant,,http://arxiv.org/abs/2008.13585,,core
322737892,2020-05-12T00:00:00,"Quality control is an essential process in manufacturing to make the product
defect-free as well as to meet customer needs. The automation of this process
is important to maintain high quality along with the high manufacturing
throughput. With recent developments in deep learning and computer vision
technologies, it has become possible to detect various features from the images
with near-human accuracy. However, many of these approaches are data intensive.
Training and deployment of such a system on manufacturing floors may become
expensive and time-consuming. The need for large amounts of training data is
one of the limitations of the applicability of these approaches in real-world
manufacturing systems. In this work, we propose the application of a Siamese
convolutional neural network to do one-shot recognition for such a task. Our
results demonstrate how one-shot learning can be used in quality control of
steel by identification of defects on the steel surface. This method can
significantly reduce the requirements of training data and can also be run in
real-time.Comment: Accepted for publication in NAMRC 4",'Elsevier BV',One-Shot Recognition of Manufacturing Defects in Steel Surfaces,10.1016/j.promfg.2020.05.146,http://arxiv.org/abs/2005.05815,,core
328196218,2020-08-14T00:00:00,"Logs have been widely adopted in software system development and maintenance
because of the rich system runtime information they contain. In recent years,
the increase of software size and complexity leads to the rapid growth of the
volume of logs. To handle these large volumes of logs efficiently and
effectively, a line of research focuses on intelligent log analytics powered by
AI (artificial intelligence) techniques. However, only a small fraction of
these techniques have reached successful deployment in industry because of the
lack of public log datasets and necessary benchmarking upon them. To fill this
significant gap between academia and industry and also facilitate more research
on AI-powered log analytics, we have collected and organized loghub, a large
collection of log datasets. In particular, loghub provides 17 real-world log
datasets collected from a wide range of systems, including distributed systems,
supercomputers, operating systems, mobile systems, server applications, and
standalone software. In this paper, we summarize the statistics of these
datasets, introduce some practical log usage scenarios, and present a case
study on anomaly detection to demonstrate how loghub facilitates the research
and practice in this field. Up to the time of this paper writing, loghub
datasets have been downloaded over 15,000 times by more than 380 organizations
from both industry and academia.Comment: Dateset available at https://zenodo.org/record/322717",,"Loghub: A Large Collection of System Log Datasets towards Automated Log
  Analytics",,http://arxiv.org/abs/2008.06448,,core
304656348,2020-01-01T00:00:00,"An important research problem in artificial intelligence is how to organize multiple agents, and coordinate them, so that they can work together to solve problems. Coordinating agents in a multi-agent system can significantly affect the systems performance-the agents can, in many instances, be organized so that they can solve tasks more efficiently, and consequently benefit collectively and individually. Central to this endeavor is coalition formation-the process by which heterogeneous agents organize and form disjoint groups (coalitions). Coalition formation often involves finding a coalition structure (an exhaustive set of disjoint coalitions) that maximizes the systems potential performance (e.g., social welfare) through coalition structure generation. However, coalition structure generation typically has no notion of goals. In cooperative settings, where coordination of multiple coalitions is important, this may generate suboptimal teams for achieving and accomplishing the tasks and goals at hand. With this in mind, we consider simultaneously generating coalitions of agents and assigning the coalitions to independent alternatives (e.g., tasks/goals), and present an anytime algorithm for the simultaneous coalition structure generation and assignment problem. This combinatorial optimization problem hasmany real-world applications, including forming goal-oriented teams. To evaluate the presented algorithms performance, we present five methods for synthetic problem set generation, and benchmark the algorithm against the industry-grade solver CPLEXusing randomized data sets of varying distribution and complexity. To test its anytime-performance, we compare the quality of its interim solutions against those generated by a greedy algorithm and pure random search. Finally, we also apply the algorithm to solve the problem of assigning agents to regions in a major commercial strategy game, and show that it can be used in game-playing to coordinate smaller sets of agents in real-time.Funding Agencies|Linkoping University; Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation</p",'Springer Science and Business Media LLC',An anytime algorithm for optimal simultaneous coalition structure generation and assignment,10.1007/s10458-020-09450-1,,,core
357359459,2020-04-10T00:00:00,"ABSTRACT Our global security environment is increasingly affected by biological systems. From the threats of pandemics and bioterrorism to the exploding cost of health care, developing the means to effectively and affordably solve problems related to biological systems is critical to our quality of life. When considering health care costs, the numbers are staggering. Approximately half of the $2.4 trillion spent annually on US health care can be categorized as preventable costs, and $300 billion of this is attributable to medical mistakes and the defensive medicine they engender. Just as the use of flight simulators and system integration concepts revolutionized the aircraft industry decades earlier, similar concepts can be applied to improve the effectiveness and efficiency of the health care industry today. Our approach is intended to leverage advanced modeling and simulation techniques to accurately represent complex clinical environments. By creating hierarchical simulated models of these systems and then validating these models against their real-world equivalents, we are able to develop a virtual system-of-systems integration laboratory for clinical environments. As with comparable tools in aviation, our goal is for simulationbased tools for health care to make analysis and training fast, safe, measureable, and reproducible. This will be a significant step forward in health care, which has trailed other fields in the adoption of software simulations, due to technological limitations and behavioral barriers. We believe that a holistic approach such as this will pave the way for the next generation of decision support aids, medical devices, and training systems for applications across the health care spectrum. In this paper, we outline our approach with detailed examples of potential savings for a number of complex clinical scenarios. ABOUT THE AUTHORS Frank Boosman is a Program Management Director in Lockheed Martin&quot;s Global Training &amp; Logistics (GTL) division, where he leads GTL&quot;s efforts to improve the efficiency and effectiveness of health care via simulationbased system-of-systems engineering and analytics. He joined Lockheed Martin when they acquired 3Dsolve, a simulation training firm for which he served as COO. He served in a variety of VP-level roles for Be Incorporated, a vendor of software platforms for Internet appliances. He was a co-founder of Red Storm Entertainment, where he served as VP of Product Development and where he co-created Tom Clancy&apos;s Rainbow Six, the first realistic firstperson tactical combat game. He has also served as VP and General Manager of Virtus Studios, which he designed Tom Clancy SSN, the first 3D submarine simulation game, and as Senior Product Marketing Manager at Adobe Systems, where he was a founding team member and the original product manager of Adobe Acrobat. Dr. Robert J. Szczerba is a Senior Fellow with Lockheed Martin in the Corporate Engineering and Technology (CE&amp;T) organization. His current responsibilities include leading the Corporation&quot;s strategic initiatives in the Systems Biology and Healthcare domains. He has 20 years&apos; experience in assessing, developing, and adapting emerging technologies and accelerating their transition into practice. Dr. Szczerba has more than 100 publications and 40 patents / patents pending in a variety of emerging technology areas including systems biology, health care, autonomous systems, unmanned vehicles, and applied artificial intelligence. He has served as principal investigator and project manager on collaborative research and development programs between government, academia, and industry. Dr. Szczerba received BS and MS degrees in electrical and computer engineering and a PhD in computer science, all from the University of Notre Dame",,Simulated Clinical Environments and Virtual System-of-Systems Engineering for Health Care Simulated Clinical Environments and Virtual System-of-Systems Engineering for Health Care,,,,core
373377799,2020-01-01T00:00:00,"Object detection is arguably one of the most important and complex tasks to enable the advent of next-generation autonomous systems. Recent advancements in deep learning techniques allowed a significant improvement in detection accuracy and latency of modern neural networks, allowing their adoption in automotive, avionics and industrial embedded systems, where performances are required to meet size, weight and power constraints.Multiple benchmarks and surveys exist to compare state-of-the-art detection networks, profiling important metrics, like precision, latency and power efficiency on Commercial-off-the-Shelf (COTS) embedded platforms. However, we observed a fundamental lack of fairness in the existing comparisons, with a number of implicit assumptions that may significantly bias the metrics of interest. This includes using heterogeneous settings for the input size, training dataset, threshold confidences, and, most importantly, platform-specific optimizations, that are especially important when assessing latency and energy-related values. The lack of uniform comparisons is mainly due to the significant effort required to re-implement network models, whenever openly available, on the specific platforms, to properly configure the available acceleration engines for optimizing performance, and to re-train the model using a homogeneous dataset.This paper aims at filling this gap, providing a comprehensive and fair comparison of the best-in-class Convolution Neural Networks (CNNs) for real-time embedded systems, detailing the effort made to achieve an unbiased characterization on cutting-edge system-on-chips. Multi-dimensional trade-offs are explored for achieving a proper configuration of the available programmable accelerators for neural inference, adopting the best available software libraries. To stimulate the adoption of fair benchmarking assessments, the framework is released to the public in an open source repository",'Institute of Electrical and Electronics Engineers (IEEE)',A Systematic Assessment of Embedded Neural Networks for Object Detection,10.1109/ETFA46521.2020.9212130,,,core
387295884,2020-10-30T00:00:00,"Industrial Artificial Intelligence (Industrial AI) is an emerging concept
which refers to the application of artificial intelligence to industry.
Industrial AI promises more efficient future industrial control systems.
However, manufacturers and solution partners need to understand how to
implement and integrate an AI model into the existing industrial control
system. A well-trained machine learning (ML) model provides many benefits and
opportunities for industrial control optimization; however, an inferior
Industrial AI design and integration limits the capability of ML models. To
better understand how to develop and integrate trained ML models into the
traditional industrial control system, test the deployed AI control system, and
ultimately outperform traditional systems, manufacturers and their AI solution
partners need to address a number of challenges. Six top challenges, which were
real problems we ran into when deploying Industrial AI, are explored in the
paper. The Petuum Optimum system is used as an example to showcase the
challenges in making and testing AI models, and more importantly, how to
address such challenges in an Industrial AI system.Comment: 9 pages, 8 figure",,Validate and Enable Machine Learning in Industrial AI,,http://arxiv.org/abs/2012.09610,,core
395652187,2020-06-15T00:00:00,"International audienceThe mobile ecosystem is witnessing an unprecedented increase in the number of malware in the wild. To fight this threat, actors from both research and industry are constantly innovating to bring concrete solutions to improve security and malware protection. Traditional solutions such as signature-based anti viruses have shown their limits in front of massive proliferation of new malware, which are most often only variants specifically designed to bypass signature-based detection. Accordingly, it paves the way to the emergence of new approaches based on Machine Learning (ML) technics to boost the detection of unknown malware variants. Unfortunately, these solutions are most often underexploited due to the time and resource costs required to adequately fine tune machine learning algorithms. In reality, in the Android community, state-of-the-art studies do not focus on model training, and most often go through an empirical study with a manual process to choose the learning strategy, and/or use default values as parameters to configure ML algorithms. However, in the ML domain, it is well known admitted that to solve efficiently a ML problem, the tunability of hyper-parameters is of the utmost importance. Nevertheless, as soon as the targeted ML problem involves a massive amount of data, there is a strong tension between feasibility of exploring all combinations and accuracy. This tension imposes to automate the search for optimal hyper-parameters applied to ML algorithms, that is not anymore possible to achieve manually. To this end, we propose a generic and scalable solution to automatically both configure and evaluate ML algorithms to efficiently detect Android malware detection systems. Our approach is based on devOps principles and a microservice architecture deployed over a set of nodes to scale and exhaustively test a large number of ML algorithms and hyper-parameters combinations. With our approach, we are able to systematically find the best fit to increase up to 15% the accuracy of two state-of-the-art Android malware detection systems",'Springer Fachmedien Wiesbaden GmbH',DroidAutoML: A microservice architecture to automate the evaluation of Android machine learning detection systems,,,,core
334462062,2020-09-04T14:54:14,"Intelligent robotic systems are becoming essential for space applications, industries, nuclear plants and for harsh environments in general, such as the European Organization for Nuclear Research (CERN) particles accelerator complex and experiments. Robotics technology has huge potential benefits for people and its ultimate scope depends on the way this technology is  used. In order to increase safety and machine availability, robots can perform repetitive, unplanned and dangerous tasks, which humans either prefer to avoid or are unable to carry out due to hazards, size constraints, or the extreme environments in which they take place. Nowadays, mechatronic systems use mature technologies that allow their robust and safe use, even in collaboration with human workers. Over the past years, the progress of robots has been based on the development of smart sensors, artificial intelligence and modular mechanical systems. Due to the multiple challenges that hazardous and unstructured environments have for the application of autonomous industrial systems, there is still a high demand for intelligent and teleoperation systems that give the control of a robot (slave) to a human operator via haptic input devices (master), as well as using human-supervised telerobotic control techniques. Modern techniques like simulation and virtual reality systems can facilitate the preparation of ad-hoc mechatronic tools and robotic intervention including recovery scenarios and failure mode analysis.  The basic contribution of this thesis is the development of a novel robotic framework for autonomous inspections and supervised teleoperations in harsh environments. The proposed framework covers all aspects of a robotic intervention, from the specification and operator training, the choice of the robot and its material in accordance with possible radiological contamination risks, to the realization of the intervention, including procedures and recovery scenarios. In a second set of contributions, new methods for mutirobots maintenance operations are developed, including intervention preparation and best practices for remote handling and advanced tools. The third set of contributions is built on a novel multimodal user-friendly human-robot interface that allows  operator training using virtual reality systems and technicians not expert in robot operation to perform inspection/maintenance tasks. In this thesis, we exploit a robotic system able to navigate autonomously and to inspect unknown environments in a safe way. A new real-time control system has been implemented in order to guarantee a fast response to environmental changes and adaptation to different type of scenarios the robot may find in a semi-structured and hazardous environment. The proposed new robotic control system  has been integrated on different robots, tested and validated with several robotic interventions in the CERN hazardous particle accelerator complex",,A Novel Robotic Framework for Safe Inspection and Telemanipulation in Hazardous and Unstructured Environments,,,,core
334906890,2020-01-28T00:00:00,"In this paper, we present a factor-graph LiDAR-SLAM system which incorporates
a state-of-the-art deeply learned feature-based loop closure detector to enable
a legged robot to localize and map in industrial environments. These facilities
can be badly lit and comprised of indistinct metallic structures, thus our
system uses only LiDAR sensing and was developed to run on the quadruped
robot's navigation PC. Point clouds are accumulated using an inertial-kinematic
state estimator before being aligned using ICP registration. To close loops we
use a loop proposal mechanism which matches individual segments between clouds.
We trained a descriptor offline to match these segments. The efficiency of our
method comes from carefully designing the network architecture to minimize the
number of parameters such that this deep learning method can be deployed in
real-time using only the CPU of a legged robot, a major contribution of this
work. The set of odometry and loop closure factors are updated using pose graph
optimization. Finally we present an efficient risk alignment prediction method
which verifies the reliability of the registrations. Experimental results at an
industrial facility demonstrated the robustness and flexibility of our system,
including autonomous following paths derived from the SLAM map.Comment: 8 pages, 9 figures, accepted for IEEE International Conference on
  Robotics and Automation (ICRA 2020",,"Online LiDAR-SLAM for Legged Robots with Robust Registration and
  Deep-Learned Loop Closure",,http://arxiv.org/abs/2001.10249,,core
333614261,2020-03-26T00:00:00,"An indoor, real-time location system (RTLS) can benefit both hospitals and
patients by improving clinical efficiency through data-driven optimization of
procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy
and robustness because Bluetooth signal strength is subject to fluctuation. We
developed a machine learning-based solution using a Long Short-Term Memory
(LSTM) network followed by a Multilayer Perceptron classifier and a posterior
constraint algorithm to improve RTLS performance. Training and validation
datasets showed that most machine learning models perform well in classifying
individual location zones, although LSTM was most reliable. However, when faced
with data indicating cross-zone trajectories, all models showed erratic zone
switching. Thus, we implemented a history-based posterior constraint algorithm
to reduce the variability in exchange for a slight decrease in responsiveness.
This network increases robustness at the expense of latency. When latency is
less of a concern, we computed the latency-corrected accuracy which is 100% for
our testing data, significantly improved from LSTM without constraint which is
96.2%. The balance between robustness and responsiveness can be considered and
adjusted on a case-by-case basis, according to the specific needs of downstream
clinical applications. This system was deployed and validated in an academic
medical center. Industry best practices enabled system scaling without
substantial compromises to performance or cost.Comment: 20 pages, 6 figures, submitted to Physics in Medicine & Biolog",'Wiley',"Development of a Real-time Indoor Location System using Bluetooth Low
  Energy Technology and Deep Learning to Facilitate Clinical Applications",10.1002/mp.14198,http://arxiv.org/abs/1907.10554,,core
355840394,2020-09-09T14:12:14,"Traditional methods for bacterial identification include Gram staining, culturing, and biochemical assays for phenotypic characterization of the causative organism. These methods can be time‐consuming because they require in vitro cultivation of the microorganisms. Recently, however, it has become possible to obtain chemical profiles for lipids, peptides, and proteins that are present in an intact organism, particularly now that new developments have been made for the efficient ionization of biomolecules. MS has therefore become the state‐of‐the‐art technology for microorganism identification in microbiological clinical diagnosis. Here, we introduce an innovative sample preparation method for nonculture‐based identification of bacteria in milk. The technique detects characteristic profiles of intact proteins (mostly ribosomal) with the recently introduced MALDI SepsityperTM Kit followed by MALDI‐MS. In combination with a dedicated bioinformatics software tool for databank matching, the method allows for almost real‐time and reliable genus and species identification. We demonstrate the sensitivity of this protocol by experimentally contaminating pasteurized and homogenized whole milk samples with bacterial loads of 103–108 colony‐forming units (cfu) of laboratory strains of Escherichia coli, Enterococcus faecalis, and Staphylococcus aureus. For milk samples contaminated with a lower bacterial load (104 cfu mL−1), bacterial identification could be performed after initial incubation at 37°C for 4 h. The sensitivity of the method may be influenced by the bacterial species and count, and therefore, it must be optimized for the specific application. The proposed use of protein markers for nonculture‐based bacterial identification allows for high‐throughput detection of pathogens present in milk samples. This method could therefore be useful in the veterinary practice and in the dairy industry, such as for the diagnosis of subclinical mastitis and for the sanitary monitoring of raw and processed milk products121727392745CONSELHO NACIONAL DE DESENVOLVIMENTO CIENTÍFICO E TECNOLÓGICO - CNPQFUNDAÇÃO DE AMPARO À PESQUISA DO ESTADO DE SÃO PAULO - FAPESPNão tem09/12751–5The authors are grateful to the Brazilian Science foundation CNPq, FAPESP (Grant 09/12751–5), and FINEP for financial assistanc",'Wiley',Nonculture‐based identification of bacteria in milk by protein fingerprinting,10.1002/pmic.201200053,,"[{'title': 'PROTEOMICS', 'identifiers': ['issn:1615-9861', '1615-9861', 'issn:1615-9853', '1615-9853']}]",core
351517700,2020-05-26T00:00:00,"Background: Eggs have acquired a greater importance as an inexpensive and high-quality protein. The Brazilian egg industry has been characterized by a constant production expansion in the last decade, increasing the number of housed animals and facilitating the spread of many diseases. In order to reduce the sanitary and financial risks, decisions regard­ing the production and the health status of the flock must be made based on objective criteria. The use of Artificial Neural Networks (ANN) is a valuable tool to reduce the subjectivity of the analysis. In this context, the aim of this study was at validating the ANNs as viable tool to be employed in the prediction and management of commercial egg production flocks.Materials, Methods & Results: Data from 42 flocks of commercial layer hens from a poultry company were selected. The data refer to the period between 2010 and 2018 and it represents a total of 600,000 layers. Six parameters were selected as “output” data (number of dead birds per week, feed consumption, number of eggs, weekly weight, weekly egg produc­tion and flock uniformity) and a total of 13 parameters were selected as “input” data (flock age, flock identification, total hens in the flock, weekly weight, flock uniformity, lineage, weekly mortality, absolute number of dead birds, eggs/hen, weekly egg production, feed consumption, flock location, creation phase). ANNs were elaborated by software programs NeuroShell Predictor and NeuroShell Classifier. The programs identified input variables for the assembly of the networks seeking the prediction of the variables called outgoing that are subsequently validated. This validation goes through the comparison between the predictions and the real data present in the database that was the basis for the work. Validation of each ANN is expressed by the specific statistical parameters multiple determination (R2) and Mean Squared Error (MSE). For instance, R2 above 0.70 expresses a good validation. ANN developed for the output variable “number of dead birds per week” presented R2= 0.9533 and MSE= 256.88. For “feed consumption”, the results were R2= 0.7382 and MSE= 274.56. For “number of eggs (eggs/hen)”, the results were R2= 0.9901 and MSE= 172.26. For “weekly weight”, R2= 0.9712 and MSE= 11154.41. For “weekly egg production”, R2= 0.8015 and MSE= 72.60. For “flock uniformity”, R2= -2.9955 and MSE= 431.82.Discussion: From the six ANN designed in this study, in five it was possible to validate the predictions by comparing predictions with the real data. In one output parameter (“flock uniformity”), it was not possible to have adequate validation due to insufficient data in our database. For “number of dead birds per week”, “feed consumption”, “weekly weight” and “uniformity”, the most important variable was “flock age” (27.5%, 52.5%, 55.2% and 37.9%, respectively). For “number of eggs (eggs/hen)”, “uniformity” (52.1%) was the most relevant variable for prediction. For “weekly egg production”, “flock age” and “number of eggs (eggs/hen)” were the most important zootechnical parameters, both with a relative contribution of 38.2%. The results showed that even with the use of a robust tool such as ANNs, it is necessary to have well-noted and clear information that expresses the reality of the flocks. In any case, the results presented allow us to state that ANNs are capable for the management of data generated in a commercial egg production facility. The process of evaluation of these data would be improved if ANNs were routinely used by the professionals linked to this activity",'Universidade Federal do Rio Grande do Sul',Artificial Neural Networks on Eggs Production Data Management,10.22456/1679-9216.101462,,,core
334578353,2020-09-16T00:00:00,"Industry 4.0 is the fourth generation of industry which will theoretically revolutionize manufacturing methods through the integration of machine learning and artificial intelligence approaches on the factory floor to obtain robustness and sped-up process changes. In particular, the use of the digital twin in a manufacturing environment makes it possible to test such approaches in a timely manner using a realistic 3D environment that limits incurring safety issues and danger of damage to resources. To obtain superior performance in an industry 4.0 setup, a modified version of a binary gravitational search algorithm is introduced which benefits from an exclusive or (XOR) operator and a repository to improve the exploration property of the algorithm. Mathematical analysis of the proposed optimization approach is performed which resulted in two theorems which show that the proposed modification to the velocity vector can direct particles to the best particles. The use of repository in this algorithm provides a guideline to direct the particles to the best solutions more rapidly. The proposed algorithm is evaluated on some benchmark optimization problems covering a diverse range of functions including unimodal and multimodal as well as those which suffer from multiple local minima. The proposed algorithm is compared against several existing binary optimization algorithms including existing versions of a binary gravitational search algorithm, improved binary optimization, binary particle swarm optimization, binary grey wolf optimization and binary dragonfly optimization. To show that the proposed approach is an effective method to deal with real world binary optimization problems raised in an industry 4.0 environment, it is then applied to optimize the assembly task of an industrial robot assembling an industrial calculator. The optimal movements obtained are then implemented on a real robot. Furthermore, the digital twin of a universal robot is developed, and its path planning is done in the presence of obstacles using the proposed optimization algorithm. The obtained path is then inspected by human expert and validated. It is shown that the proposed approach can effectively solve such optimization problems which arises in industry 4.0 environment",'MDPI AG',XOR Binary Gravitational Search Algorithm with Repository: Industry 4.0 Applications,10.3390/app10186451,https://core.ac.uk/download/334578353.pdf,,core
395085721,2020-12-18T00:00:00,"With the Industry 4.0 paradigm comes the convergence of the Internet Technologies and Operational Technologies, and concepts, such as Industrial Internet of Things (IIoT), cloud manufacturing, Cyber-Physical Systems (CPS), and so on. These concepts bring industries into the big data era and allow for them to have access to potentially useful information in order to optimise the Overall Equipment Effectiveness (OEE); however, most European industries still rely on the Computer-Integrated Manufacturing (CIM) model, where the production systems run as independent systems (i.e., without any communication with the upper levels). Those production systems are controlled by a Programmable Logic Controller, in which a static and rigid program is implemented. This program is static and rigid in a sense that the programmed routines cannot evolve over the time unless a human modifies it. However, to go further in terms of flexibility, we are convinced that it requires moving away from the aforementioned old-fashioned and rigid automation to a ML-based automation, i.e., where the control itself is based on the decisions that were taken by ML algorithms. In order to verify this, we applied a time series classification method on a scale model of a factory using real industrial controllers, and widened the variety of parts the production line has to treat. This study shows that satisfactory results can be obtained only at the expense of the human expertise (i.e., in the industrial process and in the ML process)",,A Case Driven Study of the Use of Time Series Classification for Flexibility in Industry 4.0,,,,core
362655845,2020-01-01T00:00:00,"This thesis was submitted for the award of Doctor of Philosophy and was awarded by Brunel University LondonThe purpose of this research work is to presents a novel event-based predictive modelling technique, namely, Event Modeller Data Analytic (EMDA), applicable for a large-scale real-time complex system. Borrowed from the Event Tracker and Event Clustering method, EMDA continuously estimates and builds a correlation map between system input (triggered data) and output (event data) parameters while predicting system failure based on machine performance metrics. With the aid of advanced machine learning models, EMDA can potentially predict linear and non-linear problems, thereby improving rapid decision-making for system engineering problems. For proof of concept, EMDA was used to analyse the mystery of an escalating harmonic failure trend in one of the Malaysian power plants. Analysis of the harmonic parameter in their Continuous Ship Unloader machines indicates that the power quality is stable as per IEEE standards; however, in practice, repetitive harmonic failures occur and the reasons remain unknown. The hypothesis associated with this research is that: ""A fault in a power system distribution could be influenced not only by internal events but also by external events such as environment and climate change"". In addition to the conventional method used by the power system engineers, we challenge the body of knowledge in the subject area by exploring and potentially incorporating external variables that may influence the state of the system. Software-In-the-Loop application was developed using the National Instrument LabVIEW. The purpose of this deployment was to test and validate the concept and to demonstrate whether the correlation analysis was in synchronisation with the latent knowledge (KPI) translated into the system. EMDA was also used as a tool to visualise the occurrences of the system parameters and its KPIs with a predictive analytical approach to data. This research conducts extensive experimental work on both industrial and synthetic data to evaluate the proposed method. The results of the study reveal that in addition to the known parameters that may affect harmonic filter performance, there is one new parameter that shows a reasonable correlation with performance. The previously unknown parameter is the humidity of the operational environment having a significant impact on the occurrence of harmonic failure. This proved the hypothesis set in the underpinning research endeavour presented in this thesis. By controlling the humidity of the operational environment and deploying EMDA, the state transition and trends were accurately predicted. The results of this research can help power generation plants to devise adaptive strategies to optimise the performance of plants",Brunel University London,Real-Time Event Based Predictive Modelling for Industrial Control and Monitoring,,https://core.ac.uk/download/362655845.pdf,,core
357295407,2020-04-02T00:00:00,"Artificial intelligence methodologies, as the core of discrete control and decision support systems, have been extensively applied in the industrial production sector. The resulting tools produce excellent results in certain cases; however, the NP-hard nature of many discrete control or decision making problems in the manufacturing area may require unaffordable computational resources, constrained by the limited available time required to obtain a solution. With the purpose of improving the efficiency of a control methodology for discrete systems, based on a simulation-based optimization and the Petri net (PN) model of the real discrete event dynamic system (DEDS), this paper presents a strategy, where a transformation applied to the model allows removing the redundant information to obtain a smaller model containing the same useful information. As a result, faster discrete optimizations can be implemented. This methodology is based on the use of a formalism belonging to the paradigm of the PN for describing DEDS, the disjunctive colored PN. Furthermore, the metaheuristic of genetic algorithms is applied to the search of the best solutions in the solution space. As an illustration of the methodology proposal, its performance is compared with the classic approach on a case study, obtaining faster the optimal solution",,Control of Discrete Event Systems by Means of Discrete Optimization and Disjunctive Colored PNs: Application to Manufacturing Facilities,,https://core.ac.uk/download/357295407.pdf,,core
433836885,2020-04-22T00:00:00,"Digital Advanced Era is gaining momentum today. There are objective reasons, significant advantages, and serious business challenges. The ongoing digital transformation creates some of the largest economic, political and social challenges for the European Union. Despite new opportunities, it is frequently perceived as a threat to traditional business models, current organizational structures, and well-established business operations. Economic question about the competitiveness and capabilities in new technologies are linked to needs of skills and knowledge developing in Europe. It is an incredible for exploring the processes. For Ukrainian researchers, this issue may be interesting in the context of the European integration and understanding of digitalization processes in the EU, comparing them with Ukrainian realities, and identifying possibilities for benchmarking of the European experience. The main aim of the paper is to focus on the skills necessary to managers to operate in a digital era.There are no doubts that the nature of work changes, new jobs emerge while others disappear, so people need to gain new skills. Approximately 30% of the current jobs in the European Union may disappear over the next 25 years with the emergence of new professions requiring advanced digital skills [1]. Therefore, a critical element in the digital economy is to define and to acquire skills for business development in digital era. We are trying to focus on the skills critical for the EU’s growth and mitigation of losses caused by the digital transformation (by cloud computing, big data, the Internet of Things and artificial intelligence.An important factor to be taken into consideration is the fact that the society is not fully aware of the real situation. Digital business (or digital era, digital transformation) is taken for almost granted. But in reality the digital economy is highly concentrated in two countries (USA and China):-       50% of global spending on IoT-       more than 75% of the cloud computing market-       75% of all patents related to blockchain technologies [2].Moreover:-       about 10 % of the EU labour force has no digital skills;-       35 % of the EU labour does not have basic digital skills;-       about 28 % of the EU's internet users have no software-related skills [1].Also it is important to understand that digital skills which can be defined as abilities to use digital devices, communication applications, and networks to find information are (to some extend) basic or consumer skills that are mechanically honed and do not require serious intellectual effort. While digital skills also mean abilities to manage information, it is a higher level skill, but it is still a basic skill in the digital age. The shift of value creation towards digital platforms, new security and standard solutions, big data usage and data quality call for more complex skills. According to UNCTAD Digital Economy Report 2019, the expansion of the Internet of Things and various data-driven business models will require specialists who can convert big data into information and knowledge that are:- data scientist;- data engineers;- data architects and data visualizators [2].And we're focusing on the ability to use business opportunities including the increasing amounts of data. These require broader skill sets, skills to combine analytical, software and information systems with business. Data analysts and scientists also need to be business savvy to help enterprises capture business opportunities from their analyses. Multiple skills that combine sound technical skills with entrepreneurial skills and vertical and business process management expertise are particularly important [3].It is noteworthy that only 12% of EU enterprises used big data in 2018, 9% of EU enterprises - in 2016 [3]. That is, 88% and 91% did not use. Accordingly, the issue is not only about to capture business opportunities but skills to see the existing opportunities at least. That is why the EU awareness campaigns were oriented on alert the real picture: – WATIFY - launched in 2014 with the goal to push people towards concrete action to create their digital start-up, or digitally transform their business;– e-Skills for jobs and growth – launched in 2014 with an idea to strengthen competences and e-leadership;–  Start Up Europe Initiative to strengthen the environment for Web Entrepreneurs to start and stay in Europe.In 2018, the vast majority (92 %) of EU enterprises with at least 10 persons employed used the internet 77 % of EU enterprises have a website. In 2018, 1 in 5 EU enterprises employed ICT specialists; the percentage of large enterprises employing ICT specialists (75 %) is more than 4 times higher than that for SMEs (18 %) [3]. The difference in big data and Internet usage statistics may be explained by an understanding of the benefits and personal routine of the latest one in life. Once upon a time, such breakthrough and unusual things became household. And big data or the Internet of Things are not conscious, and it is impossible to build a business model on the obscure.That is a problem for the leaders of European companies in understanding the potential of artificial intelligence or data analytics. It is not just about that Europe needs an additional 20 million early stage entrepreneurs to close the gap with the US and China, but also about a high intensity of utilization of novel digital technologies (social, big data, mobile and cloud solutions) to improve business operations, invent new models in three directions:– ability to reach out and partner with customers more effectively;– inner processes;– business models - the way value is created, delivered and captured.The digital transformation from a European management angle should include strategic vision how to combine analytical, software and information systems with business. Skills for European business development in digital era refers to leadership in the new era, it does not matter traditional or digital business – the reality is changing in general.The skills for European business development must combine:- ICT user skills to apply systems as tools in support of own work. It can be common software tools or specialised tools supporting business functions within industry;- skills required for researching, developing, designing, strategic planning, managing, producing, consulting, marketing, selling, integrating, installing, administering, maintaining, supporting and servicing ICT systems;- business skills correspond to the capabilities to exploit opportunities provided by ICT; to ensure more efficient and effective performance of different types of organisations; to explore possibilities for new ways of conducting business and organisational processes; and to establish new businesses [3].A little different set of skills is proposed by publication “E-Leadership Digital Skills for SMEs” of the EC [4]:- strategic leadership skills to lead inter‐disciplinary staff, and influence stakeholders across boundaries (functional, geographic);– business savvy: innovate business and operating models, delivering value to organisations;– digital savvy: envision and drive change for business performance, exploiting digital technology trends as innovation opportunities [4].To summarize, it should be noted that the new era creates opportunities for all businesses, the question is to see these opportunities and successfully use them. Entrepreneurship and business may not be fundamentally changing at any time, but the tool usage requires understanding, if not details, so the overall potential and range of capabilities of this tool. In the digital age, digital skills (both basic and complex) are understanding how to use the tool. On the other hand, the strategic leadership skills and the business savvy.","БІЗНЕС, ІННОВАЦІЇ, МЕНЕДЖМЕНТ: ПРОБЛЕМИ ТА ПЕРСПЕКТИВИ",SKILLS FOR EUROPEAN BUSINESS DEVELOPMENT IN DIGITAL ERA,,,,core
322843592,2020-01-23T00:00:00,"International audienceResource management in SDN (e.g. network slicing) is an emerging area that attracts the attention of academia and industry. It is an indispensable technology in 5G systems. To effectively manage and optimize network resources, more intelligence needs to be deployed. Therefore, combining real network data and Machine Learning (ML) with the benefits of SDN can be a promising solution to manage the network resources in an automated and intelligent way. However, a real network dataset can have redundant and unneeded features. Also, ML algorithms are as good as the quality of data and the SDN is a time-critical system that requires real-time processing and decision. Thus, data preprocessing is a necessary task, which helps to keep the relevant features and makes the prediction quicker and more accurate.This work presents a comparative analysis between two feature selection methods, which are Recursive Feature Elimination (RFE) and Information Gain Attribute Evaluation (InfoGain), using several classifiers on different reduced versions of the network’s dataset",HAL CCSD,Network Feature Selection based on Machine Learning for Resource Management,,,,core
327076110,2020-10-29T00:00:00,"Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models",'Institution of Engineering and Technology (IET)',"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications",10.1049/iet-cvi.2019.0897,http://arxiv.org/abs/2007.13404,,core
276538677,2020,"Proximity beacons are small, low-power devices capable of transmitting information at a limited distance via Bluetooth low energy protocol. These beacons are typically used to broadcast small amounts of location-dependent data (e.g., advertisements) or to detect nearby objects. However, researchers have shown that beacons can also be used for indoor localization converting the received signal strength indication (RSSI) to distance information. In this work, we study the effectiveness of proximity beacons for accurately locating objects within a manufacturing plant by performing extensive experiments in a real industrial environment. To this purpose, we compare localization algorithms based either on trilateration or environment fingerprinting combined with a machine-learning based regressor (k-nearest neighbors, support-vector machines, or multi-layer perceptron). Each algorithm is analyzed in two different types of industrial environments. For each environment, various configurations are explored, where a configuration is characterized by the number of beacons per square meter and the density of fingerprint points. In addition, the fingerprinting approach is based on a preliminary site characterization; it may lead to location errors in the presence of environment variations (e.g., movements of large objects). For this reason, the robustness of fingerprinting algorithms against such variations is also assessed. Our results show that fingerprint solutions outperform trilateration, showing also a good resilience to environmental variations. Given the similar error obtained by all three fingerprint approaches, we conclude that k-NN is the preferable algorithm due to its simple deployment and low number of hyper-parameters",'MDPI AG',A Comparison Analysis of BLE-Based Algorithms for Localization in Industrial Environments,10.3390/electronics9010044,https://core.ac.uk/download/pdf/276538677.pdf,,core
334984749,2020-01-01T08:00:00,"With the rapid development in wireless communications and artificial intelligence in recent years, vehicles are equipped with various smart devices to have ubiquitous access to the internet. These smart vehicles can easily exchange information with surrounding objects, e.g. vehicles, pedestrians, and roadside units, which form an enormous network. These networks that built upon vehicles are named vehicular networks. As the key enabler of the Intelligent Transportation System, vehicular networks are envisioned to simplify the traffic management, improve road safety, and provide infotainment through various vehicular services, like vehicle to everything communications, vehicular fog computing, and the location-based services. However, vehicular networks are vulnerable to various security and privacy risks due to the wireless nature and the heterogeneous network attributes. How to provide security and privacy for vehicular networks has attracted great attentions from both industry and academia. So in this dissertation, we present a study of security and privacy of vehicular networks. Novel security and privacy solutions for different vehicular services are proposed, which address the security and privacy issues in vehicle-to-everything communications, fast handover, vehicular fog computing, and location-based services. The major contributions of this dissertation are shown as follows. 1) A secure and efficient privacy-preserving authentication scheme is proposed. The proposed scheme is based on a 5G software-defined vehicular network architecture and is proved to be secure and highly efficient. 2) A secure and efficient handover scheme is proposed to provide user uninterrupted vehicular service, which is critical for the frequent handover processes on high-speed vehicles. 3) An efficient group management and key distribution scheme is proposed for vehicular fog computing paradigm, which has a lower delay compared to other existing schemes. 4) A location privacy-preserving scheme is proposed for location-based services. With the proposed scheme, vehicle users can have real-time location-based services with accurate location information updates while preserving location privacy. At the end of this dissertation, we also point out open research issues in securing vehicular networks",DigitalCommons@University of Nebraska - Lincoln,A Study of Security and Privacy in Vehicular Networks,,,,core
378425558,2020-02-17T00:00:00,"In the era of the fourth industrial revolution (Industry 4.0), many Management Information Systems (MIS) integrate real-time data collection and use technologies such as big data, machine learning, and cloud computing, to foster a wide range of creative innovations, business improvements, and new business models and processes. However, the integration of blockchain with MIS offers the blockchain trilemma of security, decentralisation and scalability. MIS are usually Web 2.0 clientserver applications that include the front end web systems and back end databases; while blockchain systems are Web 3.0 decentralised applications. MIS are usually private systems that a single party controls and manages; while blockchain systems are usually public, and any party can join and participate. This paper clariﬁes the key concepts and illustrates with ﬁgures, the implementation of public, private and consortium blockchains on the Ethereum platform. Ultimately, the paper presents a framework for building a private blockchain system on the public Ethereum blockchain. Then,integrating the Web 2.0 client-server applications that are commonly used in MIS with Web 3.0 decentralised blockchain applications",'Institute of Electrical and Electronics Engineers (IEEE)',Integration of blockchains with management information systems,10.1109/MoRSE48060.2019.8998694,,,core
335012695,2020-05-19T00:00:00,"Nowadays, digitization is transforming the way businesses work. Recently, Artificial Intelligence (AI) techniques became an essential part of the automation of business processes: In addition to cost advantages, these techniques offer fast processing times and higher customer satisfaction rates, thus ultimately increasing sales. One of the intelligent approaches for accelerating digital transformation in companies is the Robotic Process Automation (RPA). 

An RPA-system is a software tool that robotizes routine and time-consuming responsibilities such as email assessment, various calculations, or creation of documents and reports (Mohanty and Vyas, 2018). Its main objective is to organize a smart workflow and therethrough to assist employees by offering them more scope for cognitively demanding and engaging work.



Intelligent Process Automation (IPA) offers all these advantages as well; however, it goes beyond the RPA by adding AI components such as Machine- and Deep Learning techniques to conventional automation solutions. Previously, IPA approaches were primarily employed within the computer vision domain. However, in recent times, Natural Language Processing (NLP) became one of the potential applications for IPA as well due to its ability to understand and interpret human language. Usually, NLP methods are used to analyze large amounts of unstructured textual data and to respond to various inquiries. However, one of the central applications of NLP within the IPA domain – are conversational interfaces (e.g., chatbots, virtual agents) that are used to enable human-to-machine communication. Nowadays, conversational agents gain enormous demand due to their ability to support a large number of users simultaneously while communicating in a natural language. The implementation of a conversational agent comprises multiple stages and involves diverse types of NLP sub-tasks, starting with natural language understanding (e.g., intent recognition, named entity extraction) and going towards dialogue management (i.e., determining the next possible bots action) and response generation. Typical dialogue system for IPA purposes undertakes straightforward customer support requests (e.g., FAQs), allowing human workers to focus on more complicated inquiries.



In this thesis, we are addressing two potential Intelligent Process Automation (IPA) applications and employing statistical Natural Language Processing (NLP) methods for their implementation.



The first block of this thesis (Chapter 2 – Chapter 4) deals with the development of a conversational agent for IPA purposes within the e-learning domain. As already mentioned, chatbots are one of the central applications for the IPA domain since they can effectively perform time-consuming tasks while communicating in a natural language. Within this thesis, we realized the IPA conversational bot that takes care of routine and time-consuming tasks regularly performed by human tutors of an online mathematical course. This bot is deployed in a real-world setting within the OMB+ mathematical platform. Conducting experiments for this part, we observed two possibilities to build the conversational agent in industrial settings – first, with purely rule-based methods, considering the missing training data and individual aspects of the target domain (i.e., e-learning). Second, we re-implemented two of the main system components (i.e., Natural Language Understanding (NLU) and Dialogue Manager (DM) units) using the current state-of-the-art deep-learning architecture (i.e., Bidirectional Encoder Representations from Transformers (BERT)) and investigated their performance and potential use as a part of a hybrid model (i.e., containing both rule-based and machine learning methods).



The second part of the thesis (Chapter 5 – Chapter 6) considers an IPA subproblem within the predictive analytics domain and addresses the task of scientific trend forecasting. Predictive analytics forecasts future outcomes based on historical and current data. Therefore, using the benefits of advanced analytics models, an organization can, for instance, reliably determine trends and emerging topics and then manipulate it while making significant business decisions (i.e., investments). In this work, we dealt with the trend detection task – specifically, we addressed the lack of publicly available benchmarks for evaluating trend detection algorithms. We assembled the benchmark for the detection of both scientific trends and downtrends (i.e., topics that become less frequent overtime). To the best of our knowledge, the task of downtrend detection has not been addressed before. The resulting benchmark is based on a collection of more than one million documents, which is among the largest that has been used for trend detection before, and therefore, offers a realistic setting for the development of trend detection algorithms.Robotergesteuerte Prozessautomatisierung (RPA) ist eine Art von Software-Bots, die manuelle menschliche Tätigkeiten wie die Eingabe von Daten in das System, die Anmeldung in Benutzerkonten oder die Ausführung einfacher, aber sich wiederholender Arbeitsabläufe nachahmt (Mohanty and Vyas, 2018). Einer der Hauptvorteile und gleichzeitig Nachteil der RPA-bots ist jedoch deren Fähigkeit, die gestellte Aufgabe punktgenau zu erfüllen. Einerseits ist ein solches System in der Lage, die Aufgabe akkurat, sorgfältig und schnell auszuführen. Andererseits ist es sehr anfällig für Veränderungen in definierten Szenarien. Da der RPA-Bot für eine bestimmte Aufgabe konzipiert ist, ist es oft nicht möglich, ihn an andere Domänen oder sogar für einfache Änderungen in einem Arbeitsablauf anzupassen (Mohanty and Vyas, 2018). Diese Unfähigkeit, sich an veränderte Bedingungen anzupassen, führte zu einem weiteren Verbesserungsbereich für RPAbots – den Intelligenten Prozessautomatisierungssystemen (IPA).



IPA-Bots kombinieren RPA mit Künstlicher Intelligenz (AI) und können komplexe und kognitiv anspruchsvollere Aufgaben erfüllen, die u.A. Schlussfolgerungen und natürliches Sprachverständnis erfordern. Diese Systeme übernehmen zeitaufwändige und routinemäßige Aufgaben, ermöglichen somit einen intelligenten Arbeitsablauf und befreien Fachkräfte für die Durchführung komplizierterer Aufgaben.  Bisher wurden die IPA-Techniken hauptsächlich im Bereich der Bildverarbeitung eingesetzt. In der letzten Zeit wurde die natürliche

Sprachverarbeitung (NLP) jedoch auch zu einem der potenziellen Anwendungen für IPA, und zwar aufgrund von der Fähigkeit, die menschliche Sprache zu interpretieren. NLP-Methoden werden eingesetzt, um große Mengen an Textdaten zu analysieren und auf verschiedene

Anfragen zu reagieren. Auch wenn die verfügbaren Daten unstrukturiert sind oder kein vordefiniertes Format haben (z.B. E-Mails), oder wenn die in einem variablen Format vorliegen (z.B. Rechnungen, juristische Dokumente), dann werden ebenfalls die NLP Techniken angewendet, um die relevanten Informationen zu extrahieren, die dann zur Lösung verschiedener Probleme verwendet werden können.



NLP im Rahmen von IPA beschränkt sich jedoch nicht auf die Extraktion relevanter Daten aus Textdokumenten. Eine der zentralen Anwendungen von IPA sind Konversationsagenten, die zur Interaktion zwischen Mensch und Maschine eingesetzt werden. Konversationsagenten erfahren enorme Nachfrage, da sie in der Lage sind, eine große Anzahl von Benutzern gleichzeitig zu unterstützen, und dabei in einer natürlichen Sprache kommunizieren. Die Implementierung

eines Chatsystems umfasst verschiedene Arten von NLP-Teilaufgaben, beginnend mit dem Verständnis der natürlichen Sprache (z.B. Absichtserkennung, Extraktion von Entitäten) über das Dialogmanagement (z.B. Festlegung der nächstmöglichen Bot-Aktion) bis hin zur Response-Generierung. Ein typisches Dialogsystem für IPA-Zwecke übernimmt in der Regel unkomplizierte Kundendienstanfragen (z.B. Beantwortung von FAQs), so dass sich die Mitarbeiter auf komplexere Anfragen konzentrieren können.



Diese Dissertation umfasst zwei Bereiche, die durch das breitere Thema vereint sind, nämlich die Intelligente Prozessautomatisierung (IPA) unter Verwendung statistischer Methoden der natürlichen Sprachverarbeitung (NLP).



Der erste Block dieser Arbeit (Kapitel 2 – Kapitel 4) befasst sich mit der Impementierung eines Konversationsagenten für IPA-Zwecke innerhalb der E-Learning-Domäne. Wie bereits erwähnt, sind Chatbots eine der zentralen Anwendungen für die IPA-Domäne, da sie zeitaufwändige Aufgaben in einer natürlichen Sprache effektiv ausführen können. Der IPA-Kommunikationsbot, der in dieser Arbeit realisiert wurde, kümmert sich ebenfalls um routinemäßige und zeitaufwändige Aufgaben, die sonst von Tutoren in einem Online-Mathematikkurs in deutscher Sprache durchgeführt werden. Dieser Bot ist in der täglichen Anwendung innerhalb der mathematischen Plattform OMB+ eingesetzt. Bei der Durchführung von Experimenten beobachteten wir zwei Möglichkeiten, den Konversationsagenten im industriellen Umfeld zu entwickeln – zunächst mit rein regelbasierten Methoden, unter Bedingungen der fehlenden Trainingsdaten und besonderer Aspekte der Zieldomäne (d.h. E-Learning). Zweitens haben wir zwei der Hauptsystemkomponenten (Sprachverständnismodul, Dialog-Manager) mit dem derzeit fortschrittlichsten Deep Learning Algorithmus reimplementiert und die Performanz dieser Komponenten untersucht.



Der zweite Teil der Doktorarbeit (Kapitel 5 – Kapitel 6) betrachtet ein IPA-Problem innerhalb des Vorhersageanalytik-Bereichs. Vorhersageanalytik zielt darauf ab, Prognosen über zukünftige Ergebnisse auf der Grundlage von historischen und aktuellen Daten zu erstellen. Daher kann ein Unternehmen mit Hilfe der Vorhersagesysteme z.B. die Trends oder neu entstehende Themen zuverlässig bestimmen und diese Informationen dann bei wichtigen Geschäftsentscheidungen (z.B. Investitionen) einsetzen. In diesem Teil der Arbeit beschäftigen wir uns mit dem Teilproblem der Trendprognose – insbesondere mit dem Fehlen öffentlich zugänglicher Benchmarks für die Evaluierung von Trenderkennungsalgorithmen. Wir haben den Benchmark zusammengestellt und veröffentlicht, um sowohl Trends als auch Abwärtstrends zu erkennen. Nach unserem besten Wissen ist die Aufgabe der Abwärtstrenderkennung bisher nicht adressiert worden. Der resultierende Benchmark basiert auf einer Sammlung von mehr als einer Million Dokumente, der zu den größten gehört, die bisher für die Trenderkennung verwendet wurden, und somit einen realistischen Rahmen für die Entwicklung von Trenddetektionsalgorithmen bietet",Ludwig-Maximilians-Universität München,Statistical natural language processing methods for intelligent process automation,,https://core.ac.uk/download/335012695.pdf,,core
335349820,2020-01-01T00:00:00,"L’internet des objets industriels (IIoT) fait partie d’un concept plus large connu sous le nom de L’Internet des Objets, ou IdO (en anglais Internet of Things, ou IoT). Les IIoT apportent de nouvelles opportunités aux sites de production telles que la diminution des coûts des opérations et l’augmentation de la productivité dans le but d’une exploitation optimale. La technologie IIoT révolutionnera les procédés de fabrication industrielle en permettant l’acquisition des quantités importantes de données, à des vitesses beaucoup plus élevées, et bien plus efficaces qu’auparavant. Un certain nombre d’entreprises innovantes ont commencé à mettre en oeuvre l’IIoT en exploitant des appareils connectés intelligents dans leurs usines (c’est ce qu’on appelle les usines intelligentes ou Industrie 4.0). Dans une perspective d’acquisition des données, l’internet des objets a favorisé l’inclusion des sous-systèmes ainsi que leurs analyses en temps réel. Pour ce faire, l’Industrie 4.0 introduit un concept de production numérisée en permettant une intégration souple et agile de nouveaux modèles commerciaux tout en maintenant les coûts de fabrications et l’efficacité à un niveau raisonnable. Dans ce projet de recherche, nous allons étudier la maintenance prédictive des installations industrielles. Cette tâche est essentielle au bon fonctionnement de l’usine et à la sécurité des personnes. Compte tenu des coûts, il est judicieux d’établir un bon équilibre entre entretien préventif systématique et entretien correctif. La surveillance des installations concourt à limiter le niveau d’entretien préventif. Dans ce contexte, l’analyse vibratoire constitue un outil de détection puis de diagnostic de défauts de fonctionnement des installations. Aussi, après avoir décrit les principales manifestations vibratoires des défauts de fonctionnement des machines, nous allons examiner les stratégies de détection et de surveillance dans le domaine temporel et fréquentiel et la démarche de diagnostic en nous appuyant sur l’intelligence artificielle et en analysant les signaux vibratoires permettant de déduire une politique de gestion de la maintenance. Notre objectif principal est de réaliser un système permettant d’assurer l’analyser les signaux vibratoires d’une machine tournante dans le domaine temps/fréquence. Il sera ensuite aisé de le comparer avec un système d’apprentissage automatique capable de détecter et classer les défauts grâce à des algorithmes d’intelligence artificielle. L’application vise à fournir un système de détection de défauts fiable afin de réduire les temps de dépannages et favoriser un diagnostic rapide des pannes des systèmes industriels. Par conséquent, le projet va de la conception jusqu’à la mise en oeuvre des algorithmes informatiques avec des exemples réels de signaux vibratoires. Un processus d’optimisation sera mis en oeuvre lors de la prise des décisions de l’équipe humaine afin d’augmenter l’efficacité des résultats et réduire les situations à risque qui mettront les machines hors d’usage. Ce projet de recherche permettra donc d’introduire un système intelligent dans un environnement de production de l’industrie 4.0. Durant cette étude, nous avons implémenté dans un premier temps des algorithmes qui nous ont permis d’extraire des caractéristiques des signaux d’une machine tournante. Ensuite, nous avons mis en place un système de surveillance de l’état de cette machine en fixant un seuil pour le bon fonctionnement et un autre pour déclencher une alarme quand ce dernier est atteint. Dans un second temps, nous avons utilisé des algorithmes d’apprentissage automatique (ou machine learning) pour classer les différents niveaux de défaillance. Après extraction des caractéristiques des signaux dans le domaine temporel et fréquentiel nous avons obtenu une fiabilité de 99.3% avec la méthode d’estimation dite « validation croisée » (en anglais cross-validation). Ce processus d’apprentissage optimise les paramètres du modèle afin que celui-ci corresponde aux données le mieux possible. Ensuite, nous avons évalué une autre technique de validation « testset validation » (en anglais holdout method). Cette technique est recommandée pour les grands ensembles de données. Après plusieurs tests, nous avons pu obtenir un taux de classification de 100% pour les différents niveaux de défauts considérés.



The internet of industrial objects (IIoT) is part of a larger concept known as the ""Internet of Things (IoT)"". IoT's bring new opportunities to production sites such as lower operating costs and increased productivity for optimal operation. The application of IoT to the manufacturing industry is called IIoT (or Industrial Internet or Industry 4.0). The IoT will revolutionize manufacturing by enabling the acquisition and accessibility of massive amounts of data at much higher speeds and far more efficiently than before. A few innovative companies have started to implement IoT by exploiting smart connected devices in their factories (so-called smart factories or Industry 4.0). From a data acquisition perspective, the Internet of Things has favoured the inclusion of subsystems and their analysis in real time. To achieve this, Industry 4.0 introduces a digitized production concept by allowing flexible and agile integration of new business models while keeping manufacturing costs and efficiency at a reasonable level. In this thesis, we will study the predictive maintenance for industry 4.0. This method of preventing asset failure by analyzing production data to identify patterns and predict issues before they happen. However, considering the costs, it is wise to strike a better balance between routine preventative maintenance and corrective maintenance. Facility monitoring helps to reduce the level of preventative maintenance. In this context, vibration analysis is a tool for detecting and then diagnosing malfunctions of installations. Also, after describing the main vibratory manifestations of failures of the machines, we will examine the detection and surveillance strategies at the time and frequency domain and the diagnosis process based on artificial intelligence by analyzing the vibratory signals. Our main objective is to design a system that will analyze the vibrating signals of a rotating machine in the time/frequency domain. A machine learning system will be used to detect and classify the defects based on artificial intelligence algorithms. The application aims to provide a reliable fault detection system to reduce repair times and promote rapid diagnosis of industrial operations. Therefore, the project goes from the design to the implementation of new algorithms using vibratory signals. An optimization process will be implemented when the decisions of the adequate staff are made to increase the efficiency of the results and reduce the risk situations that will put the machines out of use. Therefore, this research project will introduce an intelligent system into an industry 4.0 production environment. During this research project, firstly, we will implement algorithms that allowed us to extract characteristics of the vibration signals and set up a system to monitor the state of a rotating machine by setting a threshold of operation and trigger an alarm when this threshold is reached. In a second step, we will use signal processing and machine learning toolkits to classify the different levels of machine failure and use this method to detect the presence of error when the machine is running. After extracting the characteristics of our signals at the time and frequency domain, we will be obtained with the ""cross-validation"" a recognition rate of 99.3%. The final implementation with the ""holdout validation"" recommended for large datasets allowed us to have a classification rate of 100% of the different levels of defects considered",,Application des algorithmes d’apprentissage automatique pour la détection de défauts de roulements sur les machines tournantes dans le cadre de l’Industrie 4.0,,https://core.ac.uk/download/335349820.pdf,,core
304996047,2020-01-14T14:48:19,"Due to the recent movements in Industry 4.0 and Internet of Things (IoT), accessing or generating data in the Smart Manufacturing (SM) domain has become more attainable; communication protocols such as MTConnect and OPC-UA provide access to a majority of raw data generated from machine tools while retrofit sensor packs facilitate high- frequency data acquisitions from legacy and modern equipment. These technologies have led to the generation of quantities of raw data, known as Big Data (BD), that are complex to be analyzed. Current IoT architectures and frameworks propose Cloud Computing (CC) and Centralized Training (CT) as the addressing solutions for BD and collaborative Machine Learning (ML) models. These solutions, however, have limitations such as Internet dependency and requiring expensive and high-performance cloud resources. As more data are generated, a higher performance framework is required for cloud computing of larger datasets that are either historical in nature or generated from an ever-increasing ubiquitous sensors and sensor arrays that are deployed in modern manufacturing operations. Studying IoT architectures and stream analytics is essential for creation of successful IoT platforms. In this regard, this study proposes a novel, high-performance, and data- driven IoT architecture that considers automated and scalable machine learning techniques with the focus of process control and deeper understanding of manufacturing process and systems performance in the Cyber-Physical Systems (CPS) domain. In this dissertation, first, a novel generalized three-layer IoT architecture utilizing Edge Computing (EC), Fog Computing (FC), CC, and Federated Learning (FL) is presented, where data are preprocessed in the Edge layer, ML models are incrementally trained in the Fog layer and the resulting elements of training are aggregated in the centralized cloud models. Second, two novel stream analytics engines of Outlier Detection and Bayesian Classification, capable of real-time (RT) training and prediction are proposed and analyzed for this architecture. Results show that the training latency for both the Outlier and the Bayesian engines as well as their FL algorithms remained constant as the number of data points increased. On a 1000 data point dataset, the training performances for an upcoming data point for the Outlier and Bayesian engines were on average 136 and 48 times faster, respectively, than retraining the models with all of the data points. These results suggest that the methods discussed in the proposed architecture can lead to the development of higher performance and more scalable IoT frameworks that require lower storage and computing power.Ph.D",Georgia Institute of Technology,Automated real-time machine learning for IOT for manufacturing a cloud architecture and API,,https://core.ac.uk/download/304996047.pdf,,core
287831001,2020-01-01T00:00:00,"Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the exponential Monte Carlo with counter (EMCQ). Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not (historically) be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to show the degree of interaction required to cover the code as well as finding faults. We applied Q-EMCQ on 37 real-world industrial programs written in Function Block Diagram (FBD) language, which is used for developing a train control management system at Bombardier Transportation Sweden AB. The results show that Q-EMCQ is an efficient technique for test case generation. Addition- ally, unlike the t-wise test suite generation, which deals with the minimization problem, we have also subjected Q-EMCQ to a maximization problem involving the general module clustering to demonstrate the effectiveness of our approach. The results show the Q-EMCQ is also capable of outperforming the original EMCQ as well as several recent meta/hyper-heuristic including modified choice function, Tabu high-level hyper-heuristic, teaching learning-based optimization, sine cosine algorithm, and symbiotic optimization search in clustering quality within comparable execution time",'Springer Science and Business Media LLC',An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications,10.1007/s00500-020-04769-z,,,core
357295173,2020-04-02T00:00:00,"a b s t r a c t Environmental monitoring is nowadays an important task in many industrial operations. In order to comply with strong environmental laws, they have implemented monitoring systems based on a network of air quality and meteorological stations providing real-time measurements of key variables associated to the distribution of pollutants in surrounding areas. These measurements can be contaminated by outliers, which must be discarded in order to have a consistent set of data. This work presents a nonlinear procedure for outliers detection based on residual analysis of regression with Partial Least Squares and Artificial Neural Networks. In order to minimize the negative effect of outliers in the training dataset a learning algorithm with regularization is proposed. This algorithm is based on a Quasi-Newton optimization method and it was tested on a simulated nonlinear process, on real data from environmental monitoring contaminated with synthetic outliers, and finally applied to a real environmental monitoring data obtained from a monitoring station and having natural outliers. The results are encouraging and further developments are foreseen for including information from neighboring stations and emission source operation",,Outliers detection in environmental monitoring databases,,,,core
347128417,2020-05-01T00:00:00,"Optical wireless communications (OWC) has recently gained a lot of interest among industrial and academic communities. The main inhibitor factor of this resurgence of interest is the fact that radio-frequency (RF) spectrum is already so densely occupied to handle the increasingly high demand, and hence, exploring higher frequency spectrum, including the optical range, would be a relief. Another reason behind such an interest resides in the relatively simple deployment of OWC systems. However, before a real deployment of OWC systems, there is a persistent need to establish its fundamental performance limits (e.g. capacity, secrecy capacity, and capacity region) and extract design guidelines for building efficient, reliable, and secure OWC systems. Indeed, due to different propagation channels and different transmit constraints, RF communications and OWC are fundamentally quite different. For instance, the popular intensity modulation and direct detection (IM-DD), which is a favorable scheme for OWC due to its simplicity, has some subtle differences in comparison with RF systems manifested in the nonnegativity of the transmit signal, in addition to constraints on the peak- and average-intensity of the signal. These, in turn, make the fundamental performance limits and the optimal transmission schemes for OWC based on IM-DD different from those for RF systems. Since the fundamental performance limits of OWC play a vital role in extracting guidelines and communication protocols for designing reliable and secure systems, this dissertation addresses those limits in an OWC setting. Particularly, this dissertation presents novel contributions to the understanding of the fundamental limits of multiuser OWC with and without secrecy constraints. When a secrecy constraint is imposed, this dissertation provides analytical results on the characterization of the optimal transmission schemes for secure and reliable OWC when input-dependent Gaussian noise and Poisson noise models are considered. Additionally, an asymptotic analysis of the secrecy capacity (the fundamental performance limit for secure communications) is presented. Furthermore, a two-user optical multiple access channel model, which depicts a multiuser OWC scenario without secrecy constraints, is proposed and the optimal multiuser transmission schemes that achieve the capacity region (fundamental performance limit of this multiuser scenario) are developed. Moreover, the capacity region of the considered optical multiple access channel is explicitly characterized in a closed-form expression in the regime where the peak- and average-intensity constraints are vanishingly small. After establishing the fundamental performance limits of OWC, powerful machine learning techniques, such as deep learning, are employed for the implementation of OWC systems. In particular, a simple and cost-effective learning-based system with (near-)optimal performance is proposed and is implemented by merely taking off-the-shelf deep learning models, applying them to an OWC design problem, and tuning them based on the easily generated training data.doctoral, Ph.D., Electrical and Computer Engineering -- University of Idaho - College of Graduate Studies, 2020-0",,Fundamental Limits of Multiuser Optical Wireless Communications With and Without Secrecy Constraints,,,,core
387278457,2020-11-14T00:00:00,"UML Class diagram is very important to visualize the whole software we are
working on and helps understand the whole system in the easiest way possible by
showing the system classes, its attributes, methods, and relations with other
objects. In the real world, there are two types of Class diagram engineers work
with namely 1) Forward Engineered Class Diagram (FwCD) which are hand-made as
part of the forward-looking development process, and 2). Reverse Engineered
Class Diagram (RECD) which are those diagrams that are reverse engineered from
the source code. In the software industry while working with new open software
projects it is important to know which type of class diagram it is. Which UML
diagram was used in a particular project is an important factor to be known? To
solve this problem, we propose to build a classifier that can classify a UML
diagram into FwCD or RECD. We propose to solve this problem by using a
supervised Machine Learning technique. The approach in this involves analyzing
the features that are useful in classifying class diagrams. Different Machine
Learning models are used in this process and the Random Forest algorithm has
proved to be the best out of all. Performance testing was done on 999 Class
diagrams",,"Classification of Reverse-Engineered Class Diagram and
  Forward-Engineered Class Diagram using Machine Learning",,http://arxiv.org/abs/2011.07313,,core
334194949,2020-01-01T00:00:00,"While data is becoming more and more pervasive and ubiquitous in today’s life, businesses in modern societies prefer to take advantage of using data, in particular Big Data, in their decision-making and analytical processes to increase their product efficiency. Software applications which are being utilized in the airline industry are one of the most complex and sophisticated ones for which conducting of data analyzing techniques can make many decision making processes easier and faster. Flight delays are one of the most important areas under investigation in this area because they cause a lot of overhead costs to the airline companies on one hand and airports on the other hand. The aim of this study project is to utilize different machine learning algorithms on real world data to be able to predict flight delays for all causes like weather, passenger delays, maintenance, airport congestion etc in order to create more efficient flight schedules. We will use python as the programming language to create an artifact for our prediction purposes.  We will analyse different algorithms from the accuracy perspective and propose a combined method in order to optimize our prediction results",Malmö universitet/Teknik och samhälle,Efficient flight schedules with utilizing Machine Learning prediction algorithms,,,,core
326502685,2020-07-01T00:00:00,"Neural network architectures are most often conceptually designed and
described in visual terms, but are implemented by writing error-prone code.
PrototypeML is a machine learning development environment that bridges the
dichotomy between the design and development processes: it provides a highly
intuitive visual neural network design interface that supports (yet abstracts)
the full capabilities of the PyTorch deep learning framework, reduces model
design and development time, makes debugging easier, and automates many
framework and code writing idiosyncrasies. In this paper, we detail the deep
learning development deficiencies that drove the implementation of PrototypeML,
and propose a hybrid approach to resolve these issues without limiting network
expressiveness or reducing code quality. We demonstrate the real-world benefits
of a visual approach to neural network design for research, industry and
teaching. Available at https://PrototypeML.comComment: 10 pages, 6 figures. Submitted to NeurIPS 2020. More details
  available at https://PrototypeML.co",,"PrototypeML: A Neural Network Integrated Design and Development
  Environment",,http://arxiv.org/abs/2007.01097,,core
322968728,2020-05-14T00:00:00,"Assuming hardware is the major constraint for enabling real-time mobile
intelligence, the industry has mainly dedicated their efforts to developing
specialized hardware accelerators for machine learning and inference. This
article challenges the assumption. By drawing on a recent real-time AI
optimization framework CoCoPIE, it maintains that with effective
compression-compiler co-design, it is possible to enable real-time artificial
intelligence on mainstream end devices without special hardware. CoCoPIE is a
software framework that holds numerous records on mobile AI: the first
framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,
language models, and so on; the fastest DNN pruning and acceleration framework,
up to 180X faster compared with current DNN pruning on other frameworks such as
TensorFlow-Lite; making many representative AI applications able to run in
real-time on off-the-shelf mobile devices that have been previously regarded
possible only with special hardware support; making off-the-shelf mobile
devices outperform a number of representative ASIC and FPGA solutions in terms
of energy efficiency and/or performance",,"CoCoPIE: Making Mobile AI Sweet As PIE --Compression-Compilation
  Co-Design Goes a Long Way",,http://arxiv.org/abs/2003.06700,,core
323920600,2020-06-05T00:00:00,"The advent of data-driven real-time applications requires the implementation
of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's
Tensor Processing Unit (TPU) is one such neural network accelerator that uses
systolic array-based matrix multiplication hardware for computation in its
crux. Manufacturing faults at any state element of the matrix multiplication
unit can cause unexpected errors in these inference networks. In this paper, we
propose a formal model of permanent faults and their propagation in a TPU using
the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed
using the probabilistic model checking technique to reason about the likelihood
of faulty outputs. The obtained quantitative results show that the
classification accuracy is sensitive to the type of permanent faults as well as
their location, bit position and the number of layers in the neural network.
The conclusions from our theoretical model have been validated using
experiments on a digit recognition-based DNN.Comment: 4 pages, 2 figure",,"High-level Modeling of Manufacturing Faults in Deep Neural Network
  Accelerators",,http://arxiv.org/abs/2006.03616,,core
369759401,2020-01-01T00:00:00,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing
availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by
artificial intelligence, delivering smart manufacturing and mass customization while improving resource
efficiency. However, challenges towards the development of intelligent automation architectures include
the lack of interoperability between systems, complex data preparation steps, and the inability to deal
with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial
frameworks focused on the development of standardized system architectures for Industry 4.0, closing
the gap between generic architectures and physical realizations. It proposes a platform for intelligent
automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs
in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and
automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard
over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of
the challenges are handled using fog computing and a multi-workload database. This paper presents an
implementation of the platform in the pharmaceutical industry, providing interoperability and real-time
reaction capability to changes to an industrial prototype using dynamic scheduling algorithms",'Elsevier BV',A Middleware Platform for Intelligent Automation: An Industrial Prototype Implementation,10.1016/j.compind.2020.103329,https://core.ac.uk/download/369759401.pdf,,core
355099865,2020-01-01T00:00:00,"Abstract: Poor management practices of road transport assets posed a challenge to the sustainable development of the transport system in developing countries like Nigeria. Studies in the past focused mainly on the performance of road construction process. However, few studies have evaluated the effect of the fourth industrial revolution (4.0IR) on the road transport assets in developing countries such as Nigeria. The current study aimed at assessing the effect of the fourth industrial revolution towards improving the management practice of road transport assets. Survey instruments were administered to project and facility managers in the Nigerian road construction sector of the economy using a proportionate random sampling technique. Partial least square structural equation modelling was used for data analysis utilising the Warp 7.0 PLS-SEM software algorithm. The software calculates p-values with WarpPLS based on non-parametric algorithms, resampling or stable algorithms and thus does not require that the variables to be normally distributed. The study concluded that 4.0IR drivers have a moderate effect change on the management practice of road transport assets in Nigeria at the moment. The findings imply that management of road assets in Nigeria would moderately improve due to 4.0IR technologies resulting in transport, safety and general efficiency and effectiveness of road networks in Nigeria. The study identified 4.0IR drivers to include; robotics, mobility, virtual and augmented reality, internet of things and cloud computing, machine learning, artificial intelligence, blockchain, 3D printing drones that are built with an attached 3D printer, (the drone hangs a 3D printing nozzle that's fed plastic, concrete mix or other material from a tube connected to the top of the drone's printing path that precisely plotted by software, for a promised printing accuracy of 0.1mm),and digital engineering. This study emanated from the government reports and past studies in the area of road transport asset management practice which the study investigated the major causes of poor practices and assessed the effect of the fourth industrial revolution on the practice",,Effect of the Fourth Industrial Revolution on Road Transport Asset Management Practice in Nigeria,,,,core
355100275,2020-01-01T00:00:00,"Abstract: DC motor is extensively used in various industrial applications such as robotics, automobiles, toys and many other motoring applications. This is attributable to their extraordinary flexibility, durability and low implementation cost. To obtain the desired output based on the use of the DC motor, it is imperative to control the speed, position, torque and other variables of the DC motor. Many classical techniques have been utilized in the past to control the DC motor, however, such classical methods typically take a long time, particularly when used for complex nonlinear systems. The use of metaheuristic algorithms as a way of implementing artificial intelligence (AI) in this field has proven to be highly effective in overcoming these shortcomings. In recent decades, metaheuristic algorithms have become increasingly prevalent due to their tremendous success in addressing several real-world optimization challenges in various areas of human activities, ranging from economic, pharmaceutical and industrial applications to intellectual applications. This review presents the use of different types of metaheuristic algorithm techniques in optimizing the parameters of the proportional-integralderived (PID) controller in order to control the DC motor. For a more robust review, the application of various forms of PID controller, as well as different types of DC motors, is considered",,Optimization of PID controller with metaheuristic algorithms for DC motor drives : review,,,,core
334755139,2020-01-01T00:00:00,"| openaire: EC/H2020/731558/EU//ANASTACIA | openaire: EC/H2020/871808/EU//INSPIRE-5GplusInternet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges.Peer reviewe",'Institute of Electrical and Electronics Engineers (IEEE)',A Machine Learning Security Framework for Iot Systems,10.1109/ACCESS.2020.2996214,,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
304168838,2020-01-01T00:00:00,"Photo-realistic 3D city models that represent the physical and functional state of the city are necessary components of the nation’s digital infrastructure. It facilitates much easier governing, planning, simulation, measuring and prediction, and provides numerous potentials in both scientific and industry applications. LoD3 models contain building roof and façade geometry, as well as the functions of its different components (windows, doors, etc.). Generating accurate and standard 3D city models is a manually tedious, decisively rich and non-straightforward process, and the current practice of is LoD3 city modelling still a manually intensive process.
Given the high demand for city-scale model production in the virtual Singapore program, we aim to develop an operable workflow that could produce LoD3 with the lightest possible manual involvement. A multi-data approach is used by integrating different sources of data including oblique imagery, aerial images, airborne/mobile LiDAR, and UAV images, to produce high quality LoD3 models that meet the CityGML standards. The workflow consists of three necessary work packages (WP) that develop techniques in 1) Geometry modelling, 2) Semantic labelling and texture mapping and 3) Integration of procedural modelling and interactive geometric editing. WP1 develops novel image-based and LiDAR based roof topography and façade geometry modelling with automated and semi-automated methods. WP2 applies data fusion techniques with the latest machine learning methods to perform land-cover classification, façade element attribution and texture mapping. WP2 also develops a preliminary proof of concept in change detection. To ensure high fidelity of the resulting models, WP3 develops novel visualization-driven editing procedures that efficiently correct errors of the models and integrate the procedural modelling workflow into the 3D reconstruction of buildings with regular geometric patterns.
The major objective of this project is to work towards an operationally feasible approach to generate city-scale LoD3 models, and provide preliminary proof-of-concept on efficient model maintenance, to facilitate the broader mission of the Virtual Singapore program in developing Singapore as a more intelligent and smarter city.
In the course of this project we have developed a number of novel approaches and algorithms for data processing. We have demonstrated their functionality with real data from our NUS Campus test field. However, still needed is the integration of the software packages into a fully functional operational system and a robustification of some components","Singapore ETH Centre, Future Cities Laboratory",An Operable System for LoD3 Model Generation Using Multi-Source Data and User-Friendly Interactive Editing: Final Report,10.3929/ethz-b-000409483,,,core
401560600,2020-12-28T00:00:00,"The world has entered the era of the fourth industrial revolution – a period in which digitalization plays perhaps the most significant role for production, and innovative technologies such as virtual reality, internet of things, artificial intelligence and robotics are fundamentally changing the way people work and the way they live. The backbone of modern society is a rapidly growing network of electronic knowledge and tools that includes manufacturers, suppliers, sellers, buyers and users of information in electronic form. The information sphere of the state directs its economic and innovative potential, and thus significantly affects other spheres, such as competitiveness in the international arena and the quality of citizens’ life. Today Ukraine is at a unique stage of development when there is a chance to make the so-called “digital leap” in key spheres of the economy. That is, to quickly move to a new stage of development in these spheres, bypassing intermediate stages, and starting to use modern systems at once, bypassing several generations of technologies. The percent of the digital economy in Ukraine is gradually steadily increasing, but the pace of its development is still low. In order to keep up with the world's leading economies forever, it is necessary to begin large-scale digitization of all industries as soon as possible, investing as much as possible in the development of digital infrastructures, innovations and modern technologies. Moreover, the country has potential, especially in the IT sphere, where Ukraine's position is quite good. Technologically, Ukraine is still in the last century because the state has very low domestic demand for technology. The actual tasks for Ukraine in this sphere, on the one hand, are the implementation of its own digital potential, and on the other, the implementation of relevant EU documents and projects into national legislation. In addition, it is important to develop the spheres of science and education, without which it is impossible to count on progress in the development of the information society and the knowledge economy. Every year, technologies go forward, new trends are gaining momentum, affecting all the schemes by which people do business in the digital world. And in order to stay ahead of the competition or even just to “stay in the game”, you need to learn to work with new tools, track trends and be flexible enough to adapt to these changes",'Stepan Gzhytskyi National University of Veterinary Medicine and  Biotechnologies Lviv',Digitalization in Ukraine’s economy in the context of world digitization,10.32718/nvlvet-e9602,https://core.ac.uk/download/401560600.pdf,,core
334919326,2020-03-15T00:00:00,"Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
  In this paper, we design AGL, a scalable, fault-tolerance and integrated
system, with fully-functional training and inference for GNNs. Our system
design follows the message passing scheme underlying the computations of GNNs.
We design to generate the $k$-hop neighborhood, an information-complete
subgraph for each node, as well as do the inference simply by merging values
from in-edge neighbors and propagating values to out-edge neighbors via
MapReduce. In addition, the $k$-hop neighborhood contains information-complete
subgraphs for each node, thus we simply do the training on parameter servers
due to data independency. Our system AGL, implemented on mature
infrastructures, can finish the training of a 2-layer graph attention network
on a graph with billions of nodes and hundred billions of edges in 14 hours,
and complete the inference in 1.2 hour",,AGL: a Scalable System for Industrial-purpose Graph Machine Learning,,http://arxiv.org/abs/2003.02454,,core
362674141,2020-11-27T00:00:00,"Industry 4.0 technologies integrate devices and data, bringing flexibility and efficiency,
derived from decentralization of information sources and processing, which is fundamental
to further advance applications. This work aims to introduce Cyber Physical Systems
(CPS) working with decision affecting passive objects, a system with distributed data,
and automatic planners, to achieve a self-sufficient process manipulator that does not
require external goal insertion and can self-adjust given an exception, in real-time. This
solution is more flexible and autonomous than state machines. Applying the technologies
introduced in Industry 4.0 and methods that were previously treated separately, such as
symbolic artificial intelligence and robot kinematics, the system can perform perception,
planning and actuation processes. This system is capable of extracting information inside
passive passive entities in the physical domain by using Radio Frequency IDentification
(RFID) to acquire predicates, data, about each object current and objective states using
the Predicate inside RFID Database (PRD) tool. This data is treated to produce a domain
snapshot, by joining distributed information and generating a problem definition, through
the Grouped Individual State Predicates (GISP) methodology. This problem definition
may then be fed into a planning module, implemented on an Edge or Cloud server, where
discrete-action and trajectory planning are concatenated to output control references,
using a generic symbolic planner and a numeric trajectory generator. Then, the active
agent may actuate, verify for exceptions and update the passive objects information if the
obtained state is perceived with no exceptions, else it must reiterate to satisfy the global
goal. This work structures the adaptive discrete event control architecture with a RFID
database containing parts of predicate logic.Pesquisa sem auxílio de agências de fomentoTrabalho de Conclusão de Curso (Graduação)As tecnologias da indústria 4.0 integram dispositivos e dados, trazendo flexibilidade e
eficiência, derivada da decentralização das fontes de informação e processamento, que é
fundamental para o avanço das aplicações. Esse trabalho busca introduzir sistemas ciber
físicos trabalhando com objetos passivos com capacidade de afetar decisões, um sistema
com informação distribuida, e planejadores automáticos, para alcançar um manipulador
de processos autossuficiente que não requer inserção externa de objetivos e que pode
auto-ajustar-se de acordo com exceções, em tempo de execução. Essa é uma solução
mais flexível e autônoma que o uso de máquinas de estado. Aplicando as tecnologias
introduzidas na Indústria 4.0 e métodos que eram tratados em separado, como inteligência
artificial simbólica e cinemática de robôs, o sistema pode realizar os processos de percepção,
planejamento e atuação. Esse sistema é capaz de extrair informação de entidades passivas
no domínio físico utilizando Identificação por Rádio Frequência (RFID) para adquirir
predicados, dados, sobre os estados corrente e objetivo de cada objeto através da ferramenta
PRD (Predicados dentro de base de Dados RFID). Esses dados são tratados para produzir
um retrato do domínio, através da união da informação distribuída e produção de uma
definição de problema usando a metodologia GISP (Predicados de Estado Individual
Agrupados). Essa definição de problema pode ser alimentada no módulo de planejamento,
implementado num servidor local ou na nuvem, onde planemento de ações discretas e
trajetória são concatenados para retornar referências de controle, usando um planejador
simbólico genérico e um gerador numérico de trajetória. Então, o agente ativo pode atuar,
verificar exceções e atualizar a informação nos objetos passivos caso o estado obtido seja
percebido livre de exceções, senão deve reiterar até que se satisfaça o objetivo global. Esse
trabalho estrutura a arquitetura de controle adaptativa a eventos discretos com base de dados RFID contendo partes de lógica de predicados",Engenharia Mecatrônica,Introdução ao controle adaptativo a eventos discretos em base de dados RFID com planejador automático aplicado a sistemas robóticos,,https://core.ac.uk/download/362674141.pdf,,core
352884845,2020-01-01T08:00:00,"“In a worker-centered intelligent manufacturing system, sensing and understanding of the worker’s behavior are the primary tasks, which are essential for automatic performance evaluation & optimization, intelligent training & assistance, and human-robot collaboration. In this study, a worker-centered training & assistant system is proposed for intelligent manufacturing, which is featured with self-awareness and active-guidance. To understand the hand behavior, a method is proposed for complex hand gesture recognition using Convolutional Neural Networks (CNN) with multiview augmentation and inference fusion, from depth images captured by Microsoft Kinect. To sense and understand the worker in a more comprehensive way, a multi-modal approach is proposed for worker activity recognition using Inertial Measurement Unit (IMU) signals obtained from a Myo armband and videos from a visual camera. To automatically learn the importance of different sensors, a novel attention-based approach is proposed to human activity recognition using multiple IMU sensors worn at different body locations. To deploy the developed algorithms to the factory floor, a real-time assembly operation recognition system is proposed with fog computing and transfer learning. The proposed worker-centered training & assistant system has been validated and demonstrated the feasibility and great potential for applying to the manufacturing industry for frontline workers. Our developed approaches have been evaluated: 1) the multi-view approach outperforms the state-of-the-arts on two public benchmark datasets, 2) the multi-modal approach achieves an accuracy of 97% on a worker activity dataset including 6 activities and achieves the best performance on a public dataset, 3) the attention-based method outperforms the state-of-the-art methods on five publicly available datasets, and 4) the developed transfer learning model achieves a real-time recognition accuracy of 95% on a dataset including 10 worker operations”--Abstract, page iv",Scholars\u27 Mine,Human behavior understanding for worker-centered intelligent manufacturing,,https://core.ac.uk/download/352884845.pdf,,core
328277865,2020-01-01T00:00:00,"As Digital Twins gain more traction and their adoption in industry increases, there is a need to integrate such technology with machine learning features to enhance functionality and enable decision making tasks. This has lead to the emergence of a concept known as Digital Triplet; an enhancement of Digital Twin technology through the addition of an ’intelligent activity layer’. This is a relatively new technology in Industrie 4.0 and research efforts are geared towards exploring its applicability, development and testing of means for implementation and quick adoption. This paper presents the design and implementation of a Digital Triplet for a three-floor elevator system. It demonstrates the integration of a machine learning (ML) object detection model and the system Digital Twin. This was done to introduce an additional security feature that enabled the system to make a decision, based on objects detected and take preliminary security measures. The virtual model was designed in Siemens NX and programmed via Total Integrated Automation (TIA) portal software. The corresponding physical model was fabricated and controlled using a Programmable Logic Controller (PLC) S7 1200. A control program was developed to mimic the general operations of a typical elevator system used in a commercial building setting.  Communication, between the physical and virtual models, was enabled using the OPC-Unified Architecture (OPC-UA) protocol. Object recognition using “You only look once” (YOLOV3) based machine learning algorithm was incorporated. The Digital Triplet’s functionality was tested, ensuring the virtual system duplicated actual operations of the physical counterpart through the use of sensor data. Performance testing was done to determine the impact of the ML module on the real-time functionality aspect of the system. Experiment results showed the object recognition contributed an average of 1.083s to an overall signal travel time of 1.338 s",'MDPI AG',Digital Triplet Approach for Real-Time Monitoring and Control of an Elevator Security System,10.3390/designs4020009,https://core.ac.uk/download/328277865.pdf,,core
328855123,2020-06-30T00:00:00,"In this white paper we provide a vision for 6G Edge Intelligence. Moving towards 5G and beyond the future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence become crucial for several real-world applications including but not limited to, more efficient manufacturing, novel personal smart device environments and experiences, urban computing and autonomous traffic settings. We present edge computing along with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies as shown in this series of 6G White Papers. In this white paper, we focus in the domains of edge computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, along with security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges and identify the key research questions for the development of the Intelligent Edge services. As a main outcome of this white paper, we envision a transition from Internet of Things to Intelligent Internet of Intelligent Things and provide a roadmap for development of 6G Intelligent Edge",University of Oulu,6G White Paper on Edge Intelligence,,https://core.ac.uk/download/328855123.pdf,,core
351022098,2020-08-10T07:00:00,"The advances in fifth-generation (5G) cellular technologies have shown much potential in this transformational technology which has been critical in driving economic development via numerous opportunities and applications. 5G technology is integral to realizing the full potential of the Internet of Things, edge computing, and artificial intelligence technologies in the real world. The purpose of this panel is to disseminate multi-faceted perspectives on 5G in order to more holistically understand it, such as diverse spectrum, network slicing, edge computing, cloud radio access network (C-RAN), various industries and use cases, business model, deployment, vulnerabilities; and to stimulate an engaging discussion on 5G. Three executives with expertise in the telecom industry, along with three academicians knowledgeable in the 5G and wireless telecom field, will share their perspectives and insights on 5G. They will also discuss the research agenda in the Information Systems field",AIS Electronic Library (AISeL),"5G Technologies: Insights, Opportunities & the Future",,https://core.ac.uk/download/351022098.pdf,,core
359941005,2020-01-01T00:00:00,"abstract: The Cognitive Decision Support (CDS) model is proposed. The model is widely applicable and scales to realistic, complex decision problems based on adaptive learning. The utility of a decision is discussed and four types of decisions associated with CDS model are identified. The CDS model is designed to learn decision utilities. Data enrichment is introduced to promote the effectiveness of learning. Grouping is introduced for large-scale decision learning. Introspection and adjustment are presented for adaptive learning. Triage recommendation is incorporated to indicate the trustworthiness of suggested decisions. 

The CDS model and methodologies are integrated into an architecture using concepts from cognitive computing. The proposed architecture is implemented with an example use case to inventory management. 

Reinforcement learning (RL) is discussed as an alternative, generalized adaptive learning engine for the CDS system to handle the complexity of many problems with unknown environments. An adaptive state dimension with context that can increase with newly available information is discussed. Several enhanced components for RL which are critical for complex use cases are integrated. Deep Q networks are embedded with the adaptive learning methodologies and applied to an example supply chain management problem on capacity planning. 

A new approach using Ito stochastic processes is proposed as a more generalized method to generate non-stationary demands in various patterns that can be used in decision problems. The proposed method generates demands with varying non-stationary patterns, including trend, cyclical, seasonal, and irregular patterns. Conventional approaches are identified as special cases of the proposed method. Demands are illustrated in realistic settings for various decision models. Various statistical criteria are applied to filter the generated demands. The method is applied to a real-world example.Dissertation/ThesisDoctoral Dissertation Industrial Engineering 202",,Cognitive Computing for Decision Support,,,,core
322452918,2020-05-06T00:00:00,"Recently, the Edge Computing paradigm has gained significant popularity both
in industry and academia. Researchers now increasingly target to improve
performance and reduce energy consumption of such devices. Some recent efforts
focus on using emerging RRAM technologies for improving energy efficiency,
thanks to their no leakage property and high integration density. As the
complexity and dynamism of applications supported by such devices escalate, it
has become difficult to maintain ideal performance by static RRAM controllers.
Machine Learning provides a promising solution for this, and hence, this work
focuses on extending such controllers to allow dynamic parameter updates. In
this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which
periodically updates Wait Buffer and batch sizes using on-the-fly learning
models and gradient ascent. AVAC allows Edge devices to adapt to different
applications and their stages, to improve computation performance and reduce
energy consumption. Simulations demonstrate that the proposed model can provide
up to 29% increase in performance and 19% decrease in energy, compared to
static controllers, using traces of real-life healthcare applications on a
Raspberry-Pi based Edge deployment.Comment: Accepted at 2020 IEEE International Symposium on Circuits and Systems
  (ISCAS",,"AVAC: A Machine Learning based Adaptive RRAM Variability-Aware
  Controller for Edge Devices",,http://arxiv.org/abs/2005.03077,,core
345182287,2020-01-01T08:00:00,"With the rapid development in wireless communications and artificial intelligence in recent years, vehicles are equipped with various smart devices to have ubiquitous access to the internet. These smart vehicles can easily exchange information with surrounding objects, e.g. vehicles, pedestrians, and roadside units, which form an enormous network. These networks that built upon vehicles are named vehicular networks. As the key enabler of the Intelligent Transportation System, vehicular networks are envisioned to simplify the traffic management, improve road safety, and provide infotainment through various vehicular services, like vehicle to everything communications, vehicular fog computing, and the location-based services. However, vehicular networks are vulnerable to various security and privacy risks due to the wireless nature and the heterogeneous network attributes. How to provide security and privacy for vehicular networks has attracted great attentions from both industry and academia. So in this dissertation, we present a study of security and privacy of vehicular networks. Novel security and privacy solutions for different vehicular services are proposed, which address the security and privacy issues in vehicle-to-everything communications, fast handover, vehicular fog computing, and location-based services. The major contributions of this dissertation are shown as follows. 1) A secure and efficient privacy-preserving authentication scheme is proposed. The proposed scheme is based on a 5G software-defined vehicular network architecture and is proved to be secure and highly efficient. 2) A secure and efficient handover scheme is proposed to provide user uninterrupted vehicular service, which is critical for the frequent handover processes on high-speed vehicles. 3) An efficient group management and key distribution scheme is proposed for vehicular fog computing paradigm, which has a lower delay compared to other existing schemes. 4) A location privacy-preserving scheme is proposed for location-based services. With the proposed scheme, vehicle users can have real-time location-based services with accurate location information updates while preserving location privacy. At the end of this dissertation, we also point out open research issues in securing vehicular networks",DigitalCommons@University of Nebraska - Lincoln,A Study of Security and Privacy in Vehicular Networks,,,,core
375437458,2020-04-13T00:00:00,"The emergence of Artificial Intelligence (AI) is creating new dimensions and redefining the concept and meaning of work in industrial settings. Documented success has been reported where AI is transforming industrial scenes such as scaling large operation processes, speed of execution, flexibility of processes where rigid manufacturing by dumb robots is replaced with smart individualized production following real-time customer choices, decision-making in which a huge amount of data can be quickly available at the fingertips of workers on the factory floor or even prevent problems before they happen, and personalization where AI uses data to deliver personalized user experience. According to the market research firm Trac tica, the global AI software market is expected to experience massive growth in the coming years, with revenues increasing from around US 9.5 billion in 2018 to an expected US 118.6 billion by 2025",'Institute of Electrical and Electronics Engineers (IEEE)',Future trends in I&M: Human-machine co-creation in the rise of AI,10.1109/MIM.2020.9062691,https://core.ac.uk/download/375437458.pdf,,core
429696970,2020-01-01T00:00:00,"Development work within an experimental environment, in which certain properties are investigated and optimized, requires many test runs and is therefore often associated with long execution times, costs and risks. This can affect product, material and technology development in industry and research. New digital driver technologies offer the possibility to automate complex manual work steps in a cost-effective way, to increase the relevance of the results and to accelerate the processes many times over. In this context, this article presents a low-cost, modular and open-source machine vision system for test execution and evaluates it on the basis of a real industrial application. For this purpose a methodology for the automated execution of the load intervals, the process documentation and for the evaluation of the generated data by means of machine learning to classify wear levels. The software and the mechanical structure are designed to be adaptable to different conditions, components and for a variety of tasks in industry and research. The mechanical structure is required for tracking the test object and represents a motion platform with independent positioning by machine vision operators or machine learning. An evaluation of the state of the test object is performed by the transfer learning after the initial documentation run. The manual procedure for classifying the visually recorded data on the state of the test object is described for the training material. This leads to an increased resource efficiency on the material as well as on the personnel side since on the one hand the significance of the tests performed is increased by the continuous documentation and on the other hand the responsible experts can be assigned time efficiently. The presence and know-how of the experts are therefore only required for defined and decisive events during the execution of the experiments. Furthermore, the generated data are suitable for later use as an additional source of data for predictive maintenance of the developed object",'Elsevier BV',Application of Machine Learning and Vision for real-time condition monitoring and acceleration of product development cycles,10.1016/j.promfg.2020.11.012,,,core
200815589,2020-12-30T00:00:00,"OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for
distributed real-time analytics, targeting fleets of reference vehicles in the
automotive industry. Its users are data analysts. The bulk of the data
analytics tasks are performed by clients (on-board), while a central cloud
server performs supplementary tasks (off-board). OODIDA can be automatically
packaged and deployed, which necessitates restarting parts of the system, or
all of it. As this is potentially disruptive, we added the ability to execute
user-defined Python modules on clients as well as the server. These modules can
be replaced without restarting any part of the system; they can even be
replaced between iterations of an ongoing assignment. This feature is referred
to as active-code replacement. It facilitates use cases such as iterative A/B
testing of machine learning algorithms or modifying experimental algorithms
on-the-fly. Consistency of results is achieved by majority vote, which prevents
tainted state. Active-code replacement can be done in less than a second in an
idealized setting whereas a standard deployment takes many orders of magnitude
more time. The main contribution of this paper is the description of a
relatively straightforward approach to active-code replacement that is very
user-friendly. It enables a data analyst to quickly execute custom code on the
cloud server as well as on client devices. Sensible safeguards and design
decisions ensure that this feature can be used by non-specialists who are not
familiar with the implementation of OODIDA in general or this feature in
particular. As a consequence of adding the active-code replacement feature,
OODIDA is now very well-suited for rapid prototyping.Comment: 24 pages, 4 figures, 3 code listings, 1 tabl",'Elsevier BV',"Facilitating Rapid Prototyping in the OODIDA Data Analytics Platform via
  Active-Code Replacement",10.1016/j.array.2020.100043,http://arxiv.org/abs/1903.09477,,core
387279656,2020-11-17T00:00:00,"Mature industrial sectors (e.g., aviation) collect their real world failures
in incident databases to inform safety improvements. Intelligent systems
currently cause real world harms without a collective memory of their failings.
As a result, companies repeatedly make the same mistakes in the design,
development, and deployment of intelligent systems. A collection of intelligent
system failures experienced in the real world (i.e., incidents) is needed to
ensure intelligent systems benefit people and society. The AI Incident Database
is an incident collection initiated by an industrial/non-profit cooperative to
enable AI incident avoidance and mitigation. The database supports a variety of
research and development use cases with faceted and full text search on more
than 1,000 incident reports archived to date.Comment: 6 pages, 7 figures, Pre-print accepted to Innovative Applications of
  Artificial Intelligence (IAAI-21",,"Preventing Repeated Real World AI Failures by Cataloging Incidents: The
  AI Incident Database",,http://arxiv.org/abs/2011.08512,,core
289263056,2020,"Beginning and experienced programmers will use this comprehensive guide to persistent memory programming. You will understand how persistent memory brings together several new software/hardware requirements, and offers great promise for better performance and faster application startup times—a huge leap forward in byte-addressable capacity compared with current DRAM offerings. This revolutionary new technology gives applications significant performance and capacity improvements over existing technologies. It requires a new way of thinking and developing, which makes this highly disruptive to the IT/computing industry. The full spectrum of industry sectors that will benefit from this technology include, but are not limited to, in-memory and traditional databases, AI, analytics, HPC, virtualization, and big data. Programming Persistent Memory describes the technology and why it is exciting the industry. It covers the operating system and hardware requirements as well as how to create development environments using emulated or real persistent memory hardware. The book explains fundamental concepts; provides an introduction to persistent memory programming APIs for C, C++, JavaScript, and other languages; discusses RMDA with persistent memory; reviews security features; and presents many examples. Source code and examples that you can run on your own systems are included. What You’ll Learn Understand what persistent memory is, what it does, and the value it brings to the industry Become familiar with the operating system and hardware requirements to use persistent memory Know the fundamentals of persistent memory programming: why it is different from current programming methods, and what developers need to keep in mind when programming for persistence Look at persistent memory application development by example using the Persistent Memory Development Kit (PMDK) Design and optimize data structures for persistent memory Study how real-world applications are modified to leverage persistent memory Utilize the tools available for persistent memory programming, application performance profiling, and debugging Who This Book Is For C, C++, Java, and Python developers, but will also be useful to software, cloud, and hardware architects across a broad spectrum of sectors, including cloud service providers, independent software vendors, high performance compute, artificial intelligence, data analytics, big data, etc",Springer Nature,Programming Persistent Memory,10.1007/978-1-4842-4932-1,,,core
326728497,2020-01-01T00:00:00,"Context: Deep learning has proven to be a valuable component in object detection and classification, as the technique has shown an increased performance throughput compared to traditional software algorithms. Deep learning refers to the process, in which an optimisation process learns an algorithm through a set of labeled data, where the researcher defines an architecture rather than the algorithm itself. As the resulting model contains abstract features retrieved through the optimisation process, new unsolved challenges emerge that need to be resolved before deploying these models in safety critical applications. Aim: The aim of this Licentiate thesis has been to study what extensions are necessary to verify deep neural networks. Furthermore, the thesis studies one challenge in detail: how out-of-distribution samples can be detected and excluded. Method:A comparative framework has been constructed to evaluate performance of out-of-distribution detection methods on common ground. To achieve this, the top performing candidates from recent publications were used as a reference snowballing baseline, from which a set of candidates were studied. From the study, common features were studied and included in the comparative framework. Furthermore, the thesis conducted semi-structured interviews to understand the challenges of deploying deep neural networks in industrial safety critical applications. Results: The thesis found that the main issue with deployment is traceability and quality quantification, in the form that deep learning lacks proper descriptions of how to design test cases, training datasets and robustness of the model itself. While deep learning performance is commendable, error tracing is challenging as the abstract features in the do not have any direct connection to the training samples. In addition, the training phase lacks proper measures to quantify diversity within the dataset, especially for the vastly different scenarios that exist in the real world. One safety method studied in this thesis is to utilize an out-of-distribution detector as a safety measure. The benefit of this measure is that it can both identify and mitigate potential hazards. From our literature review it became apparent that each detector was compared with different techniques, hence a framework was constructed that allowed for extensive and fair comparison. In addition, when utilizing the framework, robustness issues of the detector were found, where performance could drastically change depending on small variations in the deep neural network. Future work: Future works recommend testing the outlier detectors on real world scenarios, and show how the detector can be part of a safety strategy argumentation",,On Improving Validity of Deep Neural Networks in Safety Critical Applications,,https://core.ac.uk/download/326728497.pdf,,core
334900621,2020-01-06T00:00:00,"With the advancements in high volume, low precision computational technology
and applied research on cognitive artificially intelligent heuristic systems,
machine learning solutions through neural networks with real-time learning has
seen an immense interest in the research community as well the industry. This
paper involves research, development and experimental analysis of a neural
network implemented on a robot with an arm through which evolves to learn to
walk in a straight line or as required. The neural network learns using the
algorithms of Gradient Descent and Backpropagation. Both the implementation and
training of the neural network is done locally on the robot on a raspberry pi 3
so that its learning process is completely independent. The neural network is
first tested on a custom simulator developed on MATLAB and then implemented on
the raspberry computer. Data at each generation of the evolving network is
stored, and analysis both mathematical and graphical is done on the data.
Impact of factors like the learning rate and error tolerance on the learning
process and final output is analyzed.Comment: 8 pages, 14 figure",'eSAT Publishing House',Self learning robot using real-time neural networks,10.15623/ijret.2018.0710009,http://arxiv.org/abs/2001.02103,,core
344346767,2020-01-01T00:00:00,"Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working ""in-the-wild"" rather than only in laboratories and structured settings",'Elsevier BV',Visual Pogramming Environments for End-User Development of intelligent and social robots : a systematic review,10.1016/j.cola.2020.100970,https://core.ac.uk/download/344346767.pdf,"[{'title': 'Journal of Computer Languages', 'identifiers': ['2590-1184', 'issn:2590-1184']}]",core
326448061,2020-01-01T00:00:00,"There has been much heard that biolasers has full applications in medicine, communications, imaging, industry, electronics, and military. Tissue-biolasers are significant in monitoring or detecting subtle biological transients in tissue. Aldo improved signal to background ratio(contrast) and sensitivity. Moreover, it mimics real complex natural environments in the body and highly sensitive on-chip biosensing or biomedical imaging. Bioaser has been mostly used like a switch on/off signal by utilizing laser spectra for biosensing. By mapping laser emissions from biological samples to images is the first breakthrough. There is full information of laser modes, for example: the intelligence behind every laser pattern. The objective of this project is to use machine learning algorithms to analyze and classify images and spectral data from biological lasers to model the evolution of cancer cells for biological prediction tasks. Classify the process of the images has been approached by LabView, Matlab, and Python language. In this project, there are different software to achieve on objective. The result from classification and analysis data should lead to a clear picture of the relationship between no. of laser modes and the size of laser that helps lab users to go for the next experiment. Some research and implantation are studied to compare the advantages and disadvantages of different software. Results have demonstrated the output and analysis.Bachelor of Engineering (Electrical and Electronic Engineering",'Nanyang Technological University',Machine learning for cellular laser images and spectral data,,,,core
326497279,2020-06-19T00:00:00,"It is said that Data and Information are the new oil. One, who handles the
data, handles the emerging future of the global economy. Complex algorithms and
intelligence-based filter programs are utilized to manage, store, handle and
maneuver vast amounts of data for the fulfillment of specific purposes. This
paper seeks to find the bridge between artificial intelligence and its impact
on the international policy implementation in the light of geopolitical
influence, global economy and the future of labor markets. We hypothesize that
the distortion in the labor markets caused by artificial intelligence can be
mitigated by a collaborative international foreign policy on the deployment of
AI in the industrial circles. We, in this paper, then proceed to propose a
disposition for the essentials of AI-based foreign policy and implementation,
while asking questions such as 'could AI become the real Invisible Hand
discussed by economists?'.Comment: This is the pre-print versio","Journal of Liberty and International Affairs, Institute for Research and European Studies - Bitola","Turbulence on the Global Economy influenced by Artificial Intelligence
  and Foreign Policy Inefficiencies",10.47305/JLIA2020113ob,http://arxiv.org/abs/2006.16911,,core
334923046,2020-03-17T00:00:00,"With the merit of containing full panoramic content in one camera, Virtual
Reality (VR) and 360-degree videos have attracted more and more attention in
the field of industrial cloud manufacturing and training. Industrial Internet
of Things (IoT), where many VR terminals needed to be online at the same time,
can hardly guarantee VR's bandwidth requirement. However, by making use of
users' quality of experience (QoE) awareness factors, including the relative
moving speed and depth difference between the viewpoint and other content,
bandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical
Flow Based VR), an interactive method of VR streaming that can make use of VR
users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable
Difference through Optical Flow Estimation (JND-OFE) is explored to quantify
users' awareness of quality distortion in 360-degree videos. Accordingly, a
novel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is
proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling
scheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive BitRate(ABR).
For evaluation, we take two prior VR streaming schemes, Pano and Plato, as
baselines. Vast evaluations show that our system can increase the mean PSNR-OF
score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano
and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that
OFB-VR is a promising prototype for actual interactive industrial VR. A
prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR",,"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow
  Based QoE",,http://arxiv.org/abs/2003.07583,,core
357394465,2020-04-11T00:00:00,"Introduction Finite element modeling of manufacturing processes has been gaining wider acceptance over the last several years. Modeling prior to the start of actual production can save considerable time, effort, and money. While modeling may provide these benefits, it must be kept in mind that finite element software can only provide accurate simulations of a &quot;real&quot; process if appropriate material models are utilized. In this paper a novel material model is presented and compared to conventional models. For lack of an exact mathematical model, an intelligent algorithm, the ANN, will be used to map relationships between the hot forging parameters and the flow stress of the material. The ANN learns the patterns and offers robust and adaptive processing capabilities by implementing learning and selforganization rules. In the present work, an ANN is generated and trained based on physical testing of 6061 aluminum as found in the published literature ͓1͔. This trained network is then used as the material model, which when linked with a commercial finite element code, provides a model capable of more accurately reflecting actual experience. Much of this results from a robust ANN&apos;s ability to predict outputs between, and to some degree, outside the bounds established by a training set. For this application, values of strain, strain rate, and temperature not matching the family of curves used for training can be submitted to the network, and intermediate values of flow stress found. The conventional modeling approach requires experimental data curves to be fit to some form of hardening law and intermediate values interpolated by some means. While this may not be particularly difficult, the curve fitting process itself can be exceptionally tedious and in many cases does not produce accurate fits of the data. The ANN is much simpler to implement. Set up the network, train it, submit input values, and output is generated. To these ends, initially, a review of conventional material models and their limitations will be presented. The various factors leading to difficulties in addressing real problems will be summarized. The development and use of artificial neural networks will be covered with the specific aim of developing an unconventional material model for linking with finite element code. Conventional and ANN-based material models are then developed for 6061 aluminum using published data. The training of the ANNs is accomplished using MATLAB&apos;s Neural Network Toolbox. The conventional model is used directly with the commercial finite element code, ABAQUS. The ANN-based model requires the generation of the Fortran code that is linked by means of a subroutine within ABAQUS",,Incorporating Neural Network Material Models Within Finite Element Analysis for Rheological Behavior Prediction,,,,core
360317182,2020-06-22T03:34:10,"Blockchain is a potentially disruptive and game-changing technology that has created excitement about its potential applications. The agriculture industry in New Zealand is facing increased pressure to be able to accurately track and trace their produce in order to provide higher levels of proof to their customers. This study used a q-methodology approach to examine whether blockchain technology can be the solution to these issues and provides recommendations as to what businesses need to in order to make this a reality. The empirical research revealed four distinct groups within the industry; each with different perspectives of blockchain and its potential. Results also found that while industry experts believe blockchain implementation is inevitable and it will solve the current issues, factors such as high set-up costs and the complexity of technology may be inhibitors. Based on these findings, key recommendations on how the industry should proceed in order to overcome these factors that are preventing adoption are derived. Further research is suggested on how the challenges of food safety and security may be overcome with emerging technologies such as Blockchain, IoT and AI",,Making sense of blockchain in food supply-chains,,https://core.ac.uk/download/360317182.pdf,,core
343334536,2020-01-01T00:00:00,"It is said that Data and Information are the new oil. One, who handles the data, handles the emerging future of the global economy. Complex algorithms and intelligence-based filter programs are utilized to manage, store, handle, and maneuver vast amounts of data for the fulfillment of specific purposes. This paper seeks to find the bridge between artificial intelligence and its impact on international policy implementation in the light of geopolitical influence, the global economy, and the future of labor markets. We hypothesize that the distortion in the labor markets caused by artificial intelligence can be mitigated by a collaborative international foreign policy on the deployment of AI in the industrial circles. We, in this paper, then proceed to propose a disposition forth essentials of AI-based foreign policy and implementation, while asking questions such as: could AI become the real ‘invisible hand’ discussed by economists","Journal of Liberty and International Affairs, Institute for Research and European Studies - Bitola",Turbulence on the global economy influenced by artificial intelligence and foreign policy inefficiencies,10.47305/JLIA2020113ob,https://www.ssoar.info/ssoar/bitstream/document/69268/1/ssoar-jlibertyintaff-2020-2-osei_bonsu_et_al-Turbulence_on_the_global_economy.pdf,"[{'title': None, 'identifiers': ['1857-9760', 'issn:1857-9760']}]",core
478578486,2020-08-01T07:00:00,"The fast development of the deep learning (DL) techniques in the most recent years has drawn attention from both academia and industry. And there have been increasing applications of the DL techniques in many complex real-world situations, including computer vision, medical diagnosis, and natural language processing. The great power and flexibility of DL can be attributed to its hierarchical learning structure that automatically extract features from mass amounts of data. In addition, DL applies an end-to-end solving mechanism, and directly generates the output from the input, where the traditional machine learning methods usually break down the problem and combine the results. The end-to-end mechanism considerably improve the computational efficiency of the DL.The power system is one of the most complex artificial infrastructures, and many power system control and operation problems share the same features as the above mentioned real-world applications, such as time variability and uncertainty, partial observability, which impedes the performance of the conventional model-based methods. On the other hand, with the wide spread implementation of Advanced Metering Infrastructures (AMI), the SCADA, the Wide Area Monitoring Systems (WAMS), and many other measuring system providing massive data from the field, the data-driven deep learning technique is becoming an intriguing alternative method to enable the future development and success of the smart grid. This dissertation aims to explore the potential of utilizing the deep-learning-based approaches to solve a broad range of power system modeling and operation problems. First, a comprehensive literature review is conducted to summarize the existing applications of deep learning techniques in power system area. Second, the prospective application of deep learning techniques in several scenarios in power systems, including contingency screening, cascading outage search, multi-microgrid energy management, residential HVAC system control, and electricity market bidding are discussed in detail in the following 2-6 chapters. The problem formulation, the specific deep learning approaches in use, and the simulation results are all presented, and also compared with the currently used model-based method as a verification of the advantage of deep learning. Finally, the conclusions are provided in the last chapter, as well as the directions for future researches. It’s hoped that this dissertation can work as a single spark of fire to enlighten more innovative ideas and original studies, widening and deepening the application of deep learning technique in the field of power system, and eventually bring some positive impacts to the real-world bulk grid resilient and economic control and operation",TRACE: Tennessee Research and Creative Exchange,Deep Learning Techniques for Power System Operation: Modeling and Implementation,,https://core.ac.uk/download/478578486.pdf,,core
395397873,2020-01-01T00:00:00,"Manufacturing companies require efficient maintenance practices in order to improve business performance, ensure equipment availability and reduce process downtime. With the advent of new technology, manufacturing processes are evolving from the traditional ways into digitalized manufacturing. This transformation enables systems and machines to be connected in complex networks as a collaborative community through the industrial internet of things (IIoT) and cyber-physical system (CPS). Hence, advanced maintenance strategies should be developed in order to ensure the successful implementation of Industry 4.0, which aims to transform traditional product-oriented systems into product-service systems (PSS). Today, machines and systems are expected to gain self-awareness and self-predictiveness in order to provide management with more insight on the status of the factory. In this regards, real-time monitoring along with the application of advanced machine learning algorithms based on historical data will enable systems to understand the current operating conditions, predict the remaining useful life and detect anomalies in the process. This paper discusses the necessity of predictive maintenance to achieve a sustainable and service-oriented manufacturing system and provides a methodology to be followed for implementing proactive maintenance in the context of Industry 4.0",'Springer Science and Business Media LLC',Proactive Learning for Intelligent Maintenance in Industry 4.0,10.1007/978-981-15-2341-0_31,,"[{'title': None, 'identifiers': ['issn:1876-1100', '1876-1119', 'issn:1876-1119', '1876-1100']}]",core
402918283,2020-11-21T00:25:20,"With smart city infrastructures growing, the Internet of Things (IoT) has been widely used in the intelligent transportation systems (ITS). The traditional adaptive traffic signal control method based on reinforcement learning (RL) has expanded from one intersection to multiple intersections. In this paper, we propose a multi-agent auto communication (MAAC) algorithm, which is an innovative adaptive global traffic light control method based on multi-agent reinforcement learning (MARL) and an auto communication protocol in edge computing architecture. The MAAC algorithm combines multi-agent auto communication protocol with MARL, allowing an agent to communicate the learned strategies with others for achieving global optimization in traffic signal control. In addition, we present a practicable edge computing architecture for industrial deployment on IoT, considering the limitations of the capabilities of network transmission bandwidth. We demonstrate that our algorithm outperforms other methods over 17% in experiments in a real traffic simulation environment",'MDPI AG',An Edge Based Multi-Agent Auto Communication Method for Traffic Light Control.,10.3390/s20154291,https://opus.lib.uts.edu.au/bitstream/10453/144204/2/An%20Edge%20Based%20Multi-Agent%20Auto%20Communication%20Method%20for%20Traffic%20Light%20Control.pdf,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
440368519,2020-01-01T00:00:00,"Indoor positioning technologies have gained great interest from both industry and academia. Variety of services and applications can be built based on the availability and accessibility of indoor positioning information, for example indoor navigation and various location-based services. Different approaches have been proposed to provide indoor positioning information to users, in which an underlying system infrastructure is usually assumed to be well deployed in advance to provide the position information to users. Among many others, one common strategy is to deploy a bunch of active sensor nodes, such as WiFi APs and Bluetooth transceivers, to the indoor environment to serve as reference landmarks. The user's current location can thus be obtained directly or indirectly according to the active sensor signals collected by the user. Different from conventional infrastructure-based approaches, which put additional sensor devices to the environment, we utilize available objects in the environment as location landmarks. Leveraging wildly available smartphone devices as customer premises equipment to the user and the cutting-edge deep-learning technology, we investigate the feasibility of an infrastructure-free intelligent indoor positioning system based on visual information only. The proposed scheme has been verified by a real case study, which is to provide indoor positioning information to users in Taipei Main Station, one of the busiest transportation stations in the world. We use available pedestrian directional signage as location landmarks, which include all of the 52 pedestrian directional signs in the testing area. The Google Objection Detection framework is applied for detection and recognition of the pedestrian directional sign. According to the experimental results, we have shown that the proposed scheme can achieve as high as 98% accuracy to successfully identify the 52 pedestrian directional signs for the three test data sets which include 6,341 test images totally. Detailed discussions of the system design and the experiments are also presented in the paper",'American Institute of Mathematical Sciences (AIMS)',An intelligent indoor positioning system based on pedestrian directional signage object detection: a case study of Taipei Main Station,10.3934/mbe.2020015,,"[{'title': 'Mathematical Biosciences and Engineering', 'identifiers': ['issn:1551-0018', '1551-0018']}]",core
323104269,"March 24, 2020","The modern National Airspace System (NAS) is an extremely safe system and the aviation industry has experienced a steady decrease in fatalities over the years. This can be attributed to both improved flight critical systems with redundant hardware and software protections, as well as an increased focus on active monitoring and response to real time and historically identified vulnerabilities by implementing more resilient procedures and protocols. The main approach for identifying vulnerabilities in operations leverages domain expertise using knowledge about how the system should behave within the expected tolerances to known safety margins. This approach works well when the system has a well-defined operating condition. However, the operations in the NAS can be highly complex with various nuances that render it difficult to clearly pre-define all known safety vulnerabilities. With the advancement of data science and machine learning techniques, the potential to automatically identify emerging vulnerabilities in the observed operations has become more practical in recent years. The state-of-the-art anomaly detection approaches in aerospace data usually rely on supervised or semi-supervised learning. However, in many real-world problems such as flight safety, creating labels for the data requires huge amount of effort and is largely impractical. To address this challenge, we developed a Convolutional Variational Auto-Encoder (CVAE), which is an unsupervised learning approach for anomaly detection in high-dimensional heterogeneous time-series data. We validate performance of CVAE compared to the state-of-the-art supervised learning approach as well as unsupervised clustering-based approach using KMeans++ and kernel-based approach using One-Class Support Vector Machine (OC-SVM) on Yahoo!'s benchmark time series anomaly detection data. Finally, we showcase performance of CVAE on a case study of identifying anomalies in the first 60 seconds of commercial flights' take-offs using Flight Operational Quality Assurance (FOQA) data",,Unsupervised Anomaly Detection in High-Dimensional Flight Data Using Convolutional Variational Auto-Encoder,,https://core.ac.uk/download/pdf/323104269.pdf,,core
327064411,2020-07-20T00:00:00,"In this document, we report our proposal for modeling the risk of possible
contagiousity in a given area monitored by RGB cameras where people freely move
and interact. Our system, called Inter-Homines, evaluates in real-time the
contagion risk in a monitored area by analyzing video streams: it is able to
locate people in 3D space, calculate interpersonal distances and predict risk
levels by building dynamic maps of the monitored area. Inter-Homines works both
indoor and outdoor, in public and private crowded areas. The software is
applicable to already installed cameras or low-cost cameras on industrial PCs,
equipped with an additional embedded edge-AI system for temporary measurements.
From the AI-side, we exploit a robust pipeline for real-time people detection
and localization in the ground plane by homographic transformation based on
state-of-the-art computer vision algorithms; it is a combination of a people
detector and a pose estimator. From the risk modeling side, we propose a
parametric model for a spatio-temporal dynamic risk estimation, that, validated
by epidemiologists, could be useful for safety monitoring the acceptance of
social distancing prevention measures by predicting the risk level of the
scene",,Inter-Homines: Distance-Based Risk Estimation for Human Safety,,http://arxiv.org/abs/2007.10243,,core
343634975,2020-01-01T00:00:00,"Measuring systems are becoming increasingly sophisticated in order to tackle the challenges of modern industrial problems. In particular, the Multiphase Flow Meter (MPFM) combines different sensors and data fusion techniques to estimate quantities that are difficult to be measured like the water or gas content of a multiphase flow, coming from an oil well. The evaluation of the flow composition is essential for the well productivity prediction and management, and for this reason, the quantification of the meter measurement quality is crucial. While instrument complexity is increasing, demands for confidence levels in the provided measures are becoming increasingly more common. In this work, we propose an Anomaly Detection approach, based on unsupervised Machine Learning algorithms, that enables the metrology system to detect outliers and to provide a statistical level of confidence in the measures. The proposed approach, called AD4MPFM (Anomaly Detection for Multiphase Flow Meters), is designed for embedded implementation and for multivariate time-series data streams. The approach is validated both on real and synthetic data",'MDPI AG',Self-Diagnosis of Multiphase Flow Meters through Machine Learning-Based Anomaly Detection,10.3390/en13123136,,,core
344884735,2020-11-18T00:00:00,"International audienceComplex  industrial  systems  are  increasingly  software  driven,  rapidly  evolving  into autonomous,  self-adaptive  processing  at  industrial  field.  Artificial  intelligence technologies are spreading fast at industrial Edge, improving the industrial operations efficiency. However Edge computing systems involving artificial intelligence must also continuously ensure the safe operations. This paper assesses the state of the art of cognitive technologies with their relevance for artificial intelligence implementation at industrial  safety  critical  systems.  It  introduces  then  a  state of  the  art  for  artificial intelligence  safety  assurance  practices.  Implementation  at  the industrial  application stack is illustrated with the edge virtual operating system that operates the industrial fog. Edge operating system is evolving fast as kind of an AI intensive software. Industrial developments are illustrated with Slap OS, real case examples of swarm computing and advanced robotic",HAL CCSD,Industrial field autonomous systems: AI-assisted distributed applications at Edge,,,,core
350944836,2020-01-01T00:00:00,"| openaire: EC/H2020/731558/EU//ANASTACIA | openaire: EC/H2020/871808/EU//INSPIRE-5GplusInternet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges.Peer reviewe",'Institute of Electrical and Electronics Engineers (IEEE)',A Machine Learning Security Framework for Iot Systems,10.1109/ACCESS.2020.2996214,,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
237171923,2020-01-01T00:00:00,"Overview The fundamental motivation of this book is to contribute to the future advancement of Asset Management in the context of industrial plants and infrastructures. The book aims to foster a future perspective that takes advantage of value-based and intelligent asset management in order to make a step forward with respect to the evolution observed nowadays. Indeed, the current understanding of asset management is primarily supported by well-known standards. Nonetheless, asset management is still a young discipline and the knowledge developed by industry and academia is not set in stone yet. Furthermore, current trends ¬in new organizational concepts and technologies lead to an evolutionary path in the field. Therefore, this book aims to discuss this evolutionary path, starting first of all from the consolidated theory, then moving forward to discuss: • The strategic understanding of value-based asset management in a company; • An operational definition of value, as a concept on the background of value-based asset management; • The identification of intelligent asset management, with the aim to frame a set of “tools” recommended to support the asset-related decision-making process over the asset lifecycle. The book compiles information gathered from interesting research and innovation efforts in projects that were relevant to this scope, especially considering the evidences from state of the art and current research trends of Physical Asset Management (PAM) and Operations &amp; Maintenance (O&amp;M) of industrial plants and infrastructures. Among the new trends, digitalization is enabling new capabilities for asset management, by means of the appearance of Cyber Physical Systems (CPS), and the subsequent issues resulting from building the digital twins of the physical assets. This may lead to a new era of intelligent asset management systems. At the same time, basic principles of asset management will continue to be relevant in the new era, helping to guide the development of digitalization programs in assets intensive companies, and being transformed along the evolutionary path towards the achievement of a more digitized and intelligent management. Relevant Topics One of the main challenges in the field of physical asset management is to enhance the identification and quantification of cost and value to evaluate the total cost and value of industrial assets throughout their lifecycle. These concepts have been widely discussed in literature, by offering different perspectives and also using plenty of terms partially overlapping or providing slightly different interpretations. Terms such as TCO (Total Cost of Ownership), LCC (Life Cycle Cost), WLC (Whole Life Cost), COO (Cost of Ownership) and, if extending to values, TVO (Total Value of Ownership) and Whole Life Value (WLV), are widely cited. If one surfs the Internet, a myriad of definitions and references can be found. This does not mean that the terms are well understood and widely adopted in practice. Considering the industrial applications of TCO and TVO, it is worth remarking that their benefits are clearly envisioned (e.g., the benefits of TCO can be considered cost control support, management strategy selection, quality optimization, and best cost-effectiveness management). However, in practice, some missing links can be pointed out with regard to their use: even though the need and desire to implement life cycle costing is very much talked about, there are a number of difficulties that limit a widespread adoption by industry. This is even more challenging when extending to value and, thus, to the whole life value, which is a more recent concept. Another relevant challenge addressed by physical asset management, is the assurance of the cost and value along the asset lifecycle. Henceforth, appropriate “tools” are required in order to assure that the value delivery from industrial assets (at reasonable cost) is effectively achieved and, when not, that proper decisions are activated with the aim to guarantee value delivery. In particular, proper “tools” should be used when planning in advance, and when monitoring and controlling the effective outcomes, to eventually activate re-planning in case of extant discrepancies with respect to expectations, thus leading to a continuous improvement of what is decided over the asset life cycle. Identification and quantification of value delivered by the assets is essential in all the cases. Structure of the Book The book is divided into four Parts. In Part 1, the first Chapter introduces fundamental concepts used in this book and presents a generalized framework providing relevant dimensions of value-based and intelligent asset management. The rest of the chapters in this Part offer a long-term perspective of asset management, dealing with topics like societal impact of investments in infrastructure assets, performance and economic impacts of investments in manufacturing plants, and long-term deterioration and renewal of assets. In Part 2 the value-based decision-making approach is stressed as an overall perspective for management of the assets over their life cycle, and also exemplified in real world specific cases. The concept of value, understood as presented in the first Chapter of this book, is operationalized to drive day to day management decisions and activities. Part 3 is dedicated to different advanced developments at the operational level. Different tools are presented to predict and/or to determine properly assets conditions leading to the release and execution of the maintenance activities. Predictive analytics are used to make predictions about assets future behavior. Many techniques from data mining, statistics, modeling, machine learning, and artificial intelligence can be applied to analyze current data to make predictions about future. The scalability of these emerging models, in this new scenario of individualised asset prognostics, is another topic discussed in this part of the book, trying to find a compromise between accuracy and computational power of these tools. Part 4 is devoted to new emerging processes, and new ideas that can be implemented by exploiting the power of new technologies such as cyber-physical systems that can certainly embed more intelligence and orientation to value in existing asset management systems. European Project and Worldwide Collaboration This book results from a collaboration of the authors, strengthened within the context of SustainOwner, ‘‘Sustainable Design and Management of Industrial Assets through Total Value and Cost of Ownership’’, a project sponsored by the EU Framework Programme Horizon 2020 and based on a knowledge sharing scheme involving many universities worldwide, from the Americas, Asia and Africa. Chapters Including Previously Published Research Results This book compiles a set of Chapters that were previously published as journal papers by the research groups involved in the Sustain Owner Project. The Editors would like to idenfify the correspondence between each book Chapter and the original research paper. According to Springer policy, the publishers were asked to provide their permissions for this work to be presented in its current form. The Editors thank the publishers for their cooperation making this book possible. The referred Chapters are: - Chapter 2: Heaton, J., Parlikad, A.K., “A conceptual framework for the alignment of infrastructure assets to citizen requirements within a smart cities framework,” Cities, Volume 90, pp 32-41, 2019. - Chapter 3: Roda I., Garetti M., “Application of a Performance-driven Total Cost of Ownership (TCO) Evaluation Model for Physical Asset Management”. In: Amadi-Echendu J., Hoohlo C., Mathew J. (eds) 9th WCEAM Research Papers. Lecture Notes in Mechanical Engineering. Springer, Cham, 2015, © Springer International Publishing Switzerland 2015, DOI 10.1007/978-3-319-15536-4. - Chapter 5: Roda, I., and M Macchi. “A framework to embed Asset Management in production companies.” Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability 232, no. 4: 368-378, 2018, © IMechE 2018, DOI: 10.1177/1748006X17753501. - Chapter 6: Srinivasan, R., Parlikad, A.K., “An approach to value-based infrastructure asset management,” Infrastructure Asset Management, Volume 4, Issue 3, pp 87-95, 2017. - Chapter 9: Olivencia Polo F.A , Ferrero Bermejo J. Gómez Fernández JF., Crespo Márquez A..,”Failure mode prediction and energy forecasting of PV plants to assist dynamic maintenance tasks by ANN based models”. Renewable Energy, Volume 81, pp 227-238. 2015. - Chapter 10: Liu, B., Liang, Z., Parlikad, A.K., Xie, M., Kuo, W., “Condition-based maintenance for systems with aging and cumulative damage based on proportional hazards model,” Reliability Engineering &amp; System Safety, Volume 168, pp 200-209, 2017. - Chapter 11: C. Colace, L. Fumagalli, S. Pala, M. Macchi, N. R. Matarazzo, M. Rondi., “Implementation of a condition monitoring system on an electric arc furnace through a risk-based methodology.” Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability, Volume 229, Issue 4, August 2015, 327-342, 2015, © IMechE 2015, DOI: 10.1177/1748006X15576441. - Chapter 12: Erguido A., Crespo Márquez A.. Castellano E., Gómez Fernández JF.,”A dynamic opportunistic maintenance model to maximize energy- based availability while reducing the life cycle cost of wind farms”. Renewable Energy, Volume 114, pp 843-856. 2017. - Chapter 13: Negri E., L. Fumagalli, M. Macchi, “A Review of the Roles of Digital Twin in CPS-based Production Systems”, in Proceedings 27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, Volume 11, 939-948, 27-30 June 2017, Modena, Italy, (Eds.) Marcello Pellicciari, Margherita Peruzzini, 2017, 2351-9789, © 2017 The Authors. Published by Elsevier B.V., doi: 10.1016/j.promfg.2017.07.198. - Chapter 14: Li, H., Salvador-Palau, A., Parlikad, A.K., “A Social Network of Collaborating Industrial Assets,” Proceedings of the IMechE Part O: Journal of Risk &amp; Reliability, Volume 232, Issue 4, pp. 389-400, 2018, © IMechE 2018, DOI: 10.1177/1748006X18754975. - Chapter 15: Salvador-Palau, A., Liang, Z., Lutgehetmann, D., Parlikad, A.K., “Collaborative Prognostics in Social Asset Networks,” Future Generation Computer Systems, Volume 92, pp 987-995, 2019. - Chapter 16: Chekurov S, Metsä-Kortelainen S, Salmi M, Roda I, Jussila A., “The perceived value of additively manufactured digital spare parts in industry: an empirical investigation”. International Journal of Production Economics, 2015, 87-97, 2018, 0925-5273 © 2018 The Authors. Published by Elsevier B.V. T., DOI: 10.1016/j.ijpe.2018.09.008. Adolfo Crespo Márquez Marco Macchi Ajith Kumar Parlika",'Springer Science and Business Media LLC',Value Based and Intelligent Asset Management. Mastering the Asset Management Transformation in Industrial Plants and Infrastructures,10.1007/978-3-030-20704-5,,,core
385959184,2020-01-01T00:00:00,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure",'Institute of Electrical and Electronics Engineers (IEEE)',Machine Learning for Predictive Diagnostics at the Edge: An IIoT Practical Example,10.1109/ICC40277.2020.9148684,,,core
286067437,2019-01-01T00:00:00,"With the rapid development of big data, artificial intelligence and internet of things, digital twin technology becomes a new research hotspot in the field of intelligent manufacturing. In this paper, the digital twin technology for production line design and simulation is studied. Emphasis is laid on the building and fusion of production line model, virtual-real mapping and real-time interaction technology and virtual production line simulation and verification technology. The research content of this paper provides theoretical and technical reference for the application of digital twins in the design and implementation of manufacturing production line",'Springer Fachmedien Wiesbaden GmbH',Research on Digital Twin Technology for Production Line Design and Simulation,,,,core
227003045,2019-01-01T00:00:00,"In a low temperature district heating (LTDH) system the importance of well performing network and customer installations are essential due to reduced operation margins. The use of low-value waste heat in the production sets limitations on the supply temperature. New piping materials sets limitations on the supply temperature and the pressure level. All in all, this means an increased focus on ensuring that DH installations arewell performing. This report is about ‘individual metering concepts’. The LTDH-system is monitored by direct connection to the heat meters at all the consumers. The data from the heat meters can be used as a basis for describing, predicting and analysing the system's performance for better control of production and operation, for better conditions for troubleshooting and maintenance of the grid and the customer installations as well as for better customer service. The aim of this report is to:  give an overview over the current regulations for metering and billing of heat in Sweden and Denmark give an overview of desirable functionalities for DH meters and metering systems to provide input to applications that can be developed with the help of meter readings and which can be useful for the district heating industry describe how DH meter readings can be used for condition monitoring of service pipes and develop a method for this Regulations for metering and billing of DH follows EU regulations which says that all DH customers must be charged according to actual consumption. The overview of the current regulations for metering and billing in Denmark and Sweden shows that the two neighbouring countries have somewhat different approach to the individual metering concept. In Denmark there is a long tradition of individual metering in multifamily buildings and the tenants heat consumption is measured. In Sweden there is no such tradition and the individual metering concept generally is not in focus. Instead, the property owner is the customer. Denmark follows the Energy Efficiency Directive from 2012 (supplemented 2018) that requires individual metering and charging. The Swedish regulations on metering and billing are based on the exception in the EU regulation concerning individual metering and charging saying that individual metering must be economically justifiable. The Swedish National Board of Housing has shown that implementation of the EU regulations is not economically justifiable for the Swedish case ‐ neither for existing buildings or in new buildings. There is a general attitude that increased frequency and higher resolution of meter readings automatically results in increased possibilities for new and improved analyses of customer performance. At present there are several researchers and research groups that are using meter readings from DH customers and machine learning with the aim to improve the energy performance of both the DH customer installations as well as the DH network and production operation. From ongoing projects four areas for applications have been identified: Fault detection, Load prediction, Production planning and Operational optimization. For these applications hourly meter readings are sufficient, at least in combination with historical data and weather conditions. For improved fault detection algorithms and moving towards fault diagnosis it would be beneficial to access secondary meter readings such as temperatures in the secondary side heating‐ and domestic hot water system as well as indoor temperatures. When moving on towards fault diagnosis, the frequency may also be in focus. A temporary shift to meter readings with higher frequency may be necessary for diagnosis. All in all, it might not be the frequency of the meter readings that should be increased, it might be more beneficial to include more meter parameters from the customer side of the heat exchanger. By integration of more meter parameters, the focus may shift from a matter of frequency to the broad concept of Internet of things (IoT). One scope within this project was to evaluate the feasibility for usage of meter readings from customer installations to detect increased heat losses in service pipes due to moisture. A theoretical study was carried out, but it did not turn into a success. Results showed that even though the heat losses in the service pipe increases due to moisture content, the impact on the measured parameters, that is temperature and flow, are low and would be hard to detect. A higher resolution or increased frequency of meter readings would not improve the feasibility to use meter readings for monitoring increased heat losses in service pipes since if feasible, it would require stable heat load at the customer installation and stable and known temperature conditions in the DH network. Through studies performed within this work and by other researchers it is clear that DH utilities, both in Denmark and Sweden, has a desire to improve customer performance and to reduce the DH return temperature. A key factor for success within this field is good customer relations and access to the customer’s DH substation. This may be a driving force for DH‐utilities to offer service agreement and ICT‐platforms for greater customer engagement. Customers seems to be more willing to take actions to improve their installations if they understand why it is important. Recommendations: The following recommendations are given based on the overall picture provided by the report's different compilations and studies:  The meter should be able to measure the energy for every hour and be able to convert to measure with higher frequencies (minutes). Some types of measurement data analyses may require a higher resolution than hourly measurement, which is why the possibility of higher resolution should be provided, without that being a default setting.  It should be possible to remotely upgrading the meters and to remotely change the meter frequency. This will provide that new functionalities can be introduced in a cost‐efficient way and that expensive field visits can be avoided.  Access to secondary meter readings. The following measurements were seen as the most important: Indoor temperatures, secondary temperatures measured on branches for the heating system in the building, and measurement on the district heating differential pressure. Measuring these parameters would be beneficial for developing algorithms for fault detection and for improving system performance for the total district heating system, as well as for as being able to guarantee the quality of energy supplies at the customer substations.  There should be a digital interface that the end customers can use to access their energy consumption locally. The communication solution should also support a future standard for communication with devices in the home. In order to be of real use for the customers, this should be delivered with some kind of analysis tools that can help the customers relate their consumption or data to reference data (for example historical data, norms or set point values) and that can provide the customer with extended analysis that make sense to the customer",Cool DH,Report on improved use of individual metering concept,,,,core
299446313,2019,"Fuzzy systems have become widely accepted and applied in a host of domains such as control, electronics or mechanics. The
software for construction of these systems has traditionally been exploited from tools, platforms and languages run on-premise
computing infrastructure. On the other hand, rise and ubiquity of the cloud computing model has brought a revolutionary way
for computing services deployment. The boost of cloud services is leading towards increasingly specific service offering just
as data mining and machine learning service. Unfortunately, so far, no definition for fuzzy system as service is available. This
paper identifies this opportunity and focus on developing a proposal for fuzzy system-as-a-service definition. To achieve this, the
proposal pursues three objectives: the complete description of cloud services for fuzzy systems using semantic technology, the
composition of services and the exploitation of the model in cloud platforms for integration with other services. As an illustrative
case, a real-world problem is addressed with the proposed specification.This work was supported by the Research
Projects P12-TIC-2958 and TIN2016-81113-R (Ministry of Economy,
Industry and Competitiveness - Government of Spain)",Atlantis Press,Fuzzy Systems-as-a-Service in Cloud Computing,,10.2991/ijcis.d.190912.001,,core
237402230,"November 20, 2019","With the recent advancements in Deep Learning methods, the ability to model large complex heterogeneous data sets are fundamentally changing industry and research. Coupled with hardware improvements, and ease of implementation, a wide variety of deep neural network architectures can quickly be developed to solve a sweeping range of problems such as: object detection in images, automatic healthcare diagnosis using heterogenous data sources, real time language translating and sentence prediction, upscaling low resolution images, and forecasting of multivariate timeseries. Generally, many of these architectures outperform classical machine learning approaches in their respective tasks, however, this typically comes at a cost of interpretability. These black box algorithms generally suffer from lack of transparency in both model complexity as well as the rationale behind the prediction. This lack of comprehension, is driving an emerging area of interest in Explainable AI. An algorithm called: Deep Temporal Multiple Instance Learning1 was a recently developed to identify precursors to adverse events and has been applied in the aviation domain. The deep learning architecture is designed to capture the evolution of the probability of the outcome over the time preceding the adverse event using a multiple instance learning approach as illustrated in Figure 1. Precursors are defined when the probability of the event has exceeded a threshold at some point in the timeseries, at which point, a sensitivity analysis is performed to determine contributing factors. The contributing factors are used to explain and define the precursor during the periods where the probability score is high. The identified contributing factors are then presented to subject matter experts to provide objective insights into the leading factors associated with the particular adverse event. The algorithm has been tested on flight data from a commercial airline and has the ability to discover precursors to known adverse events that take the form of safety critical operations, such as unstable approach events on final approach. Apart from detecting precursors to adverse events, the converse can also be leveraged to discover corrective actions. These positive actions manifest themselves as periods in the timeseries when the precursor score has been lowered from an elevated state; meaning that if the system had been left uncorrected, it would have eventually reached the adverse event state. Characterizing these state changes can help identify successful interventions that may not have been known before. Policy makers and procedure designers can use this additional knowledge to craft more safety and efficient resilient procedures for future operations and therefore improve the overall performance of the National Airspace",,Deep Learning Method for Detecting Precursors to Adverse Events,,,,core
200834692,2019-04-27T00:00:00,"The increasingly fast development cycle for online course contents, along
with the diverse student demographics in each online classroom, make real-time
student outcomes prediction an interesting topic for both industrial research
and practical needs. In this paper, we tackle the problem of real-time student
performance prediction in an on-going course using a domain adaptation
framework. This framework is a system trained on labeled student outcome data
from previous coursework but is meant to be deployed on another course. In
particular, we introduce a GritNet architecture, and develop an unsupervised
domain adaptation method to transfer a GritNet trained on a past course to a
new course without any student outcome label. Our results for real Udacity
student graduation predictions show that the GritNet not only generalizes well
from one course to another across different Nanodegree programs, but also
enhances real-time predictions explicitly in the first few weeks when accurate
predictions are most challenging.Comment: Accepted as oral presentation to ICLR 2019, AI for Social Good
  Workshop. arXiv admin note: substantial text overlap with arXiv:1809.06686,
  arXiv:1804.0740",,Deep Learning to Predict Student Outcomes,http://arxiv.org/abs/1905.02530,,,core
186320024,2019-01-29T00:00:00,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy",,Structural Material Property Tailoring Using Deep Neural Networks,http://arxiv.org/abs/1901.10281,,,core
334839780,2019-07-24T00:00:00,"Attacks on industrial enterprises are increasing in number as well as in
effect. Since the introduction of industrial control systems in the 1970's,
industrial networks have been the target of malicious actors. More recently,
the political and warfare-aspects of attacks on industrial and critical
infrastructure are becoming more relevant. In contrast to classic home and
office IT systems, industrial IT, so-called OT systems, have an effect on the
physical world. Furthermore, industrial devices have long operation times,
sometimes several decades. Updates and fixes are tedious and often not
possible. The threats on industry with the legacy requirements of industrial
environments creates the need for efficient intrusion detection that can be
integrated into existing systems. In this work, the network data containing
industrial operation is analysed with machine learning- and time series- based
anomaly detection algorithms in order to discover the attacks introduced to the
data. Two different data sets are used, one Modbus-based gas pipeline control
traffic and one OPC UA-based batch processing traffic. In order to detect
attacks, two machine learning-based algorithms are used, namely \textit{SVM}
and Random Forest. Both perform well, with Random Forest slightly outperforming
SVM. Furthermore, extracting and selecting features as well as handling missing
data is addressed in this work.Comment: This is a work accepted but not yet published at the 2019 27th
  International Conference on Software, Telecommunications and Computer
  Networks (SoftCOM",,"Anomaly-based Intrusion Detection in Industrial Data with SVM and Random
  Forests",http://arxiv.org/abs/1907.10374,,,core
334867785,2019-10-07T00:00:00,"As opportunities for AI-assisted healthcare grow steadily, model deployment
faces challenges due to the specific characteristics of the industry. The
configuration choice for a production device can impact model performance while
influencing operational costs. Moreover, in healthcare some situations might
require fast, but not real time, inference. We study different configurations
and conduct a cost-performance analysis to determine the optimized hardware for
the deployment of a model subject to healthcare domain constraints. We observe
that a naive performance comparison may not lead to an optimal configuration
selection. In fact, given realistic domain constraints, CPU execution might be
preferable to GPU accelerators. Hence, defining beforehand precise expectations
for model deployment is crucial",,Impact of Inference Accelerators on hardware selection,http://arxiv.org/abs/1910.03060,,,core
478630727,2019-01-01T00:00:00,"Visual analytics are becoming increasingly important in the light of big data and related scenarios. Along this trend, the field of immersive analytics has been variously furthered as it is able to provide sophisticated visual data analytics on one hand, while preserving user-friendliness on the other. Furthermore, recent hardware developments such as smart glasses, as well as achievements in virtual-reality applications, have fanned immersive analytic solutions. Notably, such solutions can be very effective when they are applied to high-dimensional datasets. Taking this advantage into account, the work at hand applies immersive analytics to a high-dimensional production dataset to improve the digital support of daily work tasks. More specifically, a mixed-reality implementation is presented that will support manufacturers as well as data scientists to comprehensively analyze machine data. As a particular goal, the prototype will simplify the analysis of manufacturing data through the usage of dimensionality reduction effects. Therefore, five aspects are mainly reported in this paper. First, it is shown how dimensionality reduction effects can be represented by clusters. Second, it is presented how the resulting information loss of the reduction is addressed. Third, the graphical interface of the developed prototype is illustrated as it provides (1) a correlation coefficient graph, (2) a plot for the information loss, and (3) a 3D particle system. In addition, an implemented voice recognition feature of the prototype is shown, which was considered to be being promising to select or deselect data variables users are interested in when analyzing the data. Fourth, based on a machine learning library, it is shown how the prototype reduces computational resources using smart glasses. The main idea is based on a recommendation approach as well as the use of subspace clustering. Fifth, results from a practical setting are presented, in which the prototype was shown to domain experts. The latter reported that such a tool is actually helpful to analyze machine data daily. Moreover, it was reported that such a system can be used to educate machine operators more properly. As a general outcome of this work, the presented approach may constitute a helpful solution for the industry as well as other domains such as medicine",'MDPI AG',Dimensionality Reduction and Subspace Clustering in Mixed Reality for Condition Monitoring of High-Dimensional Production Data,,10.3390/s19183903,,core
304119006,2019-01-01T00:00:00,"Prognostic Health Management (PHM) is a maintenance policy aimed at predicting the occurrence of a failure in components in order to minimize unexpected downtimes of complex systems and maximize their availability. Recent developments in condition monitoring (CM) techniques and Artificial Intelligence (AI) tools enabled the collection of a huge amount of data in real-time and its transformation into meaningful information that will support the maintenance decision-making process. The emerging Cyber-Physical Systems (CPS) technologies connect distributed physical systems with their virtual representations in the cyber computational world. The PHM assumes a key role in the implementation of CPS in manufacturing contexts, since it allows to keep CPS and its machines in proper conditions. On the other hand, CPS-based PHM provide an efficient solution to maximize availability of machines and production systems.
In this paper, evolving and unsupervised approaches for the implementation of PHM at a component level are described, which are able to process streaming data in real-time and with almost-zero prior knowledge about the monitored component. A case study from a real industrial context is presented. Different unsupervised and online anomaly detection methods are combined with evolving clustering models in order to detect anomalous behaviors in streaming vibration data and integrate the so-generated knowledge into supervised and adaptive models; then, the degradation model for each identified fault is built and the resulting RUL prediction model integrated into the online analysis. Supervised methods are applied to the same dataset, in batch mode, to validate the proposed procedure",'Elsevier BV',Prognostic Health Management of Production Systems. New Proposed Approach and Experimental Evidences,,10.1016/j.promfg.2020.01.333,,core
478143273,2019-10-03T07:51:50,"The book """"Simulation and Gaming"""" discusses the following topics and research areas: game-based methods of problem solution and data processing, analysis, and information mining; educational games and game features, including game characteristics, story, mechanics, and methodology; development of integrated games tasked with helping students in interpreting, translating, and manipulating the field of kinematics through formal presentations; possibility of research integration through real and practical examples and games as well, in the field of physics; analysis of game engines from various aspects such as modularity, performance, and usability; virtual reality (VR) and interaction mechanisms used for three-dimensional (3D) game development; analysis, development, design, implementation, and evaluation of the simulation model in the field of engineering and metallurgy, according to ADDIE model; concept of computational thinking, with an accent on its inclusion in compulsory education; overview of the current prominence of AI simulation based in the gaming leisure industry, mainly for research purposes in the context of gambling and forecasting of online casino patron's churn behavior; innovative modeling and simulation approach using newly proposed advanced game-based mathematical framework, unified game-based acquisition framework, and a set of war-gaming engines to address the challenges for acquisition of future space systems; modification of simulation of a complex system and a physics model through programming, achieved with a block-based programming language",'IntechOpen',Simulation and Gaming,,10.5772/intechopen.69391,,core
286081104,2019-01-01T00:00:00,"It is not so far away when the theory of disruptive innovation and the emergence of new breakthrough technologies which have simpler, cheaper and higher performance like Artificial Intelligence, Internet of Things, Big Data, Cloud Computing, Additive Manufacturing, Blockchain, etc., radically change all sectors included Architecture, Engineering and Construction (AEC). Exponential changes are happening at a dramatic speed around us. Previous industrial revolutions liberated humankind from animal power, made mass production possible and brought digital capabilities to billions of people. The last Industrial revolution is characterized by a range of new technologies that are fusing the physical, digital and biological worlds, impacting all disciplines, especially economics and industries. The main aim of this research is to highlight the “grey zone” and side-effects related to the impact of eXtended Reality – XR – Technologies: Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) on AEC Industry. This paper argues that these technologies can be considered as means to bridge the gap between construction process and design process as well as creativity and design. It states that the extended reality applications are not limited to the projection of the models’ exterior surfaces as it was in the first-generation VR technologies, but the second generation can simulate in real-time physical phenomena and behaviors inside the models as well. One of the main attributes of extended technologies is that it can be considered as a medium that allows people with different skills and expertise to deal with a problem as they are all in the “same boat”, and thus, it enables a real “Collaborative Design” and idea exchanging happens. On the other hand, this technology is very well intertwined and related to the new design process paradigms: Collaborative Design, and Pro-Active Design. In this sense that every actor involved may have its own role and responsibility to aim the final goal project. XR technologies affect the pro-active design process in terms of suggestion offered by experts involved in the projects in advance. The virtual reality (VR), augmented reality (AR) and mixed reality (MR) technologies continue to gain more and more attraction in consumer gadgets and software. Although, many leading-edge AEC firms and also tech start-ups are investing heavily in AR and VR, the construction industry as a whole has remained laggard behind the curve when it comes to tech adoption. Much of this failure can likely be attributed to a general lack of awareness and understanding",,The Impact of eXtended Reality Technologies on AEC,,,,core
211327185,2019-01-08T00:00:00,"This study proposes a model designed to help sales representatives in the software industry to manage the complex sales pipeline. By integrating business analytics in the form of machine learning into lead and opportunity management, data-driven qualification support reduces the high degree of arbitrariness caused by professional expertise and experiences. Through the case study of a software provider, we developed an artifact consisting of three models to map the end-to-end sales pipeline process using real business data from the company’s CRM system. The results show a superiority of the CatBoost and Random Forest algorithm over other supervised classifiers such as Support Vector Machine, XGBoost, and Decision Tree as the baseline. The study also reveals that the probability of either winning or losing a sales deal in the early lead stage is more difficult to predict than analyzing the lead and opportunity phases separately. Furthermore, an explanation functionality for individual predictions is provided",'HICSS Conference Office',Business Analytics for Sales Pipeline Management in the Software Industry: A Machine Learning Perspective,https://core.ac.uk/download/211327185.pdf,10.24251/HICSS.2019.125,,core
334837202,2019-10-08T00:00:00,"Recently, a number of deep learning-based anomaly detection algorithms were
proposed to detect attacks in dynamic industrial control systems. The detectors
operate on measured sensor data, leveraging physical process models learned a
priori. Evading detection by such systems is challenging, as an attacker needs
to manipulate a constrained number of sensor readings in real-time with
realistic perturbations according to the current state of the system. In this
work, we propose a number of evasion attacks (with different assumptions on the
attacker's knowledge), and compare the attacks' cost and efficiency against
replay attacks. In particular, we show that a replay attack on a subset of
sensor values can be detected easily as it violates physical constraints. In
contrast, our proposed attacks leverage manipulated sensor readings that
observe learned physical constraints of the system. Our proposed white box
attacker uses an optimization approach with a detection oracle, while our black
box attacker uses an autoencoder (or a convolutional neural network) to
translate anomalous data into normal data. Our proposed approaches are
implemented and evaluated on two different datasets pertaining to the domain of
water distribution networks. We then demonstrated the efficacy of the real-time
attack on a realistic testbed. Results show that the accuracy of the detection
algorithms can be significantly reduced through real-time adversarial actions:
for the BATADAL dataset, the attacker can reduce the detection accuracy from
0.6 to 0.14. In addition, we discuss and implement an Availability attack, in
which the attacker introduces detection events with minimal changes of the
reported data, in order to reduce confidence in the detector",,"Real-time Evasion Attacks with Physical Constraints on Deep
  Learning-based Anomaly Detectors in Industrial Control Systems",http://arxiv.org/abs/1907.07487,,,core
334880809,2019-11-11T00:00:00,"Given the effectiveness and ease of use, Item-based Collaborative Filtering
(ICF) methods have been broadly used in industry in recent years. The key of
ICF lies in the similarity measurement between items, which however is a
coarse-grained numerical value that can hardly capture users' fine-grained
preferences toward different latent aspects of items from a representation
learning perspective. In this paper, we propose a model called REDA (latent
Relation Embedding with Dual Attentions) to address this challenge. REDA is
essentially a deep learning based recommendation method that employs an item
relation embedding scheme through a neural network structure for inter-item
relations representation. A relational user embedding is then proposed by
aggregating the relation embeddings between all purchased items of a user,
which not only better characterizes user preferences but also alleviates the
data sparsity problem. Moreover, to capture valid meta-knowledge that reflects
users' desired latent aspects and meanwhile suppress their explosive growth
towards overfitting, we further propose a dual attentions mechanism, including
a memory attention and a weight attention. A relation-wise optimization method
is finally developed for model inference by constructing a personalized ranking
loss for item relations. Extensive experiments are implemented on real-world
datasets and the proposed model is shown to greatly outperform state-of-the-art
methods, especially when the data is sparse",,"Beyond Similarity: Relation Embedding with Dual Attentions for
  Item-based Recommendation",http://arxiv.org/abs/1911.04099,,,core
334882174,2019-11-13T00:00:00,"It is critical to secure the Industrial Internet of Things (IIoT) devices
because of potentially devastating consequences in case of an attack. Machine
learning and big data analytics are the two powerful leverages for analyzing
and securing the Internet of Things (IoT) technology. By extension, these
techniques can help improve the security of the IIoT systems as well. In this
paper, we first present common IIoT protocols and their associated
vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the
utilization of machine learning in countering these susceptibilities. Following
that, a literature review of the available intrusion detection solutions using
machine learning models is presented. Finally, we discuss our case study, which
includes details of a real-world testbed that we have built to conduct
cyber-attacks and to design an intrusion detection system (IDS). We deploy
backdoor, command injection, and Structured Query Language (SQL) injection
attacks against the system and demonstrate how a machine learning based anomaly
detection system can perform well in detecting these attacks. We have evaluated
the performance through representative metrics to have a fair point of view on
the effectiveness of the methods",,"Machine Learning Based Network Vulnerability Analysis of Industrial
  Internet of Things",http://arxiv.org/abs/1911.05771,,,core
201023552,2019-04-01T00:00:00,"On the basis of the analysis of queries in Google Trends, Google Scholar, and the database of Volodymyr Vernadskyi National Library of Ukraine, a sample of historically structured knowledge systems has been obtained which can be used to describe the contribution of distance learning studies during the time slots available for each resource. According to the survey of students of four institutions of different education types (classical, technological, pedagogical and marine ones), the modern state-of-the-art of the application of distance learning technologies at higher education institutions is established. The analysis of the progressive ideas and practical achievements of countries of Europe, North America and Asia, which in recent years have made the significant progress in reforming their educational systems and the implementation of innovative technologies, has allowed to distinguish the following technological advances for the implementation of distance learning technologies: the adaptive learning technology, mobile learning, the virtual, supplemented and hybrid reality, the Internet of Things, systems of the next generation learning management, artificial intelligence and natural user interfaces. The following perspectives of the distance learning development in Ukraine are highlighted: updating of software and technical support and material resources of higher education institutions; provision of higher education institutions of Ukraine with the broadband Internet access; organization of cooperation of software developers for distance learning, distance education methodologists and lecturer; improvement the staffing of distance learning; development and distribution of platforms with intuitive non-complex software interfaces for creating distance courses; creation or adaptation of information technologies and electronic educational and methodological developments for the support of new technologies of distance learning at higher education institutions of Ukraine; studying the effectiveness of technological advances in the IT industry in the process of higher education teaching and learning, ensuring the process of obtaining an educational degree (bachelor’s degree, master’s degree) at higher education institutions of Ukraine through training at mass open distance courses",'Institute of Information Technologies and Learning Tools of NAES of Ukraine',"CONDITION, TECHNOLOGIES AND PROSPECTS OF DISTANCE LEARNING IN THE HIGHER EDUCATION OF UKRAINE",,10.33407/itlt.v70i2.2907,"[{'title': 'Information Technologies and Learning Tools', 'identifiers': ['issn:2076-8184', '2076-8184']}]",core
269491081,2019-04-04T07:00:00,"The recent advancements in computing and sensor technologies, coupled with improvements in embedded system design methodologies, have resulted in the novel paradigm called the Internet of Things (IoT). IoT is essentially a network of small embedded devices enabled with sensing capabilities that can interact with multiple entities to relay information about their environments. This sensing information can also be stored in the cloud for further analysis, thereby reducing storage requirements on the devices themselves. The above factors, coupled with the ever increasing needs of modern society to stay connected at all times, has resulted in IoT technology penetrating all facets of modern life. In fact IoT systems are already seeing widespread applications across multiple industries such as transport, utility, manufacturing, healthcare, home automation, etc.
Although the above developments promise tremendous benefits in terms of productivity and efficiency, they also bring forth a plethora of security challenges. Namely, the current design philosophy of IoT devices, which focuses more on rapid prototyping and usability, results in security often being an afterthought. Furthermore, one needs to remember that unlike traditional computing systems, these devices operate under the assumption of tight resource constraints. As such this makes IoT devices a lucrative target for exploitation by adversaries. This inherent flaw of IoT setups has manifested itself in the form of various distributed denial of service (DDoS) attacks that have achieved massive throughputs without the need for techniques such as amplification, etc. Furthermore, once exploited, an IoT device can also function as a pivot point for adversaries to move laterally across the network and exploit other, potentially more valuable, systems and services. Finally, vulnerable IoT devices operating in industrial control systems and other critical infrastructure setups can cause sizable loss of property and in some cases even lives, a very sobering fact.
In light of the above, this dissertation research presents several novel strategies for identifying known and  zero-day attacks against IoT devices, as well as identifying infected IoT devices present inside a network along with some mitigation strategies. To this end, network telescopes are  leveraged to generate Internet-scale notions of maliciousness in conjunction with signatures that can be used to identify such devices in a network. This strategy is further extended by developing a taxonomy-based methodology which is capable of categorizing unsolicited IoT behavior by leveraging machine learning (ML) techniques, such as ensemble learners, to identify similar threats in near-real time. Furthermore, to overcome the challenge of insufficient (malicious) training data within the IoT realm, a generative adversarial network (GAN) based framework is also developed to identify known and unseen attacks on IoT devices. Finally, a software defined networking (SDN) based solution is proposed to mitigate threats from unsolicited IoT devices",Scholar Commons,Security Framework for the Internet of Things Leveraging Network Telescopes and Machine Learning,https://core.ac.uk/download/269491081.pdf,,,core
231817164,2019-01-01T00:00:00,"Data-driven machine learning methods have achieved impressive performance for many industrial applications and academic tasks. Machine learning methods usually have two stages: training a model from large-scale samples, and inference on new samples after the model is deployed. The training of modern models relies on solving difficult optimization problems that involve nonconvex, nondifferentiable objective functions and constraints, which is sometimes slow and often requires expertise to tune hyperparameters. While inference is much faster than training, it is often not fast enough for real-time applications.We focus on machine learning problems that can be formulated as a minimax problem in training, and study alternating optimization methods served as fast, scalable, stable and automated solvers.   

First, we focus on the alternating direction method of multipliers (ADMM) for constrained problem in classical convex and nonconvex optimization. Some popular machine learning applications including sparse and low-rank models, regularized linear models, total variation image processing, semidefinite programming, and consensus distributed computing. We propose adaptive ADMM (AADMM), which is a fully automated solver achieving fast practical convergence by adapting the only free parameter in ADMM. We further automate several variants of ADMM (relaxed ADMM, multi-block ADMM and consensus ADMM), and prove convergence rate guarantees that are widely applicable to variants of ADMM with changing parameters. We release the fast implementation for more than ten applications and validate the efficiency with several benchmark datasets for each application. Second, we focus on the minimax problem of generative adversarial networks (GAN). We apply prediction steps to stabilize stochastic alternating methods for the training of GANs, and demonstrate advantages of GAN-based losses for image processing tasks. We also propose GAN-based knowledge distillation methods to train small neural networks for inference acceleration, and empirically study the trade-off between acceleration and accuracy.Third, we present preliminary results on adversarial training for robust models. We study fast algorithms for the attack and defense for universal perturbations, and then explore network architectures to boost robustness",'Wiley',"Alternating Optimization: Constrained Problems, Adversarial Networks, and Robust Models",,10.13016/xgwp-edlc,,core
229314141,2019-08-01T07:00:00,"Training and on-site assistance is critical to help workers master required skills, improve worker productivity, and guarantee the product quality. Traditional training methods lack worker-centered considerations that are particularly in need when workers are facing ever-changing demands. In this study, we propose a worker-centered training & assistant system for intelligent manufacturing, which is featured with self-awareness and active-guidance. Multi-modal sensing techniques are applied to perceive each individual worker and a deep learning approach is developed to understand the worker\u27s behavior and intention. Moreover, an object detection algorithm is implemented to identify the parts/tools the worker is interacting with. Then the worker\u27s current state is inferred and used for quantifying and assessing the worker performance, from which the worker\u27s potential guidance demands are analyzed. Furthermore, onsite guidance with multi-modal augmented reality is provided actively and continuously during the operational process. Two case studies are used to demonstrate the feasibility and great potential of our proposed approach and system for applying to the manufacturing industry for frontline workers",'Elsevier BV',A Self-Aware and Active-Guiding Training &  Assistant System for Worker-Centered Intelligent Manufacturing,,10.1016/j.mfglet.2019.08.003,,core
268924452,2019-01-01T00:00:00,"Актуальность исследования обусловлена необходимостью создания современных компьютерных систем для мониторинга опасных технологических объектов предприятий нефтегазовой отрасли. Цель: создание интеллектуальной системы компьютерного зрения беспилотных летательных аппаратов, позволяющей вести мониторинг опасных технологических объектов и анализ данных мониторинга в режиме реального времени на борту беспилотных летательных аппаратов. Объекты: концепция построения интеллектуальной системы компьютерного зрения; новые архитектуры свёрточных нейронных сетей, аппаратно-реализованные на программируемых логических интегральных схемах; метод унификации вычислительных блоков и способы параллельных вычислений в аппаратных свёрточных нейронных сетях; алгоритмы помехоустойчивого кодирования/декодирования данных при обменах сообщениями между наземной и бортовой компонентами интеллектуальной системы компьютерного зрения. Методы: методы классификации и детектирования объектов на изображениях с помощью свёрточных нейронных сетей; методы глубокого обучения свёрточных нейронных сетей; методы проектирования программно-аппаратных систем. Результаты. Проведён анализ современного состояния исследований в области систем мониторинга опасных технологических объектов предприятий нефтегазовой отрасли; разработана концепция создания интеллектуальной системы компьютерного зрения на основе беспилотных летательных аппаратов для мониторинга опасных объектов. Базовой в концепции является идея анализа изображений, полученных при мониторинге технологических объектов и прилегающих к ним территорий, непосредственно на борту беспилотных летательных аппаратов в режиме реального времени. Более того, показано, что для обеспечения такого анализа в реальном времени необходимо применять аппаратно-реализованные свёрточные нейронные сети. Для интеллектуальной системы компьютерного зрения разработаны архитектуры свёрточных нейронных сетей из перспективных подклассов LeNet5 и YOLO; предложены алгоритмы помехоустойчивого кодирования/декодирования данных при обмене сообщениями между наземной и бортовой компонентами системы компьютерного зрения; разработан оригинальный метод организации вычислений в аппаратных свёрточных нейронных сетях на программируемых логических интегральных схемах, отличающийся от известных использованием унифицированных вычислительных блоков; предложены новые способы параллельных вычислений в слоях таких свёрточных нейронных сетей. Разработана архитектура вычислительного устройства беспилотных летательных аппаратов, включающего блоки аппаратной свёрточной нейронной сети и кодер/декодер данных. Устройство создано на основе системы на кристалле Cyclone V SX компании Altera; получены первые результаты исследования эффективности этого устройства; разработано программное обеспечение наземной компоненты системы компьютерного зрения.The relevance of the research is caused by the necessity to develop modern computer vision systems for monitoring hazardous techno- logical objects of oil and gas industry. The main aim of the research is to develop the intelligent computer vision system for unmanned aerial vehicles, which allows monitoring dangerous technological objects and analyzing the monitoring data in real-time on the board of the unmanned aerial vehicle. Objects: the concept of construction of intelligent computer vision system; new architectures of convolutional neural networks hardware- based using field programmable gate array; the method of unification of computing blocks and ways of parallel calculation in hardware-based convolutional neural networks; algorithms of error-correction encoding and decoding data for exchanging message between ground and airborne components of the intelligent computer vision system. Methods: methods of detection and classification objects in images using convolutional neural networks; convolutional neural network deep learning methods; methods of designing software and hardware systems. Results. We have been analyzed the current state of research in the field of monitoring hazardous technological objects of the oil and gas industry and developed the concept of construction of intelligent computer vision system for unmanned aerial vehicles for monitoring dangerous objects. The idea of analyzing the images, obtained at monitoring of technological objects and surrounding areas, directly onboard of the unmanned aerial vehicle in real time was the base in this concept. Moreover, it is shown that the use of hardware- based convolutional neural networks for providing such analysis in real time is required. The authors developed the convolutional neural networks architectures for computer vision system from promising subclasses LeNet5 and YOLO and proposed the algorithms of error- correction data encoding/decoding for messages exchanging between these components, considering the specifics of ground and air- borne components. The authors developed the original method of organizing calculation in hardware-based convolutional neural net- works using field programmable gate array, which differs from the known ones by using the unified computing blocks and new ways of parallel calculation in layers in these convolutional neural networks. They proposed the architecture of computing device of the unmanned aerial vehicle which includes the blocks of the hardware-based convolutional neural networks and the data encoder/decoder. This device is based on the Altera Cyclone V SX system-on-a-chip. The paper demonstrates the first results of studying the device efficiency. The authors developed the software for the ground component of the computer vision system",'National Research Tomsk Polytechnic University',Intelligent computer vision system for unmanned aerial vehicles for monitoring technological objects of oil and gas industry,,10.18799/24131830/2019/11/2346,"[{'title': 'Izvestiya Tomskogo Politekhnicheskogo Universiteta Inziniring Georesursov', 'identifiers': ['issn:2413-1830', '2413-1830']}]",core
389889861,2019-11-21T00:00:00,"The problem of inefficient processing in the Big Data industry is touched upon. A detailed analysis of the various means to increase the percentage of processed data is provided and the experimental implementation of a way to obtain and preprocess data in a mobile device network in real-time mode is shown.During the analysis of the subject, the next fields of research were observed: Deep Learning, Machine Learning, Big Data, and GPGPU technology. The increasing numbers of publications, especially of papers that include mobile networks, were emphasized as evidence of rapid growth of the industry and a transition of neuron network algorithms toward mobile operating systems.Further analysis was focused on highlighting the most relevant and perspective objects for the research. The analysis showed that amidst currently most innovative and broadly widespread operation systems and frameworks to implement and engage neuron network algorithms Android operation system and TensorFlow framework has the most significant advantages.Due to the purpose of developing an experimental solution based on mobile device network and neuron network, different classes and types of neuron network architectures were explored. Two major types of mobile neuron networks such as quantized and integer neuron networks and the principal dissimilarity between them were described. Various neuron networks were tested on mobile devices with Internet connection via specially developed auxiliary software using GPGPU technology. Experimental results had shown that modern smartphones such as Huawei P20-Pro are capable to analyze, store and transmit the incoming from its camera sensor information at a rate of up to 40 frames per second. The usage of mobile GPU for improving the performance of the neuron networks was proved to be effective as such the number of frames processed by a neuron network per second can be elevated up to 10 times.The experimental software that task was to search for the given by user object in a real-time mode using mobile device network as both — server and processing nodes — proved to be a violable solution for the increasing amount of preprocessed up-to-the-minute information in the Big Data industry after the diligent research.As a summary, it needs to be stated that current progress in mobile device industry is making possible to bring neuron network technologies on mobile platforms and expand the capability of data aggregating services through the use of modern Deep Learning frameworks and GPGPU technology.Проаналізовано напрямки досліджень у галузі концептів Big Data, розподілених мереж мобільних пристроїв і Deep Learning. Кількісно охарактеризовано та порівняно інтенсивність розвитку сучасних бібліотек нейронних мереж для використання технологій Deep Learning, Big Data, GPGPU. Розроблено застосування для дослідження роботи згорткових нейронних мереж різної архітектури із використанням потуж-ностей мобільних CPU та GPU і з використанням API для нейронних мереж на операційній системі Android. Розроблено застосування для агрегації проаналізованих у реальному часі даних, структуризації даних на сервері та досліджено роботу застосування в мережах WiFi, 3G та 4G. Проведено аналіз різних шляхів агрегації даних",Інститут проблем реєстрації інформації НАН України,Агрегація та аналіз графічних даних  у розподіленій мережі мобільних пристроїв,,,,core
326833536,2019-10-01T07:00:00,"Modern industry deals with large amounts of data, which is often difficult for humans to process and use for decision making. Industry 4.0 proposes the automation of different procedures in enterprises, aiming to reduce human errors, operation time and costs. That includes analysis of different operation parameters in near real time, in order to facilitate management to make the right decisions at the right time. That requires the use of tools that are simple and fast to use and provide the necessary information. The present paper describes an architecture of a Business Intelligence system proposed for a Telecommunications software company. The system draws information from a proprietary ERP and is all developed using free open source software. The architecture proposed uses the power of R for statistical computing, data mining and artificial intelligence. Financial information is shown in a dashboard in near real time",AIS Electronic Library (AISeL),Near Real Time Business Intelligence Framework using R Shiny,https://core.ac.uk/download/326833536.pdf,,,core
343445450,2019-09-27T00:00:00,"Nowadays, reliability of sensors is one of the most important challenges for widespread

application of Internet-of-things data in key emerging fields such as the automotive and manufacturing

sectors. This paper presents a brief review of the main research and innovation actions at the European

level, as well as some on-going research related to sensor reliability in cyber-physical systems (CPS).

The research reported in this paper is also focused on the design of a procedure for evaluating the

reliability of Internet-of-Things sensors in a cyber-physical system. The results of a case study of

sensor reliability assessment in an autonomous driving scenario for the automotive sector are also

shown. A co-simulation framework is designed in order to enable real-time interaction between

virtual and real sensors. The case study consists of an IoT LiDAR-based collaborative map in order to

assess the CPS-based co-simulation framework. Specifically, the sensor chosen is the Ibeo Lux 4-layer

LiDAR sensor with IoT added capabilities. The modeling library for predicting error with machine

learning methods is implemented at a local level, and a self-learning-procedure for decision-making

based on Q-learning runs at a global level. The study supporting the experimental evaluation of the

co-simulation framework is presented using simulated and real data. The results demonstrate the

effectiveness of the proposed method for increasing sensor reliability in cyber-physical systems using

Internet-of-Things data.This work was partially supported by the project Power2Power: Providing next-generation silicon-based power solutions in transport and machinery for significant decarbonisation in the next decade, funded by the Electronic Component Systems for European Leadership (ECSEL-JU) Joint Undertaking and the Ministry of Science, Innovation and Universities (MICINN), under grant agreement No 826417. In addition, this work was also funded by the Spanish Ministry of Science, Innovation and Universities through the project COGDRIVE (DPI2017-86915-C3-1-R). Preparation of this publication was also partially co-financed by the Polish National Agency for Academic Exchange (NAWA) through the project: “Industry 4.0 in Production and Aeronautical Engineering (IPAE)”.Peer reviewe",'MDPI AG',Sensor Reliability in Cyber-Physical Systems Using Internet-of-Things Data: A Review and Case Study,https://core.ac.uk/download/343445450.pdf,10.3390/rs11192252,"[{'title': 'Remote Sensing', 'identifiers': ['2072-4292', 'issn:2072-4292']}]",core
226748928,2019-01-01T00:00:00,"Multi-criteria inventory classification groups inventory items into classes, each of which is managed by a specific re-order policy according to its priority. However, the tasks of inventory classification and control are not carried out jointly if the classification criteria and the classification approach are not robustly established from an inventory-cost perspective. Exhaustive simulations at the single item level of the inventory system would directly solve this issue by searching for the best re-order policy per item, thus achieving the subsequent optimal classification without resorting to any multi-criteria classification method. However, this would be very time-consuming in real settings, where a large number of items need to be managed simultaneously. In this article, a reduction in simulation effort is achieved by extracting from the population of items a sample on which to perform an exhaustive search of best re-order policies per item; the lowest cost classification of in-sample items is, therefore, achieved. Then, in line with the increasing need for ICT tools in the production management of Industry 4.0 systems, supervised classifiers from the machine learning research field (i.e. support vector machines with a Gaussian kernel and deep neural networks) are trained on these in-sample items to learn to classify the out-of-sample items solely based on the values they show on the features (i.e. classification criteria). The inventory system adopted here is suitable for intermittent demands, but it may also suit non-intermittent demands, thus providing great flexibility. The experimental analysis of two large datasets showed an excellent accuracy, which suggests that machine learning classifiers could be implemented in advanced inventory classification systems",'Informa UK Limited',Machine learning for multi-criteria inventory classification applied to intermittent demand,,10.1080/09537287.2018.1525506,,core
210994460,2019-06-05T00:00:00,"Approximately one-third of the food produced globally is spoiled or wasted in the food supply chain (FSC). Essentially, it is lost before it even reaches the end consumer. Conventional methods of food waste tracking relying on paper-based logs to collect and analyse the data are costly, laborious, and time-consuming. Hence, an automated and real-time system based on the Internet of Things (IoT) concepts is proposed to measure the overall amount of waste as well as the reasons for waste generation in real-time within the potato processing industry, by using modern image processing and load cell technologies. The images captured through a specially positioned camera are processed to identify the damaged, unusable potatoes, and a digital load cell is used to measure their weight. Subsequently, a deep learning architecture, specifically the Convolutional Neural Network (CNN), is utilised to determine a potential reason for the potato waste generation. An accuracy of 99.79% was achieved using a small set of samples during the training test. We were successful enough to achieve a training accuracy of 94.06%, a validation accuracy of 85%, and a test accuracy of 83.3% after parameter tuning. This still represents a significant improvement over manual monitoring and extraction of waste within a potato processing line. In addition, the real-time data generated by this system help actors in the production, transportation, and processing of potatoes to determine various causes of waste generation and aid in the implementation of corrective actions",'MDPI AG',Monitoring Potato Waste in Food Manufacturing Using Image Processing and Internet of Things Approach,https://core.ac.uk/download/210994460.pdf,10.3390/su11113173,,core
334827342,2019-06-24T00:00:00,"One of the most exciting technology breakthroughs in the last few years has
been the rise of deep learning. State-of-the-art deep learning models are being
widely deployed in academia and industry, across a variety of areas, from image
analysis to natural language processing. These models have grown from fledgling
research subjects to mature techniques in real-world use. The increasing scale
of data, computational power and the associated algorithmic innovations are the
main drivers for the progress we see in this field. These developments also
have a huge potential for the automotive industry and therefore the interest in
deep learning-based technology is growing. A lot of the product innovations,
such as self-driving cars, parking and lane-change assist or safety functions,
such as autonomous emergency braking, are powered by deep learning algorithms.
Deep learning is poised to offer gains in performance and functionality for
most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for
vehicle dynamics application, vehicle inspection/heath monitoring, automated
driving and data-driven product development are key areas that are expected to
get the most attention. This article provides an overview of the recent
advances and some associated challenges in deep learning techniques in the
context of automotive applications",,"Deep Learning in the Automotive Industry: Recent Advances and
  Application Examples",http://arxiv.org/abs/1906.08834,,,core
334844619,2019-10-21T00:00:00,"Artificial Intelligence (AI) is a cognitive science to enables human to
explore many intelligent ways to model our sensing and reasoning processes.
Industrial AI is a systematic discipline to enable engineers to systematically
develop and deploy AI algorithms with repeating and consistent successes. In
this paper, the key enablers for this transformative technology along with
their significant advantages are discussed. In addition, this research explains
Lighthouse Factories as an emerging status applying to the top manufacturers
that have implemented Industrial AI in their manufacturing ecosystem and gained
significant financial benefits. It is believed that this research will work as
a guideline and roadmap for researchers and industries towards the real-world
implementation of Industrial AI",,Industrial Artificial Intelligence,http://arxiv.org/abs/1908.02150,,,core
287429785,2019-01-01T00:00:00,"International audienceThe use of wireless sensor networks, which are the key ingredient in the growing Internet of Things (IoT), has surged over the past few years with a widening range of applications in the industry, healthcare, agriculture, with a special attention to monitoring and tracking, often tied with security issues. In some applications, sensors can be deployed in remote, large unpopulated areas, whereas in others, they serve to monitor confined busy spaces. In either case, clustering the sensor network’s nodes into several clusters is of fundamental benefit for obvious scalability reasons, and also for helping to devise maintenance or usage schedules that might greatly improve the network’s lifetime. In the present paper, we survey and compare popular and advanced clustering schemes and provide a detailed analysis of their performance as a function of scale, type of collected data or their heterogeneity, and noise level. The testing is performed on real sensor data provided by the UCI Machine Learning Repository, using various external validation metrics",HAL CCSD,Introducing and Comparing Recent Clustering Methods for Massive Data Management in the Internet of Things,,,,core
301379409,2019-05-15T07:00:00,"The increasing availability of data and computing capacity drives optimization potential. In the industrial context, predictive maintenance is particularly promising and various algorithms are available for implementation. For the evaluation and selection of predictive maintenance algorithms, hitherto, statistical measures such as absolute and relative prediction errors are considered. However, algorithm selection from a purely statistical perspective may not necessarily lead to the optimal economic outcome as the two types of prediction errors (i.e., alpha error ignoring system failures versus beta error falsely indicating system failures) are negatively correlated, thus, cannot be jointly optimized and are associated with different costs. Therefore, we compare the prediction performance of three types of algorithms from an economic perspective, namely Artificial Neural Networks, Support Vector Machines, and Hotelling T² Control Charts. We show that the translation of statistical measures into a single cost-based objective function allows optimizing the individual algorithm parametrization as well as the un-ambiguous comparison among algorithms. In a real-life scenario of an industrial full-service provider we derive cost advantages of more than 17% compared to an algorithm selection based on purely statistical measures. This work contributes to the theoretical and practical knowledge on predictive maintenance algorithms and supports predictive maintenance investment decisions",AIS Electronic Library (AISeL),ECONOMIC PERSPECTIVE ON ALGORITHM SELECTION FOR PREDICTIVE MAINTENANCE,https://core.ac.uk/download/301379409.pdf,,,core
201284351,2019-01-01T00:00:00,"Image-based measurement has received increasing attention as it can substantially reduce the cost of labor, measurement equipment, and installation process. Instead of using optical flow, pattern, or marker tracking to extract a displacement signal, in this study, a novel noncontact machine learning-based system was proposed to directly predict vibration frequency with high accuracy and good reliability by using image sequences acquired from a single camera. The performance of the proposed method was demonstrated through experiments conducted in a laboratory and under real-field conditions and compared with those obtained using a contacted sensor. The vibration frequency prediction results of the proposed method are compared with industry-level vibration sensor results in the frequency domain, demonstrating that the proposed method could predict the target-object-vibration frequency as accurately as an industry-level vibration sensor, even under uncontrollable real-field conditions with no additional enhancement or extra signal processing techniques. However, only the principal vibration frequency of a measurement target is predicted, and the measurement range is limited by the trained model. Nonetheless, if these limitations are resolved, this method can potentially be used in real engineering applications in mechanical or civil structural health monitoring thanks to the simple deployment and concise pipeline of this method",'Hindawi Limited',Neural Network with Confidence Kernel for Robust Vibration Frequency Prediction,,10.1155/2019/6573513,"[{'title': 'Journal of Sensors', 'identifiers': ['1687-7268', '1687-725x', 'issn:1687-7268', 'issn:1687-725X']}]",core
215187043,2019-05-01T00:00:00,"The microwave band is well suited to wireless applications, including radar, communications, and electronic warfare. While radar operations currently have priority in a portion of the microwave band, wireless companies are lobbying to change that; such a change would force current operators into a smaller total bandwidth. Interference would occur, and has already occurred at the former National Weather Radar Testbed Phased Array Radar. 

The research in this dissertation was motivated by this interference --- it occurred even without a change to radar's primacy in the microwave band. If microwave operations had to squeeze into a smaller overall bandwidth, such interference, whether originating from other radars or some other source, would only become more common. The radio frequency interference (RFI) present at the National Weather Radar Testbed Phased Array Radar altered the statistical properties at certain locations, causing targets to be erroneously detected. While harmless enough in clear air, it could affect National Weather Service decisions if it occurred during a weather event. 

The initial experiments, covered in Chapter 2, used data comprised of a single channel of in-phase and quadrature (IQ) data, reflecting the resources available to the National Weather Service's weather radar surveillance network. A new algorithm, the Interference Spike Detection Algorithm, was developed with these restrictions in mind. This new algorithm outperforms several interference detection algorithms developed by industry. Tests on this data examined algorithm performance quantitatively, using real and simulated weather data and radio frequency interference. Additionally, machine learning classification algorithms were employed for the first time to the RFI classification problem and it was found that, given enough resources, machine learning had the potential to perform even better than the other temporal algorithms.

Subsequent experiments, covered in Chapter 3, used spatial data from phased arrays and looked at methods of interference mitigation that leveraged this spatial data. Specifically, adaptive beamforming techniques could be used to mitigate interference and improve data quality. A variety of adaptive digital beamforming techniques were evaluated in terms of their performance at interference mitigation for a communications task. Additionally, weather radar data contaminated with ground clutter was collected from the sidelobe canceller channels of the former National Weather Radar Testbed Phased Array Radar and, using the reasoning that ground clutter is simply interference from the ground, adaptive digital beamforming was successfully employed to mitigate the impact of ground clutter and restore the data to reflect the statistics of the underlying weather data. 

Tests on digital equalization, covered in Chapter 4, used data from a prototype receiver for Horus, a digital phased array radar under development at the University of Oklahoma. The data suffered from significant channel mismatch, which can severely negatively impact the performance of phased arrays. Equalization, implemented both via older digital filter design methods and, for the first time, via newer machine learning regression methods, was able to improve channel matching. When used before adaptive digital beamforming, it was found that digital equalization always improved system performance",,Temporal and Spatial Interference Mitigation Strategies to Improve Radar Data Quality,https://core.ac.uk/download/215187043.pdf,,,core
226908873,2019-07-08T00:00:00,"International audienceThe Industry 4.0 framework needs new intelligent approaches. Thus, the manufacturing industries more and more pay close attention to artificial intelligence (AI). For example, smart monitoring and diagnosis, real time evaluation and optimization of the whole production and raw materials management can be improved by using machine learning and big data tools. An accurate milling process implies a high quality of the obtained material surface (roughness, flatness). With the involvement of AI-based algorithms, milling process is expected to be more accurate during complex operations.In this work, a smart milling diagnosis has been developed for composite sandwich structures based on honey-comb core. The use of such material has grown considerably in recent years, especially in the aeronautic, aerospace, sporting and automotive industries. But the precise milling of such material presents many difficulties. The objective of this work is to develop a data-driven industrial surface quality diagnosis for the milling of honey-comb material, by using supervised machine learning methods. Therefore, cutting forces and workpiece material vibrations are online measured in order to predict the resulting surface flatness.The workpiece material studied in this investigation is Nomex® honeycomb cores with thin cell walls. The Nomex® honeycomb machining presents several defects related to its composite nature (uncut fiber, tearing of the walls), the cutting conditions and to the alveolar geometry of the structure which causes vibration on the different components of the cutting effort.Given the low level of cutting forces, the quality of the obtained machined surface allows to establish criteria for determining the machinability of the honeycomb structures. Nearly 40 features are calculated in time domain and frequency domain from the raw signal in steady state behavior (transient zones are not taken into account). The features are then normalized. The input parameters for each experiment are: the tool rotation speed, the cutting speed and the depth of cut. It is then necessary to make a dimensional reduction of that feature table in order to avoid overfitting and to reduce the computing time of the learning algorithm.In this work, several classification algorithms have been implemented such as : k-nearest neighbor (kNN), Decision trees (DT), Support Vector Machine (SVM). The different supervised learning algorithms have been implemented and compared. Each AI-based model has been applied to a set of features. From the prediction results, SVM algorithm seems to be the most efficient algorithm in this application",HAL CCSD,Milling diagnosis using machine learning approaches,,,,core
227326055,2019-08-13,"International audienceRequests to improve the quality of software are increasing due to the competition in software industry and the complexity of software development integrating multiple technology domains (e.g., IoT, Big Data, Cloud, Artificial Intelligence, Security Technologies). Measurements collection and analysis is key activity to assess software quality during its development live-cycle. To optimize this activity, our main idea is to periodically select relevant measures to be executed (among a set of possible measures) and automatize their analysis by using a dedicated tool. The proposed solution is integrated in a whole PaaS platform called MEASURE. The tools supporting this activity are Software Metric Suggester tool that recommends metrics of interest according several software development constraints and based on artificial intelligence and MINT tool that correlates collected measurements and provides near real-time recommendations to software development stakeholders (i.e. DevOps team, project manager, human resources manager etc.) to improve the quality of the development process. To illustrate the efficiency of both tools, we created different scenarios on which both approaches are applied. Results show that both tools are complementary and can be used to improve the software development process and thus the final software qualit",Springer,Smart measurements and analysis for software quality enhancement,,10.1007/978-3-030-29157-0_9,,core
228383009,2019-01-01T08:00:00,"The Industry 4.0 era requires new quality management systems due to the ever increasing complexity of the global business environment and the advent of advanced digital technologies. This study presents new ideas for predictive quality management based on an extensive review of the literature on quality management and five realworld cases of predictive quality management based on new technologies. The results of the study indicate that advanced technology enabled predictive maintenance can be applied in various industries by leveraging big data analytics, smart sensors, artificial intelligence (AI), and platform construction. Such predictive quality management systems can become living ecosystems that can perform cause-effect analysis, big data monitoring and analytics, and effective decision-making in real time. This study proposes several practical implications for actual design and implementation of effective predictive quality management systems in the Industry 4.0 era. However, the living predictive quality management ecosystem should be the product of the organizational culture that nurtures collaborative efforts of all stakeholders, sharing of information, and co-creation of shared goals",DigitalCommons@University of Nebraska - Lincoln,"The quality management ecosystem for
predictive maintenance in the Industry
4.0 era",https://core.ac.uk/download/228383009.pdf,,,core
186291941,2019-04-10T00:00:00,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads
is a critical task. Deep-learning based methods using annotated LiDAR data have
been the most widely adopted approach for this. Unfortunately, annotating 3D
point cloud is a very challenging, time- and money-consuming task. In this
paper, we propose a novel LiDAR simulator that augments real point cloud with
synthetic obstacles (e.g., cars, pedestrians, and other movable objects).
Unlike previous simulators that entirely rely on CG models and game engines,
our augmented simulator bypasses the requirement to create high-fidelity
background CAD models. Instead, we can simply deploy a vehicle with a LiDAR
scanner to sweep the street of interests to obtain the background point cloud,
based on which annotated point cloud can be automatically generated. This
unique ""scan-and-simulate"" capability makes our approach scalable and
practical, ready for large-scale industrial applications. In this paper, we
describe our simulator in detail, in particular the placement of obstacles that
is critical for performance enhancement. We show that detectors with our
simulated LiDAR point cloud alone can perform comparably (within two percentage
points) with these trained with real data. Mixing real and simulated data can
achieve over 95% accuracy.Comment: 10 page",,Augmented LiDAR Simulator for Autonomous Driving,http://arxiv.org/abs/1811.07112,,,core
186314890,2019-01-16T00:00:00,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings",'MDPI AG',The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,http://arxiv.org/abs/1901.05147,10.3390/make1010016,,core
387846376,2020-10-25T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or

insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and

analysis in order to improve the process representation. This paper presents the development and implementation of quality

monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the

strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework

in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality

variable, such as surface roughness. Firstly, the Hybrid Incremental Modelling strategy is applied. Secondly, a Generalized Fuzzy

Clustering C-Means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality

indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The

manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing is

performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively

fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to

manufacturing industryThis work was partially supported by the project

Power2Power: Providing next-generation silicon-based

power solutions in transport and machinery for significant

decarbonisation in the next decade, funded by the Electronic

Component Systems for European Leadership (ECSEL-JU)

Joint Undertaking and the Spanish Ministry of Science,

Innovation and Universities (MICINN), under grant

agreement No 826417. In addition, this work was also funded

by the Polish National Agency for Academic Exchange

(NAWA) through the project: “Industry 4.0 in Production and

Aeronautical Engineering (IPAE)”.Peer reviewe",'Techno-Press',Quality Monitoring of Complex Manufacturing Systems on the basis of Model Driven Approach,,10.12989/sss.2020.26.4.495,"[{'title': 'Smart Structures and Systems', 'identifiers': ['1738-1584', '1738-1991', 'issn:1738-1584', 'issn:1738-1991']}]",core
351177880,2020-01-01T00:00:00,"Internet of Things (IoT) integrates a variety of software (e.g., autonomous vehicles and military systems) in order to enable the advanced and intelligent services. These software increase the potential of cyber-attacks because an adversary can launch an attack using system vulnerabilities. Existing software vulnerability analysis methods used to be relying on human experts crafted features, which usually miss many vulnerabilities. It is important to develop an automatic vulnerability analysis system to improve the countermeasures. However, source code is not always available (e.g., most IoT related industry software are closed source). Therefore, vulnerability detection on binary code is a demanding task. This article addresses the automatic binary-level software vulnerability detection problem by proposing a deep learning-based approach. The proposed approach consists of two phases: binary function extraction, and model building. First, we extract binary functions from the cleaned binary instructions obtained by using IDA Pro. Then, we employ the attention mechanism on top of a bidirectional long short-term memory for building the predictive model. To show the effectiveness of the proposed approach, we have collected datasets from several different sources. We have compared our proposed approach with a series of baselines including source code-based techniques and binary code-based techniques. We have also applied the proposed approach to real-world IoT related software such as VLC media player and LibTIFF project that used on Autonomous Vehicles. Experimental results show that our proposed approach betters the baselines and is able to detect more vulnerabilities",'Institute of Electrical and Electronics Engineers (IEEE)',Cyber vulnerability intelligence for Internet of Things binary,,10.1109/TII.2019.2942800,,core
343504330,2020-05-01T00:00:00,"Computing performance needs in domains such as automotive,
avionics, railway, and space are on the rise. This
is fueled by the trend towards implementing an increasing
number of product functionalities in software that ends up
managing huge amounts of data and implementing complex
artificial-intelligence functionalities [1], [2].
Manycores are able to satisfy, in a cost-efficient manner, the
computing needs of embedded real-time industry [3], [4]. In
this line, building as much as possible on manycore solutions
deployed in the high-performance (mainstream) market [5],
[6], contributes to further reduce costs and increase availability.
However, commercial off the shelf (COTS) manycores bring
several challenges for their adoption in the critical embedded
market. One of those is deriving timing bounds to tasks’
execution times as part of the overall timing validation and
verification processes [7]. In particular, the network-on-chip
(NoC) has been shown to be the main resource in which
contention arises, and hence hampers deriving tight bounds
to the timing of tasks [8]",Barcelona Supercomputing Center,Computing worst-case contention delays for networks on chip,,,,core
392254025,2020-01-03T00:00:00,"Manufacturing companies require efficient maintenance practices in order to improve business performance, ensure equipment availability and reduce process downtime. With the advent of new technology, manufacturing processes are evolving from the traditional ways into digitalized manufacturing. This transformation enables systems and machines to be connected in complex networks as a collaborative community through the industrial internet of things (IIoT) and cyber-physical system (CPS). Hence, advanced maintenance strategies should be developed in order to ensure the successful implementation of Industry 4.0, which aims to transform traditional product-oriented systems into product-service systems (PSS). Today, machines and systems are expected to gain self-awareness and self-predictiveness in order to provide management with more insight on the status of the factory. In this regards, real-time monitoring along with the application of advanced machine learning algorithms based on historical data will enable systems to understand the current operating conditions, predict the remaining useful life and detect anomalies in the process. This paper discusses the necessity of predictive maintenance to achieve a sustainable and service-oriented manufacturing system and provides a methodology to be followed for implementing proactive maintenance in the context of Industry 4.0",'Springer Science and Business Media LLC',Proactive Learning for Intelligent Maintenance in Industry 4.0,,10.1007/978-981-15-2341-0_31,"[{'title': None, 'identifiers': ['issn:1876-1100', '1876-1119', 'issn:1876-1119', '1876-1100']}]",core
395677188,2020-07-17T00:00:00,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",https://core.ac.uk/download/395677188.pdf,,,core
395059242,2020-10-21T00:00:00,"A neural network based z-vertex trigger is developed for the first level trigger of the upgraded flavor physics experiment Belle II at the high luminosity B factory SuperKEKB in Tsukuba, Japan. Using the hit and drift time information from the central drift chamber, a pool of expert neural networks estimates the 3D track parameters of the single tracks found by a 2D Hough finder. The neural networks are already implemented on parallel FPGA hardware for real time data processing and running pipelined in the online first level trigger of Belle II. Due to the anticipated high luminosity of up to 8 × 10³⁵ cm⁻²s⁻¹, Belle II will have to face severe levels of background tracks with vertices displaced along the beamline. The neural z-vertex algorithm presented in this thesis allows to reject displaced background tracks such that the requirements of the standard track trigger can be strongly relaxed. Especially for physics decay channels with a low track multiplicity in the final states, like τ pair production, or initial state radiation events with reduced center of mass energies, the trigger efficiencies can be significantly increased.



As an upgrade of the present 2D Hough finder in the neural network preprocessing, a model independent 3D track finder is developed that uses the additional stereo hit information of the drift chamber. Thus, the trigger efficiencies improve for tracks in the phase space of low transverse momenta and shallow polar angles. Since the cross sections of the physics signal events typically increase towards shallow polar angles, this enlarged acceptance of the track trigger provides a substantial gain in the signal efficiencies. By using an adapted pool of expert networks, the enlarged phase space provided by the 3D finder can be efficiently covered.



Studies on simulated MC background, on simulated initial state radiation events, and on recorded data from early Belle II runs demonstrate the high performance of the novel trigger algorithms. With the 3D finder an increase of the track finding rate of about 50 % is confirmed for signal tracks, while displaced background tracks are actively suppressed prior to the neural network. Based on z-vertex cuts on the tracks processed by the neural networks, a two track event efficiency of more than 99 % can be achieved with a purity of around 80 %",Ludwig-Maximilians-Universität München,Efficient physics signal selectors for the first trigger level of the Belle II experiment based on machine learning,https://core.ac.uk/download/395059242.pdf,,,core
343451340,2020-01-01T08:00:00,"With smart city infrastructures growing, the Internet of Things (IoT) has been widely used in the intelligent transportation systems (ITS). The traditional adaptive traffic signal control method based on reinforcement learning (RL) has expanded from one intersection to multiple intersections. In this paper, we propose a multi-agent auto communication (MAAC) algorithm, which is an innovative adaptive global traffic light control method based on multi-agent reinforcement learning (MARL) and an auto communication protocol in edge computing architecture. The MAAC algorithm combines multi-agent auto communication protocol with MARL, allowing an agent to communicate the learned strategies with others for achieving global optimization in traffic signal control. In addition, we present a practicable edge computing architecture for industrial deployment on IoT, considering the limitations of the capabilities of network transmission bandwidth. We demonstrate that our algorithm outperforms other methods over 17% in experiments in a real traffic simulation environment",'Sociological Research Online',An Edge Based Multi-Agent Auto Communication Method for Traffic Light Control,https://core.ac.uk/download/343451340.pdf,,,core
334473094,2020-01-01T00:00:00,"To identify quality issues within the production and prevent defect products to be delivered to customers is critical for most manufacturing companies, and usually performed both within and at the end of each production section. In this paper we investigate the use of deep neural networks for performing automatic quality inspections based on image processing, with the aim of eliminating today’s manual inspection processes. A deep neural network is implemented on a real-world industrial case study and its performance is evaluated and analyzed when it comes to detecting quality problems in produced products. The results show that the network has an accuracy of 94.5% which is considered good in comparison to the 70-80% accuracy that a trained human inspector can achieve.CC BY-NC-ND 4.0</p",'Elsevier BV',Image Processing based on Deep Neural Networks for Detecting Quality Problems in Paper Bag Production,,10.1016/j.procir.2020.04.158,,core
339161568,2020-01-01T00:00:00,"A novel approach is proposed for constructing models of anomaly detectors using supervised learning from the traces of normal and abnormal operations of an Industrial Control System (ICS). Such detectors are of value in detecting process anomalies in complex critical infrastructure such as power generation and water treatment systems. The traces are obtained by systematically “fuzzing”, i.e., manipulating the sensor readings and actuator actions in accordance with the boundaries/partitions that define the system's state. The proposed approach is tested in a Secure Water Treatment (SWaT) testbed – a replica of a real-world water purification plant, located at the Singapore University of Technology and Design. Multiple supervised classifiers are trained using the traces obtained from SWaT. The efficacy of the proposed approach is demonstrated through empirical evaluation of the supervised classifiers under various performance metrics. Lastly, it is shown that the supervised approach results in significantly lower false positive rates as compared to the unsupervised ones.Software Engineerin",'Association for Computing Machinery (ACM)',Domain-Based Fuzzing for Supervised Learning of Anomaly Detection in Cyber-Physical Systems,,10.1145/3387940.3391486,,core
329132892,2020-09-01T00:00:00,"As Artificial Intelligent (AI) technology advances and increasingly large
amounts of data become readily available via various Industrial Internet of
Things (IIoT) projects, we evaluate the state of the art of predictive
maintenance approaches and propose our innovative framework to improve the
current practice. The paper first reviews the evolution of reliability
modelling technology in the past 90 years and discusses major technologies
developed in industry and academia. We then introduce the next generation
maintenance framework - Intelligent Maintenance, and discuss its key
components. This AI and IIoT based Intelligent Maintenance framework is
composed of (1) latest machine learning algorithms including probabilistic
reliability modelling with deep learning, (2) real-time data collection,
transfer, and storage through wireless smart sensors, (3) Big Data
technologies, (4) continuously integration and deployment of machine learning
models, (5) mobile device and AR/VR applications for fast and better
decision-making in the field. Particularly, we proposed a novel probabilistic
deep learning reliability modelling approach and demonstrate it in the Turbofan
Engine Degradation Dataset.Comment: The 3rd International Workshop on Artificial Intelligence of Things
  (AIoT) In conjunction with the 26th ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining (KDD 2020",,"Advancing from Predictive Maintenance to Intelligent Maintenance with AI
  and IIoT",http://arxiv.org/abs/2009.00351,,,core
343441821,2020-03-01T08:00:00,"Intelligent systems designed using machine learning algorithms require a large number of labeled data. Background knowledge provides complementary, real-world factual information that can augment the limited labeled data to train a machine learning algorithm. The term Knowledge Graph (KG) is in vogue as for many practical applications, it is convenient and useful to organize this background knowledge in the form of a graph. Recent academic research and implemented industrial intelligent systems have shown promising performance for machine learning algorithms that combine training data with a knowledge graph. In this article, we discuss the use of relevant KGs to enhance the input data for two applications that use machine learning-recommendation and community detection. The KG improves both accuracy and explainability",CORE Scholar,Knowledge Graph Semantic Enhancement of Input Data for Improving AI,,,,core
326517461,2020-07-07T00:00:00,"The transition to Industry 4.0 requires smart manufacturing systems that are
easily configurable and provide a high level of flexibility during
manufacturing in order to achieve mass customization or to support cloud
manufacturing. To realize this, Cyber-Physical Systems (CPSs) combined with
Artificial Intelligence (AI) methods find their way into manufacturing shop
floors. For using AI methods in the context of Industry 4.0, semantic web
services are indispensable to provide a reasonable abstraction of the
underlying manufacturing capabilities. In this paper, we present semantic web
services for AI-based research in Industry 4.0. Therefore, we developed more
than 300 semantic web services for a physical simulation factory based on Web
Ontology Language for Web Services (OWL-S) and Web Service Modeling Ontology
(WSMO) and linked them to an already existing domain ontology for intelligent
manufacturing control. Suitable for the requirements of CPS environments, our
pre- and postconditions are verified in near real-time by invoking other
semantic web services in contrast to complex reasoning within the knowledge
base. Finally, we evaluate our implementation by executing a cyber-physical
workflow composed of semantic web services using a workflow management system.Comment: Submitted to ISWC 202",'Scitepress',Using Semantic Web Services for AI-Based Research in Industry 4.0,http://arxiv.org/abs/2007.03580,10.5220/0010135900320043,,core
334510402,2020-01-01T00:00:00,"To identify quality issues within the production and prevent defect products to be delivered to customers is critical for most manufacturing companies, and usually performed both within and at the end of each production section. In this paper we investigate the use of deep neural networks for performing automatic quality inspections based on image processing, with the aim of eliminating today’s manual inspection processes. A deep neural network is implemented on a real-world industrial case study and its performance is evaluated and analyzed when it comes to detecting quality problems in produced products. The results show that the network has an accuracy of 94.5% which is considered good in comparison to the 70-80% accuracy that a trained human inspector can achieve.CC BY-NC-ND 4.0</p",'Elsevier BV',Image Processing based on Deep Neural Networks for Detecting Quality Problems in Paper Bag Production,,10.1016/j.procir.2020.04.158,,core
326652634,2020-01-01T00:00:00,"Today\u2019s factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant",'Institute of Electrical and Electronics Engineers (IEEE)',Network Synthesis for Industry 4.0,,10.23919/DATE48585.2020.9116407,,core
233572457,2020-01-01T00:00:00,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35,000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets",'Institute of Electrical and Electronics Engineers (IEEE)',Semiautomatic Labeling for Deep Learning in Robotics,,10.1109/TASE.2019.2938316,,core
346583052,2020-01-01T00:00:00,"Climate econometrics is a new sub-discipline that has grown rapidly over the last few years. As greenhouse gas emissions like carbon dioxide (CO2), nitrous oxide (N2O) and methane (CH4) are a major cause of climate change, and are generated by human activity, it is not surprising that the tool set designed to empirically investigate economic outcomes should be applicable to studying many empirical aspects of climate change.

Economic and climate time series exhibit many commonalities. Both data are subject to non-stationarities in the form of evolving stochastic trends and sudden distributional shifts. Consequently, the well-developed machinery for modeling economic time series can be fruitfully applied to climate data. In both disciplines, we have imperfect and incomplete knowledge of the processes actually generating the data. As we don’t know that data generating process (DGP), we must search for what we hope is a close approximation to it. The data modeling approach adopted at Climate Econometrics (http://www.climateeconometrics.org/) is based on a model selection methodology that has excellent properties for locating an unknown DGP nested within a large set of possible explanations, including dynamics, outliers, shifts, and non-linearities. The software we use is a variant of machine learning which implements multi-path block searches commencing from very general specifications to discover a well-specified and undominated model of the processes under analysis. To do so requires implementing indicator saturation estimators designed to match the problem faced, such as impulse indicators for outliers, step indicators for location shifts, trend indicators for trend breaks, multiplicative indicators for parameter changes, and indicators specifically designed for more complex phenomena that have a common reaction ‘shape’ like the impacts of volcanic eruptions on temperature reconstructions. We also use combinations of these, inevitably entailing settings with more candidate variables than observations.

Having described these econometric tools, we take a brief excursion into climate science to provide the background to the later applications. By noting the Earth’s available atmosphere and water resources, we establish that humanity really can alter the climate, and is doing so in myriad ways. Then we relate past climate changes to the ‘great extinctions’ seen in the geological record. Following the Industrial Revolution in the mid-18th century, building on earlier advances in scientific, technological and medical knowledge, real income levels per capita have risen dramatically globally, many killer diseases have been tamed, and human longevity has approximately doubled. However, such beneficial developments have led to a global explosion in anthropogenic emissions of greenhouse gases. These are also subject to many relatively sudden shifts from major wars, crises, resource discoveries, technology and policy interventions. Consequently, stochastic trends, large shifts and numerous outliers must all be handled in practice to develop viable empirical models of climate phenomena. Additional advantages of our econometric methods for doing so are detecting the impacts of important policy interventions as well as improved forecasts. The econometric approach we outline can handle all these jointly, which is essential to accurately characterize non-stationary observational data. Few approaches in either climate or economic modeling consider all such effects jointly, but a failure to do so leads to mis-specified models and hence incorrect theory evaluation and policy analyses. We discuss the hazards of modeling wide-sense non-stationary data (namely data not just with stochastic trends but also distributional shifts), which also serves to describe our notation.

The application of the methods is illustrated by two detailed modeling exercises. The first investigates the causal role of CO2 in Ice Ages, where a simultaneous-equations system is developed to characterize land ice volume, temperature and atmospheric CO2 levels as non-linear functions of measures of the Earth’s orbital path round the Sun. The second turns to analyze the United Kingdom’s highly non-stationary annual CO2 emissions over the last 150 years, walking through all the key modeling stages. As the first country into the Industrial Revolution, the UK is one of the first countries out, with per capita annual CO2 emissions now below 1860’s levels when our data series begin, a reduction achieved with little aggregate cost. However, very large decreases in all greenhouse gas emissions are still required to meet the UK’s 2050 target set by its Climate Change Act in 2008 of an 80% reduction from 1970 levels, since reduced to a net zero target by that date, as required globally to stabilize temperatures. The rapidly decreasing costs of renewable energy technologies offer hope of further rapid emission reductions in that area, illustrated by a dynamic scenario analysis",'Now Publishers',Climate econometrics: An overview,,,,core
327017697,2020-12-15T00:00:00,"In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem",,Cost-sensitive learning classification strategy for predicting product failures,,,,core
305109842,2020-04-22T23:30:07,"Recent advancement in predictive machine learning has led to its application in various use cases in manufacturing. Most research focused on maximising predictive accuracy without addressing the uncertainty associated with it. While accuracy is important, focusing primarily on it poses an overfitting danger, exposing manufacturers to risk, ultimately hindering the adoption of these techniques. In this paper, we determine the sources of uncertainty in machine learning and establish the success criteria of a machine learning system to function well under uncertainty in a cyber-physical manufacturing system (CPMS) scenario. Then, we propose a multi-agent system architecture which leverages probabilistic machine learning as a means of achieving such criteria. We propose possible scenarios for which our architecture is useful and discuss future work. Experimentally, we implement Bayesian Neural Networks for multi-tasks classification on a public dataset for the real-time condition monitoring of a hydraulic system and demonstrate the usefulness of the system by evaluating the probability of a prediction being accurate given its uncertainty. We deploy these models using our proposed agent-based framework and integrate web visualisation to demonstrate its real-time feasibility",'Organisation for Economic Co-Operation and Development  (OECD)',Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System,https://core.ac.uk/download/305109842.pdf,10.17863/CAM.51696,,core
337623373,2020-01-01T00:00:00,"Software-in-the-Loop simulation has become an inevitable part of testing in the process of autonomous vehicle development. Simulation enables to test the system safely, allowing for further
development of the AI system. It solves real-world problems safely and efficiently.

This project develops an open-sourced Software-in-the-Loop (SIL) modelling, validation and assessment, that enables the testing of racing autonomous vehicles in a controlled environment. It contributes with an open-source solution to a widely extended necessity on the automotive industry of simulation testing. The simulator is developed with Gazebo and ROS, for real-time dynamic
simulation of the autonomous vehicle and associated sensors.

A performance evaluation tool has been developed with Matlab and Python, enabling the critical
analysis and validation on vehicle and sensor behaviour. Performance metrics enhances the quantitative analysis, its tuning and the optimisation of the autonomous control system algorithms. Sensors are validated with available experimental data, with a selection of appropriate noise models.

The stereo camera sensor is validated in different lighting and weather conditions. The effect
of lighting is quantified from experimental tests. With the analysis of different weather conditions,
it has been demonstrated the need of a Gaussian noise model to mimic sensor accuracy in the conditions of shadowing and raining. GPS and IMU sensors are validated with different kinds of noise, its modelling is developed with Matlab. Vehicles are modelled following provider specifications, while the FS electric vehicle OBR20 is also validated with available data from FS Austria 2018.

Results are favourable when comparing simulation against experimental data. A design study demonstrates the abilities of the SIL to assist in control system tuning - resulting in just 8% lap-time difference from the optimum racing path, with an average velocity of 33.2km/h, reaching a maximum velocity of 40km/h on the AutoCross event - indicating a great performance and the value of using simulation for the optimisation of multiple parameters on the autonomous control system.

The accurate, certain and reliable SIL, allows testing and development of new concepts on the
Autonomous vehicle system in an Open-Source and safely environment. Furthermore, the author
suggests new experimental tests and the collection of data in order to be able to model more scenarios that have an important effect on the sensors performance",'Oxford Brookes University',"Autonomous Software-in-the-Loop Modelling, Validation and Assessment",,,,core
304332463,2020-01-01T00:00:00,"This volume, entitled Intelligent Human Systems Integration 2020, aims to provide a global forum for introducing and discussing novel approaches, design tools, methodologies, techniques, and solutions for integrating people with intelligent technologies, automation, and artificial cognitive systems in all areas of human endeavor in industry, economy, government, and education. Some of the notable areas of application include, but are not limited to, energy, transportation, urbanization and infrastructure development, digital manufacturing, social development, human health, sustainability, a new generation of service systems, as well as developments in safety, risk assurance, and cybersecurity in both civilian and military contexts. Indeed, rapid progress in developments in the ambient intelligence, including cognitive computing, modeling, and simulation, as well as smart sensor technology, weaves together the human and artificial intelligence and will have a profound effect on the nature of their collaboration at both the individual and societal levels in the near future. As applications of artificial intelligence and cognitive computing become more prevalent in our daily lives, they also bring new social and economic challenges and opportunities that must be addressed at all levels of contemporary society. Many of the traditional human jobs that require high levels of physical or cognitive abilities, including human motor skills, reasoning, and decision-making abilities, as well as training capacity, are now being automated. While such trends might boost economic efficiency, they can also negatively impact the user experience and bring about many unintended social consequences and ethical concerns. The intelligent human systems integration is, to a large extent, affected by the forces shaping the nature of future computing and artificial system development. This book discusses the needs and requirements for the symbiotic collaboration between humans and artificially intelligent systems, with due consideration of the software and hardware characteristics allowing for such cooperation from the societal and human-centered design perspectives, with the focus on the design of intelligent products, systems, and services that will revolutionize future human\u2013 technology interactions. This book also presents many innovative studies of ambient artificial technology and its applications, including the human\u2013machine interfaces with a particular emphasis on infusing intelligence into the development of technology throughout the lifecycle development process, with due consideration of user experience and the design of interfaces for virtual, augmented, and mixed reality applications of artificial intelligence. Reflecting on the above-outlined perspective, the papers contained in this volume are organized into seven main sections, including:
1. Automotive design and transportation engineering
2. Humans and artificial cognitive systems
3. Intelligence, technology, and analytics
4. Computational modeling and simulation
5. Humans and artificial systems complexity
6. Materials and inclusive human systems
7. Human\u2013autonomy teaming
8. Applications and future trends
We would like to extend our sincere thanks to Axel Schulte, Stefania Campione,and Marinella Ferrara, for leading a part of the technical program that focuses on human\u2013autonomy teaming and smart materials and inclusive human systems. Our appreciation also goes to the members of Scientific Program Advisory Board who have reviewed the accepted papers that are presented in this volume, including the following individuals:
D. B\u103il\u103, Romania
H. Blaschke, Germany
S. Campione, Italy
J. Chen, USA
G. Coppin, France
M. Draper, USA
A. Ebert, Germany
M. Ferrara, Italy
M. Hou, Canada
M. Jipp, Germany
E. Karana, The Netherlands
A. Kluge, Germany
D. Lange, USA
S. Lucibello, Italy
E. Macioszek, Poland
M. Neerincx, The Netherlands
R. Philipsen, Germany
J. Platts, UK
D. Popov, USA
A. Ratti, Italy
R. Rodriquez, Italy
V. Rognoli, Italy
U. Schmid, Germany
vi Preface
A. Schulte, Germany
N. Stanton, UK
E. Suhir, USA
We hope that this book, which presents the current state of the art in intelligent human systems integration, will be a valuable source of both theoretical and applied knowledge, enabling the design and applications of a variety of intelligent products, services, and systems for their safe, effective, and pleasurable collaboration with people",'Springer Science and Business Media LLC',"Intelligent Human Systems Integration 2020, Proceedings of the 3rd International Conference on Intelligent Human Systems Integration (IHSI 2020): Integrating People and Intelligent Systems",,10.1007/978-3-030-39512-4,,core
287752697,2020-02-21T01:24:11,"Advanced technologies based on Internet of Things (IOT) are blazing a trail to effective and efficient management of an overall plant. In this context, manufacturing companies require an innovative strategy to survive in a competitive business environment, utilizing those technologies. Guided by these requirements, the so-called predictive maintenance is of paramount importance and offers a significant potential for innovation to overcome the limitations of traditional maintenance policies. However, real shop-floors often have obstacles in providing insights to facilitate the effective management of assets in smart factories. Even if a significant amount of machine and process data is available, one of the common problems of these data is the lack of annotations describing the machine status or maintenance history. For this reason, companies have limited options to analyse manufacturing data, despite the capability of advanced machine learning techniques in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations. Moreover, each machine generates highly heterogeneous data, making it difficult to integrate all the information to provide data-driven decision support for predictive maintenance. Inspired by these challenges, this research provides a hybrid machine learning approach combining unsupervised learning and semi-supervised learning. The approach and result in this article are based on the development and implementation in a large collaborative EU-funded H2020 research project entitled BOOST 4.0 i.e. Big Data Value Spaces for COmpetitiveness of European COnnected Smart FacTories",'Springer Science and Business Media LLC',A Hybrid Machine Learning Approach for Predictive Maintenance in Smart Factories of the Future,,10.1007/978-3-319-99707-0_39,,core
329129529,2020-08-25T00:00:00,"This paper provides an economic perspective on the predictive maintenance of
filtration units. The rise of predictive maintenance is possible due to the
growing trend of industry 4.0 and the availability of inexpensive sensors.
However, the adoption rate for predictive maintenance by companies remains low.
The majority of companies are sticking to corrective and preventive
maintenance. This is not due to a lack of information on the technical
implementation of predictive maintenance, with an abundance of research papers
on state-of-the-art machine learning algorithms that can be used effectively.
The main issue is that most upper management has not yet been fully convinced
of the idea of predictive maintenance. The economic value of the implementation
has to be linked to the predictive maintenance program for better justification
by the management. In this study, three machine learning models were trained to
demonstrate the economic value of predictive maintenance. Data was collected
from a testbed located at the Singapore University of Technology and Design.
The testbed closely resembles a real-world water treatment plant. A
cost-benefit analysis coupled with Monte Carlo simulation was proposed. It
provided a structured approach to document potential costs and savings by
implementing a predictive maintenance program. The simulation incorporated
real-world risk into a financial model. Financial figures were adapted from
CITIC Envirotech Ltd, a leading membrane-based integrated environmental
solutions provider. Two scenarios were used to elaborate on the economic values
of predictive maintenance. Overall, this study seeks to bridge the gap between
technical and business domains of predictive maintenance.Comment: 7 pages, 3 tables, 1 figur",,An Economic Perspective on Predictive Maintenance of Filtration Units,http://arxiv.org/abs/2008.11070,,,core
322557627,2020-01-01T08:00:00Z,"Numerical simulation and data-driven modeling are two current approaches in engineering reservoir modeling. Numerical reservoir simulation attempts to match past production history by modifying reservoir properties of the model. After multiple computationally intensive trial and error efforts, accurate history matches are identified. These history matches are used by project management for production forecasting purposes. Data-driven reservoir modeling utilizes measured data and is, therefore, free of assumptions that are often included in numerical reservoir simulations. Artificial intelligence and machine learning algorithms are technologies implemented in the development of a data-driven reservoir model with efforts to learn fluid flow through porous media from the datasets provided to the system. Training, calibration, and validation datasets ensure the success during the teaching process.
Models, such as oil, gas, and water production, reservoir pressure and water saturation, are trained, calibrated, verified to ensure the success in the teaching process. After appropriate hyper-parameter tuning, well-trained models are tested on blind datasets. This leads to the model being deployed on new datasets to again test the model’s performance during forecasting. The accuracy is based upon the model achieving a similar result to what numerical simulation found on the same dataset.
The objective of this thesis is to use a 22-year dataset from a reservoir model generated by a numerical simulator that undergoes many complexities to approach the reality that takes place in the industry through the use of operational constraints, workover events requiring shut-in, random bottom hole pressure (“BHP”) trend, etc. Should the Top Down Model (“TDM”) be able to accurately learn the relationships between input attributes and outputs from the data, a similar procedure can then be applied to real field data in the future. The TDM’s capabilities will attempt to be proved when the blind validation’s in-time results match the data from the numerical simulation reservoir model. This will be done by excluding the data from the training, calibration, and validation datasets used to create the TDM. This thesis’s results should demonstrate that data-driven modeling is capable of history matching and forecasting the complexities of a reservoir",The Research Repository @ WVU,Top-Down Model Development Using Data Generated from a Complex Numerical Reservoir Simulation with Water Injection,,,,core
322830734,2020-05-09T00:00:00,"Internet-of-Things (IoT) is omnipresent, ranging from home solutions to
turning wheels for the fourth industrial revolution. This article presents the
novel concept of Internet-of-Agro-Things (IoAT) with an example of automated
plant disease prediction. It consists of solar enabled sensor nodes which help
in continuous sensing and automating agriculture. The existing solutions have
implemented a battery powered sensor node. On the contrary, the proposed system
has adopted the use of an energy efficient way of powering using solar energy.
It is observed that around 80% of the crops are attacked with microbial
diseases in traditional agriculture. To prevent this, a health maintenance
system is integrated with the sensor node, which captures the image of the crop
and performs an analysis with the trained Convolutional Neural Network (CNN)
model. The deployment of the proposed system is demonstrated in a real-time
environment using a microcontroller, solar sensor nodes with a camera module,
and an mobile application for the farmers visualization of the farms. The
deployed prototype was deployed for two months and has achieved a robust
performance by sustaining in varied weather conditions and continued to remain
rust-free. The proposed deep learning framework for plant disease prediction
has achieved an accuracy of 99.2% testing accuracy.Comment: 23 pages, 14 figure",,"sCrop: A Internet-of-Agro-Things (IoAT) Enabled Solar Powered Smart
  Device for Automatic Plant Disease Prediction",http://arxiv.org/abs/2005.06342,,,core
388593998,2020-01-01T00:00:00,"This work aimed to develop pressure and temperature sensor based on piezo-resistive and thermoelectric phenomena, using porous polyurethane and melamine foams impregnated with a solution of Poly (3,4-ethylenedioxythiophene)-poly(styrenesulfonate) (PEDOT:PSS). This device allows the simultaneous measurement of temperature and pressure through the transduction of external stimuli into separate electrical signals, with a promising resolution of temperature and pressure detection for artificial intelligence and health application systems. It was possible to manufacture individual sensors, which achieved a maximum performance of 90% of the resistance variation, with the ability to detect, from 3 to 30 kPa, between a range of 10 and 50% of deformation, respectively. These devices generated an average thermal voltage of 2.8 mV under elevated temperature stimulus. Considering the manufacturing parameters and the performance obtained for individual sensors, a 4x4 matrix was manufactured, using an innovative approach, not yet reported in the literature. A single piece of melamine impregnated with diluted PEDOT: PSS was used to construct the device. The sensor was integrated with the Arduino microcontroller and an interface was implemented with visual interface software for real-time monitoring of pressure position and intensity. The 4x4 matrix sensor device showed promising results, with the need, however, to improve the contacts between the electrodes and the sponge, since they presented degradation with intensive use.Este trabalho teve como objetivo desenvolver um sensor de pressão e temperatura baseado em fenômenos piezo-resistivo e termoelétrico, usando espuma de poliuretana porosa e melamina impregnadas em solução de Poly(3,4-ethylenedioxythiophene)-poly(styrenesulfonate) (PEDOT:PSS). Este dispositivo permite a medida simultânea de temperatura e pressão através da transdução de estímulos externos em sinais elétricos separados, com resolução promissora de detecção de temperatura e pressão para inteligência artificial e sistemas de aplicação em saúde. Foi possível fabricar sensores individuais, que atingiram um desempenho máximo de 90% da variação da resistência, com capacidade de detectar, de 3 a 30 kPa, entre uma faixa de 10 e 50% de deformação, respectivamente. Esses dispositivos geraram uma tensão térmica média de 2,8 mV sob estímulo de alta temperatura. Levando em consideração os parâmetros de fabricação e a performance obtida para sensores individuais, foi fabricada uma matriz 4x4, utilizando uma abordagem nova, ainda não relatada na literatura. Utilizou-se uma peça única de melamina impregnada em PEDOT:PSS diluído para a construção do dispositivo. O sensor foi integrado ao microcontrolador Arduino e uma interface foi implementada com software de interface visual para monitoramento em tempo real de posição e intensidade de pressão. O dispositivo sensor matricial 4x4 apresentou resultados promissores, havendo a necessidade, contudo, de se aprimorar os contatos entre os eletrodos e a esponja, visto que estes apresentavam degradação com o uso intensivo",,Flexible temperature pressure organic sensor,https://core.ac.uk/download/388593998.pdf,,,core
336858426,2020-08-03T15:27:37,"Artificial Intelligence (AI) is considered one of the most advanced sciences which currently utilized by the designers to improve human life through their novelties. In recent decades, many researchers have been focused on studying the impact of information technology and software implementation in product design processes. Moreover, the technology of art+9ificial intelligence and the designer responsibility in the application of this technology have been illustrated. The drought of Arab Library\u27s from such researches encourages researchers to explore the impact of machine learning concept on the future of industrial design. In this work, a brief summary of artificial intelligence and machine learning concepts are presented. On the other hand, their relationship with the design’s future is discussed. Survey and questionnaire methods are exploited to collect a set of expectations for future life and product shapes as well as the anticipation of the role of the designer and the shape of the process of learning and teaching design. Finally, the expected future risks that may be resulted from the immersion of machine learning science in the individual life is investigated. Research Problem: The problem of research lies in the scarcity researches in the field of the impact of machine learning concept on the industrial design future. As the principle of standardization completely disappears in design and the principle of customization diminish, products personalization will be take place. Consequently, the style and the way of designing and manufacturing products will be affected. No doubt that, user experience is absolutely essential factor to guarantee brand survival, which makes artificial intelligence the main engine for personalizing the products of industrial design. Research Objectives  Analysis the impact of the machine learning concept on the design future in light of the technology of artificial intelligence and machine learning as well as its role in the full shift towards the design of personalized products. Artificial intelligence makes the product a partner in decision-making and not only to interact with the user as in interactive products in its contemporary image. Research Methodology : Inductive approach and Deductive approach,  qualitative approach and the quantitative approach,  Major Results: Artificial intelligence and machine learning help to predict user behavior and design useful insights to improve user experience.  Machine learning is the backbone of UX design because it is a very powerful source of real information which is a fruitful addition to the design of smarter products. Interaction with users is at the heart of the development cycle.  It is expected that in the next 10 years’ machine learning will become the norm responsible for the data used by the designers of the user\u27s products and user experiences, ultimately making the end-user experience better",Arab Journals Platform,The future of industrial design in view of “machine learning,https://core.ac.uk/download/336858426.pdf,,,core
479018889,2020-01-01T00:00:00,"Accurate building energy prediction is vital to develop optimal control strategies to enhance building energy efficiency and energy flexibility. In recent years, the data-driven approach based on machine learning algorithms has been widely adopted for building energy prediction due to the availability of massive data in building automation systems (BASs), which automatically collect and store real-time building operational data. For new buildings and most existing buildings without installing advanced BASs, there is a lack of sufficient data to train data-driven predictive models. Transfer learning is a promising method to develop accurate and reliable data-driven building energy prediction models with limited training data by taking advantage of the rich data/knowledge obtained from other buildings. Few studies focused on the influences of source building datasets, pre-training data volume, and training data volume on the performance of the transfer learning method. The present study aims to develop a transfer learning-based ANN model for one-hour ahead building energy prediction to fill this research gap. Around 400 non-residential buildings’ data from the open-source Building Genome Project are used to test the proposed method. Extensive analysis demonstrates that transfer learning can effectively improve the accuracy of BPNN-based building energy models for information-poor buildings with very limited training data. The most influential building features which influence the effectiveness of transfer learning are found to be building usage and industry. The research outcomes can provide guidance for implementation of transfer learning, especially in selecting appropriate source buildings and datasets for developing accurate building energy prediction models",'Springer Fachmedien Wiesbaden GmbH',Development of an ANN-based building energy model for information-poor buildings using transfer learning,,,,core
346576996,2020-01-01T00:00:00,"Digital technologies have changed the way supply chain operations are structured. In this article, we conduct systematic syntheses of literature on the impact of new technologies on supply chains and the related cyber risks. A taxonomic/cladistic approach is used for the evaluations of progress in the area of supply chain integration in the Industrial Internet of Things and Industry 4.0, with a specific focus on the mitigation of cyber risks. An analytical framework is presented, based on a critical assessment with respect to issues related to new types of cyber risk and the integration of supply chains with new technologies. This paper identifies a dynamic and self-adapting supply chain system supported with Artificial Intelligence and Machine Learning (AI/ML) and real-time intelligence for predictive cyber risk analytics. The system is integrated into a cognition engine that enables predictive cyber risk analytics with real-time intelligence from IoT networks at the edge. This enhances capacities and assist in the creation of a comprehensive understanding of the opportunities and threats that arise when edge computing nodes are deployed, and when AI/ML technologies are migrated to the periphery of IoT networks",Springer Open,Cyber risk at the edge: current and future trends on cyber risk analytics and artificial intelligence in the industrial internet of things and industry 4.0 supply chains,,,,core
322449666,2020-04-30T00:00:00,"In this white paper we provide a vision for 6G Edge Intelligence. Moving
towards 5G and beyond the future 6G networks, intelligent solutions utilizing
data-driven machine learning and artificial intelligence become crucial for
several real-world applications including but not limited to, more efficient
manufacturing, novel personal smart device environments and experiences, urban
computing and autonomous traffic settings. We present edge computing along with
other 6G enablers as a key component to establish the future 2030 intelligent
Internet technologies as shown in this series of 6G White Papers.
  In this white paper, we focus in the domains of edge computing infrastructure
and platforms, data and edge network management, software development for edge,
and real-time and distributed training of ML/AI algorithms, along with
security, privacy, pricing, and end-user aspects. We discuss the key enablers
and challenges and identify the key research questions for the development of
the Intelligent Edge services. As a main outcome of this white paper, we
envision a transition from Internet of Things to Intelligent Internet of
Intelligent Things and provide a roadmap for development of 6G Intelligent
Edge",,6G White Paper on Edge Intelligence,http://arxiv.org/abs/2004.14850,,,core
304108902,2020-03-24T08:44:12,"The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC",'Elsevier BV',Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT,,10.1016/j.comcom.2020.01.018,,core
333580897,2020-01-01T00:00:00,"In the paper, application of Machine Learning (ML) techniques for the continuous monitoring of the health status of mild mission-critical industrial equipment is considered. A meaningful real-life case-study is presented in order to show how acquisition conditions may severely impact on the performance of the system. In particular, it is shown that a wrong estimate of noise effects in the deployed system may induce a wrong choice of the best features feeding the ML monitoring algorithm, hence affecting accuracy of the target devices. The discussed results may provide an useful guidance to the practitioner in the field during the design phase of ML-based devices depending of the equipment specifications and environmental conditions",'Institute of Electrical and Electronics Engineers (IEEE)',Impact of Noise on Machine Learning-based Condition Monitoring Applications: a Case Study,,10.1109/i2mtc43012.2020.9129119,,core
329131898,2020-08-30T00:00:00,"In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.Comment: 27 pages, 10 Figure",,"Deep Reinforcement Learning for Contact-Rich Skills Using Compliant
  Movement Primitives",http://arxiv.org/abs/2008.13223,,,core
334905816,2020-01-23T00:00:00,"Machine learning algorithms, when applied to sensitive data, pose a potential
threat to privacy. A growing body of prior work has demonstrated that
membership inference attack (MIA) can disclose specific private information in
the training data to an attacker. Meanwhile, the algorithmic fairness of
machine learning has increasingly caught attention from both academia and
industry. Algorithmic fairness ensures that the machine learning models do not
discriminate a particular demographic group of individuals (e.g., black and
female people). Given that MIA is indeed a learning model, it raises a serious
concern if MIA ``fairly'' treats all groups of individuals equally. In other
words, whether a particular group is more vulnerable against MIA than the other
groups. This paper examines the algorithmic fairness issue in the context of
MIA and its defenses. First, for fairness evaluation, it formalizes the
notation of vulnerability disparity (VD) to quantify the difference of MIA
treatment on different demographic groups. Second, it evaluates VD on four
real-world datasets, and shows that VD indeed exists in these datasets. Third,
it examines the impacts of differential privacy, as a defense mechanism of MIA,
on VD. The results show that although DP brings significant change on VD, it
cannot eliminate VD completely. Therefore, fourth, it designs a new mitigation
algorithm named FAIRPICK to reduce VD. An extensive set of experimental results
demonstrate that FAIRPICK can effectively reduce VD for both with and without
the DP deployment.Comment: Privacy and Security, Member inference attack, 14 pages, 64 figure",,"Privacy for All: Demystify Vulnerability Disparity of Differential
  Privacy against Membership Inference Attack",http://arxiv.org/abs/2001.08855,,,core
324184067,2020-01-01T00:00:00,"Software verification approaches aim to check a software component under analysis for all possible environments. In reality, however, components are expected to operate within a larger system and are required to satisfy their requirements only when their inputs are constrained by environment assumptions. In this paper, we propose EPIcuRus, an approach to automatically synthesize environment assumptions for a component under analysis (i.e., conditions on the component inputs under which the component is guaranteed to satisfy its requirements). EPIcuRus combines search-based testing, machine learning and model checking. The core of EPIcuRus is a decision tree algorithm that infers environment assumptions from a set of test results including test cases and their verdicts. The test cases are generated using search-based testing, and the assumptions inferred by decision trees are validated through model checking. In order to improve the efficiency and effectiveness of the assumption generation process, we propose a novel test case generation technique, namely Important Features Boundary Test (IFBT), that guides the test generation based on the feedback produced by machine learning. We evaluated EPIcuRus by assessing its effectiveness in computing assumptions on a set of study subjects that include 18 requirements of four industrial models. We show that, for each of the 18 requirements, EPIcuRus was able to compute an assumption to ensure the satisfaction of that requirement, and further, ≈78% of these assumptions were computed in one hour",,Mining Assumptions for Software Components using Machine Learning,,,,core
327074708,2020-07-23T00:00:00,"How can we find the right graph for semi-supervised learning? In real world
applications, the choice of which edges to use for computation is the first
step in any graph learning process. Interestingly, there are often many types
of similarity available to choose as the edges between nodes, and the choice of
edges can drastically affect the performance of downstream semi-supervised
learning systems. However, despite the importance of graph design, most of the
literature assumes that the graph is static. In this work, we present Grale, a
scalable method we have developed to address the problem of graph design for
graphs with billions of nodes. Grale operates by fusing together different
measures of(potentially weak) similarity to create a graph which exhibits high
task-specific homophily between its nodes. Grale is designed for running on
large datasets. We have deployed Grale in more than 20 different industrial
settings at Google, including datasets which have tens of billions of nodes,
and hundreds of trillions of potential edges to score. By employing locality
sensitive hashing techniques,we greatly reduce the number of pairs that need to
be scored, allowing us to learn a task specific model and build the associated
nearest neighbor graph for such datasets in hours, rather than the days or even
weeks that might be required otherwise. We illustrate this through a case study
where we examine the application of Grale to an abuse classification problem on
YouTube with hundreds of million of items. In this application, we find that
Grale detects a large number of malicious actors on top of hard-coded rules and
content classifiers, increasing the total recall by 89% over those approaches
alone.Comment: 10 pages, 6 figures, to be published in KDD'2",'Association for Computing Machinery (ACM)',Grale: Designing Networks for Graph Learning,http://arxiv.org/abs/2007.12002,10.1145/3394486.3403302,,core
345057927,2020-10-01T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and analysis in order to improve the process representation. This paper presents the development and implementation of quality monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality variable, such as surface roughness. Firstly, the Hybrid Incremental Modelling strategy is applied. Secondly, a Generalized Fuzzy Clustering C-Means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing is performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to manufacturing industry",'Techno-Press',Quality Monitoring of Complex Manufacturing Systems on the basis of Model Driven Approach,,10.12989/sss.2020.26.4.495,,core
363990747,2020-01-01T00:00:00,"We live in the era of big data in which the advancement of sensor and monitoring technologies, data storage and management, and computer processing power enable us to acquire, store and process over 2.5 Quintilian bytes of data daily. This massive data brings the necessity of using trustable and high-performance data-driven models that extract knowledge out of data. This dissertation focuses on learning to solve highly risk-averse and complex sequential decision-making problems from retrospective data sets by deep Reinforcement Learning (RL).Deep RL has gained remarkable breakthroughs in many applications. It achieved superhuman performance in video and Atari games, defeated the world champion in game of Go, gained competent autonomy in simulated self-driving cars, and successfully learned to perform some robotic tasks. Despite all the notable advancements in deep RL, its application to real-world problems such as clinical treatment policy or industrial asset maintenance management is insignificant. Studies are underway to investigate deep RL use in realistic problems; however, none has been deployed in real-world settings. Several limitations hinder the deep RL application to real-world problems, among which trustability and excessive thirst for data are the main issues. This research is an effort to smooth the way of applying deep RL to real-world problems by addressing the above two limitations.  We first provide a concrete definition for trust in RL algorithms, and then we propose a sample efficient deep RL agent that computes a trustable solution to real-world sequential decision-making problems. The agent tackles the trust problem from two aspects. It imposes risk barriers to the RL agent's policy improvement process and provides off-policy performance estimation with a confidence bound prior to putting the agent in interaction with the actual system or environment. We address the RL significant demand for data by implementing the most advanced efficient data utilization techniques as well as deploying new techniques that improve the Trustable Deep RL sample efficiency.The proposed methodology is tested and evaluated on a novel pipeline corrosion maintenance test bench that mimics the real system restrictions. The results witness that the Trustable Deep RL algorithm efficiently digests a retrospective data set from the pipeline environment and gains a superior and trustable interaction policy","eScholarship, University of California",Trustable Deep Reinforcement Learning with Efficient Data Utilization,,,,core
287844227,2020-02-07T10:21:57,"Intelligent robotic systems are becoming essential for space applications, industries, nuclear plants and for harsh environments in general, such as the European Organization for Nuclear Research (CERN) particles accelerator complex and experiments. Robotics technology has huge potential benefits for people and its ultimate scope depends on the way this technology is  used. In order to increase safety and machine availability, robots can perform repetitive, unplanned and dangerous tasks, which humans either prefer to avoid or are unable to carry out due to hazards, size constraints, or the extreme environments in which they take place. Nowadays, mechatronic systems use mature technologies that allow their robust and safe use, even in collaboration with human workers. Over the past years, the progress of robots has been based on the development of smart sensors, artificial intelligence and modular mechanical systems. Due to the multiple challenges that hazardous and unstructured environments have for the application of autonomous industrial systems, there is still a high demand for intelligent and teleoperation systems that give the control of a robot (slave) to a human operator via haptic input devices (master), as well as using human-supervised telerobotic control techniques. Modern techniques like simulation and virtual reality systems can facilitate the preparation of ad-hoc mechatronic tools and robotic intervention including recovery scenarios and failure mode analysis.  The basic contribution of this thesis is the development of a novel robotic framework for autonomous inspections and supervised teleoperations in harsh environments. The proposed framework covers all aspects of a robotic intervention, from the specification and operator training, the choice of the robot and its material in accordance with possible radiological contamination risks, to the realization of the intervention, including procedures and recovery scenarios. In a second set of contributions, new methods for mutirobots maintenance operations are developed, including intervention preparation and best practices for remote handling and advanced tools. The third set of contributions is built on a novel multimodal user-friendly human-robot interface that allows  operator training using virtual reality systems and technicians not expert in robot operation to perform inspection/maintenance tasks. In this thesis, we exploit a robotic system able to navigate autonomously and to inspect unknown environments in a safe way. A new real-time control system has been implemented in order to guarantee a fast response to environmental changes and adaptation to different type of scenarios the robot may find in a semi-structured and hazardous environment. The proposed new robotic control system  has been integrated on different robots, tested and validated with several robotic interventions in the CERN hazardous particle accelerator complex",,A Novel Robotic Framework for Safe Inspection and Telemanipulation in Hazardous and Unstructured Environments,,,,core
344911413,2020-06-30T00:00:00,"Abstract

In this white paper, we provide a vision for 6G edge intelligence. Moving toward 5G and beyond future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence will become crucial for several real-world applications, including but not limited to more efficient manufacturing, novel personal smart device environments and experiences, urban computing, and autonomous traffic settings. We sent edge computing with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies shown in this series of 6G white papers.

In this white paper, we focus on the domains of edge-computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, as well as security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges, and identify the key research questions for the development of intelligent edge services. As the main outcome of this white paper, we envision a transition from the Internet of Things to the Intelligent Internet of Intelligent Things and provide a roadmap for the development of the 6G intelligent edge",Oulun yliopisto,6G white paper on edge intelligence,,,,core
344912383,2020-01-01T00:00:00,"Abstract

Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges",'Institute of Electrical and Electronics Engineers (IEEE)',A machine learning security framework for IoT systems,,,,core
346141431,2020-01-01T08:00:00,"Personal injuries and property damage due to the failure of snow-plowable pavement markers which detach from pavement surfaces has led to the development of new all-plastic pavement markers which are located entirely below the planar surface of the pavement. The new all-plastic design pushes existing solutions used to avoid striping over highway reflectors into obsolescence since current solutions operate using electromagnets to sense the metal housings of snow-plowable pavement markers. A replacement solution is currently sought by the highway maintenance industry and three different marker detection methods were developed and tested on real-world highways with both new and aging pavement markers to find that optimal solution. With the developed technologies accruing 106,038 observed data points, it is clear that the ideal solution to marker detection and avoidance is the deployment of a machine vision system operating on a deep learning trained model optimized for the detection of differing types of pavement markers on various pavement surfaces. The machine vision system can be further improved in several areas, the most important of which is the optimization model’s processing speeds such that the system could operate at highway speeds while providing real-time analysis of the integrity of installed pavement markers",UKnowledge,DESIGN AND ANALYSIS OF A PAVEMENT MARKER DETECTION SYSTEM,https://core.ac.uk/download/346141431.pdf,,,core
342534133,2020-10-08T00:00:00,"Machine Learning (ML) and Artificial Neural Networks (ANN) are used in many areas of industry and science today. In the course of digitalization, these technologies will become even more relevant and open up new areas for themselves. This will probably also increase the need for effective learning methods and alternative forms of representation to achieve a higher degree of understanding of algorithms and processes within KNN. A complementary form of representation or learning environment could be for example Virtual Reality (VR), as the technology offers new possibilities and perspectives regarding representation and interaction. This thesis investigates to what extent the use of a VR application for exploration of KNN could contribute to the knowledge gain of KNN. Furthermore, it is investigated which requirements there are for such a VR application with regard to content components and user-friendliness. In the context of this work, requirements were determined with the help of a literature research

and a survey. In consideration of the requirements a concept was developed, which served as framework for the implementation of the VR application. The concept was implemented and evaluated as a prototype using an appropriate development methodology. The results of the evaluation show that the VR application can be operated in a user-friendly way and could probably lead to a gain in knowledge. Further studies in this area should follow in order to validate the assumption of the knowledge gain on the one hand and to examine the VR application more closely with regard to its usability on the other hand",,Neuronale Netze im Virtuellen Raum – Entwicklung einer benutzerfreundlichen Virtual Reality Anwendung zur Exploration und Datenkommunikation Künstlicher Neuronaler Netze,,,,core
395096888,2020-02-09T00:00:00,"Freight transportation industry is characterized by several decisional problems that operations managers have to cope with. Not only the routes planning must be realized before their execution, but also other types of decisions must be taken, in order to answer events that may dynamically occur during operations, as for instance road network congestion or vehicle

failures. Each decision can involve different aspects: for instance, the price negotiation of a just-in-time order should take into consideration the current routes status and planning. Off-the-shelf decision support software, although able to independently support the decision makers in each area, tend to keep tasks compartmentalized.

Trans-Cel, a small trucking company in Padova (Italy), has a Research and Development branch developing a cloud-based platform, called Chainment, able to host different decision support tools that can communicate through a data sharing system. These tools rely on an algorithmic engine that includes a routing optimization algorithm and artificial intelligence systems. In particular, the routing problem combines express couriers requirements, generally studied in urban contexts, with routes and vehicle features typical of medium- and long-haul trips, showing interesting characteristics that are worth of study in the Operation Research field.

In this thesis, we focus on the design of an optimization algorithm able to provide a solution to a Vehicle Routing Problem (VRP) inspired by the Trans-Cel scenario, that we name Express Pickup and Delivery in freight Trucking problem (EPDT).

The classical VRP definition includes a set of customers and a fleet of vehicles and aims to define a set of routes such that all customers are visited exactly once while minimizing the overall distance traveled. In the scientific literature, the basic definition of the problem has been generalized in order to consider additional attributes, often rising from real-world scenarios, as for instance capacity of vehicles, time windows and orders with both pickup and delivery operations. Often, in real-world cases, decision makers must simultaneously deal with a large number of attributes, thus defining a class of routing problems called Multi-Attribute VRP (MAVRP), which includes EPDT.

The thesis proposes a meta-heuristic algorithm for the solution of EPDT, with the aim of embedding it in the algorithmic engine of Chainment. In order to comply with the platform  requirements, the algorithm is designed so that a solution is returned within few seconds.

The solution method we propose consists of a two-level heuristic: at the first level, a Tabu Search algorithm hybridized with a Variable Neighborhood Descent explores the order-to-vehicle assignments, while, at the second level, it makes use of a Local Search to determine the sequence of customers visited and obtain an evaluation of routes.

The algorithm efficiency is enhanced by the use of a granular exploration, by procedures for fast evaluation of solutions in the neighborhoods, and parallel implementation of specific algorithmic components. These elements are adapted to the specific attributes of EPDT and represent some of the thesis contributions. The improvement in computational times have been validated by the experimental results, verifying the desired requirements for the platform integration.

The quality of the solutions obtained with the proposed meta-heuristic algorithm has been assessed both on the field, by comparison with Trans-Cel operations managers, and through bounds obtained with mathematical programming methods. To this purpose, the thesis proposes an Integer Linear Programming formulation for EPDT and a solution method for its continuous

relaxation based on Column Generation. In particular, the thesis presents new pricing procedures suitable for the specific EPDT attributes. The available bounds show optimality or near-optimality of solutions provided by the heuristic algorithm for real instances. Moreover, the algorithm has been tested on literature benchmarks related to the Pickup and Delivery

Problem with Time Windows (PDPTW), providing solutions that are competitive with the state-of-the-art.

The thesis also proposes a preliminary study of new approaches for vehicle routing problems in dynamic contexts. In particular, the thesis explores the possibility of taking advantage from the availability of historical data on orders by means of anticipatory strategies. The first strategy is based on clustering methods that are applied to the orders to define space-time

points that aggregate the information on future demand. A second strategy is based on the concept of accessibility, as defined in the discrete choice theory and urban logistic, to represent the route capability of intercepting future orders.

The heuristic algorithm proposed for EPDT has been integrated in the algorithmic engine of the Chainment platform at Trans-Cel. The thesis describes integration and the adaptation of the proposed optimization algorithms for a proper interaction with the different modules in the operational context handled by the platform, as, for instance, initial routes planning, reacting to dynamic events or order price negotiation",,Solving a Multi-Attribute Vehicle Routing Problem in the freight delivery industry,,,,core
328334891,2020-01-01T00:00:00,"Electric machines and motors have been the subject of enormous development. New concepts in design and control allow expanding their applications in different fields. The vast amount of data have been collected almost in any domain of interest. They can be static; that is to say, they represent real-world processes at  a fixed point of time. Vibration analysis and vibration monitoring, including how to detect and monitor anomalies in vibration data are widely used techniques for predictive maintenance in high-speed rotating machines. However, accurately identifying the presence of a bearing fault can be challenging in practice, especially when the failure is still at its incipient stage, and the signal-to-noise ratio of the monitored signal is small. The main objective of this work is to design a system that will analyze the vibration signals of a rotating machine, based on recorded data from sensors, in the time/frequency domain. As a consequence of such substantial interest, there has been a dramatic increase of interest in applying Machine Learning (ML) algorithms to this task. An ML system will be used to classify and detect abnormal behavior and recognize the different levels of machine operation modes. The proposed solution can be deployed as predictive maintenance for Industry 4.0",'Springer Science and Business Media LLC',Automatic classification of rotating machinery defects using Machine Learning (ML) algorithms,,10.1007/978-981-15-5784-2_16,,core
323907506,2020-01-01T00:00:00,"© 2019 Elsevier Ltd The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects",'Elsevier BV',Guidelines for applied machine learning in construction industry—A case of profit margins estimation,https://core.ac.uk/download/323907506.pdf,10.1016/j.aei.2019.101013,,core
335024629,2020-01-01T00:00:00,"Machine vision systems are applied in industry to control the quality of production while optimizing efficiency. A machine vision and AI-based inspection of color intensity in transparent Polyethylene Terephthalate (PET) preforms is especially sensitive to backgrounds and lighting, therefore, much attention is given to its illumination conditions. The paper examines the adverse factors affecting the quality of image recognition and presents an adaptive method for reducing the influence of changing illumination conditions in the color inspection process of transparent PET preforms. The method is based on predicting measured color intensity correction parameters according to illumination conditions. To test this adaptive method, a hardware and software system for image capture and processing was developed. This system is capable of inspecting large quantities of preforms in real time using a neural network with a modified gradient descent and momentum algorithm. The experiment showed that correction of the measured color intensity value reduced the standard deviation caused by variable and uneven illumination by 61.51%, demonstrating that machine vision color intensity evaluation is a robust and adaptive solution under illuminated conditions for detecting abnormalities in machine-based PET inspection procedures",'Institute of Electrical and Electronics Engineers (IEEE)',An adaptive method for inspecting illumination of color intensity in transparent polyethylene terephthalate preforms,,10.1109/ACCESS.2020.2991474,,core
345810242,2020-01-01T00:00:00,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory&#39;s data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.&nbsp;</p",,Federated Tensor Mining for Secure Industrial Internet of Things,,,,core
328153316,2020-08-12T00:00:00,"Since 2009, the deep learning revolution, which was triggered by the
introduction of ImageNet, has stimulated the synergy between Machine Learning
(ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical
reviews have emerged that suggest that ML/DL should be used cautiously. To
improve the quality (especially the applicability and generalizability) of
ML/DL-related SE studies, and to stimulate and enhance future collaborations
between SE/AI researchers and industry practitioners, we conducted a 10-year
Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published
between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that
ML/DL and SE have had on each other. At the same time, however, we also
observed a paucity of replicable and reproducible ML/DL-related SE studies and
identified five factors that influence their replicability and reproducibility.
To improve the applicability and generalizability of research results, we
analyzed what ingredients in a study would facilitate an understanding of why a
ML/DL technique was selected for a specific SE problem. In addition, we
identified the unique trends of impacts of DL models on SE tasks, as well as
five unique challenges that needed to be met in order to better leverage DL to
improve the productivity of SE tasks. Finally, we outlined a road-map that we
believe can facilitate the transfer of ML/DL-based SE research results into
real-world industry practices",,"Synergy between Machine/Deep Learning and Software Engineering: How Far
  Are We?",http://arxiv.org/abs/2008.05515,,,core
326495128,2020-06-28T00:00:00,"Increasing interest in integrating advanced robotics within manufacturing has
spurred a renewed concentration in developing real-time scheduling solutions to
coordinate human-robot collaboration in this environment. Traditionally, the
problem of scheduling agents to complete tasks with temporal and spatial
constraints has been approached either with exact algorithms, which are
computationally intractable for large-scale, dynamic coordination, or
approximate methods that require domain experts to craft heuristics for each
application. We seek to overcome the limitations of these conventional methods
by developing a novel graph attention network formulation to automatically
learn features of scheduling problems to allow their deployment. To learn
effective policies for combinatorial optimization problems via machine
learning, we combine imitation learning on smaller problems with deep
Q-learning on larger problems, in a non-parametric framework, to allow for
fast, near-optimal scheduling of robot teams. We show that our network-based
policy finds at least twice as many solutions over prior state-of-the-art
methods in all testing scenarios.Comment: This paper has been extended to an article in IEEE Robotics and
  Automation Letters (DOI: 10.1109/LRA.2020.3002198",,"Learning to Dynamically Coordinate Multi-Robot Teams in Graph Attention
  Networks",http://arxiv.org/abs/1912.02059,,,core
391315564,2020-12-01T00:00:00,"Social practices underpin all creative processes in the performing arts.In the production of stage performance, from ideation and rehearsal to delivery, creative processes converge collaboratively, generating a shared framework for a play’s expression, atmosphere and intent. At present, many of these processes are organised through sequestered pipelines, with the creative team independently working on individual components, discussed at intervals, supported by a range of tools such as drawings, software and physical scale models. This approach constrains collective decision making as it relies on multiple communication modes for integrating and manifesting all design streams on stage. The reliance on physical co-presence and isolated desktop interaction produces barriers to unlocking creative and economic efficiencies – especially in timessuch as Covid19 where gathering in physical space is not possible.The paper thinks through ways of transforming approaches in the performance design field, focusing on how new digital technologies can enable collective work in virtual space while at the same time supporting and documenting creative processes. It presents outcomes of the iDesign ARC-Linkage Project that addresses the identified shortcomings of established pipelines by developing a networked3D cross-platform visualisation system for experimental application at NIDA and Sydney Theatre Company. This system allows comprehensive immersive set design and performance in virtual space, situating creative and technical teams in an infinitely malleable 1:1 scale 3D datascape. It enables real-time creation, development and robust testing of set designs on a digitally twinned stage, supported by an AI system. The latter acts as a virtual dramaturg – recording, monitoring and advising on design activity, based on an industry-attuned accumulative database that features customisable props, set-pieces, lighting settings and OHS protocols.iDesign hence charts avenues for reshaping the way design conversations, rehearsal and performance may be staged in the future, projecting new encounters and practices while creating a living archive of design practice – enabling users to virtually learn about the hidden and ephemeral facets of theatrical practice",,Digitally Transforming Theatrical Design Practice,,,,core
359940759,2020-12-01T00:00:00,"abstract: The use of Artificial Intelligence in assistive systems is growing in application and efficiency. From self-driving cars, to medical and surgical robots and industrial tasked unsupervised co-robots; the use of AI and robotics to eliminate human error in high-stress environments and perform automated tasks is something that is advancing society’s status quo. Not only has the understanding of co-robotics exploded in the industrial world, but in research as well. The National Science Foundation (NSF) defines co-robots as the following: “...a robot whose main purpose is to work with people or other robots to accomplish a goal” (NSF, 1). The latest iteration of their National Robotics Initiative, NRI-2.0, focuses on efforts of creating co-robots optimized for ‘scalability, customizability, lowering barriers to entry, and societal impact’(NSF, 1). While many avenues have been explored for the implementation of co-robotics to create more efficient processes and sustainable lifestyles, this project’s focus was on societal impact co-robotics in the field of human safety and well-being. Introducing a co-robotics and computer vision AI solution for first responder assistance would help bring awareness and efficiency to public safety. The use of real-time identification techniques would create a greater range of awareness for first responders in high-stress situations. A combination of environmental features collected through sensors (camera and radar) could be used to identify people and objects within certain environments where visual impairments and obstructions are high (eg. burning buildings, smoke-filled rooms, ect.). Information about situational conditions (environmental readings, locations of other occupants, etc.) could be transmitted to first responders in emergency situations, maximizing situational awareness. This would not only aid first responders in the evaluation of emergency situations, but it would provide useful data for the first responder that would help materialize the most effective course of action for said situation",,The Investigation of Low Cost Computer Vision Application for First Responder Co-robotics,,,,core
395139328,2020-01-01T00:00:00,"У статті доведено важливість формування концепції забезпечення кіберстійкості банків на сучасному етапі розвитку цифрової економіки країни, зважаючи на негативний фінансовий та нефінансовий вплив кібератак на банківську систему та економіку країни в цілому. Автором на основі узагальнення досліджень з цієї тематики уточнено зміст поняття “кіберстійкість банку” та визначено його сутнісні характеристики за якісним та кількісним підходами. В статті проведено дослідження теоретичних підходів до забезпечення кіберстійкості банків та на цій основі розроблено модель механізму забезпечення кіберстійкості, адекватну сучасному стану та умовам, в яких функціонують банки України. За результатами дослідження визначено, що ефективне функціонування механізму забезпечення кіберстійкості потребує відповідного організаційного забезпечення, зокрема створення Центра кіберстійкості банку.В статье доказана важность формирования концепции обеспечения киберустойчивости банков на современном этапе развития цифровой экономики страны, несмотря на отрицательное финансовое и нефинансовое влияние кибератак на банковскую систему и экономику страны в целом. Автором на основе обобщения исследований по этой тематике уточнено содержание понятия ""киберустойчивость банка"" и определены его сущностные характеристики согласно качественному и количественному подходам. В статье проведено исследование теоретических подходов к обеспечению киберустойчивости банков и на этой основе разработана модель механизма обеспечения киберустойчивости, адекватная условиям, в которых функционируют банки Украины. В результате исследования установлено, что эффективное функционирование механизма обеспечения киберустойчивости требует соответствующего организационного обеспечения, в частности создания Центра киберустойчивости банка.The article proves the importance of forming the concept of ensuring the cyber resilience of banks at the present stage of development of the country's digital economy in the transition to the sixth technological mode and the associated use of industry 4.0 technologies, such as artificial intelligence, «cloud» and «foggy» computing, IoT / IIoT, Big Data, Blockchain, VR / AR. This leads to a significant complication of the cyber threat landscape, an increase in the number of cyberattacks with a significant increase in the negative financial and non-financial consequences that cyberattacks have on the banking system and the economy as a whole. The author, based on the generalization of research on this topic, clarified the content of the concept of ""cyber resilience of a bank"". Its essential characteristics were determined in the context of qualitative and quantitative approaches. The article studies theoretical approaches to ensuring the cyber resilience of banks, which made it possible to develop a conceptual model of the mechanism for ensuring cyber resilience, adequate to the current state and conditions in which the banks of Ukraine operate. The developed mechanism for ensuring cyber resilience allows for: 1) the formalization of the landscape of real and potential cyber threats; 2) ensures the consistency of mechanisms and tools for countering them, adapting and/or recovering from cyber incidents; 3) allows not only to adequately respond to existing cyber threats but also to identify negative factors that can lead to the emergence and implementation of new cyber threats and cyber-attacks. According to the results of the study, it was found that the effective functioning of the mechanism for ensuring the cyber resilience of the bank requires appropriate organizational support. For this, the author substantiated the need to create a Bank Cyber Resilience Center. It should include representatives from the departments responsible for banking business continuity, cybersecurity, cyber risk management and IT quality. This will allow obtaining a synergistic effect by creating a single interconnected process-based model, including metrics of the bank's cyber resilience level and KPIs, as well as tools for monitoring, controlling and resisting external and internal cyber threats, adaptation and/or recovery after them",'DKS Center',Теорія та практика забезпечення кіберстійкості банків,,10.32702/2307-2105-2020.10.50,,core
323307134,2020-04-13T00:00:00,"The emergence of Artificial Intelligence (AI) is creating new dimensions and redefining the concept and meaning of work in industrial settings. Documented success has been reported where AI is transforming industrial scenes such as scaling large operation processes, speed of execution, flexibility of processes where rigid manufacturing by dumb robots is replaced with smart individualized production following real-time customer choices, decision-making in which a huge amount of data can be quickly available at the fingertips of workers on the factory floor or even prevent problems before they happen, and personalization where AI uses data to deliver personalized user experience. According to the market research firm Trac tica, the global AI software market is expected to experience massive growth in the coming years, with revenues increasing from around US 9.5 billion in 2018 to an expected US 118.6 billion by 2025",'Institute of Electrical and Electronics Engineers (IEEE)',Future trends in I&M: Human-machine co-creation in the rise of AI,https://core.ac.uk/download/323307134.pdf,10.1109/MIM.2020.9062691,,core
327040962,2020-08-21T00:00:00,"As machine learning for images becomes democratized in the Software 2.0 era,
one of the serious bottlenecks is securing enough labeled data for training.
This problem is especially critical in a manufacturing setting where smart
factories rely on machine learning for product quality control by analyzing
industrial images. Such images are typically large and may only need to be
partially analyzed where only a small portion is problematic (e.g., identifying
defects on a surface). Since manual labeling these images is expensive, weak
supervision is an attractive alternative where the idea is to generate weak
labels that are not perfect, but can be produced at scale. Data programming is
a recent paradigm in this category where it uses human knowledge in the form of
labeling functions and combines them into a generative model. Data programming
has been successful in applications based on text or structured data and can
also be applied to images usually if one can find a way to convert them into
structured data. In this work, we expand the horizon of data programming by
directly applying it to images without this conversion, which is a common
scenario for industrial applications. We propose Inspector Gadget, an image
labeling system that combines crowdsourcing, data augmentation, and data
programming to produce weak labels at scale for image classification. We
perform experiments on real industrial image datasets and show that Inspector
Gadget obtains better performance than other weak-labeling techniques: Snuba,
GOGGLES, and self-learning baselines using convolutional neural networks (CNNs)
without pre-training.Comment: 10 pages, 11 figure",,"Inspector Gadget: A Data Programming-based Labeling System for
  Industrial Images",http://arxiv.org/abs/2004.03264,,,core
334916064,2020-02-25T00:00:00,"Learning Machines is developing a flexible, cross-industry, advanced
analytics platform, targeted during stealth-stage at a limited number of
specific vertical applications. In this paper, we aim to integrate a general
machine system to learn a variant of tasks from simulation to real world. In
such a machine system, it involves real-time robot vision, sensor fusion, and
learning algorithms (reinforcement learning). To this end, we demonstrate the
general machine system on three fundamental tasks including obstacle avoidance,
foraging, and predator-prey robot. The proposed solutions are implemented on
Robobo robots with mobile device (smartphone with camera) as interface and
built-in infrared (IR) sensors. The agent is trained in a virtual environment.
In order to assess its performance, the learned agent is tested in the virtual
environment and reproduce the same results in a real environment. The results
show that the reinforcement learning algorithm can be reliably used for a
variety of tasks in unknown environments",,Learning Machines from Simulation to Real World,http://arxiv.org/abs/2002.10853,,,core
334937024,2020-04-28T00:00:00,"Recently, Memory-based Neural Recommenders (MNR) have demonstrated superior
predictive accuracy in the task of sequential recommendations, particularly for
modeling long-term item dependencies. However, typical MNR requires complex
memory access operations, i.e., both writing and reading via a controller
(e.g., RNN) at every time step. Those frequent operations will dramatically
increase the network training time, resulting in the difficulty in being
deployed on industrial-scale recommender systems. In this paper, we present a
novel general Chunk framework to accelerate MNR significantly. Specifically,
our framework divides proximal information units into chunks, and performs
memory access at certain time steps, whereby the number of memory operations
can be greatly reduced. We investigate two ways to implement effective
chunking, i.e., PEriodic Chunk (PEC) and Time-Sensitive Chunk (TSC), to
preserve and recover important recurrent signals in the sequence. Since
chunk-accelerated MNR models take into account more proximal information units
than that from a single timestep, it can remove the influence of noise in the
item sequence to a large extent, and thus improve the stability of MNR. In this
way, the proposed chunk mechanism can lead to not only faster training and
prediction, but even slightly better results. The experimental results on three
real-world datasets (weishi, ml-10M and ml-latest) show that our chunk
framework notably reduces the running time (e.g., with up to 7x for training &
10x for inference on ml-latest) of MNR, and meantime achieves competitive
performance.Comment: 11 page",,CmnRec: Sequential Recommendations with Chunk-accelerated Memory Network,http://arxiv.org/abs/2004.13401,,,core
344707014,2020-07-01T00:00:00,"The development of complex real-time platforms for the Internet of Things (IoT) opens up a promising future for the diagnosis and the optimization of machining processes. Many issues have still to be solved before IoT platforms can be profitable for small workshops with very flexible workloads and workflows. The main obstacles refer to sensor implementation, IoT architecture, and data processing, and analysis. In this research, the use of different machine-learning techniques is proposed, for the extraction of different information from an IoT platform connected to a machining center, working under real industrial conditions in a workshop. The aim is to evaluate which algorithmic technique might be the best to build accurate prediction models for one of the main demands of workshops: the optimization of machining processes. This evaluation, completed under real industrial conditions, includes very limited information on the machining workload of the machining center and unbalanced datasets. The strategy is validated for the classification of the state of a machining center, its working mode, and the prediction of the thermal evolution of the main machine-tool motors: the axis motors and the milling head motor. The results show the superiority of the ensembles for both classification problems under analysis and all four regression problems. In particular, Rotation Forest-based ensembles turned out to have the best performance in the experiments for all the metrics under study. The models are accurate enough to provide useful conclusions applicable to current industrial practice, such as improvements in machine programming to avoid cutting conditions that might greatly reduce tool lifetime and damage machine components.Projects TIN2015-67534-P (MINECO/FEDER, UE) of the Ministerio de Economía Competitividad of the Spanish Government and projects CCTT1/17/BU/0003 and BU085P17 (JCyL/FEDER, UE) of the Junta de Castilla y León, all of them co-financed through European-Union FEDER funds",'MDPI AG',Using ensembles for accurate modelling of manufacturing processes in an IoT data-acquisition solution,,10.3390/app10134606,"[{'title': 'Applied Sciences', 'identifiers': ['2076-3417', 'issn:2076-3417']}]",core
334903144,2020-01-15T00:00:00,"To facilitate the widespread acceptance of AI systems guiding decision-making
in real-world applications, it is key that solutions comprise trustworthy,
integrated human-AI systems. Not only in safety-critical applications such as
autonomous driving or medicine, but also in dynamic open world systems in
industry and government it is crucial for predictive models to be
uncertainty-aware and yield trustworthy predictions. Another key requirement
for deployment of AI at enterprise scale is to realize the importance of
integrating human-centered design into AI systems such that humans are able to
use systems effectively, understand results and output, and explain findings to
oversight committees.
  While the focus of this symposium was on AI systems to improve data quality
and technical robustness and safety, we welcomed submissions from broadly
defined areas also discussing approaches addressing requirements such as
explainable models, human trust and ethical aspects of AI.Comment: Proceedings for AAAI 2019 Fall Symposium Series - Human-centered AI:
  Trustworthiness of AI Models & Dat",,"AAAI FSS-19: Human-Centered AI: Trustworthiness of AI Models and Data
  Proceedings",http://arxiv.org/abs/2001.05375,,,core
363396926,2020-01-01T00:00:00,"Altres ajuts: Secretaria d'Universitats i Recerca del Departament d'Empresa i Coneixement de la Generalitat de Catalunya i del Fons Social Europeu (2020 FI_B2 000)The evolution of industry towards the Industry 4.0 paradigm has become a reality where different data-driven methods are adopted to support industrial processes. One of them corresponds to Artificial Neural Networks (ANNs), which are able to model highly complex and non-linear processes. This motivates their adoption as part of new data-driven based control strategies. The ANN-based Internal Model Controller (ANN-based IMC) is an example which takes advantage of the ANNs characteristics by modelling the direct and inverse relationships of the process under control with them. This approach has been implemented in Wastewater Treatment Plants (WWTP), where results show a significant improvement on control performance metrics with respect to (w.r.t.) the WWTP default control strategy. However, this structure is very sensible to non-desired effects in the measurements-when a real scenario showing noise-corrupted data is considered, the control performance drops. To solve this, a new ANN-based IMC approach is designed with a two-fold objective, improve the control performance and denoise the noise-corrupted measurements to reduce the performance degradation. Results show that the proposed structure improves the control metrics, (the Integrated Absolute Error (IAE) and the Integrated Squared Error (ISE)), around a 21.25% and a 54.64%, respectively",'MDPI AG',Denoising Autoencoders and LSTM-Based Artificial Neural Networks Data Processing for Its Application to Internal Model Control in Industrial Environments-The Wastewater Treatment Plant Control Case,https://core.ac.uk/download/363396926.pdf,10.3390/s20133743,,core
351948322,2020-10-11T00:00:00,"During the operation process of a system its technical state is changed. The changes take place because of the wearing factors impact. The impact depends on the flow and intensity of the operation process what is characterized by the time histories of the working parameters. Simultaneously, the changes of the technical state are correlated with the changes of the amount of the operational potential included in a system. In order to avoid the inability state occurrence the amount of this potential should be higher than the boundary value. The amount of the operational potential included in a system is determined by the values of the cardinal features of it but in the case of the real technical system the values cannot always be measured. Therefore, the amount of the operational potential and the technical state of the system cannot always be determined online. To solve this problem the model of the operational potential consumption process was created and presented in the paper. The model uses artificial intelligence techniques to calculate the change of the operational potential amount by determining the changes of the cardinal features of the system on the basis of the time histories of the working parameters. The verification of the model quality was performed using the pulverized boiler OP-650k-040 operating in the power plant. The description of the conducted research and the results of the verification were presented in the end of the paper proving the adequacy of the model implementation in the case of industrial objects",'University of Nis',FUZZY MODEL OF THE OPERATIONAL POTENTIAL CONSUMPTION PROCESS OF A COMPLEX TECHNICAL SYSTEM,https://core.ac.uk/download/351948322.pdf,10.22190/FUME200306032P,,core
287192453,2020-02-10,"A true artificial intelligence (AI) system is something that ""learns"" from the data it stores,  in order to perform tasks and solve problems that typically require human intelligence - either with the help of a human expert or independently. The area of AI is an interdisciplinary field, which has been designated as a strategic area in the European Union (EU) approach and a key driver of economic development that can bring solutions to many social challenges and problems. Due to its nature and its tendency to be digitally advanced and smarter with analytics, the financial sector is one of the early adopters of AI and expects multiple benefits from its application, that is, the ability to provide better service in the shortest time possible and at a lower cost. AI in the financial sector is based on an understanding of the business needs of financial organizations, institutions and markets and the ability to connect with technological capabilities. They are powerful tools that completely transform this sector. The basic idea of this paper is to consider where the real value of AI in the financial sector is, i.e. what are the practical aspects and business implications of AI in the financial sector globally. It is common knowledge that evolving technologies have always had a strong impact on the sectors in which they are applied because they give them the opportunity to improve existing manufacturing processes, services, customer experiences, operate more efficiently, achieve cost savings, etc. The aim of this paper is to identify areas of application of AI in the financial sector, and to explore leading AI applications that are changing the financial ecosystem, transforming the financial sector and that have the potential to significantly improve many of its functions. The paper further highlights other implications of AI implementation in the financial sector such as employment - job creation and termination of existing AI-influenced employment, the scope and potential of application in developing countries, the problem of regulation and use in the best interests of man, and the importance of properly managing specific AI risks","Ekonomski fakultet, Univerzitet u Istočnom Sarajevu",FINANCE AND ARTIFICIAL INTELLIGENCE: THE FIFTH INDUSTRIAL REVOLUTION AND ITS IMPACT ON THE FINANCIAL SECTOR,https://core.ac.uk/download/pdf/287192453.pdf,10.7251/ZREFIS1919067G,,core
337612764,2020-06-29T00:00:00,"Reinforcement Learning, as one of the main approaches of machine learning, has been gaining high popularity in recent years, which also affects the vehicle industry and research focusing on automated driving. However, these techniques, due to their self-training approach, have high computational resource requirements. Their development can be separated into training with simulation, validation through vehicle dynamics software, and real-world tests. However, ensuring portability of the designed algorithms between these levels is difficult. A case study is also given to provide better insight into the development process, in which an online trajectory planner is trained and evaluated in both vehicle simulation and real-world environments",'Periodica Polytechnica Budapest University of Technology and Economics',Fast Prototype Framework for Deep Reinforcement Learning-based Trajectory Planner,https://core.ac.uk/download/337612764.pdf,,,core
480019893,2020-01-01T00:00:00,"The paper presents the implementation of an intelligent decision support system (IDSS) to solve a real manufacturing problem at JSC “Savushkin Product”. The proposed system is intended to control the quality of product labeling, based on neuro-symbolic artificial intelligence, namely integrating deep neural networks and semantic models. The system perform localization and recognition of images from a high-speed video stream and is based on several deep neural networks. Semantic networks fulfill intelligent processing of recognition results in order to generate final decision as regards the state of the production conveyor. We demonstrate the performance of the proposed technique in the real production process. The main contribution of this paper is a novel view at the creation of a real intelligent decision support system, which combines bio inspired approach, namely neural networks and conventional technique, based on a knowledge base",'Springer Science and Business Media LLC',Neuro-Symbolic Artificial Intelligence: Application for Control the Quality of Product Labeling,https://core.ac.uk/download/480019893.pdf,10.1007/978-3-030-60447-9_6.,,core
328264402,2019-12-07T00:00:00,"Digital  disruptive  technologies  are  an  integral  component  of  the  modern  world.  These  technologies  are  transforming  the  globalindustries from traditional to more innovative and adaptive. However, the state of global real estate is yet to improve and is currently lagging the technology curve. Because of this lag, useful information is either not made available to the end-users or is shared too late that is raising concerns  among  the  online real  estate  platform  users.  This  results  in  larger  vacancy  rates  and  post-occupancy  regrets  among  the  service consumers. The current study based on the concepts of Technology Acceptance Models (TAM), presents a conceptual Real Estate Stakeholders Technology Acceptance Model (RESTAM) for addressing the key needs of the four important stakeholders of the real estate industry including the end-users or consumers, government & regulatory authorities, agents & agencies and complementary industries. Based on comprehensive literature review of 213 articles, the needs of these stakeholders are assessed and addressed through the Big9 technologies namely drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR & AR), and artificial intelligence and robotics. The resulting RESTAM framework with a specific focus on the online platform based real estate users are expected to lay the foundation for introducing the missing technology acceptance model for real estate stakeholders whereby these Big9 disruptive technologies are implemented in real estate industry to uplift it from traditional to smart real estate. This will reduce the post-occupancy regrets of the real estate service users and improve the relations between various real estate stakeholders","MUET, Pakistan","2nd International Conference on Sustainable Development in Civil Engineering (ICSDC 2019), Jamshoro Pakistan",https://www.researchgate.net/profile/Fahim_Ullah3/publication/337772796_Real_Estate_Stakeholders_Technology_Acceptance_Model_RESTAM_User-focused_Big9_Disruptive_Technologies_for_Smart_Real_Estate_Management/links/5de99409a6fdcc28370939dc/Real-Estate-Stakeholders-Technology-Acceptance-Model-RESTAM-User-focused-Big9-Disruptive-Technologies-for-Smart-Real-Estate-Management.pdf,,,core
322488950,2019-05-08T00:00:00,"A complete defect detection task aims to achieve the specific class and precise location of each defect in an image, which makes it still challenging for applying this task in practice. The defect detection is a composite task of classification and location, leading to related methods is often hard to take into account the accuracy of both. The implementation of defect detection depends on a special detection data set that contains expensive manual annotations. In this paper, we proposed a novel defect detection system based on deep learning and focused on a practical industrial application: steel plate defect inspection. In order to achieve strong classification ability, this system employs a baseline convolution neural network (CNN) to generate feature maps at each stage, and then the proposed multilevel feature fusion network (MFN) combines multiple hierarchical features into one feature, which can include more location details of defects. Based on these multilevel features, a region proposal network (RPN) is adopted to generate regions of interest (ROIs). For each ROI, a detector, consisting of a classifier and a bounding box regressor, produces the final detection results. Finally, we set up a defect detection data set NEU-DET for training and evaluating our method. On the NEU-DET, our method achieves 74.8/82.3 mAP with baseline networks ResNet34/50 by using 300 proposals. In addition, by using only 50 proposals, our method can detect at 20 ft/s on a single GPU and reach 92% of the above performance, hence the potential for real-time detection",,An end-to-end steel surface defect detection approach via fusing multiple hierarchical features,,,,core
322371941,2019-01-01T00:00:00,"By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud. - 2019 IEEE.ACKNOWLEDGMENT The corresponding author, Dr. Yang is also with CETC Key Laboratory of Data Link Technology. This work is supported in part by the National Science Foundation of China (61871454); by the Natural Science Basic Research Plan in Shaanxi Province of China (2017JZ021); by a special financial grant from the China Postdoctoral Science Foundation (2016T90894); by a special financial grant from the Shaanxi Postdoctoral Science Foundation (154066); by the CETC Key Laboratory of Data Link Technology (CLDL-20182308); the Fundamental Research Funds for the Central Universities; by the ISN02080001 and ISN90106180001; the 111 Project under Grant B08038; and the National Science Foundation of China under Grant 61671062 and Grant 91638202",'Institute of Electrical and Electronics Engineers (IEEE)',EDGE AI for Heterogeneous and Massive IoT Networks,,10.1109/ICCT46805.2019.8947193,,core
352949441,2019-01-01T00:00:00,"Complexity is a fundamental part of product design and manufacturing today, owing to increased demands for customization and advances in digital design techniques. Assembling and repairing such an enormous variety of components means that workers are cognitively challenged, take longer to search for the relevant information and are prone to making mistakes. Although in recent years deep learning approaches to object recognition have seen rapid advances, the combined potential of deep learning and augmented reality in the industrial domain remains relatively under explored. In this paper we introduce AR-ProMO, a combined hardware/software solution that provides a generalizable assistance system for identifying mistakes during product assembly and repair",'Association for Computing Machinery (ACM)',Handling Work Complexity with AR/Deep Learning,,10.1145/3369457.3370919,,core
286784470,2019-01-01T00:00:00,"4th Industrial revolution are spreading around the world, embedding the technology into societies. In the digital revolution, high technological tools and resources are regularly being developed. There has been increased attention on learning coding in education field, in order to nurture sufficient number of young generation to fill in 50 percent of jobs opportunity in science, technology, engineering, and mathematics (STEM) which are computing-related. It is increasingly clear that our new generation need to think critically to solve the ill-structured problems, uncertain and complex real world problems. Computational thinking is beneficial in providing automated or semi-automated solutions in problems solving in combination with critical thinking. Therefore, computational thinking is becoming an important implication in science and mathematics as well as in almost every other fields. This fact is reflected in the recent implementation of computer coding in the Malaysia’s school curriculum to nurture 21st century skills among students. Computational thinking is one of the conceptual foundations which required combination of data and ideas to solve problems effectively and efficiently. Computational thinking is essential to computing and information science (i.e., algorithmically, with or without the assistance of computers) to solve problems with solutions that are reusable in different contexts. Promoting different kind of skills and abilities among students is important to solve the complexity problem in real world. Thus, it is a challenging task for many instructors to create a learning environment who lack of feasible resources availability and research-based information to redesign their teaching pedagogies. This conceptual framework is aimed at two goals: (1) development of deep learning, connected computational thinking through the mathematical curriculum instructional model which able to enhance students’ problem solving abilities and (2) implementation of the designed model to assist teacher in education who has practicing difficulty in elementary classrooms after adopting teacher enactment of problem-based curriculum resources. To achieve these goals, computational mathematics problem based learning (CM-PBL) instructional strategy is developed to promote an active learning environment for classroom management involving problem solving. Rather than emphasizing student learning passively through listening, watching, practicing exercises and imitating isolated skills, the CM-PBL learning framework allowing students to simulate and build their own computational models to support their self-learning and understanding of mathematic concepts. It is also highlights ways that students can evolve from technical skills in using technology to soft engineering skills by using coding knowledge","Faculty of Educational Studies, Universiti Putra Malaysia",Why coding? Why now? From coding to computational thinking through computational mathematics problem based learning (CM-PBL),,,,core
427558837,2019-01-01T00:00:00,"Free text and hand-written reports are losing ground to digitization fast, however many hours of effort are still lost across the industry to the manual creation and analysis of these data types. Work orders in particular contain valuable information from failure rates to asset health, but at the same time present operators with such analytical difficulties and lack of structure that many are missing out on the value completely. This research challenges the current mainstream practice of manual work order analysis by presenting a methodology fit for today’s context of efficiency and digitization.  A prototype text mining software for work order analysis was developed and tested in a user-oriented approach in cooperation with industrial partners. The final prototype combines classical machine learning methods, such as hierarchical clustering, with the operator’s expert knowledge obtained via an active learning approach. A novel distance metric in this context was adapted from information-theoretical research to improve clustering performance.  Using the prototype tool in a case study with real work order data, analytical effort for certain datasets was reduced by 90% - from two working weeks to a day. In addition, the active learning framework resulted in an approach that end users described as ""practical"" and ""intuitive"" during testing. An in-depth review was also conducted regarding the uncertainty of the results – a key factor for implementation in a decision-making context.  The outcomes of this work showcase the potential of machine learning to drive the digitization of not only new installations, but also older assets, where as a result the large amount of unstructured historical data becomes an advantage rather than a hindrance. User testing results encourage a wider uptake of machine learning solutions in the industry, and particularly a shift towards more accessible in-house analytical capabilities",'Society of Petroleum Engineers (SPE)',Work orders - Value from structureless text in the era of digitisation,,10.2118/195788-MS,,core
326833981,2019-01-08T08:00:00,"This study proposes a model designed to help sales representatives in the software industry to manage the complex sales pipeline. By integrating business analytics in the form of machine learning into lead and opportunity management, data-driven qualification support reduces the high degree of arbitrariness caused by professional expertise and experiences. Through the case study of a software provider, we developed an artifact consisting of three models to map the end-to-end sales pipeline process using real business data from the company’s CRM system. The results show a superiority of the CatBoost and Random Forest algorithm over other supervised classifiers such as Support Vector Machine, XGBoost, and Decision Tree as the baseline. The study also reveals that the probability of either winning or losing a sales deal in the early lead stage is more difficult to predict than analyzing the lead and opportunity phases separately. Furthermore, an explanation functionality for individual predictions is provided",AIS Electronic Library (AISeL),Business Analytics for Sales Pipeline Management in the Software Industry: A Machine Learning Perspective,https://core.ac.uk/download/326833981.pdf,,,core
398561907,2019-01-01T00:00:00,"Adoption of digital platform innovations afford a changing nature of work, from mobile computing platforms (e.g. Apple) enabling 24/7 work connectivity, to labour marketplace platforms (e.g. Uber) enabling precarious work arrangements. Recently, organisations are adopting/investigating spatial computing platforms (e.g. Autodesk, Toyota, BNP Paribas), offering new affordances for organising (e.g. carrying out tasks, communicating and collaborating). Spatial computing concerns achieving spatial interplay between the real and digital world (Agulhon 2016), enabling perception of physically present content. An emerging paradigm of spatial computing is enabled by hardware and software innovations for; 1) digitally mapping, tracking, understanding and predicting analog audio and visual spatial fields, 2) creating digital audio and visual spatial fields, and the (3) mixing and fusing of those fields. Mixed, augmented and immersive reality is then experienced by volumetric graphic rendering onto a human's field of view (FOV) (Martín-Gutiérrez et al. 2017). Emerging marketplace examples can be seen in 'Microsoft Hololens 2' and 'Magic Leap One' platforms, both creating/enabling an ecosystem of novel applications for both industrial, educational and leisure life contexts. With further convergence of IoT, haptics, 5G, cloud and AI etc., spatial applications will range from contextually aware and interactive; digital information layering of objects, guidance and decision support systems (DSS) within business operations (such as for industrial machine manufacture, monitoring, and maintenance), digital modelling & prototyping in R&D, through to applications for communications and collaborations (such as for spatial tele co-presence of people, objects and environments). More broadly, these advances have potential to catalyse disruptions within business, through to the labour and consumer marketplace via: (1) Virtualisation of hardware resources (e.g. fully digitising workplace equipment such as displays and interfaces, raw inputs for prototyping and even digital rendering of spaces). (2) Protection and strengthening of institutional knowledge and performance via knowledge capture, guidance and decision support of labour tasks and activity (e.g. reducing labour (re)training (e.g. parts assembly), knowledge capture of practice). (3) Creation and distribution of new value propositions in goods and services (e.g. digital item ownership in a mixed-reality cloud, spatial applications for IoT enabled devices). (4) Displacement of geographic space as cost, talent, time, access and convenience constraints on business (e.g. available talent pool, partner/customer reach and relations). (5) Collaboration through new/enhanced affordances for workers (e.g. shared digitised work tools/environments). Therefore, a paradigm of spatial computing will challenge the IS community to research new ways of working, and consequences for worker experience, meaning, productivity and power. With emerging advances in AI, automation and spatial computing, one of the pertinent enquires concerns importance of workers (sense of) agency (Chandra et al. 2019). Control in the IT context has been conceptualised as control over work, control over self, and control over technology (Beaudry and Pinsonneault 2005), with prior IS work studying locus of control related to; work stress (Chandra et al. 2019), intrinsic and extrinsic motivations (Mujinga, M Eloff, MM Kroeze 2013), and performance (Vieira da Cunha et al. 2015) etc. With spatial computing platforms and their applications, what affordances of control and for whom should be developed? For example, the electronic representation of worker activity can be further enabled. Thus, tighter or looser coupling between worker activity and the reporting/outcome of work (Vieira da Cunha et al. 2015) becomes more of an organisational decision, with capability to monitor workers, and leverage AI for learning and optimisation. Furthermore, with development of spatial tele co-presence (STcP) (e.g. Mimesys), brings new affordances for communication with any worker(s), at any time, from anywhere. However, prior CMC research suggests people can choose different communication media specifically to manage social and emotional relationships (Madianou 2014) and their time (Mcloughlin et al. 2019). Hence, will such affordances serve greater identity fusion (Swann et al. 2012) and collaboration in organisations? Thus, we propose a socio-technical research agenda exploring 'control' related affordances for emerging spatial computing platforms, such as for STcP technology. In this regard, Control Theory can offer a useful starting frame, as it deals with control mechanisms governing workers organisational actions both formal (outcome and behaviour based) and informal (group and self-control), to further the interests of organisations (Kirsch 1996). We suggest, data and communication related affordances of control (e.g. privacy, exploitation, authenticity, availability and spaces) as starting points. Social Capital (Lin 2001), Social Influence (Kelman 1958), Social Identity (Ellemers and Haslam 2012), Identity Fusion (Swann et al. 2012) and Polymedia (Madianou and Miller 2012) being just some of the many relevant social theories to this endeavour",,Affordances of Control in a Paradigm of Spatial Computing Platforms,,,,core
323194776,2019-11-02T00:00:00,"This paper outlines real-world control challenges faced by modern-day biopharmaceutical facilities through the extension of a previously developed industrial-scale penicillin fermentation simulation (IndPenSim). The extensions include the addition of a simulated Raman spectroscopy device for the purpose of developing, evaluating and implementation of advanced and innovative control solutions applicable to biotechnology facilities. IndPenSim can be operated in fixed or operator controlled mode and generates all the available on-line, off-line and Raman spectra for each batch. The capabilities of IndPenSim were initially demonstrated through the implementation of a QbD methodology utilising the three stages of the PAT framework. Furthermore, IndPenSim evaluated a fault detection algorithm to detect process faults occurring on different batches recorded throughout a yearly campaign. The simulator and all data presented here are available to download at www.industrialpenicillinsimulation.com and acts as a benchmark for researchers to analyse, improve and optimise the current control strategy implemented on this facility. Additionally, a highly valuable data resource containing 100 batches with all available process and Raman spectroscopy measurements is freely available to download. This data is highly suitable for the development of big data analytics, machine learning (ML) or artificial intelligence (AI) algorithms applicable to the biopharmaceutical industry",,Modern day monitoring and control challenges outlined on an industrial-scale benchmark fermentation process,https://core.ac.uk/download/323194776.pdf,,,core
286430089,2019-01-01T00:00:00,"Artificial Intelligence (AI) systems exert a growing influence on our society. As they become more ubiquitous, their potential negative impacts also become evident through various real-world incidents. Following such early incidents, academic and public discussion on AI ethics has highlighted the need for implementing ethics in AI system development. However, little currently exists in the way of frameworks for understanding the practical implementation of AI ethics. In this paper, we discuss a research framework for implementing AI ethics in industrial settings. The framework presents a starting point for empirical studies into AI ethics but is still being developed further based on its practical utilization.peerReviewe",RWTH Aachen University,AI Ethics in Industry : A Research Framework,,,,core
304994739,2019-04-05T00:00:00,"Presented on April 5, 2019 at 12:00 p.m. in the Klaus Advanced Computing Building, Room 1116.Shang-Tse Chen is a Ph.D. Candidate in Computer Science at Georgia Tech. He works in the intersection of applied and theoretical machine learning. His research focuses on designing robust machine learning algorithms for security-critical applications. He has worked closely with industry and government partners. His research has led to patent-pending cyber threat detection technology with Symantec, open-sourced adversarial attack and defense tools with Intel, deployed fire risk prediction system with the Atlanta Fire Rescue Department.Runtime: 37:45 minutesWhile Artificial Intelligence (AI) has tremendous potential as a defense against real-world cybersecurity threats, understanding the capabilities and robustness of AI remains a fundamental challenge, especially in adversarial environments. In this talk, I address two interrelated problems that are essential to the successful deployment of AI in security settings. (1) Discovering real-world vulnerabilities of deep neural networks and countermeasures to mitigate threats. I will present ShapeShifter, the first targeted physical adversarial attack that fools state-of-the-art object detectors, and SHIELD, a real-time defense that removes adversarial noise by stochastic data compression. (2) Developing theoretically-principled methods for choosing machine models to defend against unknown future attacks. I will introduce a novel game theory concept called “diversified strategy” to help make the optimal decision with limited risk. Finally, I will share my vision on making AI more robust under different threat models, and research directions on deploying AI in security-critical and high-stakes problems",Georgia Institute of Technology,AI-infused Security: Robust Defense by Bridging Theory and Practice,,,,core
200797009,2019-02-06T00:00:00,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system",,Dynamic Pricing for Airline Ancillaries with Customer Context,http://arxiv.org/abs/1902.02236,,,core
200810936,2019-03-12T00:00:00,"The 5th edition of the International Conference on Cloud and Robotics (ICCR
2018 - http://cloudrobotics.info) will be held on November 12-14 2018 in Paris
and Saint-Quentin, France. The conference is a co-event with GDR ALROB and the
industry exposition Robonumerique (http://www.robonumerique.fr).
  The domain of cloud robotics aims to converge robots with computation,
storage and communication resources provided by the cloud. The cloud may
complement robotic resources in several ways, including crowd-sourcing
knowledge databases, context information, computational offloading or
data-intensive information processing for artificial intelligence. Today, the
paradigms of cloud/fog/edge computing propose software architecture solutions
for robots to share computations or offload them to ambiant and networked
resources. Yet, combining distant computations with the real time constraints
of robotics is very challenging. As the challenges in this domain are
multi-disciplinary and similar in other research areas, Cloud Robotics aims at
building bridges among experts from academia and industry working in different
fields, such as robotics, cyber-physical systems, automotive, aerospace,
machine learning, artificial intelligence, software architecture, big data
analytics, Internet-of-Things, networked control and distributed cloud systems",,"Proceedings of the Fifth International Conference on Cloud and Robotics
  (ICCR2018)",http://arxiv.org/abs/1903.04824,,,core
200955322,2019-04-01T00:00:00,"Introduction. It is shown that the digital twin (electronic passport) of a CNC machine is developed as a cyber-physical system. The work objective is to create neural network models to determine the operation of a CNC machine, its performance and dynamic stability under cutting.Materials and Methods. The development of mathematical models of machining processes using a sensor system and the Industrial Internet of Things is considered. Machine learning methods valid for the implementation of the above tasks are evaluated. A neural network model of dynamic stability of the cutting process is proposed, which enables to optimize the machining process at the stage of work preparation. On the basis of nonlinear dynamics approaches, the attractors of the dynamic cutting system are reconstructed, and their fractal dimensions are determined. Optimal characteristics of the equipment are selected by input parameters and debugging of the planned process based on digital twins.Research Results. Using machine learning methods allowed us to create and explore neural network models of technological systems for cutting, and the software for their implementation. The possibility of applying decision trees for the problem of diagnosing and classifying malfunctions of CNC machines is shown.Discussion and Conclusions. In real production, the technology of digital twins enables to optimize processing conditions considering the technical and dynamic state of CNC machines. This provides a highly accurate assessment of the production capacity of the enterprise under the development of the production program. In addition, equipment failures can be identified in real time on the basis of the intelligent analysis of the distributed sensor system data",'FSFEI HE Don State Technical University',Development of digital twin of CNC unit based on machine learning methods,,10.23947/1992-5980-2019-19-1-45-55,"[{'title': 'Vestnik of Don State Technical University', 'identifiers': ['issn:1992-6006', '1992-6006', '1992-5980', 'issn:1992-5980']}]",core
334779427,2019-01-25T00:00:00,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype",,Hybrid Artificial Intelligence System for the Design of Highly-Automated Production Systems,https://core.ac.uk/download/334779427.pdf,10.14279/depositonce-10583,"[{'title': 'Procedia Manufacturing', 'identifiers': ['issn:2351-9789', '2351-9789']}]",core
334844867,2019-08-06T00:00:00,"Compute and memory constraints have historically prevented traffic simulation
software users from fully utilizing the predictive models underlying them. When
calibrating car-following models, particularly, accommodations have included 1)
using sensitivity analysis to limit the number of parameters to be calibrated,
and 2) identifying only one set of parameter values using data collected from
multiple car-following instances across multiple drivers. Shortcuts are further
motivated by insufficient data set sizes, for which a driver may have too few
instances to fully account for the variation in their driving behavior. In this
paper, we demonstrate that recent technological advances can enable
transportation researchers and engineers to overcome these constraints and
produce calibration results that 1) outperform industry standard approaches,
and 2) allow for a unique set of parameters to be estimated for each driver in
a data set, even given a small amount of data. We propose a novel calibration
procedure for car-following models based on Bayesian machine learning and
probabilistic programming, and apply it to real-world data from a naturalistic
driving study. We also discuss how this combination of mathematical and
software tools can offer additional benefits such as more informative model
validation and the incorporation of true-to-data uncertainty into simulation
traces.Comment: IEEE 22nd Intelligent Transportation Systems Conference, ITSC 2019, 2
  figures, 1 tabl",,"Strengthening the Case for a Bayesian Approach to Car-following Model
  Calibration and Validation using Probabilistic Programming",http://arxiv.org/abs/1908.02427,,,core
334862103,2019-09-23T00:00:00,"In the industrial domain, the pose estimation of multiple texture-less shiny
parts is a valuable but challenging task. In this particular scenario, it is
impractical to utilize keypoints or other texture information because most of
them are not actual features of the target but the reflections of surroundings.
Moreover, the similarity of color also poses a challenge in segmentation. In
this article, we propose to divide the pose estimation process into three
stages: object detection, features detection and pose optimization. A
convolutional neural network was utilized to perform object detection.
Concerning the reliability of surface texture, we leveraged the contour
information for estimating pose. Since conventional contour-based methods are
inapplicable to clustered metal parts due to the difficulties in segmentation,
we use the dense discrete points along the metal part edges as semantic
keypoints for contour detection. Afterward, we exploit both keypoint
information and CAD model to calculate the 6D pose of each object in view. A
typical implementation of deep learning methods not only requires a large
amount of training data, but also relies on intensive human labor for labeling
the datasets. Therefore, we propose an approach to generate datasets and label
them automatically. Despite not using any real-world photos for training, a
series of experiments showed that the algorithm built on synthetic data perform
well in the real environment",,"Pose Estimation for Texture-less Shiny Objects in a Single RGB Image
  Using Synthetic Training Data",http://arxiv.org/abs/1909.10270,,,core
478869506,2019-01-01T00:00:00,"2019 annual report for the Blue Waters ProjectNSF OCI-0725070NSF ACI-123899314.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence and Dynamos in Star-Forming Molecular Clouds and a New Paradigm for Computational Astrophysics for Spherical Systems
16.	Adam Burrows, The Computational Keys to the Supernova Puzzle:  How Multiple 3D Radiation/Hydrodynamic Models Can Unlock the Supernova Mystery
18.	Manuela Campanelli, Shedding Light on Supermassive Binary Black Hole Mergers
20.	Manuela Campanelli, Accretion Dynamics of Supermassive Black Hole Binaries
22.	Matias Carrasco Kind, Achieving Probabilistic Classification of Cosmic Web Particles Using Rapidly Generated Training Data:  A Method for Classifying Galaxies into Their Cosmic Web Structural Groups Using Supervised Machine Learning
24.	Tiziana Di Mattteo, The Epoch of the First Luminous Black Holes:  Evolving the Blue Tides Simulation into the First Billion Years of Cosmic History
26.	Jerry Draayer, Advancing First-Principle Symmetry-Guided Nuclear Modeling for Studies of Nucleosynthesis and Fundamental Symmetries in Nature
28.	Charles F. Gammie, Magnetized Models of Giant Impacts
30.	Nickolayk Gnedin, Cosmic Reionization on Computers
32.	John Hawley, Elucidating the Alignment Mechanism for Black Hole Accretion Disks Subjected to Lense-Thirring Torques
34.	Philip F. Hopkins, Understanding the Origins of the Stars and Galaxies in our Universe
36.	Eliu Huerta, Deep Learning at Scale for the Construction of Galaxy Catalogs with the Dark Energy Survey
38.	Eliu Huerta, Characterization of Numerical Relativity Waveforms of Eccentric Binary Black Hole Mergers
40.	Eiu Huerta, Fusing Numerical Relativity and Deep Learning to Detect Eccentric Binary Black Hole Mergers Using Higher-Order Waveform Multipoles
42.	Athol J. Kemball, Data-and Compute-Intensive Challenges for Observational Astronomy in the Great Survey Era
44.	Deborah A. Levin, Modal Decompositions of Shock Interactions
46.	Deborah A. Levin, Plume Plasma Spacecraft Interactions
48.	Yi-Hsin Liu, The Spreading of Three-Dimensional Magnetic Reconnection in Asymmetric Geometry
50.	Felipe Menanteau, Assembling a Map of the Universe:  Shapes and Mass Distribution for the Dark Energy Survey
52.	Philipp Mosta, Petascale Simulations of Binary Neutron Star Mergers
54.	Michael L. Norman, Development of a Scalable Gravity Solver for Enzo-E
56.	Brian O’Shea, Simulating Galaxy Formation Across Cosmic Time
58.	Donald Petravick, Processing Dark Energy Camera Data to Make the World’s Best Map of the Night Sky
60.	Nikolai Pogorelov, Coupling the Solar Wind and Local Interstellar Medium in the Era of the New Horizons, Interstellar Boundary Explorer, Parker Solar Probe, Ulysses, and Voyager Spacecraft
62.	Jane Pratt, Interior Dynamics of Young Stars Revealed by 3D Hydrodynamic Simulations
64.	Thomas Quinn, Modeling of Galaxy Populations
66.	Paul Ricker, Effects of Active Galaxy Feedback on the Intracluster Medium
68.	Stuart Shapiro, Gravitational and Electromagnetic Signatures from Binary Black Hole-Neutron Star Mergers:  A Jet Engine for Short Gamma-Ray Bursts
70.	Alexander Tchekhovskoy, Feeding Black Holes:  Tilt with a Twist
72.	Gabor Toth, Scaling the BATS-R-US MHD Model to Over 100,000 Cores with Efficient Hybrid OpenMP and MPI Parallelization
74.	Mathew Turk, Numerical Study on the Fragmentation Condition in a Primordial Accretion Disk
75.	Saul A. Teukolsky, Merging Black Holes and Neutron Stars
78.	Jennifer Corcoran, Image Processing to Build a Multitemporal Vegetation Elevation Ecosystem Model of the Great Lakes Basin
80.	Larry Di Girolamo, Petascale Processing of Satellite Earth Observations
82.	Chunyuan Diao, Large-Scale Remote Monitoring of Invasive Species Dynamics Through a Petascale High-Performance Computing System
84.	Francina Domiguez, Deforestation of the Amazon Forest:  Understanding Hydroclimate Impacts by Tracing the Water that Evaporates from the Forest
86.	Patricia M. Gregg, Forecasting Volcanic Unrest and Eruption Potential Using Statistical Data Assimilation
88.	Kaiyu Guan, Monitoring Field-Scale Crop Water Use Using a Satellite Data-Driven Mechanistic Modeling Approach
90.	Kaiyu Guan, Building an Objective Seasonal Forecasting System for U.S. Corn and Soybean Yields
92.	Sonia Lasher-Trapp, Inflow and Outflow from Thunderstorms:  Tracking Their Influence on Precipitation and Further Growth
94.	Paul Morin, Petascale Polar Topography Production
96.	Stephen W. Nesbitt, High-Resolution Numerical Simulations of Convection Initiation over the Sierras de Cordoba Mountains in Argentina
98.	Leigh G. Orf, Simulations of Violently Tornadic Supercells and Damaging Thunderstorms
100.	Nikolaos Pavlis, Prediction of Geomagnetic Secular Variation with Large-Ensemble Geomagnetic Data Assimilation
102.	Nicole Riemer, Machine Learning for Error Quantification in Simulating the Climate Impacts of Atmospheric Aerosols
104.	Clay Tabor, Simulating Hydroclimate Change in Southwest North America at 21,000 Years Ago
106.	Robert J. Trapp, Implementation and Use of a Global Nonhydrostatic Model for Extended-Range Weather Prediction during the RELAMPAGO Field Campaign
108.	John Vidale, Simulating Large California Earthquakes Before They Occur
110.	Renata Wentzcovitch Materials Simulations in Geophysics
112.	Mathew West, Simulating Aerosol Impacts on Climate, One Particle at a Time:  A Regional-Scale, Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
114.	Donald J. Wuebbles, Evolving Air Quality Under the Changing Climate
116.	Xiangdong Zhang, Sensitivity of Arctic Sea Ice Thickness Distribution to Sea Ice Internal Dynamics in a Changing Climate
120.	Narayana R. Aluru, The Mechanism of Proton Diffusion in ABO3 Perovskite Oxides
122.	Narayana R. Aluru, Identification of Amino Acids with Sensitive Nanoporous MoS2:  Toward Machine Learning-Based Prediction
124.	Narayana R Aluru, Transfer-Learning-Based Course-Graining Method for Simple Fluids:  Toward Deep Inverse Liquid-State Theory
126.	Guillermo Araya, High-End Visualization of Coherent Structures and Turbulent Events in Wall-Bounded Flows with a Passive Scalar
128.	Jerzy Bernholc, Design of Atomically Precise Nanoscale Negative Differential Resistance Devices
130.	Daniel Bodony, Using OpenMP Offloading to Run Code on Blue Waters’ GPU Nodes
132.	Christoph Brehm, Numerical Investigation of Turbulence Suppression in Rotating Flows
134.	Oliver M. F. Browne, An Efficient Method for Hypersonic Laminar-Turbulent Transition Prediction
136.	Carlo Pierleoni, Quantum Simulations:  Properties of Dense Hydrogen
138.	Huck Beng Chew, Role of Interfaces on the Shear Strength and Bending Properties of vander Waals Two-Dimensional Materials
140.	Bryan Clark, Atypically Entangled Phases and New Methods for the Quantum Many-Body Problem
142.	Lian Duan, Direct Numerical Simulation of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
144.	Aida X. El-Khadra, The Anomalous Magnetic Moment of the Muon:  An Improved Ab Initio Calculation of the Hadronic Vacuum Polarization Contribution
146.	Elif Ertekin, Accelerating Thermoelectric Materials Discovery via Dopability Predictions
148.	Jonathan Freund, Machine-Learning Turbulence Models for Simulations of Turbulent Combustion
150.	Marcelo Garcia, Turbulence-Resolving Modeling of Oscillatory Boundary Layer Flows
152.	Benjamin Hooberman, Machine Learning for Particle Physics:  Employing Deep Learning for Particle Identification and Measurement of Colliders
154.	Kathryn Huff, Molten-Salt Reactors and Their Fuel Cycles
156.	Prashant K. Jain, A Novel Crystal Structure with Spin-Protected Surface Electronic Conduction
158.	Eric Johnsen, Inertial Collapse of Individual Bubbles near Solid/Free Boundaries
160.	Harley T. Johnson, Electronic Structure of Microscale Dielectric Barrier Discharges
162.	Seid Koric, Accelerating Virtual Prototyping and Certification in the Aerospace Industry with Scalable Finite-Element Analysis
164.	Jean-Pierre Leburton, Graphene Nanopore Transistor for DNA-Nick Detection
166.	Farzad Mashayek, Compressibility Effects on Spatially Developing Plane Free Shear Layer
168.	Moshe Matalon, Outwardly Propagating Turbulent Flames
170.	Mark Neubauer, Deep Learning for Higgs Boson Identification and Searches for New Physics at the Large Hadron Collider
172.	Rajib Rahman, Designing Quantum Logic Gates on Silicon Chips with Large-Scale Multiphysics Simulations
174.	Venkat Raman, Simulation of Rotating Detonation Engines
176.	Caroline Riedl, Mapping Proton Quark Structure:  Looking Inside the Proton-How do Quarks Spin?
178.	Andre’ Schleife, Electron Dynamics of Ion-Irradiated Two-Dimensional Materials
180.	Andre’ Schleife, Discovery of New Plasmonic Materials via High-Throughput Machine Learning
182.	Brian G. Thomas, Turbulent Multiphase Thermal Flow Modeling of Defect Formation Mechanisms and Electromagnetic Force Effects in Continuous Steel Casting
184.	Rafael Tinoco Lopez, Investigation of Sediment Transport Through Aquatic Vegetation Using Large-Scale High-Fidelity Turbulence Simulations
186.	Kimani Toussaint, Machine Learning-Assisted High-Throughput Computational Design of Solvents for Liquid-Exfoliation
188.	Dallas R. Trinkle, High-Throughput Materials Modeling Optimization
190.	Lela Vukovic, Detecting Neurotransmitters with DNA-Wrapped Nanotube Sensors
192.	Lucas Wagner, Accurate Effective Interactions in Quantum Materials
194.	Zhi Jian Wang, Supersonic Jet Noise Prediction Using High-Order Large-Eddy Simulation
196.	Bin Xu, Spin Spirals in Multiferroic Bismuth Ferrite and at Metal Surface:  From Fully First Principles
198.	Zhen Xu, Numerical Simulations of a Collapsing Cavitation Bubble Near an Elastically Deformable Object
200.	Yonghua Yan, Numerical Study on Shock Wave-Boundary Layer Interaction and Its Control
202.	Jinhui Yan, Free-Surface Flow Modeling of Multiple Tidal Turbines
204.	Pui-Kuen Yeung, New Insights on Intermittency and Circulation Statistics Obtained From a Massive Turbulence Simulation Database
206.	Yang Zhang, Effects of Surface Defects on Hydrophobicity at Rare-Earth Oxide Interfaces Using Molecular Dynamics Simulations Driven by Ab Initio-Based Deep Neural Network Potentials
208.	Phiala Shanahan, Constraining the Properties and Interactions of Dark Matter
212.	Donna Cox, Cinematic Scientific Data Visualization for CADENS
214.	Iwan Duursma, The Structure and Statistics of Reed-Muller Codes
216.	William Gropp, A Parallel Framework for Scaling Phylogeny Estimation Methods to Large Genomic Data Sets
218.	William Gropp, Algorithms for Extreme-Scale Systems
220.	Ravishankar Iyer, Kaleidoscope:  Live Forensics for Large-Scale Data Center Storage Systems
222.	Shantenu Jha, Extensible and Scalable Adaptive Sampling to Fold Proteins on Supercomputers
224.	Luke Olson, Improved Scalability Through Node-Aware Communicators
226.	Luke Olson, Scalable Line and Plane Solvers
228.	Marc Snir, Detection of Silent Data Corruptions Using Machine Learning	
230.	Edgar Solomonik, Pushing the Boundaries of Large-Scale Tensor Computations
232.	Tandy Warnow, Algorithms for Large-Scale Evolutionary Tree Construction:  Improving Scalability and Accuracy through Divide-and-Conquer
234.	Justin Sirigano, HPC Development of Deep Learning Models in Scientific Computing and Finance
235.	David Taflin, Optimization of a Field Data Parallel Output Library
238.	Aleksei Aksimentiev, Resolving the Structure of Bacteriophage HK97 with Atomistic Resolution
240.	Aleksei Aksimentiev, A Nanopore System for Single-Molecule Protein Sequencing
242.	Aleksei Aksimentiev, Dynamic Interactions Between Lipid-Tethered DNA and Phospholipid Membranes
244.	Rommie Amaro, Influence Virulence and Transmissibility Through the Computational Microscope
246.	Rafael C. Bernardi, How Blue Waters Is Aiding the Fight Against Sepsis
248.	Gustavo Caetano-Annoles, A Phylogenomic History of Protein Function and Dynamics
250.	Colleen E. Clancy, Predicting Drug-Induced Cardiac Arrhythmias Using Atomistic Simulations
252.	Julie Dickerson, MRNA Isoform Prediction
254.	Ken Dill, Petascale Integrative Approaches to Protein Structure Prediction
256.	Andrew Ferguson, Discovery of Slow Kinetic Modes from Molecular Simulation Trajectories
258.	Mattia Gazzola, Harnessing Viscous Streaming in Complex Active Systems:  Minibots in Fluids
260.	Jodi A. Hadden-Perilla, Molecular Dynamics Simulations of HBV Capsid
262.	Jodi A. Hadden-Perilla, Molecular Dynamics Simulations of the HBV Capsid as a Drug Target
264.	So Hirata, Toward Predictive Computational Design of Precision Molecular Optoelectronics
266.	Mathew E. Hudson, Impact of Batch Effect and Study Design Biases on Identification of Genetic Risk Factors in Sequencing Data
268.	Tao Jiang, Microscopic Identification of PIP2 Binding Sites on a Ca2+-activated Cl- Channel
270.	Fatemah Khalili-Araghi, Paracellular Ion Transport
272.	David LeBauer, The TERRA Phenotyping Refence Platform:  Open Data and Software for Precision Field Crop Measurement and Analysis
274.	Nancy Makri, Quantum-Classical Path Integral Simulation of Proton Translocation in Biological Channels
276.	Arif Masud, A New Stabilized Fluid-Structure Interaction Method:  Coupled System of Anisotropic Viscoelastic Model for Artery and Non-Newtonian Model for Blood
278.	Jeffery S. Moore, Atomic Scale Simulation of Amyloid Beta with Dismantling Peptide-Based Inhibitors
280.	Mahmoud Moradi, Transport Mechanism of POT Transporters:  Employing Loosely Coupled Molecular Dynamics Simulations to Characterize Protein Structural Dynamics
282.	Mahmoud Moradi, Activation Mechanisms of the Mechanosensitive Channel of Large Conductance:  Employing Loosely Coupled Molecular Dynamics Simulations to Characterize Protein Structural Dynamics
284.	Juan Perilla, Molecular Mechanisms of Infection by Chlamydia
286.	Joseph R. Peterson, Calibrating the SimBioSys TumorScope for the Fight on Cancer:  A Scenario Analysis Engine for Determining Optimal Therapy Choice
288.	Kimberly Prather, Investigating the Climate-Relevant Impacts of Chemical Complexity in Marine Aerosols
290.	Benoit Roux, Molecular Dynamics Binding Free Energy Calculations Offer a Window to Understand Protein-Protein Binding Specificity
292.	Diwakar Shukla, Molecular Basis of the Nitrate Transport Mechanism in Plants
294.	Diwakar Shukla, Simulations Uncover the Mechanism of Serotonin Transport in the Brain
296.	Diwakar Shukla, Elucidating the Ligand Selectivity and Activation Mechanisms of Cannabinoid Receptors
298.	Ivan Soltesz, Full-Scale Biophysical Modeling of Hippocampal Networks During Spatial Navigation
300.	Marcos Sotomayor, Desmosomal Cadherins Beating Under Tension
302.	Ashok Srinivasan, Simulation of Viral Infection Propagation During Air Travel
304.	Brad Sutton, MRI-Based Biomarkers Through High-Performance Computing
306.	Emad Tajkhorshid, Mechanobiology:  Using Blue Waters to Decipher the Physical Principles of Protein Mechanics
308.	Emad Tajkhorshid, Atomistic Simulations of a Protocell
310.	Emad Tajkhorshid, Modeling of a Zika Virus Envelope at Atomic Resolution
312.	Gregory Voth, Multiscale Simulations of Complex Self-Assembling Biomolecules:  Targeting HIV-1
314.	Victor Anisimov, Improving the Agreement of AMBER Simulation of Crystals of Nucleic Acid Bases with Experimental Data
315.	Mohammed El-Kebir, Algorithms for Cancer Phylogenetics
316.	Taras V. Pogorelov, Amphotericin-Driven Sterol Extraction:  Probing the Mechanism
320.	Yongyang Cai, Climate Policy in a Dynamic Stochastic Economy
322.	J. Stephen Downie, Characterizing Descriptivity in Writing through Text Analysis of Books from the HathiTrust Digital Library
324.	Mao Ye, High-Frequency Trading in Nanoseconds:  Analysis, Modeling, and Policy Implications
328.	Wendy K. Tam Cho, A Massively Parallel Evolutionary Markov Chain Monte Carlo Algorithm for Sampling Spatial State Spaces
332.	Eizabeth Agee, The Contributions of Root Systems to Drought Response in the Amazon Rainforest
334.	Elaad Applebaum, Star Formation in Dwarf Galaxies:  Using Simulations to Identify Key Observables to Test Models
336.	Katelyn Barber, Improving Convectively Induced Turbulence Forecast Parameters Through Bulk Numerical Simulations for Aviation Safety
338.	Maureen T. Brooks, Modeling Nonlinear Physical-Biological Interactions:  Inertia and Sargassum in the North Atlantic
340.	Iryna Butsky, Predictions About the Invisible Gas in Galaxy Clusters
342.	Robert Cieri, Computational Fluid Dynamics Investigation into Pulmonary Airflow Patterns in Monitor  Lizards(Varanidae)
344.	Mathew Clement, The Early Instability Scenario for Planet Formation in the Solar System
346.	Salme Cook, The Distribution of Shear Stress and Nutrients in a Tidally Energetic Estuary:  The Role of Numerical Resolution and Vegetation
348.	Andrew Emerick, Exascale Astrophysics with Enzo-E:  Development of Physics Modules for Galaxy-Scale and Cosmology Simulations
350.	Forrest Glines, Magnetohydrodynamic Simulation:  Galaxies
352.	Alexander Gurvich, GPU-Accelerated Interstellar Chemistry with WIND:  A General Ordinary Differential Equation Solver
254.	Jennifer M. Hayes, Using Spectroscopic Data and Molecular Simulations to Estimate Heterogeneous Ensembles:  How to Study Complicated, Flexible Proteins When Experimental Data Are Limited
356.	Joshua Lansford, Electron Density-Based Machine Learning for Accelerating Quantum Calculations
358.	Kara Marsac, Extending the Longevity of Produced Water Disposal Wells:  Evaluation Using Reactive Transport Simulation
360.	Nicole Rosato, Improved Trumpet Initial Lapse and Shift for Binary Black Hole Simulations
361.	Shanna Chu, Understanding the Physical Processes Causing Intermediate-Depth Earthquakes
362.	Micheline Soley, Escaping From an Ultracold Inferno:  The Ultracold KRb Dimer Reaction
364.	Ronald Stenz, The Impacts of Hydrometeor Centrifuging on Tornado Dynamics:  Improving the Realism of Tornado Simulations
366.	Darius Teo, Unraveling Functional Hole Hopping Pathways in The [Fe4S4]-Containing DNA Primase
368.	Walter Torres, The Transport and Dynamics of Wave-Driven Reef Jets Under the Influence of Rotation and Bottom Friction
370.	Samuel Whitman, Simulation of Bluff Body Stabilized Flames with PeleC:  Adaptively Resolving Turbulence-Combustion Interactions in Real-World Engineering ProblemsNot Peer ReviewedOpe",,Blue Waters 2019 Annual Report,,,,core
237669847,2019-01-01T00:00:00,"Free text and hand-written reports are losing ground to digitization fast, however many hours of effort are still lost across the industry to the manual creation and analysis of these data types. Work orders in particular contain valuable information from failure rates to asset health, but at the same time present operators with such analytical difficulties and lack of structure that many are missing out on the value completely. This research challenges the current mainstream practice of manual work order analysis by presenting a methodology fit for today’s context of efficiency and digitization.  A prototype text mining software for work order analysis was developed and tested in a user-oriented approach in cooperation with industrial partners. The final prototype combines classical machine learning methods, such as hierarchical clustering, with the operator’s expert knowledge obtained via an active learning approach. A novel distance metric in this context was adapted from information-theoretical research to improve clustering performance.  Using the prototype tool in a case study with real work order data, analytical effort for certain datasets was reduced by 90% - from two working weeks to a day. In addition, the active learning framework resulted in an approach that end users described as ""practical"" and ""intuitive"" during testing. An in-depth review was also conducted regarding the uncertainty of the results – a key factor for implementation in a decision-making context.  The outcomes of this work showcase the potential of machine learning to drive the digitization of not only new installations, but also older assets, where as a result the large amount of unstructured historical data becomes an advantage rather than a hindrance. User testing results encourage a wider uptake of machine learning solutions in the industry, and particularly a shift towards more accessible in-house analytical capabilities",'Society of Petroleum Engineers (SPE)',Work orders - Value from structureless text in the era of digitisation,,10.2118/195788-MS,,core
228122835,2019-08-30T00:00:00,"Reimagining our future engagement with learners through an augmented reality (AR) lens offers a range of possibilities, on a continuum from rigid materials, created with generic learning outcomes, to learner-centred, personalised and emancipatory practice. Educational paradigms are shifting to include alternatives to physical classrooms and the controlled virtual learning spaces that support traditional content delivery. Augmented reality is part of a broader mixed reality where varying degrees of virtual enhancement to the real world can be integrated into traditional delivery practice but also allow learning spaces to be explored more imaginatively and collaboratively. For educators, there are significant challenges to utilise the potential of technology to meet the increasing demands of students, institutions, industry and the expectations of society. By carefully scaffolding educators into reframing their curricula to encourage, inspire and motivate a diverse student body, technology can act as a mediator – a proxy for what Vygotsky termed the ‘more capable peer’ (Cook 2010). AR applications in education, training, marketing, medicine and other industries demonstrate both the capability of the technology and the need for designers to be aware of the possibilities. Karakus et al (2019), in their bibliometric study of augmented reality in education, identify the work by Wu et al (2013) as the most influential across the sector. This paper suggests that AR not only bridges virtual and real worlds but alsocreates an enhanced reality through a creative process. They argue that the educational values of AR are not solely based on the use of technologies but are closely related to how AR is designed, implemented and integrated into formal and informal learning settings. The recent McKinsey report (Bughin et al, 2018) modelled skills shifts in automation and artificial intelligence going forward to 2020 and found a sharp acceleration in demand for these technologies that will transform the workplace, as humans work with ever smarter machines. Transferring technology for educational use, re-use and re-purpose are key emergent themes in research as employers demand ever more technology enabled graduates, with high-level cognitive skills",'Springer Science and Business Media LLC',Augmented Reality For Education,,10.1007/978-981-13-2262-4_120-1,,core
395096924,2019-11-01T00:00:00,"During these years of my Ph.D. studies the main aim of the research work was to improve the efficiency on energy generation into industrial facilities. 



Novelties are proposed both on the devices used for energy generation and on energy consumption data analytics. In the first part of the thesis, Solid Oxide Fuel Cell (SOFC) and Reversible Solid Oxide Cell (RSOC) are proposed: these technologies have many advantages such as high efficiency on energy generation, heat available at high temperature, and modularity. 



A new heat recovery for a modular micro-cogeneration system based on SOFC is presented with the main goal of improving the efficiency of an air source heat pump with unused heat of fuel cell exhausted gases. The novelty of the system proposed is that exhaust gases after the fuel cell are firstly used to heat water and/or used to produce steam, then they are mixed with the external air to feed the evaporator of the heat pump with the aim of increasing energy efficiency of the latter. This system configuration decreases the possibility of freezing of the evaporator as well, which is one of the drawbacks for air source heat pump in climates where temperature close to 0 °C and high humidity could occur. Results show that the performance of the air source heat pump increases considerably during cold season for climates with high relative humidity and for users with high electric power demand. 



As previously cited, not only SOFC but also RSOC are deeply analysed in the thesis to define innovative energy generation system with the possibility of varying H/P ratio to match energy generation and demand in order to avoid mismatching and, consequently, integration system with a lower system. The aim is to define a modular system where each RSOC module can be switched between energy generation mode (fuel consumption to produce electricity and heat) and energy consumption (electricity and heat are consumed to produce hydrogen, working as Solid Oxide Electrolysis Cells) to vary overall H/P of the overall system. Hydrogen is a sub-product of the system and can be used for many purposes such as fuel and/or for transport sector. Then a re-vamping of the energy generation system of a paper mill by means of RSOCS is proposed and analysed: a real industrial facility, based in Italy with a production capacity of 60000 t/y of paper, is used as case study. Even if the complexity of the system increases, results show that saving between 2% and 6% occurs. Hydrogen generation is assessed, comparing the RSOC integrated system with PEM electrolysis, in terms of both primary energy and economics. Results exhibit significant primary energy and good economic performance on hydrogen production with the novel system proposed. 



In the thesis novelties are proposed not only on energy system “hardware” (component for energy generation) but also on “software”. In the second part of the thesis, artificial intelligence and machine learning methods are analysed to perform analytics on energy consumption data and consequently to improve performances on energy generation and operation strategy.  



A study on how cluster analysis could be applied to analyse energy demand data is depicted. The aim of the method is to design cogeneration systems that suit more efficiently energy demand profiles, choosing the correct type of cogeneration technology, operation strategy and, if they are necessary, energy storages. A case study of a wood industry that requires low temperature heat to dry wood into steam-powered kilns that already uses cogeneration is proposed to apply the methodology in order to design and measure improvements. An alternative cogeneration system is designed and proposed, thermodynamics benchmarks are defined to evaluate differences between as-is and alternative scenarios. Results show that the proposed innovative method allows to choose a more suitable cogeneration technology compared to the adopted one, giving suggestions on the operation strategy in order to decrease energy losses and, consequently, primary energy consumption.  



Finally, clustering is suggested for short-term forecasting of energy demand in industrial facilities. A model based on clustering and kNN is proposed to find similar pattern of consumption, to identify average consumption profiles, and then to use them to forecast consumption data. Novelties on model parameters definition such as data normalisation and clustering hyperparameters are presented to improve its accuracy. The model is then applied to the energy dataset of the wood industry previously cited. Analysis on the parameters and the results of the model are performed, showing a forecast of electricity demand with an error of 3%",,Energy efficiency in industrial facilities - Improvements on energy transformation and data analysis,,,,core
288617485,2019-10-28T19:31:26,"Scheduling is an important problem in artificial intelligence and operations research. In production processes, it deals with the problem of allocation of resources to different tasks with the goal of optimizing one or more objectives. Job shop scheduling is a classic and very common scheduling problem. In the real world, shop environments dynamically change due to events such as the arrival of new jobs and machine breakdown. In such manufacturing environments, uncertainty in shop parameters is typical. It is of vital importance to develop methods for effective scheduling in such practical settings.

Scheduling using heuristics like dispatching rules is very popular and suitable for such environments due to their low computational cost and ease of implementation. For a dynamic manufacturing environment with varying shop scenarios, using a universal dispatching rule is not very effective. But manual development of effective dispatching rules is difficult, time consuming and requires expertise. Genetic programming is an evolutionary approach which is suitable for automatically designing effective dispatching rules. Since the genetic programming approach searches in the space of heuristics (dispatching rules) instead of building up a schedule, it is considered a hyper-heuristic approach.

Genetic programming like many other evolutionary approaches is computationally expensive. Therefore, it is of vital importance to present the genetic programming based hyper-heuristic (GPHH) system with scheduling problem instances which capture the complex shop scenarios capturing the difficulty in scheduling. Active learning is a related concept from machine learning which concerns with effective sampling of those training instances to promote the accuracy of the learned model.

The overall goal of this thesis is to develop effective and efficient genetic programming based hyper-heuristic approaches using active learning techniques for dynamic job shop scheduling problems with one or more objectives.

This thesis develops new representations for genetic programming enabling it to incorporate the uncertainty information about processing times of the jobs. Furthermore, a cooperative co-evolutionary approach is developed for GPHH which evolves a pair of dispatching rules for bottleneck and non-bottleneck machines in the dynamic environment with uncertainty in processing times arising due to varying machine characteristics. The results show that the new representations and training approaches are able to significantly improve the performance of evolved dispatching rules.

This thesis develops a new GPHH framework in order to incorporate active learning methods toward sampling DJSS instances which promote the evolution of more effective rules. Using this framework, two new active sampling methods were developed to identify those scheduling problem instances which promoted evolution of effective dispatching rules. The results show the advantages of using active learning methods for scheduling under the purview of GPHH.

This thesis investigates a coarse-grained model of parallel evolutionary approach for multi-objective dynamic job shop scheduling problems using GPHH. The outcome of the investigation was utilized to extend the coarse-grained model and incorporate an active sampling heuristic toward identifying those scheduling problem instances which capture the conflict between the objectives. The results show significant improvement in the quality of the evolved Pareto set of dispatching rules.

Through this thesis, the following contributions have been made. (1) New representations and training approaches for GPHH  to incorporate uncertainty information about processing times of jobs into dispatching rules to make them more effective in a practical shop environment. (2) A new GPHH framework which enables active sampling of scheduling problem instances toward evolving dispatching rules effective across complex shop scenarios.  (3) A new active sampling heuristic based on a coarse-grained model of parallel evolutionary approach for GPHH for multi-objective scheduling problems",'Victoria University of Wellington Library',Active Learning Methods for Dynamic Job Shop Scheduling using Genetic Programming under Uncertain Environment,,,,core
334881302,2019-11-12T00:00:00,"Static program analysis today takes an analytical approach which is quite
suitable for a well-scoped system. Data- and control-flow is taken into
account. Special cases such as pointers, procedures, and undefined behavior
must be handled. A program is analyzed precisely on the statement level.
However, the analytical approach is ill-equiped to handle implementations of
complex, large-scale, heterogeneous software systems we see in the real world.
Existing static analysis techniques that scale, trade correctness (i.e.,
soundness or completeness) for scalability and build on strong assumptions
(e.g., language-specificity). Scalable static analysis are well-known to report
errors that do *not* exist (false positives) or fail to report errors that *do*
exist (false negatives). Then, how do we know the degree to which the analysis
outcome is correct?
  In this paper, we propose an approach to scale-oblivious greybox program
analysis with bounded error which applies efficient approximation schemes
(FPRAS) from the foundations of machine learning: PAC learnability. Given two
parameters $\delta$ and $\epsilon$, with probability at least $(1-\delta)$, our
Monte Carlo Program Analysis (MCPA) approach produces an outcome that has an
average error at most $\epsilon$. The parameters $\delta>0$ and $\epsilon>0$
can be chosen arbitrarily close to zero (0) such that the program analysis
outcome is said to be probably-approximately correct (PAC). We demonstrate the
pertinent concepts of MCPA using three applications:
$(\epsilon,\delta)$-approximate quantitative analysis,
$(\epsilon,\delta)$-approximate software verification, and
$(\epsilon,\delta)$-approximate patch verification.Comment: 10+2 pages. Feedback and (industry/research) collaborations welcom",,MCPA: Program Analysis as Machine Learning,http://arxiv.org/abs/1911.04687,,,core
237689453,2019-01-01T00:00:00,"Asset optimization has recently become a crucial issue in Oil&amp;Gas industry, considering oil price conjuncture and an increased awareness on environmental aspects. In this paper, an Artificial Intelligence (AI) technique is presented, which is able to manage big dataset to automatically match the entire production model against measured field data. The tool is based on a hybrid in-house developed AI technique, integrating deep neural networks, biogenetical algorithms, commercial simulators and real-time data. The workflow starts with the modeling of the production system through physics-based commercial simulators. A sensitivity analysis identifies the critical variables, which are then randomly varied with a Sobol distribution, exploring the entire solution domain. With these data, a proxy model to the commercial software is generated using an artificial neural network. Finally, the AI tool fed by real-time data is used to match the field behavior: uncertain parameters are modified through a differential evolution algorithm that minimizes the error between calculated and measured variables. The matching parameters are, then, passed to the simulators achieving a field representative model. The tool has been developed considering an operating field in offshore western Africa. The typical uncertain parameters in this kind of field are related to the fluid characteristics, in particular densities and compositions, but also to the physical characterization of the pipelines such as roughness and heat transfer characteristics. The matching process has been performed coupling the proxy model, which is a neural network able to replicate the field behavior, and a differential evolution algorithm as the optimization algorithm. The fitness function to be minimized is a Mean Absolute Percentage Error (MAPE) that represents the distance between the actual field production parameters and the modelled ones. The best configuration of both the neural network and the differential evolution algorithm required a computational time of 6 seconds with a MAPE equal to 2.6%. These results are compared to the one obtained coupling the same differential evolution algorithm with the commercial simulator to perform the matching. The required computational time is equal to about 20 hours (70400s) and a MAPE equal to 2.2%. The big gain with the novel approach is clearly the knocking down of computational time with a comparable error. In this paper, it has been shown how substituting the physical model with a proxy one can give substantial advantages in terms of computational time. In principle, with the velocity of the tool implemented, the matching procedure could be done on a daily basis. This is a breakthrough because it allows having the simulator model always tuned and ready to be utilized. © Copyright 2018, Society of Petroleum Engineers",'Society of Petroleum Engineers (SPE)',Hybrid Artificial Intelligence Techniques for Automatic Simulation Models Matching with Field Data,,10.2118/193080-MS,,core
326329672,2019,"Complexity is a fundamental part of product design and manufacturing today, owing to increased demands for customization and advances in digital design techniques. Assembling and repairing such an enormous variety of components means that workers are cognitively challenged, take longer to search for the relevant information and are prone to making mistakes. Although in recent years deep learning approaches to object recognition have seen rapid advances, the combined potential of deep learning and augmented reality in the industrial domain remains relatively under explored. In this paper we introduce AR-ProMO, a combined hardware/software solution that provides a generalizable assistance system for identifying mistakes during product assembly and repair",,Handling Work Complexity with AR/Deep Learning,,10.1145/3369457.3370919,,core
267812972,2019-08-01T00:00:00,"Text in EnglishIn recent years, there have been tremendous advances in information technology, robotics, communication technology, nanotechnology, and artificial intelligence, resulting in the merging  of physical, digital, and biological worlds that have come to be known as the ""fourth industrial revolution”. In this context, the present study engages such technology in the green economy and to tackle the techno-economic environmental impact assessments challenges associated with floating solar system applications in the agricultural sector of South Africa. In response, this exploratory study aimed to examine the development of a Geographical Information System (GIS)-based support platform for Environmental Impact Assessment (EIA) and due-diligence analyses for future planned agricultural floating solar systems, especially with the goal to address the vast differences between the environmental impacts for land-based and water-based photovoltaic energy systems.
A research gap was identified in the planning processes for implementing floating solar
systems in South Africa’s agricultural sector. This inspired the development of a novel GIS-based modelling tool to assist with floating solar system type energy infrastructure planning in the renewable energy discourse. In this context, there are significant challenges and future research avenues for technical and environmental performance modelling in the new sustainable energy transformation. The present dissertation and geographical research ventured into the conceptualisation, designing and development of a software GIS-based decision support tool to assist environmental impact practitioners, project owners and landscape architects to perform environmental scoping and environmental due-diligence analysis for planned floating solar systems in the local agricultural sector. In terms of the aims and objectives of the research, this project aims at the design and development of a dedicated GIS toolset to determine the environmental feasibility around the use of floating solar systems in agricultural applications in South Africa. In this context, the research objectives of this study included the use of computational modelling and simulation techniques to theoretically determine the energy yield predictions and computing environmental impacts/offsets for future planned agricultural floating solar systems in South
Africa. The toolset succeeded in determining these aspects in applications where floating
solar systems would substitute Eskom grid power. The study succeeded in developing a
digital GIS-based computer simulation model for floating solar systems capable of (a) predicting the anticipated energy yield, (b) calculating the environmental offsets achieved by substituting coal-fired generation by floating solar panels, (c) determining the environmental impact and land-use preservation benefits of any floating solar system, and (d) relating these metrics to water-energy-land-food (WELF) nexus parameters suitable for user project viability analysis and decision support. The research project has demonstrated how the proposed GIS toolset supports the body of geographical knowledge in the fields of Energy and Environmental Geography. The new toolset, called EIAcloudGIS, was developed to assist in solving challenges around
energy and environmental sustainability analysis when planning new floating solar installations on farms in South Africa. Experiments conducted during the research showed how the geographical study in general, and the toolset in particular, succeeded in solving a real-world problem. Through the formulation and development of GIS-based computer simulation models embedded into GIS layers, this new tool practically supports the National Environmental Management Act (NEMA Act No. 107 of 1998), and in particular, associated EIA processes. The tool also simplifies and semi-automates certain aspects of environmental impact analysis processes for newly envisioned and planned floating solar installations in South Africa.GeographyM.Sc. (Geography",,Development of a GIS-based decision support tool for environmental impact assessment and due-diligence analyses of planned agricultural floating solar systems,https://core.ac.uk/download/267812972.pdf,,,core
326824637,2019-01-01T00:00:00,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan",'Institute of Electrical and Electronics Engineers (IEEE)',Real-time wireless monitoring for three phase motors in industry: a cost-effective solution using IoT,,10.1109/ICSIMA47653.2019.9057343,,core
220155640,2019-01-01T00:00:00,"Adequate testing of AI applications is essential to ensure their quality. However, it is often prohibitively difficult to generate realistic test cases or to check software correctness. This paper proposes a new method called datamorphic testing, which consists of three components: a set of seed test cases, a set of datamorphisms for transforming test cases, and a set of metamorphisms for checking test results. With an example of face recognition application, the paper demonstrates how to develop datamorphic test frameworks, and illustrates how to perform testing in various strategies, and validates the approach using an experiment with four real industrial applications of face recognition",,Datamorphic testing: A method for testing intelligent applications,https://core.ac.uk/download/220155640.pdf,,,core
287584333,2019-12-09T00:00:00,"Today, with the emergence of the industry revolu- tion systems such as Industry 4.0, Internet of Things and big data frameworks pose new challenges in terms of storage and processing of real time data. As systems scale in humongous sizes, a crucial task is to administer the variety of different sub-systems and applications to ensure high performance. This is directly related with the identification and elimination of system failures and errors, while the system runs. In particular, database systems, may experience abnormalities related with decreased throughput or increased resource usage, that in turn affects system performance. In this work, we focus on NoSQL database systems, that are ideal for storing sensor data in the concept of Industry 4.0. This typically includes a variety of applications and workloads that are difficult to online monitor, thus making anomaly detection a challenging task. Creating a robust platform to serve such infrastructures with minimum hardware or software failures is a key challenge. In this work, we propose RADAR, an anomaly detection system that works on real-time. RADR is a data-driven decision-making system for NoSQL systems by providing process information extraction during resource monitoring and by associating resource usage with the top processes, to identify anomalous cases. In this work, we focus on anomalies such as hardware failures or software bugs that could lead to abnormal application runs, without necessarily stopping system functionality e.g. due to a system crash, but by affecting its performance e.g. decreased database system throughput. Although, different patterns may occur through time, we focus on periodic running workloads (e.g. monitoring daily usage) that are very common for NoSQL systems, and Internet of Things scenarios where data streams are forwarded to the Cloud for storage and processing. We apply various machine learning algorithms such as autoregressive integrated moving average (ARIMA), seasonal ARIMA and long-short-term memory recurrent neural networks. We experimentally analyse our solution to demonstrate the benefits of supporting online erroneous state identification and characterisation for modern applications",'Institute of Electrical and Electronics Engineers (IEEE)',Real time anomaly detection of NoSQL systems based on resource usage monitoring,,10.1109/TII.2019.2958606,,core
288551532,2019,"In proposing a machine learning approach for a flow shop scheduling problem with alternative resources, sequence-dependent setup times, and blocking, this paper seeks to generate a tree-based priority rule in terms of a well-performing decision tree (DT) for dispatching jobs. Furthermore, generating a generic DT and RF that yields competitive results for instance scenarios that structurally differ from the training instances was another goal of our research. The proposed DT relies on high quality solutions, obtained using a constraint programming (CP) formulation. Novel aspects include a unified representation of job sequencing and machine assignment decisions, as well as the generation of random forests (RF) to counteract overfitting behaviour. To show the performance of the proposed approaches, different instance scenarios for two objectives (makespan and total tardiness minimisation) were implemented, based on randomised problem data. The background of this approach is a real-world physical system of an industrial partner that represents a typical shop floor for many production processes, such as furniture and window construction. The results of a comparison of the DT and RF approach with two priority dispatching rules, the original CP solutions and tight lower bounds retrieved from a strengthened mixed-integer programming (MIP) formulation show that the proposed machine learning approach performs well in most instance sets for the makespan objective and in all sets for the total tardiness objective.© The Author(s) 201",'Springer Science and Business Media LLC',"A machine learning approach for flow shop scheduling problems with alternative resources, sequence-dependent setup times, and blocking",,10.1007/s00291-019-00567-8,"[{'title': None, 'identifiers': [' 0171-6468', 'ISSN: 0171-6468']}]",core
427382360,2019-09-19T00:00:00,"Driven by the demand to accommodate today’s growing mobile traffic, 5G is designed to

be a key enabler and a leading infrastructure provider in the information and communication technology

industry by supporting a variety of forthcoming services with diverse requirements. Considering the everincreasing

complexity of the network, and the emergence of novel use cases such as autonomous cars,

industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML)

is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential

solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised,

unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of

ML in the context of mobile and wireless communication, organizing the literature in terms of the types of

learning.We then discuss the promising approaches for how ML can contribute to supporting each target 5G

network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have

on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G),

providing future research directions for how ML can contribute to realizing B5G. This article is intended

to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of

autonomous 5G/B5G mobile and wireless communications",'Institute of Electrical and Electronics Engineers (IEEE)',"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions",,10.1109/ACCESS.2019.2942390,,core
288353473,2019-07-16T15:11:34,"Supplementary Information Files for ""Monitoring Potato Waste in Food Manufacturing Using Image Processing and Internet of Things Approach""Abstract:Approximately one-third of the food produced globally is spoiled or wasted in the food supply chain (FSC). Essentially, it is lost before it even reaches the end consumer. Conventional methods of food waste tracking relying on paper-based logs to collect and analyse the data are costly, laborious, and time-consuming. Hence, an automated and real-time system based on the Internet of Things (IoT) concepts is proposed to measure the overall amount of waste as well as the reasons for waste generation in real-time within the potato processing industry, by using modern image processing and load cell technologies. The images captured through a specially positioned camera are processed to identify the damaged, unusable potatoes, and a digital load cell is used to measure their weight. Subsequently, a deep learning architecture, specifically the Convolutional Neural Network (CNN), is utilised to determine a potential reason for the potato waste generation. An accuracy of 99.79% was achieved using a small set of samples during the training test. We were successful enough to achieve a training accuracy of 94.06%, a validation accuracy of 85%, and a test accuracy of 83.3% after parameter tuning. This still represents a significant improvement over manual monitoring and extraction of waste within a potato processing line. In addition, the real-time data generated by this system help actors in the production, transportation, and processing of potatoes to determine various causes of waste generation and aid in the implementation of corrective actions. </div",,"Supplementary Information Files for ""Monitoring Potato Waste in Food Manufacturing Using Image Processing and Internet of Things Approach""",,10.17028/rd.lboro.8879552.v1,,core
334839005,2019-07-22T00:00:00,"Most state-of-the-art person re-identification (re-id) methods depend on
supervised model learning with a large set of cross-view identity labelled
training data. Even worse, such trained models are limited to only the
same-domain deployment with significantly degraded cross-domain generalization
capability, i.e. ""domain specific"". To solve this limitation, there are a
number of recent unsupervised domain adaptation and unsupervised learning
methods that leverage unlabelled target domain training data. However, these
methods need to train a separate model for each target domain as supervised
learning methods. This conventional ""{\em train once, run once}"" pattern is
unscalable to a large number of target domains typically encountered in
real-world deployments. We address this problem by presenting a ""train once,
run everywhere"" pattern industry-scale systems are desperate for. We formulate
a ""universal model learning' approach enabling domain-generic person re-id
using only limited training data of a ""{\em single}"" seed domain. Specifically,
we train a universal re-id deep model to discriminate between a set of
transformed person identity classes. Each of such classes is formed by applying
a variety of random appearance transformations to the images of that class,
where the transformations simulate the camera viewing conditions of any domains
for making the model training domain generic. Extensive evaluations show the
superiority of our method for universal person re-id over a wide variety of
state-of-the-art unsupervised domain adaptation and unsupervised learning re-id
methods on five standard benchmarks: Market-1501, DukeMTMC, CUHK03, MSMT17, and
VIPeR",,Universal Person Re-Identification,http://arxiv.org/abs/1907.09511,,,core
421630821,2019-01-01T00:00:00,"Today, recommendation algorithms are widely used by companies in multiple sectors with the aim of increasing their profits or
offering a more specialized service to their customers. Moreover, there are countless applications in which classification algorithms
are used, seeking to find patterns that are difficult for people to detect or whose detection cost is very high. Sometimes, it
is necessary to use a mixture of both algorithms to give an optimal solution to a problem. .is is the case of the ADAGIO, a R&D
project that combines machine learning (ML) strategies from heterogeneous data sources to generate valuable knowledge based
on the available open data. In order to support the ADAGIO project requirements, the main objective of this paper is to provide a
clear vision of the existing classification and recommendation ML systems to help researchers and practitioners to choose the best
option. To achieve this goal, this work presents a systematic review applied in two contexts: scientific and industrial. More than a
thousand papers have been analyzed resulting in 80 primary studies. Conclusions show that the combination of these two
algorithms (classification and recommendation) is not very used in practice. In fact, the validation presented for both cases is very
scarce in the industrial environment. From the point of view of software development life cycle, this review also shows that the
work being done in the ML (for classification and recommendation) research and industrial environment is far from earlier stages
such as business requirements and analysis. .is makes it very difficult to find efficient and effective solutions that support real
business needs from an early stage. It is therefore that the article suggests the development of new ML research lines to facilitate its
application in the different domains.Ministerio de Economía y Competitividad  TIN2016-76956-C3-2-RCentro para el Desarrollo Tecnológico Industrial P106-16/E09Agencia Estatal de Investigación MTM2017-86875-C3-2-RJunta de Extremadura GR1810",'Hindawi Limited',Recommendation and Classification Systems: A Systematic Mapping Study,,,,core
195567356,2019-01-01T00:00:00,"The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software devel- opment in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source commu- nities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Arti- ficial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.Álvaro López García, Ignacio Heredia, Giang Nguyen, Viet Tran, Stefan Dlugolinsky,

Martin Bobák, and Ladislav Hluchý are supported by the project DEEP-HybridDataCloud “Designing and Enabling E-infrastructures for intensive Processing in a Hybrid DataCloud” that has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreement No. 777435.

Giang Nguyen, Viet Tran, Stefan Dlugolinsky, Martin Bobák, and Ladislav Hluchý are also supported by the Project VEGA 2/0167/16 “Methods and algorithms for the semantic processing of Big Data in distributed computing environment”. The authors would like to thanks to all colleagues, especially for Ján Astaloš for

knowledge sharing and teamwork.Peer reviewe",'Japanese Society of Applied Entomology & Zoology',Machine learning and deep learning frameworks and libraries for large-scale data mining: A survey,,10.13039/501100000780,"[{'title': 'Artificial Intelligence Review', 'identifiers': ['issn:1573-7462', 'issn:0269-2821', '0269-2821', '1573-7462']}]",core
334840073,2019-10-22T00:00:00,"Training deep learning models is compute-intensive and there is an
industry-wide trend towards hardware specialization to improve performance. To
systematically benchmark deep learning platforms, we introduce ParaDnn, a
parameterized benchmark suite for deep learning that generates end-to-end
models for fully connected (FC), convolutional (CNN), and recurrent (RNN)
neural networks. Along with six real-world models, we benchmark Google's Cloud
TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep
dive into TPU architecture, reveal its bottlenecks, and highlight valuable
lessons learned for future specialized system design. We also provide a
thorough comparison of the platforms and find that each has unique strengths
for some types of models. Finally, we quantify the rapid performance
improvements that specialized software stacks provide for the TPU and GPU
platforms",,"Benchmarking TPU, GPU, and CPU Platforms for Deep Learning",http://arxiv.org/abs/1907.10701,,,core
334884086,2019-11-18T00:00:00,"Machine learning models are vulnerable to adversarial inputs that induce
seemingly unjustifiable errors. As automated classifiers are increasingly used
in industrial control systems and machinery, these adversarial errors could
grow to be a serious problem. Despite numerous studies over the past few years,
the field of adversarial ML is still considered alchemy, with no practical
unbroken defenses demonstrated to date, leaving PHM practitioners with few
meaningful ways of addressing the problem. We introduce turbidity detection as
a practical superset of the adversarial input detection problem, coping with
adversarial campaigns rather than statistically invisible one-offs. This
perspective is coupled with ROC-theoretic design guidance that prescribes an
inexpensive domain adaptation layer at the output of a deep learning model
during an attack campaign. The result aims to approximate the Bayes optimal
mitigation that ameliorates the detection model's degraded health. A
proactively reactive type of prognostics is achieved via Monte Carlo simulation
of various adversarial campaign scenarios, by sampling from the model's own
turbidity distribution to quickly deploy the correct mitigation during a
real-world campaign.Comment: International Journal of Prognostics and Health Management, Special
  Issue: PHM Applications of Deep Learning and Emerging Analytics, 201",,Deep Detector Health Management under Adversarial Campaigns,http://arxiv.org/abs/1911.08090,,,core
286704229,2019-01-01T00:00:00,"Over the last decade, the advanced driver assistance system (ADAS) and autonomous driving research have grown rapidly. The entire automotive industry is looking forward to autonomous vehicles and ADAS technologies. Fully autonomous driving by the automobile model year 2021/2022 with security level 4 or 5 requires the use of multiple heterogeneous sensors' system. Automotive sensors, such as camera, millimeter (mmWave) radar and lidar, have evolved fast in signal processing for the perception of surroundings. Sensor fusion and deep learning to understand the environment implemented in automobiles are drastically changing the current sensor research. The automotive radar has been served as an essential sensor in the race to develop ADAS and autonomous vehicles. Its affordable price and reliable detection are raising attention from both industry and academia. In 2018, shipments of passenger automotive radars have grown 54 % in units compared to 2017. Another trend is that with camera and radar getting fused, it can provide more reliable ADAS capabilities. In this dissertation, a series of signal processing techniques are studied for improving the resolution and target recognition of mmWave radar. First, a sensor fusion technique for better tracking and detecting targets using mmWave radar and camera is presented. The fusion system takes consideration of error bounds (EBs) of the two different coordinate systems from the heterogeneous sensors, and further designed a new fusion extended Kalman filter (fusion-EKF) to adapt to the two sensors. The details such as synchronization between sensors, multi-target tracking, and association are also considered and illustrated. The experiment shows that the proposed fusion system can realize a range accuracy of 0.29 m with an angular accuracy of 0.013 rad in real-time. Therefore, the proposed fusion system is effective, reliable and computationally efficient for real-time kinematic fusion applications. A clustering method, REDBSCAN, for radar point cloud data is also presented. Secondly, for enhancing target recognition, a neural network is developed for mmWave radar to classify human behavior in real-time. Thirdly, to improve the angular resolution for mmWave radar, a circular synthetic aperture radar MMWCSAR with high-resolution technique, e.g., compressed sensing is presented",The University of Arizona.,"mmWave Radar: Enhancing Resolution, Target Recognition, and Fusion with Other Sensors",,,,core
328792741,2019-01-01T00:00:00,"Impressive growth in the number of wearable health monitoring devices has affected global health industry as they provide rapid and intricate details related to physical examinations, such as discomfort, heart rate, and blood glucose level, which enable doctors to efficiently diagnose sensitive heart troubles. The Internet of Medical Things (IoMT) is a phenomenon wherein computer networks and medical equipment are connected through the Internet to provide real-time interaction between physicians and patients. In this article, we present a comprehensive view of the IoMT and its related Machine Learning (ML)-based developed frameworks designed, or being utilized, in the last decade, i.e., from 2010 to 2019. The presented techniques are designed for monitoring limbs, controlling rural healthcare, identifying e-health applications, monitoring health through mobile apps, classifying heart sounds, detecting stress in drivers, monitoring cardiac diseases, making the decision to predict heart attacks, recognizing human activities, and classifying breast cancer. The aim is to provide a clear picture of the existing IoMT environment so that the analysis may pave the way for the diagnosis of critical disorders such as cancer, heart attack, and blood pressure among others. In the end, we also provide some unresolved challenges that are confronted in the deployment of the secure IoMT-based healthcare systems. - 2013 IEEE.The authors are grateful to the Deanship of Scientific Research, King Saud University for funding through Vice Deanship of Scientific Research Chairs",'Institute of Electrical and Electronics Engineers (IEEE)',A Decade of Internet of Things: Analysis in the Light of Healthcare Applications,,10.1109/ACCESS.2019.2927082,,core
189162755,2019-03-01T00:00:00,"With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the fi eld of Asset Management has moved from  fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must  nd a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, de ned as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they ignore their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares  t of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet.EU H202",'Organisation for Economic Co-Operation and Development  (OECD)',Collaborative prognostics in Social Asset Networks,https://core.ac.uk/download/189162755.pdf,10.17863/CAM.22392,,core
200772010,2019-01-01T00:00:00,"Automatic event detection is of vital importance for real-time microseismic or passive seismic monitoring in the oil and gas industry. With the recent advances in artificial intelligence and computing power, deep learning has now become a reliable tool for automatic microseismic event detection and other problems. We present a novel “classification is detection” strategy by leveraging convolutional neural networks (CNN) based deep learning method for automatic microseismic event detection in low signal-to-noise ration environment. The CNN model is trained using thousands of manually picked and labelled genuine event and noise segments. Validation on additional field data demonstrates the model's capability in recognising microseismic event patterns and distinguishing them from various noises. The trained model can also be deployed in real-time monitoring scenario to provide automated real-time event detection capability",'EAGE Publications',Automatic microseismic event detection using deep learning: a classification is detection method,,10.3997/2214-4609.201900761,,core
443832386,2019-01-01T00:00:00,"Today, recommendation algorithms are widely used by companies in multiple sectors with the aim of increasing their profits or offering a more specialized service to their customers. Moreover, there are countless applications in which classification algorithms are used, seeking to find patterns that are difficult for people to detect or whose detection cost is very high. Sometimes, it is necessary to use a mixture of both algorithms to give an optimal solution to a problem. This is the case of the ADAGIO, a R&D project that combines machine learning (ML) strategies from heterogeneous data sources to generate valuable knowledge based on the available open data. In order to support the ADAGIO project requirements, the main objective of this paper is to provide a clear vision of the existing classification and recommendation ML systems to help researchers and practitioners to choose the best option. To achieve this goal, this work presents a systematic review applied in two contexts: scientific and industrial. More than a thousand papers have been analyzed resulting in 80 primary studies. Conclusions show that the combination of these two algorithms (classification and recommendation) is not very used in practice. In fact, the validation presented for both cases is very scarce in the industrial environment. From the point of view of software development life cycle, this review also shows that the work being done in the ML (for classification and recommendation) research and industrial environment is far from earlier stages such as business requirements and analysis. This makes it very difficult to find efficient and effective solutions that support real business needs from an early stage. It is therefore that the article suggests the development of new ML research lines to facilitate its application in the different domains",'Hindawi Limited',Recommendation and Classification Systems: A Systematic Mapping Study,,10.1155/2019/8043905,"[{'title': 'Scientific Programming', 'identifiers': ['1058-9244', 'issn:1058-9244', '1875-919x', 'issn:1875-919X']}]",core
270200593,2019-01-01T08:00:00,"Trespassing is the leading cause of rail-related deaths and has been on the rise for the past 10 years. Detection of unsafe trespassing of railroad tracks is critical for understanding and preventing fatalities. Witnessing these events has become possible with the widespread deployment of large volumes of surveillance video data in the railroad industry. This potential source of information requires immense labor to monitor in real time. To address this challenge this paper describes an artificial intelligence (AI) framework for the automatic detection of trespassing events in real time. This framework was implemented on three railroad video live streams, a grade crossing and two right-of-ways, in the United States. The AI algorithm automatically detects trespassing events, differentiates between the type of violator (car, motorcycle, truck, pedestrian, etc.) and sends an alert text message to a designated destination with important information including a video clip of the trespassing event. In this study, the AI has analyzed hours of live footage with no false positives or missed detections yet. This paper and its subsequent studies aim to provide the railroad industry with state-of-the-art AI tools to harness the untapped potential of an existing closed-circuit television infrastructure through the real-time analysis of their data feeds. The data generated from these studies will potentially help researchers understand human factors in railroad safety research and give them a real-time edge on tackling the critical challenges of trespassing in the railroad industry",'SAGE Publications',Artificial Intelligence-Aided Automated Detection of Railroad Trespassing,,10.1177/0361198119846468,,core
217419140,2019-06-04T01:44:35,"Throughout the history of oil well drilling, service providers have been continuously striving to improve performance and reduce total drilling costs to operating companies. Despite constant improvement in tools, products, and processes, data science has not played a large part in oil well drilling. With the implementation of data science in the energy sector, companies have come to see significant value in efficiently processing the massive amounts of data produced by the multitude of internet of thing (IOT) sensors at the rig. The scope of this project is to combine academia and industry experience to analyze data from 13 different wells drilled in an area of 2 x 4 miles. The data was collected in the same rig and contains over 12 million electronic drilling recorder data points, driller’s activity logs and well profiles. The main focus is to propose a detailed workflow to clean and process real drilling data. Once cleaned, the data can be fed into data analytics platforms and machine learning models to efficiently analyze trends and plan future well more efficiently. This roadmap will serve as a basis for drilling optimization. The objective of this work is to detail the various steps needed to prepare field drilling data for business analysis, as well discuss about data analytics and machine learning application in drilling operations. The results to be presented are the detailed workflow and description of the data preparation steps, an example analysis of the drilling data and an example application of a machine learning model in drilling",LSU Digital Commons,Field Drilling Data Cleaning and Preparation for Data Analytics Applications,https://core.ac.uk/download/217419140.pdf,,,core
293746602,2019-01-01T00:00:00,"Fast and energy efficient processing of data has always been a key requirement in processor design. The latest developments in technology emphasize these requirements even further.
The widespread usage of mobile devices increases the demand of energy efficient solutions. Many new applications like advanced driver assistance systems focus more and more on machine learning algorithms and have to process large data sets in hard real time.
Up to the 1990s the increase in processor performance was mainly achieved by new and better manufacturing technologies for processors. That way, processors could operate at higher clock frequencies, while the processor microarchitecture was mainly the same. At the beginning of the 21st century this development stopped. New manufacturing technologies made it possible to integrate more processor cores onto one chip, but almost no improvements were achieved anymore in terms of clock frequencies. This required new approaches in both processor microarchitecture and software design. Instead of improving the performance of a single processor, the current problem has to be divided into several subtasks that can be executed in parallel on different processing elements which speeds up the application.

One common approach is to use multi-core processors or GPUs (Graphic Processing Units) in which each processing element calculates one subtask of the problem. This approach requires new programming techniques and legacy software has to be reformulated.
Another approach is the usage of hardware accelerators which are coupled to a general purpose processor. For each problem a dedicated circuit is designed which can solve the problem fast and efficiently. The actual computation is then executed on the accelerator and not on the general purpose processor. The disadvantage of this approach is that a new circuit has to be designed for each problem. This results in an increased design effort and typically the circuit can not be adapted once it is deployed.

This work covers reconfigurable hardware accelerators. They can be reconfigured during runtime so that the same hardware is used to accelerate different problems. During runtime, time consuming code fragments can be identified and the processor itself starts a process that creates a configuration for the hardware accelerator. This configuration can now be loaded and the code will then be executed on the accelerator faster and more efficient.
A coarse grained reconfigurable architecture was chosen because creating a configuration for it is much less complex than creating a configuration for a fine grained reconfigurable architecture like an FPGA (Field Programmable Gate Array). Additionally, the smaller overhead for the reconfigurability results in higher clock frequencies.
One advantage of this approach is that programmers don't need any knowledge about the underlying hardware, because the acceleration is done automatically during runtime. It is also possible to accelerate legacy code without user interaction (even when no source code is available anymore).

One challenge that is relevant for all approaches, is the efficient and fast data exchange between processing elements and main memory.
Therefore, this work concentrates on the optimization of the memory interface between the coarse grained reconfigurable hardware accelerator and the main memory. To achieve this, a simulator for a Java processor coupled with a coarse grained reconfigurable hardware accelerator was developed during this work.
Several strategies were developed to improve the performance of the memory interface. The solutions range from different hardware designs to software solutions that try to optimize the usage of the memory interface during the creation of the configuration of the accelerator.
The simulator was used to search the design space for the best implementation. With this optimization of the memory interface a performance improvement of  22.6% was achieved.

Apart from that, a first prototype of this kind of accelerator was designed and implemented on an FPGA to show the correct functionality of the whole approach and the simulator",,"Optimization of the Memory Subsystem of a Coarse
Grained Reconfigurable Hardware Accelerator",,,,core
289956463,2019,"This thesis describes the development of an autonomous weeding vehicle for the agricultural industry. It has been done as a project at RMIT University in Melbourne and the aim was to develop a design for the vehicle which could later be used to develop a commercial product. The current trend in the agricultural industry is larger machines that can benefit from economies of scale. These heavy machines are causing three major problems for farmers; serious subsoil compaction, longer disruptions due to single vehicle failure and an inability to efficiently deal with weeds which have resulted in a rapid increase of herbicide resistant weeds. This project is an attempt at solving these problems by integrating the already existing stationary farming robot FarmBot with a lightweight, cost efficient, aesthetically pleasing and durable frame on wheels that can operate autonomously on farms and is being powered by solar energy. The project started by researching the agricultural industry and benchmarking similar products, which gave an understanding of the problem and how it could best be solved. Since many different parts had to be developed, it was decided that the complex problem was to be divided into several sub problems which were then solved parallel with each other. The different concepts were evaluated and a final product was developed visually using CAD. The finished vehicle was primarily made out of parts that can be ordered off the shelves as well as parts that can be 3D printed. It weighs 154 kg, the parts costs around USD 5300 and it is equipped to be able to operate autonomously for at least a day at a time. It is very easy to assemble and has a modular design, which simplifies further development.Smarter, more advanced farming methods are needed to stop the rapid increase of herbicide resistant weeds and overuse of herbicides. Autonomous farming robots are the future of farming, and by using machine learning and advanced image analysis a robot is being developed that can target specific weeds and exterminate them in the most optimal way, reducing herbicide usage by up to 90%. The current farming methods are in a dire need of a revolution. Due to broadacre spraying weeds are starting to become resistant to herbicides at an alarming rate, the run-off from herbicide usage has severe consequences for the environment and heavy machinery is reducing the crop yield for farmers due to soil compaction. In Melbourne, Australia a small team is working on disrupting the industry by creating a lightweight autonomous farming robot that can target the weeds and exterminate them by using the optimal amount and type of herbicide. The foundation for this robot is the open source CNC farming robot FarmBot that can take care of a garden plot without human interaction. By using the intuitive web-based interface people can set up their plot and control the robot through most devices. It can plant seeds, measure the soil moisture level, exterminate weeds, water the plants and even notify you when they’re ripe for picking. It navigates by using a coordinate system, which means it can perform its tasks with millimeter accuracy. The FarmBot is a stationary robot, but if you strip it of non-essential parts and functions you have an inexpensive and lightweight robot that which can be mounted on a frame capable of autonomously navigating a farm. This is exactly what was envisioned by Hormoz Marzbani, lecturer at Royal Melbourne Institute of Technology when he first set eyes on the FarmBot. By utilizing an already existing product the project got a jumpstart, and a first prototype was quickly built to be able to test out the steering and image analysis. When it was decided that the prototype worked well, it was time for the next phase of the project, the design phase. Master students Lisa Ralsgård and Simon Axbom, Lund University, were then put in charge of developing a complete design for the robot in a CAD software. Numerous autonomous weeding robots have been developed, but they all have one thing in common, they are all very expensive. The goal of this project was to exploit that business opportunity so it was important that the robot was inexpensive to manufacture. During a 20-week period concepts were continuously generated and evaluated to be able to design a robot that would meet all the project requirements as well as the requirements from the farmers. In the end they managed to come up with a solar powered design that enabled the robot to operate autonomously without needing to be refill herbicide for more than 30 hours. With a slimmed down design it weighs only 154 kg and the total cost for parts is less than $5500. By only using off the shelf parts, except for the cover, all the parts could essentially be ordered online and assembled by anyone. The next phase of the project is to order all the parts and assemble them into a working prototype. The design has been tested theoretically, but it’s time to see how well it performs in the real, harsh agricultural environment. If the next prototype phase goes smoothly, a new modern way of managing weeds is close on the horizon",Lunds universitet/Produktutveckling,Design of an Autonomous Weeding Vehicle,,,,core
286393031,"October 28, 2019","Automated planning is a key Artificial Intelligence technology enabling Unmanned Aerial Systems (UAS) and the eminent reality of Urban Air Mobility (UAM). It produces plans, which formalize procedures often performed by humans. Plans differ from other kinds of computer programs in their ability to react and interact with a dynamically changing environment. Aviation plans must encode the procedural knowledge, reasoning capability, and capacity for multi-tasking held by competent human pilots. Correct execution of these plans (performed by software called an executive) in the dynamic airspace environment is vital to the success of each automated flight, and the safety of the vehicle and all things in its path. In the early 2000s NASA developed a plan representation language and executive called PLEXIL (Plan Execution Interchange Language) that has successfully been applied in several NASA aviation and UAS projects. Autonomy Operating System (AOS), Cockpit Hierarchical Automated Planning and Execution (CHAP-E), and ICAROUS are all projects that have used PLEXIL to help encode and automatically execute flight procedures, some normally performed by human pilots. AOS also automates a subset of pilot/Air Traffic Control communication towards enabling UAS entry into the National Airspace. PLEXIL has been open-source software since 2008 and has seen usage in a wide range of prototypical autonomy applications in academia, government, and industry. In this presentation, we describe PLEXIL and highlight its significant accomplishments in the aviation domain",,Overview of the PLEXIL Plan Execution Technology and its Applications in Autonomous Piloting Projects at NASA,https://core.ac.uk/download/pdf/286393031.pdf,,,core
334844340,2019-08-01T00:00:00,"In this paper we present DELTA, a deep learning based language technology
platform. DELTA is an end-to-end platform designed to solve industry level
natural language and speech processing problems. It integrates most popular
neural network models for training as well as comprehensive deployment tools
for production. DELTA aims to provide easy and fast experiences for using,
deploying, and developing natural language processing and speech models for
both academia and industry use cases. We demonstrate the reliable performance
with DELTA on several natural language processing and speech tasks, including
text classification, named entity recognition, natural language inference,
speech recognition, speaker verification, etc. DELTA has been used for
developing several state-of-the-art algorithms for publications and delivering
real production to serve millions of users.Comment: White paper for an open source library:
  https://github.com/didi/delta. 13 pages, 3 figure",,DELTA: A DEep learning based Language Technology plAtform,http://arxiv.org/abs/1908.01853,,,core
231954004,2019-08-29T00:00:00,"The paper aims to organize and structure data collected and associated to technologies that powers the abroad concept of Industry 4.0. It starts with the historic evolution of industry, separated by date landmarks and approaches the last transition between 3.0 to 4.0. Apart from the differences between industry models, production data stats show a huge and important transformation in the amount of data related to manufacturing and how that knowledge is processed. The paper also aims to put on debate the lack of solutions regarding the knowledge extraction of data from machines and systems, needed for data analytics. Approaches with cyber-physical systems, machine learning, virtual environments, Industrial IoT 1 and augmented reality, in an
industrial scale, are some of the strategies to power the reading and interpretation of data, in order to promote industrial efficiency.
Real context industrial applications are taken into account in order to state the importance of collected data in the efficiency of a production process. Exploring technologies and concepts to improve digital twins systems, perception and perceived systems as well as maintenance processes are some of the explored implemented strategies that make Industry 4.0. Some possible strategies are presented, as well as the transition for Industry 5.0.publishe",'Association for Computing Machinery (ACM)',Industry focused in data collection: how industry 4.0 is handled by big data,https://core.ac.uk/download/231954004.pdf,10.1145/3352411.3352414,,core
200818222,2019-03-28T00:00:00,"Software systems trained via machine learning to automatically classify
open-ended answers (a.k.a. verbatims) are by now a reality. Still, their
adoption in the survey coding industry has been less widespread than it might
have been. Among the factors that have hindered a more massive takeup of this
technology are the effort involved in manually coding a sufficient amount of
training data, the fact that small studies do not seem to justify this effort,
and the fact that the process needs to be repeated anew when brand new coding
tasks arise. In this paper we will argue for an approach to building verbatim
classifiers that we will call ""Interactive Learning"", and that addresses all
the above problems. We will show that, for the same amount of training effort,
interactive learning delivers much better coding accuracy than standard
""non-interactive"" learning. This is especially true when the amount of data we
are willing to manually code is small, which makes this approach attractive
also for small-scale studies. Interactive learning also lends itself to reusing
previously trained classifiers for dealing with new (albeit related) coding
tasks. Interactive learning also integrates better in the daily workflow of the
survey specialist, and delivers a better user experience overall.Comment: To appear in the International Journal of Market Researc",,Building Automated Survey Coders via Interactive Machine Learning,http://arxiv.org/abs/1903.12110,,,core
200844605,2019-05-28T00:00:00,"There are hardly any data sets publicly available that can be used to
evaluate intrusion detection algorithms. The biggest threat for industrial
applications arises from state-sponsored and criminal groups. Often, formerly
unknown exploits are employed by these attackers, so-called 0-day exploits.
They cannot be discovered with signature-based intrusion detection. Thus,
statistical or machine learning based anomaly detection lends itself readily.
These methods especially, however, need a large amount of labelled training
data. In this work, an exemplary industrial use case with real-world industrial
hardware is presented. Siemens S7 Programmable Logic Controllers are used to
control a real world-based control application using the OPC UA protocol: A
pump, filling and emptying water tanks. This scenario is used to generate
application specific network data. Furthermore, attacks are introduced into
this data set. This is done in three ways: First, the normal process is
monitored and captured. Common attacks are then synthetically introduced into
this data set. Second, malicious behaviour is implemented on the Programmable
Logic Controller program and executed live, the traffic is captured as well.
Third, malicious behaviour is implemented on the Programmable Logic Controller
while still keeping the same output behaviour as in normal operation. An
attacker could exploit an application but forge valid sensor output so that no
anomaly is detected. Sensors are employed, capturing temperature, sound and
flow of water to create data that can be correlated to the network data and
used to still detect the attack. All data is labelled, containing the ground
truth, meaning all attacks are known and no unknown attacks occur. This makes
them perfect for training of anomaly detection algorithms. The data is
published to enable security researchers to evaluate intrusion detection
solutions",,"Implementing SCADA Scenarios and Introducing Attacks to Obtain Training
  Data for Intrusion Detection Methods",http://arxiv.org/abs/1905.12443,,,core
287943106,2019-01-01T00:00:00,"International audienceThe use of wireless sensor networks, which are the key ingredient in the growing Internet of Things (IoT), has surged over the past few years with a widening range of applications in the industry, healthcare, agriculture, with a special attention to monitoring and tracking, often tied with security issues. In some applications, sensors can be deployed in remote, large unpopulated areas, whereas in others, they serve to monitor confined busy spaces. In either case, clustering the sensor network’s nodes into several clusters is of fundamental benefit for obvious scalability reasons, and also for helping to devise maintenance or usage schedules that might greatly improve the network’s lifetime. In the present paper, we survey and compare popular and advanced clustering schemes and provide a detailed analysis of their performance as a function of scale, type of collected data or their heterogeneity, and noise level. The testing is performed on real sensor data provided by the UCI Machine Learning Repository, using various external validation metrics",'MDPI AG',Introducing and Comparing Recent Clustering Methods for Massive Data Management in the Internet of Things,,,,core
227472636,2019-05-01T00:00:00,"Ancillaries in the travel industry have become a major source of income and profitability. However, conventional pricing strategies are based on poorly optimized business rules that do not respond to changing market conditions.

This study describes the dynamic pricing model that we have developed in conjunction with Deepair solutions, an AI technology provider for travel suppliers. We present a pricing model that provides dynamic pricing recommendations specific to each customer interaction and optimizes expected revenue per customer. The unique nature of personalized pricing provides the opportunity to search over the market space to find the optimal price-point of each ancillary for each customer, without violating customer privacy.

In this study, we present and compare three approaches for dynamic pricing of ancillaries, with increasing levels of sophistication: (1) a two-stage forecasting and optimization model using a logistic mapping function; (2) a two-stage model that uses a deep neural network for forecasting, coupled with a revenue maximization technique using discrete exhaustive search; (3) a single-stage end-to-end deep neural network that recommends the optimal price. We describe the performance of these models based on both offline and online evaluations. We also measure the real-world business impact of these approaches by deploying them in an A/B test on an airline's internet booking website. We show that traditional machine learning techniques outperform human rule-based approaches in an online setting by improving conversion by 36% and revenue per offer by 10%. We also provide results for our offline experiments which show that deep learning algorithms outperform traditional machine learning techniques for this problem. 

Additionally, we propose a meta-learning approach for synchronous deployment of multiple models. This approach is currently under production with our partner airline. Our end-to-end deep learning model is currently being deployed by the airline in their booking system",,Dynamic pricing for airline ancillaries with customer context,,,,core
231787170,2019-01-22T00:00:00,"© 2018 IEEE. Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out",'Institute of Electrical and Electronics Engineers (IEEE)',Ensemble Machine Learning Systems for the Estimation of Steel Quality Control,http://hdl.handle.net/10453/134560,10.1109/BigData.2018.8622583,,core
232115745,2019-08-21T00:00:00,"The evolution of the current most popular mobile network (4G), the so-called 5G, is targeting an increased traffic load at a lower cost. Thus, optimization of the delivery network plays an essential role at 5G; another aspect of the evolution is that 5G has the ambition to be highly customized, e.g., reliable enough to be used in industrial automation and cheap enough to be used for mobile broadband services. In this context, this thesis assesses two aspects of 5G: the first is to use information-centric networking (ICN) to improve the efficiency of multimedia delivery in mobile broadband services; and the second is the application of a reinforcement learning strategy as an enabler for the highly configurable network, which could pose a challenge to be understood and configured manually. ICN aims at circumventing several issues of current internet protocol, among them, achieving a more efficient multimedia distribution. Given the significant growth rate of video transmission over mobile networks, it is sensible to consider how mobile networks can leverage ICN. There is a substantial body of work considering ICN for fixed networks and also for the core of mobile networks. Less attention has been dedicated to ICN on the radio access network (RAN) or ICN-RAN, which has currently a user plane based on many connection-oriented protocols. To fully benefit from ICN, mobile networks must enable it on the RAN, not only on the core. This work details an ICN deployment on the RAN of the fourth and fifth generation of mobile networks and also presents a testbed that enables proofs of concept of this ICN-RAN using 4G. The results indicate, for example, that evolving ICN features can be tested with currently available tools, but the lack of hardware accelerators and optimized code limit the bit rate that can be achieved in real-time processing. In the context of network customization, the most prominent enablers are the so-called network slices. Slices can be understood as a part of the network that is customized to deliver certain services. The service requirements are imposed by the tenant, which acquire slices from an infrastructure provider. The 5G infrastructure provider must optimize the infrastructure resource utilization, usually admitting as many slices as possible. However, infrastructure resources are finite and admitting all the slices could increase the probability of service level agreement violation. This thesis investigates the application of reinforcement learning agents that learn how to increase the infrastructure provider revenue by intelligently admitting network slices that bring the most revenue to the system. We present a neural networks-driven agent for network slice admission that learns the characteristics of the slices deployed by the tenants from their resource requirements profile and balances the benefits of slice admission against orchestration and resource management costs.A evolução das redes móveis mais populares atualmente (4G), as 5G, tem como um dos objetivos suportar aumento de tráfego e ao mesmo tempo diminuir o custo. Assim otimização na entrega de conteúdo é importante para essa nova rede; um outro aspecto é que 5G tem a ambição de ser uma rede altamente adaptável, isto é, ela deve ser confiável o suficiente para ser utilizada em automação industrial e ao mesmo tempo barata o suficiente para ser usada em serviços de banda larga. Nesse contexto, esta tese estuda dois aspectos do 5G, o primeiro é o emprego de redes orientadas a conteúdo (ICN) para melhorar a eficiência de entrega de conteúdo multimídia em serviços de banda larga móvel; o segundo é o desenvolvimento de um agente que utiliza aprendizado por reforço como um facilitador para as novas redes altamente configuráveis, as quais podem se tornar um desafio para serem entendidas e configuradas manualmente. O ICN tem como objetivo circunver vários problemas do atual protocolo de internet, dentre eles, uma entrega de conteúdo mais eficiente. Dado significativa taxa de crescimento de transmissão de vídeos em redes móveis, é sensível avaliar como as redes 4G/5G podem se beneficiar de ICN. Existem muitos trabalhos que avaliam o emprego de ICN em redes fixas e para o núcleo das redes móveis. Menos atenção tem sido dedicada ao emprego de ICN nas redes de acesso a rádio (RAN) ou ICN-RAN. Este trabalho descreve o emprego de ICN na RAN de 4G/5G, e também apresenta uma bancada de testes que permite o desenvolvimento de provas de conceitos usando ICN-RAN em 4G. Os resultados indicam, por exemplo, que a avaliação de diversas funcionalidades de ICN podem ser realizadas, mas que a falta de aceleradores de hardware e código otimizado limitam a taxa de bit que pode ser alcançada em tempo real. No contexto de adaptação da rede, a tecnologia mais promissora é o fatiamento da rede. Fatia de rede pode ser entendida como parte da rede que é personalizada para determinados serviços. Os requisitos de cada serviço são impostos pelo inquilino, o qual adquire fatias do provedor de infraestrutura. O provedor de infraestrutura 5G tem que otimizar a utilização de seus recursos, costumeiramente essa utilização é aumentada ao admitir fatias, porém, os recursos na infraestrutura são finitos e admitir todas as fatias pode aumentar o risco de violação de acordos de prestação de serviços, o que implica em multas que podem diminuir o lucro. Nesta tese, é investigado o uso de um agente treinado por aprendizado por reforço que aprende como aumentar o lucro do provedor de infraestrutura. Tal agente, baseado em redes neurais, aprende as consequências da admissão de fatias na rede baseado no inquilino e no seu perfil de utilização de recursos, aprendendo assim, a balancear os benéficos da admissão em contraste com os custos de orquestração e gerenciamento de recursos",Programa de Pós-Graduação em Engenharia Elétrica,Admissão de fatia de rede usando aprendizado reforçado e redes centradas em informações para redes móveis,,,,core
146473775,2019-08-13T00:00:00,"Everyday robotics are challenged to deal with autonomous product handling in
applications like logistics or retail, possibly causing damage on the items
during manipulation. Traditionally, most approaches try to minimize physical
interaction with goods. However, this paper proposes to take into account any
unintended object motion and to learn damage-minimizing manipulation strategies
in a self-supervised way. The presented approach consists of a simulation-based
planning method for an optimal manipulation sequence with respect to possible
damage. The planned manipulation sequences are generalized to new, unseen
scenes in the same application scenario using machine learning. This learned
manipulation strategy is continuously refined in a self-supervised,
simulation-in-the-loop optimization cycle during load-free times of the system,
commonly known as mental simulation. In parallel, the generated manipulation
strategies can be deployed in near-real time in an anytime fashion. The
approach is validated on an industrial container-unloading scenario and on a
retail shelf-replenishment scenario",'Springer Science and Business Media LLC',"Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via
  Mental Simulation",http://arxiv.org/abs/1712.07452,10.1007/s11370-019-00286-7,,core
322372320,2019-01-01T00:00:00,"It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods. - 2014 IEEE.Manuscript received January 16, 2019; revised April 1, 2019 and
April 12, 2019; accepted April 13, 2019. Date of publication April 18, 2019;
date of current version July 31, 2019. This work was supported by NPRP
through the Qatar National Research Fund (a member of Qatar Foundation)
under Grant NPRP 10-901-2-370. The work of M. A. Teixeira was supported
in part by the São Paulo Research Foundation (FAPESP) under Grant
2017/01055-4 and in part by the Instituto Federal de Educação, Ciência e
Tecnologia de São Paulo. (Corresponding author: Maede Zolanvari.",'Institute of Electrical and Electronics Engineers (IEEE)',Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things,https://core.ac.uk/download/322372320.pdf,10.1109/JIOT.2019.2912022,"[{'title': None, 'identifiers': ['6822-6834', 'issn:6822-6834']}]",core
334875812,2019-11-25T00:00:00,"Artificial Intelligence (AI) systems exert a growing influence on our
society. As they become more ubiquitous, their potential negative impacts also
become evident through various real-world incidents. Following such early
incidents, academic and public discussion on AI ethics has highlighted the need
for implementing ethics in AI system development. However, little currently
exists in the way of frameworks for understanding the practical implementation
of AI ethics. In this paper, we discuss a research framework for implementing
AI ethics in industrial settings. The framework presents a starting point for
empirical studies into AI ethics but is still being developed further based on
its practical utilization.Comment: This paper further discusses the research framework introduced in
  ""Implementing Ethics in AI: Initial results of an industrial multiple case
  study"" Vakkuri, Kemell & Abrahamsson (arXiv:1906.12307",,AI Ethics in Industry: A Research Framework,http://arxiv.org/abs/1910.12695,,,core
200861384,2019-05-01T00:00:00,"In a smart home linked to a smart grid (SG), demand-side management (DSM) has the potential to reduce electricity costs and carbon/chlorofluorocarbon emissions, which are associated with electricity used in today&#8217;s modern society. To meet continuously increasing electrical energy demands requested from downstream sectors in an SG, energy management systems (EMS), developed with paradigms of artificial intelligence (AI) across Internet of things (IoT) and conducted in fields of interest, monitor, manage, and analyze industrial, commercial, and residential electrical appliances efficiently in response to demand response (DR) signals as DSM. Usually, a DSM service provided by utilities for consumers in an SG is based on cloud-centered data science analytics. However, such cloud-centered data science analytics service involved for DSM is mostly far away from on-site IoT end devices, such as DR switches/power meters/smart meters, which is usually unacceptable for latency-sensitive user-centric IoT applications in DSM. This implies that, for instance, IoT end devices deployed on-site for latency-sensitive user-centric IoT applications in DSM should be aware of immediately analytical, interpretable, and real-time actionable data insights processed on and identified by IoT end devices at IoT sources. Therefore, this work designs and implements a smart edge analytics-empowered power meter prototype considering advanced AI in DSM for smart homes. The prototype in this work works in a cloud analytics-assisted electrical EMS architecture, which is designed and implemented as edge analytics in the architecture described and developed toward a next-generation smart sensing infrastructure for smart homes. Two different types of AI deployed on-site on the prototype are conducted for DSM and compared in this work. The experimentation reported in this work shows the architecture described with the prototype in this work is feasible and workable",'MDPI AG',Design and Implementation of Cloud Analytics-Assisted Smart Power Meters Considering Advanced Artificial Intelligence as Edge Analytics in Demand-Side Management for Smart Homes,,10.3390/s19092047,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
197719965,2019,"This paper reports the development of a manipulation system for electric wires, implemented by means of a commercial gripper installed on an industrial manipulator and equipped with cameras and suitably designed tactile sensors. The purpose of this system is the execution of wire insertion on commercial electromechanical components. The synergy between computer vision and tactile sensing is necessary because, in a real environment, the tight spaces very often prevent the possibility to use the vision system, also when the same task is performed by a human being. A novel technique to speed up the generation of training data sets for convolutional neural networks (CNNs) is proposed. Therefore, this technique is used to train a CNN in order to detect small objects (such as wire terminals). Moreover, aiming to prevent faults during the task and to interact with the environment safely, several machine learning approaches are used to produce an affordable output from the tactile sensor. The proposed approach shows how a cheap sensor embedded with suitable intelligence can provide information comparable to a more expensive force sensor",,Integration of Robotic Vision and Tactile Sensing for Wire-Terminal Insertion Tasks,,10.1109/TASE.2018.2847222,,core
236315312,2019-01-01T08:00:00,"Across all industries, from manufacturing to services, decision-makers must deal day to day with the outcomes from past and current decisions that affect their business. Last-mile delivery is the term used in supply chain management to describe the movement of goods from a hub to final destinations. This research proposes a methodology that supports decision making for the execution of last-mile delivery operations in a supply chain. This methodology offers diverse, hybrid, and complementary techniques (e.g., optimization, simulation, machine learning, and geographic information systems) to understand last-mile delivery operations through data-driven decision-making. The hybrid modeling might create better warning systems and support the delivery stage in a supply chain. The methodology proposes self-learning procedures to iteratively test and adjust the gaps between the expected and real performance. This methodology supports the process of making effective decisions promptly, optimization, simulation, and machine learning models are used to support execution processes and adjust plans according to changes in conditions, circumstances, and critical factors. This research is applied in two case studies. The first one is in maritime logistics, which discusses the decision process to find the type of vessels and routes to deliver petroleum from ships to villages. The second is in city logistics, where a network of stakeholders during the city distribution process is analyzed, showing the potential benefits of this methodology, especially in metropolitan areas. Potential applications of this system will leverage growing technological trends (e.g., machine learning in supply chain management and logistics, internet of things). The main research impact is the design and implementation of a methodology, which can support real-time decisions and adjust last-mile operations depending on the circumstances. The methodology allows taking decisions under conditions of stakeholder behavior patterns like vehicle drivers, customers, locations, and traffic. As the main benefit is the possibility to predict future scenarios and plan strategies for the most likely situations in last-mile delivery. This will help determine and support the accurate calculation of performance indicators. The research brings a unified methodology, where different solution approaches can be used in a synchronized form, which allows researches and other interested people to see the connection between techniques. With this research, it was possible to bring advanced technologies in routing practices and algorithms to decrease operating cost and leverage the use of offline and online information, thanks to connected sensors to support decisions",'Information Bulletin on Variable Stars (IBVS)',A Methodology for Data-Driven Decision-Making in Last Mile Delivery Operations,https://core.ac.uk/download/236315312.pdf,,,core
226995571,2019-12-01T00:00:00,"Drilling wells in challenging oil/gas environments implies in large capital expenditure on wellbore's construction. In order to optimize the drilling related operation, real-time decisions making have been put in place, so that prediction of rate of penetration (ROP) with accuracy is essential. Despite many efforts (theoretical and experimental) throughout the years, modeling the ROP as a mathematical function of some key variables is not so trivial, due to the highly non-linearity behavior experienced. Therefore, several researches in the recent years have been proposing to use data-driven models from artificial intelligence field for ROP prediction and optimization.



This paper presents an extensive review of the literature on ROP prediction, especially, with machine learning techniques, as well as how these models can be used to optimize the drilling activities. The ROP models are classified as traditional models (based on physics-models), statistical models (e.g. multiple regression), or machine learning methods. This review enables to see that machine learning techniques can potentially outperform in terms of ROP-prediction accuracy on top of traditional or statistical models. Throughout this work, an extensive analysis of different ways of obtaining ROP models is carried out, concluding with different strategies adopted in literature to perform data-driven model optimization.



Despite the saving potential which can be achieved with real-time optimization based on data-driven ROP models, it is noticeable that there is a lack of implementation of those techniques in the industry, as per literature review. To take a step forward in real implementations, the petroleum industry must be aware that yet no rule of thumb already exists on this specific area, but still, good and very reasonable results can be achieved by following the best practices identified in this review. In addition, the modern practices of machine learning provide promising guidelines for implementing projects in oil and gas industry.



    Previous article in issu",'Elsevier BV',Machine learning methods applied to drilling rate of penetration prediction and optimization - A review,,10.1016/j.petrol.2019.106332,,core
402100536,2019-01-01T08:00:00,"Blockchain is a potentially disruptive and game-changing technology that has created excitement about its potential applications. The agriculture industry in New Zealand is facing increased pressure to be able to accurately track and trace their produce in order to provide higher levels of proof to their customers. This study used a q-methodology approach to examine whether blockchain technology can be the solution to these issues and provides recommendations as to what businesses need to in order to make this a reality. The empirical research revealed four distinct groups within the industry; each with different perspectives of blockchain and its potential. Results also found that while industry experts believe blockchain implementation is inevitable and it will solve the current issues, factors such as high set-up costs and the complexity of technology may be inhibitors. Based on these findings, key recommendations on how the industry should proceed in order to overcome these factors that are preventing adoption are derived.  Further research is suggested on how the challenges of food safety and security may be overcome with emerging technologies such as Blockchain, IoT and AI",AIS Electronic Library (AISeL),Making Sense of Blockchain in Food Supply-Chains,https://core.ac.uk/download/402100536.pdf,,,core
300005449,2019-01-01T00:00:00,"Object recognition and 6D pose estimation are imperative for robots to relate to the real world. However, due to occlusion, clutter and the properties of various objects in a scene, it might be challenging and tedious for a robot to recognize and estimate the 6D pose of objects. Various methods have been presented throughout the years with concern to this topic. However, many of these methods have its set back and limitations. Due to these reasons, rose the motivation to develop a robust and versatile real time system capable of accurate object recognition and 6D pose estimation with respect to the industrial standards. Over the years, with the advancement in technology, computing power have improved drastically. Algorithms, techniques and methods that were once infeasible to implement due to high computational power requirements could now be done with ease. One such implementation is none other than deep learning. It is now the current state-of-the-art technology. Since, this project is in relation with computer vision, the deep learning architecture proposed would be a convolutional neural network. Hence, the dataset used would consist of images. A deep learning framework known as PoseCNN is explored for object recognition and 6D pose estimation capabilities. In this project, a detailed literature review of PoseCNN, as well as comparisons with current approaches, will be reviewed. Finally, the results obtained using the YCB dataset would be presented.Bachelor of Engineering (Electrical and Electronic Engineering",,Object recognition and 6D pose estimation using deep learning,,,,core
301271596,2019-01-01T00:00:00,"The book covers a variety of topics in Information and Communications Technology (ICT) and their impact on innovation and business. The authors discuss various innovations, business and industrial motivations, and impact on humans and the interplay between those factors in terms of finance, demand, and competition. Topics discussed include the convergence of Machine to Machine (M2M), Internet of Things (IoT), Social, and Big Data. They also discuss AI and its integration into technologies from machine learning, predictive analytics, security software, to intelligent agents, and many more. Contributions come from academics and professionals around the world.



Covers the most recent practices in ICT related topics pertaining to technological growth, innovation, and business; Presents a survey on the most recent technological areas revolutionizing how humans communicate and interact; Features four sections: IoT, Wireless Ad Hoc & Sensor Networks, Fog Computing, and Big Data Analytics.(Chapter) The recent advancements in robotic systems set new challenges for robotic simulation software, particularly for planning. It requires the realistic behavior of the robots and the objects in the simulation environment by incorporating their dynamics. Furthermore, it requires the capability of reasoning about the action effects. To cope with these challenges, this study proposes an open-source simulation tool for knowledge-oriented physics-based motion planning by extending The Kautham Project, a C++ based open-source simulation tool for motion planning. The proposed simulation tool provides a flexible way to incorporate the physics, knowledge and reasoning in planning process. Moreover, it provides ROS-based interface to handle the manipulation actions (such as push/pull) and an easy way to communicate with the real robotsPeer Reviewe",'Springer Science and Business Media LLC',A tool for knowledge-oriented physics-based motion planning and simulation,,10.1007/978-3-319-99966-1,,core
305125096,2019-01-01T00:00:00,"In recent years, machine learning (ML) and, more noticeably, deep learning (DL), have be- come increasingly ubiquitous. Applications of these technologies are being seen in many fields, including health care, manufacturing, and end-consumer services. In terms of deployment, deep neural networks (DNNs) are found in consumer devices, small internet-of-things devices, embedded in vehicles, and on a large scale in data centers and servers. The trend indicates that the use of DL in smart applications will continue to increase in the coming years.As the name suggests, learning is an integral part of the functionality of DNNs, whether this learning takes place off-line before deployment, or happens in real time while the DNN is carrying out its assigned task. As part of the learning process, training is required to set the parameters, also known as weights, of the DNN in order to achieve high accuracy in the assigned task. Without training, the DNN is rendered useless, given that the parameters are not set correctly. It has been shown that this training process requires large amounts of data and a high number of training iterations for the DNN model to be effective. The weights are updated in each iteration based on the subset of the training data provided. The training process has proven to be a challenge given the long timescale involved. The amount of training data, the number of weights, and the computational complexity of updating those weights are all factors that contribute to this challenge. One way to reduce training time is to allocate processes to a multitude of processors, thus achieving some sort of sub-optimal parallelism. One approach is to have this decision be carried out by ML or DL experts. The problem with this is the absence of concrete information to ensure the best decision is taken: the time it takes for a particular process to run on a particular processor, and the costs of inter-communication between processors, are in fact unknown. Even with the intuition of an expert in this domain, a sub-optimal solution that outperforms a single-processor use case is not achieved.In this dissertation, a hybrid-based multi-step optimization framework is presented. The framework explores the vast design space of mapping processes to processors. The search and evaluation are conducted in real time while training the DNN. In the first stage of the framework we compare the algorithmic intuitive approach with the Bayesian optimization (BO) approach. In the second stage of the framework, we create a predictive function for the performance of a single iteration of training, comparing the accuracy of different predictive functions created by different ML algorithms. The developed predictive model is then used as a surrogate function when identifying the best mapping. This stage in the search applies genetic algorithms (GA). An adaptive feature is also presented and tested for responsiveness to any changes that affect the performance of the training in the system.We also present heterogeneous earliest finish time (HEFT): a deterministic approach to map- ping. In addition, we present the concept of node splitting, which refers to the computational graph of the DNN being split in order to accommodate a higher level of model parallelism. It is noted that this would also affect the accuracy of the DNN, since the hyperparameters are affected.The framework and methodologies were evaluated in real, non-simulated systems using wall- clock time. The DNNs were built using Google’s ML/DL library, TensorFlow (TF)","eScholarship, University of California",Deep Learning Performance Optimization via Model Parallelization,,,,core
234929028,2019,"In recent years, the number of Industry 4.0 enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. 

At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. 

Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. 

To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, machine learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack",'Institute of Electrical and Electronics Engineers (IEEE)',"PREMISES, a scalable data-driven service to predict alarms in slowly-degrading multi-cycle industrial processes",,10.1109/BigDataCongress.2019.00032,,core
185560963,2019-01-01T00:00:00,"Extreme classification is a rapidly growing research area within machine learning focusing on multi-class and multi-label problems involving an extremely large number of labels (even more than a million). Many applications of extreme classification have been found in diverse areas ranging from language modeling to document tagging in NLP, face recognition to learning universal feature representations in computer vision, gene function prediction in bioinformatics, etc. Extreme classification has also opened up a new paradigm for key industrial applications such as ranking and recommendation by reformulating them as multi-label learning tasks where each item to be ranked or recommended is treated as a separate label. Such reformulations have led to significant gains over traditional collaborative filtering and content-based recommendation techniques. Consequently, extreme classifiers have been deployed in many real-world applications in industry.

Extreme classification has raised many new research challenges beyond the pale of traditional machine learning including developing log-time and log-space algorithms, deriving theoretical bounds that scale logarithmically with the number of labels, learning from biased training data, developing performance metrics, etc. The seminar aimed at bringing together experts in machine learning, NLP, computer vision, web search and recommendation from academia and industry to make progress on these problems. We believe that this seminar has encouraged the inter-disciplinary collaborations in the area of extreme classification, started discussion on identification of thrust areas and important research problems, motivated to improve the algorithms upon the state-of-the-art, as well to work on the theoretical foundations of extreme classification","Dagstuhl Reports. Dagstuhl Reports, Volume 8, Issue 7",Extreme Classification (Dagstuhl Seminar 18291),,10.4230/DagRep.8.7.62,,core
334844348,2019-08-05T00:00:00,"In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a
semi-automatic method which leverages on moving a 2D camera by means of a
robot, proving precise camera tracking, and an augmented reality pen to define
initial object bounding box, to create large labeled datasets with minimal
human intervention. By removing the burden of generating annotated data from
humans, we make the Deep Learning technique applied to computer vision, that
typically requires very large datasets, truly automated and reliable. With the
ARS pipeline, we created effortlessly two novel datasets, one on
electromechanical components (industrial scenario) and one on fruits
(daily-living scenario), and trained robustly two state-of-the-art object
detectors, based on convolutional neural networks, such as YOLO and SSD. With
respect to the conventional manual annotation of 1000 frames that takes us
slightly more than 10 hours, the proposed approach based on ARS allows
annotating 9 sequences of about 35000 frames in less than one hour, with a gain
factor of about 450. Moreover, both the precision and recall of object
detection is increased by about 15\% with respect to manual labeling. All our
software is available as a ROS package in a public repository alongside the
novel annotated datasets",,Semi-Automatic Labeling for Deep Learning in Robotics,http://arxiv.org/abs/1908.01862,,,core
304332468,2019-01-01T00:00:00,"Prognostic Health Management (PHM) is a maintenance policy aimed at predicting the occurrence of a failure in components and consequently minimizing unexpected downtimes of complex systems. Recent developments in condition monitoring (CM) techniques and Artificial Intelligence (AI) tools enabled the collection of a huge amount of data in real-time and its transformation into meaningful information that will support the maintenance decision-making process. The emerging Cyber-Physical Systems (CPS) technologies connect distributed physical systems with their virtual representations in the cyber computational world. The PHM assumes a key role in the implementation of CPS in manufacturing contexts, since it allows to keep CPS and its machines in proper conditions. On the other hand, CPS-based PHM provide an efficient solution to maximize availability of machines and production systems. In this paper, evolving and unsupervised approaches for the implementation of PHM at a component level are described, which are able to process streaming data in real-time and with almost-zero prior knowledge about the monitored component. A case study from a real industrial context is presented. Different unsupervised and online anomaly detection methods are combined with evolving clustering models in order to detect anomalous behaviours in streaming vibration data and integrate the so-generated knowledge into supervised and adaptive models; then, the degradation model for each identified fault is built and the resulting RUL prediction model integrated into the online analysis. Supervised methods are applied to the same dataset, in batch mode, to validate the proposed procedure",'Elsevier BV',Prognostic Health Management of Production Systems. New Proposed Approach and Experimental Evidences,,10.1016/j.promfg.2020.01.333,,core
322499579,2019,"In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process",'Elsevier BV',Machine learning framework for predictive maintenance in milling,,10.1016/j.ifacol.2019.11.172,,core
287592334,2019-11-01T00:00:00,"This thesis describes the design, implementation and experimental evaluation of a prototype instrumentation system for burner condition monitoring and NOx emissions prediction on fossil-fuel-fired furnaces. 



A review of methodologies and technologies for burner condition monitoring and NOx emissions prediction is given, together with the discussions of existing problems and technical requirements in their applications. A technical strategy, incorporating digital imaging, UV-visible spectrum analysis and soft computing techniques, is proposed. Based on these techniques, a prototype flame imaging system is developed. The system consists mainly of an optical and fibre probe protected by water-air cooling jacket, a digital camera, a miniature spectrometer and a mini-motherboard with associated application software. Detailed system design, implementation, calibration and evaluation are reported. 



A number of flame characteristic parameters are extracted from flame images and spectral signals. Luminous and geometric parameters, temperature and oscillation frequency are collected through imaging, while flame radical information is collected by the spectrometer. These parameters are then used to construct a neural network model for the burner condition monitoring and NOx emission prediction. 



Extensive experimental work was conducted on a 120 MWth gas-fired heat recovery boiler to evaluate the performance of the prototype system and developed algorithms. Further tests were carried out on a 40 MWth coal-fired combustion test facility to investigate the production of NOx emissions and the burner performance. 



The results obtained demonstrate that an Artificial Neural Network using the above inputs has produced relative errors of around 3%, and maximum relative errors of 8% under real industrial conditions, even when predicting flame data from test conditions not disclosed to the network during the training procedure. This demonstrates that this off the shelf hardware with machine learning can be used as an online prediction method for NOx",,Advanced Flame Monitoring and Emission Prediction through Digital Imaging and Spectrometry,https://core.ac.uk/download/287592334.pdf,,,core
478671759,2019-01-01T00:00:00,"The industry is moving towards maintenance strategies that consider component health, which require extensive collection and analysis of data. Condition monitoring methods that require manual feature extraction and analysis, become infeasible on an industrial scale. Machine learning algorithms can be used to automatically detect and classify faults, however, obtaining sufficient data for training is required for deep learning and other data-driven classification approaches. Data from healthy machine operation is generally available in abundance, while data from representative fault- and operating conditions is limited. This limits both development and deployment of deep learning-based CM systems on an industrial scale. This paper addresses both the challenges of automated analysis and lack of training data. A deep learning classifier architecture utilizing 1-dimensional dilated convolutions is proposed. Dilation of the convolution kernel allows for analysis of raw vibration signals while simultaneously maintaining the receptive field of the classifier enough to capture temporal patterns. The proposed method performs classification in time domain on signal segments of 1 second or shorter. With knowledge of the bearing specification, artificial vibration signals with similar characteristics as an actual bearing fault can be created. In this work, generated fault signals are combined with healthy operational data to obtain training data for a deep classifier. Parameters of the vibration model is chosen as distributions rather than fixed values. By using a range parameters in the vibration model, the classifier learns to recognize temporal features from the training data that generalize to unseen data. The effectiveness of the proposed method is demonstrated by training classifiers on generated data and testing on real signals from faulty bearings at both low and high speed. One dataset containing seeded faults and three run-to-failure tests are used for the demonstration",,Simulation-driven Deep Classification of Bearing Faults from Raw Vibration Data,,,,core
154991132,2019-11-05T00:00:00,"This paper presents BigDL (a distributed deep learning framework for Apache
Spark), which has been used by a variety of users in the industry for building
deep learning applications on production big data platforms. It allows deep
learning applications to run on the Apache Hadoop/Spark cluster so as to
directly process the production data, and as a part of the end-to-end data
analysis pipeline for deployment and management. Unlike existing deep learning
frameworks, BigDL implements distributed, data parallel training directly on
top of the functional compute model (with copy-on-write and coarse-grained
operations) of Spark. We also share real-world experience and ""war stories"" of
users that have adopted BigDL to address their challenges(i.e., how to easily
build end-to-end data analysis and deep learning pipelines for their production
data).Comment: In ACM Symposium of Cloud Computing conference (SoCC) 201",'Association for Computing Machinery (ACM)',BigDL: A Distributed Deep Learning Framework for Big Data,http://arxiv.org/abs/1804.05839,10.1145/3357223.3362707,,core
267815482,2019-11-08T00:00:00,"Current culture-based methods for detection and determination of Campylobacter levels on processed chickens takes at least 2\ua0days. Here we sought to develop a new complete, low-cost and rapid (approximately 2·5\ua0h) detection system requiring minimal operator input.We observed a strong correlation between culture-based cell counts and our ability to detect either Campylobacter jejuni or Campylobacter coli by loop-mediated isothermal amplification from the same samples. This knowledge was used to develop a rapid and simple five-step assay to quantify Campylobacter, which was subsequently assessed for its specificity, reproducibility and accuracy in quantifying Campylobacter levels from processed chickens. The assay was found to be highly specific for C. jejuni and C. coli and was capable of distinguishing between samples that are either within or exceeding the industry set target of 6000 Campylobacter colony forming units (CFU) per carcass (equivalent to 12\ua0CFU per ml of chicken rinse) with >90% accuracy relative to culture-based methods.Our method can reliably quantify Campylobacter counts of processed chickens with an accuracy comparable to culture-based assays but provides results within hours as opposed to days.The research presented here will help improve food safety by providing fast Campylobacter detection that will enable the implementation of real-time risk management strategies in poultry processing plants to rapidly test processed chickens and identify effective intervention strategies. This technology is a powerful tool that can be easily adapted for other organisms and thus could be highly beneficial for a broad range of industries",'Wiley',"An easy‐to‐perform, culture‐free Campylobacter point‐of‐management assay for processing plant applications",,10.1111/jam.14509,,core
287621918,2019-01-01T00:00:00,"In order for autonomous systems like robots, drones, and self-driving cars to be reliably introduced into our society, they must have the ability to actively account for safety during their operation. While safety analysis has traditionally been conducted offline for controlled environments like cages on factory floors, the much higher complexity of open, human-populated spaces like our homes, cities, and roads makes it unviable to rely on common design-time assumptions, since these may be violated once the system is deployed. Instead, the next generation of robotic technologies will need to reason about safety online, constructing high-confidence assurances informed by ongoing observations of the environment and other agents, in spite of models of them being necessarily fallible.This dissertation aims to lay down the necessary foundations to enable autonomous systems to ensure their own safety in complex, changing, and uncertain environments, by explicitly reasoning about the gap between their models and the real world. It first introduces a suite of novel robust optimal control formulations and algorithmic tools that permit tractable safety analysis in time-varying, multi-agent systems, as well as safe real-time robotic navigation in partially unknown environments; these approaches are demonstrated on large-scale unmanned air traffic simulation and physical quadrotor platforms. After this, it draws on Bayesian machine learning methods to translate model-based guarantees into high-confidence assurances, monitoring the reliability of predictive models in light of changing evidence about the physical system and surrounding agents. This principle is first applied to a general safety framework allowing the use of learning-based control (e.g. reinforcement learning) for safety-critical robotic systems such as drones, and then combined with insights from cognitive science and dynamic game theory to enable safe human-centered navigation and interaction; these techniques are showcased on physical quadrotors—flying in unmodeled wind and among human pedestrians—and simulated highway driving. The dissertation ends with a discussion of challenges and opportunities ahead, including the bridging of safety analysis and reinforcement learning and the need to ``close the loop'' around learning and adaptation in order to deploy increasingly advanced autonomous systems with confidence","eScholarship, University of California",Game-Theoretic Safety Assurance for Human-Centered Robotic Systems,https://core.ac.uk/download/287621918.pdf,,,core
231795431,2019-01-01T00:00:00,"The increasing interest in the usage of Artificial Intelligence techniques (AI) from the research community and industry to tackle “real world” problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high level scripting languages or frameworks such as TensorFlow and the installation processes often requires connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API’s such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud",,IXPUG 2019 Annual Conference at CERN,,,,core
334879916,2019-11-07T00:00:00,"In recent years, domain-specific hardware has brought significant performance
improvements in deep learning (DL). Both industry and academia only focus on
throughput when evaluating these AI accelerators, which usually are custom
ASICs deployed in datacenter to speed up the inference phase of DL workloads.
Pursuing higher hardware throughput such as OPS (Operation Per Second) using
various optimizations seems to be their main design target. However, they
ignore the importance of accuracy in the DL nature. Motivated by this, this
paper argue that a single throughput metric can not comprehensively reflect the
real-world performance of AI accelerators. To reveal this pitfall, we evaluates
several frequently-used optimizations on a typical AI accelerator and
quantifies their impact on accuracy and throughout under representative DL
inference workloads. Based on our experimental results, we find that some
optimizations cause significant loss on accuracy in some workloads, although it
can improves the throughout. Furthermore, our results show the importance of
end-to-end evaluation in DL",,The Pitfall of Evaluating Performance on Emerging AI Accelerators,http://arxiv.org/abs/1911.02987,,,core
334859051,2019-09-15T00:00:00,"Deep Learning (DL) has recently achieved tremendous success. A variety of DL
frameworks and platforms play a key role to catalyze such progress. However,
the differences in architecture designs and implementations of existing
frameworks and platforms bring new challenges for DL software development and
deployment. Till now, there is no study on how various mainstream frameworks
and platforms influence both DL software development and deployment in
practice. To fill this gap, we take the first step towards understanding how
the most widely-used DL frameworks and platforms support the DL software
development and deployment. We conduct a systematic study on these frameworks
and platforms by using two types of DNN architectures and three popular
datasets. (1) For development process, we investigate the prediction accuracy
under the same runtime training configuration or same model weights/biases. We
also study the adversarial robustness of trained models by leveraging the
existing adversarial attack techniques. The experimental results show that the
computing differences across frameworks could result in an obvious prediction
accuracy decline, which should draw the attention of DL developers. (2) For
deployment process, we investigate the prediction accuracy and performance
(refers to time cost and memory consumption) when the trained models are
migrated/quantized from PC to real mobile devices and web browsers. The DL
platform study unveils that the migration and quantization still suffer from
compatibility and reliability issues. Meanwhile, we find several DL software
bugs by using the results as a benchmark. We further validate the results
through bug confirmation from stakeholders and industrial positive feedback to
highlight the implications of our study. Through our study, we summarize
practical guidelines, identify challenges and pinpoint new research directions",,"An Empirical Study towards Characterizing Deep Learning Development and
  Deployment across Different Frameworks and Platforms",http://arxiv.org/abs/1909.06727,,,core
334898074,2019-12-27T00:00:00,"Coding diagnosis and procedures in medical records is a crucial process in
the healthcare industry, which includes the creation of accurate billings,
receiving reimbursements from payers, and creating standardized patient care
records. In the United States, Billing and Insurance related activities cost
around $471 billion in 2012 which constitutes about 25% of all the U.S hospital
spending. In this paper, we report the performance of a natural language
processing model that can map clinical notes to medical codes, and predict
final diagnosis from unstructured entries of history of present illness,
symptoms at the time of admission, etc. Previous studies have demonstrated that
deep learning models perform better at such mapping when compared to
conventional machine learning models. Therefore, we employed state-of-the-art
deep learning method, ULMFiT on the largest emergency department clinical notes
dataset MIMIC III which has 1.2M clinical notes to select for the top-10 and
top-50 diagnosis and procedure codes. Our models were able to predict the
top-10 diagnoses and procedures with 80.3% and 80.5% accuracy, whereas the
top-50 ICD-9 codes of diagnosis and procedures are predicted with 70.7% and
63.9% accuracy. Prediction of diagnosis and procedures from unstructured
clinical notes benefit human coders to save time, eliminate errors and minimize
costs. With promising scores from our present model, the next step would be to
deploy this on a small-scale real-world scenario and compare it with human
coders as the gold standard. We believe that further research of this approach
can create highly accurate predictions that can ease the workflow in a clinical
setting.Comment: This is a shortened version of the Capstone Project that was accepted
  by the Faculty of Indiana University, in partial fulfillment of the
  requirements for the degree of Master of Science in Health Informatics in Dec
  201",,"Natural language processing of MIMIC-III clinical notes for identifying
  diagnosis and procedures with neural networks",http://arxiv.org/abs/1912.12397,,,core
295497995,2019-02-19,"In the past, methods for hand sign recognition have been successfully tested in Human Robot Interaction (HRI) using traditional methodologies based on static image features and machine learning. However, the recognition of gestures in video sequences is a problem still open, because current detection methods achieve low scores when the background is undefined or in unstructured scenarios. Deep learning techniques are being applied to approach a solution for this problem in recent years. In this paper, we present a study in which we analyse the performance of a 3DCNN architecture for hand gesture recognition in an unstructured scenario. The system yields a score of 73% in both accuracy and F1. The aim of the work is the implementation of a system for commanding robots with gestures recorded by video in real scenarios.This work was funded by the Ministry of Economy, Industry and Competitiveness from the Spanish Government through the DPI2015-68087-R and the pre-doctoral grant BES-2016-078290, by the European Commission and FEDER funds through the project COMMANDIA (SOE2/P1/F0638), action supported by Interreg-V Sudoe",SciTePress,3DCNN Performance in Hand Gesture Recognition Applied to Robot Arm Interaction,,10.5220/0007570208020806,,core
299377742,2019-09-10T00:00:00,"Visual analytics are becoming more and more important in the light of big data and related scenarios. Along this trend, the field of immersive analytics has been variously furthered as it is able to provide sophisticated visual data analytics on one hand, while preserving user-friendliness on the other. Furthermore, recent hardware developments like smart glasses, as well as achievements in virtual-reality applications, have fanned immersive analytic solutions. Notably, such solutions can be very effective when they are applied to high-dimensional data sets. Taking this advantage into account, the work at hand applies immersive analytics to a high-dimensional production data set in order to improve the digital support of daily work tasks. More specifically, a mixed-reality implementation is presented that shall support manufactures as well as data scientists to comprehensively analyze machine data. As a particular goal, the prototype shall simplify the analysis of manufacturing data through the usage of dimensionality reduction effects. Therefore, five aspects are mainly reported in this paper. First, it is shown how dimensionality reduction effects can be represented by clusters. Second, it is presented how the resulting information loss of the reduction is addressed. Third, the graphical interface 

of the developed prototype is illustrated as it provides a (1) correlation coefficient graph, a (2) plot for the information loss, and a (3) 3D particle system. In addition, an implemented voice recognition feature of the prototype is shown, which was considered as being promising to select or deselect data variables users are interested in when analyzing the data. Fourth, based on a machine learning library, it is shown how the prototype reduces computational resources by the use of smart glasses. The main idea is based on a recommendation approach as well as the use of subspace clustering. Fifth, results from a practical setting are presented, in which the prototype was shown to domain experts. The latter reported that such a tool is actually helpful to analyze machine data on a daily basis. Moreover, it was reported that such system can be used to educate machine operators more properly. As a general outcome of this work, the presented approach may constitute a helpful solution for the industry as well as other domains like medicine",'MDPI AG',Dimensionality Reduction and Subspace Clustering in Mixed Reality for Condition Monitoring of High-Dimensional Production Data,https://core.ac.uk/download/299377742.pdf,,,core
231832166,2019-01-01T00:00:00,"Deep Convolutional Neural Networks (CNN) have

been extensively applied in various computer vision tasks. Although such approaches have demonstrated exceptionally high

performance in various open challenges, adapting them to more

specialised tasks can be non-trivial. In this paper we discuss

our design and implementation of a batchcode detection system

capable of accurate segmentation of batchcode regions within

images of consumer products. A batchcode is a unique identifier

printed on the packaging of many products that encodes useful

information such as date and location of manufacture. Detection

of batchcodes in images of products is a useful step in many

processes, including quality control, supply chain tracking and

counterfeit detection. Beginning with a unique dataset of product

images and a set of crowdsourced coarse annotations that roughly

correspond to the locations of batchcodes, we demonstrate that

such annotations are insufficient for training a reliable model,

and subsequently describe a novel label refinement process, which

we call the Maximally Stable Global Region (MSGR) method,

that we use to generate accurate ground-truth data suitable for

training a robust neural network. We also show that detection

accuracy can be further improved by applying MSGR to the

output of the neural network. We evaluate our approach using a

manually labelled test dataset of images of shampoo bottles, and

demonstrate the efficacy of the proposed method for accurate

real-time batchcode detection",'Institute of Electrical and Electronics Engineers (IEEE)',Coarse annotation refinement for segmentation of dot-matrix batchcodes.,https://core.ac.uk/download/231832166.pdf,10.1109/ICMLA.2019.00320,,core
231905919,2019-01-01T00:00:00,"The scope of this PhD is to propose, develop and assess several upgrades to existing shape optimization methods based on Evolutionary Algorithms (EAs). The efficiency of the proposed improvements is demonstrated in a number of real-world applications in the field of fluid mechanics (aerodynamic, hydrodynamics and turbomachinery) which are associated with computationally expensive evaluation software. They noticeably reduce the computational cost of optimization compared to the available (background) methods, which are still based on EAs enhanced by metamodels (Metamodel-Assisted EAs or MAEAs) and distributed search. Metamodels, mainly Radial Basis Function networks, are online trained personalized surrogate evaluation models, meaning that a local metamodel is trained for the pre-evaluationof each new individual generated during the evolution. This is in contrast to the common use of offline trained metamodels widely used by other relevant methods. Parallelization, in the form of concurrent evaluations of the candidate solutions on the multiprocessor platform of the Parallel CFD & Optimization Unit (PCOpt) of the Lab of Thermal Turbomachines of the NTUA is an indispensable feature of the proposed method variants. All developments have been made in the generic optimization platform EASY (Evolutionary Algorithm SYstem) developed by the PCOpt/NTUA. In all but one optimization problems, the problem-specificmodel to evaluate the candidate solutions is the GPU-enabled CFD solver PUMA developed by the same group. Only in the case of the optimization of the valveless diaphragm micropump, a different in-house CFD tool based on the cut-cell method is used instead. The most important contributions of this thesis are listed below:a) The use of Principal Component Analysis (PCA) to assist the EAs during the evolution. In this thesis, the Kernel PCA is used and is shown to provide better results compared to the Linear PCA used so far. In each generation of the EA, the PCA performs an eigendecomposion of the offspring population. The resulting eigenvectors define a new feature space, which the population members are transformed into; the evolution operators are applied in the feature space in which they perform optimally. Moreover, the PCA assists the MAEAs. Metamodels are only trained on the most important variables (directions in the feature space) indicated by the PCA, while the rest are safely truncated, as these generate noise at the predictions. The metamodels are trained with transformed by the PCApatterns, with truncated design variables, leading to reduced training cost and more dependable predictions. The two-fold usage of PCA drives the EA-based search in a much better way.b) A PCAbased Hybrid Algorithm aiming at maximum efficiency in Multi-Objective Optimization (MOO). This hybrid method combines the advantages of EA and Gradient-Based(GB) optimization. The EA explores the design space while the GB method regularly upgrades the most promising solutions. The required gradients of the objective functions with respect to the design variables are efficiently computed with the continuous adjoint method developed and programmed in the PCOpt/NTUA, at a cost which is independent of the number of design variables. In MOO, the direction along which the GB method updates the selected individuals is of outmost importance. Herein, the Linear PCA computes the principalcomponents of the objective space by processing the objective function values of individuals forming the current front of non-dominated solutions. The principal component (direction) corresponding to the minimum variance is perpendicular to the current front and points towards the direction of the simultaneous improvement of all objective functions, so this is used for the GB refinement. The proposed hybrid method performs better than the non-hybridized EA-based search.c) Multi-Criteria Decision Making (MCDM) within EAs to account for the Decision Maker’s (DM) preferences during the evolution. In contrast to standard multi-objective EAs which may insufficiently populate the preferred area(s) of the objective space, more non-dominatedsolutions are now driven towards them. This is achieved by using the MCDM Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), which affects the parent selection and the non-dominated front trimming operators.d) Flow prediction with Deep Neural Networks (DNNs) to assist the design/optimization of aerodynamic shapes. Trained on databases of CFD simulations, DNNs learn to predict the flow field around/inside these bodies, such as airfoils, wings and turbomachinery cascades. In this thesis, inputs and outputs are processed as images, in 2D cases, or raw data, in 3D cases. The DNNs are validated on new shapes and their ability to replicate the CFD results with high precision and low computational cost is demonstrated. The DNNs are employed as offline trained metamodels during the EA-based search, in contrast to the online trained metamodels used in the aforementioned MAEAs. The background and the aforementioned methods can work synergistically or separately to improve the performance of EA-basedoptimization methods as it is demonstrated in two groups of CFD applications. The first group consists of some ""standard"" CFD-based optimization problems, the so-called benchmarkcases. Each time a new variant is presented, these are revisited. By doing so, the reader should clearly assess the improvement offered by the proposed method. In a separate chapter of this thesis, a number of industrial cases are presented and optimized with the most efficient methods presented. These include the shape optimization of : (a) an Aircraft Wing-Body Configuration, (b) the DrivAer Car, (c) an Ultralight Aircraft, (d) a Francis Runner and (e) a Valveless Diaphragm Micropump.Σκοπός της Διδακτορικής Διατριβής είναι να προτείνει, προγραμματίσσει και αξιολογήσειαναβαθμίσεις σε υπάρχουσες μεθόδους βελτιστοποίησης αεροδυναμικών/υδροδυναμικών μορφών με βάση τους εξελικτικούς αλγορίθμους (EA). Η αποδοτικότητα των προτεινόμενων βελτιώσεων επιδεικνύεται σε πραγματικές εφαρμογές απο τον τομέα της υππολογιστικής ρευστοδυναμικής (ΥΡΔ) που σχετίζονται με υπολογιστικά ακριβό λογισμικό αξιολόγησης. Οι προτεινόμενες μέθοδοι μειώνουν αισθητά το υπολογιστικό κόστος βελτιστοποίησης σε σύγκριση με  προϋπάρχουσες μεθόδους, οι οποίες βασίζονται σε ΕΑ ενισχυμένους με χρήση μεταπροτύπων  (Metamodel-Assisted ΕΑ, ΜΑΕΑ) και κατανεμημένης αναζήτησης. Όλα τα μεταπρότυπα, κυρίως δίκτυα ακτινικής βάσης, είναι εξατομικευμένα και συνδεδεμένα με την εξέλιξη, που σημαίνει ότι, κατά τη διάρκεια της εξέλιξης, ένα μεταπρότυπο τοπικής εμβέλειας εκπαιδεύεται για κάθε νέο άτομο προς αξιολόγηση. Η χρήση αυτή των μεταπροτύπων έρχεται σε αντίθεση με τη χρήση μεταπροτύπων αποσυνδεδεμένων από την εξέλιξη, η οποία συνηθίζεται στην πλειονότητα των συναφών μεθόδων της βιβλιογραδίας. Δεδομένου του υψηλού υπολογιστικού κόστους ανά αξιολόγηση, η παραλληλοποίηση, με τη έννοια της ταυτόχρονης αξιολόγησης των υποψήφιων λύσεων στην πολυεπεξεργαστική συστοιχία υπολογιστών της  Μονάδας Παράλληλης Υπολογιστικής Ρευστοδυναμικής & Βελτιστοποίησης (ΜΠΥΡΒ) του Εργαστηρίου Θερμικών Στροβιλομηχανών του ΕΜΠ, είναι απολύτως απαραίτητη. Όλες οι προτεινόμενες μέθοδοι έχουν αναπτυχθεί στη γενικής φύσης πλατφόρμα βελτιστοποίησης EASY (Evolutionary Algorithm SYstem) που αναπτύχθηκε από την ΜΠΥΡΒ. Σε όλα τα προβλήματα βελτιστοποίησης, το λογισμικό αξιολόγησης για κάθε υποψήφια λύση είναι  ο βασιζόμενος σε κάρτες γραφικών επιλύτης ροών με τεχνικές ΥΡΔ PUMA, που έχει αναπτυχθεί από την ίδια ομάδα. Μόνο στην περίπτωση της βελτιστοποίησης διαφραγματικής μικρο-αντλίας χωρίς βαλβίδες, χρησιμοποιείται ένας διαφορετικός  επιλύτης ΥΡΔ που βασίζεται στη μέθοδο των τεμνόμενων κυψελών. Οι πιο σημαντικές συνεισφορές αυτής της εργασίας είναι οι ακόλουθες:α) Η χρήση της Ανάλυσης Κύριων Συνιστωσών (ΑΚΣ) για την υποβοήθηση των EA. Σε αυτήν την εργασία, χρησιμοποιείται η ΑΚΣ με συναρτήσεις πυρήνα και  δείχνεται ότι παρέχει καλύτερα αποτελέσματα σε σύγκριση με τη γραμμική ΑΚΣ, που χρησιμοποιείτο προηγουμένως. Στην αρχή κάθε γενιάς, η ΑΚΣ εκτελεί μια ιδιοανάλυση του πληθυσμού των απογόνων. Τα προκύπτοντα ιδιοδιανύσματα ορίζουν ένα νέο χώρο των χαρακτηριστικών, στον οποίο μετασχηματίζονται τα μέλη του πληθυσμού. Οι τελεστές εξέλιξης εφαρμόζονται στο χώρο των χαρακτηριστικών όπου εκεί λειτουργούν βέλτιστα. Τα μεταπρότυπα εκπαιδεύονται με τις πιο σημαντικές μεταβλητές ενώ οι υπόλοιπες αποκόπτονται με ασφάλεια, καθώς εισάγουν θόρυβο στις προβλέψεις. Η ΑΚΣ προσδιορίζει τις πιο σημαντικές κατευθύνσεις (μεταβλητές) στο χώρο των χαρακτηριστικών. Τα μεταπρότυπα εκπαιδεύονται με δεδομένα τα οποία μετασχηματίζονται στο χώρο των χαρακτηριστικών όπου αποκόπτονται οι λιγότερο σημαντικές μεταβλητές. Κατά συνέπεια, μειώνεται το κόστος εκπαίδευσης των μεταπροτύπων και οι προβλέψεις τους γίνονται πιο αξιόπιστες. Η διπλή χρήση της ΑΚΣ οδηγεί σε καλύτερη απόδοση του EA.β) Ένας Υβριδικός Αλγόριθμος βασισμένος στην ΑΚΣ με στόχο τη μέγιστη απόδοση σε προβλήματα πολυκριτηριακής βελτιστοποίησης. Αυτή η υβριδική μέθοδος συνδυάζει τα πλεονεκτήματα του EA και εκείνα της μεθόδου βελτιστοποίησης με παραγώγους (ΒμΠ).Ο ΕΑ διερευνά το χώρο σχεδιασμού, ενώ η μέθοδος ΒμΠ ανανεώνει τακτικά τις πιο υποσχόμενες λύσεις. Οι απαιτούμενες κλίσεις των αντικειμενικών συναρτήσεων ως προς τις μεταβλητές σχεδιασμού υπολογίζονται αποτελεσματικά με τη συνεχή συζυγή μέθοδο που αναπτύχθηκε και προγραμματίστηκε στη ΜΠΥΡΒ, το κόστος της οποίας είναι ανεξάρτητο από τον αριθμό των μεταβλητών σχεδιασμού. Στη πολυκριτηριακή βελτιστοποίηση, η κατεύθυνση κατά την οποία η μέθοδος ΒμΠ ανανεώνει τα επιλεγμένα άτομα είναι εξαιρετικής σημασίας. Η γραμμική ΑΚΣ υπολογίζει τις κύριες συνιστώσες του χώρου των αντικειμενικών συναρτήσεων χρησιμοποιώντας τις τιμές των αντικειμενικών συναρτήσεων των ατόμων του τρέχοντος μέτωπου των μη-κυριαρχούμενων λύσεων. Η κύρια συνιστώσα (κατεύθυνση) που αντιστοιχεί στην ελάχιστη διακύμανση είναι κάθετη στο τρέχον μέτωπο και δείχνει προς την κατεύθυνση η οποία ταυτόχρονα βελτιώνει όλες τις αντικειμενικές συναρτήσεις, για αυτό και χρησιμοποιείται για την ανανέωση με ΒμΠ. Η προτεινόμενη υβριδική μέθοδος αποδίδει καλύτερα από τους μη-υβριδικούς ΕΑ.γ) Πολυκριτηριακή Λήψη Αποφάσεων (ΠΛΑ) εντός των EA για να ληφθούν υπόψην οι προτιμήσεις του Λήπτη Αποφάσεων κατά την εξέλιξη. Σε αντίθεση με τους EA πολλών στόχων που ενδέχεται να μην καλύπτουν επαρκώς τις προτιμώμενες περιοχες του χώρου των αντικειμενικών, η χρήση τεχνικών ΠΛΑ οδηγεί περισσότερες μη-κυριαρχούμενες λύσεις προς τις περιοχές αυτές. Αυτό, στη Διδακροτική αυτή Διατριβή, επιτυγχάνεται με τη χρήση της μεθόδου ΠΛΑ Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), η οποία επηρεάζει την επιλογή γονέων και την αποκοπή των μη-κυριαρχούμενων λύσεων από το τρέχον μέτωπο.δ) Προβλέψη ροής με Βαθιά Νευρωνικά Δίκτυα (ΒΝΔ) για την υποβοήθηση στο σχεδιασμό/βελτιστοποίηση αεροδυναμικών σχημάτων. Αφού έχουν εκπαιδευτεί  σε βάσεις δεδομένων προσομοιώσεων οι οποίες πραγματοποιήθηκαν με την χρήση λογισμικού ΥΡΔ, τα ΒΝΔ μαθαίνουν να προβλέπουν το πεδίο ροής γύρω/μέσα στα αεροδυναμικά σώματα, όπως αεροσκάφη, πτερύγια και θερμικές στροβιλομηχανές. Στην Διδακτορική αυτή  Διατριβή, η επεξεργασία των δεδομένων εισόδου και εξόδου του ΒΝΔ γίνεται σε μορφή εικόνων, σε 2D περιπτώσεις, και σε μορφή ανεπεξέργαστων δεδομένων, σε 3D περιπτώσεις. Τα ΒΝΔ δοκιμάζονται σε νέα αεροδυναμικά σχήματα και αποδεικνύεται η ικανότητά τους να αναπαράγουν τα αποτελέσματα  κωδίκσν ΥΡΔ με υψηλή ακρίβεια και χαμηλό υπολογιστικό κόστος (μη συμπεριλαμβανομένου του κόστους εκπαίδευσης). Τα ΒΝΔ χρησιμοποιούνται ως αποσυνδεδεμένα από την εξέλιξη μεταπρότυπα, σε αντίθεση με  τα συνδεδεμένα με την εξέλιξη μεαταπρότυπα που παρουσιάστηκαν προηγουμένως.Οι προαναφερθείσες μέθοδοι μπορούν να λειτουργούν συνεργατικά ή ξεχωριστά για τη βελτίωση της απόδοσης των μεθόδων βελτιστοποίησης που βασίζονται σε EA, όπως αποδεικνύεται σε δύο ομάδες εφαρμογών. Η πρώτη ομάδα αποτελείται από κάποια ""τυπικά"" προβλήματα αεροδυναμικής βελτιστοποίησης, που ονομάζονται προβλήματα αναφοράς. Κάθε φορά που παρουσιάζεται μια νέα προτεινόμενη μέθοδος, τα προβλήματα αυτά επανεξετάζονται. Με αυτόν τον τρόπο, ο αναγνώστης θα μπορεί να αξιολογεί με σαφήνεια και συγκριτικά τα πλεονεκτήματα κάθε προτεινόμενης παραλλαγής. Σε ξεχωριστό κεφάλαιο της Διδακτορικής Διατριβής, παρουσιάζονται και βελτιστοποιούνται ορισμένα βιομηχανικά προβλήματα  με τις πιο αποτελεσματικές από τις προτεινόμενες μεθόδους. Στα προβλήματα αυτά περιλαμβάνουν τη βελτιστοποίηση του σχήματος: (α) μιας διαμόρφωσης πτέρυγας-ατράκτου αεροσκάφους, (β) του αυτοκινήτου DrivAer, (γ) ενός ελαφρού ανεμόπτερου, (δ) ενός δρομέα υδροστροβίλου Francis και (ε) μιας διαφραγματικής μικρο-αντλίας χωρίς βαλβίδες",Εθνικό Μετσόβιο Πολυτεχνείο (ΕΜΠ),Εξελικτικοί αλγόριθμοι χαμηλού κόστους υποβοηθούμενοι από μεταπρότυπα και εφαρμογές τους στη βελτιστοποίηση  μορφής στη ρευστοδυναμική,,,,core
328148978,2019-12-15T08:00:00,"Data mining and predictive analytics in the sustainable-biomaterials industries is currently not feasible given the lack of organization and management of the database structures. The advent of artificial intelligence, data mining, robotics, etc., has become a standard for successful business endeavors and is known as the ‘Fourth Industrial Revolution’ or ‘Industry 4.0’ in Europe. Data quality improvement through real-time multi-layer data fusion across interconnected networks and statistical quality assessment may improve the usefulness of databases maintained by these industries. Relational databases with a high degree of quality may be the gateway for predictive modeling and enhanced business analytics.	Data quality is a key issue in the sustainable bio-materials industry. Untreated data from multiple databases (e.g., sensor data and destructive test data) are generally not in the right structure to perform advanced analytics. Some inherent problems of data from sensors that are stored in data warehouses at millisecond intervals include missing values, duplicate records, sensor failure data (data out of feasible range), outliers, etc. These inherent problems of the untreated data represent information loss and mute predictive analytics. The goal of this data science focused research was to create a continuous real-time software algorithm for data cleaning that automatically aligns, fuses, and assesses data quality for missing fields and potential outliers. The program automatically reduces the variable size, imputes missing values, and predicts the destructive test data for every record in a database. Improved data quality was assessed using 10-fold cross-validation and the normalized root mean square error of prediction (NRMSEP) statistic. 	The impact of outliers and missing data were tested on a simulated dataset with 201 variations of outlier percentages ranging from 0-90% and missing data percentages ranging from 0-90%.  The software program was also validated on a real dataset from the wood composites industry. One result of the research was that the number of sensors needed for accurate predictions are highly dependent on the correlation between independent variables and dependent variables. Overall, the data cleaning software program significantly decreased the NRMSEP ranging from 64% to 12% of quality control variables for key destructive test values (e.g., internal bond, water absorption and modulus of rupture)",TRACE: Tennessee Research and Creative Exchange,Improving Manufacturing Data Quality with Data Fusion and Advanced Algorithms for Improved Total Data Quality Management,https://core.ac.uk/download/328148978.pdf,,,core
237701782,2019-05-01T00:00:00,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously",'Institute of Electrical and Electronics Engineers (IEEE)',Visual guidance and automatic control for robotic personalized stent graft manufacturing,,10.1109/icra.2019.8794123,,core
288820462,2019-12-01T00:00:00,"International audienceThe use of wireless sensor networks, which are the key ingredient in the growing Internet of Things (IoT), has surged over the past few years with a widening range of applications in the industry, healthcare, agriculture, with a special attention to monitoring and tracking, often tied with security issues. In some applications, sensors can be deployed in remote, large unpopulated areas, whereas in others, they serve to monitor confined busy spaces. In either case, clustering the sensor network's nodes into several clusters is of fundamental benefit for obvious scalability reasons, and also for helping to devise maintenance or usage schedules that might greatly improve the network's lifetime. In the present paper, we survey and compare popular and advanced clustering schemes and provide a detailed analysis of their performance as a function of scale, type of collected data or their heterogeneity, and noise level. The testing is performed on real sensor data provided by the UCI Machine Learning Repository, using various external validation metrics",'MDPI AG',Introducing and Comparing Recent Clustering Methods for Massive Data Management in the Internet of Things,,10.3390/jsan8040056,,core
286140236,2019-08-25T08:19:25,"Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.This work was partially supported by NSFC Program (No. 61602403 and 61572426)",'Springer Science and Business Media LLC',Inference of development activities from interaction with uninstrumented applications,,10.1007/s10664-017-9547-8,"[{'title': 'Empirical Software Engineering', 'identifiers': ['issn:1382-3256', '1382-3256']}]",core
275657672,2019,"Current demand for sustainment of critical aerospace assets requires management of complex systems and a product life cycle solution. This is a major concern both in civil and defence sectors in Australia. The emergence of new technologies including additive manufacturing (AM), virtual prototyping, simulated/augmented reality (AR) and artificial intelligence (AI) provide a unique opportunity for Innovative Sustainment. This paper discusses the importance of a network/system approach to sustainment and the role that emerging technologies including AM and AI can play in overcoming current challenges. In addition, the paper discusses the strategic research opportunities that can be harnessed by all stakeholders using a collaborative framework. The proposed framework can result in reduced cost of ownership, reduced logistics footprint, and enhanced resilience and flexibility by in-depth analyses of the challenges ahead. Aerospace composite material systems are used to exemplify the implementation pathway of this framework","Engineers Australia, Royal Aeronautical Society (Melbourne, Australia)","Virtual Design, Optimisation and Testing (VDOT) framework for innovative sustainment",,,,core
334892251,2019-12-10T00:00:00,"With the rapid growth of the applications of machine learning (ML) and other
artificial intelligence (AI) techniques, adequate testing has become a
necessity to ensure their quality. This paper identifies the characteristics of
AI applications that distinguish them from traditional software, and analyses
the main difficulties in applying existing testing methods. Based on this
analysis, we propose a new method called datamorphic testing and illustrate the
method with an example of testing face recognition applications. We also report
an experiment with four real industrial application systems of face recognition
to validate the proposed approach.Comment: This technical report is an extended version of conference paper:
  [Zhu, H., Liu, D., Ian Bayley, I., Harrison, R. and Cuzzolin, F., Datamorphic
  Testing: A Method for Testing Intelligent Applications, The 1st IEEE
  International Conference On Artificial Intelligence Testing (IEEE AITest
  2019), San Francisco, California, USA, April, 4 - 9, 2019.",,Datamorphic Testing: A Methodology for Testing AI Applications,http://arxiv.org/abs/1912.04900,,,core
200827801,2019-04-22T00:00:00,"In recent years, convolutional neural networks (CNNs) have become deeper in
order to achieve better classification accuracy in image classification.
However, it is difficult to deploy the state-of-the-art deep CNNs for
industrial use due to the difficulty of manually fine-tuning the
hyperparameters and the trade-off between classification accuracy and
computational cost. This paper proposes a novel multi-objective optimization
method for evolving state-of-the-art deep CNNs in real-life applications, which
automatically evolves the non-dominant solutions at the Pareto front. Three
major contributions are made: Firstly, a new encoding strategy is designed to
encode one of the best state-of-the-art CNNs; With the classification accuracy
and the number of floating point operations as the two objectives, a
multi-objective particle swarm optimization method is developed to evolve the
non-dominant solutions; Last but not least, a new infrastructure is designed to
boost the experiments by concurrently running the experiments on multiple GPUs
across multiple machines, and a Python library is developed and released to
manage the infrastructure. The experimental results demonstrate that the
non-dominant solutions found by the proposed algorithm form a clear Pareto
front, and the proposed infrastructure is able to almost linearly reduce the
running time.Comment: conditionally accepted by gecco201",,"Evolving Deep Neural Networks by Multi-objective Particle Swarm
  Optimization for Image Classification",http://arxiv.org/abs/1904.09035,,,core
200838166,2019-05-15T00:00:00,"Thanks to digitization of industrial assets in fleets, the ambitious goal of
transferring fault diagnosis models fromone machine to the other has raised
great interest. Solving these domain adaptive transfer learning tasks has the
potential to save large efforts on manually labeling data and modifying models
for new machines in the same fleet. Although data-driven methods have shown
great potential in fault diagnosis applications, their ability to generalize on
new machines and new working conditions are limited because of their tendency
to overfit to the training set in reality. One promising solution to this
problem is to use domain adaptation techniques. It aims to improve model
performance on the target new machine. Inspired by its successful
implementation in computer vision, we introduced Domain-Adversarial Neural
Networks (DANN) to our context, along with two other popular methods existing
in previous fault diagnosis research. We then carefully justify the
applicability of these methods in realistic fault diagnosis settings, and offer
a unified experimental protocol for a fair comparison between domain adaptation
methods for fault diagnosis problems.Comment: Presented at 2019 Prognostics and System Health Management Conference
  (PHM 2019) in Paris, Franc",,Domain Adaptive Transfer Learning for Fault Diagnosis,http://arxiv.org/abs/1905.06004,,,core
187290654,2019-01-21T15:20:42,"The European Union's Energy Efficiency Directive is placing an increased focus on the measurement and verification (M&V) of demand side energy savings. The objective of M&V is to quantify energy savings with minimum uncertainty. M&V is currently undergoing a transition to practices, known as M&V 2.0, that employ automated advanced analytics to verify performance. This offers the opportunity to effectively manage the transition from short-term M&V to long-term monitoring and targeting (M&T) in industrial facilities. The original contribution of this paper consists of a novel, robust and technology agnostic framework that not only satisfies the requirements of M&V 2.0, but also bridges the gap between M&V and M&T by ensuring persistence of savings. The approach features a unique machine learning-based energy modelling methodology, model deployment and an exception reporting system that ensures early identification of performance degradation. A case study demonstrates the effectiveness of the approach. Savings from a real-world project are found to be 177,962 +/- 12,334 kWh with a 90% confidence interval. The uncertainty associated with the savings is 8.6% of the allowable uncertainty, thus highlighting the viability of the framework as a reliable and effective tool",'Institute of Electrical and Electronics Engineers (IEEE)',From M&V to M&T: An artificial intelligence-based framework for real-time performance verification of demand-side energy savings,,10.1109/SEST.2018.8495711,,core
200333178,2019-01-01T00:00:00,"The need for tools to help guide decision making is growing within the manufacturing industry. The analysis performed by these tools will help operators and engineers to understand the behaviour of the manufacturing stations better and thereby take data-driven decisions to improve them. The tools use techniques borrowed from fields such as Data Analytics, BigData, Predictive Modelling, and Machine Learning. However, to be able to use these tools efficiently, data from the factory floor is required as input. This data needs to be extracted from two sources, the PLCs, and the robots. In practice, methods to extract usable data from robots are rather scarce. The present work describes an approach to capture data from robots, which can be applied to both legacy and current state-of-the-art manufacturing systems. The described approach is developed using Sequence Planner - a tool for modelling and analyzing production systems - and is currently implemented at an automotive company as a pilot project to visualize and examine the ongoing process. By exploiting the robot code structure, robot actions are converted to event streams that are abstracted into operations. We then demonstrate the applicability of the resulting operations, by visualizing the ongoing process in real-time as Gantt charts, that support the operators performing maintenance. And, the data is also analyzed off-line using process mining techniques to create a general model that describes the underlying behaviour existing in the manufacturing station. Such models are used to derive insights about relationships between different operations, and also between resources",'Elsevier BV',"From factory floor to process models: A data gathering approach to generate, transform, and visualize manufacturing processes",,10.1016/j.cirpj.2018.12.002,,core
186317113,2019-01-17T00:00:00,"Recently, the advancement in industrial automation and high-speed printing
has raised numerous challenges related to the printing quality inspection of
final products. This paper proposes a machine vision based technique to assess
the printing quality of text on industrial objects. The assessment is based on
three quality defects such as text misalignment, varying printing shades, and
misprinted text. The proposed scheme performs the quality inspection through
stochastic assessment technique based on the second-order statistics of
printing. First: the text-containing area on printed product is identified
through image processing techniques. Second: the alignment testing of the
identified text-containing area is performed. Third: optical character
recognition is performed to divide the text into different small boxes and only
the intensity value of each text-containing box is taken as a random variable
and second-order statistics are estimated to determine the varying printing
defects in the text under one, two and three sigma thresholds. Fourth: the
K-Nearest Neighbors based supervised machine learning is performed to provide
the stochastic process for misprinted text detection. Finally, the technique is
deployed on an industrial image for the printing quality assessment with
varying values of n and m. The results have shown that the proposed SAML-QC
technique can perform real-time automated inspection for industrial printing.Comment: 7 Pages, 10 figures, 6 Table",,"SAML-QC: a Stochastic Assessment and Machine Learning based QC technique
  for Industrial Printing",http://arxiv.org/abs/1901.07370,,,core
286126314,2019-12-01T00:00:00,"The capital-intensive oil & gas industry invests billions of dollars in equipment annually and it is important to keep the equipment in top operating condition to help maintain efficient process operations and improve the rate of return by predicting failures before incidents. Digitalization has taken over the world with advances in sensor technology, wireless communication and computational capabilities, however oil & gas industry has not taken full advantage of this despite being technology centric.   
Dynamic seals are a vital part of reciprocating and rotary equipment such as compressor, pumps, engines, etc. and are considered most frequently failing component. Polymeric seals are increasingly complex and non-linear in behavior and have been the research of interest since 1950s. Most of the prognostic studies on seals are physics-based and requires direct estimation of different physical parameters to assess the degradation of seals, which are often difficult to obtain during operation. Another feasible approach to predict the failure is from performance related sensor data and is termed as data-driven prognostics. The offline phase of this approach is where the performance related data from the component of interest are acquired, pre-processed and artificial intelligence tools or statistical methods are used to model the degradation of a system. The developed models are then deployed online for a real-time condition monitoring. There is a lack of research on the data-driven based tools and methods for dynamic seal prognosis. The primary goal in this dissertation is to develop offline data-driven intelligent condition monitoring and prognostic methods for two types of dynamic seals used in the oil & gas industry, to avoid fatal breakdown of rotary and reciprocating equipment.
Accordingly, the interest in this dissertation lies in developing models to effectively evaluate and classify the running condition of rotary seals; assess the progression of degradation from its incipient to failure and to estimate the remaining useful life (RUL) of reciprocating seals.
First, a data-driven prognostic framework is developed to classify the running condition of rotary seals. An accelerated aging and testing procedure simulating rotary seal operation in oil field is developed to capture the behavior of seals through their cycle of operation until failure. The diagnostic capability of torque, leakage and vibration signal in differentiating the health states of rotary seals using experiments are compared. Since the key features that differentiate the health condition of rotary seals are unknown, an extensive feature extraction in time and frequency domain is carried out and a wrapper-based feature selection approach is used to select relevant features, with Multilayer Perceptron neural network utilized as classification technique. The proposed approach has shown that features extracted from torque and leakage lack a better discriminating power on its own, in classifying the running condition of seals throughout its service life.  The classifier built using optimal set of features from torque and leakage collectively has resulted in a high classification accuracy when compared to random forest and logistic regression, even for the data collected at a different operating condition. 
Second, a data-driven approach to predict the degradation process of reciprocating seals based on friction force signal using a hybrid Particle Swarm Optimization - Support Vector Machine is presented. There is little to no knowledge on the feature that reflects the degradation of reciprocating seals and on the application of SVM in predicting the future running condition of polymeric components such as seals. Controlled run-to-failure experiments are designed and performed, and data collected from a dedicated experimental set-up is used to develop the proposed approach. A degradation feature with high monotonicity is used as an indicator of seal degradation. The pseudo nearest neighbor is used to determine the essential number of inputs for forecasting the future trend.  The most challenging aspect of tuning parameters in SVM is framed in terms of an optimization problem aimed at minimizing the prediction error. The results indicate the effectiveness and better accuracy of the proposed approach when compared to GA-SVM and XGBoost. 
Finally, a deep neural network-based approach for estimating remaining useful life of reciprocating seals, using force and leakage signals is presented. Time domain and frequency domain statistical features are extracted from the measurements. An ideal prognostic feature should be well correlated with degradation time, monotonically increasing or decreasing and robust to outliers. The identified metrics namely: monotonicity, correlation and robustness are used to evaluate the goodness of extracted features. Each of the three metric carries a relative importance in the RUL estimation and a weighted linear combination of the metrics are used to rank and select the best set of prognostic features. The redundancy in the selected features is eliminated using Kelley-Gardner-Sutcliffe penalty function-based correlation-clustering algorithm to select a representative feature from each of the clusters. Finally, RUL estimation is modeled using a deep neural network model. Run-to-failure data collected from a reciprocating set-up was used to validate this approach and the findings show that the proposed approach can improve the accuracy of RUL prediction when compared to PSO-SVM and XGBoost regression. 
This research has important contribution and implications to rotary and reciprocating seal domain in utilizing sensors along with machine learning algorithms in assessing the health state and prognosis of seals without any direct measurements. This research has paved the way to move from a traditional fail-and-fix to predict-and-prevent approach in maintenance of seals. The findings of this research are foundational for developing an online degradation assessment platform which can remotely monitor the performance degradation of seals and provide action recommendations on maintenance decisions. This would be of great interest to customers and oil field operators to improve equipment utilization, control maintenance cost by enabling just-in-time maintenance and increase rate of return on equipment by predicting failures before incidents",,Intelligent Condition Monitoring and Prognostic Methods with Applications to Dynamic Seals in the Oil & Gas Industry,https://core.ac.uk/download/286126314.pdf,,,core
344900376,2019-01-01T00:00:00,"In Big Data contexts, many batch and streaming oriented technologies have emerged to deal with the high valuable sources of events, such as Internet of Things (IoT) platforms, the Web, several types of databases, among others. The huge amount of heterogeneous data being constantly generated by a world of interconnected things and the need for (semi)-automated decision-making processes through Complex Event Processing (CEP) and Machine Learning (ML) have raised the need for innovative architectures capable of processing events in a streamlined, scalable, analytical, and integrated way. This paper presents the Intelligent Event Broker, a CEP system built upon flexible and scalable Big Data techniques and technologies, highlighting its system architecture, software packages, and classes. A demonstration case in Bosch’s Industry 4.0 context is presented, detailing how the system can be used to manage and improve the quality of the manufacturing process, showing its usefulness for solving real-world event-oriented problems.This work has been supported by FCT –Fundação para a Ciência e Tecnologiawithin the Project Scope: UID/CEC/00319/2019 and the Doctoral scholarship PD/BDE/135101/2017. This paper uses icons made by Freepik, from www.flaticon.com",Association for Information Systems (AIS),Intelligent event broker: a complex event processing system in big data contexts,https://core.ac.uk/download/344900376.pdf,,,core
478076158,2019-01-01T00:00:00,"Mixed norms that promote structured sparsity have numerous applications in signal processing and machine learning problems. In this work, we present a new algorithm, based on a Newton root search technique, for computing the projection onto the l ∞,1 ball, which has found application in cognitive neuroscience and classification tasks. Numerical simulations show that our proposed method is between 8 and 10 times faster on average, and up to 20 times faster for very sparse solutions, than the previous state of the art. Tests on real functional magnetic resonance image data show that, for some data distributions, our algorithm can obtain speed improvements by a factor of between 10 and 100, depending on the implementation. © 2019 Society for Industrial and Applied Mathematics",'Society for Industrial & Applied Mathematics (SIAM)',"Efficient projection onto the ℓ ∞,1 mixed-norm ball using a newton root search method",,10.1137/18M1212525,"[{'title': 'SIAM Journal on Imaging Sciences', 'identifiers': ['1936-4954', 'issn:1936-4954']}]",core
288353915,2019-06-05T00:00:00,"Approximately one-third of the food produced globally is spoiled or wasted in the food supply chain (FSC). Essentially, it is lost before it even reaches the end consumer. Conventional methods of food waste tracking relying on paper-based logs to collect and analyse the data are costly, laborious, and time-consuming. Hence, an automated and real-time system based on the Internet of Things (IoT) concepts is proposed to measure the overall amount of waste as well as the reasons for waste generation in real-time within the potato processing industry, by using modern image processing and load cell technologies. The images captured through a specially positioned camera are processed to identify the damaged, unusable potatoes, and a digital load cell is used to measure their weight. Subsequently, a deep learning architecture, specifically the Convolutional Neural Network (CNN), is utilised to determine a potential reason for the potato waste generation. An accuracy of 99.79% was achieved using a small set of samples during the training test. We were successful enough to achieve a training accuracy of 94.06%, a validation accuracy of 85%, and a test accuracy of 83.3% after parameter tuning. This still represents a significant improvement over manual monitoring and extraction of waste within a potato processing line. In addition, the real-time data generated by this system help actors in the production, transportation, and processing of potatoes to determine various causes of waste generation and aid in the implementation of corrective actions",,Monitoring potato waste in food manufacturing using image processing and Internet of Things approach,https://core.ac.uk/download/288353915.pdf,,,core
286618536,2019-01-01T00:00:00,"1.	Abu-Taieh C., Evon J.: Technology Engineering and Management in Aviation: Advancements and Discoveries. Information Science Reference, 2011.
2.	Ajam M, Woolard C, Wiljoen CL. Biomass pyrolysis oil as a renewable feedstock for bio-jet fuel. In: Proceedings of the 13th international conference on stability, handling and use of liquid fuels (IASH2013), Rhodes, Greece; October 2013. p. 6–10.
3.	Аnnual report to Parliament on the renewable transport fuel obligation. Renewable Fuels Agency. The Stationery Office, 2011.
4.	Agarwal S., Chhibber V. K., Bhatnagar A. K.:Tribological behavior of diesel fuels and the effect of anti-wear additives. Fuel. Vol. 106, 2013, p. 21–29, 
5.	Alves S. M., Barros B.S., Trajano M.F.: Tribological behavior of vegetable oil-based lubricants with nanoparticles of oxides in boundary lubrication conditions. Tribology International. Vol. 65, 2013, p. 28–36.
6.	Asgari H., Chen X., Sainudiin R.: Modelling and simulation of gas turbines. International Journalof Modelling, Identification and Control, Vol.25, No.3, 2013, p. 1–15.
7.	Bartis James T. LaTourrette T., Dixon L.: Oil Shale Development in the United States: Prospects and Policy Issues. Santa Monica, Calif.: RAND Corporation, MG-414-NETL, 2005.
8.	Bassam N. El.: Handbook of Bioenergy Crops: A Complete Reference to Species. Development and Applications Earthscan, 2010.
9.	Bazazzadeh M., Badihi H., Shahriari A.: Gas Turbine Engine Control Design Using Fuzzy Logic and Neural Networks. International Journal of Aerospace Engineering. Vol. 1, 2011, p. 1–13. 
10.	Blakey S, Rye L, Wilson C.W.: Aviation gas turbine alternative fuels: A review.  P Combust Inst, No. 33, 2011, p. 2863–2885.
11.	Boichenko S., Iakovlieva A., Vovk O.: Traditional and alternative jet fuels: problems of quality standardization. Journal of Petroleum & Environmental Biotechnology. Vol. 4. Iss. 3, 2013.
12.	Boichenko S., Shkilniuk I., Turchak V.. The problems of biopollution with jet fuels and the way of achieving solution. Transport. 23, 2008; p. 253–257.
13.	Boichenko S., Yakovleva A. Prospects of biofuels introduction into aviation. Transport engineering and management: Proceedings of the 15-th conference for Lithuania Junior researchers. Science – future of Lithuania, 4 May 2012. Vilnius: Technika. p. 90–94.
14.	Boichenko S., Yakovlieva A., Gryshchenko O., Zinchuk A. Prospects of using different generations biofuels for minimizing impact of modern aviation on environment, Энерготехнологии и ресурсосбережение, № 1, 2018, p. 10–20.
15.	Boichenko S., Lejda K., Yakovlieva A., Vovk O. Comparative characteristics of low-temperature properties of jet fuels modified with bio-additives, International Automotive Conference (KONMOT2018). IOP Conf. Series: Materials Science and Engineering 421, 2018.
16.	Breil C., Meullemiestre A., Vian M., Chemat F.: Bio-Based Solvents for Green Extraction of Lipids from Oleaginous Yeast Biomass for Sustainable Aviation Biofuel. Molecules. Iss. 21(196), 2016, p. 1–14.
17.	Carels N., Sujatha M., Bahadur B.: Jatropha, Challenges for a New Energy Crop. Vol. 1: Farming, Economics and Biofuel. Springer Science & Business Media, 2012.
18.	Cavani F., Albonetti S., Basile F., Gandini A.: Chemicals and Fuels from Bio-Based Building Blocks. John Wiley & Sons, 2015.
19.	Cermak S. C., Evangelista R. L., Kenar J. A.: Distillation of Natural Fatty Acids and Their Chemical Derivatives, Distillation - Advances from Modeling to Applications, Dr. Sina Zereshki (Ed.), InTech, 2012. – р. 5. – 140. 
20.	Chai M. Thermal Decomposition of Methyl Esters in Biodiesel Fuel: Kinetics, Mechanisms and Products, Ph.D. Thesis, University оf Cincinnati, 2012.
21.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 1: emulsion production. Biomass Bioenergy, No. 25, 2003, p. 85–99.
22.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 2: tests in diesel engines. Biomass Bioenergy, No. 25, 2003, p. 101–11.
23.	Chuck C.J., Donnelly J.: The compatibility of potential bioderived fuels with Jet A-1 aviation kerosene. Applied Energy. Vol. 118, 2014, p. 83–91.
24.	Cleveland C.J., Morris C. G.: Handbook of energy. Volume II: Cronologies, top ten lists, and words clouds. Elsvier Inc., 2014.
25.	Cushion E., Whiteman A., Dieterle G.: Bioenergy Development: Issues and Impacts for Poverty and Natural Resource Management. World Bank Publications, 2010.
26.	Daggett D. L., Hendricks R.C., Walther R., Corporan E.: Alternative fuels for use in commercial aircrafts. The Boeing Company, 2007.
27.	Dahlquist E.: Biomass as Energy Source. Resources, Systems and Applications. CRC Press, 2013.
28.	Delmon B., Grange P., Froment G.F.: Hydrotreatment and Hydrocracking of Oil Fractions. Elsevier, 1999.
29.	Doc 9889 Airport Air Quality Manual. International Civil Aviation Organization, 2011. 
30.	Doc 9977. Manual on Civil Aviation Jet Fuel Supply, 2012.
31.	Edwards T.: Advancements in Gas Turbine Fuels from 1943 to 2005. J Eng Gas Power, No. 129, 2007, p. 13–20.
32.	Firrisa M. T., Van Duren I., Voinov A.: Energy efficiency for rapeseed biodiesel production in different farming systems. Energy Efficiency, 2013.
33.	Garcia-Anton J., Monzo J., Guninon J.L.: Study of corrosion on copper strips by petroleum naphtha in the ASTM D-130 test by means of electronic microscopy (SEM) and energy dispersive X-ray (EDX). Fresenius Journal of Analytical Chemistry. Iss. 337, 1990, p. 382–388.
34.	Garcia Santander C.M., Gymez Rueda S.M., de Lima da Silva N.: Measurements of normal boiling points of fatty acid esters and triacylglycerols by thermogravimetric analysis, Fuel, Iss. 92, 2012, p. 158–161.
35.	Geller D. P., Goodrum J.: W. Effects of speciﬁc fatty acid methyl esters on diesel fuel lubricity, Fuel, Vol. 83, 2004, p. 2351–2356.
36.	Gupta, K. K, Rehman A, Sarviya R. M.: Bio-fuels for the gas turbine: A review. Renew. Sust. Energ. Rev. No. 14, 2010, p. 2946–2955.
37.	Harvey B. G, Merriman W.W., Koontz T.A.: High-Density Renewable Diesel and Jet Fuels Prepared from Multicyclic Sesquiterpanes and a 1‑Hexene-Derived Synthetic Paraffinic Kerosene, Energy Fuels, 2013.
38.	Hemighaus G., Boval T., Bosley C.: Alternative Jet Fuels. Addendum 1 to Aviation Fuels Technical Review (FTR-3/A1). Chevron Corporation, 2006.
39.	Hileman J.I., Stratton R.W.: Alternative jet fuel feasibility. Transport Policy. Vol. 34, 2014, p. 52–62.
40.	Hileman J.I., Wong H.M., Waitz I.: Near-Term Feasibility of Alternative Jet Fuels. Santa Monica, California: RAND Corporation, 2009.
41.	Hileman, J. Ortiz D., Bartis J.: Near-Term Feasibility of Alternative Jet Fuels. Jointly published by the RAND Corporation (Report No. TR-554-FAA) and the Partnership for Air Transportation Noise and Emissions Reduction, 2009.
42.	Honga T.D., Soerawidjajab T.H., Reksowardojoa I.K.: A study on developing aviation biofuel for the Tropics: Production process – Experimental and theoretical evaluation of their blends with fossil kerosene, Chemical Engineering and Processing: Process Intensification, Vol. 74, 2013, p. 124–130.
43.	Hristova M., Tchaoushev S.: Сalculation of flash points and flammability limits of substances and mixtures. Journal of the University of Chemical Technology and Metallurgy, Iss. 41(3), p. 291–296, 2006.
44.	Hu J., Du Z., Li C., Min E.: Study on the lubrication properties of biodiesel as fuel lubricity enhancers, Fuel. Vol. 84, 2005. p. 1601–1606. 
45.	Iakovlieva A., Boichenko S., Vovk O.: Investigation of the fractional composition of rape oil-derived aviation biofuels. Aviation in the XXI-st century. Safety in aviation and space technologies: the fifth world congress, 25–27 September 2012: abstracts. Kyiv, Vol. 3, 2012, p. 5.41–5.43.
46.	Iakovlieva A.V. Boichenko S.V., Vovk O.O.: Overview of innovative technologies for aviation fuels production. Journal of Chemistry and chemical technology, Vol. 7. Iss. 3, 2013, p. 305–312.
47.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Peculiarities of the development and implementation of aviation biofuels in Ukraine. World Congress on Petrochemistry and Chemical Engineering. Journal of Petroleum & Environmental Biotechnology. November 2013, San Antonio. Vol.4. Iss. 6, 2013, p. 47.
48.	Iakovlieva A., Boichenko S., Gay A.: Cause-Effect Analysis of the Modern State in Production of Jet Fuels. Journal of Сhemistry & Chemical Technology. Vol. 8. No 1, 2014, p. 107–116.
49.	Iakovlieva A., Boichenko S., Vovk O., Lejda K.: Potential of jet biofuels production and application in Ukraine and Poland. International Journal of Sustainable Aviation. Vol. 1. No.4, 2015, p. 314–323.
50.	Iakovlieva A., Boichenko S., Lejda K.: Impact of rape oil ethyl esters additives on some characteristics of jet fuel. Проблеми хіммотології. Теорія та практика раціонального використання традиційних і альтернативних паливно -мастильних матеріалів: V міжнар. наук.-техн. конф., 6–10 жовт. 2014. Київ, c. 286 – 289. 
51.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Skilniuk I.: Vacuum Distillation of Rapeseed Oil Esters for Production of Jet Fuel Bio-Additives, Procedia Engineering, Vol. 187, 2017, p. 363 – 370. 
52.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Рotential of jet biofuels production and application in Ukraine and Poland. Proceedings of the 1st International Simposium on Sustainable Aviation.–31 May–03 June 2015, Isntanbul, p. 137.
53.	Iakovlieva A., Boichenko S., Lejda K.: Experimental study on antiwear properties for blends of jet fuel with biocomponents derived from rapeseed oil. Eastern-European journal of enterprise technologies. No. 5/8(77), 2015, p. 20–28.
54.	Iakovlieva A., Vovk O., Boichenko S.: Еxperimental study of rape oil esters influence on physical-chemical properties of jet fuels. Proceedings of the 19th Conference for Junior Researchers ‘Science – Future of Lithuania’ Тransport engineering and management, 6 May 2016, Vilnius. p. 85–89.
55.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Kuszewski H. Improvement of technological scheme of fatty acids ethyl esters production for use as jet fuels biocomponents. International Journal of Theoretical and Applied Science. Iss. 11(19), 2014, p. 44–55.
56.	International Air Transport organization. Vision 2050. Report. Montreal. Geneva, 2011.
57.	Jansen R. A.: Second Generation Biofuels and Biomass: Essential Guide for Investors, Scientists and Decision Makers. Wiley. 2012.
58.	Jenkins R.W., Munro M., Christopher S.N., Chuck C.: Potential renewable oxygenated biofuels for the aviation and road transport sectors. Fuel, Vol. 103, 2013, p. 593–599.
59.	Jacyna M., Żak J., Jacyna-Gołda I., Merkisz J., Merkisz-Guranowska A., Pielecha J.: Selected aspects of the model of proecological transport system. Journal of KONES Powertrain and Transport, Vol. 20, No. 3, 2013, p. 193 – 202.
60.	Kallio P., Pasztor A., Akhtar M.K., Jones P.R.: Renewable jet fuel. Current Opinion in Biotechnology. Vol. 26, 2014, p. 50–55.
61.	Kandaramath Hari T., Yaakob Z., Binitha N.N.: Aviation biofuel from renewable resources: Routes, opportunities and challenges. Renewable and Sustainable Energy Reviews. Vol. 42, 2015, p. 1234–1244. 
62.	Kinder J. D., Rahmes T.: Evaluation of Bio-Derived Synthetic Paraffinic Kerosene (Bio-SPK). The Boeing Company Sustainable Biofuels Research&Technology Program, 2009.
63.	Kirklin P.W., David. P.: Aviation Fuel: Thermal Stability. ASTM International, 1992.
64.	Lapuerta M., Rodriguez-Fernandeza J., Estevez C., Bayarri N.: Properties of fatty acid glycerol formal ester (FAGE) for use as a component in blends for diesel engines. Biomass and bioenergy. Vol. 76, 2015, p. 130–140. 
65.	Lebedevas S., Vaicekauskas A.: Research into the application of biodiesel in the transport sector of Lithuania. Transport. Vol. 21, Iss. 2, 2006, p. 80–87.
66.	Liu G., Yan B., Chen G.: Technical review on jet fuel production. Renewable and Sustainable Energy Reviews. Vol. 25, 2013, p. 59–70. 
67.	Lu M., Chai M.: Experimental Investigation of the Oxidation of Methyl Oleate: One of the Major Biodiesel Fuel Components Synthetic Liquids Production and Refining. Chapter 13, P. 289–312. American Chemical Society. 2011
68.	Merkisz J., Merkisz-Guranowska, A., Pielecha J., Nowak M., Jacyna M., Lewczuk K., Żak J.: Exhaust emission measurements in the development of sustainable road transport. Journal of KONES Powertrain and Transport, Vol. 20, No. 4 2013, p. 277 – 284.
69.	Maksimuk Yu., Antonova Z., Fes’ko V., Kursevich V.: Diesel biofuel viscosity and heat of combustion. Chemistry and technology of fuels and oils. Iss. 45, 2009, p. 343–346.
70.	Maru M. M., Trommer R.M., Cavalcanti K.F.: The Stribeck curve as a suitable characterization method of the lubricity of biodiesel and diesel blends. Energy. Vol. 69, 2014, p. 673–681.
71.	Maurice L.Q., Lander H., Edwards T., Harrison W.E.: Advanced aviation fuels: a look ahead via a historical perspective. Fuel. Vol. 80, Iss. 5, 2001, p. 747–756.
72.	Merkisz J., Markowski J., Pielecha J. Emission tests of the AI-14RA aircraft engine under real operating conditions of PZL-104"" Wilga"" plane. Silniki Spalinowe. No. 3, 2009, p. 64–70.
73.	Merkisz J., Galant M., Karpiński D., Kubiak, K. Evaluation of possibility to use the LTO cycle for emission test on example of the model turbine engine GTM-120 Journal of Mechanical and Transport Engineering. Vol. 66, No. 2, 2014, p. 25—33.
74.	Murphy D.J., Hall C.A.S.: Year in review—EROI or energy return on (energy) invested. Annals of the New York academy of sciences. Issue: Ecological Economics Reviews. Iss. 1185, 2010, p. 102–118.
75.	Murphy D.J., Hall C.A.S., Powers B.:New perspectives on the energy return on (energy) investment (EROI) of corn ethanol. Environment, Development and Sustainability. Vol. 13, Iss. 1, 2011, p. 179–202.
76.	Naik S.N., Goud V.V., Rout P.K., Dalai A.K.: Production of first and second generation biofuels: A comprehensive review. Renew. Sust. Energ. Rev., No. 14, 2010, p. 578–597.
77.	Nollet Leo M. L.: Handbook of Food Analysis: Physical characterization and nutrient analysis. CRC Press, 2004.
78.	Orszulik S.: Environmental Technology in the Oil Industry. Springer Science & Business Media, 2013.
79.	Pandey A.: Biofuels: Alternative Feedstocks and Conversion Processes. Academic Press, 2011.
80.	Pearlson M.N.: A techno-economic and environmental assessment of hydroprocessed renewable distillate fuels. Master of Science in Technology and Policy. Massachiussets Institute of Technology. June 2011.
81.	Prag P.: Renewable Energy in the Countryside. Taylor & Francis, 2014.
82.	Prussi M, Chiaramonti D, Recchia L, Martelli F, Guidotti F, Pari L.: Alternative feedstock for the biodiesel and energy production: the OVEST project. Energy Journal, No. 58, 2013, p. 2–8.
83.	Rahmes T.F., Kinder J.D., Henry T.M., etc.: Sustainable Bio-Derived Synthetic Paraffinic Kerosene (BioSPK) Jet Fuel Flights and Engine Tests Program Results. American Institute of Aeronautics and Astronautics, 2009.
84.	Rajagopal D., Zilberman D.: Environmental, Economic and Policy Aspects of Biofuels. Nеw Publishers Inc., 2008.
85.	Report on alternative fuels. International Air Transport Association IATA. http://www.iata.org/publications/Documents/2012-report-alternativefuels. pdf; 2012
86.	Rosillo Calle F, Trhan D, Seiffert M, Teeluckingh S. The potential and role of biofuels in commercial air transport – biojetfuels. Task 40 sustainable international bioenergy trade. IEA Bioenergy
87.	Sarin R., Kumar R., Srivastav B., etc.: Biodiesel surrogates: Achieving performance demands. Bioresource Technology. Vol. 100, Iss. 12, 2009, p. 3022–3028. 
88.	Shen Y.. Аn experimental study on thermal stability of FAEE biodiesel fuel with ethanol. Master Thesis, 2015.
89.	Shepherd J.E., Nuyt C.D., Lee J.J.: Flash Point and Chemical Composition of Aviation Kerosene (Jet A). National Transportation Safety Board, 2000.
90.	Singh B.: Biofuel Crops: Production, Physiology and Genetics. CABI, 2013.
91.	Singh B.: Biofuel Crop Sustainability. John Wiley & Sons, 2013.
92.	Sperling D., Cannon J.S.: Reducing Climate Impacts in the Transportation Sector. Springer Science & Business Media, 2011.
93.	Szczerek M., Tuszyсski W. Tribological researches – scuffing. Radom: Institute for Sustainable Technologies – National Research Institute, 2000.
94.	The jet engine. Rolls-Royce plc. Renault Printing Co Ltd., 1996.
95.	T-02U. Aparat czterokulowy – instrukcja obsługi. Radom: Wydawnictwo Instytutu Technologii Eksploatacji, 2011.
96.	Wcisło G.: Determination of the impact of FAME biocomponent on the fractional composition of diesel engine fuels. Combustion Engines. Iss. 154(3), 2013, p. 1098–1103.
97.	Xu Y., Wang Q., Hu X.: Characterization of the lubricity of bio-oil/diesel fuel blends by high frequency reciprocating test rig. Energy. Vol. 35, Iss. 1, 2010, p. 283–287. 
98.	Yakovleva A.V., Boichenko S.V., Lejda K, Vovk O.O., Kuszewski H.: Antiwear Properties of Plant—Mineral-Based Fuels for Airbreathing Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 1, 2017, p. 1–9. 
99.	Yakovlieva A.V., Boichenko S.V., Leida K., Vovk O.A., Kuzhevskii Kh.. Influence of Rapeseed Oil Ester Additives on Fuel Quality Index for Air Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 3, 2017. p. 308–317.
100.	Yakovlieva A., Boichenko S., Vovk O., Lejda K., Gryshchenko O.. Case Study of Alternative Jet Fuel Production with Bio-additives from Plant Oils in Ukraine and Poland. Advances in Sustainable Aviation. Springer International Publishing, 2018. Chapter 4.
101.	Yakovlieva A., Boshkov V. Experimental study of low-temperature properties of alternative aviation fuels, Proceedings of the 21th Conference for Junior Researchers ‘Science – Future of Lithuania’ Transport Engineering and Management, 4-5 May 2018, Vilnius, Lithuania. 2018. p. 130 – 134.
102.	Yildirim U, Abanteriba S.: Manufacture, qualification and approval of new aviation turbine fuels and additives, proceedia Engineering, No. 49, 2012, p. 310 – 315.
103.	Yutko B. and Hansman J., Approaches to Representing Aircraft Fuel Efficiency Performance for the Purpose of a Commercial Aircraft Certification Standard, MITInternational Center for Air Transportation, Cambridge, Mass, 2011.
104.	Zhu Y.: An Experimental Study on Thermal Stability of Biodiesel Fuel. Master Thesis. – 2012. – 160 p.
105.	Авиационный турбореактивный двигатель РУ 19A-300, руководство по эксплуатации и техническому обслуживанию, ЗАО «АНТЦ Технолог», 2001.
106.	Азев В.С., Середа А.В.: Влияние соединений серы на противоизносные свойства дизельных топлив, Химия и технология топлив и масел. № 3, 2009, c. 23–27.
107.	Андіїшин М.П., Марчук Я.С., Бойченко С.В., Рябоконь Л.А.: Газ природний, палива та оливи. Одеса: Астропринт, 2010.
108.	Бойченко С.В., Спіркін В.Г. Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт, Ч.1., 2009.
109.	Бойченко С.В., Любінін Й.А., Спіркін В.Г.: Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт. Ч.2., 2010.
110.	Бойченко С.В., Черняк Л.М., Яковлєва А.В.: Традиційні технології виробництва палив для повітряно-реактивних двигунів. Вісник Національного авіаційного університету. № 2 (55), 2013, с. 195–209.
111.	Бойченко С. В., Яковлева А. В., Волошинец В. А., Лейда К. Модифицирование эфиров рапсового масла вакуумным фракционированием, Технологии нефти и газа, №5, 2018, c. 15–20
112.	Братичак М.М.: Основи промислової нафтохімії, Львів: Вид-во НУ «Львівська політехніка», 2008.
113.	Васильев И.П.: Влияние топлив растительного происхождения на экологические и экономические показатели дизеля, Луганск: Изд-во ВНУ им. В. Даля, 2009.
114.	Волошинець В.А. Фізична та колоїдна хімія: Фізико-хімія дисперсних систем та полімерів: навч.посіб. Львів : Вид-во Львів. політехніки, 2013. – 200 с.
115.	Голоскоков А.Н. Критерии сравнения эффективности традиционных и альтернативных энергоресурсов. Нефтегазовое дело. № 1, 2011, c. 285–301.
116.	Голоскоков А.Н. Пик добычи нефти и начало мирового энергетического кризиса. Нефтегазовое дело. 2010, c. 1–13.
117.	Данилов А.М., Каминский Э.Ф., Хавкин В.А.: Альтернативные топлива: достоинства и недостатки. Проблемы применения. Российский химический журнал (Журнал Российского химического общества им. Д.И. Менделеева). Т. XLVII. № 6, 2003, c. 4–11.
118.	Дворецкий С.И., Нагорнов С.А., Романцова С.В. и др.: Производство биодизельного топлива из органического сырья. Вопросы современной науки и практики. № 39, 2012, c. 126– 35.
119.	Девянин С.Н., Марков В.А., Семенов В.Г.: Растительные масла и топлива на их основе для дизельных двигателей. Харьков: Новое слово. 2007.
120.	Ергин Д.: Добыча: Всемирная история борьбы за нефть, деньги и власть. Москва,: Альпина Паблишер, 2011.
121.	Запорожець А.О.: Дослідження стехіометричної суміші «повітря ‒ паливо» органічних сполук. Частина 1. Алкани. Наукоємні технології. № 2(22), 2014, c. 163–167.
122.	Кириченко В., Бойченко С., Кириченко В., Нездоровин В.: Комплексная переработка технических растительных масел: концепция, методы и технологи. «Systems and means of motor transport» Seria: Transport. Monografia. № 4, 2013, p. 357–370.
123.	Колодницька Р.В., Семенов В.Г.: Моделювання низькотемпературних властивостей біодизельних палив. Вісник СевНТУ. Серія: Машиноприладобудування та транспорт. № 134, 2012, c. 135–138.
124.	Коллоидная химия нефти и нефтепродуктов: Сборник материалов, посвященных научной деятельности проф. Г.И. Фукса. Москва: Изд-во «Техника». ООО «Тума Групп», 2001.
125.	Крылов И.Ф., Емельянов В.Е.: Альтернативные моторные топлива. Производство, применение",'National Aviation University',Modification of jet fuels composition with renewable bio-additives,https://core.ac.uk/download/286618536.pdf,10.18372/37895,,core
153684881,2018-02-01T00:00:00Z,"Jellyfish can form erratic blooms in response to seasonal and irregular changes in environmental conditions with often large, transient effects on local ecosystem structure as well as effects on several sectors of the marine and maritime economy. Early warning systems able to detect conditions for jelly fish proliferation can enable management responses to mitigate such effects providing benefit to local ecosystems and economies. We propose here the creation of a research team in response to the EU call for proposal under the European Maritime and Fisheries Fund called “Blue Labs: innovative solutions for maritime challenges”. The project will establish a BLUELAB team with a strong cross-sectorial component that will benefit of the expertise of researchers in IT and Marine Biology, Computer Vision and embedded systems, which will work in collaboration with Industry and Policy maker to develop an early warning system using a new underwater imaging system based on Time of Flight Laser cameras. The camera will be combined to machine learning algorithm allowing autonomous early detection of jellyfish species (e.g. polyp, ephyra and planula stages). The team will develop the system and the companion software and will demonstrate its applications in real case conditions","[{'title': None, 'identifiers': ['2367-7163', 'issn:2367-7163']}]",Pensoft Publishers,Jellyfish Identification Software for Underwater Laser Cameras (JTRACK),10.3897/rio.4.e24716,,core
288595498,2018,"Industrial energy management is an important topic of discussion nowadays for both economic and sustainability reasons. A monitoring and control system able to guarantee the practice of a real-time control is a key point in enacting an effective management of energy consumption in a complex organization. In this context, the ISO 50000 family of standards suggest the application of different types of energy performance indicators (EnPIs), in a range of varying complexity: from simple absolute values of energy consumption, to statistical models, to engineering models. The evolution of machine learning techniques falls between the statistical and the engineering models, depending on the volume of data and the human involvement required for building a model. Therefore, the value of the present work is to explore the use of these tools, already consolidated in other fields, but not yet adequately assessed for energy performance control. In particular, the generation and distribution of compressed air is among the biggest uses of energy in production plants. This work starts with the application of the classical statistical approach and then proceeds to compare two different machine learning techniques, artificial neural networks and support vector machines, for the creation of energy performance indicators. The analysis begins comparing the feasibility of application, implementation complexity, data and level of human interaction required, making use of the results of a real application to a compressed air generation unit in a production plant. The comparison was then carried out using various performance indicators (R-squared, Mean Squared Error, Mean Absolute Percentage Error) as well as a graphical inspection of the resulting control charts produced with the different models. The work demonstrates the applicability of machine learning techniques in this specific context, proving them as an efficient compromise between the complexity and accuracy of statistical and engineering models",,AIDI - Italian Association of Industrial Operations Professors,Evaluation of machine learning techniques to enact energy consumption control of compressed air generation in production plants,,,core
224986396,2018-01-01T00:00:00,"Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The 'smart farm' model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business",,'Institute of Electrical and Electronics Engineers (IEEE)',Smart Farms for a Sustainable and Optimized Model of Agriculture,10.23919/AEIT.2018.8577226,,core
328259578,2018-09-03T00:00:00,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology’s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders—consumers, agents and associations, government and regulatory authorities, and complementary industries—and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers’ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers’ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",,'MDPI AG',"A Systematic Review of Smart Real Estate Technology: Drivers of, and Barriers to, the Use of Digital Disruptive Technologies and Online Platforms",10.3390/su10093142,http://www.mdpi.com/2071-1050/10/9/3142,core
186300971,2018-12-18T00:00:00,"Conventional feedback control methods can solve various types of robot
control problems very efficiently by capturing the structure with explicit
models, such as rigid body equations of motion. However, many control problems
in modern manufacturing deal with contacts and friction, which are difficult to
capture with first-order physical modeling. Hence, applying control design
methodologies to these kinds of problems often results in brittle and
inaccurate controllers, which have to be manually tuned for deployment.
Reinforcement learning (RL) methods have been demonstrated to be capable of
learning continuous robot controllers from interactions with the environment,
even for problems that include friction and contacts. In this paper, we study
how we can solve difficult control problems in the real world by decomposing
them into a part that is solved efficiently by conventional feedback control
methods, and the residual which is solved with RL. The final control policy is
a superposition of both control signals. We demonstrate our approach by
training an agent to successfully perform a real-world block assembly task
involving contacts and unstable objects.Comment: 7 page",,,Residual Reinforcement Learning for Robot Control,,http://arxiv.org/abs/1812.03201,core
222869263,2018-08-26T00:00:00,"Part 5: Industry 4.0 - Digital TwinInternational audienceAdvanced technologies based on Internet of Things (IOT) are blazing a trail to effective and efficient management of an overall plant. In this context, manufacturing companies require an innovative strategy to survive in a competitive business environment, utilizing those technologies. Guided by these requirements, the so-called predictive maintenance is of paramount importance and offers a significant potential for innovation to overcome the limitations of traditional maintenance policies. However, real shop-floors often have obstacles in providing insights to facilitate the effective management of assets in smart factories. Even if a significant amount of machine and process data is available, one of the common problems of these data is the lack of annotations describing the machine status or maintenance history. For this reason, companies have limited options to analyse manufacturing data, despite the capability of advanced machine learning techniques in supporting the identification of failure symptoms in order to optimize scheduling of maintenance operations. Moreover, each machine generates highly heterogeneous data, making it difficult to integrate all the information to provide data-driven decision support for predictive maintenance. Inspired by these challenges, this research provides a hybrid machine learning approach combining unsupervised learning and semi-supervised learning. The approach and result in this article are based on the development and implementation in a large collaborative EU-funded H2020 research project entitled BOOST 4.0 i.e. Big Data Value Spaces for COmpetitiveness of European COnnected Smart FacTories",,'Springer Science and Business Media LLC',A Hybrid Machine Learning Approach for Predictive Maintenance in Smart Factories of the Future,10.1007/978-3-319-99707-0_39,,core
217593290,2018-01-01T00:00:00,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98 % given different tests with different types of residuals and duct structures. \ua9 2005-2012 IEEE",,'Institute of Electrical and Electronics Engineers (IEEE)',Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,10.1109/TII.2018.2807797,,core
189835451,2018-06-29T00:00:00,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context","[{'title': 'Proceedings of the IEEE', 'identifiers': ['0018-9219', 'issn:0018-9219']}]",'Institute of Electrical and Electronics Engineers (IEEE)',"Navigating the landscape for real-time localisation and mapping for robotics, virtual and augmented reality",10.1109/JPROC.2018.2856739,,core
296841133,2018-08-07T06:46:12,"Orientadores: Rubens Maciel Filho, Aline Carvalho da CostaDissertação (mestrado) - Universidade Estadual de Campinas, Faculdade de Engenharia QuimicaResumo: A utilização do etanol como combustível tem muitas vantagens; ele tem competido economicamente com a gasolina e diversos outros combustíveis, substituindo-os em várias utilidades. Desta forma, existe um grande interesse em se otimizar todos os passos da produção de etanol. Apenas um conhecimento profundo da dinâmica do processo gera uma operação ótima, e este pode ser conseguido através de simulações realizadas usando um modelo preciso. Muitos modelos fenomenológicos foram desenvolvidos considerando condições industriais, mas estes só são válidos para condições específicas nas quais foram determinados, invalidando a predição do modelo em outras condições. Mudanças acontecem normalmente em uma unidade industrial e a re-estimação freqüente dos parâmetros do modelo é usualmente difícil e demorada. O objetivo deste trabalho é desenvolver um modelo híbrido neuronal para o processo de fermentação alcoólica usando balanços de massa combinados com redes neuronais do tipo Functional Link. Será implementado um esquema para atualização dos pesos da rede sempre que esta não descrever o comportamento dinâmico da planta. O modelo desenvolvido será usado para descrever um processo real no lugar dos modelos fenomenológicos existentes, já que estes têm sido capazes de descrever o processo apenas por curtos espaços de tempoAbstract: The use of ethanol as a fuel has many advantages; it has economically competed with gasoline and others fuels, substituting them in various uses. Thus, there is a great interest in optimizing all the steps of ethanol production. Only a detailed knowledge of the process dynamics can lead to optimal operation and this can be achieved through simulation using an accurate model. Many phenomenological models were developed considering industrial fermentations, but they are only valid for specific conditions. Changes occur frequently in an industrial unity and frequent reestimation of model parameters is usually expensive and time consuming. The objective of this work is to develop a hybrid neural model for the alcoholic fermentation process using mass balances combined with Functional Link Neural Networks. A scheme to update network weights always that it does not describe plant behavior accurately is implemented. The developed model is used to describe an industrial process substituting the existing phenomenological models, since they have been able to describe the process only for short periodsMestradoDesenvolvimento de Processos BiotecnologicosMestre em Engenharia Químic",,[s.n.],Hybrid neural network model of an alcoholic fermentation process,,https://core.ac.uk/download/296841133.pdf,core
160435451,2018-05-09T16:53:48Z,"<p>Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.</p>

<p><strong>Reference: </strong>Chen, Ning, Steven CH Hoi, and Xiaokui Xiao. ""Software process evaluation: A machine learning approach."" <em>Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering</em>. IEEE Computer Society, 2011.</p",,,Defect Prediction: SPE,10.5281/zenodo.268487,,core
228050071,2018-10-26T00:00:00,"We propose and study a novel artificial neural network framework, which allows us to model surgical interventions on a physical system. Our approach was developed to predict power flows in power transmission grids, in which high voltage lines are disconnected and reconnected with one-another from time to time, either accidentally or willfully. However, we anticipate a broader applicability. For several exemplary cases, we illustrate by simulation that our methodology permits learning from empirical data to predict the effect of a subset of interventions (ele-mentary interventions) and then generalize to combinations of interventions never seen during training. We verify this property mathematically in the additive perturbation case. In terms of transfer learning, this is equivalent to training on data from a few source domains then, with a zero-shot learning, generalizing to new target domains (super-generalization). Our architecture bears resemblance with the successful ResNets, with the simple modification that interventions are encoded as an addition of units in the neural network. For applications to real historical data, from the French high voltage power transmission company RTE, we evaluate the viability of this technique to rapidly assess curative actions that human operators take in emergency situations. Integrated in an overall planning and control system, methods deriving from our approach could allow Transmission System Operators (TSO) to assess in real time many more alternative actions, reaching a better exploration-exploitation tradeoff, compared to presently deployed physical system simulator. 1 Background and motivations In this paper, we are interested in speeding up the computation of power flows in power transmission grids using artificial neural networks, to emulate slower physical simulators. Key to our approach is the possibility of simulating the effect of actions on the grid topology. Such neural networks may then be used as part of an overall computer-assisted decision process in which human operators (dispatchers) ensure that the power grid is operated in security at all times, namely that the currents flowing in all lines are below certain thresholds (line thermal limits). We describe our application setting for concreteness, but anticipate a broader applicability of the techniques developed in this paper in various domains of physics, chemistry, manufacturing, biomedicine and others, in which some actions can be combined with each other, but running extensive simulations for each possible combination of such actions is computationally untractable. Electric power generated in production nodes (such as power plants) is transmitted towards consumption nodes in a power grid. The power lines enable this transmission through substations interconnecting them. Each pattern of connections is referred to as a grid topology. This topology is * Benjamin Donnot corresponding authors: benjamin.donnot@inria.com 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada",,HAL CCSD,Latent Surgical Interventions in Residual Neural Networks,,,core
159217765,2018-01-01T00:00:00,Different types of software components and data have to be combined to solve an artificial intelligence challenge. An emerging marketplace for these components will allow for their exchange and distribution. To facilitate and boost the collaboration on the marketplace a solution for finding compatible artifacts is needed. We propose a concept to define compatibility on such a marketplace and suggest appropriate scenarios on how users can interact with it to support the different types of required compatibility. We also propose an initial architecture that derives from and implements the compatibility principles and makes the scenarios feasible. We matured our concept in focus group workshops and interviews with potential marketplace users from industry and academia. The results demonstrate the applicability of the concept in a real-world scenario,,'Springer Science and Business Media LLC',Artifact Compatibility for Enabling Collaboration in the Artificial Intelligence Ecosystem,10.1007/978-3-030-04840-2_5,,core
160100367,2018-01-01T00:00:00,"The manufacturing industry represents a data rich environment, in which larger and larger volumes of data are constantly being generated by its processes. However, only a relatively small portion of it is actually taken advantage of by manufacturers. As such, the proposed Intelligent Data Analysis and Real-Time Supervision (IDARTS) framework presents the guidelines for the implementation of scalable, flexible and pluggable data analysis and real-time supervision systems for manufacturing environments. IDARTS is aligned with the current Industry 4.0 trend, being aimed at allowing manufacturers to translate their data into a business advantage through the integration of a Cyber-Physical System at the edge with cloud computing. It combines distributed data acquisition, machine learning and run-time reasoning to assist in fields such as predictive maintenance and quality control, reducing the impact of disruptive events in production.info:eu-repo/semantics/publishedVersio","[{'title': 'Computers in Industry', 'identifiers': ['0166-3615', 'issn:0166-3615']}]",'Elsevier BV',IDARTS – Towards intelligent data analysis and real-time supervision for industry 4.0,10.1016/j.compind.2018.07.004,,core
154985357,2018-03-30T00:00:00,"Computer vision has advanced significantly that many discriminative
approaches such as object recognition are now widely used in real applications.
We present another exciting development that utilizes generative models for the
mass customization of medical products such as dental crowns. In the dental
industry, it takes a technician years of training to design synthetic crowns
that restore the function and integrity of missing teeth. Each crown must be
customized to individual patients, and it requires human expertise in a
time-consuming and labor-intensive process, even with computer-assisted design
software. We develop a fully automatic approach that learns not only from human
designs of dental crowns, but also from natural spatial profiles between
opposing teeth. The latter is hard to account for by technicians but important
for proper biting and chewing functions. Built upon a Generative Adversar-ial
Network architecture (GAN), our deep learning model predicts the customized
crown-filled depth scan from the crown-missing depth scan and opposing depth
scan. We propose to incorporate additional space constraints and statistical
compatibility into learning. Our automatic designs exceed human technicians'
standards for good morphology and functionality, and our algorithm is being
tested for production use",,,"Learning Beyond Human Expertise with Generative Models for Dental
  Restorations",,http://arxiv.org/abs/1804.00064,core
286807617,2018-12-13,,,HAL CCSD,"Research presented in this report is based on:1. Problem Submission: Firms submitted corporate challenges relating to digital business models across several industries. Problems were screened and selected.2. Problem Framing: Professor Darwin conducted individual sessions with individual firms to solicit input from both open innovation researchers and practitioners.3. Problem Solving: Input, feedback, and recommendations provided by a community of academic experts and open innovation practitioners across industries who worked deliberated in groups of eight during a one-hour session per challenge.Challenge #1: WIPRO ...................................................................................................................................... 3“Wipro is getting ready to deploy its newly developed digital technology (AR/VR/AI) “do-your-own repairs tool” that will help people repair their white goods in their homes. With what business model could the company deploy this new technology?”Challenge #2: DAIMLER AG ............................................................................................................................. 8As Daimler enters new markets, such as autonomous vehicles, how can a mobility services firm accelerate internal innovation against uncharted territories in the uncertain times of digital transformation?Challenge #3: KANEKA .................................................................................................................................. 12Kaneka wants to improve innovation internally through the following lens: how can Kaneka accelerate internal innovation utilizing a two-sided digital ideation/challenge platform through which it can address company’s internal and external challenges?Challenge #4: ALLERGAN ............................................................................................................................... 16What innovative digitally driven design would you suggest for supporting busy physicians and/or patients, to get them relevant, accurate, reliable and real-time information and analysis quickly?Challenge #5: APPLIED MATERIAL ................................................................................................................. 22How AMAT can leverage its materials engineering capabilities to enter new markets with platform extensions powered by collaborations with external ecosystem partners?Challenge #6: XIAOMI ................................................................................................................................... 26Xiaomi offers High-Value/Low-Cost/Low-Margin products to all customer segments in emerging markets. This demands severe cost curtailment strategies in manufacturing, operation, advertisement, sales, distribution and servicing of its products. Xiaomi cannot deliver this value alone without an ecosystem to sustain and scale the business. How can the government, corporations and other institutions help create a win-win for all? In addition, rural communities lack infrastructure (reliable connectivity, power, healthcare, clean water, accessible roads). What must Xiaomi do to serve and expand the market when this infrastructure is lacking?",,,core
163114948,2018-10-19T09:15:08,"Artificial intelligence is an unavoidable asset of Industry 4.0. Artificial actors participate in real-time decision-making and problem solving in various industrial processes, including planning, production, and management. Their efficiency, as well as intelligent and autonomous behavior is highly dependent on the ability to learn from examples, which creates new vulnerabilities exploited by security threats. Today's disruptive attacks of hackers go beyond system's infrastructures targeting not only hard-coded software or hardware, but foremost data and trained decision models, in order to approach system's intelligence and compromise its work. This paper intends to reveal security threats which are new in the industrial context by observing the latest discoveries in the AI domain. Our focus is data poisoning attacks caused by adversarial training samples and subsequent corruption of machine learning process.peerReviewe",,'IOS Press',Industry 4.0 Intelligence under Attack : From Cognitive Hack to Data Poisoning,10.3233/978-1-61499-888-4-110,,core
215385330,2018-07-10T16:30:00,"A smart environment is a physical space that is seamlessly embedded with sensors, actuators, displays, and computing devices, connected through communication networks for data collection, to enable various pervasive applications. Radio frequency identification (RFID) and Wireless Sensor Networks (WSNs) can be used to create such smart environments, performing sensing, data acquisition, and communication functions, and thus connecting physical devices together to form a smart environment.
This thesis first examines the features and requirements a smart industrial environment. It then focuses on the realization of such an environment by integrating RFID and industrial WSNs. ISA100.11a protocol is considered in particular for WSNs, while High Frequency RFID is considered for this thesis. This thesis describes designs and implementation of the hardware and software architecture necessary for proper integration of RFID and WSN systems. The hardware architecture focuses on communication interface and AI/AO interface circuit design; while the driver of the interface is implemented through embedded software. Through Web-based Human Machine Interface (HMI), the industrial users can monitor the process parameters, as well as send any necessary alarm information. In addition, a standard Mongo database is designed, allowing access to historical and current data to gain a more in-depth understanding of the environment being created. The information can therefore be uploaded to an IoT Cloud platform for easy access and storage.
Four scenarios for smart industrial environments are mimicked and tested in a laboratory to demonstrate the proposed integrated system. The experimental results have showed that the communication from RFID reader to WSN node and the real-time wireless transmission of the integrated system meet design requirements. In addition, compared to a traditional wired PLC system where measurement error of the integrated system is less than 1%. The experimental results are thus satisfactory, and the design specifications have been achieved",,Scholarship@Western,Integration of RFID and Industrial WSNs to Create A Smart Industrial Environment,,https://core.ac.uk/download/215385330.pdf,core
186292144,2018-11-18T00:00:00,"Deep Learning is increasingly being adopted by industry for computer vision
applications running on embedded devices. While Convolutional Neural Networks'
accuracy has achieved a mature and remarkable state, inference latency and
throughput are a major concern especially when targeting low-cost and low-power
embedded platforms. CNNs' inference latency may become a bottleneck for Deep
Learning adoption by industry, as it is a crucial specification for many
real-time processes. Furthermore, deployment of CNNs across heterogeneous
platforms presents major compatibility issues due to vendor-specific technology
and acceleration libraries. In this work, we present QS-DNN, a fully automatic
search based on Reinforcement Learning which, combined with an inference engine
optimizer, efficiently explores through the design space and empirically finds
the optimal combinations of libraries and primitives to speed up the inference
of CNNs on heterogeneous embedded devices. We show that, an optimized
combination can achieve 45x speedup in inference latency on CPU compared to a
dependency-free baseline and 2x on average on GPGPU compared to the best vendor
library. Further, we demonstrate that, the quality of results and time
""to-solution"" is much better than with Random Search and achieves up to 15x
better results for a short-time search",,'Institute of Electrical and Electronics Engineers (IEEE)',"Learning to infer: RL-based search for DNN primitive selection on
  Heterogeneous Embedded Systems",10.23919/DATE.2019.8714959,http://arxiv.org/abs/1811.07315,core
160788434,2018-08-30T00:00:00,"Cyber-physical systems often consist of entities that interact with each
other over time. Meanwhile, as part of the continued digitization of industrial
processes, various sensor technologies are deployed that enable us to record
time-varying attributes (a.k.a., time series) of such entities, thus producing
correlated time series. To enable accurate forecasting on such correlated time
series, this paper proposes two models that combine convolutional neural
networks (CNNs) and recurrent neural networks (RNNs). The first model employs a
CNN on each individual time series, combines the convoluted features, and then
applies an RNN on top of the convoluted features in the end to enable
forecasting. The second model adds additional auto-encoders into the individual
CNNs, making the second model a multi-task learning model, which provides
accurate and robust forecasting. Experiments on two real-world correlated time
series data set suggest that the proposed two models are effective and
outperform baselines in most settings.
  This report extends the paper ""Correlated Time Series Forecasting using
Multi-Task Deep Neural Networks,"" to appear in ACM CIKM 2018, by providing
additional experimental results",,,"Correlated Time Series Forecasting using Deep Neural Networks: A Summary
  of Results",,http://arxiv.org/abs/1808.09794,core
195730721,2018,"Industrial energy management is an important topic of discussion nowadays for both economic and sustainability reasons. A monitoring and control system able to guarantee the practice of a real-time control is a key point in enacting an effective management of energy consumption in a complex organization. In this context, the ISO 50000 family of standards suggest the application of different types of energy performance indicators (EnPIs), in a range of varying complexity: from simple absolute values of energy consumption, to statistical models, to engineering models. The evolution of machine learning techniques falls between the statistical and the engineering models, depending on the volume of data and the human involvement required for building a model. Therefore, the value of the present work is to explore the use of these tools, already consolidated in other fields, but not yet adequately assessed for energy performance control. In particular, the generation and distribution of compressed air is among the biggest uses of energy in production plants. This work starts with the application of the classical statistical approach and then proceeds to compare two different machine learning techniques, artificial neural networks and support vector machines, for the creation of energy performance indicators. The analysis begins comparing the feasibility of application, implementation complexity, data and level of human interaction required, making use of the results of a real application to a compressed air generation unit in a production plant. The comparison was then carried out using various performance indicators (R-squared, Mean Squared Error, Mean Absolute Percentage Error) as well as a graphical inspection of the resulting control charts produced with the different models. The work demonstrates the applicability of machine learning techniques in this specific context, proving them as an efficient compromise between the complexity and accuracy of statistical and engineering models",,AIDI - Italian Association of Industrial Operations Professors,Evaluation of machine learning techniques to enact energy consumption control of compressed air generation in production plants,,,core
196212456,2018,"Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-toend transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%",,,Real-time Performance Evaluation of LTE for IIoT,10.1109/LCN.2018.8638081,,core
226738815,2018-01-01T00:00:00,"It is with great pleasure that we present this Special Issue of
the Journal of Signal Processing Systems (JSPS) dedicated to
Embedded Computer Vision! We are pleased to include six
state-of-the-art papers from the leaders in this field, both from
industry and academia, who keep pushing the embedded computer vision technology forward.
While the idea for this special issue originated between the Guest Editors at one of the CVPR workshops
on the same topic that we have organized, it is the work
of the contributing authors that makes it a success. The
papers were solicited from the workshop participants
and through an open call for papers, so the initial submissions were in many ways already pre-filtered. Out of
24 submitted papers, the highly selective review process
yielded the six papers included here. They cover a
broad range of challenges that are encountered in practical deployment of embedded vision systems, especially
when high computational performance needs meet limited resources. We present papers describing a range of
novel solutions: a deep learning accelerator, a robust
aerial tracking system, an FPGA-based aerial visual
servoing task solution, an approach to use low-cost
hardware for real-time vision, a real-time motion detector, and an image enhancement approach based on human vision",,'Springer Science and Business Media LLC',Guest Editorial: Special Issue on Embedded Computer Vision,10.1007/s11265-018-1365-8,,core
304992940,2018-01-22T21:00:55,"The development of appropriate flight tests has proven to be a critical element in the development process of many revolutionary next-generation aerospace vehicles. For example, in the case of hypersonic vehicles with air-breathing SCRAMjet engines, sophisticated computational analyses have been developed which require extensive validation and calibration with physical test data. The current state of hypersonic ground testing facilities has not yet been able to accommodate these demands due to the inability to replicate hypersonic flow conditions with sufficient accuracy. These deficiencies have put increased demand and pressure on hypersonic flight testing experiments which have historically proven to produce the highest quality results but at the potential price of extreme complexity and expense. In the case of hypersonic flight testing for SCRAMjet vehicles, the combination of high expense, high complexity, and high modeling uncertainties has led to conservative, risk-averse experiments. These efforts have historically yielded little gain in knowledge, observing only marginal improvements to prediction confidence in the computational models. There is an entire discipline devoted to the process of design and information extraction from aerospace-type experiments known as aircraft system identification (SysID) which combines three interdependent topics: (i) computational modeling and simulation, (ii) experimental design methods, and (iii) statistical estimation techniques. Essentially, SysID attempts to develop time-dynamic experiments so that statistical estimation techniques can most effectively be used to identify high-confidence physics-based models. An implicit limitation to this process lies within the topic of dynamic experiment design, often posed as a mixed parameter optimization/optimal control problem for the concurrent design of aircraft maneuver inputs, instrumentation system parameters, flight conditions, test duration, etc. Here, Fisher information-based optimality criterion are sought to be used for the quantification of information quality; however, these metrics can only be accurately computed if the true values of the unknown model parameters (e.g. SCRAMjet aero-propulsive-elastic stability and control coefficients, vehicle mass/inertia parameters, etc.) are known prior to conducting an actual experiment, which is often not the case. This is commonly referred to as the circulatory problem in statistics literature, suggesting that dynamic optimal experiment design (DOED) requires an augmented robust-optimization approach (DROED) to account for modeling uncertainties. This research focuses on the design of flight-dynamic experiments from the perspective of an integrated system for the concurrent design of information-dense flight experiments which are robust with respect to model parameter uncertainties. The proposed methodology is called TEMPUS, which stands for Time-dynamic Experiment design using a Model-based approach to Propagate Uncertainty for System identification. By using the top-down design decision support process within the Georgia Tech Integrated Product/Process Development methodology (GT-IPPD), TEMPUS fuses elements from two existing experiment design methodologies to enable a systems engineering approach to the design of large-scale robust-optimal dynamic system identification experiments (such as the design of SCRAMjet-powered flight tests). Within this method the generation of feasible design alternatives is achieved via a sizing and synthesis method, providing for the concurrent design of measurement system parameters, control system architecture and parameters, probabilistic uncertainty models, aero-thermal-fluids models, design constraints, and even vehicle geometry and mission-level parameters. To assess the performance of a given experiment design, a variety of different information quality metrics are able to be calculated from a dynamic high-order sensitivity analysis, providing for an a priori estimate of expected goodness-of-fit quality in the a posteriori parameter estimators. To evaluate feasible alternatives, a virtual experimentation strategy is utilized to assess information performance metrics of a given alternative via nondeterministic techniques (e.g. Monte Carlo methods). Implementation of TEMPUS depends on the capability to perform a high-order dynamic sensitivity analysis on nonlinear industrial-sized aerospace flight-dynamic models (including guidance, navigation, and control logic) in a fashion that is both automatable and easily implementable by flight test designers and control systems engineers, all the while without introducing computational uncertainties. To address this challenge, an automatic differentiation tool specialized for use in dynamic experiment design was developed, providing for the ability to automatically compute robust-optimal Fisher information performance metrics by constructing variational asymptotic expansions (i.e. time-dynamic arrays of multivariate Taylor series expansions, parameterized by design and uncertainty perturbation variables). In general, these variational asymptotic expansions (VAEs) allow for a number of desirable capabilities for SysID applications, because they can essentially be considered as asymptotically accurate surrogate models to solutions of dynamic systems, including: (i) the construction of nominal Fisher information metrics (requiring at least 1st-order output-to-parameter sensitivity trajectories to be computed); (ii) the construction of arbitrarily high-order robust-optimal Fisher information metrics using both (deterministic and nondeterministic approaches to calculate robustness); (iii) rapid exploration of neighboring solutions to optimal control problems; and (iv) the implementation of arbitrarily high-order optimization algorithms (e.g. high-accuracy nonlinear parameter estimators in SysID) (not considered in this work). High-order VAEs can suffer from many of the same complications that often hinder high-order multivariate response surface equations (RSEs), such as: (i) poor numerical conditioning, (ii) diminishing returns on accuracy (e.g. slow rates of convergence, finite radii of convergence, etc.), and (iii) the curse of dimensionality (e.g. large computational times and memory requirements). Therefore prior to using VAEs for dynamic experiment design problems within TEMPUS, four developmental experiments were designed to study the adverse effects of diminishing returns on accuracy, the curse of dimensionality, and application of VAEs to create surrogates of optimal control problems on simple dynamic systems. These include: (i) investigating the potential improvements of using alternative sets of basis functions on problems where diminishing returns on accuracy are observed for the standard Taylor (monomial) basis; (ii) investigating the effects of diminishing returns on accuracy in dynamic uncertainty propagation using high-order VAEs and various probabilistic uncertainty models; (iii) investigating the computational time and memory complexities of high-order, high-dimensional VAEs for use in dynamic experiment design; and (iv) investigating how automatic differentiation can be used to generate high-order VAEs to solutions of optimal control problems. The objective of the fourth experiment is to overcome the limitations that many indirect numerical optimization methods possess, namely, being cumbersome, nonautomatable analyses which hinder the ability to perform design space exploration and uncertainty propagation analyses due to a human-in-the-loop dependency. The results of the first experiment suggest that the use of Chebyshev basis functions can alleviate problems where the diminishing returns on accuracy are observed when Taylor basis functions are used. In the second experiment, it was observed that even for uncertainty propagation with high-order VAEs that slow/poor convergence characteristics can result in adverse effects, such as artificial multi-modality in propagated uncertainty distributions. The results of the third experiment suggest that high-dimensional problems (such as experiment design problems) scale exponentially with increasing order, and therefore high-performance computing capabilities will be necessary to practically obtain robust-optimal dynamic experiment designs for large industrial-sized aerospace problems. In the final experiment, two high-order optimal control formulations were developed for computing VAE surrogates. Promising results were observed for a simple optimal control problem where VAE surrogates were successfully computed; however, more effort is needed before these formulations can be applied to larger dynamic experiment design problems. In light of the results of the aforementioned experiments, the TEMPUS methodology was applied to two design problems: (i) a simple mass-spring-damper problem under sinusoidal forcing, and (ii) the Generic Hypersonic Vehicle (GHV) model to design information-dense SCRAMjet-powered flight tests at steady-level flight under multi-sine forcing. In the first study, the small problem size allowed for investigation of high-order VAEs without experiencing the adverse effects due to the curse of dimensionality. Here, it was observed that robust-optimal experiment designs did produce probabilistic information metric distributions with better robustness with respect to parameter uncertainties than designs using the traditional nominal information metrics, and all experiment designs were found to produce intuitive results, serving as a form of validation (e.g. the sinusoidal forcing frequency was designed to excite the system near the expected natural frequency to maximize output-to-parameter sensitivities). For the flight test design problems, a nonlinear robust-adaptive flight controller is required to ensure safe operation throughout flight, because the GHV open-loop dynamics possess unstable, non-minimum phase behavior in the aero-propulsive-elastic modes in addition to the parametric uncertainties within the aero-propulsive-elastic stability and control coefficients. As a result, the complexity of the overall closed-loop model is greatly increased; however, computation of high-order VAEs for this system does not require any special attention in regards to practical implementation, but a substantial increase in computation time and memory was observed. The objective of experiment designs for the SCRAMjet-powered flight tests was to generate data for the system identification of eight thrust force stability and control coefficients: CTPA3, CTPA2, CTPA, CTP, CTA3, CTA2, CTA, CT0. For the combination of the adaptive control architecture and multi-sine excitation maneuvers implemented here, this experimental objective proved difficult to obtain where adaptation is known to have a canceling effect on the open-loop dynamics, therefore, making it difficult to excite the system enough to generate sufficient amounts of the high angle of attack data for improving the information content of the high-order coefficients CTPA3 and CTA3. It is hypothesized that alternative control strategies, employing machine learning for real-time estimation of open-loop natural frequencies, may improve the information quality, but implementation of this is beyond the scope of this work. Nevertheless, TEMPUS does allow for the robust-optimal assessment of information quality for alternative flight test designs (by using the computation of variational asymptotic expansions to overcome the deficiencies of the circulatory problem), implying that trade-offs between alternative controls architectures, measurement systems, etc. is now an available capability to the flight test designer and controls system engineer.Ph.D",,Georgia Institute of Technology,TEMPUS: A methodology for model-based robust-optimal design of time-dynamic system identification experiments using variational asymptotic expansions,,https://core.ac.uk/download/304992940.pdf,core
297209002,2018-06-19T00:00:00,"The measurement of the composition variable in real time is of paramount importance for the
control of the industrial processes, due to the need to follow the final quality of the product and
to the possibility of applying an effective control system. However, a number of obstacles make
this goal impractical, either for reasons where the appropriate sensor does not exist or because
these analytical measurements are too expensive, such as the online process analyzer. Some
models of system identification are used for inference of chemical variables, among which
artificial neural networks (RNA), with virtual sensors estimating difficult-to-measure variables
through easily measurable input variables. Thus, the work proposes a methodology for the
construction of virtual sensors implemented in software with the objective of estimating the
output composition in a productive process of ethylbenzene (EB) and use it in process control.
The production process in question was interesting for the present work, considering the
occurrence of transient effects in the compositions of high purity in the top and bottom streams
of the second distillation column. In addition, the production of ethylbenzene represents the
beginning of the production chain of styrene, a monomer that has a large market in Brazil and
worldwide, requiring more than 25 million tons per year worldwide. The results of the work
range from the design of the unit to the construction of virtual sensors and control proposal.
Therefore, it was verified that the overall balance of the unit presented results consistent with
those of the literature, the variable selection algorithm showed efficiency, the virtual sensor
using artificial neural networks presented satisfactory results and the control structure
minimized the transient effects of the process.
Keywords: virtual sensor, simulation, distillation column, variable selection, neural networksA medição da variável composição em tempo real é de suma importância para o controle dos
processos industriais, devido à necessidade do acompanhamento da qualidade final do produto
e à possibilidade de aplicação de um sistema de controle efetivo. No entanto, uma série de
obstáculos torna este objetivo impraticável, seja por razões nas quais o sensor adequado não
exista ou por essas medições analíticas serem demasiadamente caras, como é o caso do
analisador em linha de processos. Alguns modelos de identificação de sistemas são utilizados
para inferência de variáveis químicas, dentre os quais se destacam as redes neurais artificiais
(RNA), com os sensores virtuais estimando variáveis de difícil medição através de variáveis de
entrada facilmente mensuráveis. Assim, o trabalho propõe uma metodologia para a construção
de sensores virtuais implementados em software com o objetivo de estimar a composição de
saída em um processo produtivo de etilbenzeno (EB) e utilizá-la no controle de processos. O
processo de produção em questão mostrou-se interessante para o presente trabalho, haja vista a
ocorrência de efeitos transientes nas composições de alta pureza nas correntes de topo e de base
da segunda coluna de destilação. Além disso, a produção do etilbenzeno representa o início da
cadeia produtiva do estireno, monômero que possui mercado amplo no Brasil e no mundo,
demandando mais de 25 milhões de toneladas por ano mundialmente. Os resultados do trabalho
vão desde o projeto da unidade até a construção dos sensores virtuais e proposta de controle.
Logo, verificou-se que o balanço global da unidade apresentou resultados coerente com os da
literatura, o algoritmo de seleção de variáveis mostrou-se eficiência, o sensor virtual utilizando
redes neurais artificiais apresentou resultados satisfatórios e a estrutura de controle minimizou
os efeitos transientes do processo",,'Portal de Periodicos UFPB',Controle inferencial em colunas de destilação utilizando redes neurais artificiais dinâmicas,,https://core.ac.uk/download/297209002.pdf,core
387218988,2018-01-01T00:00:00,"The use of computer-readable visual codes became common in our 
everyday life both in industrial environments and for private 
use. The reading process of visual codes consists of two 
steps, namely, localization and data decoding. In this paper 
we examine the localization step of visual codes using 
conventional and deep rectifier neural networks. They are also 
evaluated in the discrete cosine transform domain and shown to 
be efficient, which makes full decompression unnecessary for 
setups involving JPEG images. This approach is also efficient 
from a storage viewpoint and computation cost viewpoint, since 
camera hardware can provide a JPEG stream as output in many 
cases. The use of neural networks implemented on graphics 
processing unit allows real-time automatic code object 
localization. In our earlier studies, the proposed approach 
was evaluated on the most popular code type, quick response 
code, and some other 2D codes as well. Here, we also prove 
that deep rectifier networks are also suitable for 1D barcode 
localization and present extensive evaluation and comparison 
to state-of-the-art approaches",,'Springer Science and Business Media LLC',Efficient visual code localization with neural networks,10.1007/s10044-017-0619-6,,core
427481811,2018-01-01T00:00:00,"Customers have rapidly changing behaviours and expectations and hence a company needs to rapidly adapt to the changing market needs. As a leading telecommunications infrastructure provider, CommScope engages in top-notch strategic measures in staying up to date with the evolving market needs. The evolution of the telecommunications industry is now growing towards the connectivity of IoT devices. Like in most industries, there is a value chain associated with IoT when it comes to the telecommunication services.  This research project not only looks into IoT but delves into the business operations of CommScope, exploring various Industry 4.0 technologies whilst aligning them with KPIs to determine the potential impact in the business operations of CommScope. CommScope is a multi-national telecommunications infrastructure provider headquartered in North Carolina, United States. It is located in over 130 countries with over 20,000 employees worldwide in NAR, EMEA, APAC and CALA geographical regions. Since the fourth industrial revolution is the move towards digitisation. The question is - What impact will this industrial revolution have on CommScope? Industry 4.0 can be described at the fourth industry revolution, the current mega trend impacting and shaping companies around the world today. It envisions the automation and connection between people, machines and products within and across an enterprise. This revolution promises to reshape the face of manufacturing by merging the traditional manufacturing techniques with industrial technology to aid data analysis across machines for faster, flexible and more efficient processes which will ultimately lead to the production of high quality products and/or services at reduced costs.  CommScope can be categorised as a product company which manufactures a diverse range of product such copper cables, fibre optic cables, connector panels, racking and metal. For a tangible implementation of Industry 4.0, a tailored approach into a particular business division and a focused into one product line was considered for optimum results. Following successful implementation as well as if desired objectives are met in this product line, it may then be replicated in other areas of the business. Industry 4.0 will enable automatic data collection that can be used by CommScope to further aid advancements in big data and powerful analytics which means that systems can sift through the huge sets of gathered data and produce insights that can be acted upon quickly. This will provide effective communications and right channelling of analysed and backed up data securely through systems to the relevant people, processes or machines. This Industry 4.0 network between people, processes and machines involves four main characteristics : vertical networking, horizontal integration, through-engineering across the entire value chain; as well as acceleration through exponential technologies. We conducted a strategic research conducted beginning with exploring the various Industry 4.0 technologies such as Big Data Analytics, Automation, Augmented Reality, Internet of Things (IoT), Artificial Intelligence, Blockchain, Shared Economies, eCommerce, 3D Printing, Robotics, Nano Technology, Fintech and aligning them with KPIs in order to determine their relevance and application. Following this, the technologies were clustered into the following segments for a more focused and interrelated use in 1) eCommerce, 2) Manufacturing, 3) Product development and customisation; as well as 4) Supply Chain and Logistics. Currently, CommScope has challenges in its business operations in terms of speed of deployment, production performance, supply chain performance which ultimately impacts customer experience, brand perception and costs. In order to tackle these challenges and ensure CommScope's future value creation, we performed an Industry 4.0 readiness assessment to determine CommScope's current readiness, its ambition and measure the gap between CommScope and other industry players through a benchmark. This assessment entailed interviews with internal stakeholders within CommScope in the EMEA and NAR region. Interpreting the result of CommScope's readiness assessment of Industry 4.0, CommScope is at an experienced level 3 in the legal considerations dimension with the rest of the other five dimensions namely products and services, manufacturing and operations, strategy and organisation and business model at an intermediate level 2. This was also similar to the overall results of the 53 benchmarked companies scoring an intermediate level 2 in all dimensions. Following these results, we shortlisted manufacturing operations and supply chain as the most challenging areas of CommScope's business operations. With this focus, we further identified gap areas in the manufacturing operations and supply chain divisions and recommended key capabilities that CommScope should develop to fill these gaps. In conclusion, we have outlined CommScope's vision and designed a roadmap to the stipulated goal of the ""Amazon one-click experience"" as a guideline with potential action steps which upon successful implementation can be replicated across all relevant product lines and/or business divisions. For this vision, there are key capabilities that also been identified to be developed to transform CommScope's business operations by moving towards digitisation. This Industry 4.0 journey calls for deliberate commitment in terms of investments of time and resources which will ultimately broaden CommScope's reach and capabilities and open up new potential market opportunities",,,Analysis of industry 4.0 and its impact in business operations,,,core
160785022,2018-08-20T00:00:00,"Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.Comment: Proceedings of the IEEE 201",,'Institute of Electrical and Electronics Engineers (IEEE)',"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality",10.1109/JPROC.2018.2856739,http://arxiv.org/abs/1808.06352,core
395008361,2018-01-01T00:00:00,"Human-robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze-object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The ""gaze object sequence"" was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments",,"eScholarship, University of California",Exploiting Three-Dimensional Gaze Tracking for Action Recognition During Bimanual Manipulation to Enhance Human-Robot Collaboration.,,,core
212913506,2018-11-05T08:00:00,"The operation of the smart grid is expected to be heavily reliant on microprocessor-based control. Thus, there is a strong need for interoperability standards to address the heterogeneous nature of the data in the smart grid. In this research, we analyzed in detail the security threats of the Generic Object Oriented Substation Events (GOOSE) and Sampled Measured Values (SMV) protocol mappings of the IEC 61850 data modeling standard, which is the most widely industry-accepted standard for power system automation and control. We found that there is a strong need for security solutions that are capable of defending the grid against cyber-attacks, minimizing the damage in case a cyber-incident occurs, and restoring services within minimal time.
To address these risks, we focused on correlating cyber security algorithms with physical characteristics of the power system by developing intelligent agents that use this knowledge as an important second line of defense in detecting malicious activity. This will complement the cyber security methods, including encryption and authentication. Firstly, we developed a physical-model-checking algorithm, which uses artificial neural networks to identify switching-related attacks on power systems based on load flow characteristics.
Secondly, the feasibility of using neural network forecasters to detect spoofed sampled values was investigated. We showed that although such forecasters have high spoofed-data-detection accuracy, they are prone to the accumulation of forecasting error. In this research, we proposed an algorithm to detect the accumulation of the forecasting error based on lightweight statistical indicators. The effectiveness of the proposed algorithms was experimentally verified on the Smart Grid testbed at FIU. The test results showed that the proposed techniques have a minimal detection latency, in the range of microseconds.
Also, in this research we developed a network-in-the-loop co-simulation platform that seamlessly integrates the components of the smart grid together, especially since they are governed by different regulations and owned by different entities. Power system simulation software, microcontrollers, and a real communication infrastructure were combined together to provide a cohesive smart grid platform. A data-centric communication scheme was selected to provide an interoperability layer between multi-vendor devices, software packages, and to bridge different protocols together",,FIU Digital Commons,Secure Control and Operation of Energy Cyber-Physical Systems Through Intelligent Agents,,https://core.ac.uk/download/212913506.pdf,core
201801018,2018-09-01T00:00:00,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology&rsquo;s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders&mdash;consumers, agents and associations, government and regulatory authorities, and complementary industries&mdash;and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers&rsquo; needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers&rsquo; needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports","[{'title': 'Sustainability', 'identifiers': ['2071-1050', 'issn:2071-1050']}]",'MDPI AG',"A Systematic Review of Smart Real Estate Technology: Drivers of, and Barriers to, the Use of Digital Disruptive Technologies and Online Platforms",10.3390/su10093142,,core
219376652,2018-06-01T07:00:00,"[Excerpt] The ideas and uses for Artificial Intelligence (AI) are abundant, and each business is seemingly ripe for disruption, including HR. As the hype surrounding AI continues to be championed by popular press, we began our research in order to determine whether the press’ biased view that AI was here and ready to implement was accurate. We found that in reality, AI programs were far behind the progress discussed, as the software was slower, more expensive, and there was a general lack of amalgamation throughout the industry. From there, we asked CAHRS partners to tell us where AI was used in their company, and how it helped them deliver HR differently. Our research focused on how AI technology will disrupt, change, or bolster the HR function, specifically in Talent Acquisition and Learning and Development (L&D) spaces.
We found our CAHRS partners dove into AI, and represented three key points along a spectrum of AI implementation. Of the 59 participants at 32 companies, 26% are Observers, 48% are Explorers, and 26% are Implementers. Observers were companies that did not believe AI fits with their strategy, and therefore do not intend to implement AI right now. Explorers are companies that have begun to actively explore AI through industry research, vendor exploration, and piloting AI and machine learning (ML) technologies. Implementers are companies that have either built in house or worked with an external vendor to implement an AI or machine learning technology. The CAHRS partners represented such a wide range along this spectrum because there are no best practices for AI implementation. However, each of our partners that leveraged AI understood the tool, while also understanding their business needs, people, and technology, which allowed them to utilize AI technology",,DigitalCommons@ILR,CAHRS Partners\u27 Implementation of Artificial Intelligence,,https://core.ac.uk/download/219376652.pdf,core
95686640,2018-01,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are
less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.status: publishe","[{'title': None, 'identifiers': ['1872-6291', 'issn:1872-6291', 'issn:0020-0255', '0020-0255']}]",Elsevier,Incorporating negative information to process discovery of complex systems,,,core
161546101,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",,Taylor&Francis Group,Digitalization of the power business: How to make this work?,,,core
162560495,2018-11-19T00:00:00,"In recent years, the socio-economic development of the population, the growth of commercial and industrial sectors, as well as the ever-increasing installation of new electrical loads, have generated great evolution in demand of electricity consumption. In turn, to obtain more efficient systems, the manufacturers have produced equipment more energy efficient for residential, commercial and industrial use. However, these loads due their non-linearities, have contributed significantly to the increase in harmonic distortion levels of voltage and current, raising the concern of the power sector managers with respect to the power quality, mainly, due to the difficulty in the identification of the origin of the harmonic distortion. Therefore, to anticipate the harmonic effects and meet the current legislation, through computational techniques, this work emphasis is placed on the common coupling point (CCP) of consumers and utility, regardless of consumption characteristics and loads, to assess the harmonic impacts in his grid, besides comparing the reliability level of the techniques through the mean absolute error (MAE). The proposed methodology uses the Electrical Power Quality System (SISQEE) software that allows the use of three different computational techniques, such as Linear Regression, Artificial Neural Networks and Regression Trees, to evaluate the harmonic contribution of each feeder at the point of interest of the chosen electric grid. To prove the validity of the methodology, two case studies, based on real measurements at a university and at an industrial district, was carried out with a minimum sampling period of seven days using power quality analyzers, according to the distribution procedures by ANEEL (PRODIST). As a result of the power quality, it was verified how much each feeder impacts the voltage and current distortion at the CCP, besides classifying the feeders in relation to their respective impacts in the studied electrical grid. Also, as a result, the studies allowed the evaluation of performance between the different techniques, with different time intervals (weekly, daily and per load level), allowing to classify the behavior and reliability of each technique in each period. As a conclusion of the work, the proposed methods and analyzes presented allow managers to perform a more efficient mitigation action of the harmonic impacts caused in the electrical network and, also, to identify the differences between the techniques and their degree of reliability, in accordance with the time intervals studied.Nos últimos anos, o desenvolvimento socioeconômico da população, o crescimento dos setores comercial e industrial, assim como a instalação cada vez mais crescente de novas cargas, têm gerado grande evolução na demanda do consumo de energia elétrica. Por sua vez, buscando obter sistemas mais eficientes, os fabricantes têm produzido equipamentos energeticamente mais eficientes para utilização residencial, comercial e industrial. No entanto, essas cargas, devido à sua não linearidade, têm contribuído significativamente para o aumento dos níveis de distorção harmônica de tensão e corrente, elevando a preocupação dos gestores do setor elétrico quanto a qualidade de energia elétrica (QEE), principalmente, pela dificuldade na identificação da origem da distorção harmônica. Logo, visando antecipar os efeitos harmônicos e atender a regulamentação vigente, por meio de técnicas computacionais, no presente trabalho dá-se ênfase no ponto de acoplamento comum (PAC), independente das características de consumo e cargas, com o intuito de avaliar os impactos harmônicos em sua rede, além de comparar o nível de confiabilidade das técnicas por meio do erro absoluto médio (EAM). A metodologia proposta utiliza o software de Sistema de Qualidade de Energia Elétrica (SISQEE) que possibilita a utilização de três técnicas computacionais distintas, sendo Regressão Linear, Redes Neurais Artificiais e Árvores de Regressão, para avaliar a contribuição harmônica de cada alimentador no ponto de interesse das redes elétricas escolhidas. Para comprovar a validade da metodologia, são elaborados dois estudos de caso baseadas em medições reais em uma universidade e em um polo industrial. As medições foram realizadas com o período mínimo amostral de sete dias através de analisadores de QEE, conforme procedimentos de distribuição da ANEEL (PRODIST). Como resultado da QEE, verificou-se o quanto cada alimentador impacta a distorção de tensão e corrente no PAC, além de classificar os alimentadores com relação a seu respectivo impacto na rede elétrica estudada. Também como resultado, os estudos propiciaram a avaliação de desempenho entre as diferentes técnicas, com diferentes intervalos de tempo (semanal, diário e por patamar de carga), permitindo classificar o comportamento e a confiabilidade de cada técnica em cada período. Como conclusão do trabalho, os métodos propostos e as análises apresentadas dão subsídios aos gestores para efetuar uma ação mitigadora mais eficiente dos impactos harmônicos causados na rede elétrica e, também, identificar as diferenças entre as técnicas e seu grau de confiabilidade, de acordo com os intervalos temporais estudados",,Programa de Pós-Graduação em Engenharia Elétrica,"Comparison between linear regression, artificial neural networks and regression trees to quantify the harmonic impact of multiple loads on distribution networks.",,,core
287899403,2018-03-01T08:00:00,"Creating an embodied virtual agent is often a complex process. It involves 3D modeling and animation skills, advanced programming knowledge, and in some cases artificial intelligence or the integration of complex interaction models. Features like lip-syncing to an audio file, recognizing the users’ speech, or having the character move at certain times in certain ways, are inaccessible to researchers that want to build and use these agents for education, research, or industrial uses. VAIF, the Virtual Agent Interaction Framework, is an extensively documented system that attempts to bridge that gap and provide inexperienced researchers the tools and means to develop their own agents in a centralized, lightweight platform that provides all these features through a simple interface within the Unity game engine. In this paper we present the platform, describe its features, and provide a case study where agents were developed and deployed in mobile-device, virtual-reality, and augmented-reality platforms by users with no coding experience",,ScholarWorks@UTEP,Virtual Agent Interaction Framework (VAIF): A Tool for Rapid Development of Social Agents,,,core
236628368,2019-08-06T00:00:00,"Industry 4.0 refers to the new technological development occurred at the industrial production systems. It evolved as a result of integrating Internet of Things, Cyber-Physical Systems, Big-Data, Artificial Intelligence, and Cloud Computing in the industrial systems. This integration aided new capabilities to achieve a higher level of business excellence, efficiency, and effectiveness. Total Quality Management (TQM) is a managerial approach to achieve an outstanding business excellence. There are several approaches to apply TQM principles at any organization. Industry 4.0 could be utilized as a key enabler for TQM especially by integrating its techniques with the TQM best practices. This paper suggests a theoretical framework for integrating Industry 4.0 features with the TQM principles (according to ISO 9000:2015 standards family) in order to open the door for further research to address the real impact of utilizing Industry 4.0 for serving the TQM implementation approaches",,'Periodica Polytechnica Budapest University of Technology and Economics',Industry 4.0 as a Key Enabler toward Successful Implementation of Total Quality Management Practices,,https://core.ac.uk/download/236628368.pdf,core
395072474,2019-01-01T00:00:00,"Adoption of digital platform innovations afford a changing nature of work, from mobile computing platforms (e.g. Apple) enabling 24/7 work connectivity, to labour marketplace platforms (e.g. Uber) enabling precarious work arrangements. Recently, organisations are adopting/investigating spatial computing platforms (e.g. Autodesk, Toyota, BNP Paribas), offering new affordances for organising (e.g. carrying out tasks, communicating and collaborating). Spatial computing concerns achieving spatial interplay between the real and digital world (Agulhon 2016), enabling perception of physically present content. An emerging paradigm of spatial computing is enabled by hardware and software innovations for; 1) digitally mapping, tracking, understanding and predicting analog audio and visual spatial fields, 2) creating digital audio and visual spatial fields, and the (3) mixing and fusing of those fields. Mixed, augmented and immersive reality is then experienced by volumetric graphic rendering onto a human's field of view (FOV) (Martín-Gutiérrez et al. 2017). Emerging marketplace examples can be seen in 'Microsoft Hololens 2' and 'Magic Leap One' platforms, both creating/enabling an ecosystem of novel applications for both industrial, educational and leisure life contexts. With further convergence of IoT, haptics, 5G, cloud and AI etc., spatial applications will range from contextually aware and interactive; digital information layering of objects, guidance and decision support systems (DSS) within business operations (such as for industrial machine manufacture, monitoring, and maintenance), digital modelling & prototyping in R&D, through to applications for communications and collaborations (such as for spatial tele co-presence of people, objects and environments). More broadly, these advances have potential to catalyse disruptions within business, through to the labour and consumer marketplace via: (1) Virtualisation of hardware resources (e.g. fully digitising workplace equipment such as displays and interfaces, raw inputs for prototyping and even digital rendering of spaces). (2) Protection and strengthening of institutional knowledge and performance via knowledge capture, guidance and decision support of labour tasks and activity (e.g. reducing labour (re)training (e.g. parts assembly), knowledge capture of practice). (3) Creation and distribution of new value propositions in goods and services (e.g. digital item ownership in a mixed-reality cloud, spatial applications for IoT enabled devices). (4) Displacement of geographic space as cost, talent, time, access and convenience constraints on business (e.g. available talent pool, partner/customer reach and relations). (5) Collaboration through new/enhanced affordances for workers (e.g. shared digitised work tools/environments). Therefore, a paradigm of spatial computing will challenge the IS community to research new ways of working, and consequences for worker experience, meaning, productivity and power. With emerging advances in AI, automation and spatial computing, one of the pertinent enquires concerns importance of workers (sense of) agency (Chandra et al. 2019). Control in the IT context has been conceptualised as control over work, control over self, and control over technology (Beaudry and Pinsonneault 2005), with prior IS work studying locus of control related to; work stress (Chandra et al. 2019), intrinsic and extrinsic motivations (Mujinga, M Eloff, MM Kroeze 2013), and performance (Vieira da Cunha et al. 2015) etc. With spatial computing platforms and their applications, what affordances of control and for whom should be developed? For example, the electronic representation of worker activity can be further enabled. Thus, tighter or looser coupling between worker activity and the reporting/outcome of work (Vieira da Cunha et al. 2015) becomes more of an organisational decision, with capability to monitor workers, and leverage AI for learning and optimisation. Furthermore, with development of spatial tele co-presence (STcP) (e.g. Mimesys), brings new affordances for communication with any worker(s), at any time, from anywhere. However, prior CMC research suggests people can choose different communication media specifically to manage social and emotional relationships (Madianou 2014) and their time (Mcloughlin et al. 2019). Hence, will such affordances serve greater identity fusion (Swann et al. 2012) and collaboration in organisations? Thus, we propose a socio-technical research agenda exploring 'control' related affordances for emerging spatial computing platforms, such as for STcP technology. In this regard, Control Theory can offer a useful starting frame, as it deals with control mechanisms governing workers organisational actions both formal (outcome and behaviour based) and informal (group and self-control), to further the interests of organisations (Kirsch 1996). We suggest, data and communication related affordances of control (e.g. privacy, exploitation, authenticity, availability and spaces) as starting points. Social Capital (Lin 2001), Social Influence (Kelman 1958), Social Identity (Ellemers and Haslam 2012), Identity Fusion (Swann et al. 2012) and Polymedia (Madianou and Miller 2012) being just some of the many relevant social theories to this endeavour",,,Affordances of Control in a Paradigm of Spatial Computing Platforms,,,core
224837116,2019-01-01T00:00:00,"This work aims to increase the impact of computer vision on robotic positioning and grasping in industrial assembly lines. Real-time object detection and localization problem is addressed for robotic grasp-and-place operation using Selective Compliant Assembly Robot Arm (SCARA). The movement of SCARA robot is guided by deep learning-based object detection for grasp task and edge detection-based position measurement for place task. Deep Convolutional Neural Network (CNN) model, called KSSnet, is developed for object detection based on CNN Alexnet using transfer learning approach. SCARA training dataset with 4000 images of two object categories associated with 20 different positions is created and labeled to train KSSnet model. The position of the detected object is included in prediction result at the output classification layer. This method achieved the state-of-the-art results at 100% precision of object detection, 100% accuracy for robotic positioning and 100% successful real-time robotic grasping within 0.38 seconds as detection time. A combination of Zerocross and Canny edge detectors is implemented on a circular object to simplify the place task. For accurate position measurement, the distortion of camera lens is removed using camera calibration technique where the measured position represents the desired location to place the grasped object. The result showed that the robot successfully moved to the measured position with positioning Root Mean Square Error (0.361, 0.184) mm and 100% for successful place detection",,'Institute of Electrical and Electronics Engineers (IEEE)',Real-time robotic grasping and localization using deep learning-based object detection technique,10.1109/I2CACIS.2019.8825093,https://core.ac.uk/download/224837116.pdf,core
286549694,2019-01-01T00:00:00,"Chemical processes can benefit tremendously from fast and accurate effluent composition prediction for plant design, control, and optimization. The Industry 4.0 revolution claims that by introducing machine learning into these fields, substantial economic and environmental gains can be achieved. The bottleneck for high-frequency optimization and process control is often the time necessary to perform the required detailed analyses of, for example, feed and product. To resolve these issues, a framework of four deep learning artificial neural networks (DL ANNs) has been developed for the largest chemicals production process-steam cracking. The proposed methodology allows both a detailed characterization of a naphtha feedstock and a detailed composition of the steam cracker effluent to be determined, based on a limited number of commercial naphtha indices and rapidly accessible process characteristics. The detailed characterization of a naphtha is predicted from three points on the boiling curve and paraffins, iso-paraffins, olefins, naphthenes, and aronatics (PIONA) characterization. If unavailable, the boiling points are also estimated. Even with estimated boiling points, the developed DL ANN outperforms several established methods such as maximization of Shannon entropy and traditional ANNs. For feedstock reconstruction, a mean absolute error (MAE) of 0.3 wt% is achieved on the test set, while the MAE of the effluent prediction is 0.1 wt%. When combining all networks-using the output of the previous as input to the next-the effluent MAE increases to 0.19 wt%. In addition to the high accuracy of the networks, a major benefit is the negligible computational cost required to obtain the predictions. On a standard Intel i7 processor, predictions are made in the order of milliseconds. Commercial software such as COILSIM1D performs slightly better in terms of accuracy, but the required central processing unit time per reaction is in the order of seconds. This tremendous speed-up and minimal accuracy loss make the presented framework highly suitable for the continuous monitoring of difficult-to-access process parameters and for the envisioned, high-frequency real-time optimization (RTO) strategy or process control. Nevertheless, the lack of a fundamental basis implies that fundamental understanding is almost completely lost, which is not always well-accepted by the engineering community. In addition, the performance of the developed networks drops significantly for naphthas that are highly dissimilar to those in the training set. (C) 2019 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company",,'Elsevier BV',Artificial intelligence in steam cracking modeling : a deep learning algorithm for detailed effluent prediction,10.1016/j.eng.2019.02.013,https://core.ac.uk/download/286549694.pdf,core
334885889,2019-11-22T00:00:00,"The manual design of soft robots and their controllers is notoriously
challenging, but it could be augmented---or, in some cases, entirely
replaced---by automated design tools. Machine learning algorithms can
automatically propose, test, and refine designs in simulation, and the most
promising ones can then be manufactured in reality (sim2real). However, it is
currently not known how to guarantee that behavior generated in simulation can
be preserved when deployed in reality. Although many previous studies have
devised training protocols that facilitate sim2real transfer of control
polices, little to no work has investigated the simulation-reality gap as a
function of morphology. This is due in part to an overall lack of tools capable
of systematically designing and rapidly manufacturing robots. Here we introduce
a low cost, open source, and modular soft robot design and construction kit,
and use it to simulate, fabricate, and measure the simulation-reality gap of
minimally complex yet soft, locomoting machines. We prove the scalability of
this approach by transferring an order of magnitude more robot designs from
simulation to reality than any other method. The kit and its instructions can
be found here: https://github.com/skriegman/sim2real4design",,,Scalable sim-to-real transfer of soft robot designs,,http://arxiv.org/abs/1911.10290,core
200838146,2019-05-16T00:00:00,"The concept of Industry 4.0 brings a disruption into the processing industry.
It is characterised by a high degree of intercommunication, embedded
computation, resulting in a decentralised and distributed handling of data.
Additionally, cloud-storage and Software-as-a-Service (SaaS) approaches enhance
a centralised storage and handling of data. This often takes place in
third-party networks. Furthermore, Industry 4.0 is driven by novel business
cases. Lot sizes of one, customer individual production, observation of process
state and progress in real-time and remote maintenance, just to name a few. All
of these new business cases make use of the novel technologies. However, cyber
security has not been an issue in industry. Industrial networks have been
considered physically separated from public networks. Additionally, the high
level of uniqueness of any industrial network was said to prevent attackers
from exploiting flaws. Those assumptions are inherently broken by the concept
of Industry 4.0. As a result, an abundance of attack vectors is created. In the
past, attackers have used those attack vectors in spectacular fashions.
Especially Small and Mediumsized Enterprises (SMEs) in Germany struggle to
adapt to these challenges. Reasons are the cost required for technical
solutions and security professionals. In order to enable SMEs to cope with the
growing threat in the cyberspace, the research project IUNO Insec aims at
providing and improving security solutions that can be used without specialised
security knowledge. The project IUNO Insec is briefly introduced in this work.
Furthermore, contributions in the field of intrusion detection, especially
machine learning-based solutions, for industrial environments provided by the
authors are presented and set into context.Comment: PREPRINT, published in the proceedings of the 24th ITG Fachtagung
  Mobilkommunikatio",,,"Modern Problems Require Modern Solutions: Hybrid Concepts for Industrial
  Intrusion Detection",,http://arxiv.org/abs/1905.05984,core
334850656,2019-10-23T00:00:00,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.Comment: 24 page",,,AIBench: An Industry Standard Internet Service AI Benchmark Suite,,http://arxiv.org/abs/1908.08998,core
229562339,2019-04-23T00:00:00,"The paper presents the design and implementation of a fuzzy inference system (FIS) trained with adaptive neural networks for the generation of specification references in high frequency current (HFC) hardening processes. The specification references are then further used for the control of the process in obtaining the desired outcomes in terms of material hardening and resistance. The FIS is trained using data obtained from experimentation on an industrial HFC device. The trained FIS is then compared to a manually tuned FIS, resulting from expert and operator designs. The results led to the development of intelligent control interfaces in real time through the ANFIS method",,'International University of Sarajevo',Adaptive neural network fuzzy inference system for HFC processes,10.21533/pen.v7i1.337,https://core.ac.uk/download/229562339.pdf,core
327011218,2019-03-01T00:00:00,"With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the field of Asset Management has moved from fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must find a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, defined as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they ignore their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares fit of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet",,'Elsevier BV',Collaborative prognostics in Social Asset Networks,,,core
322991098,2019-01-01T00:00:00,"1.	Abu-Taieh C., Evon J.: Technology Engineering and Management in Aviation: Advancements and Discoveries. Information Science Reference, 2011.
2.	Ajam M, Woolard C, Wiljoen CL. Biomass pyrolysis oil as a renewable feedstock for bio-jet fuel. In: Proceedings of the 13th international conference on stability, handling and use of liquid fuels (IASH2013), Rhodes, Greece; October 2013. p. 6–10.
3.	Аnnual report to Parliament on the renewable transport fuel obligation. Renewable Fuels Agency. The Stationery Office, 2011.
4.	Agarwal S., Chhibber V. K., Bhatnagar A. K.:Tribological behavior of diesel fuels and the effect of anti-wear additives. Fuel. Vol. 106, 2013, p. 21–29, 
5.	Alves S. M., Barros B.S., Trajano M.F.: Tribological behavior of vegetable oil-based lubricants with nanoparticles of oxides in boundary lubrication conditions. Tribology International. Vol. 65, 2013, p. 28–36.
6.	Asgari H., Chen X., Sainudiin R.: Modelling and simulation of gas turbines. International Journalof Modelling, Identification and Control, Vol.25, No.3, 2013, p. 1–15.
7.	Bartis James T. LaTourrette T., Dixon L.: Oil Shale Development in the United States: Prospects and Policy Issues. Santa Monica, Calif.: RAND Corporation, MG-414-NETL, 2005.
8.	Bassam N. El.: Handbook of Bioenergy Crops: A Complete Reference to Species. Development and Applications Earthscan, 2010.
9.	Bazazzadeh M., Badihi H., Shahriari A.: Gas Turbine Engine Control Design Using Fuzzy Logic and Neural Networks. International Journal of Aerospace Engineering. Vol. 1, 2011, p. 1–13. 
10.	Blakey S, Rye L, Wilson C.W.: Aviation gas turbine alternative fuels: A review.  P Combust Inst, No. 33, 2011, p. 2863–2885.
11.	Boichenko S., Iakovlieva A., Vovk O.: Traditional and alternative jet fuels: problems of quality standardization. Journal of Petroleum & Environmental Biotechnology. Vol. 4. Iss. 3, 2013.
12.	Boichenko S., Shkilniuk I., Turchak V.. The problems of biopollution with jet fuels and the way of achieving solution. Transport. 23, 2008; p. 253–257.
13.	Boichenko S., Yakovleva A. Prospects of biofuels introduction into aviation. Transport engineering and management: Proceedings of the 15-th conference for Lithuania Junior researchers. Science – future of Lithuania, 4 May 2012. Vilnius: Technika. p. 90–94.
14.	Boichenko S., Yakovlieva A., Gryshchenko O., Zinchuk A. Prospects of using different generations biofuels for minimizing impact of modern aviation on environment, Энерготехнологии и ресурсосбережение, № 1, 2018, p. 10–20.
15.	Boichenko S., Lejda K., Yakovlieva A., Vovk O. Comparative characteristics of low-temperature properties of jet fuels modified with bio-additives, International Automotive Conference (KONMOT2018). IOP Conf. Series: Materials Science and Engineering 421, 2018.
16.	Breil C., Meullemiestre A., Vian M., Chemat F.: Bio-Based Solvents for Green Extraction of Lipids from Oleaginous Yeast Biomass for Sustainable Aviation Biofuel. Molecules. Iss. 21(196), 2016, p. 1–14.
17.	Carels N., Sujatha M., Bahadur B.: Jatropha, Challenges for a New Energy Crop. Vol. 1: Farming, Economics and Biofuel. Springer Science & Business Media, 2012.
18.	Cavani F., Albonetti S., Basile F., Gandini A.: Chemicals and Fuels from Bio-Based Building Blocks. John Wiley & Sons, 2015.
19.	Cermak S. C., Evangelista R. L., Kenar J. A.: Distillation of Natural Fatty Acids and Their Chemical Derivatives, Distillation - Advances from Modeling to Applications, Dr. Sina Zereshki (Ed.), InTech, 2012. – р. 5. – 140. 
20.	Chai M. Thermal Decomposition of Methyl Esters in Biodiesel Fuel: Kinetics, Mechanisms and Products, Ph.D. Thesis, University оf Cincinnati, 2012.
21.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 1: emulsion production. Biomass Bioenergy, No. 25, 2003, p. 85–99.
22.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 2: tests in diesel engines. Biomass Bioenergy, No. 25, 2003, p. 101–11.
23.	Chuck C.J., Donnelly J.: The compatibility of potential bioderived fuels with Jet A-1 aviation kerosene. Applied Energy. Vol. 118, 2014, p. 83–91.
24.	Cleveland C.J., Morris C. G.: Handbook of energy. Volume II: Cronologies, top ten lists, and words clouds. Elsvier Inc., 2014.
25.	Cushion E., Whiteman A., Dieterle G.: Bioenergy Development: Issues and Impacts for Poverty and Natural Resource Management. World Bank Publications, 2010.
26.	Daggett D. L., Hendricks R.C., Walther R., Corporan E.: Alternative fuels for use in commercial aircrafts. The Boeing Company, 2007.
27.	Dahlquist E.: Biomass as Energy Source. Resources, Systems and Applications. CRC Press, 2013.
28.	Delmon B., Grange P., Froment G.F.: Hydrotreatment and Hydrocracking of Oil Fractions. Elsevier, 1999.
29.	Doc 9889 Airport Air Quality Manual. International Civil Aviation Organization, 2011. 
30.	Doc 9977. Manual on Civil Aviation Jet Fuel Supply, 2012.
31.	Edwards T.: Advancements in Gas Turbine Fuels from 1943 to 2005. J Eng Gas Power, No. 129, 2007, p. 13–20.
32.	Firrisa M. T., Van Duren I., Voinov A.: Energy efficiency for rapeseed biodiesel production in different farming systems. Energy Efficiency, 2013.
33.	Garcia-Anton J., Monzo J., Guninon J.L.: Study of corrosion on copper strips by petroleum naphtha in the ASTM D-130 test by means of electronic microscopy (SEM) and energy dispersive X-ray (EDX). Fresenius Journal of Analytical Chemistry. Iss. 337, 1990, p. 382–388.
34.	Garcia Santander C.M., Gymez Rueda S.M., de Lima da Silva N.: Measurements of normal boiling points of fatty acid esters and triacylglycerols by thermogravimetric analysis, Fuel, Iss. 92, 2012, p. 158–161.
35.	Geller D. P., Goodrum J.: W. Effects of speciﬁc fatty acid methyl esters on diesel fuel lubricity, Fuel, Vol. 83, 2004, p. 2351–2356.
36.	Gupta, K. K, Rehman A, Sarviya R. M.: Bio-fuels for the gas turbine: A review. Renew. Sust. Energ. Rev. No. 14, 2010, p. 2946–2955.
37.	Harvey B. G, Merriman W.W., Koontz T.A.: High-Density Renewable Diesel and Jet Fuels Prepared from Multicyclic Sesquiterpanes and a 1‑Hexene-Derived Synthetic Paraffinic Kerosene, Energy Fuels, 2013.
38.	Hemighaus G., Boval T., Bosley C.: Alternative Jet Fuels. Addendum 1 to Aviation Fuels Technical Review (FTR-3/A1). Chevron Corporation, 2006.
39.	Hileman J.I., Stratton R.W.: Alternative jet fuel feasibility. Transport Policy. Vol. 34, 2014, p. 52–62.
40.	Hileman J.I., Wong H.M., Waitz I.: Near-Term Feasibility of Alternative Jet Fuels. Santa Monica, California: RAND Corporation, 2009.
41.	Hileman, J. Ortiz D., Bartis J.: Near-Term Feasibility of Alternative Jet Fuels. Jointly published by the RAND Corporation (Report No. TR-554-FAA) and the Partnership for Air Transportation Noise and Emissions Reduction, 2009.
42.	Honga T.D., Soerawidjajab T.H., Reksowardojoa I.K.: A study on developing aviation biofuel for the Tropics: Production process – Experimental and theoretical evaluation of their blends with fossil kerosene, Chemical Engineering and Processing: Process Intensification, Vol. 74, 2013, p. 124–130.
43.	Hristova M., Tchaoushev S.: Сalculation of flash points and flammability limits of substances and mixtures. Journal of the University of Chemical Technology and Metallurgy, Iss. 41(3), p. 291–296, 2006.
44.	Hu J., Du Z., Li C., Min E.: Study on the lubrication properties of biodiesel as fuel lubricity enhancers, Fuel. Vol. 84, 2005. p. 1601–1606. 
45.	Iakovlieva A., Boichenko S., Vovk O.: Investigation of the fractional composition of rape oil-derived aviation biofuels. Aviation in the XXI-st century. Safety in aviation and space technologies: the fifth world congress, 25–27 September 2012: abstracts. Kyiv, Vol. 3, 2012, p. 5.41–5.43.
46.	Iakovlieva A.V. Boichenko S.V., Vovk O.O.: Overview of innovative technologies for aviation fuels production. Journal of Chemistry and chemical technology, Vol. 7. Iss. 3, 2013, p. 305–312.
47.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Peculiarities of the development and implementation of aviation biofuels in Ukraine. World Congress on Petrochemistry and Chemical Engineering. Journal of Petroleum & Environmental Biotechnology. November 2013, San Antonio. Vol.4. Iss. 6, 2013, p. 47.
48.	Iakovlieva A., Boichenko S., Gay A.: Cause-Effect Analysis of the Modern State in Production of Jet Fuels. Journal of Сhemistry & Chemical Technology. Vol. 8. No 1, 2014, p. 107–116.
49.	Iakovlieva A., Boichenko S., Vovk O., Lejda K.: Potential of jet biofuels production and application in Ukraine and Poland. International Journal of Sustainable Aviation. Vol. 1. No.4, 2015, p. 314–323.
50.	Iakovlieva A., Boichenko S., Lejda K.: Impact of rape oil ethyl esters additives on some characteristics of jet fuel. Проблеми хіммотології. Теорія та практика раціонального використання традиційних і альтернативних паливно -мастильних матеріалів: V міжнар. наук.-техн. конф., 6–10 жовт. 2014. Київ, c. 286 – 289. 
51.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Skilniuk I.: Vacuum Distillation of Rapeseed Oil Esters for Production of Jet Fuel Bio-Additives, Procedia Engineering, Vol. 187, 2017, p. 363 – 370. 
52.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Рotential of jet biofuels production and application in Ukraine and Poland. Proceedings of the 1st International Simposium on Sustainable Aviation.–31 May–03 June 2015, Isntanbul, p. 137.
53.	Iakovlieva A., Boichenko S., Lejda K.: Experimental study on antiwear properties for blends of jet fuel with biocomponents derived from rapeseed oil. Eastern-European journal of enterprise technologies. No. 5/8(77), 2015, p. 20–28.
54.	Iakovlieva A., Vovk O., Boichenko S.: Еxperimental study of rape oil esters influence on physical-chemical properties of jet fuels. Proceedings of the 19th Conference for Junior Researchers ‘Science – Future of Lithuania’ Тransport engineering and management, 6 May 2016, Vilnius. p. 85–89.
55.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Kuszewski H. Improvement of technological scheme of fatty acids ethyl esters production for use as jet fuels biocomponents. International Journal of Theoretical and Applied Science. Iss. 11(19), 2014, p. 44–55.
56.	International Air Transport organization. Vision 2050. Report. Montreal. Geneva, 2011.
57.	Jansen R. A.: Second Generation Biofuels and Biomass: Essential Guide for Investors, Scientists and Decision Makers. Wiley. 2012.
58.	Jenkins R.W., Munro M., Christopher S.N., Chuck C.: Potential renewable oxygenated biofuels for the aviation and road transport sectors. Fuel, Vol. 103, 2013, p. 593–599.
59.	Jacyna M., Żak J., Jacyna-Gołda I., Merkisz J., Merkisz-Guranowska A., Pielecha J.: Selected aspects of the model of proecological transport system. Journal of KONES Powertrain and Transport, Vol. 20, No. 3, 2013, p. 193 – 202.
60.	Kallio P., Pasztor A., Akhtar M.K., Jones P.R.: Renewable jet fuel. Current Opinion in Biotechnology. Vol. 26, 2014, p. 50–55.
61.	Kandaramath Hari T., Yaakob Z., Binitha N.N.: Aviation biofuel from renewable resources: Routes, opportunities and challenges. Renewable and Sustainable Energy Reviews. Vol. 42, 2015, p. 1234–1244. 
62.	Kinder J. D., Rahmes T.: Evaluation of Bio-Derived Synthetic Paraffinic Kerosene (Bio-SPK). The Boeing Company Sustainable Biofuels Research&Technology Program, 2009.
63.	Kirklin P.W., David. P.: Aviation Fuel: Thermal Stability. ASTM International, 1992.
64.	Lapuerta M., Rodriguez-Fernandeza J., Estevez C., Bayarri N.: Properties of fatty acid glycerol formal ester (FAGE) for use as a component in blends for diesel engines. Biomass and bioenergy. Vol. 76, 2015, p. 130–140. 
65.	Lebedevas S., Vaicekauskas A.: Research into the application of biodiesel in the transport sector of Lithuania. Transport. Vol. 21, Iss. 2, 2006, p. 80–87.
66.	Liu G., Yan B., Chen G.: Technical review on jet fuel production. Renewable and Sustainable Energy Reviews. Vol. 25, 2013, p. 59–70. 
67.	Lu M., Chai M.: Experimental Investigation of the Oxidation of Methyl Oleate: One of the Major Biodiesel Fuel Components Synthetic Liquids Production and Refining. Chapter 13, P. 289–312. American Chemical Society. 2011
68.	Merkisz J., Merkisz-Guranowska, A., Pielecha J., Nowak M., Jacyna M., Lewczuk K., Żak J.: Exhaust emission measurements in the development of sustainable road transport. Journal of KONES Powertrain and Transport, Vol. 20, No. 4 2013, p. 277 – 284.
69.	Maksimuk Yu., Antonova Z., Fes’ko V., Kursevich V.: Diesel biofuel viscosity and heat of combustion. Chemistry and technology of fuels and oils. Iss. 45, 2009, p. 343–346.
70.	Maru M. M., Trommer R.M., Cavalcanti K.F.: The Stribeck curve as a suitable characterization method of the lubricity of biodiesel and diesel blends. Energy. Vol. 69, 2014, p. 673–681.
71.	Maurice L.Q., Lander H., Edwards T., Harrison W.E.: Advanced aviation fuels: a look ahead via a historical perspective. Fuel. Vol. 80, Iss. 5, 2001, p. 747–756.
72.	Merkisz J., Markowski J., Pielecha J. Emission tests of the AI-14RA aircraft engine under real operating conditions of PZL-104"" Wilga"" plane. Silniki Spalinowe. No. 3, 2009, p. 64–70.
73.	Merkisz J., Galant M., Karpiński D., Kubiak, K. Evaluation of possibility to use the LTO cycle for emission test on example of the model turbine engine GTM-120 Journal of Mechanical and Transport Engineering. Vol. 66, No. 2, 2014, p. 25—33.
74.	Murphy D.J., Hall C.A.S.: Year in review—EROI or energy return on (energy) invested. Annals of the New York academy of sciences. Issue: Ecological Economics Reviews. Iss. 1185, 2010, p. 102–118.
75.	Murphy D.J., Hall C.A.S., Powers B.:New perspectives on the energy return on (energy) investment (EROI) of corn ethanol. Environment, Development and Sustainability. Vol. 13, Iss. 1, 2011, p. 179–202.
76.	Naik S.N., Goud V.V., Rout P.K., Dalai A.K.: Production of first and second generation biofuels: A comprehensive review. Renew. Sust. Energ. Rev., No. 14, 2010, p. 578–597.
77.	Nollet Leo M. L.: Handbook of Food Analysis: Physical characterization and nutrient analysis. CRC Press, 2004.
78.	Orszulik S.: Environmental Technology in the Oil Industry. Springer Science & Business Media, 2013.
79.	Pandey A.: Biofuels: Alternative Feedstocks and Conversion Processes. Academic Press, 2011.
80.	Pearlson M.N.: A techno-economic and environmental assessment of hydroprocessed renewable distillate fuels. Master of Science in Technology and Policy. Massachiussets Institute of Technology. June 2011.
81.	Prag P.: Renewable Energy in the Countryside. Taylor & Francis, 2014.
82.	Prussi M, Chiaramonti D, Recchia L, Martelli F, Guidotti F, Pari L.: Alternative feedstock for the biodiesel and energy production: the OVEST project. Energy Journal, No. 58, 2013, p. 2–8.
83.	Rahmes T.F., Kinder J.D., Henry T.M., etc.: Sustainable Bio-Derived Synthetic Paraffinic Kerosene (BioSPK) Jet Fuel Flights and Engine Tests Program Results. American Institute of Aeronautics and Astronautics, 2009.
84.	Rajagopal D., Zilberman D.: Environmental, Economic and Policy Aspects of Biofuels. Nеw Publishers Inc., 2008.
85.	Report on alternative fuels. International Air Transport Association IATA. http://www.iata.org/publications/Documents/2012-report-alternativefuels. pdf; 2012
86.	Rosillo Calle F, Trhan D, Seiffert M, Teeluckingh S. The potential and role of biofuels in commercial air transport – biojetfuels. Task 40 sustainable international bioenergy trade. IEA Bioenergy
87.	Sarin R., Kumar R., Srivastav B., etc.: Biodiesel surrogates: Achieving performance demands. Bioresource Technology. Vol. 100, Iss. 12, 2009, p. 3022–3028. 
88.	Shen Y.. Аn experimental study on thermal stability of FAEE biodiesel fuel with ethanol. Master Thesis, 2015.
89.	Shepherd J.E., Nuyt C.D., Lee J.J.: Flash Point and Chemical Composition of Aviation Kerosene (Jet A). National Transportation Safety Board, 2000.
90.	Singh B.: Biofuel Crops: Production, Physiology and Genetics. CABI, 2013.
91.	Singh B.: Biofuel Crop Sustainability. John Wiley & Sons, 2013.
92.	Sperling D., Cannon J.S.: Reducing Climate Impacts in the Transportation Sector. Springer Science & Business Media, 2011.
93.	Szczerek M., Tuszyсski W. Tribological researches – scuffing. Radom: Institute for Sustainable Technologies – National Research Institute, 2000.
94.	The jet engine. Rolls-Royce plc. Renault Printing Co Ltd., 1996.
95.	T-02U. Aparat czterokulowy – instrukcja obsługi. Radom: Wydawnictwo Instytutu Technologii Eksploatacji, 2011.
96.	Wcisło G.: Determination of the impact of FAME biocomponent on the fractional composition of diesel engine fuels. Combustion Engines. Iss. 154(3), 2013, p. 1098–1103.
97.	Xu Y., Wang Q., Hu X.: Characterization of the lubricity of bio-oil/diesel fuel blends by high frequency reciprocating test rig. Energy. Vol. 35, Iss. 1, 2010, p. 283–287. 
98.	Yakovleva A.V., Boichenko S.V., Lejda K, Vovk O.O., Kuszewski H.: Antiwear Properties of Plant—Mineral-Based Fuels for Airbreathing Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 1, 2017, p. 1–9. 
99.	Yakovlieva A.V., Boichenko S.V., Leida K., Vovk O.A., Kuzhevskii Kh.. Influence of Rapeseed Oil Ester Additives on Fuel Quality Index for Air Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 3, 2017. p. 308–317.
100.	Yakovlieva A., Boichenko S., Vovk O., Lejda K., Gryshchenko O.. Case Study of Alternative Jet Fuel Production with Bio-additives from Plant Oils in Ukraine and Poland. Advances in Sustainable Aviation. Springer International Publishing, 2018. Chapter 4.
101.	Yakovlieva A., Boshkov V. Experimental study of low-temperature properties of alternative aviation fuels, Proceedings of the 21th Conference for Junior Researchers ‘Science – Future of Lithuania’ Transport Engineering and Management, 4-5 May 2018, Vilnius, Lithuania. 2018. p. 130 – 134.
102.	Yildirim U, Abanteriba S.: Manufacture, qualification and approval of new aviation turbine fuels and additives, proceedia Engineering, No. 49, 2012, p. 310 – 315.
103.	Yutko B. and Hansman J., Approaches to Representing Aircraft Fuel Efficiency Performance for the Purpose of a Commercial Aircraft Certification Standard, MITInternational Center for Air Transportation, Cambridge, Mass, 2011.
104.	Zhu Y.: An Experimental Study on Thermal Stability of Biodiesel Fuel. Master Thesis. – 2012. – 160 p.
105.	Авиационный турбореактивный двигатель РУ 19A-300, руководство по эксплуатации и техническому обслуживанию, ЗАО «АНТЦ Технолог», 2001.
106.	Азев В.С., Середа А.В.: Влияние соединений серы на противоизносные свойства дизельных топлив, Химия и технология топлив и масел. № 3, 2009, c. 23–27.
107.	Андіїшин М.П., Марчук Я.С., Бойченко С.В., Рябоконь Л.А.: Газ природний, палива та оливи. Одеса: Астропринт, 2010.
108.	Бойченко С.В., Спіркін В.Г. Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт, Ч.1., 2009.
109.	Бойченко С.В., Любінін Й.А., Спіркін В.Г.: Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт. Ч.2., 2010.
110.	Бойченко С.В., Черняк Л.М., Яковлєва А.В.: Традиційні технології виробництва палив для повітряно-реактивних двигунів. Вісник Національного авіаційного університету. № 2 (55), 2013, с. 195–209.
111.	Бойченко С. В., Яковлева А. В., Волошинец В. А., Лейда К. Модифицирование эфиров рапсового масла вакуумным фракционированием, Технологии нефти и газа, №5, 2018, c. 15–20
112.	Братичак М.М.: Основи промислової нафтохімії, Львів: Вид-во НУ «Львівська політехніка», 2008.
113.	Васильев И.П.: Влияние топлив растительного происхождения на экологические и экономические показатели дизеля, Луганск: Изд-во ВНУ им. В. Даля, 2009.
114.	Волошинець В.А. Фізична та колоїдна хімія: Фізико-хімія дисперсних систем та полімерів: навч.посіб. Львів : Вид-во Львів. політехніки, 2013. – 200 с.
115.	Голоскоков А.Н. Критерии сравнения эффективности традиционных и альтернативных энергоресурсов. Нефтегазовое дело. № 1, 2011, c. 285–301.
116.	Голоскоков А.Н. Пик добычи нефти и начало мирового энергетического кризиса. Нефтегазовое дело. 2010, c. 1–13.
117.	Данилов А.М., Каминский Э.Ф., Хавкин В.А.: Альтернативные топлива: достоинства и недостатки. Проблемы применения. Российский химический журнал (Журнал Российского химического общества им. Д.И. Менделеева). Т. XLVII. № 6, 2003, c. 4–11.
118.	Дворецкий С.И., Нагорнов С.А., Романцова С.В. и др.: Производство биодизельного топлива из органического сырья. Вопросы современной науки и практики. № 39, 2012, c. 126– 35.
119.	Девянин С.Н., Марков В.А., Семенов В.Г.: Растительные масла и топлива на их основе для дизельных двигателей. Харьков: Новое слово. 2007.
120.	Ергин Д.: Добыча: Всемирная история борьбы за нефть, деньги и власть. Москва,: Альпина Паблишер, 2011.
121.	Запорожець А.О.: Дослідження стехіометричної суміші «повітря ‒ паливо» органічних сполук. Частина 1. Алкани. Наукоємні технології. № 2(22), 2014, c. 163–167.
122.	Кириченко В., Бойченко С., Кириченко В., Нездоровин В.: Комплексная переработка технических растительных масел: концепция, методы и технологи. «Systems and means of motor transport» Seria: Transport. Monografia. № 4, 2013, p. 357–370.
123.	Колодницька Р.В., Семенов В.Г.: Моделювання низькотемпературних властивостей біодизельних палив. Вісник СевНТУ. Серія: Машиноприладобудування та транспорт. № 134, 2012, c. 135–138.
124.	Коллоидная химия нефти и нефтепродуктов: Сборник материалов, посвященных научной деятельности проф. Г.И. Фукса. Москва: Изд-во «Техника». ООО «Тума Групп», 2001.
125.	Крылов И.Ф., Емельянов В.Е.: Альтернативные моторные топлива. Производство, применение",,'National Aviation University',Modification of jet fuels composition with renewable bio-additives,10.18372/37895,https://core.ac.uk/download/322991098.pdf,core
302961333,2019-01-01T00:00:00,"Booking cancellations negatively contribute to the production of accurate forecasts, which comprise a critical tool in the hospitality industry. Research has shown that with today’s computational power and advanced machine learning algorithms it is possible to build models to predict bookings cancellation likelihood. However, the effectiveness of these models has never been evaluated in a real environment. To fill this gap and investigate how these models can be implemented in a decision support system and its impact on demand-management decisions, a prototype was built and deployed in two hotels. The prototype, based on an automated machine learning system designed to learn continuously, lead to two important research contributions. First, the development of a training method and weighting mechanism designed to capture changes in cancellations patterns over time and learn from previous days’ predictions hits and errors. Second, the creation of a new measure – Minimum Frequency – to measure the precision of predictions over time. From a business standpoint, the prototype demonstrated its effectiveness, with results exceeding 84% in accuracy, 82% in precision, and 88% in Area Under the Curve (AUC). The system allowed hotels to predict their net demand and thus making better decisions about which bookings to accept and reject, what prices to make, and how many rooms to oversell. The systematic prediction of bookings with high probability of being canceled allowed hotels to reduce cancellations by 37 percentage points by acting to avoid their cancellation.info:eu-repo/semantics/publishedVersio","[{'title': 'Data Science Journal', 'identifiers': ['issn:1683-1470', '1683-1470']}]","'Ubiquity Press, Ltd.'",An automated machine learning based decision support system to predict hotel booking cancellations,10.5334/dsj-2019-032,https://core.ac.uk/download/302961333.pdf,core
301304096,2019-01-01T08:00:00,"Automated Parking is a low speed manoeuvring scenario which is quite unstructured and complex, requiring full 360° near-field sensing around the vehicle. In this paper, we discuss the design and implementation of an automated parking system from the perspective of camera based deep learning algorithms. We provide a holistic overview of an industrial system covering the embedded system, use cases and the deep learning architecture. We demonstrate a real-time multi-task deep learning network called FisheyeMultiNet, which detects all the necessary objects for parking on a low-power embedded system. FisheyeMultiNet runs at 15 fps for 4 cameras and it has three tasks namely object detection, semantic segmentation and soiling detection. To encourage further research, we release a partial dataset of 5,000 images containing semantic segmentation and bounding box detection ground truth via WoodScape project [Yogamani et al., 2019]",,Dublin Institute of Technology,FisheyeMultiNet: Real-time Multi-task Learning Architecture for Surround-view Automated Parking System.,,https://core.ac.uk/download/301304096.pdf,core
200814648,2019-06-11T00:00:00,"Automated surface-anomaly detection using machine learning has become an
interesting and promising area of research, with a very high and direct impact
on the application domain of visual inspection. Deep-learning methods have
become the most suitable approaches for this task. They allow the inspection
system to learn to detect the surface anomaly by simply showing it a number of
exemplar images. This paper presents a segmentation-based deep-learning
architecture that is designed for the detection and segmentation of surface
anomalies and is demonstrated on a specific domain of surface-crack detection.
The design of the architecture enables the model to be trained using a small
number of samples, which is an important requirement for practical
applications. The proposed model is compared with the related deep-learning
methods, including the state-of-the-art commercial software, showing that the
proposed approach outperforms the related methods on the specific domain of
surface-crack detection. The large number of experiments also shed light on the
required precision of the annotation, the number of required training samples
and on the required computational cost. Experiments are performed on a newly
created dataset based on a real-world quality control case and demonstrates
that the proposed approach is able to learn on a small number of defected
surfaces, using only approximately 25-30 defective training samples, instead of
hundreds or thousands, which is usually the case in deep-learning applications.
This makes the deep-learning method practical for use in industry where the
number of available defective samples is limited. The dataset is also made
publicly available to encourage the development and evaluation of new methods
for surface-defect detection.Comment: Journal of Intelligent Manufacturing 201",,'Springer Science and Business Media LLC',Segmentation-Based Deep-Learning Approach for Surface-Defect Detection,10.1007/s10845-019-01476-x,http://arxiv.org/abs/1903.08536,core
200820485,2019-04-02T00:00:00,"One type of machine learning, text classification, is now regularly applied
in the legal matters involving voluminous document populations because it can
reduce the time and expense associated with the review of those documents. One
form of machine learning - Active Learning - has drawn attention from the legal
community because it offers the potential to make the machine learning process
even more effective. Active Learning, applied to legal documents, is considered
a new technology in the legal domain and is continuously applied to all
documents in a legal matter until an insignificant number of relevant documents
are left for review. This implementation is slightly different than traditional
implementations of Active Learning where the process stops once achieving
acceptable model performance. The purpose of this paper is twofold: (i) to
question whether Active Learning actually is a superior learning methodology
and (ii) to highlight the ways that Active Learning can be most effectively
applied to real legal industry data. Unlike other studies, our experiments were
performed against large data sets taken from recent, real-world legal matters
covering a variety of areas. We conclude that, although these experiments show
the Active Learning strategy popularly used in legal document review can
quickly identify informative training documents, it becomes less effective over
time. In particular, our findings suggest this most popular form of Active
Learning in the legal arena, where the highest-scoring documents are selected
as training examples, is in fact not the most efficient approach in most
instances. Ultimately, a different Active Learning strategy may be best suited
to initiate the predictive modeling process but not to continue through the
entire document review.Comment: 2017 IEEE International Conference on Big Data (Big Data",,'Institute of Electrical and Electronics Engineers (IEEE)',"Empirical Evaluations of Active Learning Strategies in Legal Document
  Review",10.1109/BigData.2017.8258076,http://arxiv.org/abs/1904.01719,core
429063705,2019-10-01T00:00:00,"In this paper we consider the problem of feedforward controller design for industrial linear motors. These motors are safety-critical high-precision mechatronics systems that pose stringent requirements on the feedforward design: safe and predictable behavior for the desired motion profiles, tracking performance within the 10μ m range in the presence of nonlinear friction and real-time implementation within the 1ms range. We investigate and compare several possibilities to design data-driven feedforward controllers using neural networks (NN) and we show that a two-step inverse estimation method is the most suitable approach, due to robustness to noisy data. We also show that basic knowledge about the system dynamics and the friction behavior can be exploited to design neural feedforward controllers with a simple structure, suitable for real-time implementation in industrial linear motors. The developed data-driven neural feedforward controllers are tested and compared with standard mass-acceleration feedforward and iterative learning controllers in realistic simulations",,'Institute of Electrical and Electronics Engineers (IEEE)',Data-driven neural feedforward controller design for industrial linear motors,10.1109/icstcc.2019.8885434,,core
333949663,2019-01-01T00:00:00,"With the rapid development of big data, artificial intelligence and internet of things, digital twin technology becomes a new research hotspot in the field of intelligent manufacturing. In this paper, the digital twin technology for production line design and simulation is studied. Emphasis is laid on the building and fusion of production line model, virtual-real mapping and real-time interaction technology and virtual production line simulation and verification technology. The research content of this paper provides theoretical and technical reference for the application of digital twins in the design and implementation of manufacturing production line",,'Springer Fachmedien Wiesbaden GmbH',Research on Digital Twin Technology for Production Line Design and Simulation,,,core
478373242,2019-07-16T00:00:00,"Artificial Intelligence (AI) has wide range of applications in all areas and is gaining the understanding of the society as necessity instead of luxury. AI start ups are working to improve the quality of social interactions (social good), Education, Agriculture, manufacturing, health and medicine and public services. Hence the cost of not developing AI or developing it late is enormous. Despite the opportunities AI technologies may offer, there is a real risk that without thoughtful intervention it may in fact exacerbate structural, economic, social, and political imbalances, and further reinforce entrenched inequalities. For regulators and policymakers around the world, uneven access to technology remains a major concern because of its potential impacts on social and economic inequality. The author has conducted an exploratory research by reviewing related literatures on AI opportunities and challenges from experiences of the developed world and provided a discussion to identify the potential opportunities and expected challenges for AI adoption and implementation in Ethiopia. Finally the author has recommended what should be done.Keywords: Artificial, Intelligence,, Exploratory, Research, Skills, Infrastructure, Data, Privac",,'African Journals Online (AJOL)',Artificial intelligence for Ethiopia: opportunities and challenges,10.4314/ict.v16i1.,,core
224837114,2019-01-01T00:00:00,"The quantity of oil and chemical composition of the Agarwood essential oil should be evaluated to determine the performance of an extraction system. The aim of this work was to investigate the improvement to the three hydrodistillation (HD) systems when heat transfer control (HTC) approach using low-cost portable data logger with multi-temperature sensors is applied. The study focuses on the quantitative and qualitative characteristics of extracted essential oil from inoculated Agarwood for the real-time monitored HD compared to a conventional hydrodistillation (CHD). The extractions by conventional and HTC-ed HD procedures were carried out by supplying heat from liquefied petroleum gas (LPG); the ratio of the raw material to be extracted and the liquid solvent was 0.1 g·mL-1 and the extraction time was 72 hours. The compositions of the extracted essential oils (using HTC-ed HD and CHD) were assessed using gas chromatography with a flame ionization detector (GC–FID). The results of the extraction processes showed that the extraction of inoculated Agarwood essential oil assisted by multi-channel data logger was faster and produced higher yields compared to the CHD without a process monitoring device. Further, the testing of the chemical properties of the Agarwood oil showed that essential oil obtained by HTC-ed HD had better quality compared to the oil obtained by conventional HD. The implementation of real-time thermal management in HTC-ed HD technology in Agarwood essential oil production industry is therefore of great importance",,'Universiti Malaysia Pahang Publishing',Process improvement of multiple agarwood oil extractions using low-cost multi-channel temperature data logger,,https://core.ac.uk/download/224837114.pdf,core
249321724,2019-04-23T00:00:00,"As the Internet of Things (IoT) continues to evolve, the need arises to keep a constant eye on the integrity of IoT implementations to ensure that everything is running as it should. This is especially true in industrial sector. Machine Health Monitoring (MHM) was created. The primary goal of MHM is to ensure that all devices within an Industrial IoT (IIoT) implementation are functioning as expected at all times. For our specific usage of MHM, we are monitoring various readings from a 3D printer. We have currently implemented an array of sensors, and a live camera feed, with an accelerometer and a power meter soon to come. We have also created a web server running off of a Raspberry Pi 3 to hold our database and provide accessibility to the data. Through this setup, a user could access our storage and view up to the last 60 readings from the sensors in real time. The user can also view the live camera feed. We plan to use that range to implement fault detection with an alert system. We also look to eventually add predictive AI in an attempt to prevent faults before they occur. We believe our implementation provides a more cost-effective and scalable architecture for MHM. It also provides large amounts of customizability and adaptability due to our usage of generalized sensor data inputs and storage. The system is also self-monitoring, providing proper detection of its own internal issues",,Tennessee Tech University,Internet of Things (IoT) for Live Monitoring and Analysis of 3D Printers,,https://core.ac.uk/download/249321724.pdf,core
300008603,2019-01-01T00:00:00,"Residential fire is one of the most neglected problem in Singapore. Very few
residential premises are installed with fire detection system. The situation is becoming
worse as more and more people are getting e-scooter at home. These e-scooters could
easily catch fire or even explode due to over-charging. This causes the residential fire
more frequent and uncontrollable.
Most of the fire detectors are not effective in implementation in residential premises
due to false alarm. Cooking or burning incense paper could easily trigger the fire
detector. These activities are, in fact, not a real fire situation which generally happen
in every household on a daily basis. Without any better solution specifically for
residential application, excessive false alarm will happen and cause unnecessary
disruption to the residents.
This project aims to propose a new fire detection framework that could be effectively
implemented in residential premises. We’ve come out with an idea of using artificial
intelligence to detect the elements of SAFE state of fire. This idea is mainly inspired
by industrial fire watcher and ‘no-fire detector’ by Rainer Siebel. 4 hot works
(activities with elevated temperature to achieve useful purpose) that generally happen
in residential premises are targeted in this project, including cooking, burning incense
paper, lighted cigarette and mosquito coil.
The results show that our approach succeed to identify the hot works and being more
immune to false alarm as compared to smoke detector. Our approach could
complement with the existing detector to work as a 2-layer detection system, stopping
the detector from triggering unwanted alarm if the hot work is detected by our
framework.Bachelor of Engineering (Mechanical Engineering",,,Automation of fire watcher : sentry approach,,,core
200818569,2019-06-05T00:00:00,"Quality product descriptions are critical for providing competitive customer
experience in an e-commerce platform. An accurate and attractive description
not only helps customers make an informed decision but also improves the
likelihood of purchase. However, crafting a successful product description is
tedious and highly time-consuming. Due to its importance, automating the
product description generation has attracted considerable interests from both
research and industrial communities. Existing methods mainly use templates or
statistical methods, and their performance could be rather limited. In this
paper, we explore a new way to generate the personalized product description by
combining the power of neural networks and knowledge base. Specifically, we
propose a KnOwledge Based pErsonalized (or KOBE) product description generation
model in the context of e-commerce. In KOBE, we extend the encoder-decoder
framework, the Transformer, to a sequence modeling formulation using
self-attention. In order to make the description both informative and
personalized, KOBE considers a variety of important factors during text
generation, including product aspects, user categories, and knowledge base,
etc. Experiments on real-world datasets demonstrate that the proposed method
out-performs the baseline on various metrics. KOBE can achieve an improvement
of 9.7% over state-of-the-arts in terms of BLEU. We also present several case
studies as the anecdotal evidence to further prove the effectiveness of the
proposed approach. The framework has been deployed in Taobao, the largest
online e-commerce platform in China.Comment: KDD 2019 Camera-ready. Website:
  https://sites.google.com/view/kobe201",,'Association for Computing Machinery (ACM)',"Towards Knowledge-Based Personalized Product Description Generation in
  E-commerce",10.1145/3292500.3330725,http://arxiv.org/abs/1903.12457,core
322915690,2019-01-01T00:00:00,"Для теоретического обоснования построения эффективного оборудования рационально использовать метод динамического доминирующего загрязнителя (ДДЗ) (загрязнитель многокомпонентных сточных вод, который в данный момент времени при фактическом составе стоков необходимо в первую очередь удалить): оценка эффективности водоочистки за вторичным загрязнителем продемонстрировала, что при использовании комбинированных электротехнологических комплексов достигается его удаление, при устранении базового ДДЗ; проведения очистки не по ее ориентации на первоочередное устранение ДДЗ, а на действие на другие загрязнители продемонстрировали значительное ухудшение критерия энергоэффективности работы оборудования. Результатом применения нечетких нейронных сетей (с дальнейшей реализацией алгоритма Сугено) для получения новых знаний в сфере технического регулирования экологической безопасности процессов электротехнологической водоочистки на исследуемых предприятиях (производство бытовой химии) стали: функции принадлежности, которые структурируют значение загрязнителей при определении зависимости ранжира ДДЗ от их значений – выбирались по критерию «минимальная погрешность обучения»; базы знаний по формированию в режиме реального времени ранжира ДДЗ. Разработана методология настройки систем промышленной водоочистки с использованием виртуальной меры энергоэффективности водоочистки (ВМЭВ) дает возможность, с использованием метода ДДЗ настраивать промышленные системы, которые используют следующие базовые способы: биологический (через расчет подачи компрессором кислорода и известных параметров его окисляющего воздействия на органические загрязнители), физический (оценивая фильтрацию через сорбционный фильтр), химический (путем установления степени окисления в окислителях и эффективности коагуляции в емкости реактора), физико-химический (ключевой метод ВМЭВ) (оценивая работу электролизера рН-корректора и окислителей). Применение метода доминирующего динамического загрязнителя на основе лимитирующего показателя загрязненности (ЛПЗ) позволило обосновать комплексную методику управления экологической безопасностью систем очистки сточных вод промышленных объектов: уменьшение количества измеряемых ЛПЗ (до 3 раз) создаёт предпосылки для повышения эффективности управления экологической безопасностью промышленных объектов, поскольку улучшается общая надежность систем мониторинга, способных работать в режиме реального времени путем оптимизации количества их входных каналов.Для теоретичного обґрунтування побудови ефективного обладнання раціонально використовувати метод динамічного домінуючого забруднювача (ДДЗ) (забруднювач багатокомпонентних стічних вод, який в даний момент часу при фактичному складі стоків необхідно першочергово видалити): оцінка ефективності водоочищення за вторинним забруднювачем продемонструвала, що при використанні комбінованих електротехнологічних комплексів досягається його видалення при усуненні базового ДДЗ; проведення очищення не з її орієнтуванням на першочергове усунення ДДЗ, а на дію на інші забруднювачі, продемонстрували значне погіршення критерію енергоефективності роботи обладнання. Результатом застосування нечітких нейронних мереж (із подальшої реалізацією алгоритму Сугено) для отримання нових знань щодо технічного регулювання екологічною безпекою процесів електротехнологічного водоочищення на досліджуваних підприємствах (виробництво побутової хімії) стали: функції приналежності, які структурують значення забруднювачів при визначенні залежності ранжиру ДДЗ від їх значень – вибирались за критерієм «мінімальна похибка навчання»; бази знань щодо формування в режимі реального часу ранжиру ДДЗ. Розроблена методологія налаштування систем промислового водоочищення із використанням віртуальної міри енергоефективності водоочищення (ВМЕВ) дає можливість із використанням методу ДДЗ налаштовувати промислові системи, які використовують такі базові способи: біологічний (через розрахунок подачі компресором кисню та відомих параметрів його окислювальної дії на органічні забруднювачі); фізичний (оцінюючи фільтрацію через сорбційний фільтр); хімічний (шляхом встановлення ступеня окислення в окислювачах та ефективності коагуляції в ємності реакторові); фізико-хімічний (ключовий метод ВМЕВ) (оцінюючи роботу електролізера-рН-коректора та окислювачів). Застосування методу домінуючого динамічного забруднювача на основі лімітуючого показника забрудненості (ЛПЗ) дозволило обґрунтувати комплексну методику управління екологічною безпекою систем очищення стічних вод промислових об'єктів: зменшення кількості вимірюваних ЛПЗ (до 3 разів), створюючи передумови щодо покращення ефективності керування екологічною безпекою промислових об’єктів, оскільки покращується загальна надійність систем моніторингу здатних працювати у режимі реального часу шляхом оптимізації кількості їх вхідних каналів.For the theoretical justification for constructing efficient equipment, it is rational to use the method dynamic dominant pollutant (DDP) (multicomponent wastewater pollutant, which at given time with the actual composition wastewater must first be removed): assessment of the water treatment efficiency after the secondary pollutant has demonstrated that when using combined electrotechnological complexes, its removal is achieved, with the elimination of the basic remote sensing; cleaning, not by its focus on the primary elimination remote sensing, but on the effect on other pollutants, showed significant deterioration in the criterion energy efficiency of the equipment. The result of using fuzzy neural networks (with further implementation of the Sugeno algorithm) to gain new knowledge in the field technical regulation environmental safety of the processes electrotechnological water treatment at the enterprises under study (production household chemicals) was: membership functions that structure the value pollutants when determining the dependence of the range remote sensing data on their values – were selected according to the criterion “minimum learning error”; knowledge base on the formation in real time of the DDZ ranking. The methodology has been developed for setting up industrial water treatment systems using virtual water treatment energy efficiency measure (VWTEM), which makes it possible, using the DDP method, to set up industrial systems that use the following basic methods: biological (by calculating the compressor’s oxygen supply and known parameters of its oxidizing effects on organic pollutants), physical (evaluating filtration through sorption filter), chemical (by establishing the degree oxidation in oxidizing agents and the effect the effects coagulation in the reactor vessel), physicochemical (key method VWTEM) (evaluating the operation of the electrolyzer pH corrector and oxidizing agents). The application of the dominant dynamic pollutant method based on the limiting pollution index (LPI) allowed us to substantiate a comprehensive environmental management method for industrial wastewater treatment systems: reducing the number measured LPI (up to 3 times) creates the prerequisites for improving the environmental management industrial facilities, since the overall reliability monitoring systems capable working in real time by optimizing the number of the nature their input channels",,,Application of the dominant dynamic pollutant method for managing ecological safety of the industrial waste cleaning systems,,,core
334897044,2019-12-23T00:00:00,"Automated Parking is a low speed manoeuvring scenario which is quite
unstructured and complex, requiring full 360{\deg} near-field sensing around
the vehicle. In this paper, we discuss the design and implementation of an
automated parking system from the perspective of camera based deep learning
algorithms. We provide a holistic overview of an industrial system covering the
embedded system, use cases and the deep learning architecture. We demonstrate a
real-time multi-task deep learning network called FisheyeMultiNet, which
detects all the necessary objects for parking on a low-power embedded system.
FisheyeMultiNet runs at 15 fps for 4 cameras and it has three tasks namely
object detection, semantic segmentation and soiling detection. To encourage
further research, we release a partial dataset of 5,000 images containing
semantic segmentation and bounding box detection ground truth via WoodScape
project \cite{yogamani2019woodscape}.Comment: Accepted for publication at Irish Machine Vision and Image Processing
  (IMVIP) 201",,,"FisheyeMultiNet: Real-time Multi-task Learning Architecture for
  Surround-view Automated Parking System",,http://arxiv.org/abs/1912.11066,core
200834778,2019-06-16T00:00:00,"For short-term solar irradiance forecasting, the traditional point
forecasting methods are rendered less useful due to the non-stationary
characteristic of solar power. The amount of operating reserves required to
maintain reliable operation of the electric grid rises due to the variability
of solar energy. The higher the uncertainty in the generation, the greater the
operating-reserve requirements, which translates to an increased cost of
operation. In this research work, we propose a unified architecture for
multi-time-scale predictions for intra-day solar irradiance forecasting using
recurrent neural networks (RNN) and long-short-term memory networks (LSTMs).
This paper also lays out a framework for extending this modeling approach to
intra-hour forecasting horizons thus, making it a multi-time-horizon
forecasting approach, capable of predicting intra-hour as well as intra-day
solar irradiance. We develop an end-to-end pipeline to effectuate the proposed
architecture. The performance of the prediction model is tested and validated
by the methodical implementation. The robustness of the approach is
demonstrated with case studies conducted for geographically scattered sites
across the United States. The predictions demonstrate that our proposed unified
architecture-based approach is effective for multi-time-scale solar forecasts
and achieves a lower root-mean-square prediction error when benchmarked against
the best-performing methods documented in the literature that use separate
models for each time-scale during the day. Our proposed method results in a
71.5% reduction in the mean RMSE averaged across all the test sites compared to
the ML-based best-performing method reported in the literature. Additionally,
the proposed method enables multi-time-horizon forecasts with real-time inputs,
which have a significant potential for practical industry applications in the
evolving grid.Comment: 19 pages, 12 figures, 3 tables, under review for journal submissio",,,"An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting
  Using Deep Learning",,http://arxiv.org/abs/1905.02616,core
334851531,2019-08-26T00:00:00,"In this paper, we present a real-world conversational AI system to search for
and book hotels through text messaging. Our architecture consists of a
frame-based dialogue management system, which calls machine learning models for
intent classification, named entity recognition, and information retrieval
subtasks. Our chatbot has been deployed on a commercial scale, handling tens of
thousands of hotel searches every day. We describe the various opportunities
and challenges of developing a chatbot in the travel industry.Comment: Accepted to IEEE AI4I 2019 (International Conference on Artificial
  Intelligence for Industries",,,Real-world Conversational AI for Hotel Bookings,,http://arxiv.org/abs/1908.10001,core
286606337,2019-01-01T00:00:00,"\u3cp\u3eIn this paper we consider the problem of feedforward controller design for industrial linear motors. These motors are safety-critical high-precision mechatronics systems that pose stringent requirements on the feedforward design: safe and predictable behavior for the desired motion profiles, tracking performance within the 10μ m range in the presence of nonlinear friction and real-time implementation within the 1ms range. We investigate and compare several possibilities to design data-driven feedforward controllers using neural networks (NN) and we show that a two-step inverse estimation method is the most suitable approach, due to robustness to noisy data. We also show that basic knowledge about the system dynamics and the friction behavior can be exploited to design neural feedforward controllers with a simple structure, suitable for real-time implementation in industrial linear motors. The developed data-driven neural feedforward controllers are tested and compared with standard mass-acceleration feedforward and iterative learning controllers in realistic simulations.\u3c/p\u3",,'Institute of Electrical and Electronics Engineers (IEEE)',Data-driven neural feedforward controller design for industrial linear motors,,,core
337272580,2019-11-09T00:00:00,"A key challenge in carrying out product design research is obtaining rich contextual information about use in the wild. We present a method that algorithmically mediates between participants, researchers, and objects in order to enable real-time collaborative sensemaking. It facilitates contextual inquiry, revealing behaviours and motivations that frame product use in the wild. In particular, we are interested in developing a practice of use driven design, where products become research tools that generate design insights grounded in user experiences. The value of this method was explored through the deployment of a collection of Bluetooth speakers that capture and stream live data to remote but co-present researchers about their movement and operation. Researchers monitored a visualisation of the real-time data to build up a picture of how the speakers were being used, responding to moments of activity within the data, initiating text conversations and prompting participants to capture photos and video. Based on the findings of this explorative study, we discuss the value of this method, how it compares to contemporary research practices, and the potential of machine learning to scale it up for use within industrial contexts. As greater agency is given to both objects and algorithms, we explore ways to empower ethnographers and participants to actively collaborate within remote real-time research",,,Supporting Real-Time Contextual Inquiry through Sensor Data,,https://core.ac.uk/download/337272580.pdf,core
226755775,2019-06-17T00:00:00,"In light of the increasing demand and capacity in the railway industry, it is imperative to maintain safety in relation to the complexities of the substantial railway stations. Thus, it is important to take note of the time where investments in new technologies directed at the safety of the railway enable safety and protection in this area. Novel technological techniques such as big data analysis (BDA), data mining or machine learning (ML) have been developed and applied in many areas such as sales, banking and healthcare. The development of such methods has important benefits within the context of railway safety, however, these new methods need to be implemented and developed with consideration of whether these operational models can help to solve the various difficulties that currently exist in the risk analysis of railway stations. Moreover, as the adoption of the Internet of thing (IoT) grows, it is expected that analytical needs for handling data will also increase. It has been shown that the progression towards automation and applying such innovative new technologies such as BDA may be a powerful tool for integration in the future of transportation in general and the railway industry in particular, whereby analytical predictions can aid in the development of safer railway stations which have greater potential for ensuring the safety of passengers. In this paper a Bow Tie (BT) framework model has been created to combine BDA into the risk assessment process. The BDA can be beneficial to the risk assessment, support the decision makers in real time, and reduce human errors. This method can be fully integrated into passenger data and the business model for the railway station. Employing the existing safety records utilizing BDA is expected to mitigate risks, predict hazards, raise safety and security efficiency and reduce the cost",,IOP Publishing Ltd,Utilizing big data for enhancing passenger safety in railway stations,,https://core.ac.uk/download/226755775.pdf,core
395096808,2019-12-30T00:00:00,"Robotics systems are now increasingly widespread in our day-life. For instance, robots have been successfully used in several fields, like, agriculture, construction, defense, aerospace, and hospitality. However, there are still several issues to be addressed for allowing the large scale deployment of robots. Issues related to security, and manufacturing and operating costs are particularly relevant. Indeed, differently from industrial applications, service robots should be cheap and capable of operating in unknown, or partially-unknown environments, possibly with minimal human intervention. To deal with these challenges, in the last years the research community focused on deriving learning algorithms capable of providing flexibility and adaptability to the robots. In this context, the application of Machine Learning and Reinforcement Learning techniques turns out to be especially useful. In this manuscript, we propose different learning algorithms for robotics systems. In Chapter 2, we propose a solution for learning the geometrical model of a robot directly from data, combining proprioceptive measures with data collected with a 2D camera. Besides testing the accuracy of the kinematic models derived with real experiments, we validate the possibility of deriving a kinematic controller based on the model identified. Instead, in Chapter 3, we address the robot inverse dynamics problem. Our strategy relies on the fact that the robot inverse dynamics is a polynomial function in a particular input space. Besides characterizing the input space, we propose a data-driven solution based on Gaussian Process Regression (GPR). Given the type of each joint, we define a kernel named Geometrically Inspired Polynomial (GIP) kernel, which is given by the product of several polynomial kernels. To cope with the dimensionality of the resulting polynomial, we use a variation of the standard polynomial kernel, named Multiplicative Polynomial kernel, further discussed in Chapter 6. Tests performed on simulated and real environments show that, compared to other data-driven solutions, the GIP kernel-based estimator is more accurate and data-efficient.

In Chapter 4, we propose a proprioceptive collision detection algorithm based on GPR. Compared to other proprioceptive approaches, we closely inspect the robot behaviors in quasi-static configurations, namely, configurations in which joint velocities are null or close to zero. Such configurations are particularly relevant in the Collaborative Robotics context, where humans and robots work side-by-side sharing the same environment. Experimental results performed with a UR10 robot confirm the relevance of the problem and the effectiveness of the proposed solution.

Finally, in Chapter 5, we present MC-PILCO, a model-based policy search algorithm inspired by the PILCO algorithm. As the original PILCO algorithm, MC-PILCO models the system evolution relying on GPR, and improves the control policy minimizing the expected value of a cost function. However, instead of approximating the expected cost by moment matching, MC-PILCO approximates the expected cost with a Monte Carlo particle-based approach; no assumption about the type of GPR model is necessary. Thus, MC-PILCO allows more freedom in designing the GPR models, possibly leading to better models of the system dynamics. Results obtained in a simulated environment show consistent improvements with respect to the original algorithm, both in terms of speed and success rate",,,Learning algorithms for robotics systems,,,core
226908814,2019-07-08T00:00:00,"International audienceManufacturing companies are under a constant pressure due to multiple factors: new competition, disruptive innovations, cost reduction request, etc. To survive, they must strive to innovate and adapt their business model to improve their productivity. Recent developments based on the concept of Industry 4.0 such as big data, new communication protocols and artificial intelligence provide several new avenues to explore. In the specific context of machining, we are working toward the development of a system capable of making the prognostic of the quality (in terms of dimensional conformance) of a workpiece in real time while it is being manufactured. The goal of this paper is to showcase a prototype of the data acquisition aspect of this system and a case study presenting our first results. This case study has been conducted at our industrial partner facility (Quebec, Canada) and is based on the manufacturing of an aircraft component made from Inconel alloy 625 (AMS5666). The proposed prototype is a data acquisition system installed on a 5 axis CNC machines (GROB model G352) used to acquire and to contextualize the vibration signal obtained from the CNC machine sensor. The contextualization of the data is a key component for future work regarding the development of a prognostic system based on supervised machine learning algorithms. In the end, this paper depicts the system architecture as well as its interactions between the multiple systems and software already in place at our industrial partner. This paper also shows preliminary results describing the relationship between the workpiece quality (in terms of respect toward the dimensional requirements) and the extracted features from the sensors signals. We conclude that it is now possible to do the diagnostic of a cutting operation. Additionally, with the same information we show that it is possible to quickly do the general diagnostic of the health state of the machine. Future work regarding this project will include data acquisition from a wider range of products (i.e. different shapes, materials, processes, etc.) and the development of a machine learning based prognostic model",,HAL CCSD,Toward the quality prognostic of an aircraft engine workpiece in Inconel Alloy 625: case study and proposed system architecture,,,core
211061457,2019-01-01T00:00:00,"Business Process Management (BPM) is a central element of today’s organizations. Over the years, its main focus has been the support of business processes (BPs) in highly controlled domains. However—in the current era of Big Data and Internet-of-Things—several real-world domains are becoming cyber-physical (e.g., consider the shift from traditional manufacturing to Industry 4.0), characterized by ever-changing requirements, unpredictable environments and increasing amounts of data and events that influence the enactment of BPs. In such unconstrained settings, BPM professionals lack the needed knowledge to model all possible BP variants/contingencies at the outset. Consequently, BPM systems must increase their level of automation to provide the reactivity and flexibility necessary for process management. On the other hand, the Artificial Intelligence (AI) community has concentrated its efforts on investigating dynamic domains that involve active control of computational entities and physical devices (e.g., robots, software agents). In this context, automated planning, which is one of the oldest areas in AI, is conceived as a model-based approach to synthesize autonomous behaviors in automated way from a model. In this paper, we discuss how automated planning techniques can be leveraged to enable new levels of automation and support for solving concrete problems in the BPM field that were previously tackled with hard-coded solutions. To this aim, we first propose a methodology that shows how a researcher/practitioner should approach the task of encoding a concrete problem as an appropriate planning problem. Then, we discuss the required steps to integrate the planning technology in BPM environments. Finally, we show some concrete examples of the successful application of planning techniques to the different stages of the BPM life cycle",,'Springer Science and Business Media LLC',Automated planning for business process management,10.1007/s13740-018-0096-0,,core
390037696,2019-09-11T00:00:00,"The article deals with the prerequisites and generalizes approaches to defining the concept of “creative economy”; on the theoretical basis and practices of creative economy in leading countries of the world an internals and the main principles of its development are identified; genesis of the concept of creative economy is considered; goals, policies, implementation practices in countries around the world are systematized. The position of Ukraine and other countries of the world in the ranking of the global index of creativity are characterized; features of the creative industry in Ukraine are revealed; index of activity of the creative industry of Ukraine is calculated, which indicates a stable tendency of its development.  Potentials of digital technologies, in particular, artificial intelligence, augmented and virtual reality, blockchain technology, to the transformation of creative economy are grounded.The article deals with the prerequisites and generalizes approaches to defining the concept of “creative economy”; on the theoretical basis and practices of creative economy in leading countries of the world an internals and the main principles of its development are identified; genesis of the concept of creative economy is considered; goals, policies, implementation practices in countries around the world are systematized. The position of Ukraine and other countries of the world in the ranking of the global index of creativity are characterized; features of the creative industry in Ukraine are revealed; index of activity of the creative industry of Ukraine is calculated, which indicates a stable tendency of its development.  Potentials of digital technologies, in particular, artificial intelligence, augmented and virtual reality, blockchain technology, to the transformation of creative economy are grounded",,Черкаський навчально-науковий інститут Університету банківської справи,The theory and practice of creative economy in the conditions of digitalization,,,core
475180207,2019-04-04T07:00:00,"The recent advancements in computing and sensor technologies, coupled with improvements in embedded system design methodologies, have resulted in the novel paradigm called the Internet of Things (IoT). IoT is essentially a network of small embedded devices enabled with sensing capabilities that can interact with multiple entities to relay information about their environments. This sensing information can also be stored in the cloud for further analysis, thereby reducing storage requirements on the devices themselves. The above factors, coupled with the ever increasing needs of modern society to stay connected at all times, has resulted in IoT technology penetrating all facets of modern life. In fact IoT systems are already seeing widespread applications across multiple industries such as transport, utility, manufacturing, healthcare, home automation, etc.
Although the above developments promise tremendous benefits in terms of productivity and efficiency, they also bring forth a plethora of security challenges. Namely, the current design philosophy of IoT devices, which focuses more on rapid prototyping and usability, results in security often being an afterthought. Furthermore, one needs to remember that unlike traditional computing systems, these devices operate under the assumption of tight resource constraints. As such this makes IoT devices a lucrative target for exploitation by adversaries. This inherent flaw of IoT setups has manifested itself in the form of various distributed denial of service (DDoS) attacks that have achieved massive throughputs without the need for techniques such as amplification, etc. Furthermore, once exploited, an IoT device can also function as a pivot point for adversaries to move laterally across the network and exploit other, potentially more valuable, systems and services. Finally, vulnerable IoT devices operating in industrial control systems and other critical infrastructure setups can cause sizable loss of property and in some cases even lives, a very sobering fact.
In light of the above, this dissertation research presents several novel strategies for identifying known and  zero-day attacks against IoT devices, as well as identifying infected IoT devices present inside a network along with some mitigation strategies. To this end, network telescopes are  leveraged to generate Internet-scale notions of maliciousness in conjunction with signatures that can be used to identify such devices in a network. This strategy is further extended by developing a taxonomy-based methodology which is capable of categorizing unsolicited IoT behavior by leveraging machine learning (ML) techniques, such as ensemble learners, to identify similar threats in near-real time. Furthermore, to overcome the challenge of insufficient (malicious) training data within the IoT realm, a generative adversarial network (GAN) based framework is also developed to identify known and unseen attacks on IoT devices. Finally, a software defined networking (SDN) based solution is proposed to mitigate threats from unsolicited IoT devices",,Scholar Commons,Security Framework for the Internet of Things Leveraging Network Telescopes and Machine Learning,,,core
386635884,2019-01-01T00:00:00,"Cyber fraud is rampant. The recent Covid 19 pandemic is a good example of the same. Domain Tools in April 2020 identified over 65,000 websites have been identified as fraud scams related to Covid-19. Organisations have lost

billions of money in online scams, and in particular with payment diversion fraud (‘PDF’) and ransomware. PDF is a type of cyber-attack where an entity is tricked into making a direct payment from its account to a false supplier/entity often using real-time payment methods. Ransomware is a type of malicious software that prevents users from accessing their system or

personal files usually by locking them through encryption, and demands ransom payment in order to regain access. Based on the professional experience of the authors, coupled with current literature, there is a growing trend of automation, with the use of machine-learning and artificial intelligence. This article discusses PDF and ransomware in the context of mechanics and emerging trends for systematic attacks and response by private industry. These case studies illustrate the limited role that the law plays in the investigation and response to cyber fraud",,"India, Indian Journal of Law and Technology",Artificial intelligence enabled cyber fraud : a detailed look into payment diversion fraud and ransomware,,,core
304994832,2019-05-22T13:05:45,"A growing number of commercial and enterprise systems are increasingly relying on compute-intensive machine learning algorithms. While the demand for these apaplications is growing, the performance benefits from general-purpose platforms is diminishing. This challenge has coincided with the explosion of data where the rate of data generation has reached an overwhelming level that is beyond the capabilities of current computing systems. Therefore, applications such as machine learning and robotics can benefit from hardware acceleration. Traditionally, to accelerate a set of workloads, we pro- file the code optimized for CPUs and offload the hot functions on hardware compute units designed specially for that particular function, hence providing higher performance and energy efficiency. Instead in this work, we take a revolutionary approach where we delve into the algorithmic properties of applications to define domain-generic hardware acceleration solutions. We leverage the property that a wide range of machine learning algorithms can be modeled as stochastic optimization problems. Using this insight we devise compute stacks for hardware acceleration that are built independent of the CPU. These stacks expose a high-level mathematical programming interface and automatically generate accelerators for users who have limited knowledge about hardware design, but can benefit from large performance and efficiency gains for their programs. 
Keeping these ambitious goals in mind, our work (1) strikes a balance between generality and specialization by breaking the long-held traditional abstraction of the Instruction Set Architecture (ISA) in favor of a more algorithm-centric approach; (2) develops hard- ware acceleration frameworks by co-designing a language, compiler, runtime system, and hardware to provide high performance and efficiency, in addition to flexibility and programmability; (3) segregates algorithmic specification from implementation to shield the programmer from continual hardware/software modifications while allowing them to benefit from the emerging heterogeneity of modern compute platforms; and (4) develops real cross-stack prototypes to evaluate these innovative solutions in a real-world setting and make them open-source to maximize community engagement and industry impact. Our work TABLA (http://act-lab.org/artifacts/tabla/) is public, and defines the very first open-source hardware platform for machine learning and artificial intelligence.Ph.D",,Georgia Institute of Technology,Balancing generality and specialization for machine learning in the post-ISA era,,https://core.ac.uk/download/304994832.pdf,core
185531452,2019-01-01T00:00:00,"The book covers a variety of topics in Information and Communications Technology (ICT) and their impact on innovation and business. The authors discuss various innovations, business and industrial motivations, and impact on humans and the interplay between those factors in terms of finance, demand, and competition. Topics discussed include the convergence of Machine to Machine (M2M), Internet of Things (IoT), Social, and Big Data. They also discuss AI and its integration into technologies from machine learning, predictive analytics, security software, to intelligent agents, and many more. Contributions come from academics and professionals around the world.

Covers the most recent practices in ICT related topics pertaining to technological growth, innovation, and business; Presents a survey on the most recent technological areas revolutionizing how humans communicate and interact; Features four sections: IoT, Wireless Ad Hoc & Sensor Networks, Fog Computing, and Big Data Analytics.(Chapter) The recent advancements in robotic systems set new challenges for robotic simulation software, particularly for planning. It requires the realistic behavior of the robots and the objects in the simulation environment by incorporating their dynamics. Furthermore, it requires the capability of reasoning about the action effects. To cope with these challenges, this study proposes an open-source simulation tool for knowledge-oriented physics-based motion planning by extending The Kautham Project, a C++ based open-source simulation tool for motion planning. The proposed simulation tool provides a flexible way to incorporate the physics, knowledge and reasoning in planning process. Moreover, it provides ROS-based interface to handle the manipulation actions (such as push/pull) and an easy way to communicate with the real robotsPeer ReviewedPostprint (author's final draft",,'Springer Science and Business Media LLC',A tool for knowledge-oriented physics-based motion planning and simulation,10.1007/978-3-319-99966-1,https://core.ac.uk/download/185531452.pdf,core
304117829,2019-01-01T00:00:00,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search",,'Institute of Electrical and Electronics Engineers (IEEE)',Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,10.23919/DATE.2019.8714959,https://core.ac.uk/download/304117829.pdf,core
297650656,,"Fault detection and diagnosis is a critical element in the power generation sector.
Early faults detection ensures that correct mitigation measures can be taken,
whilst false alarms should be eschewed to avoid unnecessary cost of operation,
interruption and downtime. Modern power plant is equipped with thousands of
sensors for monitoring, diagnosis and sensor validation application. By utilizing
these features, we can use the collected operational data to develop a data-driven
condition monitoring method. Intelligent Early Warning System (IEWS)
represented by Artificial Neural Network (ANN), which was developed by
training the network with real operational data, can be proven useful for real-time
monitoring of a power plant. In this work, an integrated data preparation method
was proposed. The ANN models and the hybrid artificial intelligence (AI) of
ANN with Genetic Algorithm (GA), which is able to detect steam turbine trip for
Malaysia Jana Manjung (MNJ) power station were developed. The AI models
adopting ANN and GA were trained with real data from the MNJ station. The
developed models were capable of detecting the specific trip earlier before the
actual trip occurrence was detected by the existing control system. The AI model
provides a good opportunity for further research and implementation of AI in the
power generation industry especially in fault detection and diagnosis initiatives",,,Development of intelligent early warning system for steam turbine,,,core
286362674,2019-01-01T00:00:00,"The rise of DNA sequencing and synthesis technologies over the past two decades has ushered in a new wave of forward engineering genetic circuits, synthetic biology. Synthetic biology has since been deployed for applications spanning therapeutics, industrial chemical biosynthesis, and environmental sensing. Coupled with advances in genomics and systems biology, synthetic biology has revolutionized our ability to investigate biological networks. Further utilized with the fine-tuned experimental control of microfluidics, synthetic biology has enabled the precise interrogation of single nodes in these biological networks. The emergence of genome-scale microfluidic devices has bridged the gap between the multiplexing of -omics technology and the dynamics of microfluidics. Towards this end, we have developed an elegant, simple microfluidic platform capable of monitoring the temporal gene expression of 2,176 unique microbes with both research and industrial application. In Chapter 1, I provide a brief overview of synthetic biology and microfluidics in the context of biological research. In Chapter 2, I describe the high-throughput microfluidic platform we have engineered and the protocol for building these devices. In Chapter 3, I demonstrate the platform’s utility as an environmental biosensor and research tool, where the dynamics of 1,807 E. coli GFP-promoter strains coupled with machine learning algorithms are used to detect the presence of six heavy metals in real-time in both laboratory and real-world settings. Finally in Chapter 4, I show the device’s application for the dynamical screening of synthetic gene circuit libraries. In all, I highlight the need for the further development of such multiplexed microfluidic platforms and demonstrate their utility for biological research, synthetic gene circuit engineering, and environmental biosensing",,"eScholarship, University of California","Highly multiplexed microfluidics for dynamic genome interrogation, synthetic gene circuit screening, and multi-target biosensing applications.",,,core
237690220,2019-01-01T00:00:00,"In the Architecture, Engineering, Construction and Operations (AECO) there is a growing interest in the use of the Building Information Modelling (BIM). Through integration of information and processes in a digital model, BIM can optimise resources along the lifecycle of a physical asset. Despite the potential savings are much higher in the operational phase, BIM is nowadays mostly used in design and construction stages and there are still many barriers hindering its implementation in Facility Management (FM). Its scarce integration with live data, i.e. data that changes at high frequency, can be considered one of its major limitations in FM. The aim of this research is to overcome this limit and prove that buildings or infrastructures operations can benefit from a digital model updated with live data. The scope of the research concerns the optimisation of FM operations. The optimisation of operations can be further enhanced by the use of maintenance smart contracts allowing a better integration between users’ behaviour and maintenance implementation. In this case study research, the Image Recognition (ImR), a type of Artificial Intelligence (AI), has been used to detect users’ movements in an office building, providing real time occupancy data. This data has been stored in a BIM model, employed as single reliable source of information for FM. This integration can enhance maintenance management contracts if the BIM model is coupled with a smart contract. Far from being a comprehensive case study, this research demonstrates how the transition from BIM to the Asset Information Model (AIM) and, finally, to the Digital Twin (i.e. a near-real-time digital clone of a physical asset, of its conditions and processes) is desirable because of the outstanding benefits that have already been measured in other industrial sectors by applying the principles of Industry 4.0",,'WITPRESS LTD.',Office building occupancy monitoring through image recognition sensors,10.2495/SAFE-V9-N4-371-380,,core
334855055,2019-09-04T00:00:00,"Precise robotic grasping is important for many industrial applications, such
as assembly and palletizing, where the location of the object needs to be
controlled and known. However, achieving precise grasps is challenging due to
noise in sensing and control, as well as unknown object properties. We propose
a method to plan robotic grasps that are both robust and precise by training
two convolutional neural networks - one to predict the robustness of a grasp
and another to predict a distribution of post-grasp object displacements. Our
networks are trained with depth images in simulation on a dataset of over 1000
industrial parts and were successfully deployed on a real robot without having
to be further fine-tuned. The proposed displacement estimator achieves a mean
prediction errors of 0.68cm and 3.42deg on novel objects in real world
experiments.Comment: Submitted and accepted to 12th Conference on Field and Service
  Robotics (FSR 2019",,,"Towards Precise Robotic Grasping by Probabilistic Post-grasp
  Displacement Estimation",,http://arxiv.org/abs/1909.02129,core
200812390,2019-03-18T00:00:00,"This paper presents an upgraded, real world application oriented version of
gym-gazebo, the Robot Operating System (ROS) and Gazebo based Reinforcement
Learning (RL) toolkit, which complies with OpenAI Gym. The content discusses
the new ROS 2 based software architecture and summarizes the results obtained
using Proximal Policy Optimization (PPO). Ultimately, the output of this work
presents a benchmarking system for robotics that allows different techniques
and algorithms to be compared using the same virtual conditions. We have
evaluated environments with different levels of complexity of the Modular
Articulated Robotic Arm (MARA), reaching accuracies in the millimeter scale.
The converged results show the feasibility and usefulness of the gym-gazebo 2
toolkit, its potential and applicability in industrial use cases, using modular
robots",,,"gym-gazebo2, a toolkit for reinforcement learning using ROS 2 and Gazebo",,http://arxiv.org/abs/1903.06278,core
200842252,2019-05-24T00:00:00,"The increasing interest in the usage of Artificial Intelligence techniques
(AI) from the research community and industry to tackle ""real world"" problems,
requires High Performance Computing (HPC) resources to efficiently compute and
scale complex algorithms across thousands of nodes. Unfortunately, typical data
scientists are not familiar with the unique requirements and characteristics of
HPC environments. They usually develop their applications with high-level
scripting languages or frameworks such as TensorFlow and the installation
process often requires connection to external systems to download open source
software during the build. HPC environments, on the other hand, are often based
on closed source applications that incorporate parallel and distributed
computing API's such as MPI and OpenMP, while users have restricted
administrator privileges, and face security restrictions such as not allowing
access to external systems. In this paper we discuss the issues associated with
the deployment of AI frameworks in a secure HPC environment and how we
successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.Comment: 6 pages, 2 figures, 2019 IEEE High Performance Extreme Computing
  Conferenc",,'Institute of Electrical and Electronics Engineers (IEEE)',Deploying AI Frameworks on Secure HPC Systems with Containers,10.1109/HPEC.2019.8916576,http://arxiv.org/abs/1905.10090,core
270088355,2019-10-28T00:00:00,"The human motor system is robust, adaptive and very flexible. The underlying principles of human motion provide inspiration for robotics. Pointing at different targets is a common robotics task, where insights about human motion can be applied. Traditionally in robotics, when a motion is generated it has to be validated so that the robot configurations involved are appropriate. The human brain, in contrast, uses the motor cortex to generate new motions reusing and combining existing knowledge before executing the motion. We propose a method to generate and control pointing motions for a robot using a biological inspired architecture implemented with spiking neural networks. We outline a simplified model of the human motor cortex that generates motions using motor primitives. The network learns a base motor primitive for pointing at a target in the center, and four correction primitives to point at targets up, down, left and right from the base primitive, respectively. The primitives are combined to reach different targets. We evaluate the performance of the network with a humanoid robot pointing at different targets marked on a plane. The network was able to combine one, two or three motor primitives at the same time to control the robot in real-time to reach a specific target. We work on extending this work from pointing to a given target to performing a grasping or tool manipulation task. This has many applications for engineering and industry involving real robots",,Frontiers Media,Generating pointing motions for a humanoid robot by combining motor primitives,10.5445/IR/1000099401,https://core.ac.uk/download/270088355.pdf,core
289166719,2019-08-01T00:00:00,"Industry 4.0 refers to the evolution in manufacturing from computerization to
fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical
control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts.
In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0
applications. We also explore sensing techniques, including cameras (and depth sensors), and
other light-based solutions for object positioning and detection along with their respective
limitations. We then demonstrate an application of positioning for real time robot control in
an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience
with this cyber-physical-virtual application, we propose Ray-Surface Positioning (RSP), a
novel VLP technique, as a low cost positioning system for Industry 4.0.Accepted manuscrip","[{'title': None, 'identifiers': ['issn:2154-0217', '2154-0217']}]",'Institute of Electrical and Electronics Engineers (IEEE)',Visible light positioning for location-based services in Industry 4.0,10.1109/ISWCS.2019.8877305,https://open.bu.edu/bitstream/2144/39474/1/TR.pdf,core
232687139,2019-01-01T08:00:00,"Effective management of runoff from rain and snowmelt is critical as increased water flows can negatively affect efficiency and reliability at treatment facilities, as well as potentially damage property or the natural environment. Implementation of artificial intelligence for real-time decision making and support in wet weather infrastructure is a recent technological development; as such, a problem has emerged: experience and knowledge of best practices for successful implementation is limited. Artificial intelligence is being employed to inform operational decisions that are intended to improve the efficiency and reliability of physical wet weather infrastructure. The goal of municipalities and utilities in utilizing artificial intelligence is to maximize use of the existing physical infrastructure and reduce the need for future capital investment. Because artificial intelligence for real-time decision making and support in wet weather infrastructure is a relatively new technology, experience and knowledge of best practices for successful implementation is limited. In addition, staff have been reluctant to embrace or trust the decisions and support made by the AI systems in certain cases. This study approaches the problem through comprehensive review of recent literature and interviews of those responsible for previous implementations of artificial intelligence in Saint Paul, MN, Buffalo, NY, and Kansas City, MO. Best practices include continuous operator input and ongoing training throughout the project, effective and proper maintenance of the “inputs” to the artificially intelligent system, and incorporation of failsafe mechanisms in the design. As artificial intelligence becomes more prevalent in the civil engineering industry and computers are increasingly given real-time control of systems, this study could provide future designers with a framework for successful implementation of artificial intelligence in wet weather infrastructure projects",,IdeaExchange@UAkron,Artificial Intelligence in Wet Weather Infrastructure,,https://core.ac.uk/download/232687139.pdf,core
223022076,2019-01-01T08:00:00,"Continued outbreaks of foodborne illness involving dairy products in the United States stress the importance for rapid methods of detection of pathogenic microorganisms in food processing environments. Pathogenic microorganisms, such as Salmonella are widespread, and can be found in a variety of foods, ingredients and in industrial environments. The presence of pathogens in dairy products constitutes great risk for increased exposure, illness and reduces overall quality of the foodstream. As a result, emphasis has been placed on adapting or developing sensitive techniques to rapidly detect notable pathogens, such as Salmonella, Listeria monocytogenes and Escherichia coli O157:H7 in both contaminated foods and industrial environments. Common assays employed in the detection of pathogenic microorganisms, though effective in identification, are time consuming and may require several days for processing. The necessity to quickly screen food products and industrial environments has led to an emphasis to develop rapid, sensitive, automated techniques in food processing operations. Numerous methods of identification and detection have been implemented in food processing environments.
An optimal approach to the rapid detection of microbial pathogens would incorporate several advantages including: 1) improved time-to-result, 2) low-cost, 3) ease of operation and 4) simple interpretation. Such an approach may enable simple and cost-effective sampling of pathogenic microorganisms, which can be used to improve industrial efficiency. As a possible alternative to existing detection efforts, low-cost diagnostic (LCD) tools, particularly paper-based analytical devices (PADs), may be employed for rapid, sensitive and selective detection. PADs are frequently combined with colorimetric detection, in which chromogenic substrates are used to yield a visual representation of detection. Different enzyme-substrate pairs may be employed to accomplish various goals—from simple “presence/absence” to species-specificity. While “presence/absence” is limited, the use of shared enzymes is advantageous during detection and identification of metabolic state. Depending upon environmental factors, bacteria may exist in active or dormant states; reversion of a pathogen from dormancy to a metabolically active state may result in rapid growth and instances of illness.
As the level of enzymatic expression varies between metabolic states, oxidoreductases and alkaline phosphatases (ALP) were investigated as vehicles for colorimetric detection. Oxidoreductases are present in greater amounts in metabolically active bacteria, and are capable of reducing 2-(4-iodophenyl)-3-(4-nitrophenyl)-5-phenyl-2H-tetrazolium chloride (INT) to formazans. Nitrophenyl phosphate (PNPP) is present in dormant bacteria, and cleaves phosphate groups from para-nitrophenyl-phosphate salts, resulting in para-nitrophenol. Combined use of enzymatic substrates, including INT and 5-methylphenazin-5-ium methyl sulfate (PMS) for metabolically active bacteria, and INT and PNPP for dormant bacteria, yielded an improved colorimetric readout visible by eye within 30 min. With detection achieved within 30 min, the two assays, INT-PMS and INT-PNPP, decrease time-to-result, are portable and may be amenable to on-site detection in agricultural, environmental and industrial settings.
While the use of non-specific bacterial enzymes may limit some applications, immobilization of bacteria-specific bacteriophage (P22, T4) onto paper can provide an additional layer of specificity. Bacteriophage are robust, and may be easily absorbed onto paper. In this work, immobilized bacteriophage facilitated specific capture of Salmonella Typhimurium on paper, followed by detection of metabolic state with either the INT-PMS or INT-PNPP assay. This combined approach can be applied to the analysis of mixed cultures, given the generally genera-specific nature of the selected bacteriophages. Moreover, the use of chromogenic substrates simplifies assay design, as color change is easily interpreted by the eye or with basic instrumentation. However, despite these advantages, the requirement for a 48-hour absorption period represents a drawback, lengthening time-to-result.
An alternative to the use of bacteriophage for cell capture are magnetic ionic liquids (MILs). MILs are magnetoactive “molten salt” solvents, containing a paramagnetic component integrated into the cation or anion moiety of the salt. MILs are considered “green” solvents, and are nonvolatile, nonflammable, with tunable physicochemical properties. Due to their hydrophobic and liquid nature, MILs can be quickly be distributed with agitation (stirring or vortexing) throughout aqueous food samples as liquid micro- or nanodispersions. After encountering and binding bacterial cells, cell-MIL complexes can then be collected magnetically or after density-driven sedimentation for further processing. MIL-based capture of bacteria has been previously combined with real-time polymerase chain reaction (qPCR) for the rapid detection of E. coli. While use of qPCR obviates the need for time-consuming steps such as gel electrophoresis, its inherent complexity and cost may prohibit its use in point-of-care or resource-limited settings. Isothermal methods for nucleic acid amplification, such as recombinase polymerase amplification (RPA), may have considerable advantages as alternatives to PCR. RPA results in exponential amplification of nucleic acids and operates at a constant, near-physiological temperature (~40°C), eliminating the need for a thermocycler, generating target-specific amplicons in less than 20 min.
The combined use of MIL-based extraction and rapid, streamlined pathogen detection using RPA was investigated. The ability of MIL solvents to quickly extract Salmonella Typhimurium was first examined by dispersing MIL into an aqueous suspension, followed by rapid (~30 s) physical enrichment (concentration) and extraction using an applied magnetic field. Following extraction, viable bacteria were desorbed from the MIL extraction phase with exposure to a nutrient-rich broth (Luria Bertani medium), referred here to as a “back-extraction” step. In efforts to improve back-extraction, recovery of the model Gram-negative bacterium Serratia marcescens from the MIL extraction phase was investigated using several back-extraction media varying in ionic strength and nutrient composition. The highest recovery of cells was obtained using a nutrient-rich tryptone medium supplemented with NaCl. This modification of the extraction protocol enabled improvement in MIL-based bacterial concentration, enriching cells by a factor of 5 - 6X within 3–5 min.
The improved MIL assay was then examined in conjunction with RPA for rapid detection of Salmonella Typhimurium. MIL-based sample preparation was compared with use of a commercial sample preparation solution, PrepMan® Ultra Sample Preparation Reagent (PMU), for detection of Salmonella Typhimurium in artificially-contaminated pasteurized foods. PMU is commonly coupled with PCR to eliminate or inactivate PCR inhibitors and uses both heating and centrifugation steps. As an established method for sample preparation, use of PMU served as a benchmark method against which our MIL-based process was compared. In aqueous suspensions of Salmonella Typhimurium, detection was achieved as low as 103 CFU mL-1 using the combined MIL-RPA approach, which is equivalent to the previously investigated MIL-qPCR method, and, in our hands, outperformed the PMU method by an order of magnitude. Visualization of amplified products was achieved using gel electrophoresis or lateral flow readouts. Nucleic acid lateral flow immunoassays (NALFIA) require less than 5 min for amplicon visualization, are portable, require minimal technical expertise during interpretation and are easy to implement outside of laboratory settings. The need for electric-based heating elements for RPA incubation was eliminated through the use of low-cost, portable, supersaturated sodium acetate heat packs. This repurposing of consumer-grade hand warmers for nucleic acid amplification is a novel approach and easily incorporated into the MIL-RPA scheme.
While MILs have been successfully used for capture and concentration of bacteria from foods prior to culture- or nucleic acid-based detection, little is known about their interactions with bacteria—including modes of physical association or potential antimicrobial activities. Further understanding these interactions may facilitate optimization of MIL-based capture in challenging food matrices, as well as modification of downstream procedures to mitigate the impacts of potential bacterial injury during extraction and concentration. To begin this work, a series of multi-strain panels, including seven representative Salmonella DNA subgroups and eight strains of E. coli O157:H7, were exposed to the Ni(II) MIL and plated in parallel on non-selective and selective media. Calculated enrichment factors (EF) were similar between media types, while individual cell counts were nearly identical, suggesting that the Ni(II) MIL, as applied during our capture and concentration assay, does not cause assay-limiting cellular injury in these two pathogens. Observed variability between EF values may result from differences in the extraction efficiency of the MIL, with some strains exhibiting weaker affinity for the MIL compared to other strains tested, which is an area of ongoing research. Importantly, our results demonstrate capture and recovery of strains representative of all seven Salmonella DNA subgroups and all eight strains of E. coli O157:H7 tested, with comparable recovery on non-selective and selective media. This initial and ongoing research on characterization of MIL-bacterial interactions establishes the foundation for further evaluation of new MIL structures for improving the preconcentration and recovery of viable microorganisms from complex food matrices",,Iowa State University Digital Repository,"Novel approaches to the low-cost, portable and rapid detection of bacterial pathogens in foods and food-processing environments",,https://core.ac.uk/download/223022076.pdf,core
287763797,2019-01-01T00:00:00,"Fuzzy systems have become widely accepted and applied in a host of domains such as control, electronics or mechanics. The
software for construction of these systems has traditionally been exploited from tools, platforms and languages run on-premise
computing infrastructure. On the other hand, rise and ubiquity of the cloud computing model has brought a revolutionary way
for computing services deployment. The boost of cloud services is leading towards increasingly specific service offering just
as data mining and machine learning service. Unfortunately, so far, no definition for fuzzy system as service is available. This
paper identifies this opportunity and focus on developing a proposal for fuzzy system-as-a-service definition. To achieve this, the
proposal pursues three objectives: the complete description of cloud services for fuzzy systems using semantic technology, the
composition of services and the exploitation of the model in cloud platforms for integration with other services. As an illustrative
case, a real-world problem is addressed with the proposed specification.This work was supported by the Research
Projects P12-TIC-2958 and TIN2016-81113-R (Ministry of Economy,
Industry and Competitiveness - Government of Spain)",,'Atlantis Press',Fuzzy Systems-as-a-Service in Cloud Computing,10.2991/ijcis.d.190912.001,https://core.ac.uk/download/287763797.pdf,core
322514828,2019-08-01T07:00:00,"Production innovations are occurring faster than ever. Manufacturing workers thus need to frequently learn new methods and skills. In fast changing, largely uncertain production systems, manufacturers with the ability to comprehend workers\u27 behavior and assess their operation performance in near real-time will achieve better performance than peers. Action recognition can serve this purpose. Despite that human action recognition has been an active field of study in machine learning, limited work has been done for recognizing worker actions in performing manufacturing tasks that involve complex, intricate operations. Using data captured by one sensor or a single type of sensor to recognize those actions lacks reliability. The limitation can be surpassed by sensor fusion at data, feature, and decision levels. This paper presents a study that developed a multimodal sensor system and used sensor fusion methods to enhance the reliability of action recognition. One step in assembling a Bukito 3D printer, which composed of a sequence of 7 actions, was used to illustrate and assess the proposed method. Two wearable sensors namely Myo-armband captured both Inertial Measurement Unit (IMU) and electromyography (EMG) signals of assembly workers. Microsoft Kinect, a vision based sensor, simultaneously tracked predefined skeleton joints of them. The collected IMU, EMG, and skeleton data were respectively used to train five individual Convolutional Neural Network (CNN) models. Then, various fusion methods were implemented to integrate the prediction results of independent models to yield the final prediction. Reasons for achieving better performance using sensor fusion were identified from this study",,Scholars\u27 Mine,Action Recognition in Manufacturing Assembly using Multimodal Sensor Fusion,,https://core.ac.uk/download/322514828.pdf,core
265120533,2019-12-13,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",,HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",,,core
201123411,2019-03-01T00:00:00Z,"Abstract The Industry 4.0 era requires new quality management systems due to the ever increasing complexity of the global business environment and the advent of advanced digital technologies. This study presents new ideas for predictive quality management based on an extensive review of the literature on quality management and five real-world cases of predictive quality management based on new technologies. The results of the study indicate that advanced technology enabled predictive maintenance can be applied in various industries by leveraging big data analytics, smart sensors, artificial intelligence (AI), and platform construction. Such predictive quality management systems can become living ecosystems that can perform cause-effect analysis, big data monitoring and analytics, and effective decision-making in real time. This study proposes several practical implications for actual design and implementation of effective predictive quality management systems in the Industry 4.0 era. However, the living predictive quality management ecosystem should be the product of the organizational culture that nurtures collaborative efforts of all stakeholders, sharing of information, and co-creation of shared goals","[{'title': None, 'identifiers': ['2363-7021', 'issn:2363-7021']}]",SpringerOpen,The quality management ecosystem for predictive maintenance in the Industry 4.0 era,10.1186/s40887-019-0029-5,,core
231433057,2019-01-01T00:00:00,"The present material is written for students enrolled in actuarial master programs and practicing actuaries, who would like to gain a better understanding of insurance data analytics. It is built in three volumes, starting from the celebrated Generalized Linear Models, or GLMs and continuing with tree-based methods and neural networks. After an introductory chapter, this first volume starts with a recap’ of the basic statistical aspects of insurance data analytics and summarizes the state of the art using GLMs and their various extensions: GAMs, mixed models and credibility, and some nonlinear versions, or GNMs. Analytical tools from Extreme Value Theory are also presented to deal with tail events that arise in liability insurance or survival analysis. This book also goes beyond mean modeling, considering volatility modeling (double GLMs) and the general modeling of location, scale and shape parameters (GAMLSS). Throughout this book, we alternate between methodological aspects and numerical illustrations or case studies to demonstrate practical applications of the proposed techniques. The numerous examples cover all areas of insurance, not only property and casualty but also life and health, being based on real data sets from the industry or collected by regulators. The R statistical software has been found convenient to perform the analyses throughout this book. It is a free language and environment for statistical computing and graphics. In addition to our own R code, we have benefited from many R packages contributed by the members of the very active community of R-users. We provide the readers with information about the resources available in R throughout the text as well as in the closing section to each chapter. The open-source statistical software R is freely available from https://www.r-project.org/. The technical requirements to understand the material are kept at a reasonable level so that this text is meant for a broad readership. We refrain from proving all results but rather favor an intuitive approach with supportive numerical illustrations, providing the reader with relevant references where all justifications can be found, as well as more advanced material. These references are gathered in a dedicated section at the end of each chapter. The three authors are professors of actuarial mathematics at the universities of Brussels and Louvain-la-Neuve, Belgium. Together, they accumulate decades of teaching experience related to the topics treated in the three books, in Belgium and throughout Europe and Canada. They are also scientific directors at Detralytics, a consulting office based in Brussels. Within Detralytics as well as on behalf of actuarial associations, the authors have had the opportunity to teach the material contained in the three volumes of “Effective Statistical Learning Methods for Actuaries” to various audiences of practitioners. The feedback received from the participants to these short courses greatly helped to improve the exposition of the topic. Throughout their contacts with the industry, the authors also implemented these techniques in a variety of consulting and R&D projects. This makes the three volumes of “Effective Statistical Learning Methods for Actuaries” the ideal support for teaching students and CPD events for professionals",,'Springer Science and Business Media LLC',Effective Statistical Learning Methods for Actuaries I : GLMs and Extensions,10.1007/978-3-030-25820-7,,core
226752432,2019-01-01T00:00:00,"This paper reports the development of a manipulation system for electric wires, implemented by means of a commercial gripper installed on an industrial manipulator and equipped with cameras and suitably designed tactile sensors. The purpose of this system is the execution of wire insertion on commercial electromechanical components. The synergy between computer vision and tactile sensing is necessary because, in a real environment, the tight spaces very often prevent the possibility to use the vision system, also when the same task is performed by a human being. A novel technique to speed up the generation of training data sets for convolutional neural networks (CNNs) is proposed. Therefore, this technique is used to train a CNN in order to detect small objects (such as wire terminals). Moreover, aiming to prevent faults during the task and to interact with the environment safely, several machine learning approaches are used to produce an affordable output from the tactile sensor. The proposed approach shows how a cheap sensor embedded with suitable intelligence can provide information comparable to a more expensive force sensor. Note to Practitioners - This paper was motivated by the lack of commercial solution for the automatic cabling of switchgears. Existing approaches to this problem are in some way limited to specific large-scale products or simple layouts. This paper investigated a robust and flexible solution, based on the exploitation of multiple sensors and machine learning algorithms, for wire detection, grasping, and connection. The proposed approach is characterized by simple design and self-tuning capabilities, and it can be easily employed on a wide range of switchgear layouts thanks to the large workspace of the manipulator. Experimental results show that the proposed system is able to achieve a 95% success rate within a realistic admissible region. In the future research, we will integrate the proposed solution with an electromechanical component localization module and a terminal fastening system to evaluate the performance on the real production line",,'Institute of Electrical and Electronics Engineers (IEEE)',Integration of robotic vision and tactile sensing for wire-terminal insertion tasks,10.1109/TASE.2018.2847222,,core
442409320,2019-01-01T00:00:00,"Within the strongly regulated avionic engineering field, conventional graphical desktop hardware and software application programming interface (API) cannot be used because they do not conform to the avionic certification standards. We observe the need for better avionic graphical hardware, but system engineers lack system design tools related to graphical hardware. The endorsement of an optimal hardware architecture by estimating the performance of a graphical software, when a stable rendering engine does not yet exist, represents a major challenge. As proven by previous hardware emulation tools, there is also a potential for development cost reduction, by enabling developers to have a first estimation of the performance of its graphical engine early in the development cycle. In this paper, we propose to replace expensive development platforms by predictive software running on a desktop computer. More precisely, we present a system design tool that helps predict the rendering performance of graphical hardware based on the OpenGL Safety Critical API. First, we create nonparametric models of the underlying hardware, with machine learning, by analyzing the instantaneous frames per second (FPS) of the rendering of a synthetic 3D scene and by drawing multiple times with various characteristics that are typically found in synthetic vision applications. The number of characteristic combinations used during this supervised training phase is a subset of all possible combinations, but performance predictions can be arbitrarily extrapolated. To validate our models, we render an industrial scene with characteristic combinations not used during the training phase and we compare the predictions to those real values. We find a median prediction error of less than 4 FPS","[{'title': 'Scientific Programming', 'identifiers': ['1058-9244', 'issn:1058-9244', '1875-919x', 'issn:1875-919X']}]",'Hindawi Limited',Avionics Graphics Hardware Performance Prediction with Machine Learning,10.1155/2019/9195845,,core
297906931,2019,"Predictive Maintenance concerns the smart monitoring of machine to avoid possible future failures, since because it is better to intervene before the damage occurs, saving time and money. In this paper, a Predictive Maintenance methodology based on Machine learning approach is presented and it is applied to a real cutting machine, a woodworking machinery in a real industrial group, producing accurate estimations. This kind of strategy is important to deal with maintenance problems given the ever increasing need to reduce downtime and associated costs. The Predictive Maintenance methodology implemented allows dynamical decision rules that have to be considered for maintenance prediction using a combined approach on Azure Machine Learning Studio. The Three models (RF, GBM and XGBM) allowed the accurately predict machine down ever gripped bearing thanks to the pre-processing phase",,,An event based machine learning framework for predictive maintenance in industry 4.0,10.1115/DETC2019-97917,,core
227281710,2019,"The increasing availability of data and computing capacity drives optimization potential. In the industrial context, predictive maintenance is particularly promising and various algorithms are available for implementation. For the evaluation and selection of predictive maintenance algorithms, hitherto, statistical measures such as absolute and relative prediction errors are considered. However, algorithm selection from a purely statistical perspective may not necessarily lead to the optimal economic outcome as the two types of prediction errors (i.e., alpha error ignoring system failures versus beta error falsely indicating system failures) are negatively correlated, thus, cannot be jointly optimized and are associated with different costs. Therefore, we compare the prediction performance of three types of algorithms from an economic perspective, namely Artificial Neural Networks, Support Vector Machines, and Hotelling T² Control Charts. We show that the translation of statistical measures into a single cost-based objective function allows optimizing the individual algorithm parametrization as well as the un-ambiguous comparison among algorithms. In a real-life scenario of an industrial full-service provider we derive cost advantages of more than 17% compared to an algorithm selection based on purely statistical measures. This work contributes to the theoretical and practical knowledge on predictive maintenance algorithms and supports predictive maintenance investment decisions",,,Economic Perspective on Algorithm Selection for Predictive Maintenance,,,core
334862702,2019-09-24T00:00:00,"The availability of large image data sets has been a crucial factor in the
success of deep learning-based classification and detection methods. While data
sets for everyday objects are widely available, data for specific industrial
use-cases (e.g. identifying packaged products in a warehouse) remains scarce.
In such cases, the data sets have to be created from scratch, placing a crucial
bottleneck on the deployment of deep learning techniques in industrial
applications.
  We present work carried out in collaboration with a leading UK online
supermarket, with the aim of creating a computer vision system capable of
detecting and identifying unique supermarket products in a warehouse setting.
To this end, we demonstrate a framework for using synthetic data to create an
end-to-end deep learning pipeline, beginning with real-world objects and
culminating in a trained model.
  Our method is based on the generation of a synthetic dataset from 3D models
obtained by applying photogrammetry techniques to real-world objects. Using
100k synthetic images generated from 60 real images per class, an InceptionV3
convolutional neural network (CNN) was trained, which achieved classification
accuracy of 95.8% on a separately acquired test set of real supermarket product
images. The image generation process supports automatic pixel annotation. This
eliminates the prohibitively expensive manual annotation typically required for
detection tasks. Based on this readily available data, a one-stage RetinaNet
detector was trained on the synthetic, annotated images to produce a detector
that can accurately localize and classify the specimen products in real-time",,,"Synthetic dataset generation for object-to-model deep learning in
  industrial applications",,http://arxiv.org/abs/1909.10976,core
288328958,2019-01-01T00:00:00,"The industry is moving towards maintenance strategies that consider component health, which require extensive collection and analysis of data. Condition monitoring methods that require manual feature extraction and analysis, become infeasible on an industrial scale. Machine learning algorithms can be used to automatically detect and classify faults, however, obtaining sufficient data for training is required for deep learning and other data-driven classification approaches. Data from healthy machine operation is generally available in abundance, while data from representative fault- and operating conditions is limited. This limits both development and deployment of deep learning-based CM systems on an industrial scale. This paper addresses both the challenges of automated analysis and lack of training data. A deep learning classifier architecture utilizing 1-dimensional dilated convolutions is proposed. Dilation of the convolution kernel allows for analysis of raw vibration signals while simultaneously maintaining the receptive field of the classifier enough to capture temporal patterns. The proposed method performs classification in time domain on signal segments of 1 second or shorter. With knowledge of the bearing specification, artificial vibration signals with similar characteristics as an actual bearing fault can be created. In this work, generated fault signals are combined with healthy operational data to obtain training data for a deep classifier. Parameters of the vibration model is chosen as distributions rather than fixed values. By using a range parameters in the vibration model, the classifier learns to recognize temporal features from the training data that generalize to unseen data. The effectiveness of the proposed method is demonstrated by training classifiers on generated data and testing on real signals from faulty bearings at both low and high speed. One dataset containing seeded faults and three run-to-failure tests are used for the demonstration.publishedVersio",,,Simulation-driven Deep Classification of Bearing Faults from Raw Vibration Data,,https://core.ac.uk/download/288328958.pdf,core
195755544,2019-01-01T00:00:00,"Multi-criteria inventory classification groups inventory items into classes, each of which is managed by a specific re-order policy according to its priority. However, the tasks of inventory classification and control are not carried out jointly if the classification criteria and the classification approach are not robustly established from an inventory-cost perspective. Exhaustive simulations at the single item level of the inventory system would directly solve this issue by searching for the best re-order policy per item, thus achieving the subsequent optimal classification without resorting to any multi-criteria classification method. However, this would be very time-consuming in real settings, where a large number of items need to be managed simultaneously. In this article, a reduction in simulation effort is achieved by extracting from the population of items a sample on which to perform an exhaustive search of best re-order policies per item; the lowest cost classification of in-sample items is, therefore, achieved. Then, in line with the increasing need for ICT tools in the production management of Industry 4.0 systems, supervised classifiers from the machine learning research field (i.e. support vector machines with a Gaussian kernel and deep neural networks) are trained on these in-sample items to learn to classify the out-of-sample items solely based on the values they show on the features (i.e. classification criteria). The inventory system adopted here is suitable for intermittent demands, but it may also suit non-intermittent demands, thus providing great flexibility. The experimental analysis of two large datasets showed an excellent accuracy, which suggests that machine learning classifiers could be implemented in advanced inventory classification systems",,'Informa UK Limited',Machine learning for multi-criteria inventory classification applied to intermittent demand,10.1080/09537287.2018.1525506,https://core.ac.uk/download/195755544.pdf,core
334862068,2019-09-23T00:00:00,"Portfolio optimization emerged with the seminal paper of Markowitz (1952).
The original mean-variance framework is appealing because it is very efficient
from a computational point of view. However, it also has one well-established
failing since it can lead to portfolios that are not optimal from a financial
point of view. Nevertheless, very few models have succeeded in providing a real
alternative solution to the Markowitz model. The main reason lies in the fact
that most academic portfolio optimization models are intractable in real life
although they present solid theoretical properties. By intractable we mean that
they can be implemented for an investment universe with a small number of
assets using a lot of computational resources and skills, but they are unable
to manage a universe with dozens or hundreds of assets. However, the emergence
and the rapid development of robo-advisors means that we need to rethink
portfolio optimization and go beyond the traditional mean-variance optimization
approach. Another industry has faced similar issues concerning large-scale
optimization problems. Machine learning has long been associated with linear
and logistic regression models. Again, the reason was the inability of
optimization algorithms to solve high-dimensional industrial problems.
Nevertheless, the end of the 1990s marked an important turning point with the
development and the rediscovery of several methods that have since produced
impressive results. The goal of this paper is to show how portfolio allocation
can benefit from the development of these large-scale optimization algorithms.
Not all of these algorithms are useful in our case, but four of them are
essential when solving complex portfolio optimization problems. These four
algorithms are the coordinate descent, the alternating direction method of
multipliers, the proximal gradient method and the Dykstra's algorithm.Comment: 66 pages, 6 figure",,,Machine Learning Optimization Algorithms & Portfolio Allocation,,http://arxiv.org/abs/1909.10233,core
289201335,2019-10-28T19:31:26Z,"Scheduling is an important problem in artificial intelligence and operations research. In production processes, it deals with the problem of allocation of resources to different tasks with the goal of optimizing one or more objectives. Job shop scheduling is a classic and very common scheduling problem. In the real world, shop environments dynamically change due to events such as the arrival of new jobs and machine breakdown. In such manufacturing environments, uncertainty in shop parameters is typical. It is of vital importance to develop methods for effective scheduling in such practical settings.

Scheduling using heuristics like dispatching rules is very popular and suitable for such environments due to their low computational cost and ease of implementation. For a dynamic manufacturing environment with varying shop scenarios, using a universal dispatching rule is not very effective. But manual development of effective dispatching rules is difficult, time consuming and requires expertise. Genetic programming is an evolutionary approach which is suitable for automatically designing effective dispatching rules. Since the genetic programming approach searches in the space of heuristics (dispatching rules) instead of building up a schedule, it is considered a hyper-heuristic approach.

Genetic programming like many other evolutionary approaches is computationally expensive. Therefore, it is of vital importance to present the genetic programming based hyper-heuristic (GPHH) system with scheduling problem instances which capture the complex shop scenarios capturing the difficulty in scheduling. Active learning is a related concept from machine learning which concerns with effective sampling of those training instances to promote the accuracy of the learned model.

The overall goal of this thesis is to develop effective and efficient genetic programming based hyper-heuristic approaches using active learning techniques for dynamic job shop scheduling problems with one or more objectives.

This thesis develops new representations for genetic programming enabling it to incorporate the uncertainty information about processing times of the jobs. Furthermore, a cooperative co-evolutionary approach is developed for GPHH which evolves a pair of dispatching rules for bottleneck and non-bottleneck machines in the dynamic environment with uncertainty in processing times arising due to varying machine characteristics. The results show that the new representations and training approaches are able to significantly improve the performance of evolved dispatching rules.

This thesis develops a new GPHH framework in order to incorporate active learning methods toward sampling DJSS instances which promote the evolution of more effective rules. Using this framework, two new active sampling methods were developed to identify those scheduling problem instances which promoted evolution of effective dispatching rules. The results show the advantages of using active learning methods for scheduling under the purview of GPHH.

This thesis investigates a coarse-grained model of parallel evolutionary approach for multi-objective dynamic job shop scheduling problems using GPHH. The outcome of the investigation was utilized to extend the coarse-grained model and incorporate an active sampling heuristic toward identifying those scheduling problem instances which capture the conflict between the objectives. The results show significant improvement in the quality of the evolved Pareto set of dispatching rules.

Through this thesis, the following contributions have been made. (1) New representations and training approaches for GPHH  to incorporate uncertainty information about processing times of jobs into dispatching rules to make them more effective in a practical shop environment. (2) A new GPHH framework which enables active sampling of scheduling problem instances toward evolving dispatching rules effective across complex shop scenarios.  (3) A new active sampling heuristic based on a coarse-grained model of parallel evolutionary approach for GPHH for multi-objective scheduling problems",,Victoria University of Wellington,Active Learning Methods for Dynamic Job Shop Scheduling using Genetic Programming under Uncertain Environment,,,core
296903618,2019-08-16T14:32:12,"Orientador: Alberto Luiz SerpaDissertação (mestrado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: O monitoramento de bombas centrífugas é essencial para o adequado funcionamento de diversas aplicações industriais. A confiabilidade dos sistemas de elevação artificial de petróleo depende substancialmente do desempenho das bombas centrífugas submersíveis (BCS). Essas bombas podem operar sujeitas a condições severas, como escoamento viscoso e multifásico. Nos últimos anos, as tecnologias de monitoramento em tempo real baseadas em algoritmos de aprendizado automático ganharam força devido à capacidade de aproveitar os dados históricos do maquinário para previsões de eventos futuros. Muitos tipos de falhas incipientes estão relacionadas a mudanças no fluido ou maquinário externo associados ao processo. O presente trabalho propõe a utilização de Árvores de Decisão (AD) e uma estrutura particular de Árvores de Decisão nomeada aqui como Cadeia de Árvores de Decisão (CAD) para a detecção e classificação de falhas. A metodologia projetada na CAD pretende melhorar a distribuição na classificação mantendo relações simplificadas entre as variáveis monitoradas e as falhas. Falhas operacionais são simuladas, monitoradas e rotuladas em uma bomba centrífuga multiestágio em dois diferente cenários, apenas com óleo e com uma mistura de petróleo e nitrogênio, considerando os casos de fechamento abrupto da válvula choke (CV), diminuição da pressão de entrada (DIP), aumento da viscosidade do fluido (VI) para o caso monofásico, e no caso de escoamento bifásico incluindo o aumento da fração mássica de gás (GI). Experimentos foram realizados para rotular e selecionar as variáveis monitoradas que melhor representam as falhas. As variáveis e suas variações no tempo foram usados como entradas para os classificadores supervisionados. Os resultados obtidos usando AD e CAD são comparados em termos da exatidão e do erro na classificação, sendo a CAD aquela que obteve melhores resultados na
determinação do tipo de falha. Os algoritmos implementados detectaram entre 80 e 90% das falhas corretamente. A classificação foi avaliada através da matriz de confusão, obtendo-se acertos entre 75 e 82%. As árvores de decisão testadas possuem potencial de aplicação na detecção de falhas em BCS devido a sua capacidade de detecção, classificação e interpretabilidadeAbstract: The monitoring of centrifugal pumps is essential for the suitable operation of several industrial applications. The reliability of the petroleum artificial lifting systems depends substantially on the performance of electrical submersible centrifugal pumps (ESP). These pumps can operate subjected to severe operating conditions like viscous and multi-phase flow. In recent years, real-time technologies based on machine learning algorithms have gained visibility due to the capability to take advantage of the machinery historical data for future event predictions. Many kinds of incipient faults are related to changes in the fluid or external machinery associated with the process. This document proposes the use of
decision trees (AD) and a particular decision tree structure named here as a decision tree chain (CAD) for the detection and classification of failures. The methodology proposed in the CAD intends to improve the classification distribution maintaining simplified relationships between monitored variables and failures. Operational failures are simulated, monitored and labeled in a multistage centrifugal pump operating in two different scenarios, in one-phase petroleum regime and with a mixture of petroleum and nitrogen, considering as faults the abruptly close of choke valve (CV), the decreasing of input pressure (DIP), the increasing of fluid viscosity (VI) one-phase flow, and for two-phase case including the increasing of gas flow rate (GI). Experiments were performed to label and to select the features that strongly represent the faults. The features and its time variations were used as inputs to the supervised classifiers. The results obtained using AD and CAD are compared in terms of accuracy and misclassification error getting better results in determining the fault kind for the CAD structure. The implemented algorithms correctly detected 80 to 90% of the faults. The classification was measured using the confusion matrix metrics, obtaining an accuracy between 75 and 82%. The decision trees evaluated have potential to detect faults in the BCS applications due to its detection, classification and interpretability capabilityMestradoMecanica dos Sólidos e Projeto MecanicoMestre em Engenharia Mecânic",,[s.n.],Monitoring of operational failures in multistage centrífugal pumps,,,core
301380248,2019-07-12T23:23:23,"In Big Data contexts, many batch and streaming oriented technologies have emerged to deal with the high valuable sources of events, such as Internet of Things (IoT) platforms, the Web, several types of databases, among others. The huge amount of heterogeneous data being constantly generated by a world of interconnected things and the need for (semi)-automated decision-making processes through Complex Event Processing (CEP) and Machine Learning (ML) have raised the need for innovative architectures capable of processing events in a streamlined, scalable, analytical, and integrated way. This paper presents the Intelligent Event Broker, a CEP system built upon flexible and scalable Big Data techniques and technologies, highlighting its system architecture, software packages, and classes. A demonstration case in Bosch’s Industry 4.0 context is presented, detailing how the system can be used to manage and improve the quality of the manufacturing process, showing its usefulness for solving real-world event-oriented problems",,AIS Electronic Library (AISeL),Intelligent Event Broker: A Complex Event Processing System in Big Data Contexts,,,core
222699469,2019-05-31T00:00:00,"Free text and hand-written reports are losing ground to digitization fast, however many hours of effort are still lost across the industry to the manual creation and analysis of these data types. Work orders in particular contain valuable information from failure rates to asset health, but at the same time present operators with such analytical difficulties and lack of structure that many are missing out on the value completely. This research challenges the current mainstream practice of manual work order analysis by presenting a methodology fit for today’s context of efficiency and digitization. A prototype text mining software for work order analysis was developed and tested in a user-oriented approach in cooperation with industrial partners. The final prototype combines classical machine learning methods, such as hierarchical clustering, with the operator’s expert knowledge obtained via an active learning approach. A novel distance metric in this context was adapted from information-theoretical research to improve clustering performance. Using the prototype tool in a case study with real work order data, analytical effort for certain datasets was reduced by 90% - from two working weeks to a day. In addition, the active learning framework resulted in an approach that end users described as “practical” and “intuitive” during testing. An in-depth review was also conducted regarding the uncertainty of the results – a key factor for implementation in a decision-making context. The outcomes of this work showcase the potential of machine learning to drive the digitization of not only new installations, but also older assets, where as a result the large amount of unstructured historical data becomes an advantage rather than a hindrance. User testing results encourage a wider uptake of machine learning solutions in the industry, and particularly a shift towards more accessible in-house analytical capabilities",,,Work orders - value from structureless text in the era of digitisation,,https://core.ac.uk/download/222699469.pdf,core
228203281,2019-01-01T08:00:00,"The Industry 4.0 era requires new quality management systems due to the ever increasing complexity of the global business environment and the advent of advanced digital technologies. This study presents new ideas for predictive quality management based on an extensive review of the literature on quality management and five realworld cases of predictive quality management based on new technologies. The results of the study indicate that advanced technology enabled predictive maintenance can be applied in various industries by leveraging big data analytics, smart sensors, artificial intelligence (AI), and platform construction. Such predictive quality management systems can become living ecosystems that can perform cause-effect analysis, big data monitoring and analytics, and effective decision-making in real time. This study proposes several practical implications for actual design and implementation of effective predictive quality management systems in the Industry 4.0 era. However, the living predictive quality management ecosystem should be the product of the organizational culture that nurtures collaborative efforts of all stakeholders, sharing of information, and co-creation of shared goals",,DigitalCommons@University of Nebraska - Lincoln,"The quality management ecosystem for
predictive maintenance in the Industry
4.0 era",,https://core.ac.uk/download/228203281.pdf,core
159235367,2018-01-01T08:00:00,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS",'Elsevier BV',Data-driven model-free slip control of anti-lock braking systems using reinforcement Q-learning,10.1016/j.neucom.2017.08.036,,,core
215548348,2018-07-01T07:00:00,"Aerosol jet printing (AJP)—a direct-write, additive manufacturing technique—has emerged as the process of choice particularly for the fabrication of flexible and hybrid electronics. AJP has paved the way for high-resolution device fabrication with high placement accuracy, edge definition, and adhesion. In addition, AJP accommodates a broad range of ink viscosity, and allows for printing on non-planer surfaces. Despite the unique advantages and host of strategic applications, AJP is a highly unstable and complex process, prone to gradual drifts in machine behavior and deposited material. Hence, real-time monitoring and control of AJP process is a burgeoning need. In pursuit of this goal, the objectives of the work are, as follows: (i) In situ image acquisition from the traces/lines of printed electronic devices right after deposition. To realize this objective, the AJP experimental setup was instrumented with a high-resolution charge-coupled device (CCD) camera, mounted on a variable-magnification lens (in addition to the standard imaging system, already installed on the AJ printer). (ii) In situ image processing and quantification of the trace morphology. In this regard, several customized image processing algorithms were devised to quantify/extract various aspects of the trace morphology from online images. In addition, based on the concept of shape-from-shading (SfS), several other algorithms were introduced, allowing for not only reconstruction of the 3D profile of the AJ-printed electronic traces, but also quantification of 3D morphology traits, such as thickness, cross-sectional area, and surface roughness, among others. (iii) Development of a supervised multiple-input, single-output (MISO) machine learning model—based on sparse representation for classification (SRC)—with the aim to estimate the device functional properties (e.g., resistance) in near real-time with an accuracy of ≥ 90%. (iv) Forwarding a computational fluid dynamics (CFD) model to explain the underlying aerodynamic phenomena behind aerosol transport and deposition in AJP process, observed experimentally.
Overall, this doctoral dissertation paves the way for: (i) implementation of physics-based real-time monitoring and control of AJP process toward conformal material deposition and device fabrication; and (ii) optimal design of direct-write components, such as nozzles, deposition heads, virtual impactors, atomizers, etc",The Open Repository @ Binghamton (The ORB),"COMPUTATIONAL FLUID DYNAMICS MODELING AND IN SITU PHYSICSBASED MONITORING OF AEROSOL JET PRINTING TOWARD FUNCTIONAL ASSURANCE OF ADDITIVELY-MANUFACTURED, FLEXIBLE AND HYBRID ELECTRONICS",,https://core.ac.uk/download/215548348.pdf,,core
229252727,2018-08-01T07:00:00,"Drilling boreholes through hydrogen sulfide (H₂S) bearing formations such as Umm Er Radhuma and Tayarat Formations poses a critical challenge for the oil and gas industry in southern Iraq. In this era of increased concern for personal safety and environmental factors, the industry needs additional tools and methods for handling this deadly and corrosive gas. This paper describes how actual formation field data is entered into unsupervised learning software to train a H2S monitoring simulator to improve the drilling operators\u27 ability to detect H2S, analyse and then adjust mud pH to neutralize free H2S present in drilling fluids. The H2S concentration measurements for five drilled wells in the Umm Er Radhuma and Tayarat fields in the South of Iraq used in a previous study are extended to include data from an additional 12 wells with 22 H2S events. This paper describes a real-time H2S simulator which consists of a computer interface kit implementing MATLAB code. The kit is provided with three LEDs (green, yellow, and red) representing different alert levels. The system monitors changes in H2S levels and triggers alarms depending on the levels. The setup of the alert levels for the simulator are adopted from both Health Safety Environment (HSE) policy and unsupervised vector quantization via Fuzzy Adaptive Resonance Theory (ART) neural network category ranges. Results show that using Fuzzy ART to train the H2S simulator alert levels based on formation data leads to a smarter and more sophisticated alert system compared to the typical HSE set of alerts. Specifically, the Fuzzy ART derived alerts enable earlier detection of H2S events, faster response to changing H2S levels, and it computes workers\u27 exposure over time to H2S, to prevent excessive accumulation of H2S in the respiratory system. The system demonstrates a smarter and more robust method for reducing risks to drilling personnel, rig equipment, and the environment while drilling in areas with H2S hazards",Scholars\u27 Mine,Learning from Experience: Real-Time H₂S Monitoring System using Fuzzy ART Unsupervised Learning,,,,core
301378559,2018-11-28T08:00:00,"The Digital Transformation alters business models in all fields of application, but not all industries transform at the same speed. While recent innovations in smart products, big data, and machine learning have profoundly transformed business models in the high-tech sector, less digitalized industries—like agriculture—have only begun to capitalize on these technologies. Inspired by predictive maintenance strategies for industrial equipment, the purpose of this paper is to design, implement, and evaluate a predictive maintenance method for agricultural machines that predicts future defects of a machine’s components, based on a data-driven analysis of service records. An evaluation with 3,407 real-world service records proves that the method predicts damaged parts with a mean accuracy of 86.34%. The artifact is an exaptation of previous design knowledge from high-tech industries to agriculture—a sector in which machines move through rough territory and adverse weather conditions, are utilized extensively for short periods, and do not provide sensor data to service providers. Deployed on a platform, the prediction method enables co-creating a predictive maintenance service that helps farmers to avoid resources shortages during harvest seasons, while service providers can plan and conduct maintenance service preemptively and with increased efficiency",AIS Electronic Library (AISeL),Designing Predictive Maintenance for Agricultural Machines,,https://core.ac.uk/download/301378559.pdf,,core
478159177,2018-01-01T00:00:00,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology’s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders—consumers, agents and associations, government and regulatory authorities, and complementary industries—and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers’ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers’ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",'MDPI AG',"A systematic review of smart real estate technology: drivers of, and barriers to, the use of digital disruptive technologies and online platforms",10.3390/su10093142,,,core
186306043,2018-12-19T00:00:00,"We present a low barrier magnet based compact hardware unit for analog
stochastic neurons and demonstrate its use as a building-block for neuromorphic
hardware. By coupling circular magnetic tunnel junctions (MTJs) with a CMOS
based analog buffer, we show that these units can act as leaky-integrate-and
fire (LIF) neurons, a model of biological neural networks particularly suited
for temporal inferencing and pattern recognition. We demonstrate examples of
temporal sequence learning, processing, and prediction tasks in real time, as a
proof of concept demonstration of scalable and adaptive signal-processors.
Efficient non von-Neumann hardware implementation of such processors can open
up a pathway for integration of hardware based cognition in a wide variety of
emerging systems such as IoT, industrial controls, bio- and photo-sensors, and
Unmanned Autonomous Vehicles.Comment: 4 pages, 4 figures, under revie",,Analog Signal Processing Using Stochastic Magnets,,http://arxiv.org/abs/1812.08273,,core
195314069,2018-02-27T00:00:00,"Real-time advertising allows advertisers to bid for each impression for a visiting user. To optimize specific goals such as maximizing revenue and return on investment (ROI) led by ad placements, advertisers not only need to estimate the relevance between the ads and user's interests, but most importantly require a strategic response with respect to other advertisers bidding in the market. In this paper, we formulate bidding optimization with multi-agent reinforcement learning. To deal with a large number of advertisers, we propose a clustering method and assign each cluster with a strategic bidding agent. A practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed and implemented to balance the tradeoff between the competition and cooperation among advertisers. The empirical study on our industry-scaled real-world data has demonstrated the effectiveness of our methods. Our results show cluster-based bidding would largely outperform single-agent and bandit approaches, and the coordinated bidding achieves better overall objectives than purely self-interested bidding agents",'Center for Open Science',Real-time bidding with multi-agent reinforcement learning in display advertising,,https://core.ac.uk/download/195314069.pdf,,core
212720231,2018-01-01T08:00:00,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out",'Sociological Research Online',Ensemble Machine Learning Systems for the Estimation of Steel Quality Control,,https://core.ac.uk/download/212720231.pdf,,core
322435195,2018-08-01T00:00:00,"Standard security systems are widely implemented in the industry. These systems consume considerable computational resources. Devices in the Internet of Things [IoT] are very limited with processing capacity, memory and storage. Therefore, existing security systems are not applicable for IoT. To cope with it, we propose downsizing of existing security processes. In this chapter, we describe three areas, where we reduce the required storage space and processing power. The first is the classification process required for ongoing anomaly detection, whereby values accepted or generated by a sensor are classified as valid or abnormal. We collect historic data and analyze it using machine learning techniques to draw a contour, where all streaming values are expected to fall within the contour space. Hence, the detailed collected data from the sensors are no longer required for real-time anomaly detection. The second area involves the implementation of the Random Forest algorithm to apply distributed and parallel processing for anomaly discovery. The third area is downsizing cryptography calculations, to fit IoT limitations without compromising security. For each area, we present experimental results supporting our approach and implementation",'IntechOpen',An Adaptive Lightweight Security Framework Suited for IoT,10.5772/intechopen.73712,https://core.ac.uk/download/322435195.pdf,,core
162435661,2018,"This work aims to show how to manage heterogeneous information and data coming from real datasets that collect physical, biological, and sensory values. As productive companies-public or private, large or small-need increasing profitabilitywith costs reduction, discovering appropriateways to exploit data that are continuously recorded and made available can be the right choice to achieve these goals. The agricultural field is only apparently refractory to the digital technology and the ""smart farm"" model is increasingly widespread by exploiting the Internet of Things (IoT) paradigm applied to environmental and historical information through time-series. The focus of this study is the design and deployment of practical tasks, ranging from crop harvest forecasting to missing or wrong sensors data reconstruction, exploiting and comparing various machine learning techniques to suggest toward which direction to employ efforts and investments. The results show how there are ample margins for innovation while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agriculture industrial business, investing not only in technology, but also in the knowledge and in skilled workforce required to take the best out of it",'MDPI AG',Machine learning applications on agricultural datasets for smart farm enhancement,10.3390/machines6030038,,,core
188681324,2018,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data?publishedVersionPublished by Taylor & Francis. Made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0",Taylor & Francis,Digitalization of the power business: How to make this work?,10.1201/9781351174664,,,core
478699432,2018-01-01T00:00:00,"In the phase of industry digitalization, data are collected from many sensors and signal processing techniques play a crucial role. Data preprocessing is a fundamental step in the analysis of measurements, and a first step before applying machine learning. To reduce the influence of distortions from signals, selective digital filtering is applied to minimize or remove unwanted components. Standard software and hardware digital filtering algorithms introduce a delay, which has to be compensated for to avoid destroying signal associations. The delay from filtering becomes more crucial when the analysis involves measurements from multiple sensors, therefore in this paper we provide an overview and comparison of existing digital filtering methods with an application based on real-life marine examples. In addition, the design of special-purpose filters is a complex process and for preprocessing data from many sources, the application of digital filtering in the time domain can have a high numerical cost. For this reason we describe discrete Fourier transformation digital filtering as a tool for efficient sensor data preprocessing, which does not introduce a time delay and has low numerical cost. The discrete Fourier transformation digital filtering has a simpler implementation and does not require expert-level filter design knowledge, which is beneficial for practitioners from various disciplines. Finally, we exemplify and show the application of the methods on real signals from marine systems",'Academy of Traumatology',Comparison of Delayless Digital Filtering Algorithms and Their Application to Multi-Sensor Signal Processing,,,,core
132529050,2018-01-01T00:00:00,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.Peer ReviewedPostprint (author's final draft",'Elsevier BV',Incorporating negative information to process discovery of complex systems,10.1016/j.ins.2017.09.027,https://core.ac.uk/download/132529050.pdf,"[{'title': 'Information Sciences', 'identifiers': ['issn:0020-0255', '0020-0255']}]",core
235627968,2018-11-26T23:18:30,"FORCEDECKS WORKSHOP - COMMERCIALISATION & FUNDING by Philip Graham Smith
This workshop aims to improve your ability to attract and create commercialisation and funding opportunities. This workshop will challenge delegates to identify their real areas of expertise and consider ways in which they can attract and create funding opportunities. The aim is to help academics of all ages to focus their expertise, to manage their time more effectively and to explore new avenues to make their careers more rewarding, fulfilling and hopefully less stressful. Having been a former Head of Department and Associate Head of School (Business & Engagement), Dr Graham-Smith has been in the trenches and acknowledges the increasing demands and pressures of working in academia. The workshop will help delegates to strip back the various aspects of their roles, and to examine ways in which their teaching, research, consultancy and funding expectations can be managed successfully. Phil will be reflective on his own career and share experiences of working in academia, professional sport and private industry.
SPRINZ WORKSHOP - ORAL PRESENTATION FEEDBACK by Joshua McGeown, Gillian Weir, Professor Mike McGuigan and SPRINZ PhD students
This workshop aims to help you engaging your audience during your ISBS presentation. This workshop aims to provide delegates with tips and feedback as to how best present their research for the ISBS 2018 congress. This interactive workshop will help delegates to learn how to distill and communicate complex ideas, structure your narrative and how to best visualize your data. Participants are encouraged to bring their ISBS presentations to practice and receive constructive feedback.
NZ HERALD WORKSHOP - HOW TO WORK WITH THE MEDIA TO AMPLIFY YOUR WORK by Dylan Cleaver, Editor at large with the New Zealand Herald
This workshop will help delegates be able to interact with media to be able to amplify their work.
Never before has there been so much attention given to the injury toll in elite sport, with the spotlight firmly centered on head injuries and the potential for long-term cognitive damage to those afflicted. With so much important research being done in the field of sports injury, it is important to know how to work with the media to highlight it. This workshop aims to give a brief overview of the fast-changing modern media landscape. It will offer advice as to how to establish contacts in the media and how to use those contacts wisely. It will demonstrate how to get your key messages across using simple language, without dumbing down the issue. It will traverse ethical issues and, finally, what to do when the message goes wrong. Attendees will use the lessons learnt from the examples, to workshop during the session how they can work with media to amplify their work.
WORKSHOP - JAPAN COLLABORATION by Sayumi Iwamoto, Erika Ikeda, Ryu Nagahara, and Aaron Uthoff
Do you want to share your experience with other researchers who are keen to conduct international research collaboration? The workshop will share experiences and key tips to enable successfully working together. “There are many positives with working with Japanese researchers, but the one that stands out the most to me is their willingness to share knowledge and lend a helping hand.” (Aaron Uthoff)
AUT ENGINEERING WORKSHOP - AI CHALLENGES by Boris Bacic & Russell Pears from Auckland University of Technology Engineering School
This workshop will help you to consider pushing your boundaries of biomechanics and sport science by embracing artificial intelligence (Dr Boris Bačić and Assoc. Prof Russel Pears, Auckland University of Technology, NZ). Pushing the boundaries of biomechanics and sport science also means embracing artificial intelligence (AI) to advance and augment ways in which sport is coached, played, promoted, broadcasted and commercialised. Technologies capable of capturing human motion enable the advancement of research and can create strategic differences in elite sport, which is reflected by their increasing presence in the growing market of sport gadgets, exergames and rehabilitation technologies. Data-driven machine-learning AI approaches have the potential to provide insights from data, find patterns in specific contexts, generate knowledge, validate expert’s common-sense rules, and offload support decisions and automate cognitive activities. The workshop will provide a theoretical introduction and a set of analytical and model-designing visual tools for getting started. For those interested in Matlab or other languages, code samples will be provided. The participants will be able to use free open source software alternatives as part of hands-on exercises in a supervised lab.
SPRINGER WORKSHOP - WHAT MAKES A SUCCESSFUL PAPER – AN EDITOR’S PERSPECTIVE by Steve McMillan from Springer’s Sports Medicine journal
This workshop will help delegates increase their likelihood of success in publishing in journals such as Sports Medicine. From a compelling cover letter to a concise conclusion, Sports Medicine’s Co-Editor in Chief, Steve McMillan, will provide an editor’s perspective on what makes a successful paper. Sports Medicine receives over 600 submissions a year and can publish only a quarter of these … How do the editors decide which manuscripts to send to peer review? Which manuscripts survive peer review? What details are essential to enable readers to best understand your research and allow for potential replication? What information is required from an ethical perspective? Why do word counts matter anyway?! This interactive workshop will guide you on how to produce an impressive manuscript and increase your chances of getting published in a reputable journal.
NORAXON WORKSHOP - ELECTROMYOGRAPHY IN SPORTS PERFORMANCE by Coleman Bessert and Erin Feser from NORAXON.
Noraxon USA (www.noraxon.com) will be hosting a workshop on electromyography (EMG) use in sports performance settings. “You will be able to develop a better understanding of how EMG fits into an athlete monitoring program or research investigation by learning what can, and cannot, be determined with EMG data and reporting. Participants will see hands-on use of precision EMG systems and biomechanics analysis software with practical, sport-specific examples.” Erin Feser , Director of Education for Noraxon USA",NMU Commons,ISBS 2018 AUCKLAND CONFERENCE WORKSHOPS PROGRAMME,,https://core.ac.uk/download/235627968.pdf,,core
286600336,2018-01-01T00:00:00,": Deep learning is the most recent approach to achieve artificial intelligence. Especially neural networks are used for solving many human problems - from repetitive operations to intelligent recognizing in image, sound and text processing. They are used in medicine, car industry, game industry and robotics. Business companies also try to find the way of exploitation of the latest technology despite the fact that it is the long way to the point where machines will be capable to replace the human intelligence. Authors of this paper explore possibilities of semi-supervised learning application in accounting. One of the latest deep learning algorithm is successfully used to reconstruct the journal entry key columns. The model was trained and tested on a real-world dataset so it could become base for developing the wide pallet of accounting and audit applications - as anomaly detection module of Enterprise Resource Planning (ERP) software or as a standalone application",,Journal entries with deep learning model,,,,core
162988217,2018-10-26T00:00:00,"We propose and study a novel artificial neural network framework, which allows us to model surgical interventions on a physical system. Our approach was developed to predict power flows in power transmission grids, in which high voltage lines are disconnected and reconnected with one-another from time to time, either accidentally or willfully. However, we anticipate a broader applicability. For several exemplary cases, we illustrate by simulation that our methodology permits learning from empirical data to predict the effect of a subset of interventions (ele-mentary interventions) and then generalize to combinations of interventions never seen during training. We verify this property mathematically in the additive perturbation case. In terms of transfer learning, this is equivalent to training on data from a few source domains then, with a zero-shot learning, generalizing to new target domains (super-generalization). Our architecture bears resemblance with the successful ResNets, with the simple modification that interventions are encoded as an addition of units in the neural network. For applications to real historical data, from the French high voltage power transmission company RTE, we evaluate the viability of this technique to rapidly assess curative actions that human operators take in emergency situations. Integrated in an overall planning and control system, methods deriving from our approach could allow Transmission System Operators (TSO) to assess in real time many more alternative actions, reaching a better exploration-exploitation tradeoff, compared to presently deployed physical system simulator. 1 Background and motivations In this paper, we are interested in speeding up the computation of power flows in power transmission grids using artificial neural networks, to emulate slower physical simulators. Key to our approach is the possibility of simulating the effect of actions on the grid topology. Such neural networks may then be used as part of an overall computer-assisted decision process in which human operators (dispatchers) ensure that the power grid is operated in security at all times, namely that the currents flowing in all lines are below certain thresholds (line thermal limits). We describe our application setting for concreteness, but anticipate a broader applicability of the techniques developed in this paper in various domains of physics, chemistry, manufacturing, biomedicine and others, in which some actions can be combined with each other, but running extensive simulations for each possible combination of such actions is computationally untractable. Electric power generated in production nodes (such as power plants) is transmitted towards consumption nodes in a power grid. The power lines enable this transmission through substations interconnecting them. Each pattern of connections is referred to as a grid topology. This topology is * Benjamin Donnot corresponding authors: benjamin.donnot@inria.com 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada",HAL CCSD,Latent Surgical Interventions in Residual Neural Networks,,https://core.ac.uk/download/162988217.pdf,,core
299950683,2018-01-01T00:00:00,"Additive manufacturing (AM) processes are capable of producing objects of high precision. There are many AM methods and Fused Deposition Modelling (FDM) will be the method that will be focused on in this report. The precision of printing is dependent on many parameters that have to be controlled well during printing. These parameters are subjected to errors happening during the print and may deviate from their original values as a result. Typically, a printing process will take many hours and will unlikely be under manual observation. As a result, these errors go undetected and the printer ultimately produces a defective product, resulting in material and time wastage. A solution to reducing material and time wastage is to develop a process monitoring system which is capable of detecting errors and taking counter measures in real time. 
 In this project, the concept of machine learning and visual detection is used to train neural networks to identify and classify errors that happen during a print job. Data sets used to train the neural networks are generated using image editing software. These data sets consist of different errors that are digitally generated. Three data sets representing different errors of stringing, under extrusion and misalignment are generated. They are subsequently used to train the neural networks and results show that the training with the stringing data set is effective. The neural networks are able to correctly detect and identify the type of error that occur on a layer of material. This project proves that visual detection can indeed be effective in detecting errors that may occur during a print job. This will greatly reduce material and time wastage as well as eliminate the need for manual observation of a print process.Bachelor of Engineering (Aerospace Engineering",,Design of process monitoring system for extrusion based 3D printers,,,,core
211981919,2018-03-21T10:45:25,"The design of new streaming systems is becoming a major area of research to deploy services targeted in the Internet-of-Things (IoT) era. In this context, the new High Efficiency Video Coding (HEVC) standard provides high efficiency and scalability of quality at the cost of increased computational complexity for edge nodes, which is a new challenge for the design of IoT systems. The usage of hardware acceleration in conjunction with general-purpose cores in Multiprocessor Systems-on-Chip (MPSoCs) is a promising solution to create heterogeneous computing systems to manage the complexity of real-time streaming for high-end IoT systems, achieving higher throughput and power efficiency when compared to conventional processors alone. Furthermore, Machine Learning (ML) provides a promising solution to efficiently use this next-generation of heterogeneous MPSoC designs that the EDA industry is developing by dynamically optimizing system performance under diverse requirements such as frame resolution, search area, operating frequency and stream allocation. In this work, we propose an ML-based approach for stream allocation and Dynamic Voltage and Frequency Scaling (DVFS) management on a heterogeneous MPSoC composed of ARM cores and FPGA fabric containing hardware accelerators for the motion estimation of HEVC encoding. Our experiments on a Zynq7000 SoC outline 20% higher throughput when compared to the state-of-the-art streaming systems for next-generation IoT devices",,A Machine Learning-Based Strategy for Efficient Resource Management of Video Encoding on Heterogeneous MPSoCs,,https://core.ac.uk/download/211981919.pdf,,core
217586045,2018-01-01T00:00:00,"Embedded vision is a disruptive new technology in the vision industry. It is a revolutionary concept with far reaching implications, and it is opening up new applications and shaping the future of entire industries. It is applied in self-driving cars, autonomous vehicles in agriculture, digital dermascopes that help specialists make more accurate diagnoses, among many other unique and cutting-edge applications. The design of such systems gives rise to new challenges for embedded Software developers. Embedded vision applications are characterized by stringent performance constraints to guarantee real-time behaviours and, at the same time, energy constraints to save battery on the mobile platforms. In this paper, we address such challenges by proposing an overall view of the problem and by analysing current solutions. We present our last results on embedded vision design automation over two main aspects: the adoption of the model-based paradigm for the embedded vision rapid prototyping, and the application of heterogeneous programming languages to improve the system performance. The paper presents our recent results on the design of a localization and mapping application combined with image recognition based on deep learning optimized for an NVIDIA Jetson TX2",'Institute of Electrical and Electronics Engineers (IEEE)',Rapid Prototyping of Embedded Vision Systems: Embedding Computer Vision Applications into Low-Power Heterogeneous Architectures,10.1109/RSP.2018.8631995,https://core.ac.uk/download/217586045.pdf,,core
158354020,2018-05-23T00:00:00,"Physical law based models (also known as white box models) are widely applied in the aerospace industry, providing models for dynamic systems such as helicopter flight simulators. To meet the criteria of real-time simulation, simplifications to the underlying physics sometimes have to be applied, leading to errors in the model’s predictions. Grey-box models use both physics-based and data-based models. They have potential to reduce the difference between a simulator’s and real rotorcraft’s response. In the current work, a preliminary step to the grey-box approach, a machine learnt data-based, i.e ‘black box’ model is applied to the dynamic response of a helicopter. The machine learning methods used are probabilistic and can capture uncertainties associated with the model’s prediction. In the current paper, machine learning is used to create a Gaussian Process (GP) non-linear autoregressive (NARX) model that predicts pitch, roll and yaw rate. The predictions are compared to a physical law based model created using FLIGHTLAB software. The GP outperforms the FLIGHTLAB model in terms of root mean squared error, when predicting the pitch, roll and yaw rate of a Bo105 helicopter",,Towards Gaussian Process Models of Complex Rotorcraft Dynamics,,https://core.ac.uk/download/158354020.pdf,,core
186279212,2018-10-17T00:00:00,"The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it",,"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality",,http://arxiv.org/abs/1810.07829,,core
216783932,2018-01-01T00:00:00,"Estratégias de monitoramento, baseadas na análise da condição de equipamentos utilizando ferramentas de processamento digital de sinais, inteligência artificial e tolerância a falhas, tornam-se cada vez mais necessárias nos processos industriais. As técnicas de manutenção inteligente conferem confiabilidade, disponibilidade e eficácia, e são estudadas, neste trabalho, no atual estado da arte. Porém, grande parte delas utiliza medidas com estados e parâmetros do processo que são dispendiosas e envolvem elevado tempo de amostragem e análise. O objetivo deste trabalho é desenvolver um novo sistema capaz de estimar a condição de saúde de um equipamento a partir das leituras de vibração e torque de sensores, e assim, viabilizar a detecção, predição e identificação de falhas online em atuadores elétricos utilizados em linhas de transporte de petróleo e/ou derivados. Para isso, foi desenvolvida uma técnica que, por meio de um dispositivo computacional, possibilita monitorar, considerando ruído e, de forma interativa, as variações dos parâmetros de um processo físico, tais como: falhas abruptas, incipientes e intermitentes. Isso corresponde às atividades de detecção, identificação de falhas e previsões sobre possíveis problemas que venham a surgir em consequência de pequenos desvios do comportamento normal do sistema. A metodologia empregada é baseada na estrutura do modelo Open Systems Architecture for Condition-Based Maintenance (OSA-CBM), que permite atuar nas seguintes camadas: 1) Aquisição de dados; 2) Manipulação de dados; 3) Monitoramento das condições; 4) Avaliação da saúde O sistema compreende a análise simultânea das propriedades de tempo e frequência do sinal, extração de características e filtragem adaptativa. Uma bancada de testes foi utilizada para reproduzir algumas falhas típicas que podem causar degradação na operação de atuadores fabricados no mercado. O sistema foi denominado Fault Detection System (FDS) e é baseado em técnicas de processamento de sinais que tem como saída um sinal de resíduo ou erro quando na ocorrência de uma falha correspondente nos equipamentos monitorados. A versão em software do sistema foi registrada no Instituto Nacional da Propriedade Industrial (INPI) no ""BR 51 2016 000863-6"". Uma nova versão para prototipagem em hardware do FDS em conjunto com um bloco auxiliar denominado Fault Detection Index (FDI), que também é proposto neste trabalho, foi desenvolvido na linguagem Verilog e implementado utilizando uma biblioteca Complementary Metal-Oxide-Semiconductor (CMOS) de 90 nm visando baixo consumo de energia ( 654 μW), baixa utilização de área em silício ( 0, 14 mm2) e processamento em tempo real. Os resultados demonstram a eficácia do método de detecção, diagnóstico e identificação de falhas apresentadas em atuadores elétricos empregados para controle de válvulas.Monitoring strategies based on the analysis of equipment condition with information derived from digital signal processing, artificial intelligence and fault tolerance tools become increasingly necessary in industrial process. In this context, intelligent maintenance techniques provide reliability, availability and are being increasingly studied in the current state of the art researches. However, most of them are based on measurements with states and process parameters that are costly and involve high sampling and analysis time. In order to avoid this problem, this work presents a new system capable of estimating the health condition of an equipment from the vibration and torque measurements of sensors, thus enabling online detection, prediction and identification of faults in electric actuators. The developed system represents a technique that, by means of a computational device, allows to monitor the variations of the parameters of a physical process such as abrupt, incipient and intermittent failures. This corresponds to the activities of fault detection, identification and prediction of possible problems that may arise due to minor deviations of the normal behavior state of the system. The methodology is based on the Open Systems Architecture for Condition-Based Maintenance (OSA-CBM) framework, which allows to act in the following layers: 1) Data acquisition; 2) Data manipulation; 3) Condition monitoring; 4) Health assessment. The system comprises the simultaneous analysis of signal time and frequency properties, feature extraction and adaptive filtering A testbench structure has been used to reproduce some typical faults that can cause degradation in the operation of the available commercial actuators. The results show the effectiveness of the method of detection, diagnosis and identification of faults that may occur in electric valves. The system is denominated Fault Detection System (FDS) and it is based on digital signal processing techniques producing a residue signal or error in the occurrence of a corresponding fault in the monitored equipment. A software version of the system was registered with the Instituto Nacional da Propriedade Industrial (INPI) no ""BR 51 2016 000863-6"". A new version for hardware prototyping of FDS together with the Fault Detection Index (FDI), which is also proposed in this work, was using Ver- ilog language and implemented in a 90 nm Complementary Metal-Oxide-Semiconductor (CMOS) library for low power consumption ( 654 μW), low silicon area utilization ( 0.14 mm2) and real time processing. The results demonstrate the effectiveness of the method of detection, diagnosis and identification of faults present in electric actuators used for controling fluidic valves",,Desenvolvimento de um sistema em chip de processamento online para manutenção inteligente,,,,core
301378568,2018-11-28T08:00:00,"Abstract The purpose of the paper is to evaluate the feasibility of business-to-consumer (B2C) customer relationship analytics in the industrial business-to-business (B2B) context, in particular spare part sales. The contribution of the paper is twofold; the article identifies analytics approaches with value potential for B2B decision-making, and illustrates their value in use. The identified analytics approaches, customer segmentation, market basket analysis and target customer selection, are common in the B2C marketing and e-commerce. However, in the industrial B2B marketing, the application of these approaches is not yet common.. The different kinds of analytics under examination in this paper use machine learning (ML) techniques. The examination takes into account the applicability and usefulness of the techniques as well as implementation challenges. The research suggests that the identified analytics may serve different business purposes and may be relatively straightforward to implement. This requires careful examination of the desired purposes of use in a particular business context. However, the continuous and real-time use of such analyses remains a challenge for further examination also in information systems research. Keywords: Business analytics, B2B decision-making, Machine learning, Data mining, Artificial intelligence, CR",AIS Electronic Library (AISeL),FEASIBILITY OF B2C CUSTOMER RELATIONSHIP ANALYTICS IN THE B2B INDUSTRIAL CONTEXT,,https://core.ac.uk/download/301378568.pdf,,core
160723764,2018-08-09,"2018-08-10According to the Axiomatic Design Theory (ADT), a design concept that can satisfy the upstream objectives under downstream constraints with the minimal relative complexity can lead to the most ideal design. As stated by Suh’s Complexity Theory, the relative complexity of a design concept is caused by couplings between functional requirements (FRs) and design parameters (DPs), and can be reduced by strategically ordering the execution (i.e., implementation) sequence of DPs. However, it is generally very difficult in the current design practice to obtain this “execution sequence” with existing methods due to their inherent limitations and/or many real-world restrictions. Meanwhile, many practical methods, such as the modular design approach, have been widely used in industries to produce real-world design results that don’t necessarily conform with the principles required by those ideal design theories. As a result, from the perspectives of design theories, most real-world designs are “not ideal” (i.e., having some relative complexities due to FR-DP couplings) and therefore can (and should) be improved by better sequencing their DPs. This is the motivation under which the Design Coupling Sequence (DCS) method was developed in this thesis research. The DCS method can assist designers to automatically obtain the “execution sequences,” in the forms of functional sets, that can yield the minimal relative complexity, hence making a practical design concept most ideal (i.e., as close to the ideal concept with the minimal relative complexity as possible) while taking into practical considerations (such as increasing the modularity to lower the production costs) in real-world conceptual designs. ❧ The DCS method defines the ‘precedence’ between ‘functional sets’ to manage coupled design concepts to support the modular approach during conceptual design. It identifies the ‘precedence’ by the level of functional coupling to determine the proper sequencing order to minimize the overall complexity. Two types of functional sets are defined in DCS as 1) the complete independently set U: the collection of all the functionally dependent DPs in the system so that the set is independent to other U sets, and 2) the indivisible coupled set C: the collection of coupled concepts that can’t be decoupled by sequencing, so it prescribes the designer to consider the group of DPs together as a set to match existing modules in the database. To handle the real complexity of design concepts which require redesign, the DCS algorithm helps to determine the proper execution sequence. To minimize the imaginary complexity, which occurs when design concepts “appear” to be functionally coupled due to a lack of understanding of the system structure, the DCS method provides a formula to reveal the number of acceptable execution sequences that can lead to the simplest design implementation. Compared with existing methods, the DCS method is applicable for any design cases with known design matrices, including the square, rectangular, zero-at-diagonal, large, and/or numerical matrices. In short, for all practical design cases, DCS can organize functional-coupled design concepts as “functional sets” with execution sequences of DPs that lead to the minimal complexity of this design concept. ❧ The foundation, hypothesis, algorithm and its usability of the DCS method are validated by four case studies in this research. The faucet design case demonstrates how to apply the DCS method and shows the differences between ADT and DCS results. The case of coffee maker design shows how the DCS method manages the functional sets based on the design matrices reengineered from existing design concepts. The vehicle tire design case demonstrates how different DCS strategies within the Innovative Design Thinking (IDT framework during the conceptual design stage can work in a real-world product development situation. The IDT framework prescribes four consecutive steps: (1) following the top-down process to ideate new design concepts that satisfy the principles/axioms suggested by the design theory to reach a certain layer of details, (2) following the bottom-up process to identify some existing design modules from available engineering database (or catalogs) that can satisfy the functional requirements at this detail layer, (3) constructing the design matrix that shows the couplings between FRs and DPs at this detail layer, and (4) apply the DCS algorithm to determine the execution sequence of DPs based on the above design matrix. This will yield a new design concept with an execution sequence that is most creative (because it satisfies the design principles at the top layers) and most practical (because it utilizes the existing modules at the bottom layers). Finally, a case of collision avoidance planning presents one of the possible extensions of the DCS algorithm. ❧ The results of this research have significant impacts on both design theory and design practice. Theoretically, the approach in this research 1) guides designers to improve concepts not only organizing design matrix but also extract additional coupling information to increase modularity and 2) is a more generalized approach than the previous methods that can be applied to any design cases with design matrix. Practically, the research 1) demonstrates the usability of the DCS algorithm within an executive program to generate the DCS functional sets automatically for large design system and 2) allows the principle of functional dependency and the practice of modular design to be considered simultaneously as much as possible during the conceptual design stage. It is a fundamental contribution that demonstrates how the ideal principles (or axioms) of design theories can be used together strategically with practical design methods (or considerations) in industry practices to generate real-world design results that are both most practical and creative. For future research, there would be three aspects- DCS algorithm, DCS sets, and DCS strategies. Number one, DCS algorithm would be further revised to extend to software design or machine learning with functional sets in terms of a component diagram. Number two, with DCS sets, the three-dimensional design matrix could be studied further. Number three, DCS strategies would be investigated further for applying on detailed design cases. ❧ As a recap, the research prescribes a functional coupling managing algorithm with functional sets for suggesting acceptable execution sequences in conceptual design. It not only helps designers with complexity reduction but also bridges the ideal design theory to practical modules. The designer can create better designs that are most creative and yet practical by using the DCS strategies",University of Southern California. Libraries,Managing functional coupling sequences to reduce complexity and increase modularity in conceptual design,,,,core
160788772,2018-08-30T00:00:00,"For any autonomous driving vehicle, control module determines its road
performance and safety, i.e. its precision and stability should stay within a
carefully-designed range. Nonetheless, control algorithms require vehicle
dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are
obscure to calibrate in real time. As a result, to achieve reasonable
performance, most, if not all, research-oriented autonomous vehicles do manual
calibrations in a one-by-one fashion. Since manual calibration is not
sustainable once entering into mass production stage for industrial purposes,
we here introduce a machine-learning based auto-calibration system for
autonomous driving vehicles. In this paper, we will show how we build a
data-driven longitudinal calibration procedure using machine learning
techniques. We first generated offline calibration tables from human driving
data. The offline table serves as an initial guess for later uses and it only
needs twenty-minutes data collection and process. We then used an
online-learning algorithm to appropriately update the initial table (the
offline table) based on real-time performance analysis. This longitudinal
auto-calibration system has been deployed to more than one hundred Baidu Apollo
self-driving vehicles (including hybrid family vehicles and electronic
delivery-only vehicles) since April 2018. By August 27, 2018, it had been
tested for more than two thousands hours, ten thousands kilometers (6,213
miles) and yet proven to be effective",,"Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and
  Learning based Vehicle Longitude Dynamic Calibrating Algorithm",,http://arxiv.org/abs/1808.10134,,core
162022928,2018-01-01T00:00:00,"Python has evolved to become the most popular language for data science. It sports state-of-the-art libraries for analytics and machine learning, like Sci-Kit Learn. However, Python lacks the computational performance that a industrial system requires for high frequency real time predictions.

Building upon a year long research project heavily based on SciKit Learn (sklearn), we faced performance issues in deploying to production. Replacing sklearn with a better performing framework would require re-evaluating and tuning hyperparameters from scratch. Instead we developed a python embedding in a C++ based server application that increased performance by up to 20x, achieving linear scalability up to a point of convergence. Our implementation was done for mainstream cost effective hardware, which means we observed similar performance gains on small as well as large systems, from a laptop to an Amazon EC2 instance to a high-end server",,Distributed C++-Python embedding for fast predictions and fast prototyping,,https://core.ac.uk/download/162022928.pdf,,core
185689665,2018-10-21T00:00:00,"International audienceMore and more domains such as industry, sport, medicine, Human Computer Interaction (HCI) and education analyze user motions to observe human behavior, follow and predict its action, intention and emotion, to interact with computer systems and enhance user experience in Virtual (VR) and Augmented Reality (AR). In the context of human learning of movements, existing software applications and methods rarely use 3D captured motions for pedagogical feedback. This comes from several issues related to the highly complex and dimensional nature of these data, and by the need to correlate this information with the observation needs of the teacher. Such issues could be solved by the use of machine learning techniques, which could provide efficient and complementary feedback in addition to the expert advice, from motion data. The context of the presented work is the improvement of the human learning process of a motion, based on clustering techniques. The main goal is to give advice according to the analysis of clusters representing user profiles during a learning situation. To achieve this purpose, a first step is to work on the separation of the motions into different categories according to a set of well-chosen features. In this way, allowing a better and more accurate analysis of the motion characteristics is expected. An experimentation was conducted with the Bottle Flip Challenge. Human motions were first captured and filtered, in order to compensate for hardware related errors. Descriptors related to speed and acceleration are then computed, and used in two different automatic approaches. The first one tries to separate the motions, using the computed descriptors, and the second one, compares the obtained separation with the ground truth. The results show that, while the obtained partitioning is not relevant to the degree of success of the task, the data are separable using the descriptors",HAL CCSD,Clustering and Analysis of User Motions to Enhance Human Learning: A First Study Case with the Flip Bottle Challenge,,https://core.ac.uk/download/185689665.pdf,,core
153598490,2018-04-01T00:00:00Z,"Human–robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze–object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The “gaze object sequence” was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments",Frontiers Media S.A.,Exploiting Three-Dimensional Gaze Tracking for Action Recognition During Bimanual Manipulation to Enhance Human–Robot Collaboration,10.3389/frobt.2018.00025/full,,"[{'title': None, 'identifiers': ['issn:2296-9144', '2296-9144']}]",core
186289700,2018-11-12T00:00:00,"Academic literature on machine learning modeling fails to address how to make
machine learning models work for enterprises. For example, existing machine
learning processes cannot address how to define business use cases for an AI
application, how to convert business requirements from offering managers into
data requirements for data scientists, and how to continuously improve AI
applications in term of accuracy and fairness, and how to customize general
purpose machine learning models with industry, domain, and use case specific
data to make them more accurate for specific situations etc. Making AI work for
enterprises requires special considerations, tools, methods and processes. In
this paper we present a maturity framework for machine learning model lifecycle
management for enterprises. Our framework is a re-interpretation of the
software Capability Maturity Model (CMM) for machine learning model development
process. We present a set of best practices from our personal experience of
building large scale real-world machine learning models to help organizations
achieve higher levels of maturity independent of their starting point.Comment: 10 pages, 1 figure, 1 tabl",,Characterizing machine learning process: A maturity framework,,http://arxiv.org/abs/1811.04871,,core
289954305,2018-01-01T00:00:00,"The number of devices connected to the Internet grows constantly. This information entity has been labeled the Internet of Things (IoT). One important aspect of this is the industrial applications, sometimes labeled the Industrial Internet of Things (IIoT). Collecting and analyzing the massive amounts of data that industry generates will only become more and more important as technology and the need for efficiency increase. Novotek is a company with long and extensive experience of industrial IT and automation. Together with their customer Quant Service they are launching a project for predictive maintenance. This aims to monitor several different industrial sites using an industrial platform and the IIoT framework. The monitoring will allow for tracking of machine status and maintenance needs from both near and afar. One of the sites for this project is a veneer production line for composite wood products. As a part of the monitoring and predictive maintenance project, this report looks at the possibility of using the ThingWorx IIoT platform’s analytics functionality to determine the need for maintenance of the cutting knife on a veneer lathe. The goal is to look at its uses for monitoring and predictive maintenance for this particular case but also as a general method. The process for this will be twofold. Since the project uses the IIoT framework one part is how to collect the data from the site and then passing it through the platform and to the analytics program. The second part is the machine learning and statistical methods and algorithms used to analyze the data for predictions. For benchmarking, it will be compared to another analytics product. The results of the project are not conclusive concerning the knife predictions. Development of the measurement setup is needed. The IIoT platform does however show potential in being used for the intended purpose.Predictive Maintenance with the Industrial Internet of Things The Industrial Internet of Things is growing every day. When machines talk to each other, they will revolutionize industry as we know it. The Industrial Internet of Things (IIoT), meaning real-time interconnectedness of industrial devices, is said to play a big part in the next industrial revolution, Industry 4.0. Pretty much all industrial devices, or Things, generate data. But data is not information. If it is to be valuable, it must be analyzed with the right tools so that the right decisions can be made. Ultimately, this will lead to a complete automation of the industrial process with smart machines talking and giving advice to each other. Novotek, a company with long experience in the areas of industrial IT and automation, is launching an IIoT project together with a customer. As a part of this, a MSc thesis study was done on using an IIoT platform for predictive maintenance. The object of study was a veneer peeling lathe used in the manufacturing of composite wood products. Wood cutting constantly dulls the tools involved and they need to be sharpened or exchanged several times during a workday. If it is possible for the machine to “know” the sharpness of its knife, it can decide when the optimal point of maintenance should be. One possible method to predict this is to monitor overall vibrations in the lathe and look for any patterns. To handle all the communication, storing and analysis of the data, specialized tools are needed. One such tool is the IIoT platform ThingWorx. ThingWorx has functionality for a multitude of applications. It can keep track of all your Things and handle the communication between them. It also has components for advanced analysis of data, using machine learning and statistical algorithms. The results of the study are not conclusive but tests for the process imply the usefulness of the IIoT framework. The application implemented creates a well-defined path for data to follow. This functions both for the modeling of the problem as well facilitating predictive process monitoring in actual operation. Once an IIoT solution has been implemented a company has a complete structure for connecting and monitoring all parts of their business. This goes beyond just reading production parameters from afar. This kind of connected industry can monitor itself. It can make predictions and take the right decisions for the manufacturing autonomously, only involving humans when needed. The possibilities for optimization and efficiency goes far beyond what was thought possible only a decade ago",Lunds universitet/Industriell elektroteknik och automation,Monitoring of a Veneer Lathe Knife by the use of an Industrial Internet of Things- Platform,,https://core.ac.uk/download/pdf/289954305.pdf,,core
188688866,2018,"In the phase of industry digitalization, data are collected from many sensors and signal processing techniques play a crucial role. Data preprocessing is a fundamental step in the analysis of measurements, and a first step before applying machine learning. To reduce the influence of distortions from signals, selective digital filtering is applied to minimize or remove unwanted components. Standard software and hardware digital filtering algorithms introduce a delay, which has to be compensated for to avoid destroying signal associations. The delay from filtering becomes more crucial when the analysis involves measurements from multiple sensors, therefore in this paper we provide an overview and comparison of existing digital filtering methods with an application based on real-life marine examples. In addition, the design of special-purpose filters is a complex process and for preprocessing data from many sources, the application of digital filtering in the time domain can have a high numerical cost. For this reason we describe discrete Fourier transformation digital filtering as a tool for efficient sensor data preprocessing, which does not introduce a time delay and has low numerical cost. The discrete Fourier transformation digital filtering has a simpler implementation and does not require expert-level filter design knowledge, which is beneficial for practitioners from various disciplines. Finally, we exemplify and show the application of the methods on real signals from marine systems.acceptedVersion© 2018. This is the authors' accepted and refereed manuscript to the article. The final authenticated version is available online at: https://doi.org/10.1177%2F014233121879914",SAGE Publications,Comparison of Delayless Digital Filtering Algorithms and Their Application to Multi-Sensor Signal Processing,10.1177/0142331218799148,,,core
392173855,2018-01-01T00:00:00,"Open Access. Link to publishers version: https://www.taylorfrancis.com/books/e/9781351174657/chapters/10.1201%2F9781351174664-316As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",Taylor&Francis Group,Digitalization of the power business: How to make this work?,,,,core
154761340,2018-01-01T00:00:00,"In machine learning, a bias occurs whenever training sets are not representative for the test data, which results in unreliable models. The most common biases in data are arguably class imbalance and covariate shift. In this work, we aim to shed light on this topic in order to increase the overall attention to this issue in the field of machine learning. We propose a scalable novel framework for reducing multiple biases in high-dimensional data sets in order to train more reliable predictors. We apply our methodology to the detection of irregular power usage from real, noisy industrial data. In emerging markets, irregular power usage, and electricity theft in particular, may range up to 40% of the total electricity distributed. Biased data sets are of particular issue in this domain. We show that reducing these biases increases the accuracy of the trained predictors. Our models have the potential to generate significant economic value in a real world application, as they are being deployed in a commercial software for the detection of irregular power usage",,On the Reduction of Biases in Big Data Sets for the Detection of Irregular Power Usage,,https://core.ac.uk/download/154761340.pdf,,core
301379098,2018-12-06T08:00:00,"Smart tourism destination as: an innovative tourist destination, built on an infrastructure of state-of-the-art technology guaranteeing the sustainable development of tourist areas, accessible to everyone, which facilitates the visitor’s interaction with and integration into his or her surroundings, increases the quality of the experience at the destination, and improves residents’ quality of life. Lopez de Avila (2015). Smart tourism involves multiple components and layers of “smart” include (1) Smart Destinations which was special cases of smart cities integration of ICT’s into physical infrastructure, (2) Smart experience which specifically focus on technology-mediated tourism experience and their engagement through personalization, context-awareness and real-time monitoring, (3) Smart business refer to the complex business ecosystem that creates and supports the exchange of touristic resource and the co-creation of tourism experience. Gretzel et al, (2015). Smart tourism also clearly relies on the ability to not only collect enormous of data but to intelligently store, process, combine, analyze and use big data to inform business innovation, operations and services by artificial intelligence and big data technique. The rapid development of information communication technology (ICT) such as artificial intelligent, cloud computing, mobile device, big data mining and social media cause computing, storage and communication relevant software and hardware popular. Facebook, Amazon, Apple, Microsoft and Google have risen rapidly since 2000. In recent years, Emerging technologies such as Artificial Intelligence, Internet of Thing, Robotic, Cyber Security, 3D printer and Block chain also accelerate the development of industry toward digital transformation trend such as Fintech, e-commerce, smart cities, smart tourism, smart healthcare, smart manufacturing... This study proposes a conceptual framework that integrates (1) artificial intelligence/machine learning, (2) institution/organizational and (3) business processes to assist smart tourism stake holder to leverage artificial intelligence to integrate cross-departmental business and streamline key performance metrics to build a business-level IT Strategy. Artificial intelligence as long as the function includes (1) Cognitive engagement to (voice/pattern recognition function) (2) Cognitive process automation (Robotic Process Automation) (3) Cognitive insight (forecast, recommendation)",AIS Electronic Library (AISeL),Artificial Intelligence in Smart Tourism: A Conceptual Framework,,https://core.ac.uk/download/301379098.pdf,,core
299961236,2018-01-01T00:00:00,"With the progression of semiconductor and integrated circuit industry, a significantly increasing number of system on chip (SOC) related applications are widely developed in various areas. This final year project (FYP) is a typical example of biomedical IC design application, especially intended for diabetic patients to monitor their body pressures such as foot pressure. Feet ulcer is a common symptom for terminal diabetic patients and usually causes serious problems. Most medical reports related to the disease of diabetes have pointed out that the nervous system of feet is often weaken for diabetic individual and as a result they will lose the tactile sense, either partially or totally. To avoid such damages to feet, the work of this project is intended to provide warnings to diabetic patients by measuring the real-time foot pressures with pressure sensor array implemented on socks. Precisely speaking, this project includes two major parts. In the first part, signals from pressure sensor array could be conveyed to a user terminal on time by passing through a series of electronic modules such as amplifiers, Analog to digital conversions (ADC) and so on. In the second part, in-device analysis about their motion activity was performed based on some concepts of machine learning, which can relate the real-time feet pressure values to different motion activities like walking or jogging. These two parts will work together to continuously generate the real-time foot pressure values of the user and give appropriate feedback under different scenarios of motion activities. In summary, this FYP will be beneficial to diabetic patients by helping them to avoid large abnormal pressures occurred on feet. Novel wearable product was designed in the end at low cost and it has large potential market prospect.Bachelor of Engineerin",,Digital signal processor system for motion sensor monitoring device,,,,core
195337226,2018-09-01T00:00:00,"The European Union\u27s Energy Efficiency Directive is placing an increased focus on the measurement and verification (M&V) of demand side energy savings. The objective of M&V is to quantify energy savings with minimum uncertainty. M&V is currently undergoing a transition to practices, known as M&V 2.0, that employ automated advanced analytics to verify performance. This offers the opportunity to effectively manage the transition from short-term M&V to long-term monitoring and targeting (M&T) in industrial facilities. The original contribution of this paper consists of a novel, robust and technology agnostic framework that not only satisfies the requirements of M&V 2.0, but also bridges the gap between M&V and M&T by ensuring persistence of savings. The approach features a unique machine learning-based energy modelling methodology, model deployment and an exception reporting system that ensures early identification of performance degradation. A case study demonstrates the effectiveness of the approach. Savings from a real-world project are found to be 177,962 +/- 12,334 kWh with a 90% confidence interval. The uncertainty associated with the savings is 8.6% of the allowable uncertainty, thus highlighting the viability of the framework as a reliable and effective tool",'Institute of Electrical and Electronics Engineers (IEEE)',From M&V to M&T: An artificial intelligence-based framework for real-time performance verification of demand-side energy savings,,,,core
201147855,2018-12-01T00:00:00,"The active implementation of digital technologies into all spheres of public life, as well as the rapid development of artificial intelligence, is assuming a serious dimension, thus requiring a special attention of the legislator. The article examines the current state of the legal regulation of the artificial intelligence. The author considers the Strategy of the Information Society Development in the Russian Federation for 2017-2030, as well as provides some clear examples of active implementation of artificial intelligence into social reality. The author also provides the McKinsey consulting group's research findings which reflect the prospects for replacing human labor by robots. It is pointed out that the issue of total computerization and the corresponding displacement of a human from the sphere of intellectual activity is rather controversial. The article also discusses the main possible problems related to the artificial intelligence technologies: the problems of responsibility that may arise in the operation of industrial robots; the continuity of digital activity can affect the psychoemotional state. The issue of a possibility for creating robots with intelligence and endowed with personality is being considered from the Philosophy perspective. The conclusion is drawn that the theoretical study of the intellect and the ""electronic person"" is one of the possible redirections of the Russian law development in modern conditions",'Peoples'' Friendship University of Russia',Artificial Intelligence in the Legal Space,10.22363/2313-2337-2018-22-3-315-328,,"[{'title': 'RUDN JOURNAL OF LAW', 'identifiers': ['2408-9001', 'issn:2408-9001', 'issn:2313-2337', '2313-2337']}]",core
157651040,2018-05-01T00:00:00Z,"The prediction of internal defects of metal casting immediately after the casting process saves unnecessary time and money by reducing the amount of inputs into the next stage, such as the machining process, and enables flexible scheduling. Cyber-physical production systems (CPPS) perfectly fulfill the aforementioned requirements. This study deals with the implementation of CPPS in a real factory to predict the quality of metal casting and operation control. First, a CPPS architecture framework for quality prediction and operation control in metal-casting production was designed. The framework describes collaboration among internet of things (IoT), artificial intelligence, simulations, manufacturing execution systems, and advanced planning and scheduling systems. Subsequently, the implementation of the CPPS in actual plants is described. Temperature is a major factor that affects casting quality, and thus, temperature sensors and IoT communication devices were attached to casting machines. The well-known NoSQL database, HBase and the high-speed processing/analysis tool, Spark, are used for IoT repository and data pre-processing, respectively. Many machine learning algorithms such as decision tree, random forest, artificial neural network, and support vector machine were used for quality prediction and compared with R software. Finally, the operation of the entire system is demonstrated through a CPPS dashboard. In an era in which most CPPS-related studies are conducted on high-level abstract models, this study describes more specific architectural frameworks, use cases, usable software, and analytical methodologies. In addition, this study verifies the usefulness of CPPS by estimating quantitative effects. This is expected to contribute to the proliferation of CPPS in the industry",MDPI AG,Implementation of Cyber-Physical Production Systems for Quality Prediction and Operation Control in Metal Casting,10.3390/s18051428,,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
151255160,2018-09-11T00:00:00,"Real-time advertising allows advertisers to bid for each impression for a
visiting user. To optimize specific goals such as maximizing revenue and return
on investment (ROI) led by ad placements, advertisers not only need to estimate
the relevance between the ads and user's interests, but most importantly
require a strategic response with respect to other advertisers bidding in the
market. In this paper, we formulate bidding optimization with multi-agent
reinforcement learning. To deal with a large number of advertisers, we propose
a clustering method and assign each cluster with a strategic bidding agent. A
practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed
and implemented to balance the tradeoff between the competition and cooperation
among advertisers. The empirical study on our industry-scaled real-world data
has demonstrated the effectiveness of our methods. Our results show
cluster-based bidding would largely outperform single-agent and bandit
approaches, and the coordinated bidding achieves better overall objectives than
purely self-interested bidding agents",'Association for Computing Machinery (ACM)',"Real-Time Bidding with Multi-Agent Reinforcement Learning in Display
  Advertising",10.1145/3269206.3272021,http://arxiv.org/abs/1802.09756,,core
213622948,2018-12-01T00:00:00,"Avec l’intérêt que la technologie d’aujourd’hui a sur les données, il est facile de supposer que l’information est au bout des doigts, prêt à être exploité. Les méthodologies et outils de recherche sont souvent construits sur cette hypothèse. Cependant, cette illusion d’abondance

se brise souvent lorsqu’on tente de transférer des techniques existantes à des applications industrielles.

Par exemple, la recherche a produit divers méthodologies permettant d’optimiser l’utilisation des ressources de grands systèmes complexes, tels que les avioniques de l’Airbus A380. Ces approches nécessitent la connaissance de certaines mesures telles que les temps d’exécution, la consommation de mémoire, critères de communication, etc. La conception de ces systèmes complexes a toutefois employé une combinaison de compétences de différents domaines (probablement avec des connaissances en génie logiciel) qui font que les données caractéristiques au système sont incomplètes ou manquantes. De plus, l’absence d’informations

pertinentes rend difficile de décrire correctement le système, de prédire son comportement, et améliorer ses performances. Nous faisons recours au modèles probabilistes et des techniques d’apprentissage automatique pour remédier à ce manque d’informations pertinentes. La théorie des probabilités, en particulier, a un grand potentiel pour décrire les systèmes partiellement observables. Notre objectif est de fournir des approches et des solutions pour produire des informations pertinentes. Cela permet une description appropriée des systèmes complexes pour faciliter l’intégration, et permet l’utilisation des techniques d’optimisation existantes. Notre première étape consiste à résoudre l’une des difficultés rencontrées lors de l’intégration de système : assurer le bon comportement temporelle des composants critiques des systèmes. En raison de la mise à l’échelle de la technologie et de la dépendance croissante à l’égard des architectures à multi-coeurs, la surcharge de logiciels fonctionnant sur différents coeurs et le partage d’espace mémoire n’est plus négligeable. Pour tel, nous étendons la boîte à outils des système temps réel avec une analyse temporelle probabiliste statique qui estime avec précision l’exécution d’un logiciel avec des considerations pour les conflits de mémoire partagée. Le

modèle est ensuite intégré dans un simulateur pour l’ordonnancement de systèmes temps réel multiprocesseurs. ----------ABSTRACT: In today’s data-driven technology, it is easy to assume that information is at the tip of our fingers, ready to be exploited. Research methodologies and tools are often built on top of this assumption. However, this illusion of abundance often breaks when attempting

to transfer existing techniques to industrial applications. For instance, research produced various methodologies to optimize the resource usage of large complex systems, such as the avionics of the Airbus A380. These approaches require the knowledge of certain metrics such as the execution time, memory consumption, communication delays, etc. The design of these complex systems, however, employs a mix of expertise from different fields (likely with limited knowledge in software engineering) which might lead to incomplete or missing specifications. Moreover, the unavailability of relevant information makes it difficult to properly describe

the system, predict its behavior, and improve its performance. We fall back on probabilistic models and machine learning techniques to address this lack of

relevant information. Probability theory, especially, has great potential to describe partiallyobservable systems. Our objective is to provide approaches and solutions to produce relevant information. This enables a proper description of complex systems to ease integration, and allows the use of existing optimization techniques. Our first step is to tackle one of the difficulties encountered during system integration: ensuring the proper timing behavior of critical systems. Due to technology scaling, and with the growing reliance on multi-core architectures, the overhead of software running on different cores and sharing memory space is no longer negligible. For such, we extend the real-time

system tool-kit with a static probabilistic timing analysis technique that accurately estimates the execution of software with an awareness of shared memory contention. The model is then incorporated into a simulator for scheduling multi-processor real-time systems",,Handling Information and its Propagation to Engineer Complex Embedded Systems,,https://core.ac.uk/download/213622948.pdf,,core
162631921,2018-01-01T00:00:00,"Quantifying the effects of competition for natural resources between different sectors and interests is a key part of natural resource management globally. A major form of land use conflict in natural forests is between water production and timber production. Here we explore trade-offs in water yield resulting from logging in the forested water catchments north-east of Melbourne – the second largest urban settlement in Australia with a current population of five million. It has long been understood that logging significantly decreases water yields in Melbourne’s water catchments. However, the extent of losses of water yield from past logging have rarely been documented. Here, we model changes in water yield in Melbourne’s largest single catchment, the Thomson Catchment, resulting from: (1) past forest management activities (especially clearfell logging), and (2) future forest management scenarios. Our particular focus was on the effects of logging on water yields from ash-type eucalypt forests. This is because these areas have the greatest impact on water runoff due to them receiving the most rainfall and being the forest types subject to the most intensive and extensive industrial logging. We modelled four key scenarios: 
	Scenario (1) Historical logging of the Thomson Catchment with continued logging in the future (current reality/status quo); 
	Scenario (2) If there had been no logging and none was planned (past, present or future) in the Thomson Catchment; 
	Scenario (3) Logging ceasing in 1967 (as specified under the first Wood Pulp Agreement Act 1936 – but which never occurred); and
	Scenario (4) Impacts of the past logging, but with cessation of logging in 2018. 
Our initial spatial analysis revealed that 42% of the ash-type eucalypt forests in the Thomson Catchment have been logged. Moreover, there are 4,000 hectares of Ash forest assigned for logging in the next 5 years under the existing Timber Release Plan for the Central Highlands region. Our analyses revealed that the current (in 2018) reduction in water yield due to historical logging of the ash forests across the Thomson Catchment exceeds 15,000 ML annually. This loss is projected to increase to nearly 35,156 ML by 2050. Under Scenario (3), where logging would have ceased in 1967 if the first Wood Pulp Agreement 1936 was implemented, the loss in water yield by 2018 was projected to be 1,079 ML, annually. This loss is a result of logging occurring prior to 1967. This was modelled to remain constant through to 2050. Under Scenario (4), where logging ceases in 2018, we projected that approximately 20,149 ML would have been returned to the Thomson Catchment by 2050 compared with Scenario (2) of no historical logging. Losses in water yield as a result of logging correspond to 9%-20% of the ash forest catchment water yield for 2018 and 2050, respectively. Based on an estimated consumption of 161 litres of water per person per day, the loss in water yield resulting from logging would equate to the lost water for nearly 600,000 people by 2050. 
Given the strategic importance of water from the Thomson Catchment, our analyses suggest that native forest logging should be excluded from this catchment, particularly in the context of increasing human consumption of water and decreasing stream inflows from the catchments. Previous work has shown that the economic value of the water across all of Melbourne’s Water Catchments, including the Thomson Catchment, is 25.5 times greater than the economic value of the timber produced from the all native forests, based on integrated economic and environmental accounting (e.g. under the System of Environmental and Economic Accounting [SEEA] developed by the United Nations). It is not the difference in value between water and timber that is important, it is the change due to the use of an ecosystem service, resulting in the reduction of water yield. Therefore, we suggest that ongoing logging of the Thomson Catchment, when it is known to reduce water yields, is a questionable natural resource management policy.This research was partly supported by the National Environmental Science Program Threatened Species Recovery Hub",The Australian National University. Fenner School of Environment and Society,Resource Conflict Across Melbourne’s Largest Domestic Water Supply Catchment,10.25911/5beb630e45d35,https://core.ac.uk/download/162631921.pdf,,core
296885525,2018-08-31T03:55:57Z,"Orientadores: Amauri Garcia, Noé CheungTese (doutorado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: A solidificação de produtos metálicos via lingotamento contínuo (LC) com minimização de defeitos é um desafio constante na indústria metalúrgica, no sentido de se manter competitiva no mercado. Uma das abordagens utilizadas planejamento de operações de LC, em particular na indústria siderúrgica, consiste na aplicação de modelos matemáticos de transferência de calor à solidificação, uma vez que grande parte dos defeitos metalúrgicos no lingotamento contínuo está ligada ao comportamento térmico do lingote, os quais decorrem de condições de resfriamento impróprias. A intensificação na utilização de diferentes técnicas de modelagem da solidificação de metais em processos industriais de LC, com ênfase em pacotes comerciais, tem sido notável tanto no controle do processo como na otimização da produção na busca de melhor qualidade dos produtos e redução de custos. O presente trabalho apresenta o desenvolvimento de modelos matemáticos bidimensionais em geometrias cartesianas e cilíndricas, para uma análise teórica da solidificação no processo de LC, envolvendo tarugos quadrados, placas, tarugos circulares maciços e vazados (tubos). A simulação numérica do processo de solidificação só pode produzir informação confiável se as condições de contorno do modelo matemático, como o coeficiente de transferência de calor na interface metal/molde, e as propriedades termofísicas do aço forem conhecidas com precisão. Os resultados obtidos pelo modelo em geometria cartesiana de LC foram comparados com resultados de outros modelos existentes na literatura e apresentaram ótima consonância. O modelo em geometria cartesiana foi aplicado na análise de situações de operações reais de uma máquina de LC. Nesse modelo também foram utilizados algoritmos evolutivos de inteligência artificial associados a um método de cálculo inverso de transferência de calor para otimizar a busca por condições de resfriamento a partir de temperaturas superficiais conhecidas. Os modelos em geometria cilíndrica de LC ¿ para tarugos maciços e tubos ¿ foram adaptados para a condição de lingotamento estático para efeito de validação com resultados experimentais da literatura. O modelo em geometria cilíndrica foi aplicado para analisar a influência da geometria na evolução da camada solidificada e na evolução da temperatura superficial de tarugos maciços com área circular transversal equivalente a de um tarugo de seção quadrada em uma real máquina de LC. No caso de LC de tubos, o modelo em geometria cilíndrica foi aplicado para dimensionar as condições de resfriamento de uma máquina de LC de tubos de grandes dimensões de um aço API (American Petroleum Institute), de interesse da indústria petroquímicaAbstract: The solidification of metallic components by continuous casting (CC), with minimization of defects, is a constant challenge for the metallurgical industry in search of competitiveness in the industrial market. One of the main approaches used in the planning of CC operation, in particular in the steel industry, is the use of heat-transfer solidification models since metallurgical defects are associated with the casting thermal behavior, which are usually related to inappropriate cooling conditions. The increasing use of different modeling techniques for dealing with solidification of metals in industrial CC processes, in particular commercial software packages, has led to remarkable results in both the process control and production optimization in search of quality improvements associated with cost savings. In the present study two-dimensional mathematical models are developed in Cartesian and cylindrical coordinates, to the use in theoretical analyses of solidification during the CC process of square billets, slabs, massive and hollow cylinders (tubes). The numerical simulation can only provide reliable information if accurate boundary conditions of the mathematical model are available, such as the metal/mold heat transfer coefficients and the thermophysical properties of steels. The results that were obtained by the developed Cartesian version of the CC model have been compared with models from the literature and very good agreement has been observed. The mentioned model has been applied to real operating conditions of a CC machine. The model has also been interconnected to evolutionary algorithms of artificial intelligence with a view to permitting an inverse heat conduction problem technique to be applied in the search of optimized cooling conditions based on experimental surface temperatures. The developed model for cylindrical geometries in CC - massive cylinders and tubes- has been adapted for the condition of static castings for validation against results from the literature. This model has also been applied to analyse the effect of geometry on the evolution of solidifying thickness and casting temperature of massive cylinders, by comparing a similar cross section with that of an equivalent square billet for real CC machine conditions. In the case of CC of tubes, the developed model has been applied in the design of cooling conditions of a CC machine for the production of large API (American Petroleum Institute) steel tubes used by the petrochemical industryDoutoradoMateriais e Processos de FabricaçãoDoutora em Engenharia Mecânic",[s.n.],Two-dimensional modeling of solidification in the continuous casting process of cartesian and cylindrical geometries,,,,core
289290465,2018-09-12T00:00:00,"The development of novel digital technologies connected to the Internet of Things, along with advancements in artificial intelligence and automation, is enabling a new wave of manufacturing innovation. “Smart factories” will leverage industrial equipment that communicates with users and with other machines, automated processes, and mechanisms to facilitate real-time communication between the factory and the market to support dynamic adaptation and maximize efficiency. Smart factories can yield a range of benefits, such as increased process efficiency, product quality, sustainability, and safety and decreased costs. However, companies face immense challenges in implementing smart factories, given the large-scale, systemic transformation the move requires. We use data gathered from in-depth studies of five factories in two leading automotive manufacturers to analyze these challenges and identify the key steps needed to implement the smart factory concept. Based on our analysis, we offer a preliminary maturity model for smart factory implementation built around three overarching principles: cultivating digital people, introducing agile processes, and configuring modular technologies.fi=vertaisarvioitu|en=peerReviewed",'Informa UK Limited',"Smart factory implementation and process innovation : a preliminary maturity model for leveraging digitalization in manufacturing moving to smart factories presents specific challenges that can be addressed through a structured approach focused on people, processes, and technologies",10.1080/08956308.2018.1471277,https://core.ac.uk/download/289290465.pdf,,core
234970498,2018-03-14T00:00:00,"The Reporter is a publication produced by Western Carolina University featuring news, events, and campus community updates for faculty and staff. The publication began in August of 1970 and continues digitally today. Click on the link in the “Related Materials” field to access recent issues.THE REPORTER A Weekly Newsletter for the Faculty and Staff of Western Carolina University * Cullowhee, N.C. * November 1, 1976
WCU TEACHER CORPS PIONEERS IN USE OF COMPUTER TO TEACH MATH TO MIDDLE-GRADE CHILDREN
EdLLtoA'A no te,: The WCU TeacheA
ColpA this ifQjxA. kaA IntAodaced
computeA-aA&iAtexl InAtAuction at Log
Cabin ElementaAy School. AI BloeAeA,
TeacheA Coip6 aA&l6tant diAectoA, haA
pAcpaAed thl& Aepoit o& the pAoject.
By Alfred H. Bloeser
Western Carolina University
Teacher Corps
Gone are the days of dull, old
math drill because ""Charlie Computer""
now teaches the class. No more mul­tiplication
tables to recite, or
choral responses to flash cards
because ""Charlie"" has taken it all
on.
You can see the excitement in
the students' eyes at Log Cabin Ele­mentary
School whenever ""Charlie""
begins his daily routine, because,
although ""Charlie"" is a computer
terminal and not a real person, he
does ""talk"" to the boys and girls
in Miss Pennington's and Mrs. Fore's
classes.
""Charlie"" begins a typical day
by being ""logged-in"" by his proctor,
Mrs. Toni Brooks. Through a compli­cated
system of telephone lines, a
computer, located at the Research
Triangle Park, midway between Raleigh,
Durham, and Chapel Hill, is contacted
and ""Charlie"" is readied for a full
day of work.
He asks his first charge of the
day to identify himself and then
calling on his ""memory and record
bank"" produces the appropriate math
lesson with a cheery, ""Hello, Sally.
Today's lesson will be on fractions.
Here we go.""
""Charlie"" has identified the
student, tested her to find the appro­priate
level of study, and monitored
her work so that today's problem will
be just what Sally needs.
Sally, who is fascinated by see­ing
her work on an electronic screen,
concentrates with all her effort to
catch ""Charlie"" asleep, if she can,
for the next ten minutes, and without
being aware of it learns a great deal
about adding and subtracting fractions
It all seems so simple when each step
is monitored and ""Charlie"" is right
there to show her where she makes a
mistake. One of these days she is
going to catch ""Charlie"" in a mistake,
but she needs a great deal of prac­tice
first.
At the end of the week ""Charlie""
reports that Sally has improved a
great deal but that she still makes
errors and needs more work. Perhaps
her teacher can help her with some
personalized instruction.
""Charlie"" is the talk of the Log
Cabin Elementary School, one of the
schools in the Jackson County school
system. ""Computer-assisted instruc­tion""
via the Hewlett-Packard Math
program, has been introduced to this
school through the Eleventh Cycle
Teacher Corps Project at Western
Carolina University.
This program is the only one of
its kind being used in North Carolina
elementary schools, according to Miss
Betsy Little of the North Carolina
Education Computing Service. The
students in the fourth, fifth, and
sixth grades meet ""Charlie"" every day
in order to give them the necessary
experience to raise their.grade ievel
scores in math and are advancing
rapidly.
""Computer-assisted instruction
will be a very challenging and reward­ing
experience,"" Miss Doris Pennington,
fifth- and sixth-grade teacher at Log
Cabin School, recently commented. Her
students are excited and intensely in­terested
in this new method of learn­ing.
Mrs. Patricia Fore, who teaches
fourth and fifth grades at the same
school, stated that she was very ex­cited
about the program, both as a
parent and as a teacher. ""Any child
is fortunate to participate in such a
program and I would love to have my
own boys involved in the math program,""
she said.
""Charlie"" is the special care of
Mrs. Toni Brooks at Log Cabin Elemen­tary
School. She is readily available
to assist any student who may have
some difficulty with the computer ter­minal,
or who may not understand what
is expected of him. She also is re­sponsible
for daily and weekly
reports on each student and keeping in
close contact with the student's tea­cher
so that weaknesses and need for
additional help can be noted.
""Charlie"" is capable of many
things. He can play games with the
students, give them more advanced work
to do, or make available to them many
types of educational programs. He can
be of special help to teachers, too,
by giving them the opportunity to
secure complete lessons, test, etc.,
in advance, as well as furnishing them
with complete records of their stu­dents
.
When students demonstrate compe­tence
in course material, they are
automatically skipped to a higher
level of difficulty or on to new mate­rial.
On the other hand, when a stu­dent
needs drill, appropriate review
lessons are given automatically. The
A REMINDER
The Computer Center is offering an
elementary short course covering enough
of the language and the system commands
to enable the novice user to run BASIC
programs on the Xerox 560. No previous
programming experience is required for
the course, which is being taught by
C. J. Duckenfield.
The course will meet Nov. 1, 3, 5,
and 8 from 3 to 4 p.m. in room 117 of
Forsyth Building.
teacher is kept informed of each
student's progress by a variety of
reports that are supplied on demand.
The Teacher Corps at Western Caro­lina
University, the Jackson County
School System, and the principal and
teachers at Log Cabin School are all
intensely excited over the possibili­ties
of computer-assisted education.
Although this is a pilot program,
there is every indication that it will
prove to be very beneficial to both
students and educators, and may very
well be the forerunner of a ""new""
approach to educating students on
the elementary level.
METROPOLITAN OPERA SOPRANO
TO SING HERE
Soprano Annette Parker will pre­sent
a recital with Lawrence Skrobacs,
pianist, and Lucy Cross, lutenist,
Thursday (Nov. 4) at 8:15 p.m. in
Hoey Auditorium.
The program will include songs
from the ""Italienisches Liederbuch""
of Hugo Wolf, ""Zaide"" by Hector
Berlioz, and ""Fiancailles Pour Rire"" .
by Poulenc. Also on the program are
songs by Dowland, Morley, Philip
Rosseter, Peter Mathews, and Fernando
Obradors.
A native of Charleston, S.C.,
Miss Parker went to New York as a
regional winner of the 1970 Metropoli­tan
Opera Auditions. She was immedi­ately
engaged for two nationwide tours
with the Columbia Operatic Trio, and
was later invited to join the Metro­politan
Opera Studio.
Since then she has performed with
many orchestras, including the Cleve­land
Orchestra, the New Orleans Sym­phony,
and the Opera Orchestra of New
York.
She has sung in operas in
Brussels, St. Paul, New Orleans, Fort
Worth, and at several summer festivals.
Last season she returned to Europe to
sing Eurydice in ""Orfeo"" with the
Netherlands Opera.
Miss Parker received her early
training at Converse College, and did
graduate work at the Villa Schifanoia
in Florence, Italy.
In addition to study grants from
the Metropolitan Opera National Coun­cil
and the Metropolitan Opera Studio,
Miss Parker has received three William
Matheus Sullivan awards and been a
finalist in several national competi­tions.
She made her Carnegie Recital
Hall debut in February of this year,
as the only singer chosen for a Con­cert
Artists Guild Award in the 1975
auditions.
Her performance here is sponsored
by the Lectures, Concerts, and Exhibi­tions
Committee. Admission will be
free to WCU students and subscription
series members of the LCE, $1 for
other adults, and 50 cents for
children.
ARTS AND SCIENCES
ADVISORY GROUP NAMED
John McCrone (dean, Arts and
Sciences) has appointed a Dean's
Student Advisory Committee within the
School of Arts and Sciences. The
Student Advisory Committee consists
of one student selected from each de­partment
in the School. The duty of
this committee is to advise the Dean
on matters of common interest and con­cern
to the students and student body.
The following students were
selected for Arts and Sciences Stu­dent
Advisory Committee for 1976-77:
Bryan McDowell, Art; Alice Bowers,
Biology; Orvilla Smith, Chemistry;
Peter Palmer, Earth Sciences; Susan
Allman, English; Mary Church, History;
Denise Lilley, Mathematics; Vincent
Clark, Physics; Stacy Rae Place,
Military Science; Sandra Crews, Mod­ern
Foreign Languages; Jacqueline
Culpepper, Music; Nancy Hope,
Political Science; Wanda Mull,
Social Work; Rob Daves, Sociology and
Anthropology; and Sam Gray, Speech
and Theatre Arts.
The committee already has held
its first meeting and will meet as
convenient during the quarter to
discuss items of interest and con­cern
to the students.
PIEDMONT BRASS QUINTET TO PERFORM
The Piedmont Brass Quintet, WCU's
resident instrumental ensemble, will
present its first home performance of
the season Nov. 11 at 8:15 p.m. in Hoey
Auditorium.
The program will include six se­lections
representing four centuries.
""Die Bankelsangerlieder"" is an anony­mous
work written around 1684, Hein-rich
Finck's ""Greiner Zanner"" dates
from the late 15th or early 16th cen­tury,
and ""Contrapunctus I"" from J. S.
Bach's ""Art of the Fugue"" is an early
18th century work.
Three 20th century works will
complete the program—Morley Calvert's
""Suite from the Monteregian Hills,""
Ingolf Dahl's ""Music for Brass In­struments""
(1944), and ""Sonatine""
(1951) by Eugene Bozza.
The members of the Piedmont Brass
Quintet are Mary Lazarus and Ned Gard­ner,
trumpet; Gordon Campbell, French
horn; John Woolley, trombone; and John
Sizemore. Each of them is a visiting
artist-in-residence at WCU, and serves
as a brass instructor in the Depart­ment
of Music.
The group's musical director is
Mary Lazarus.
The quintet was organized three
years ago in Winston-Salem, and chose
the name ""Piedmont"" to honor the peo­ple
of the region whose support
nurtured the group during its early
years.
The performance here is sponsored
by the Department of Music. There
will be no admission charge.
""HOT L BALTIMORE"" TO RUN HERE
""Hot 1 Baltimore,"" the Lanford
Wilson comedy that won the New York
Critics Circle Award, will be the Uni­versity
Players' next production. It
will be performed at 7:30 p.m. Novem­ber
11-13 and 15-18 in the Little
Theatre in Stillwell Science Building.
Stephen L. Carr, instructor in
speech and theatre arts, will direct
the production with the assistance of
Mike Gundy of High Point.
The play is set in the lobby of
the run-down Hotel Baltimore, so seedy
that the letter ""e"" is missing from
its marquee. The everyday encounters
of the hotel residents during the
course of one day form a mosaic of
human comedy.
Members of the cast include Beth
Thomas, Cathy Dixon, and Dennis West
of Cullowhee, David Dorsey of Sylva,
Carolyn Fulton of Franklin, Vanessa
Drake of Hendersonville, Tom Caudle of
Waynesville, Ken Stikeleather of Wax-haw,
Frank Joyner and Susan Davis of
Winston-Salem, Dan Spence of Roanoke
Rapids, Diana Marshall of Pfafftown,
Tonya Lamm of Wilson,' Tim Dickenson of
Greensboro, and Armando Erba of New
Haven, Conn.
Tickets are $2 for adults, $1 for
students, and 50 cents for children.
All seats are reserved, and tickets
are available from the Department of
Speech and Theatre Arts, telephone
293-7491.
MADRIGAL DINNER TICKETS
ON SALE MONDAY
The Madrigal Christmas Dinners,
a popular holiday entertainment at
WCU from 1970 through 1973, will be
resumed this year. Dinners will be
held in the Grandroom of Hinds Uni­versity
Center Nov. 30 and Dec. 1.
Admission will be $5 and will include
the cost of dinner and the musical
entertainment.
Tickets for this year's dinners
will go on sale Monday (Nov. 8) at 9
9 a.m. at the information desk of
Hinds University Centfer. Seating is
limited, and no reservations will be
accepted before Monday at 9 a.m. The
dinners have traditionally been sell­outs.
The Grandroom will accommodate
340 patrons each night. As in pre­vious
years, there will be 10 seats
to a table, and individuals may
purchase tickets singly or in groups.
For purposes of the madrigal
dinners, the Grandroom becomes an
Elizabethan banquet hall. The dinners
are served with wassail bowl, boar's
head, trumpet fanfares, and other
pageantry in the manner of the 16th
century.
The dinners have been cancelled
for the past two years because final
examinations were held the week after
Thanksgiving, when the dinners would
normally have been scheduled.
i/
KOINONIA HOUSE LUNCHEON
A ""Soup and Sandwich"" Luncheon
will be held from 12 until 12:5fr*p.m.
each Tuesday through November 16 at
the Koinonia House (Baptist Student
Union). A period of informal dia­logue
and sharing between faculty and
students will be held along with the
meal. Persons wishing to attend may
call 293-9030 from 1:00 p.m. Tuesday
until 5:30 p.m. the following Monday
to make reservations at the rate of
$1 per person.
The speaker for next Tuesday
(Nov. 9) will be Dr. Lewis Sutton,
and the luncheon will feature beef
or pork roast.
WITH THE FACULTY
Thomas B. Westcott (CAP Center)
recently attended the Southeastern
Conference of Counseling Center Per­sonnel
at Appalachian State University.
Or. Westcott presented a paper
entitled ""New Relationships for Ser­vice
to the Campus Community: The
Counseling, Advising and Placement
Center at Western Carolina University.""
The Department of Information
Systems hosted the North Carolina
Association for Educational Data Sys­tems
at A-B Tech in Asheville on
October 18. Drs. Beegle, Hunter,
Morton, and Woods attended the
conference. Dr. Hunter was elected
to a three-year term with the Board
of Directors. Dr. John Bennett
(Philosophy and Religion) spoke at
the luncheon on ""Colonel Elliott
White Springs: Industrial and
Literary Maverick.""
Bob Mason (School of Nursing and
Health Sciences) participated in a
workshop at the University of Kentucky
in Lexington during the week of
October 17. The workshop was designed
to instruct participants on the
development and implementation of
""Interdisciplinary Continuing Educa­tion
Programs in Allied Health.""
Wilma B. Cosper (head, Home
Economics) was elected Vice-Chairperson
of the Southwestern Child Development
Commission Board at a recent meeting
in Franklin.
Tom O'Toole (History) attended a
Guided Designs System Approach Work­shop
sponsored by the Exxon Education
Foundation Impact Program October 13-
15 at West Virginia University.
Bob Mason (Nursing and Health
Sciences) attended the statewide
meeting of the North Carolina Area
Health Education centers in Boone on
October 27 and 28 where he presented
a paper entitled ""AHEC and the Commu­nity
College.""
Donald Josif (Earth Sciences)
was coordinator for the North Carolina
Geographical Society's annual meeting,
October 22-23, in Asheville. WCU and
ASU were co-hosts for the meeting.
THE REPORTER A Weekly Newsletter for the Faculty and Staff of Western Carolina University * Cullowhee, N.C. * November 10, 1976
PRESENTATION OF DISTINGUISHED SERVICE, OTHER AWARDS MARKS 1976 HOMECOMING
The vice president of IBM's
European marketing and services opera­tions
and the president-elect of the
North Carolina Association of Educa­tors
won Western Carolina University's
Alumni Distinguished Service awards
Saturday night.
Raymond H. Fentriss of Paris,
France, a 1956 graduate, received the
award for service outside the field
of education and Mrs. Linda Israel
Rader of Gastonia, a Canton native
and a 1958 graduate, received the
award for service in the field of
education.
The presentations were made by
Dr. H. F. Robinson, chancellor, at
the Alumni Association annual
awards banquet in the Waynesville
Country Club.
The banquet climaxed the univer­sity's
1976 Homecoming celebrations
that included the selection of Miss
Melanie Ann Shore of Peachtree City,
Ga., as homecoming queen. She was
crowned by Chancellor Robinson at the
half-time of the WCU-Lenoir-Rhyne
football game. Other members of the
court afe Kimber'ly Cockman of Brown
Summit, Andrea Zaher of Pembroke
Pines, Fla., Allison Miller of Rock­well,
Patricia Mitchum of Monroe and
LeAnne West of Greenville (N.C.).
Bob Terrell, associate editor of
the Citizen-Times and well-known
columnist and author, a member of the
WCU class of 1951, spoke at the
evening banquet, recounting many of
the humorous experiences he has had
as a newspaperman.
Two other members of the 1951
class shared the podium with Terrell—
Herbert L. Hyde, Asheville attorney,
as master of ceremonies, and Charles
West of CBS news in New York, who was
in charge of presenting other special
awards.
Fentriss, who with his wife,
Jane, also a WCU graduate, came from
Paris for the occasion, joined IBM
the year he graduated and moved up
rapidly in the corporation. Four
years after starting with IBM as a
sales trainee, he was singled out by
A. K. Watson, president of IBM World
Trade Corporation to become his
administrative assistant.
In a series of appointments,
Fentriss became regional manager of
IBM operations in Germany, Switzer­land
and Austria, and was picked to
manage IBM's entry into the Eastern
European Socialist countries,
tice first.
In 1972, when the United States
and Russia embarked on ""detente,""
Fentriss began a study of the USSR
market and the next year was named to
head up the corporation's business
efforts in the Soviet Union. About a
year ago, he moved back to Paris to
assume the vice presidency in Europe.
Mrs. Rader, who earned both her
bachelor's and master's degrees at WCU,
has taught in the Gaston County and
Gastonia school systems since her
graduation. As president-elect of the
state's major educational asso­ciation,
she will take over its lead­ership
in April, 1977.
In 1962, she was a Fulbright
Exchange Program teacher in England
and later was chosen by the National
Education Association as one of the
outstanding women in education in the
United States, receiving that honor
for her role in elevating the stand­ards
of the teaching profession.
She has been a member of the
NCAE board of directors, member of
the General Assembly's special com­mittee
to study the teacher's retire­ment
system, a member of the Governor's
Study Committee on Relationships
Between Boards of Education and
Professional Educators, a member of the
State Board of Education Committee to
Study Sick Leave and Substitute
Teacher Regulations, a member and vice
chairman of the NCAE Professional
Rights and Responsibilities Commission,
vice president of the North Carolina
Association of Classroom Teachers,
president of the Gastonia Association
of Childhood Education, and a delegate
to the NEA national convention.
Two special awards made at the
banquet went to the alumni chapter
with the most members present from
the farthest distance—Florida
chapter—and the individual alumnus
traveling the greatest distance (other
than Fentriss) to attend the meeting—
Dr. Robert Failing of Santa Barbara,
Calif.
An additional ""Unsung Hero"" award
was presented to Doug Reed, WCU
Director of Public Information, for
selfless and dedicated"" service to
the alumni association.
Theme for this year's on-campus
homecoming parade and other activities
was ""Catstruction '76,"" in recognition
of several multi-million dollar build­ing
projects currently under way.
Professor Sue Fields was chairman of
the homecoming committee and alumni
activities were planned under the
direction of James A. Ballard, director
of development and alumni affairs, and
two staff members, Mrs. Jean Robinson
and Miss Toni Knott.
WCU'S HONORS PROGRAM—A FIRST REPORT
Most people, especially students,
would agree that it's hard to get pro­fessors
and students to agree.
But in a few special classes at
Western Carolina University, the stu­dents
are doing more talking, the
professors are doing more listening,
and both agree that the students are
coming away with more knowledge than
is usually gleaned in traditionally
structured courses.
These honors courses are five
beginning classes for freshmen in
chemistry, biology, philosophy, anth­ropology,
and history.
Dr. Ellerd M. Hulbert, interim
chairman of the honors faculty who is
teaching the history honors class
this quarter, said that one of the
reasons the honors program was begun
at WCU was the quality of students.
""Western Carolina is getting a
substantial number of high quality
students from high schools,"" the
history professor said, ""and we have
to insure that they are working at
their capacity.""
The honors students were invited
to take any of the courses on the
basis of their high SAT scores or
class rank.
Although Dr. Hulbert said it was
too early to appraise the effects of
the program, the faculty is encour­aged
and plans are being made to offer
upper level honors courses in subse­quent
years. Next year the honors
courses will be offered as the
present freshmen become sophomores,
so that in four years honors pro­grams
will be available during all
four years of undergraduate work.
Three of the five courses have a
second part—chemistry, biology, and
history—and the second part of the
sequence will be offered during the
winter quarter. Instead of anthro­pology,
an introductory sociology
honors course will be offered in the
winter. In the spring quarter,
honors courses in other departments
will be offered.
Dr. Hulbert said that WCU is
exploring the possibility of offer­ing
honors courses not only in the
School of Arts and Sciences but in
the economics and psychology depart­ments.
What makes the honors programs
different from other classes?
Dr. Patrick Morris, head of the
Department of Sociology and Anthro­pology,
said his class differs in at
least three ways.
""The class is smaller, for one
thing,"" he explained, ""and most of
the classroom time is spent in dis­cussion
rather than lecture.""
Dr. Morris also said there is
much more work for students in his
honors class to do.
He said his main objective in
the course i","Hunter Library Digital Collections, Western Carolina University, Cullowhee, NC 28723;","The Reporter, November 1976",,,,core
148791234,2018-04-11T00:00:00,"Evolutionary Computation (EC) has been an active research area for over 60 years, yet its commercial/home uptake has not been as prolific as we might have expected. By way of comparison, technologies such as 3D printing, which was introduced about 35 years ago, has seen much wider uptake, to the extent that it is now available to home users and is routinely used in manufacturing. Other technologies, such as immersive reality and artificial intelligence have also seen commercial uptake and acceptance by the general public. In this paper we provide a brief history of EC, recognizing the significant contributions that have been made by its pioneers. We focus on two methodologies (Genetic Programming and Hyper-heuristics), which have been proposed as being suitable for automated software development, and question why they are not used more widely by those outside of the academic community. We suggest that different research strands need to be brought together into one framework before wider uptake is possible. We hope that this position paper will serve as a catalyst for automated software development that is used on a daily basis by both companies and home users",'Institute of Electrical and Electronics Engineers (IEEE)',Is Evolutionary Computation evolving fast enough?,10.1109/MCI.2018.2807019,https://core.ac.uk/download/148791234.pdf,,core
478685821,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",'Informa UK Limited',Digitalization of the power business: How to make this work?,,,,core
160668560,2018-01-01T00:00:00,"Everyone’s talking about machine learning, and not long ago, we spent significant time setting up and building tools from scratch before data capturing and processing begins for ML. We’ve seen revolutionary changes in the hardware and software that are making ML accessible for any developer or data scientist. I'll step through the tools Google Cloud provides including data storage, data processing, APIs and TensorFlow. You'll have an understanding of what options exist in any cloud platform and real world applications.

About the speaker

Melanie Warrick is a Developer Relations Engineer for ML and the Cloud at Google and speaks frequently about AI and ML including co-hosting the weekly GCP Podcast. She was a founding engineer on a Java based deep learning platform prior to Google and worked on machine learning engineering at Change.org. In another life she had a comprehensive consulting career leading teams both domestic and international and before that she was working behind the scenes in the film industry. </p",,Machine Learning with Google Cloud,,,,core
151247716,2018-06-04T00:00:00,"It is common practice for developers of user-facing software to transform a
mock-up of a graphical user interface (GUI) into code. This process takes place
both at an application's inception and in an evolutionary context as GUI
changes keep pace with evolving features. Unfortunately, this practice is
challenging and time-consuming. In this paper, we present an approach that
automates this process by enabling accurate prototyping of GUIs via three
tasks: detection, classification, and assembly. First, logical components of a
GUI are detected from a mock-up artifact using either computer vision
techniques or mock-up metadata. Then, software repository mining, automated
dynamic analysis, and deep convolutional neural networks are utilized to
accurately classify GUI-components into domain-specific types (e.g.,
toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates
a suitable hierarchical GUI structure from which a prototype application can be
automatically assembled. We implemented this approach for Android in a system
called ReDraw. Our evaluation illustrates that ReDraw achieves an average
GUI-component classification accuracy of 91% and assembles prototype
applications that closely mirror target mock-ups in terms of visual affinity
while exhibiting reasonable code structure. Interviews with industrial
practitioners illustrate ReDraw's potential to improve real development
workflows.Comment: Accepted to IEEE Transactions on Software Engineerin",,"Machine Learning-Based Prototyping of Graphical User Interfaces for
  Mobile Apps",,http://arxiv.org/abs/1802.02312,,core
196236580,2018-01-01T00:00:00,"The unprecedented proliferation of miniaturized sensors and intelligent communication, computing, and control technologies have paved the way for the development of the Industrial Internet of Things. The IIoT incorporates machine learning and massively parallel distributed systems such as clouds, clusters, and grids for big data storage, processing, and analytics. In IIoT, end devices continuously generate and transmit data streams, resulting in increased network traffic between device-cloud communication. Moreover, it increases in-network data transmissions. requiring additional efforts for big data processing, management, and analytics. To cope with these engendered issues, this article first introduces a novel concentric computing model (CCM) paradigm composed of sensing systems, outer and inner gateway processors, and central processors (outer and inner) for the deployment of big data analytics applications in IIoT. Second, we investigate, highlight, and report recent research efforts directed at the IIoT paradigm with respect to big data analytics. Third, we identify and discuss indispensable challenges that remain to be addressed for employing CCM in the IIoT paradigm. Lastly, we provide several future research directions (e.g., real-Time data analytics, data integration, transmission of meaningful data, edge analytics, real-Time fusion of streaming data, and security and privacy)",'Institute of Electrical and Electronics Engineers (IEEE)',Big Data Analytics in Industrial IoT Using a Concentric Computing Model,10.1109/MCOM.2018.1700632,,,core
293625039,2018-01-01T00:00:00,"Estratégias de monitoramento, baseadas na análise da condição de equipamentos utilizando ferramentas de processamento digital de sinais, inteligência artificial e tolerância a falhas, tornam-se cada vez mais necessárias nos processos industriais. As técnicas de manutenção inteligente conferem confiabilidade, disponibilidade e eficácia, e são estudadas, neste trabalho, no atual estado da arte. Porém, grande parte delas utiliza medidas com estados e parâmetros do processo que são dispendiosas e envolvem elevado tempo de amostragem e análise. O objetivo deste trabalho é desenvolver um novo sistema capaz de estimar a condição de saúde de um equipamento a partir das leituras de vibração e torque de sensores, e assim, viabilizar a detecção, predição e identificação de falhas online em atuadores elétricos utilizados em linhas de transporte de petróleo e/ou derivados. Para isso, foi desenvolvida uma técnica que, por meio de um dispositivo computacional, possibilita monitorar, considerando ruído e, de forma interativa, as variações dos parâmetros de um processo físico, tais como: falhas abruptas, incipientes e intermitentes. Isso corresponde às atividades de detecção, identificação de falhas e previsões sobre possíveis problemas que venham a surgir em consequência de pequenos desvios do comportamento normal do sistema. A metodologia empregada é baseada na estrutura do modelo Open Systems Architecture for Condition-Based Maintenance (OSA-CBM), que permite atuar nas seguintes camadas: 1) Aquisição de dados; 2) Manipulação de dados; 3) Monitoramento das condições; 4) Avaliação da saúde O sistema compreende a análise simultânea das propriedades de tempo e frequência do sinal, extração de características e filtragem adaptativa. Uma bancada de testes foi utilizada para reproduzir algumas falhas típicas que podem causar degradação na operação de atuadores fabricados no mercado. O sistema foi denominado Fault Detection System (FDS) e é baseado em técnicas de processamento de sinais que tem como saída um sinal de resíduo ou erro quando na ocorrência de uma falha correspondente nos equipamentos monitorados. A versão em software do sistema foi registrada no Instituto Nacional da Propriedade Industrial (INPI) no ""BR 51 2016 000863-6"". Uma nova versão para prototipagem em hardware do FDS em conjunto com um bloco auxiliar denominado Fault Detection Index (FDI), que também é proposto neste trabalho, foi desenvolvido na linguagem Verilog e implementado utilizando uma biblioteca Complementary Metal-Oxide-Semiconductor (CMOS) de 90 nm visando baixo consumo de energia ( 654 μW), baixa utilização de área em silício ( 0, 14 mm2) e processamento em tempo real. Os resultados demonstram a eficácia do método de detecção, diagnóstico e identificação de falhas apresentadas em atuadores elétricos empregados para controle de válvulas.Monitoring strategies based on the analysis of equipment condition with information derived from digital signal processing, artificial intelligence and fault tolerance tools become increasingly necessary in industrial process. In this context, intelligent maintenance techniques provide reliability, availability and are being increasingly studied in the current state of the art researches. However, most of them are based on measurements with states and process parameters that are costly and involve high sampling and analysis time. In order to avoid this problem, this work presents a new system capable of estimating the health condition of an equipment from the vibration and torque measurements of sensors, thus enabling online detection, prediction and identification of faults in electric actuators. The developed system represents a technique that, by means of a computational device, allows to monitor the variations of the parameters of a physical process such as abrupt, incipient and intermittent failures. This corresponds to the activities of fault detection, identification and prediction of possible problems that may arise due to minor deviations of the normal behavior state of the system. The methodology is based on the Open Systems Architecture for Condition-Based Maintenance (OSA-CBM) framework, which allows to act in the following layers: 1) Data acquisition; 2) Data manipulation; 3) Condition monitoring; 4) Health assessment. The system comprises the simultaneous analysis of signal time and frequency properties, feature extraction and adaptive filtering A testbench structure has been used to reproduce some typical faults that can cause degradation in the operation of the available commercial actuators. The results show the effectiveness of the method of detection, diagnosis and identification of faults that may occur in electric valves. The system is denominated Fault Detection System (FDS) and it is based on digital signal processing techniques producing a residue signal or error in the occurrence of a corresponding fault in the monitored equipment. A software version of the system was registered with the Instituto Nacional da Propriedade Industrial (INPI) no ""BR 51 2016 000863-6"". A new version for hardware prototyping of FDS together with the Fault Detection Index (FDI), which is also proposed in this work, was using Ver- ilog language and implemented in a 90 nm Complementary Metal-Oxide-Semiconductor (CMOS) library for low power consumption ( 654 μW), low silicon area utilization ( 0.14 mm2) and real time processing. The results demonstrate the effectiveness of the method of detection, diagnosis and identification of faults present in electric actuators used for controling fluidic valves",,Desenvolvimento de um sistema em chip de processamento online para manutenção inteligente,,,,core
187762998,2018-01-01T00:00:00,"Handling multiple number of objectives in industrial optimization problems is a regular affair. The journey of development of evolutionary algorithms for handling such problems occurred in two phases. In the first phase, multi-objective optimization algorithms are developed that worked quite satisfactorily while finding Pareto set of solutions for two to three objectives. However, their success rates for finding the Pareto optimal solutions for higher number of objectives were limited which triggered the development of different sets of evolutionary algorithms under the name of many-objective optimization algorithms. In this work, we intend to compare the performance of these two classes of algorithms for an industrial hot rolling operation from a real-life steel plant. Several process, chemistry and geometry related parameters are modelled to yield different mechanical properties such as % elongation, ultimate tensile strength and yield strength of final hot rolled steel product through data-based techniques such as artificial neural networks (ANN) . Using this ANN model, the mechanical properties are maximized to obtain the Pareto trade-off solutions using both non-dominated sorting genetic algorithms II (NSGA-II) and many-objective evolutionary algorithm decomposition and dominance (MOEA/DD) and their solutions are compared using a suitable metric for identifying the extent of convergence and diversity. This kind of Pareto set provides a designer with ample of alternatives before choosing a solution for final implementation",'Springer Science and Business Media LLC',Comparative Study of Multi/Many-Objective Evolutionary Algorithms on Hot Rolling Application,10.1007/978-3-030-01641-8_12,,,core
301258141,2018-01-01T00:00:00,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.Peer Reviewe",'Elsevier BV',Incorporating negative information to process discovery of complex systems,10.1016/j.ins.2017.09.027,,"[{'title': 'Information Sciences', 'identifiers': ['issn:0020-0255', '0020-0255']}]",core
146482298,2018-01-17T00:00:00,"Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads",,"The Case for Automatic Database Administration using Deep Reinforcement
  Learning",,http://arxiv.org/abs/1801.05643,,core
250086447,2018-03-12T16:25:13,"Neste trabalho é proposta uma metodologia para construção de sensores virtuais

implementados em software, com objetivo de estimar e prever o comportamento de impurezas na corrente de base de uma coluna de destilação de alta pureza, do processo produtivo do 1,2 Dicloroetano (C2H4Cl2). A aquisição dos dados utilizados na construção dos sensores virtuais foi realizada através do modelo matemático do processo, simulado com dados reais de uma planta industrial. O estudo específico engloba a modelagem matemática/termodinâmica e avaliação do comportamento estacionário e dinâmico dessa torre, simulada aqui no software Aspen Plus e DynamicsTM. Desse modo, o modelo fornece os dados necessários para inferência das impurezas relacionadas, que são os teores dos compostos tetracloreto de carbono (CCl4) e clorofórmio (CHCl3), ambos devem ser mantidos, respectivamente, em valores ≤3000 e ≤400 ppm (partes por

milhão). A metodologia também aborda dois algoritmos de seleção de variáveis secundárias, que utilizam técnicas estatísticas multivariadas (algoritmo de todas as regressões possíveis-TRP e da análise de componentes principais-PCA). Verifica-se também nos dados gerados quanto a real ou não necessidade de remoção de erros grosseiros (outliers), por isso é também inserida na metodologia uma etapa de pré-processamento de dados. Foram selecionados os dez melhores modelos de inferência para cada uma das saídas. Diante dessa informação, os melhores modelos

produzidos não utilizavam as concentrações dos compostos das correntes de alimentação e sim medições de temperaturas ao longo da torre. Uma importante conclusão do ponto de vista de construção de sensores virtuais, porque na maioria dos trabalhos desenvolvidos essas variáveis são cruciais na produção de bons resultados. O treinamento dos sensores virtuais foi efetuado em um ambiente ruidoso, haja vista que foram simulados ruídos inerentes às medições (ruídos brancos Gaussianos). Na etapa final, os sensores virtuais são construídos utilizando uma técnica de modelagem empírica, redes neurais artificiais (RNA), onde foram utilizadas RNA do tipo Perceptron Multicamadas (MLP). Foram também avaliadas diversas variações quanto ao número

de neurônios e camadas ocultas das RNA, empregando como critério de parada a técnica de validação cruzada. Os sensores virtuais desenvolvidos apresentaram erros satisfatórios do ponto de vista de engenharia, uma boa análise de regressão e um bom erro médio quadrático. Logo, com essas estimativas espera-se a minimização e a previsão do comportamento transiente dos compostos no referido processo.This work proposes a methodology for software implementation to make soft sensors.

The goal is to estimate and predict the behavior of impurities in the bottom current of a highpurity distillation column, for 1,2-Dichloroethane or 1,2-DCE (C2H4Cl2) production. The data acquisition used in the construction of soft sensors was performed through a mathematical simulation of the process, with real industrial data taken from an industrial plant. A specific study involves the mathematical modeling, thermodynamics, evaluation of the steady state and the dynamic behavior of this process, simulated here in Aspen Plus and Aspen DynamicsTM software. Thus, the model provides the necessary data to infer the contents of the carbon impurities mentioned above, Tetrachloride (CCl4) and Chloroform (CHCl3), which are to be fixed approximately below 3000 and 400 ppm (parts per million) respectively. The methodology also covers selection algorithms of secondary variables, using multivariate statistical techniques: All Possible Regressions (TRP) and Principal Component Analysis (PCA). The data generated was checked in order to know whether to include or not a step for removal of outliers, so it was also included in the methodology one preprocessing data step. The ten best inference models

were selected for each output concentration. With this information, these models do not use concentrations measurements in the feed streams but measurements of the temperature along the column. This is an important conclusion from the point of view of virtual sensors building, because in most of the literature reported these variables are crucial in getting good results. The training of soft sensors was done in a noisy environment, considering that simulated noise was inherent to measurements (Gaussian noise). In the final step, the soft sensors devices are constructed using an empirical modeling technique of artificial neural networks (ANN), which were generated ANN type Multilayer Perceptron (MLP). Several variations were also evaluated on the number of neurons and hidden layers of networks, employing as a stopping criterion the

cross-validation technique. The developed soft sensors presented satisfactory errors from the engineering viewpoint, a good regression and a good mean square error. Finally, with these estimations it is expected to minimize and predict the transient behavior of the compounds in the referred process",UFCG,Use of soft sensors to estimate impurities in high purity distillation columns.,,,,core
390249735,2018-08-01T00:00:00,"International audienceThe role and function of maintenance is today subject to a profound transformation. Compared to past developments, it now attains a broader scope. Maintenance decisions are no longer addressing a narrow scope limited to technical and operational considerations. Instead, they are increasingly related to a comprehensive systems engineering approach, linked to extended enterprise and risk management considerations and adopting new technology, management and business perspectives. Hence, maintenance entails different processes and multi-disciplinary approaches, requiring the integration of a wide range of engineering and management-related methodologies, information and communication technologies, management philosophies, models and strategies, as well as considerations on business strategies and models. It is a value adding function for a range of industrial assets and products and is increasingly linked to complex product-service propositions, thus requiring increasing connectivity of involved entities, ranging from low level component and asset interactions, to higher level multi-stakeholder performance and service delivery considerations. This special issue provides evidence of the evolving transformation of the maintenance function, the breadth of activities contributing to it and the increasing impact that emerging information and communication technologies have on enabling such advancements. It comprises 11 contributions, which are revised and extended manuscripts, following the third International Federation of Automatic Control (IFAC) workshop on Advanced Maintenance Engineering Services and Technology (IFAC A-MEST’16) which took place in Biarritz, France, 19–21 October 2016. The contributions included in this issue are a clear reflection of the fact that the research agenda in maintenance engineering and management is currently integrating new topics and transforming traditional ones in view of the new challenges and needs for multi-disciplinary approaches.The first two papers deal with different perspectives of risk management in maintenance services and production management.    Liu et al. propose a new modelling technique based on an extended Bayesian approach to tackle uncertainty and risk modelling challenges raised by large-scale and complex real-world systems. The Extended Object-Oriented Bayesian Networks (EOOBNs) approach taken adopts object orientation to enable the required flexibility in modelling scenarios for such complex systems, while retaining the same modelling structure for the entities of the modelled network. Adaptation mechanisms are included to take into account system dynamics when modelling machinery state evolution and its impact on productivity. The adoption of the EOOBN is illustrated on modelling a selected industrial process, that is, cement manufacturing. Overall, the modelling approach may facilitate risk management and decision-making in large-scale and complex systems.    Erguido et al. provide a methodological approach for risk-based optimisation of after-sales maintenance services. The approach is concerned with the implementation of opportunistic maintenance strategies to identify a range of solutions, which represent different trade-offs between service level and costs. The multi-objective optimisation is performed by a variation of the popular non-dominated sorting (multi-objective) genetic algorithm NSGA-II. Simulation modelling is employed to handle the stochastic nature of processes involved in after-sales maintenance services and to evaluate the different strategies. Based on different considerations from the original equipment manufacturers’ (OEMs) and users’ needs and the analysis of the impact of uncertainty sources on the after-sales services, the proposed approach enables to assess the economic risk of the services to be offered. The whole system modelling is illustrated in a wind energy case study.The transformation of the maintenance function in the context of asset and production management is the prime focus of the next four papers.    Roda and Macchi propose a framework to integrate asset management (AM) in production companies. Taking into account concepts from the ISO 55000 AM standard, the framework consists of two dimensions, the asset life cycle and the hierarchical level of the asset control activities, and four founding principles namely life cycle, system, risk and asset-centric orientation, as levers relevant for the integration. The framework is first defined based on literature analysis and then confirmed by industrial experts through the development of focus groups. An empirical investigation, through multiple case study analyses in Italy, allows to map the elements of the framework against the real mechanisms in industrial practice, eventually demonstrating the framework’s validity and potential as a support to implement gap analysis on AM practices in production companies. The research findings are interesting for managers and engineers in this context to be aware of the main principles and dimensions required to structure an AM system.    Do et al. investigate the use of energy efficiency (EE) as a key performance indicator for condition - based maintenance (CBM). Therefore, an EE-based CBM policy is developed. In particular, the energy efficiency indicator (EEI), defined as the amount of energy consumption to produce one output unit, is adopted for preventive maintenance decision-making: the EEI level, acting as the representative of the system heath condition, is monitored at regular inspection times and is used to trigger preventive maintenance actions. A cost-benefit model, considering both the output performance benefit and energy cost, is also developed and employed to optimise a CBM policy. The approach is demonstrated on a case study of a laboratory scale production line, comparing EE-based CBM with conventional CBM, highlighting the benefits of introducing the EEI indicator.    Internet of things–enabled connectivity is the key enabler for the vision of a Social Internet of Industrial Assets (SIoIAs) by Hao et al. In SIoIA, autonomous assets are proposed to act as collaborating (social) agents. The authors identify the building blocks of the SIoIA and present a general architecture of the proposed social network. Subsequently, two illustrative examples of SIoIA applications are presented. Overall, the paper introduces a framework to improve AM policies, based on collaborating industrial assets, allowing asset managers to have a much broader knowledge of their fleets of well-instrumented assets.    Fasanotti et al. consider the maintenance services of geographically dispersed industrial systems via an innovative viewpoint, proposing an artificial immune system (AIS) agent-based maintenance management approach. Examples of how to apply this exploratory study approach on industrial problems, such as in a wastewater treatment plant and an oil/gas pipeline system, are provided. The study argues about the flexibility of AIS methodologies to be employed for diagnostic and prognostic functionality, coupled with the flexibility of a multi-agent architecture as enabler of a mix of autonomy and distributed processing. The approach has potential in contexts with a large number of devices and assets, located in disparate geographic areas.The next three papers deal with asset-level activities and are especially focussed on inspection, diagnostics, and prognostics and health management (PHM).    Zhang et al. propose a PHM approach for improving the quality of remaining useful life (RUL) predictions of a proton exchange membrane fuel cell (PEMFC). While particle filter–based approaches for PHM are well established in the literature, including for PEMFC, the interest here is in combining this with a model-based approach that takes into account degradation recovery phenomena. Overall, the proposed method is applicable to fuel cell stacks under both stationary and quasi-dynamic operating regimes. The experimental results demonstrate the improvements in the prognostic performance, especially for long-term predictions.    Assaf et al. study the degradation process of complex multi-component systems, with a particular focus on modelling degradation interactions between components. To this end, a general wear model is proposed where the degradation process of a component may be dependent on the operating conditions, the component’s own state, and the state of the other components. This is combined with a methodology to extract health indicators from multi-component systems by means of a time–frequency domain analysis. The approach is demonstrated on a lab-based gearbox-accelerated life-testing platform, studying the impact of old–new component couplings in reduced life expectancy of new, healthy components in the multi-component system.    The paper by Moustakidis et al. introduces a methodology for handling some of the challenges when applying thermography techniques for the inspection of composite structures commonly used for aircraft components. Focusing on lock-in thermography, the authors analyse the needs and introduce an approach for excitation invariant pre-processing of thermographic images. The proposed pre-processing techniques were demonstrated on testing composite samples with pre-determined defects. Whereas, thermography has been widely applied to aircraft component inspections, including composites, extending the applicability of such techniques critically depends on introducing invariances in any image pre-processing and therefore the paper is of added value to a growing body of literature in the field.The final two papers address issues related to maintenance strategies and optimisation.    Kathab et al. develop a new mathematical model for the condition-based selective maintenance problem (CBSMP). The formulation considers components with stochastic degradation, periodic inspection and imperfect maintenance. The proposed approach targets the identification of components which should be maintenance targets, while optimising the level of maintenance actions to be performed within time-limited scheduled breaks. Parameters impacting on the operational requirements of the maintained assets, such as the minimum required reliability to achieve the next mission, are taken into account. Simulation based degradation experiments are employed to show the applicability of the proposed model.    Finally, Sera et al. adopt a case study approach to jointly address cost and value in a criticality-based analysis of risks for priority setting in preventive maintenance optimisation. The approach is explained step by step in the case study, dealing with gas network infrastructure. The provided example should be of considerable interest to practitioners, while the paper highlights the need for further work in the direction of identifying the right business and risk drivers for maintenance action prioritisation in a systematic, risk-based framework.The breadth of the thematic coverage of the contributions in this special issue provides evidence that maintenance is becoming a comprehensive and multi-disciplinary business function. It increasingly capitalises on a blend of engineering and management activities, supported by rapidly evolving and maturing technology enablers, with a particular contribution from internet of things, advanced modelling and simulation, as well as various aspects of data analytics and machine learning, all contributing to a vision of smart and interconnected physical assets, as key constituents of the extended production and services ecosystems of the future. We therefore hope that, whether the focus is on component and asset-specific activities or all the way up to global maintenance service delivery and asset life cycle management, these contributed papers will be of added value and inspiration for further advancing the integration of maintenance engineering and management within a broader risk-based and life cycle-based maintenance strategy thinking",'SAGE Publications',"Editorial: August Special Issue on Advanced Maintenance Engineering, Services, and Technology (AMEST)",10.1177/1748006X18788932,,,core
153716693,2018-04-04T09:08:28Z,"<p>Human–robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze–object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The “gaze object sequence” was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments.</p",,Video_1.MP4,10.3389/frobt.2018.00025.s001,,,core
322490268,2018-09-17T00:00:00,"© 2018 The Author(s).This paper presents a methodology to monitor the fatigue life of aerospace structures and hence the remaining allowable fatigue life. In fatigue clearance, conservative load assumptions are made. However, in reality, a structure may see much lower loads and so would be usable for much longer. An example ofthis is air carried guided missiles. In the UK, missiles must be decommissioned after a period of carriage. The implementation of a system that can monitor the usage of a missile during its time in service is advantageous to the military customer and provides a competitive advantage for the missile manufacture inexport markets where reduced through-life costs, longer in-service lives and increased safety are desired. The proposed methodology provides a means to monitor the service life of a missile. This paper describes how machine learning algorithms can be used with accelerometers to determine loads on a missile structure which would then be used to predict how long the missile has left in service","KU Leuven, Faculty of Arts",A methodology using health and usage monitoring system data for payload life prediction,,,,core
293738204,2018,"Control of nonlinear systems on continuous domains is a challenging task for various reasons.
For robust and accurate control of complex systems a precise model of the system dynamics is
essential. Building such highly precise dynamics models from physical knowledge often requires
substantial manual effort and poses a great challenge in industrial applications. Acquiring a model
automatically from system measurements employing regression techniques allows to decrease
manual effort and, thus, poses an interesting alternative to knowledge-based modeling. Based on
such a learned dynamics model, an approximately optimal controller can be inferred automatically.
Such approaches are the subject of model-based reinforcement learning (RL) and learn optimal
control from interactions with the system. Especially when probabilistic dynamics models such
as Gaussian processes are employed, model-based RL has been tremendously successful and has
attracted much attention from both the control and machine learning communities. However,
several problems need to be solved to facilitate widespread deployment of model-based RL for
learning control in real world scenarios. In this thesis, we address two current limitations of
model-based RL that are indispensable prerequisites for widespread deployment of model-based
RL in real world tasks.
In many real world applications a poor controller can cause severe damage to the system or
even put the safety of humans at risk. Thus, it is essential to ensure that the controlled system
behaves as desired. While this question has been studied extensively in classical control, stability
of closed-loop control systems with dynamics given as a Gaussian process has not been considered
yet. We propose an automatic tool to compute regions of the state space where the desired behavior
of the system can be guaranteed. We consider dynamics given as the mean of a GP as well as
the full GP posterior distribution. In the first case, the proposed tool constructs regions of the
state space, such that the trajectories starting in this region converge to the target state. From this
asymptotic result, we follow statements for finite time horizons and stability under the presence
of disturbances. In the second case the system dynamics is given as a GP posterior distribution.
Thus, computation of multi-step-ahead predictions requires averaging over all plausible dynamics
models given the observations. A a consequence, multi-step-ahead predictions become analytically
intractable. We propose an approximation based on numerical quadrature that can handle complex
state distributions, e.g., with multiple modes and provides upper bounds for the approximation
error. Exploiting these error bounds, we present an automatic tool to compute stability regions. In
these regions of the state space, our tool guarantees that for a finite time horizon the system behaves
as desired with a given probability. Furthermore, we analyze asymptotic behavior of closed-loop
control systems with dynamics given as a GP posterior distribution. In this case we show that for
some common choices of the prior, the system has a unique stationary distribution to which the
system state converges irrespective of the starting state.
Another major challenge of RL for real world control applications is to minimize interactions
with the system required for learning. While RL approaches based on GP dynamics models
have demonstrated great data efficiency, the average amount of required system interactions can further be reduced. To achieve this goal, we propose to employ the numerical quadrature based
approximation to propagate the value of a state. To show how this approximation can further
increase data efficiency, we employ it in the two main classes of model-based RL: policy search
and value iteration. In policy search, the state distribution must be computed to evaluate the
expected long-term reward for a policy. The proposed numerical quadrature based approximation
substantially improves estimates of the expected long-term reward and its gradients. As a result,
data efficiency is significantly increased.
For the value function based approaches for policy learning, the value propagation step is
completely characterized by the Bellman equation. However, this equation is intractable for
nonlinear dynamics. In this case, we propose a projection-based value iteration approach. We
employ numerical quadrature to facilitate projection of the value function onto a linear feature
space. Suitable features for value function representation are learned online without manual effort.
This feature learning is constructed such that upper bounds for the projection error can be obtained.
The proposed value iteration approach learns globally optimal policies and significantly benefits
from the introduced highly accurate approximations",,Gaussian Processes in Reinforcement Learning: Stability Analysis and Efficient Value Propagation,,,,core
186275921,2018-10-10T00:00:00,"Over the past decades, deep learning (DL) systems have achieved tremendous
success and gained great popularity in various applications, such as
intelligent machines, image processing, speech processing, and medical
diagnostics. Deep neural networks are the key driving force behind its recent
success, but still seem to be a magic black box lacking interpretability and
understanding. This brings up many open safety and security issues with
enormous and urgent demands on rigorous methodologies and engineering practice
for quality enhancement. A plethora of studies have shown that the
state-of-the-art DL systems suffer from defects and vulnerabilities that can
lead to severe loss and tragedies, especially when applied to real-world
safety-critical applications. In this paper, we perform a large-scale study and
construct a paper repository of 223 relevant works to the quality assurance,
security, and interpretation of deep learning. We, from a software quality
assurance perspective, pinpoint challenges and future opportunities towards
universal secure deep learning engineering. We hope this work and the
accompanied paper repository can pave the path for the software engineering
community towards addressing the pressing industrial demand of secure
intelligent applications",,"Secure Deep Learning Engineering: A Software Quality Assurance
  Perspective",,http://arxiv.org/abs/1810.04538,,core
200708745,2018-09-05T02:43:27Z,"<div><p>Abstract  Introduction: The interest in Expert systems has increased in the medical area. Some of them are employed even for diagnosis. With the variability of transcatheter prostheses, the most appropriate choice can be complex. This scenario reveals an enabling environment for the use of an Expert system. The goal of the study was to develop an Expert system based on artificial intelligence for supporting the transcatheter aortic prosthesis selection.  Methods: The system was developed on Expert SINTA. The rules were created according to anatomical parameters indicated by the manufacturing company. Annular aortic diameter, aortic area, aortic perimeter, ascending aorta diameter and Valsalva sinus diameter were considered. After performing system accuracy tests, it was applied in a retrospective cohort of 22 patients with submitted to the CoreValve prosthesis implantation. Then, the system indications were compared to the real heart team decisions.  Results: For 10 (45.4%) of the 22 patients there was no concordance between the Expert system and the heart team. In all cases with discordance, the software was right in the indication. Then, the patients were stratified in two groups (same indication vs. divergent indication). The baseline characteristics did not show any significant difference. Mortality, stroke, acute myocardial infarction, atrial fibrillation, atrioventricular block, aortic regurgitation and prosthesis leak did not present differences. Therefore, the maximum aortic gradient in the post-procedure period was higher in the Divergent Indication group (23.9 mmHg vs. 11.9 mmHg, P=0.03), and the mean aortic gradient showed a similar trend.  Conclusion: The utilization of the Expert system was accurate, showing good potential in the support of medical decision. Patients with divergent indication presented high post-procedure aortic gradients and, even without clinical repercussion, these parameters, when elevated, can lead to early prosthesis dysfunction and the necessity of reoperation.</p></div",,Development and Application of a System Based on Artificial Intelligence for Transcatheter Aortic Prosthesis Selection,10.6084/m9.figshare.7045724.v1,,,core
427481819,2018-01-01T00:00:00,"In chapter 1, an introduction to KBC Securities and KBC Markets is provided. It is mentioned that KBC Securities plays a significant role in the Benelux capital markets and has been rewarded with the title of Equity Finance House of the Year and Cash Market Brokerage House of the Year in 2017. Nevertheless, KBC Securities and KBC Markets operate in a changing environment and are facing a series of challenges: new regulation, advances in innovative technology, strong competitors, new entrants and higher demanding clients. As a result, the following research question was formulated: ""How can KBC Securities and KBC Markets stay relevant in the future?"". Additionally, we researched several subquestions such as: ""What are the reasons clients (do not) choose for KBC Securities? What activities do clients expect and want, now and in the future? Are KBC Securities clients satisfied with the service level they receive? How does KBC Securities' client experience compare to competitors?"". In chapter 2, an overview of the selected methodologies is given in order to tackle the research question. This study is exploratory and has an inductive approach using qualitative research in order to fulfil the purpose of the paper. Semi-structured interviews with clients, non-clients, industry experts, academics and employees were chosen as a way to gather empirical data for research. An overview of the interview questions can be found in this chapter. Additionally, we supplemented our findings using secondary research: journals, books, articles and working papers conducted by academics and global professional management firms. In chapter 3, several conceptual strategy frameworks are presented which are used to tackle the research question in a structured way. First, a strategic framework of Bain & Company is given which involves making deliberate choices in three areas: ambition, where-to-play and how-to-win. The main focus of this paper will be on the ""how-to-win"" question. Second, the three value disciplines of Treacy & Wiersema are used to answer the ""how-to-win"" question and it is concluded that ""customer intimacy"" is the appropriate strategy for KBC Securities. This strategy focuses on offering unique customer services that allow tailored solutions to meet different customer demand. Companies who pursue this strategy bundle services and products into a solution designed specifically for the customer's problem. As a result of this strategy, KBC Securities will face a new key challenge: switch from being product-centric (the so called 'product-factory) to being client-centric. Thirdly, a framework of prof. dr. Kurt Verweire is used to implement customer intimacy based on seven building blocks: (1) strategic positioning, (2) customer relationship management (CRM), (3) client segmentation, (4) client profitability, (5) key account management, (6) tracking client satisfaction and (7) a dynamic and proactive approach. In chapter 4, the paper elaborates on the seven building blocks that are needed to achieve customer intimacy: (1) Strategic positioning: KBC Securities should position itself as a local specialist. A local specialist is an investment bank with deep capabilities in individual markets or regions, and strong local knowledge, networks and connectivity. (2) Customer relationship management: In context of achieving customer intimacy, CRM systems are indispensable. Two elements are essential to successfully implement CRM, namely CRM data collection and CRM data usage. The benefits of a good CRM system are listed and several recommendations for KBC Securities are given. (3) Client segmentation: We suggest implementing a value-driven segmentation system based on the client's current value, potential value and behaviour. This results in three client segments: priority, specialised and self service. For each segment, a distinct differentiation approach and clear rules for coverage structure, content access and execution and platform capabilities should be developed. (4) Client profitability: Client relationships should be managed in terms of profitability, not revenues. KBC Securities should consider both client revenues as well the cost-to-serve these clients. (5) Key Account Management (KAM): KAM is an approach to act as a real business partner for clients. This enables to build loyalty, customer commitment and give the client a single point of contact. We believe an appropriate person to fulfil the key account manager's position is the senior corporate banker. Another option would be to develop the function of a key account manager in-house at KBC Securities. This person could then join client meetings with the corporate banker and proactively approach clients of KBC Securities to identify business opportunities. (6) Tracking client satisfaction: We believe it is essential for KBC Securities to track client satisfaction more rigorously. This happens in three simple stages: gather input data, analyse the input data and adjust the services based on the outcome of this analysis. Internally, senior management's involvement and the documenting of outputs are important. (7) Dynamic and proactive approach: Based on client interviews, it seems that KBC Securities sometimes lacks dynamism and proactivity in its business approach when compared to competitors. We elaborate on this concept and emphasise the importance of staying sharp in every business interaction. In chapter 5, we elaborate on some additional findings which are relevant for KBC Securities, but not directly related to implementing customer intimacy. More specifically, it covers the following topics: innovation & technology and some suggestions for starting activities. Concerning innovation and technology, we identify some critical areas where IT investments should be focused on: dealing with legacy IT and customer-centric solutions. We also elaborate on the implementation of a 'Technology Plan' which is based on three elements: a budget, a time-period and an 'IT Innovation Officer'. The IT Innovation Officer should be responsible for developing a cohesive innovation and technology strategy for KBC Securities. Related to the suggestions for starting activities, we elaborate on (1) starting in-house private equity activities, (2) cross-selling in KBC Securities, (3) the organisational set-up of the corporate finance team and (4) convertible bonds. (1) Starting of in-house private equity activities: Having in-house private equity activities might be beneficial for KBC Securities, as well as for KBC bank. Potential advantages could be the realisation of informational synergies, cross-selling opportunities, better serving of the upper wealth management clients, reducing agency problems and a better brand image. Additionally, it would be in line with KBC Securities' vision of serving companies in every stage of their lifecycle. (2) Cross-selling in KBC Securities: We focus on how private, corporate and investment bankers should work together to generate more revenues as a whole. We elaborate on the importance of knowledge training, good incentive systems, alignment of every party involved, proper internal communication systems and the support of senior management to effectively realise cross-selling opportunities within the bank. (3) Organisational set-up of the corporate finance team: For the reason that ECM activities and M&A activities serve different type of clients, require different skills and networks to grow, we believe (after performing research and conducting interviews with industry experts) that it would be better to split up the corporate finance team into two separate teams: an ECM team and a M&A team. (4) Convertible bonds: As KBC Securities currently has no in-house expertise regarding convertible bonds, we want to encourage KBC Securities' management to consider if it is worth starting this activity. Chapter 6 is the conclusion of the In-Company Project. Moreover, the research question which was presented in chapter 1, is answered. KBC Securities should implement customer intimacy as a strategy in order to stay relevant. Furthermore, the following limitations of this project were discussed in this chapter, namely: the limited number of conducted interviews, the limited amount of available academic research regarding the research question and the time constraint. We emphasised that this project is a purely strategic exercise (formulation), with limited focus on strategy execution. As a result, compliance, risk, regulation and operation-related matters are out of the scope of this project. This chapter finishes with some actions for KBC Securities going forward. The formulation of a sound strategy, as presented in this report, was only a first step in becoming a customer intimate bank that puts the client at the centre of everything it does. However, there still is a long way to go. Next steps for KBC Securities should focus on the actual execution and implementation of this strategy across the investment bank, with impact on its day-to-day operations. In chapter 7, additional research is conducted on the sell-side research function. MiFID II will have a major impact on sell-side research firms. Buy-side firms will become much more selective in deciding which research reports to buy and the budgets for research will likely decline, which means quality becomes the key differentiator. Additionally, competition is expected to increase. We conclude that KBC Securities should position itself as a regional champion where they should leverage their home field advantage in the Benelux to compete with other sell-side firms covering the same stocks. Lastly, we conclude that artificial intelligence and technology will be important in the future and that research analysts should focus more on client-facing activities instead of performing basic written research. KBC Securities should take actions to make sure analysts' time is spent on activities that are valued by the client and activities that the client wants to pay for, instead of spending most of their time on less-valued activities",,How can KBC securities and KBC markets stay relevant in the future?,,,,core
151300082,2018-03-06T00:00:00,"Dentro da área da Robótica, Robôs Móveis têm recebido crescente atenção. Robôs móveis se propõem a realizar uma variedade de tarefas mais complexas que seus antecessores, os robôs industriais. Para tal, são necessárias técnicas que lhes permitam interagir de forma efetiva com o ambiente. A parte mais essencial desta interação é o Sistema de Navegação  que é um conjunto de métodos e procedimentos que o robô utiliza para se locomover e encontrar seu caminho no mundo. Infelizmente, as pesquisas até agora têm demonstrado pouco sucesso quando os robôs são submetidos a tarefas do mundo real. Métodos baseados em modelagem matemática são inadequados para os robôs móveis, porque seu ambiente é dinâmico e mutável. Já os métodos que rejeitam inteiramente os modelos do mundo e simplesmente reagem às contingências do ambiente, não conseguem ser escalados para problemas complexos. Apesar destas dificuldades, a natureza parece ter se saído particularmente bem ao dotar animais e seres humanos da capacidade de navegação. Uma abordagem recente é buscar inspiração nela. Esta abordagem é representada pelo estudo dos Mapas Cognitivos - estruturas mentais, encontradas em desde ratos até seres humanos, que permitem registrar fatos e raciocinar a respeito dos espaços. Os Mapas Cognitivos da natureza são implementados em Redes Neurais Naturais - os cérebros. Pesquisadores de computação e engenharia procuram imitá-lo com as Redes Neurais Artificiais. Este trabalho propõe criar um sistema de navegação para robô móvel, inspirado no mecanismo de mapa cognitivo, implementado através de Redes Neurais Artificiais. O objetivo é obter um sistema robusto, capaz de responder as exigências de desempenho presentes em tarefas do mundo real.In the arca of Robotics, Mobile Robots have been receiving increasing attention, mainly because they can perform a larger variety of tasks than their predecessors, the industrial robots. Mobile Robots need techniques that enable them to interact effectively with the environment. The most essential part of this interaction is the Navigation System - the set of methods and procedures they need to walk, and find their way in the world. Unfortunately, research has shown small success when robots are applied to real world tasks. Methods of mathematical modeling are inadequate to mobile robots, because their environment is dynamic and mutable. On the other hand, methods that completely reject world models, and just react to environment c,ontingencies, do not scale well. Despite these difficulties, Nature has been successful in providing animais and human beings with navigational capabilities. A recent approach is to get inspiration from Nature. This approach is represented by the study of Cognitive Maps, which are mental structures found from mice to human beings, that enables the registration of facts and the reasoning about the environment. Nature\'s cognitive maps are implemented in natural neural networks - the brains. Technology tries to imitate them with Artificial Neural Networks. This dissertation proposes to build a mobile robot navigation system, inspired by the cognitive map mechanism, and implemented by Artificial Neural Networks. The aim is to obtain a robust system able to respond to the performance demand of real world tasks","'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",Not available,10.11606/D.55.2018.tde-06032018-105039,,,core
225895087,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data?Digitalization of the power business: How to make this work?publishedVersionNivå",,Digitalization of the power business: How to make this work?,,https://core.ac.uk/download/225895087.pdf,,core
160766925,2018-06-30T00:00:00,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance.Comment: Accepted to appear in Archives of Industrial Engineering Journa",,Harnessing constrained resources in service industry via video analytics,,http://arxiv.org/abs/1807.00139,,core
188689893,2018,"With the onset of Industry 4.0 several technological possibilities are offered in industry such as big data analytics, digital twin and augmented reality. The result is a more digitalised industry where faster and better decisions are possible. In long term this should provide a more reliable production with increased plant capacity and reduced downtime. To succeed with these possibilities a Cyber Physical Systems (CPS) must be established for the company. Currently, an own framework for CPS is under development and is expected to be tailored for Norwegian manufacturing. When building on the principle in Industry 4.0, big data capability with machine learning will be a fundamental model. Nevertheless, Industry 4.0 should also include other models for big data capability such as reliability modelling. The aim in this article is to present the current status of CPS framework and how it could be implemented in manufacturing industries. In particular, the article discusses and demonstrates the balance between machine learning and reliability engineering in big data analytics.publishedVersionPublished by Taylor & Francis. Made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0",Taylor & Francis,Reliability-based Cyber Plant,10.1201/9781351174664,,,core
201537294,2018-06-01T00:00:00,"Monitoring the status of the facilities and detecting any faults are considered an important technology in a smart factory. Although the faults of machine can be analyzed in real time using collected data, it requires a large amount of computing resources to handle the massive data. A cloud server can be used to analyze the collected data, but it is more efficient to adopt the edge computing concept that employs edge devices located close to the facilities. Edge devices can improve data processing and analysis speed and reduce network costs. In this paper, an edge device capable of collecting, processing, storing and analyzing data is constructed by using a single-board computer and a sensor. And, a fault detection model for machine is developed based on the long short-term memory (LSTM) recurrent neural networks. The proposed system called LiReD was implemented for an industrial robot manipulator and the LSTM-based fault detection model showed the best performance among six fault detection models",'MDPI AG',LiReD: A Light-Weight Real-Time Fault Detection System for Edge Computing Using LSTM Recurrent Neural Networks,10.3390/s18072110,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
215391949,2018-04-13T20:15:00,"Accurate speech quality measures are highly attractive and beneficial in the design, fine-tuning, and benchmarking of speech processing algorithms, devices, and communication systems. Switching from narrowband telecommunication to wideband telephony is a change within the telecommunication industry which provides users with better speech quality experience but introduces a number of challenges in speech processing. Noise is the most common distortion on audio signals and as a result there have been a lot of studies on developing high performance noise reduction algorithms. Assistive hearing devices are designed to decrease communication difficulties for people with loss of hearing. As the algorithms within these devices become more advanced, it becomes increasingly crucial to develop accurate and robust quality metrics to assess their performance. Objective speech quality measurements are more attractive compared to subjective assessments as they are cost-effective and subjective variability is eliminated. Although there has been extensive research on objective speech quality evaluation for narrowband speech, those methods are unsuitable for wideband telephony. In the case of hearing-impaired applications, objective quality assessment is challenging as it has to be capable of distinguishing between desired modifications which make signals audible and undesired artifacts. In this thesis a model is proposed that allows extracting two sets of features from the distorted signal only. This approach which is called reference-free (nonintrusive) assessment is attractive as it does not need access to the reference signal. Although this benefit makes nonintrusive assessments suitable for real-time applications, more features need to be extracted and smartly combined to provide comparable accuracy as intrusive metrics. Two feature vectors are proposed to extract information from distorted signals and their performance is examined in three studies. In the first study, both feature vectors are trained on various portions of a noise reduction database for normal hearing applications. In the second study, the same investigation is performed on two sets of databases acquired through several hearing aids. Third study examined the generalizability of the proposed metrics on benchmarking four wireless remote microphones in a variety of environmental conditions. Machine learning techniques are deployed for training the models in the three studies. The studies show that one of the feature sets is robust when trained on different portions of the data from different databases and it also provides good quality prediction accuracy for both normal hearing and hearing-impaired applications",Scholarship@Western,Learning-Based Reference-Free Speech Quality Assessment for Normal Hearing and Hearing Impaired Applications,,https://core.ac.uk/download/215391949.pdf,,core
84092377,2018-09-13T00:00:00,"Click-through rate prediction is an essential task in industrial
applications, such as online advertising. Recently deep learning based models
have been proposed, which follow a similar Embedding\&MLP paradigm. In these
methods large scale sparse input features are first mapped into low dimensional
embedding vectors, and then transformed into fixed-length vectors in a
group-wise manner, finally concatenated together to fed into a multilayer
perceptron (MLP) to learn the nonlinear relations among features. In this way,
user features are compressed into a fixed-length representation vector, in
regardless of what candidate ads are. The use of fixed-length vector will be a
bottleneck, which brings difficulty for Embedding\&MLP methods to capture
user's diverse interests effectively from rich historical behaviors. In this
paper, we propose a novel model: Deep Interest Network (DIN) which tackles this
challenge by designing a local activation unit to adaptively learn the
representation of user interests from historical behaviors with respect to a
certain ad. This representation vector varies over different ads, improving the
expressive ability of model greatly. Besides, we develop two techniques:
mini-batch aware regularization and data adaptive activation function which can
help training industrial deep networks with hundreds of millions of parameters.
Experiments on two public datasets as well as an Alibaba real production
dataset with over 2 billion samples demonstrate the effectiveness of proposed
approaches, which achieve superior performance compared with state-of-the-art
methods. DIN now has been successfully deployed in the online display
advertising system in Alibaba, serving the main traffic.Comment: Accepted by KDD 201",,Deep Interest Network for Click-Through Rate Prediction,,http://arxiv.org/abs/1706.06978,,core
154989865,2018-04-12T00:00:00,"Autonomous sorting is a crucial task in industrial robotics which can be very
challenging depending on the expected amount of automation. Usually, to decide
where to sort an object, the system needs to solve either an instance retrieval
(known object) or a supervised classification (predefined set of classes)
problem. In this paper, we introduce a new decision making module, where the
robotic system chooses how to sort the objects in an unsupervised way. We call
this problem Unsupervised Robotic Sorting (URS) and propose an implementation
on an industrial robotic system, using deep CNN feature extraction and standard
clustering algorithms. We carry out extensive experiments on various standard
datasets to demonstrate the efficiency of the proposed image clustering
pipeline. To evaluate the robustness of our URS implementation, we also
introduce a complex real world dataset containing images of objects under
various background and lighting conditions. This dataset is used to fine tune
the design choices (CNN and clustering algorithm) for URS. Finally, we propose
a method combining our pipeline with ensemble clustering to use multiple images
of each object. This redundancy of information about the objects is shown to
increase the clustering results.Comment: Paper published in International Journal of Artificial Intelligence
  and Applications (IJAIA), March 2018, Volume 9, Number 2 17 pages, 5 figures,
  7 tables. arXiv admin note: text overlap with arXiv:1707.0170",,Unsupervised robotic sorting: Towards autonomous decision making robots,10.5121/ijaia.2018.9207,http://arxiv.org/abs/1804.04572,,core
386153350,2018-02-09T08:00:00,"Inspections are a proven approach for improving software requirements quality. Owing to the fact that inspectors report both faults and non-faults (i.e., false-positives) in their inspection reports, a major chunk of work falls on the person who is responsible for consolidating the reports received from multiple inspectors. We aim at automation of fault-consolidation step by using supervised machine learning algorithms that can effectively isolate faults from non-faults. Three different inspection studies were conducted in controlled environments to obtain real inspection data from inspectors belonging to both industry and from academic backgrounds. Next, we devised a methodology to separate faults from non-faults by first using ten individual classifiers from five different classification families to categorize different fault-types (e.g., omission, incorrectness, and inconsistencies). Based on the individual performance of classifiers for each fault-type, we created targeted ensembles that are suitable for identification of each fault-type. Our analysis showed that our selected ensemble classifiers were able to separate faults from non-faults with very high accuracy (as high as 85-89% for some fault-types), with a notable result being that in some cases, individual classifiers performed better than ensembles. In general, our approach can significantly reduce effort required to isolate faults from false-positives during the fault consolidation step of requirements inspections. Our approach also discusses the percentage possibility of correctly classifying each fault-type",Montclair State University Digital Commons,Validating Requirements Reviews By Introducing Fault-Type Level Granularity:  A Machine Learning Approach,,,,core
132585739,2018-03-15T00:00:00,"Over the years, collaborative mobility proved to be an important but challenging component of the smart
cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of whatif decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment",,A New Modelling Framework over Temporal Graphs for Collaborative Mobility Recommendation Systems,10.1109/itsc.2017.8317664,,,core
478681809,2018-01-01T00:00:00,"With the onset of Industry 4.0 several technological possibilities are offered in industry such as big data analytics, digital twin and augmented reality. The result is a more digitalised industry where faster and better decisions are possible. In long term this should provide a more reliable production with increased plant capacity and reduced downtime. To succeed with these possibilities a Cyber Physical Systems (CPS) must be established for the company. Currently, an own framework for CPS is under development and is expected to be tailored for Norwegian manufacturing. When building on the principle in Industry 4.0, big data capability with machine learning will be a fundamental model. Nevertheless, Industry 4.0 should also include other models for big data capability such as reliability modelling. The aim in this article is to present the current status of CPS framework and how it could be implemented in manufacturing industries. In particular, the article discusses and demonstrates the balance between machine learning and reliability engineering in big data analytics",'Informa UK Limited',Reliability-based Cyber Plant,,,,core
196646001,2018-01-01T00:00:00,"Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-toend transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%.ISBN för värdpublikation: 978-1-5386-4413-3, 978-1-5386-4414-0</p",'Institute of Electrical and Electronics Engineers (IEEE)',Real-time Performance Evaluation of LTE for IIoT,10.1109/LCN.2018.8638081,,,core
186287042,2018-11-06T00:00:00,"One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility",,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots",,http://arxiv.org/abs/1811.02213,,core
162620817,2018-01-01T00:00:00,"This thesis presents a novel synthetic environment for supporting advanced explorations of user interfaces and interaction modalities for future transport systems. The main goal of the work is the definition of novel interfaces solutions designed for increasing trust in self-driving vehicles. The basic idea is to provide insights to the passengers concerning the information available to the Artificial Intelligence (AI) modules on-board of the car, including the driving behaviour of the vehicle and its decision making. Most of currently existing academic and industrial testbeds and vehicular simulators are designed to reproduce with high fidelity the ergonomic aspects associated with the driving experience. However, they have very low degrees of realism for what concerns the digital components of the various traffic scenarios. These includes the visuals of the driving simulator and the behaviours of both other vehicles on the road and pedestrians.  High visual testbed fidelity becomes an important pre-requisite for supporting the design and evaluation of future on-board interfaces. An innovative experimental testbed based on the hyper-realistic video game GTA V, has been developed to satisfy this need. To showcase its experimental flexibility, a set of selected user studies, presenting novel self-driving interfaces and associated user experience results, are described. These explore the capabilities of inducing trust in autonomous vehicles and explore Heads-Up Diplays (HUDs), Augmented Reality (ARs) and directional audio solutions. The work includes three core phases focusing on the development of software for the testbed, the definition of relevant interfaces and experiments and focused testing with panels comprising different user demographics. Specific investigations will focus on the design and exploration of a set of alternative visual feedback mechanisms (adopting AR visualizations) to gather information about the surrounding environment and AI decision making. The performances of these will be assessed with real users in respect of their capability to foster trust in the vehicle and on the level of understandability of the provided signals. Moreover, additional accessory studies will focus on the exploration of different designs for triggering driving handover, i.e. the transfer vehicle control from AI to human drivers, which is a central problem in current embodiments of self-driving vehicles.QC 20181010</p",,Novel synthetic environment to design and validate future onboard interfaces for self-driving vehicles,,,,core
201207793,2018-08-01T00:00:00,"Introduction
Current approaches to the development and application of predictive studies is inefficient and difficult to reproduce. Thousands of predictive health algorithms have been developed; however, less than 2\% have been assessed outside their original setting and even fewer have been applied and evaluated in practice.



Objectives and Approach
Objective: To develop a standardized workflow for algorithm development, dissemination and implementation.



Existing predictive analytics workflow and open standards were adapted and expanded for health research and health care settings. The approach was designed to work within multidisciplinary teams and to improve research transparency, reproducibility, quality, efficiency and application. Key components include standardized algorithm description files, documentation and code libraries. All libraries and programming packages, which were created for/with open-source software, can be used for a wide range of statistical and machine learning models. Publicly-available repositories contain the algorithms, validation data, R code and other supporting infrastructure.



Results
Algorithm development involves variable pre-specification and documentation of model variables, followed by creation of data preprocessing code to generate model variables from the study dataset. Preprocessing uses algorithm specification documentation and a function library, building upon and integrating with existing algorithms when possible to preventing code duplication. Models are output as a Predictive Modelling Markup Language (PMML) file, a portable industry standard for describing and scoring predictive models. A separate scoring ""engine"" is used to implement PMML-described algorithms in a range of settings, including algorithm validation at other research institutions. Algorithm applications currently include the Project Big Life (www.projectbiglife.ca) online calculators, population, health services and public health planning uses and an algorithm visualization tool. An API permits use of the calculator engine by other organizations.



Conclusion/Implications
Barriers to the implementation of predictive analytics in real-world settings—such as within electronic medical records or decision aid applications—can be mitigated with well described algorithms that are easy to replicate and implement, especially as access to big health data increases and algorithms become increasingly complex",'Swansea University',A Data Science Approach to Predictive Analytic Research and Knowledge Translation,10.23889/ijpds.v3i4.797,https://core.ac.uk/download/201207793.pdf,"[{'title': 'International Journal for Population Data Science', 'identifiers': ['issn:2399-4908', '2399-4908']}]",core
304993973,2018-08-20T15:33:41,"Over the last decades, general-purpose computing stack and its abstractions have provided both performance and productivity, which have been the main drivers for the revolutionary advances in IT industry. However, the computational demand of emerging applications grows rapidly, and the rate of data generation exceeds the level where the capabilities of current computing systems can match. The challenges have coincided with the Dark Silicon era in which conventional technologies offer insufficient performance and energy efficiency. Thus, it is timely to move beyond conventional techniques and explore radical approaches that can overcome the limitations of general-purpose systems and deliver large gains in performance and efficiency. One such approach is specialization, where the hardware and systems are developed for a domain of applications. However, the specialization creates a tension between the performance and productivity, since (1) programmers need to delve into the details of specialized hardware, and (2) perform low-level programming. Hence, the objectives are (1) delivering large gains in performance and efficiency (2) while retaining automation and productivity through high-level abstractions. Achieving both of these conflicting objectives is a crucial challenge to place the specialization techniques in a position of practical utility, which is the main focus of this dissertation research. My works offer algorithm-driven computing stacks, which span from algorithms and languages to micro-architectural designs. I have primarily focused on two paradigms of specialization: acceleration and approximation. A growing number of commercial and enterprise systems increasingly rely on compute-intensive Machine Learning (ML) algorithms. Hardware accelerators offer several orders of magnitude higher performance than general-purpose processors and provide a promising path forward to accommodate the needs of ML algorithms. Even software companies have begun to incorporate various forms of accelerators in their data centers. Microsoft's Project Brainwave integrated FPGAs in datacenter scale for real-time AI calculations and Google developed the TPU as a specialized matrix multiplication engine for machine learning. However, not only do the benefits come with the cost of lower programmability, but also the acceleration requires long development cycles and extensive expertise in hardware design. Moreover, conventionally, accelerators are integrated with the existing computing stack by profiling hot regions of code and offloading the computation to the accelerators. This approach is suboptimal since the stack is designed and optimized merely for CPUs, the sole processing platform up until very recently. To tackle these challenges, we developed cross-stack and algorithm-hardware co-designed solutions that rebuild the computing stack for acceleration of machine learning. These solutions break the conventional abstractions of computing stack by reworking the entire layers of computing stack, which include programming language, compiler, system software, accelerator architecture, and circuit generator. Approximate computing is another form of specialization, which brings forth an unconventional yet innovative computing paradigm that trades accuracy of computation for otherwise hard-to-achieve performance and efficiency. This new computing paradigm is built upon the property that emerging applications (e.g., sensor processing, translation, vision, and data analytics) are increasingly tolerant to imprecision. Leveraging this property, approximation techniques are able to provide orders of magnitude higher performance and efficiency gains, while maintaining the acceptable level of functionalities. However, these techniques are only pragmatic when (1) they are easy to use for the programmers, and (2) they produce acceptable output quality from the perspective of application users. To this end, my research efforts for approximation focus on improving productivity and utility of approximation technologies by developing programming language and crowdsourcing-based software engineering solutions.Ph.D",Georgia Institute of Technology,Breaking the abstractions for productivity and performance in the era of specialization,,,,core
200591291,2018-12-12T00:00:00Z,"Continued
emphasis in the pharmaceutical industry toward development
of hybrid and continuous processes has led to a resurgence in process
analytical technologies. The need to consistently and rapidly monitor
the quality of material being made from these processes is a significant
challenge that requires integrated, online analytics. Often the most
challenging aspect of implementing real-time monitoring is ensuring
that the analytical instrumentation is receiving a representative
sample from the process. The Small Molecule Design and Development
organization at Eli Lilly and Company has developed a highly adaptable
process-to-analytics interface that has been broadly implemented across
the development portfolio. The automated sample carts use only gravity
and low-pressure nitrogen to obtain representative process samples,
typically on the order of 0.3–6 mL. Subsequent sample preparation
operations including quenching, derivatization, and most commonly
dilution (10–250× is customary, but not restrictive) in
a high-performance liquid chromatography-compatible diluent. Samples
are then delivered to the receiving online chromatographic instrument
in a manner conducive to injection and analysis for that specific
platform. These carts have demonstrated robust performance, and qualification
data routinely meets less than 2.0% relative standard deviation on
sample-to-sample reproducibility and less than 5% carryover between
samples when optimized. Implementation within and across pharmaceutical
processes have spanned 5 orders of magnitude in volumetric scale,
while completing tens of thousands of on-process cycles. Automated
sampling has proven invaluable to our organization, significantly
increasing the speed of development/optimization for continuous unit
operations by providing real-time process understanding and rapid
diagnosis and troubleshooting of events. Operational criteria and
performance assessment of the carts are provided. Several applications
will be highlighted with integration to a variety of continuous manufacturing
platforms in both pharmaceutical development laboratories and good
manufacturing processes manufacturing. Cart function is thoroughly
described in Supporting Information",,"Development of Universal, Automated Sample Acquisition,
Preparation, and Delivery Devices and Methods for Pharmaceutical Applications",10.1021/acs.oprd.8b00280.s001,,,core
286982314,2018-12-13T00:00:00,"Research presented in this report is based on:1. Problem Submission: Firms submitted corporate challenges relating to digital business models across several industries. Problems were screened and selected.2. Problem Framing: Professor Darwin conducted individual sessions with individual firms to solicit input from both open innovation researchers and practitioners.3. Problem Solving: Input, feedback, and recommendations provided by a community of academic experts and open innovation practitioners across industries who worked deliberated in groups of eight during a one-hour session per challenge.Challenge #1: WIPRO : “Wipro is getting ready to deploy its newly developed digital technology (AR/VR/AI) “do-your-own repairs tool” that will help people repair their white goods in their homes. With what business model could the company deploy this new technology?”;Challenge #2: DAIMLER AG : ""As Daimler enters new markets, such as autonomous vehicles, how can a mobility services firm accelerate internal innovation against uncharted territories in the uncertain times of digital transformation?""; Challenge #3: KANEKA : ""Kaneka wants to improve innovation internally through the following lens: how can Kaneka accelerate internal innovation utilizing a two-sided digital ideation/challenge platform through which it can address company’s internal and external challenges?"";Challenge #4: ALLERGAN ""What innovative digitally driven design would you suggest for supporting busy physicians and/or patients, to get them relevant, accurate, reliable and real-time information and analysis quickly?"";Challenge #5: APPLIED MATERIAL ""How AMAT can leverage its materials engineering capabilities to enter new markets with platform extensions powered by collaborations with external ecosystem partners?"";Challenge #6: XIAOMI "" Xiaomi offers High-Value/Low-Cost/Low-Margin products to all customer segments in emerging markets. This demands severe cost curtailment strategies in manufacturing, operation, advertisement, sales, distribution and servicing of its products. Xiaomi cannot deliver this value alone without an ecosystem to sustain and scale the business. How can the government, corporations and other institutions help create a win-win for all? In addition, rural communities lack infrastructure (reliable connectivity, power, healthcare, clean water, accessible roads). What must Xiaomi do to serve and expand the market when this infrastructure is lacking?",HAL CCSD,Digital Transformation for sustainability,,,,core
93952323,2018-02-06T00:00:00,"In the research area of reinforcement learning (RL), frequently novel and
promising methods are developed and introduced to the RL community. However,
although many researchers are keen to apply their methods on real-world
problems, implementing such methods in real industry environments often is a
frustrating and tedious process. Generally, academic research groups have only
limited access to real industrial data and applications. For this reason, new
methods are usually developed, evaluated and compared by using artificial
software benchmarks. On one hand, these benchmarks are designed to provide
interpretable RL training scenarios and detailed insight into the learning
process of the method on hand. On the other hand, they usually do not share
much similarity with industrial real-world applications. For this reason we
used our industry experience to design a benchmark which bridges the gap
between freely available, documented, and motivated artificial benchmarks and
properties of real industrial problems. The resulting industrial benchmark (IB)
has been made publicly available to the RL community by publishing its Java and
Python code, including an OpenAI Gym wrapper, on Github. In this paper we
motivate and describe in detail the IB's dynamics and identify prototypic
experimental settings that capture common situations in real-world industry
control problems",'Institute of Electrical and Electronics Engineers (IEEE)',A Benchmark Environment Motivated by Industrial Control Problems,10.1109/SSCI.2017.8280935,http://arxiv.org/abs/1709.09480,,core
151253769,2018-10-23T00:00:00,"Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.Comment: In The 27th ACM International Conference on Information and Knowledge
  Management (CIKM 18), October 22-26, 2018, Torino, Italy. ACM, New York, NY,
  USA, 9 page",'Association for Computing Machinery (ACM)',"Budget Constrained Bidding by Model-free Reinforcement Learning in
  Display Advertising",10.1145/3269206.3271748,http://arxiv.org/abs/1802.08365,,core
211493720,2018-01-01T00:00:00,"Proactive event-driven computing refers to the use of event-driven information systems having the ability to eliminate or mitigate the impact of future undesired events, or to exploit future opportunities, on the basis of real-time sensor data and decision making technologies. Maintenance management can benefit from these advancements in order to tackle with the increasing challenges in today’s dynamic and complex manufacturing environment in the context of Industry 4.0. To this end, the current thesis combines and brings together the research fields of Industry 4.0, Maintenance Management and Proactive Computing in order to frame maintenance management and information systems in the context of Industry 4.0. Therefore, it paves the way for the next generation of maintenance management in the frame of Industry 4.0, i.e. Proactive Maintenance. The focus of the current thesis is on proactive decision making. Consequently, it proposes proactive decision methods, capable of handling uncertainty, applicable to maintenance management and its interrelationships with other manufacturing operations, algorithms for continuous improvement of proactive decision making through the proposed Sensor-Enabled Feedback (SEF) approach and algorithms for context-awareness in proactive decision making. To do this, it utilizes methods and techniques for operational research, data analytics and machine learning.The aforementioned algorithms have been embedded in a proactive information system for decision making which was integrated with other tools in order to implement all the steps of the Proactive Maintenance framework. The system has been deployed and evaluated in real industrial environment, while further evaluation was conducted with extensive simulation experiments. Finally, the lessons learned and the managerial implications of the proposed approaches are discussed.Η προδραστική πληροφορική οδηγούμενη από γεγονότα αφορά τη χρήση πληροφοριακών συστημάτων οδηγούμενων από γεγονότα που έχουν την ικανότητα να εξαλείφουν ή να αμβλύνουν την επίδραση μελλοντικών ανεπιθύμητων γεγονότων ή να αξιοποιούν μελλοντικές ευκαιρίες με βάση δεδομένα αισθητήρων πραγματικού χρόνου και τεχνολογίες λήψης αποφάσεων. Η διοίκηση συντήρησης μπορεί να επωφεληθεί από την προδραστική πληροφορική για να αντιμετωπίσει τις προκλήσεις στο πλαίσιο της Βιομηχανίας 4.0 (Industry 4.0).Για το σκοπό αυτό, η παρούσα διατριβή συνδυάζει τους ερευνητικούς τομείς της Βιομηχανίας 4.0, της Διοίκησης Συντήρησης και της Προδραστικής Πληροφορικής. Με αυτό τον τρόπο, ανοίγει το δρόμο για την επόμενη γενιά διοίκησης συντήρησης στο πλαίσιο της Βιομηχανίας 4.0, την Προδραστική Συντήρηση (Proactive Maintenance). Το επίκεντρο της διατριβής είναι η λήψη προδραστικών αποφάσεων. Συνεπώς, προτείνει μεθόδους προδραστικών αποφάσεων για βιομηχανική συντήρηση, αλγόριθμους για συνεχή βελτίωση της λήψης προδραστικών αποφάσεων μέσω της προτεινόμενης προσέγγισης Ανατροφοδότηση Υποβοηθούμενη από Αισθητήρες (Sensor-Enabled Feedback - SEF) και αλγόριθμους για την επίγνωση πλαισίου. Για να γίνει αυτό, αξιοποιεί μεθόδους και τεχνικές επιχειρησιακής έρευνας, ανάλυσης δεδομένων και μηχανικής μάθησης.Οι προαναφερθέντες αλγόριθμοι έχουν ενσωματωθεί σε ένα προδραστικό πληροφοριακό σύστημα για τη λήψη αποφάσεων το οποίο ολοκληρώθηκε με άλλα εργαλεία για την υλοποίηση όλων των βημάτων του πλαισίου της Προδραστικής Συντήρησης. Το σύστημα εγκαταστάθηκε και αξιολογήθηκε σε πραγματικό βιομηχανικό περιβάλλον, ενώ πραγματοποιήθηκε περαιτέρω αξιολόγηση με εκτεταμένα πειράματα προσομοίωσης",Εθνικό Μετσόβιο Πολυτεχνείο (ΕΜΠ),Προδραστική πληροφορική στη λήψη αποφάσεων βιομηχανικής συντήρησης,,,,core
234970564,2018-04-11T00:00:00,"The Reporter is a publication produced by Western Carolina University featuring news, events, and campus community updates for faculty and staff. The publication began in August of 1970 and continues digitally today. Click on the link in the “Related Materials” field to access recent issues.The Reporter
News for the Faculty and Staff of Western Carolina University
June 24, 1998
Food Services to
Switch July 1
See you at Starbucks!
Cullowhee, North Carolina
Western's Learning Communities
Helping Freshmen Find a Niche
Jr Dining in takes on
a new dimension
July 1 at Western
with a changeover4
in food services. ARAMARK
Corp. has been selected to operate
the university's campus dining
services, including those at
Dodson and Brown cafeterias, the
food court areas at Hinds
University Center and Dodson,
concessions at the Ramsey
Regional Activity Center, and
catering services throughout
the campus.
Among the changes expected
under ARAMARKs management
are new menus and formats
providing a wide variety of student
dining options. These include addi­tional
nationally known franchises,
such as Starbucks Coffee and Little
Ceasars Pizza, as well as demon­stration
cooking areas where
customers can watch as their food
is prepared, and expansion of full
menu cafeteria-style service and
""grab-and-go"" items.
Clete Myers, operations
manager of food services at
Clemson University, will become
general manager of campus dining
services at Western. ARAMARK
hopes to retain personnel currently
employed in WClFs food service
when the management change
occurs.
Retention Services Director Susan Clarke Smith (far right) and Peer Mentor
Bryan Dodge (center), meet entering LC students.
""An institution's capacity to retain students is directly related to its ability to reach out and make
contact with students and integrate them into the social and intellectual fabric of institutional life.
It hinges on the establishment of a healthy, caring environment which enables individuals to find a
niche in the social and intellectual communities of the institution."" —VINCENT TINTO IN LEAVING COLLEGE
a ±JL good many of us came of age
when linking the notion of caring
with any institution would have been
greeted with skepticism—if not out­right
scorn. But Western, changing
with, and in many instances ahead of,
the times, will set the standard for
high tech and high touch this fall.
By embracing the idea of learning
communities as part of its freshman
year experience program, Western
makes use of a proven method for
meeting both the individual's need
to belong and the institution's need
to retain.
Randomly selected as participants
in the Learning Community (or LC
Pilot) project beginning this fall, some
170 of Western's estimated 1,200
first-year students will live together
and learn together in classes linked to
their common interests and conduct­ed
by a core group of instructors—all
in an effort to address the problems
presented by the transition from high
school to college. Meanwhile, the
institution will look closely to see if
the initiative addresses its own prob­lems
associated with an unusually
high rate of student attrition,
particularly after the first year.
Frank Prochaska, associate vice
chancellor for academic affairs and
chief architect of the LC Pilot project,
believes that establishing learning
communities at Western, even on a
limited basis initially, will deliver on
both counts. The numbers would
appear to bear him out, with
universities where LCs have become
a standard for first-year programs
reporting significantly higher GPAs
and retention rates among students
who participate.
Elizabeth Shelly coordinates the
Freshman Year Experience Program
through Student Affairs and is herself
a product of an undergraduate LC.
Shelly describes it as setting out in
college life with an ""instant group of
friends"" to go to at any time for
support. She explains that Western's
approach to the idea conforms to a
pretty standard model but incorporates
some new features made possible by
our unique environment.
The pilot group is divided into eight
communities of about twenty students
each, with several of the communities
grouped according to a declared major
or according to undergraduate college.
Three LCs are made up of freshmen
undecided upon majors. Each LC will
be housed in a suite-style layout in
Walker Hall with common areas set
aside for studying and group
activities. In addition to a resident
assistant on the floor, each commu­nity
will be assigned an upperclass
peer mentor who will live with the
group and work with the USI130
class designed for it.
A revamped USI course is key to
the LC Pilot, according to Prochaska.
The USI 130 course for the individual
communities will be co-taught by a
faculty member and a student affairs
professional, both trained in freshman
issues. Themes introduced for
discussion in this course will carry
over to the other General Education
courses linked for the purposes of each
community and taught by specially
selected instructors. For example,
Instructor Nory Prochaska's USI 130
section, and its linked English,
continued on page 2
Learning, continued
computer science, and math courses,
will examine the advantages and
disadvantages of the ""virtual
university"" idea, from the perspective
of LC students who have a particular
interest in technology and a close
comfort with using electronic media.
Susan Clarke Smith, director of
Retention Services, sees the linked
courses as an effective means of
addressing the ""intimidation factor,""
identified by most retention experts
such as Vincent Tinto as one of the
main reasons students leave college.
Smith asserts that by establishing
opportunities for interaction in and
out of the classroom, students and
faculty can begin to ""break down
barriers."" Smith says, ""Students can
see that their professors are human,
and faculty can feel less hesitation, as
part of a wider network of support, to
communicate concern on a more
personal level.""
To some extent, we need look no
farther than our own campus for an
example of a learning community
beginning to deliver on its promise.
Now entering its second year, The
Honors College combines an academic
emphasis with a residential and social
component. Brian Railsback, acting
dean of the college, calculates a 92
percent fall-spring retention rate for
honors freshmen for 1997-98. (The
university's rate for freshmen was
83 percent.) ""That figure,"" Railsback
says, ""obviously points to a good bond
and a real desire for these students to
stay together.""
Perhaps the most ambitious and
the most advantageous aspects of
initiating a learning community
program now come to a confluence
with the computer implementation
project, also spearheaded by
Prochaska. Student Affairs Vice
Chancellor Robert Caruso sees the
efforts as quite extraordinary. ""The
added component of new technologies
in the classroom and the residence
halls will revolutionize our whole
notion of community,"" Caruso says.
""With something on the order of only
forty higher education institutions
out of about 2,800 nationwide that
have a computer requirement, I think
everyone is going to be looking to
Western as a model for creating the
learning community of the twenty-first
century.""
Michael Dougherty
Named Dean of
Education, Allied
Professions
Carolina
Board of
Governors.
Associate
dean of the
College of
Education
and Allied
Professions
since 1996, Dougherty is a professor
in the Department of Human
Services and is a former head of the
department. He earned his bachelor's
degree from the University of Notre
Dame in 1968, master's degrees from
Oakland University in 1970 and
Notre Dame in 1971, and doctoral
degree from Indiana State University
in 1974. He has been a member of the
human services faculty since 1976.
The 1988 recipient of Western's
Paul A. Reid Distinguished Service
Award for Faculty, he also has been
nominated for the Chancellor's
Distinguished Teaching Award and
the Taft Botner Award for Superior
Teaching. Prior to coming to Western,
he was a teacher and counselor in
public schools in Detroit; Mattoon, 111.;
and Taylor County, Fla.
Dougherty is a member of several
professional organizations, including
the American Counseling Association
and the Association for Educational
and Psychological Consultation. His
research activities have focused on
study skills and locus of control, the
effects of counseling techniques on
incarcerates, and consultation styles.
Dougherty's appointment will be
effective July 1, upon the retirement
of Chambers, who served seventeen
years as the college's dean.
Highlighting the summer's cultural
events on campus are this weekend's
concerts by the Atlanta Ballet and the
Cassatt String Quartet.
Hailed as one of the nation's
outstanding young ensembles, the
Cassatt String Quartet will perform in
recital at 8 p.m. this Friday and will
also perform as part of the Atlanta
Ballet programs at 2 p.m. and 8 p.m.
on Saturday. All performances are in
Hoey Auditorium.
The oldest continually operating
ballet company in the United States,
the acclaimed Atlanta Ballet travels to
the mountains each year for a summer
residency. The two performances on
Saturday come in conjunction with
Western's hosting the second annual
Atlanta Ballet Centre for Dance Educa­tion
summer dance camp. Selected
participants in the June 14-July 4
camp, which attracts more than fifty of
the top ballet students in the Southeast,
will share the stage with the
professional dancers for one piece.
The ballet programs on Saturday
include ""Intermezzo,"" featuring three
couples in an intricate series of
dances set to music by Johannes
Brahms; ""Prisma,"" featuring music
by Charles Ives and Atlanta Ballet
executive/music director Robert
Chumbley performed by the Cassatt
String Quartet; and ""II Distrato,"" an
abstract ballet in five movements
demonstrating how the different
parts of the dancer's body work
separately and as a unit.
The Cassatt String Quartet's
Friday program features music for
strings by Beethoven and Ravel.
Admission for the quartet's recital
is $10 for adults, $5 for WCU
students and children. Admission for
the ballet's performances is $20 for
adults and $5 for WCU students and
children. For tickets, call 227-7397.
Architects Tapped for Construction Projects
A pair of major construction projects planned for Western moved closer to reality
recently with the board of trustees' selection of two Charlotte architecture firms
to design an expansion of the university center and a federally funded workforce
development facility.
The trustees named Lee Nichols Hepler Architecture of Charlotte to design
the expansion of the Hinds University Center and Jenkins-Peer Architects of
Charlotte to design the proposed new Western North Carolina laborforce high-technology
education and training center.
The Hinds University Center project is the second in a three-phased
expansion effort designed to enhance the quality of student life at WCU. Plans
call for the construction of approximately 31,000 additional square feet to add a
retail shopping area, a movie theater, increased meeting and office space for
student organizations, and a multicultural center. Preliminary estimates for the
expansion set the cost at about $4.5 million.
The regional high-tech workforce training center, announced last fall by U.S.
Rep. Charles Taylor, is designed to help raise the economic potential of the
region by improving the availability of high-technology education to the
mountains' workforce. The center could include such high-tech training tools as
an industrial laser lab, artificial intelligence lab, geographic information lab,
robotics training, and sound and video production facilities complete with digital
editing capabilities. The facility, pending funding from Congress and the federal
Economic Development Administration, is expected to be built adjacent to the
Belk Building.
Michael Dougherty, associate dean of
the College of Education and Allied
Professions, has been named by
WClFs board of trustees to succeed
Gurney E. Chambers as dean.
Appointment of Dougherty to the
dean's post was approved by the
board Wednesday, June 10, at its
quarterly meeting. The appointment
is subject to the approval of the
University
I of North
WCU Campus
Plays Host to
Weekend of CI
Music and
June 24,1998 • T he Reporter • p age 2
University Awards Top Honors for Teaching, Research, and Service
Western's top faculty and staff
awards for teaching, research, and
service for the 1997-98 academic
year were presented at the annual
spring General Faculty and Awards
Convocation in May.
Mary C. ""Katie"" Ray, assistant
professor of elementary and middle
grades education, won the
Chancellor's Distinguished Teaching
Award. The Paul A. Reid Distin­guished
Service Award for faculty
went to Gordon Mercer, professor
of political science and public
affairs, and the Paul A. Reid
Distinguished Service Award for
administrative staff went to
Stephen White, sports information
director. David J. Butcher,
associate professor of chemistry,
received the University Scholar
Award for distinguished scholarly
achievement.
The honors, presented by
Chancellor John Bardo, carry $1,000
cash awards and engraved plaques
for each recipient. Bardo also
presented the Academic Award of
Excellence to the Department of
English. The award provides
$10,000 for program and staff
development.
Chancellor's Distinguished
Teaching Award
Katie Ray joined WCU's faculty
in 1994 after seven years of teaching
in elementary and middle schools in
New York City. In presenting the
award, Bardo quoted from Ray's
comments to the awards selection
committee.
""For students to be great
teachers, they must be passionate
about living and learning,"" Ray said.
""Consequently, I must show
students by my own passion. The
challenge I face every day is not to
know about good teaching, but to
demonstrate it in every interaction I
have with my students.""
Paul A. Reid
Distinguished Service
Award—Faculty
Gordon Mercer has been
a member of the WCU
faculty since 1980. Among
his accomplishments are the
creation of the annual
Undergraduate Research
| Conference and the
organization of faculty
forum assemblies to foster
communication about
athletic director. He received twenty-six
publication awards from the
College Sports Information Directors of
America, and eight Football Writers
Association of America awards for
""Outstanding Press Box Service.""
Following his retirement on June 30,
integrate computers and new
technology with writing and research
in a way that reflects ""real-world""
practices. The department has become
a primary user of the electronic
classrooms, and during the 1997-98
academic year, every freshman
years of continuous service leave
from usual work commitments to
pursue concentrated scholarly work.
Recipients are chosen on a competi­tive
basis by a faculty committee.
important campus issues.
He has held leadership
positions in WCU's Faculty
Senate and the University of
North Carolina's Faculty Assembly.
Mercer also has been described as ""a
superior teacher"" by his students and
has been nominated frequently for
campus teaching awards.
Paul A. Reid Distinguished Service
Award—Staff
Steve White will retire at the end of
June as WCU's sports information
director. A 1967 graduate of WCU,
White has also served as associate
Distinguished Teacher Katie Ray, University Scholar David Butcher, and Reid Service honoree
Gordon Mercer receive their awards from Chancellor John Bardo.
White will head the Catamount Sports
Network, Inc., the radio broadcaster of
WCU's football and basketball games.
University Scholar Award
David Butcher joined WCU's faculty
in 1990. An analytical chemist, he has
received several external grants and
has published numerous articles on his
scientific research, which includes the
atomic absorption spectroscopy, atomic
fluorescence spectrometry, and the
search for potential
chemical causes for
Sports Information Director Steve White receives the Reid
Service Award for Staff from Chancellor John Bardo.
of Excellence, Bardo
praised the
Department of
English for its
innovative work to
studied composition in the electronic
environment.
Other major awards recognized at
the convocation were the Beyond the
Classroom Teaching Award and the
Scholarly Development Assignment
Program Awards.
The Beyond the Classroom
Teaching Award is given to an
academic teaching unit that excels
in enhancing students' learning
through such activities outside the
classroom as mentoring programs,
effective academic advising or
cooperative learning experience. The
1998 winner of the award, which is
funded by the UNC Board of
Governors, is the Department of
Health Sciences.
Recipients in the Scholarly
Development Assignment
Program are Richard Boyer
(English); Barbara Lovin (head,
Health Sciences); and Dan Pittillo
(Biology). The Scholarly Develop­ment
Assignment Program awards
provide full-time tenured faculty
members who have a minimum of six
the decline of Fraser
fir trees in the
Southern Appala­chian
Mountains.
Academic Award
of Exc ellence
In presenting
the Academic Award
June 24,1998 • The Reporter • page 3
Bruce Henderson Receives UNC System Teaching Award
Bruce B. Henderson, professor
of psychology, was among
sixteen recipients of the fourth
annual Awards for Excellence
in Teaching, presented by the
UNC Board of Governors.
Henderson accepted the award
at a special academic convoca­tion
held at N.C. Central
University in conjunction with
the inauguration of Molly
Corbett Broad as president of the
University of North Carolina.
Winners from
each campus
received a
bronze medallion
and a $7,500
cash prize.
Recipients were
nominated by
special commit­tees
from each of
the sixteen UNC
campuses and selected by the Board
of Governors Committee on Teach­ing
Awards.
Established by the Board of
Governors in 1994 to underscore
the importance of teaching and to
reward good teaching across the
university system, the awards are
given annually to a tenured faculty
member from each UNC campus.
Winners must have taught at their
present institutions at least seven
years, and no one may receive the
award more than once.
Henderson, on WCU's faculty
since 1978 and former head of the
psychology department, received
Western's Botner Superior
Teaching Award in 1988. He co-edited
the book Curiosity and
Exploration, focusing on how
intrinsic rewards affect behavior
in children. Henderson received
his bachelor's and master's degrees
from Bucknell University and
his doctorate from the University
of Minnesota.
WCU Colleges Present Awards
Awards for teaching, service, and scholarship were presented on a college-level
at the end of spring semester. The following is a listing of award
winners by college:
College of Arts and Sciences
• Curtis Wood (History) received
the Creighton Sossomon Professor­ship
for outstanding teacher-scholars
in American, English or European
history. Appointment to the
professorship is for a three-year term.
• Richard Bruce (Biology and
director, Highlands Biological
Station) received the H.F. and
Katherine P. Robinson Professorship.
• Robert Holquist (Music and
director of choral activities) received
the James Dooley Excellence in
Music Teaching Award, which carries
a $500 cash stipend.
• Betty Farmer (Communication
and Theatre Arts) received the Board
of Governors College of Arts and
Sciences Teaching Award, which
carries a $1,000 prize.
• Faculty members in the College of
Arts and Sciences also presented
acting dean J.C. Alexander Jr. with
a ""lifetime achievement award"" in
appreciation for service as acting
dean of the college.
The College of Business
• Roger Lirely (Accounting and
Information Systems) received the
Jay I. Kneedler Professor of
Excellence Award, which includes a
$1,000 cash prize and a plaque.
• Board of Governors Creative and
Innovative Teaching Awards went to
Julie Johnson (Business Adminis­tration,
Law and Marketing); Susan
Kask (Economics); Reagan
McLaurin (Business Administration,
Law and Marketing); and Max
Schreiber (Economics, Finance and
International Business). Each award
carries a $250 stipend.
The College of Education and
Allied Professions
• Carol Burton (director, Teaching
Fellows) received the annual Taft B.
Botner Award for Superior Teaching,
which includes a $750 cash prize and
a plaque.
• Board of Governors Awards for
Superior Teaching and $250 stipends
went to Barbara Bell (Elementary
and Middle Grades Education and
director, Reading Center); Cindy
Cavanaugh (Health and Human
Performance); Richard Haynes
(Administration, Curriculum and
Instruction and director of field
experiences and teacher education);
and Hedy White (Psychology).
The College of Applied Sciences
• The Board of Governors Innovation
in Teaching Award, which carries a
$1,000 stipend, went to Walter
Floreani (Health Sciences).
Trustees Approve Appointments and
Campus Name Changes
WCU's board of trustees approved a number of administrative appoint­ments
for the coming year at its quarterly meeting June 10.
• Terry L. Ballman, assistant professor of Hispanic studies at the
University of Northern Colorado, as associate professor and head of the
Department of Modern Foreign Languages.
• Paul F. Brandt as head of the Department of Chemistry and Physics.
• James A. Lewis as head of the Department of History.
• Carol C. Stephens as director of the Master of Science in Nursing ~
Program.
• Paul Wright as head of the Department of Biology.
• Kathleen S. Wright as head of the Department of Communication
and Theatre Arts.
• Jerry L. Kinard to continue as head of the Department of Manage­ment
through spring 1999.
• John A. Wade III to continue as head of the Department of Econom­ics,
Finance and International Business through spring 2000.
The trustees also approved several administrative and departmental
changes within the College of Business. The department ","Hunter Library Digital Collections, Western Carolina University, Cullowhee, NC 28723;","The Reporter, June 1998",,,,core
322490272,2018-09-17T00:00:00,"© 2018 The Author(s).This paper presents a methodology to monitor the fatigue life of aerospace structures and hence the remaining allowable fatigue life. In fatigue clearance, conservative load assumptions are made. However, in reality, a structure may see much lower loads and so would be usable for much longer. An example ofthis is air carried guided missiles. In the UK, missiles must be decommissioned after a period of carriage. The implementation of a system that can monitor the usage of a missile during its time in service is advantageous to the military customer and provides a competitive advantage for the missile manufacture inexport markets where reduced through-life costs, longer in-service lives and increased safety are desired. The proposed methodology provides a means to monitor the service life of a missile. This paper describes how machine learning algorithms can be used with accelerometers to determine loads on a missile structure which would then be used to predict how long the missile has left in service","KU Leuven, Faculty of Arts",A methodology using health and usage monitoring system data for payload life prediction,,https://core.ac.uk/download/322490272.pdf,,core
322434249,2018-07-04T00:00:00,"Long-term companionship, emotional attachment and realistic interaction with robots have always been the ultimate sign of technological advancement projected by sci-fi literature and entertainment industry. With the advent of artificial intelligence, we have indeed stepped into an era of socially believable robots or humanoids. Affective computing has enabled the deployment of emotional or social robots to a certain level in social settings like informatics, customer services and health care. Nevertheless, social believability of a robot is communicated through its physical embodiment and natural expressiveness. With each passing year, innovations in chemical and mechanical engineering have facilitated life-like embodiments of robotics; however, still much work is required for developing a “social intelligence” in a robot in order to maintain the illusion of dealing with a real human being. This chapter is a collection of research studies on the modeling of complex autonomous systems. It will further shed light on how different social settings require different levels of social intelligence and what are the implications of integrating a socially and emotionally believable machine in a society driven by behaviors and actions",'IntechOpen',Socially Believable Robots,10.5772/intechopen.71375,https://core.ac.uk/download/322434249.pdf,,core
146482282,2018-04-03T00:00:00,"In machine learning, a bias occurs whenever training sets are not
representative for the test data, which results in unreliable models. The most
common biases in data are arguably class imbalance and covariate shift. In this
work, we aim to shed light on this topic in order to increase the overall
attention to this issue in the field of machine learning. We propose a scalable
novel framework for reducing multiple biases in high-dimensional data sets in
order to train more reliable predictors. We apply our methodology to the
detection of irregular power usage from real, noisy industrial data. In
emerging markets, irregular power usage, and electricity theft in particular,
may range up to 40% of the total electricity distributed. Biased data sets are
of particular issue in this domain. We show that reducing these biases
increases the accuracy of the trained predictors. Our models have the potential
to generate significant economic value in a real world application, as they are
being deployed in a commercial software for the detection of irregular power
usage",,"On the Reduction of Biases in Big Data Sets for the Detection of
  Irregular Power Usage",,http://arxiv.org/abs/1801.05627,,core
160760483,2018-06-14T00:00:00,"Building multi-turn information-seeking conversation systems is an important
and challenging research topic. Although several advanced neural text matching
models have been proposed for this task, they are generally not efficient for
industrial applications. Furthermore, they rely on a large amount of labeled
data, which may not be available in real-world applications. To alleviate these
problems, we study transfer learning for multi-turn information seeking
conversations in this paper. We first propose an efficient and effective
multi-turn conversation model based on convolutional neural networks. After
that, we extend our model to adapt the knowledge learned from a resource-rich
domain to enhance the performance. Finally, we deployed our model in an
industrial chatbot called AliMe Assist
(https://consumerservice.taobao.com/online-help) and observed a significant
improvement over the existing online model.Comment: ",,"Transfer Learning for Context-Aware Question Matching in
  Information-seeking Conversations in E-commerce",,http://arxiv.org/abs/1806.05434,,core
227007466,2018-01-01T00:00:00,"Internet of Things: Technologies and Applications for a New Age of Intelligence outlines the background and overall vision for the Internet of Things (IoT) and Cyber-Physical Systems (CPS), as well as associated emerging technologies. Key technologies are described including device communication and interactions, connectivity of devices to cloud-based infrastructures, distributed and edge computing, data collection, and methods to derive information and knowledge from connected devices and systems using artificial intelligence and machine learning. Also included are system architectures and ways to integrate these with enterprise architectures, and considerations on potential business impacts and regulatory requirements.  New to this edition: • Updated material on current market situation and outlook. • A description of the latest developments of standards, alliances, and consortia. More specifically the creation of the    Industrial Internet Consortium (IIC) and its architecture and reference documents, the creation of the Reference   Architectural Model for Industrie 4.0 (RAMI 4.0), the exponential growth of the number of working groups in the Internet   Engineering Task Force (IETF), the transformation of the Open Mobile Alliance (OMA) to OMA SpecWorks and the introduction of OMA LightweightM2M device management and service enablement protocol, the initial steps in the specification of the architecture of Web of Things (WoT) by World Wide Consortium (W3C), the GS1 architecture and    standards,  the transformation of ETSI-M2M to oneM2M, and a few key facts about the Open Connectivity Forum (OCF),   IEEE, IEC/ISO, AIOTI, and NIST CPS. • The emergence of new technologies such as distributed ledgers, distributed cloud and edge computing, and the use of   machine learning and artificial intelligence for IoT. • A chapter on security, outlining the basic principles for secure IoT installations. • New use case description material on Logistics, Autonomous Vehicles, and Systems of CPS      Standards organizations covered: IEEE, 3GPP, IETF, IEC/ISO, Industrial Internet Consortium (IIC), ITU-T, GS1, Open    Geospatial Consortium (OGC), Open Mobile Alliance (OMA, e.g. LightweightM2M), Object Management Group (OMG,   e.g. Business Process Modelling Notation (BPMN)), oneM2M, Open Connectivity Forum (OCF), W3C     Key technologies for IoT covered: Embedded systems hardware and software, devices and gateways, capillary networks, local and wide area networking, IoT data management and data warehousing, data analytics and big data, complex event processing and stream analytics, control systems, machine learning and artificial intelligence, distributed   cloud and edge computing, and business process and enterprise integration.     In-depth security solutions for IoT systems     Technical explanations combined with design features of IoT and use cases, which help the development of real-world solutions     Detailed descriptions of the architectures and technologies that form the basis of IoT     Clear examples of IoT use cases from real-world implementations such as Smart Grid, Smart Buildings, Smart Cities,    Logistics and Participatory Sensing, Industrial Automation, and Systems of CPS     Market perspectives, IoT evolution, and future outloo",'Cambridge University Press (CUP)',Internet of Things: technologies and applications for a new age of intelligence,,,,core
251177151,2018-01-01T00:00:00,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are
less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.status: publishe",'Elsevier BV',Incorporating negative information to process discovery of complex systems,10.1016/j.ins.2017.09.027,,"[{'title': 'Information Sciences', 'identifiers': ['1872-6291', 'issn:1872-6291', 'issn:0020-0255', '0020-0255']}]",core
186298799,2018-12-03T00:00:00,"Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque ""black boxes"". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks",,Sensitivity based Neural Networks Explanations,,http://arxiv.org/abs/1812.01029,,core
201293367,2018-12-01T00:00:00,"Interest of the authors focuses on the process of introduction and test of different types of virtual reality in sphere of mass media. In the framework of the active integration and update of new digital technologies whose development is pushed by technogenic revolution, media industry becomes a kind of platform to test innovative projects which brings the dynamics to development of the civilization based on digital economy. Currently the most prospective innovations are artificial intelligence, virtual reality (VR) and augmented reality (AR). VR considered as a new communicative format introduces fundamental changes to the media system, demands to review radically the approaches of production and consumption of media content and causes conflict situations. That is why theoretical understanding and description of those changes is required and should be based on interdisciplinary approach. The purpose of the article is to highlight and update social aspects of VR implementation, initiate the discussion what is relevant for number of humanitarian scientific studies",'Peoples'' Friendship University of Russia',Virtual and media reality: trends and forecasts of media system evolution,10.22363/2312-9220-2018-23-4-410-421,,"[{'title': 'RUDN Journal of Studies in Literature and Journalism', 'identifiers': ['2312-9220', '2312-9247', 'issn:2312-9220', 'issn:2312-9247']}]",core
215318337,2018-07-27T00:00:00,"The Internet of Things (IoT) is reshaping our world. Soon our world will be based on smart technologies. According to IHS Markit forecasts, the number of connected devices will grow from 15.4 billion in 2015 to 30.7 billion in 2020. Forrester Research predicts that fleet management and the transportation sectors lead others in IoT growth. This may come as no surprise, since the infrastructure (roadways, bridges, airports, etc.) is a prime candidate for sensor integration, providing real-time measurements to support intelligent decisions. The energy cost required to support the anticipated enormous number of predicted deployed devices is unknown. Currently, experts estimate that 2 to 4% of worldwide carbon emissions can be attributed to power consumption in the information and communication industry [1].
This thesis presents several algorithms to optimize power consumption of an intelligent vehicle counter and classifier sensor (iVCCS) based on an event-driven methodology wherein a control block orchestrates the work of various components and subsystems. Data buffering and triggered vehicle detection techniques were developed to reduce duty cycle of corresponding components (e.g., microSD card, magnetometer, and processor execution). A sleep mode is also incorporated and activated by an artificial intelligence-enabled, reinforcement learning algorithm that utilizes the field environment to select proper processor mode (e.g., run or sleep) relative to traffic flow conditions. Sensor life was extended from 48 hours to more than 200 days when leveraging 2300 mAh battery along with algorithms and techniques introduced in this thesis",,Intelligent Power Aware Algorithms for Traffic Sensors,,https://core.ac.uk/download/215318337.pdf,,core
161940563,2018-11-04T00:00:00,"This is the author accepted manuscript. The final version is available from ASME via the DOI in this recordOffshore wind assets have reached multi-GW scale and additional
capacity is being installed and developed. To achieve demanding
cost of energy targets, awarded by competitive auctions,
the operation and maintenance (O&M) of these assets has to become
increasingly efficient, whilst ensuring compliance and effectiveness.
Existing offshore wind farm assets generate a significant
amount of inhomogeneous data related to O&M processes.
These data contain rich information about the condition of the
assets, which is rarely fully utilized by the operators and service
providers. Academic and industrial research and development
efforts have led to a suite of tools trying to apply sensor data
and build machine learning models to diagnose, trend and predict
component failures. This study presents a decision support
framework incorporating a range of different supervised and unsupervised
learning algorithms. The aim is to provide guidance
for asset owners on how to select the most relevant datasets, apply
and choose the different machine learning algorithms and
how to integrate the data stream with daily maintenance procedures.
The presented methodology is tested on a real case example
of an offshore wind turbine gearbox replacement at Teesside
offshore wind farm. The study uses kNN and SVM algorithms to
detect the fault using SCADA data and an autoregressive model
for the CMS data. The implementation of all the algorithms has
resulting in an accuracy higher than 94%. The results of this
paper will be of interest to offshore wind farm developers and operators to streamline and optimize their O&M planning activities
for their assets and reduce the associated costs.This research was funded by the Energy Technology Institute
and the RCUK Energy Programme (Grant number:
EP/J500847/1) and EDF Energ",'ASME International',Operational Data to Maintenance Optimization: Closing the Loop in Offshore Wind O&M,10.1115/IOWTC2018-1058,,,core
168394807,2018-01-01T00:00:00,"This thesis presents a novel synthetic environment for supporting advanced explorations of user interfaces and interaction modalities for future transport systems. The main goal of the work is the definition of novel interfaces solutions designed for increasing trust in self-driving vehicles. The basic idea is to provide insights to the passengers concerning the information available to the Artificial Intelligence (AI) modules on-board of the car, including the driving behaviour of the vehicle and its decision making. Most of currently existing academic and industrial testbeds and vehicular simulators are designed to reproduce with high fidelity the ergonomic aspects associated with the driving experience. However, they have very low degrees of realism for what concerns the digital components of the various traffic scenarios. These includes the visuals of the driving simulator and the behaviours of both other vehicles on the road and pedestrians.  High visual testbed fidelity becomes an important pre-requisite for supporting the design and evaluation of future on-board interfaces. An innovative experimental testbed based on the hyper-realistic video game GTA V, has been developed to satisfy this need. To showcase its experimental flexibility, a set of selected user studies, presenting novel self-driving interfaces and associated user experience results, are described. These explore the capabilities of inducing trust in autonomous vehicles and explore Heads-Up Diplays (HUDs), Augmented Reality (ARs) and directional audio solutions. The work includes three core phases focusing on the development of software for the testbed, the definition of relevant interfaces and experiments and focused testing with panels comprising different user demographics. Specific investigations will focus on the design and exploration of a set of alternative visual feedback mechanisms (adopting AR visualizations) to gather information about the surrounding environment and AI decision making. The performances of these will be assessed with real users in respect of their capability to foster trust in the vehicle and on the level of understandability of the provided signals. Moreover, additional accessory studies will focus on the exploration of different designs for triggering driving handover, i.e. the transfer vehicle control from AI to human drivers, which is a central problem in current embodiments of self-driving vehicles.QC 20181010</p",,Novel synthetic environment to design and validate future onboard interfaces for self-driving vehicles,,,,core
160450812,2018-01-01T00:00:00,"Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-toend transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%.ISBN för värdpublikation: 978-1-5386-4413-3, 978-1-5386-4414-0</p",'Institute of Electrical and Electronics Engineers (IEEE)',Real-time Performance Evaluation of LTE for IIoT,10.1109/LCN.2018.8638081,,,core
160769123,2018-10-03T00:00:00,"Neural Machine Translation (NMT) has been widely adopted recently due to its
advantages compared with the traditional Statistical Machine Translation (SMT).
However, an NMT system still often produces translation failures due to the
complexity of natural language and sophistication in designing neural networks.
While in-house black-box system testing based on reference translations (i.e.,
examples of valid translations) has been a common practice for NMT quality
assurance, an increasingly critical industrial practice, named in-vivo testing,
exposes unseen types or instances of translation failures when real users are
using a deployed industrial NMT system. To fill the gap of lacking test oracle
for in-vivo testing of an NMT system, in this paper, we propose a new approach
for automatically identifying translation failures, without requiring reference
translations for a translation task; our approach can directly serve as a test
oracle for in-vivo testing. Our approach focuses on properties of natural
language translation that can be checked systematically and uses information
from both the test inputs (i.e., the texts to be translated) and the test
outputs (i.e., the translations under inspection) of the NMT system. Our
evaluation conducted on real-world datasets shows that our approach can
effectively detect targeted property violations as translation failures. Our
experiences on deploying our approach in both production and development
environments of WeChat (a messenger app with over one billion monthly active
users) demonstrate high effectiveness of our approach along with high industry
impact.Comment: 10 page",,Testing Untestable Neural Machine Translation: An Industrial Case,,http://arxiv.org/abs/1807.02340,,core
160779299,2018-08-01T00:00:00,"In this work we study an application of machine learning to the construction
industry and we use classical and modern machine learning methods to categorize
images of building designs into three classes: Apartment building, Industrial
building or Other. No real images are used, but only images extracted from
Building Information Model (BIM) software, as these are used by the
construction industry to store building designs. For this task, we compared
four different methods: the first is based on classical machine learning, where
Histogram of Oriented Gradients (HOG) was used for feature extraction and a
Support Vector Machine (SVM) for classification; the other three methods are
based on deep learning, covering common pre-trained networks as well as ones
designed from scratch. To validate the accuracy of the models, a database of
240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and
above 89% for the neural networks.Comment: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessibl",'Institute of Electrical and Electronics Engineers (IEEE)',"Classification of Building Information Model (BIM) Structures with Deep
  Learning",10.1109/EUVIP.2018.8611701,http://arxiv.org/abs/1808.00601,,core
160782140,2018-08-10T00:00:00,"Focusing on Business AI, this article introduces the AIQ quadrant that
enables us to measure AI for business applications in a relative comparative
manner, i.e. to judge that software A has more or less intelligence than
software B. Recognizing that the goal of Business software is to maximize value
in terms of business results, the dimensions of the quadrant are the key
factors that determine the business value of AI software: Level of Output
Quality (Smartness) and Level of Automation. The use of the quadrant is
illustrated by several software solutions to support the real life business
challenge of field service scheduling. The role of machine learning and
conversational digital assistants in increasing the business value are also
discussed and illustrated with a recent integration of existing intelligent
digital assistants for factory floor decision making with the new version of
Google Glass. Such hands free AI solutions elevate the AIQ level to its
ultimate position",,AIQ: Measuring Intelligence of Business AI Software,,http://arxiv.org/abs/1808.03454,,core
390020542,2018-02-27T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and more.Рассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что Интернет вещей создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применения в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернету речей (ІР), місця в ньому інтелектуальної власності. Показано роль дер- жави у сприянні розвитку ІР, проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал і невикористані мож- ливості України в розвитку технологій ІР. Обґрунтовано висновок, що в широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",Науково-дослідний інститут інтелектуальної власності НAПрН України,ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТУ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,,,core
201808813,2018-09-01T00:00:00,"This work aims to show how to manage heterogeneous information and data coming from real datasets that collect physical, biological, and sensory values. As productive companies&mdash;public or private, large or small&mdash;need increasing profitability with costs reduction, discovering appropriate ways to exploit data that are continuously recorded and made available can be the right choice to achieve these goals. The agricultural field is only apparently refractory to the digital technology and the &ldquo;smart farm&rdquo; model is increasingly widespread by exploiting the Internet of Things (IoT) paradigm applied to environmental and historical information through time-series. The focus of this study is the design and deployment of practical tasks, ranging from crop harvest forecasting to missing or wrong sensors data reconstruction, exploiting and comparing various machine learning techniques to suggest toward which direction to employ efforts and investments. The results show how there are ample margins for innovation while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agriculture industrial business, investing not only in technology, but also in the knowledge and in skilled workforce required to take the best out of it",'MDPI AG',Machine Learning Applications on Agricultural Datasets for Smart Farm Enhancement,10.3390/machines6030038,,"[{'title': 'Machines', 'identifiers': ['2075-1702', 'issn:2075-1702']}]",core
42679818,2016-02-06T00:00:00,"Autoscaling is a hallmark of cloud computing as it allows flexible
just-in-time allocation and release of computational resources in response to
dynamic and often unpredictable workloads. This is especially important for web
applications whose workload is time dependent and prone to flash crowds. Most
of them follow the 3-tier architectural pattern, and are divided into
presentation, application/domain and data layers. In this work we focus on the
application layer. Reactive autoscaling policies of the type ""Instantiate a new
Virtual Machine (VM) when the average server CPU utilisation reaches X%"" have
been used successfully since the dawn of cloud computing. But which VM type is
the most suitable for the specific application at the moment remains an open
question. In this work, we propose an approach for dynamic VM type selection.
It uses a combination of online machine learning techniques, works in real time
and adapts to changes in the users' workload patterns, application changes as
well as middleware upgrades and reconfigurations. We have developed a
prototype, which we tested with the CloudStone benchmark deployed on AWS EC2.
Results show that our method quickly adapts to workload changes and reduces the
total cost compared to the industry standard approach","Dynamic Selection of Virtual Machines for Application Servers in Cloud
  Environments",http://arxiv.org/abs/1602.02339,,,,core
299962837,2016-01-01T00:00:00,"Cracking in plane turbine blades is the significant defect for the airplanes. Cracking usually occurs because of high pressure and temperature during manufacturing processes. In this project, automatic crack inspection system will be developed and implemented on real-time system. Inspection system is implemented on Jetson Tk1 embedded hardware and Robot Operating System (ROS). Many researched methods will be compared and inspection algorithm is developed based on the comparison results. Inspection algorithm includes a sequence of image processing methods and machine learning classifier to correctly output the defect location. For the final step, defects location coordinates will then return to ABB Industrial Robot for further executions and corrections. Accuracy rate of 89% was achieved at the final classification stage of the system with the average processing time less than 1 second.Bachelor of Engineering (Computer Science",Evaluation of low-power vision platform for robotic industrial application,,,,,core
144117019,2017-06-29T00:00:00,"Atualmente vive-se uma transformação digital na indústria, que está sendo referenciada como uma nova revolução e conhecida como a quarta revolução industrial. Essa nova revolução foi precedida por três anteriores, sendo que a primeira foi baseada no carvão como fonte de energia, impulsionando, assim, as máquinas a vapor e transformando o trabalho artesanal em automatizado; posteriormente, houve a segunda revolução industrial, baseada em conceitos de eletricidade para atingir a produção em massa; já a terceira revolução industrial baseou-se em sistemas eletrônicos e computacionais, tendo como o seu maior expoente os sistemas Supervisory Control and Data Aquisition (SCADA), que foram utilizados para aprimoramento e eficiência da linha de produção. Hoje, vive-se no limiar da quarta revolução industrial, que se apoia fortemente nas tecnologias habilitadoras, tais como: Internet of Things (IoT), machine learning, big data analytics, cyber-physical systems (CPS), machine-to-machine (M2M) e cloud computing. Essas tecnologias, trabalhando cooperativamente, são utilizadas para promover a transformação digital descrita nas visões ao redor do mundo, entre as quais se destacam: Industry 4.0, Industrial Internet Consortium e Manufatura Avançada. A presente pesquisa visa caracterizar as tecnologias habilitadoras, as visões da transformação digital e o cenário Brasil para esta nova realidade. O objetivo central é a definição de uma plataforma de transformação digital aplicada ao cenário da indústria de Utilities. Para a identificação do setor industrial, optou-se pela aplicação de um questionário direcionado às indústrias da região metropolitana de Campinas. Esse questionário serviu para traçar o nível de conhecimento, aderência e perfil profissional desejado pelas empresas relacionadas com a transformação digital. Na análise do questionário para o grupo estudado, verificou-se desconhecimento estratégico sobre a transformação digital e resistência para aplicação dos novos conceitos na cadeia de suprimentos existente. Para suprir esta necessidade e como prova de conceito foi proposta uma plataforma para transformação digital para a questão da água, com o objetivo de alcançar uma gestão eficiente dos recursos atrelada ao uso racional da água. Para a definição da plataforma foi realizado o levantamento de todo o processo de uma planta de tratamento de água, implantada nos moldes da terceira revolução industrial, de modo a melhorar o processo aplicando-se os conceitos da transformação digital na nova plataforma definida. Como resultado, o estudo de caso contribuiu para o projeto PURA-USP juntamente com o projeto SafeCity, do convênio Huawei-USP no âmbito do conceito de cidades inteligentes, integrando a transformação digital na gestão eficiente dos recursos hídricos no campus USP da capital. Adicionalmente, inseriu-se a capacitação técnica também no projeto Huawei-USP, denominada Centro de Internet do Futuro, habilitando e capacitando os profissionais para as novas tecnologias.Nowadays, there´s a digital transformation in the industry, which is being referred to as a new revolution, known as the fourth industrial revolution. This new revolution was preceded by three previous ones, the first one was based on coal as a way of source energy, boosting the steam engines and turning the manual labor into automated; posteriorly the second industrial revolution was based on electricity concepts to achieve mass production; and the third industrial revolution, which was based on electronics and computer systems, having as its greatest exponent the systems Supervisory Control and Data Acquisition (SCADA) and programmable logic controllers (PLC), which were used for improvement and production line efficiency. Today we are on the bound of the fourth industrial revolution, which strongly supports itself at enabling technologies, such as: Internet of Things (IoT), big data analytics, cyber-physical systems (CPS), machine-to-machine (M2M) and cloud computing. These technologies are working cooperatively used to promote digital transformation described in the sights around the world, between them are: Industry 4.0, Industrial Internet Consortium and Advanced Manufacturing. This research aims to characterize the enabling technologies, the visions of the digital transformation and Brazil scenario for this new reality. The main objective is the definition of a digital transformation platform applied in the industry scenario. To identify the industry it was chosen the application of a questionnaire targeting the industries at the metropolitan region of Campinas. This questionnaire was used to trace the level of knowledge, adherence and professional profile required of companies related to digital transformation. In the questionnaire analysis for the group in the scope there was strategic unfamiliarity about the digital transformation and resistance to application of new concepts in the existing supply chain. To meet this need and as a proof of concept, a platform for digital transformation for the water issue was proposed with the objective of achieve an efficient management of resources linked to the rational use of water. For the platform definition was performed a survey of the whole process of a water treatment plant that was implemented along the lines of the third industrial revolution and improved the process by applying the concepts of digital transformation in the new set platform. The case study aimed to contribute to the PURA-USP project, integrating the digital transformation in the efficient management of water resources on campus USP capital. Additionally, technical training was also included in the Huawei-USP project, denominated the Future Internet Center, enabling and training professionals for new technologies",Digital transformation in the industry: industry 4.0 and the smart network water on Brasil.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.3.2017.tde-28062017-110639,,core
211503018,2017-12-01T00:00:00,"The typical presentation of precipitation and climate information is organised according to the Gregorian calendar defined by the months and, in alignment with the temperate, savanna and desert climate zones of Australia, the three monthly seasons. While this is sufficient for many human-centred operations and the currency acceptable norm, grazing land managers (and potentially workers in other agricultural based enterprises) are reportedly restricted in their use of precipitation and climate information presented in this form. Compounding this issue with standard temporal packaging of climate information is the lack of reliability of the forecasts and spatial resolution capacity in the existing precipitation prediction tools that are being promoted to the graziers. Due to a lack of temporally and spatially robust information, graziers are managing their operations in the presence of significant risks, threatening their contributions to Australia’s $17 billion red meat and other industries.



Grazing land managers require access to near real-time drought data that will enable the timely and informed decisions to be made about the movement of stock from the cattle stations to the grazing and/or growing properties. Providing graziers with this information having appropriate temporal resolution of the drought status for specific locations will enable the most productive use of the available grazing lands to grow the cattle

to specified weights before slaughter. Hence a novel forecasting approach using data driven models of the monthly rainfall decile drought index (RDDI) is being considered in this paper, where the calculation of relative monthly indices are updated on a running weekly basis, providing land managers with spatially and temporally refined information. A similar process is proposed for the determination of relative seasonal rainfall indices, in

addition to the consideration of the alternative definitions of Australian seasons as identified in existing literature.



In future development of this research work, this approach will be used to forecast precipitation patterns, where the machine learning models’ architecture will be trained and evaluated with historical records of precipitation and other significant climate variables from a selection of sites relevant to the cattle industry around Queensland, Australia. The forecasts are to be derived from the novel implementation of a data intensive

hierarchical categorizing support vector machine framework (or alternatively a regression-based data intelligent model) which is being proposed to deliver the graziers with the appropriate information to plan their operations within the stochastic nature of Australia’s climate.



When compared with the seasonal and calendar monthly deciles, the more frequent forecast feeds (i.e., over weekly updated drought status, yet utilizing the concept of decile-based drought) presents a more detailed and robustly reported distribution of rain over the future seasons at specific sites, whilst catering to the graziers’ reported decision making processes. The more temporally refined presentation of the predicted rainfall events,

for specified sites, has the potential to provide graziers (and other agricultural ventures), with the most relevant information to allow for more confident and profitable management of their enterprises",Re-imagining standard timescales in forecasting precipitation events for Queensland’s grazing enterprises,,Modelling and Simulation Society of Australia and New Zealand Inc.,,,core
474975486,2016-01-01T00:00:00,"With renewable energy becoming more common, energy prices fluctuate more depending on environmental factors such as the weather. Consuming energy without taking volatile prices into consideration can not only become expensive, but may also increase the peak load, which requires energy providers to generate additional energy using less environment-friendly methods. In the Netherlands, pumping stations that maintain the water levels of polder canals are large energy consumers, but the controller software currently used in the industry does not take real-time energy availability into account. We investigate if existing AI planning techniques have the potential to improve upon the current solutions. In particular, we propose a light weight but realistic simulator and investigate if an online planning method (UCT) can utilise this simulator to improve the cost-efficiency of pumping station control policies. An empirical comparison with the current control algorithms indicates that substantial cost, and thus peak load, reduction can be attained",Energy- and Cost-Efficient Pumping Station Control,,AAAI Press,,,core
108915114,2016-10-04,"Abstract—Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial softwar",Software Process Evaluation: A Machine Learning Approach,,,,,core
299912629,2016-11-24,"One consequence of the pervasive use of computers is that most documents originate in digital form. Widespread use of the Internet makes them readily available. Text mining – the process of analyzing unstructured natural-language text – is concerned with how to extract information from these documents.

Developed from the authors’ highly successful Springer reference on text mining, Fundamentals of Predictive Text Mining is an introductory textbook and guide to this rapidly evolving field. Integrating topics spanning the varied disciplines of data mining, machine learning, databases, and computational linguistics, this uniquely useful book also provides practical advice for text mining. In-depth discussions are presented on issues of document classification, information retrieval, clustering and organizing documents, information extraction, web-based data-sourcing, and prediction and evaluation. Background on data mining is beneficial, but not essential. Where advanced concepts are discussed that require mathematical maturity for a proper understanding, intuitive explanations are also provided for less advanced readers.

Topics and features: presents a comprehensive, practical and easy-to-read introduction to text mining; includes chapter summaries, useful historical and bibliographic remarks, and classroom-tested exercises for each chapter; explores the application and utility of each method, as well as the optimum techniques for specific scenarios; provides several descriptive case studies that take readers from problem description to systems deployment in the real world; includes access to industrial-strength text-mining software that runs on any computer; describes methods that rely on basic statistical techniques, thus allowing for relevance to all languages (not just English); contains links to free downloadable software and other supplementary instruction material.

Fundamentals of Predictive Text Mining is an essential resource for IT professionals and managers, as well as a key text for advanced undergraduate computer science students and beginning graduate students",Fundamentals of Predictive Text Mining,,Springer,,,core
201560971,2017-04-01T00:00:00,"For centuries, humans’ capacity to capture and depict physical space has played a central role in industrial and societal development. However, the digital revolution and the emergence of networked devices and services accelerate geospatial capture, coordination, and intelligence in unprecedented ways. Underlying the digital transformation of industry and society is the fusion of the physical and digital worlds – ‘perceptality’ – where geospatial perception and reality merge. This paper analyzes the myriad forces that are driving perceptality and the future of geospatial intelligence and presents real-world implications and examples of its industrial application. Applications of sensors, robotics, cameras, machine learning, encryption, cloud computing and other software, and hardware intelligence are converging, enabling new ways for organizations and their equipment to perceive and capture reality. Meanwhile, demands for performance, reliability, and security are pushing compute ‘to the edge’ where real-time processing and coordination are vital. Big data place new restraints on economics, as pressures abound to actually use these data, both in real-time and for longer term strategic analysis and decision-making. These challenges require orchestration between information technology (IT) and operational technology (OT) and synchronization of diverse systems, data-sets, devices, environments, workflows, and people",The future of geospatial intelligence,,'Informa UK Limited',10.1080/10095020.2017.1337318,"[{'title': 'Geo-spatial Information Science', 'identifiers': ['issn:1993-5153', 'issn:1009-5020', '1993-5153', '1009-5020']}]",core
144786975,2017-06-12,"Vitaprint extruder 1.0
Table of contents

Bill of materials
Housing
Movement
Thermoregulation


Manufacturing and Assembly
Overview
Part Manufacturing
Assembly Brief
Arduino Temperature Regulation


Extruder calibration
calibration set-up
calibration procedure


Thermistor calibration

Bill of materials <a id=""billmat""></a>
Housing: <a id=""house""></a>

Aluminium plates (300mm x 400mm x 5mm)
Aluminium blocks:
SYRINGE MOUNT: block 35mm x 45mm x 70mm (minimum)
FRONT CAP: block 40mm x 12mm x 60mm (minimum)
24BYJ48 NUT: block 15mm x 25mm x 30mm (minimum)


M3x12 DIN965 philips screw 10 pcs
M3x12 BN6404 torx sxrew 20pcs
2 x fi3 stainless steel rod (L = 110mm)
40mm x 8mm x 2mm rubber sheet

Movement <a id=""mov""></a>

Stepper motor Nema11 planetary (1:5 gear ratio)
28BYJ48 unipolar motor ([modified into a bipolar motor] http://www.jangeox.be/2013/10/change-unipolar-28byj-48-to-bipolar.html )
Linear rail with carrige (L = 100mm)
Sliding rods (stainles steel, fi 3mm)
Brass spindle (M5x0.5mm, L = 90mm)
NUT: 3D printed 100% infill ABS block (28x28x55mm)

Thermoregulation <a id=""thermo""></a>

Thermistor (read how to calibrate your own thermistor in the Calibration section)
40W ceramic heater for 3D printers
24V DC power supply
In our iteration we used PID thermoregulation algorithm (parameters need to be tuned by the user, depending on the control setup). Other temperature control systems can also be applied with this given hardware (relay switchin etc.).

Manufacturing and Assembly <a id=""manuass""></a>
1 Overview and Toolset <a id=""OVER""></a>
Here you can find manufacturing and assembly details for one Vitaprint unit. In this repository you can find STEP files for the Vitaprint unit in both orientations - Left and Right. The procedure is the same for both orientations (the final product is only mirrored) therefore you will find details for assembly for one unit in this repository.
1.1 General

Manufacturing time: approximately 8h
Assembly time: 30 min

NOTE: manufacturing and assembly time estimates assume well-skilled user and presence of all the tools listed below
1.2 Tool List and Skills Required

3D printer (ABS)
CNC mill
CNC router
band saw
lathe
HAND TOOLS:
torx srewdriver
phillips screwdriver
super glue
two-sided tape
scalpel



1.3 Necessary Steps

purchase of raw materials and components
manufacturing of parts
assembly
calibration

2 Part Manufacturing <a id=""MANUFACTURING""></a>
<img src=""https://cloud.githubusercontent.com/assets/14543226/24997531/3aaafcba-2037-11e7-8800-1aba4ec7eacb.png"" alt=""bubble"" width=""450"" height=""400"">
<img src=""https://cloud.githubusercontent.com/assets/14543226/24997449/fa3d8c7e-2036-11e7-8a56-a97ed070c891.png"" alt=""table"" width=""500"" height=""600"">
3 Assembly Details <a id=""ASSEMBLY""></a>

manufacture all parts
assemble each section separately, then join them together
| SECTION A | SECTION B | SECTION C |
|-----------|-----------|-----------|
|<img src=""https://cloud.githubusercontent.com/assets/14543226/24998545/c2ab57ce-203a-11e7-9a3c-8f3a5b85e27a.png"" alt=""sectionB"" width= ""150"" > | <img src=""https://cloud.githubusercontent.com/assets/14543226/24998406/4e023a1e-203a-11e7-8f65-00fdcdb5f56f.png"" alt=""sectionB"" width= ""150"" > |<img src=""https://cloud.githubusercontent.com/assets/14543226/24998669/3aab29d4-203b-11e7-93f6-95d102f12aa9.png"" alt=""sectionB"" width= ""150"">|



Use philips DIN965 screws wherever you see the hole chamfers, use torx BN6404 screws elsewhere
for the following steps gently apply hammer:
sliding the ball bearing into the rod holder
sliding the M5 threaded rod into the coupler
sliding the fi3 stainless steel rod into rod holder at the bottom and N11 motor plate at the top



4 Arduino: Temperature Regulation <a id=""CODE""></a>
To download Arduino firmware for temperature regulation please visit vitaprint_heat_regulator. This is an external temperature control system and can be replaced by any system you may have available.
Extruder calibration <a id=""extcal""></a>
To ensure the stepper movement translates into the piston movement accordingly to the g-code, it needs to be calibrated properly.
Calibration set-up <a id=""calset""></a>
In principle, the calibration set-up requires:

CNC control of the piston powering motor (we use Nema 11 with a planetary reductor)
the assembled extruder (at least the moving parts)
a precise displacement measurement device (calipers, etc., high precision indicators are preferrable)

The calibration protocol requires a set-up, where the piston translation, can be precisely measured in the axis of extrusion. For this purpose, the measurement device is fixed on the extruder in a way, that the extruder movement is translated directly into the measurement.
Calibration procedure <a id=""calpro""></a>

adjust the steps/unit value of the motor settings in the cnc control software (or use default values)
command piston movement for an exact distance
measure the real movement distance and compare
repeat steps 1-3, until the g-code value coincides with the measured value

For a Nema 11 motor (with a planetary reductor) + an M5 spindle (0.5mm pitch/rotation), the default STEPS/UNIT value is 37914.30.
Thermistor calibration <a id=""thermcal""></a>
Every thermistor analog output values have to be mapped to actual temperature values via an equation. Every thermistor needs to be calibrated as described in this section.
EQUIPMENT: thermo block/plate (heat source), alcochol thermometer, thermistor, setup for reading analog signal from the thermistor, oil, glass.

pour a small glass of oil (50mL or so)
place the glass on the heat source
fit a thermistor to the alcochol thermometer
sink the thermistor - thermometer setup into the glass with oil
prepare the system to read analog values 
turn on the heat source and record sensor analog output at every 1°C increment
plot data (x - axis: analog output, y-axis: real temperature)
fit the best trendline to the data obtained (MATLAB or EXCEL are good tools for completing this task but there are also others)
software will yield a fit line equation which then has to be transcribed into microcontroller firmware (we used Arduino Mega)

Now your thermistor will give out the right temperature values.
step_vitaprint.zi",symbiolab/Vitaprint_extruder: Vitaprint extruder,,,10.5281/zenodo.806762,,core
159324899,2017-09-18T00:00:00,"Conference of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2017 ; Conference Date: 18 September 2017 Through 22 September 2017; Conference Code:209269International audienceWe present WHODID: a turnkey intuitive web-based interface for fault detection, identification and diagnosis in production units. Fault detection and identification is an extremely useful feature and is becoming a necessity in modern production units. Moreover, the large deployment of sensors within the stations of a production line has enabled the close monitoring of products being manufactured. In this context, there is a high demand for computer intelligence able to detect and isolate faults inside production lines, and to additionally provide a diagnosis for maintenance on the identified faulty production device, with the purpose of preventing subsequent faults caused by the diagnosed faulty device behavior. We thus introduce a system which has fault detection, isolation, and identification features, for retrospective and on-the-fly monitoring and maintenance of complex dynamical production processes. It provides real-time answers to the questions: "" is there a fault? "" , "" where did it happen? "" , "" for what reason? "". The method is based on a posteriori analysis of decision sequences in XGBoost tree models, using recurrent neural networks sequential models of tree paths. The particularity of the presented system is that it is robust to missing or faulty sensor measurements, it does not require any modeling of the underlying, possibly exogenous manufacturing process, and provides fault diagnosis along with confidence level in plain English formulations. The latter can be used as maintenance directions by a human operator in charge of production monitoring and control","WHODID: Web-based interface for Human-assisted factory Operations in fault Detection, Identification and Diagnosis",,'Springer Science and Business Media LLC',10.1007/978-3-319-71273-4_47,,core
44550335,2016-01-01T00:00:00,"The modern financial industry has been required to deal with large and diverse portfolios in a variety of asset classes often with limited market data available. Financial Signal Processing and Machine Learning unifies a number of recent advances made in signal processing and machine learning for the design and management of investment portfolios and financial engineering. This book bridges the gap between these disciplines, offering the latest information on key topics including characterizing statistical dependence and correlation in high dimensions, constructing effective and robust risk measures, and their use in portfolio optimization and rebalancing. The book focuses on signal processing approaches to model return, momentum, and mean reversion, addressing theoretical and implementation aspects. It highlights the connections between portfolio theory, sparse learning and compressed sensing, sparse eigen-portfolios, robust optimization, non-Gaussian data-driven risk measures, graphical models, causal analysis through temporal-causal modeling, and large-scale copula-based approaches. Key features: * Highlights signal processing and machine learning as key approaches to quantitative finance. * Offers advanced mathematical tools for high-dimensional portfolio construction, monitoring, and post-trade analysis problems. * Presents portfolio theory, sparse learning and compressed sensing, sparsity methods for investment portfolios. including eigen-portfolios, model return, momentum, mean reversion and non-Gaussian data-driven risk measures with real-world applications of these techniques. * Includes contributions from leading researchers and practitioners in both the signal and information processing communities, and the quantitative finance community",Financial signal processing and machine learning,,Wiley-IEEE Press,,,core
73893860,2016-06-15,"With the booming of large scale data related applications, cognitive systems that leverage modern data processing technologies, e.g., machine learning and data mining, are widely used in various industry fields. These application bring challenges to conventional computer systems on both semiconductor manufacturing and computing architecture. The invention of neuromorphic computing system (NCS) is inspired by the working mechanism of human-brain. It is a promising architecture to combat the well-known memory bottleneck in Von Neumann architecture. The recent breakthrough on memristor devices and crossbar structure made an important step toward realizing a low-power, small-footprint NCS on-a-chip. However, the currently low manufacturing reliability of nano-devices and circuit level constrains, .e.g., the voltage IR-drop along metal wires and analog signal noise from the peripheral circuits, bring challenges on scalability, precision and robustness of memristor crossbar based NCS.\ud
In this dissertation, we quantitatively analyzed the robustness of memristor crossbar based NCS when considering the device process variations, signal fluctuation and IR-drop. Based on our analysis, we will explore deep understanding on hardware training methods, e.g., on-device training and off-device training. Then, new technologies, e.g., noise-eliminating training, variation-aware training and adaptive mapping, specifically designed to improve the training quality on memristor crossbar hardware will be proposed in this dissertation. A digital initialization step for hardware training is also introduced to reduce training time. The circuit level constrains\ud
will also limit the scalability of a single memristor crossbar, which will decrease the efficiency of implementation of NCS. We also leverage system reduction/compression techniques to reduce the required crossbar size for certain applications. Besides, running machine learning algorithms on embedded systems bring new security concerns to the service providers and the users. In this dissertation, we will first explore the security concerns by using examples from real applications. These examples will demonstrate how attackers can access confidential user data, replicate a sensitive data processing model without any access to model details and how expose some key features of training data by using the service as a normal user. Based on our understanding of these security concerns, we will use unique property of memristor device to build a secured NCS",Neuromorphic System Design and Application,,,,,core
104081211,2016-08-14,"Abstract—In this paper we explore the concept of a real-time workflow collaboration platform. The work presents how a cloud-based Workflow Management System (WfMS) combines the technologic features which are offered by the cloud computing paradigm with a developed resource model for collaboration and reuse of experiential knowledge in workflows. It is based on a prototypical generic software system for integrated process and knowledge management and addresses the concept for collaborative workflow modeling. The concept for the reuse of workflows combines the ability of the resource model to share workflows with Case-Based Reasoning, a specific field of Artificial Intelligence. In particular, a sample workflow of a process in the financial industry is discussed. By enabling collaborative workflow modeling and by providing expert knowledge to large group of users, we aim at the improvement of the quality of workflows. Consequently, workload can be reduced, thus facilitating the work of all process stakeholders. I",Real-Time Collaboration and Experience Reuse For Cloud-Based Workflow Management Systems,,,,,core
222994415,2017-01-01T08:00:00,"The U.S. Securities and Exchange Commission (SEC) maintains a publicly-accessible database of all required filings of all publicly traded companies. Known as EDGAR (Electronic Data Gathering, Analysis, and Retrieval), this database contains documents ranging from annual reports of major companies to personal disclosures of senior managers. However, the common user and particularly the retail investor are overwhelmed by the deluge of information, not empowered. EDGAR as it currently functions entrenches the information asymmetry between these retail investors and the large financial institutions with which they often trade. With substantial research staffs and budgets coupled to an industry standard of “playing both sides” of a transaction, these investors “in the know” lead price fluctuations while others must follow.
In general, this thesis applies recent technological advancements to the development of software tools that will derive valuable insights from EDGAR documents in an efficient time period. While numerous such commercial products currently exist, all come with significant price tags and many still rely on significant human involvement in deriving such insights. Recent years, however, have seen an explosion in the fields of Machine Learning (ML) and Natural Language Processing (NLP), which show promise in automating many of these functions with greater efficiency. ML aims to develop software which learns parameters from large datasets as opposed to traditional software which merely applies a programmer’s logic. NLP aims to read, understand, and generate language naturally, an area where recent ML advancements have proven particularly adept.
Specifically, this thesis serves as an exploratory study in applying recent advancements in ML and NLP to the vast range of documents contained in the EDGAR database. While algorithms will likely never replace the hordes of research analysts that now saturate securities markets nor the advantages that accrue to large and diverse trading desks, they do hold the potential to provide small yet significant insights at little cost.
This study first examines methods for document acquisition from EDGAR with a focus on a baseline efficiency sufficient for the real-time trading needs of market participants. Next, it applies recent advancements in ML and NLP, specifically recurrent neural networks, to the task of standardizing financial statements across different filers. Finally, the conclusion contextualizes these findings in an environment of continued technological and commercial evolution",Realizing EDGAR: eliminating information asymmetries through artificial intelligence analysis of SEC filings,https://core.ac.uk/download/222994415.pdf,UNI ScholarWorks,,,core
132212972,2017-11-08T00:00:00,"This paper will describe and present results for a local flood risk reduction system which utilises existing in-network storage capacity to attenuate flow peaks. The storage capacity is mobilised through active flow control automatically regulated by an Artificial Intelligence system using local level monitoring. The effects of climate change, population growth and urbanisation are putting increasing pressure on sewer and drainage networks both in the UK and overseas. The capacity of networks to cope with runoff at the required rate often falls short of requirements leading to localised floods and/or increased CSO spills to receiving waters. Smart Water/ Wastewater Network technologies have the potential to deliver improved service to customers and cost-effective performance improvements for the water industry. CENTAUR aims to provide an innovative, cost effective, local autonomous data driven in-sewer flow control system whose operation will attenuate peaks and reduce the risk of surface water flooding. The system enables the capacity of existing infrastructure to be utilised more efficiently as a very economical alternative to capital-intensive solutions, for example building extra storage capacity. The system is also quick to implement with virtually no enabling works prior to installation. CENTAUR comprises level monitors which relay data to an intelligent controller, which instructs a flow control device regulated by a novel and robust artificial intelligence routine based on Fuzzy Logic. The level monitors and intelligent controller are located locally and utilise real time data to provide effective real time control (RTC). The CENTAUR Fuzzy Logic control algorithm was developed in Matlab. The Matlab RTC algorithm was linked to a SWMM hydro-dynamic model of a test network to prove its efficiency. Further rigorous testing was carried out by the University of Sheffield on the full-scale test facility designed to replicate field conditions. The CENTAUR system has been further developed and it is now implemented and fully functional in trial site in Coimbra, Portugal. Results of successful testing in the laboratory and the Coimbra field trial will be presented",CENTAUR: Smart Utilisation of Wastewater storage capacity to prevent flooding,,CIWEM (The Chartered Institution of Water and Environmental Management),10.5281/zenodo.1051200,,core
154431480,2017-03-01T00:00:00,"Increased market demand for composite products and shortage of expert laminators is compelling the

composite industry to explore ways to acquire layup skills from experts and transfer them to novices and

eventually to machines. There is a lack of holistic methods in literature for capturing composite layup

skills especially involving complex moulds. This research aims to develop an informatics-based method,

enabled by consumer-grade gaming technology and machine learning, to capture and digitise

manufacturing task knowledge from skill-intensive hand layup. The digitisation is underpinned by the

proposed human-workpiece interaction theory and implemented to automatically extract and decode

key knowledge constituents such as layup strategies, ply manipulation techniques, motion mechanics

and problem-solving during hand layup, collectively categorised as layup skills. The significance of this

research is its potential to facilitate cost-effective transfer of skills from experts to novices, real-time

automated supervision of hand layup and automation of layup tasks in the future",Digitisation of manual composite layup task knowledge using gaming technology,https://core.ac.uk/download/154431480.pdf,'Elsevier BV',10.1016/j.compositesb.2016.12.050,,core
141539396,2017-12-07T22:21:37,"A gradual move in the electric power industry towards Smart Grids brings new challenges to the system's  efficiency and dependability. With a growing complexity and massive introduction of renewable generation,  particularly at the distribution level, the number of faults and, consequently, disturbances (errors and failures) is  expected to increase significantly. This threatens to compromise grid's availability as traditional, reactive  management approaches may soon become insufficient. On the other hand, with grids' digitalization, real-time  status data are becoming available. These data may be used to develop advanced management and control  methods for a sustainable, more efficient and more dependable grid. A proactive management approach, based  on the use of real-time data for predicting near-future disturbances and acting in their anticipation, has already  been identified by the Smart Grid community as one of the main pillars of dependability of the future grid. The  work presented in this dissertation focuses on predicting disturbances in Active Distributions Networks (ADNs)  that are a part of the Smart Grid that evolves the most. These are distribution networks with high share of  (renewable) distributed generation and with systems in place for real-time monitoring and control. Our main goal is  to develop a methodology for proactive network management, in a sense of proactive mitigation of disturbances,  and to design and implement a method for their prediction. We focus on predicting voltage sags as they are  identified as one of the most frequent and severe disturbances in distribution networks. We address Smart Grid  dependability in a holistic manner by considering its cyber and physical aspects. As a result, we identify Smart  Grid dependability properties and develop a taxonomy of faults that contribute to better understanding of the  overall dependability of the future grid. As the process of grid's digitization is still ongoing there is a general  problem of a lack of data on the grid's status and especially disturbance-related data. These data are necessary to  design an accurate disturbance predictor. To overcome this obstacle we introduce a concept of fault injection to  simulation of power systems. We develop a framework to simulate a behavior of distribution networks in the  presence of faults, and fluctuating generation and load that, alone or combined, may cause disturbances. With the  framework we generate a large set of data that we use to develop and evaluate a voltage-sag disturbance  predictor. To quantify how prediction and proactive mitigation of disturbances enhance availability we create an  availability model of a proactive management. The model is generic and may be applied to evaluate the effect of  proactive management on availability in other types of systems, and adapted for quantifying other types of  properties as well. Also, we design a metric and a method for optimizing failure prediction to maximize availability  with proactive approach. In our conclusion, the level of availability improvement with proactive approach is  comparable to the one when using high-reliability and costly components. Following the results of the case study  conducted for a 14-bus ADN, grid's availability may be improved by up to an order of magnitude if disturbances  are managed proactively instead of reactively. The main results and contributions may be summarized as follows:  (i) Taxonomy of faults in Smart Grid has been developed; (ii) Methodology and methods for proactive  management of disturbances have been proposed; (iii) Model to quantify availability with proactive management  has been developed; (iv) Simulation and fault-injection framework has been designed and implemented to  generate disturbance-related data; (v) In the scope of a case study, a voltage-sag predictor, based on machine- learning classification algorithms, has been designed and the effect of proactive disturbance management on  downtime and availability has been quantified",Online disturbance prediction for enhanced availability in smart grids,https://core.ac.uk/download/141539396.pdf,,,,core
160081036,2017-09-18T00:00:00,"Conference of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2017 ; Conference Date: 18 September 2017 Through 22 September 2017; Conference Code:209269International audienceWe present WHODID: a turnkey intuitive web-based interface for fault detection, identification and diagnosis in production units. Fault detection and identification is an extremely useful feature and is becoming a necessity in modern production units. Moreover, the large deployment of sensors within the stations of a production line has enabled the close monitoring of products being manufactured. In this context, there is a high demand for computer intelligence able to detect and isolate faults inside production lines, and to additionally provide a diagnosis for maintenance on the identified faulty production device, with the purpose of preventing subsequent faults caused by the diagnosed faulty device behavior. We thus introduce a system which has fault detection, isolation, and identification features, for retrospective and on-the-fly monitoring and maintenance of complex dynamical production processes. It provides real-time answers to the questions: "" is there a fault? "" , "" where did it happen? "" , "" for what reason? "". The method is based on a posteriori analysis of decision sequences in XGBoost tree models, using recurrent neural networks sequential models of tree paths. The particularity of the presented system is that it is robust to missing or faulty sensor measurements, it does not require any modeling of the underlying, possibly exogenous manufacturing process, and provides fault diagnosis along with confidence level in plain English formulations. The latter can be used as maintenance directions by a human operator in charge of production monitoring and control","WHODID: Web-based interface for Human-assisted factory Operations in fault Detection, Identification and Diagnosis",,'Springer Science and Business Media LLC',10.1007/978-3-319-71273-4_47,,core
299967783,2017-01-01T00:00:00,"The proliferation of artificial intelligence technologies and computerization has given rise to an era of automation. Human and robot interaction is now inevitable as robotic systems has become one of the pillars of industrial growth. From primordial tools like spears and stones in the past to the intricate robots used today, the relationship between man and tools is ever-changing. The robots, coupled with multiple customizations, must satisfy the needs of their respective industries to allow operators to utilise the robots to their fullest potential. As such, human-robot interaction should keep up with the roles of the robots as they evolve to suit industrial needs. Today, the human-robot systems are shifting their focus to the collaboration between the two, by using a more interactive human-robot interface to facilitate the system. However, the current keyboards and mouse interface is inadequate in meeting the requirements of the autonomous and semi-autonomous robots employed in the industrial applications. These industries applications demand swift and effective communication and exchange of data between human and robot. Therefore, the purpose of this project will be to improve human-robot systems through using augmented reality technology which is on the rise. This project employs the augmented reality technology to enhance the collaborative nature (2-way interaction) of human-robot systems by improving its system's interface, and aims to aid the control of robotic systems via a single-handed multi-modal interface for automatic robot taping. The taping is done autonomously by an agile robotic system with intuitive control. A unique framework was implemented for a collaborative human-robot interaction, incorporating a user-centered design to enhance feedback to users which improves robot usability.Bachelor of Engineering (Mechanical Engineering",Single-handed multi-modal interface for robotic application in an industrial environment,,,,,core
347692253,2016-11-14T00:00:00,"Nowadays, monitoring of people and events is a common matter in the street, in the industry or at home, and acoustic event detection is commonly used. This increases the knowledge of what is happening in the soundscape, and this information encourages any monitoring system to take decisions depending on the measured events. Our research in this field includes, on one hand, smart city applications, which aim is to develop a low cost sensor network for real time noise mapping in the cities, and on the other hand, ambient assisted living applications through audio event recognition at home. This requires acoustic signal processing for event recognition, which is a challenging problem applying feature extraction techniques and machine learning methods. Furthermore, when the techniques come closer to implementation, a complete study of the most suitable platform is needed, taking into account computational complexity of the algorithms and commercial platforms price. In this work, the comparative study of several platforms serving to implement this sensing application is detailed. An FPGA platform is chosen as the optimum proposal considering the application requirements and taking into account time restrictions of the signal processing algorithms. Furthermore, we describe the first approach to the real-time implementation of the feature extraction algorithm on the chosen platform",An FPGA Platform Proposal for Real-Time Acoustic Event Detection: Optimum Platform Implementation for Audio Recognition with Time Restrictions,,'MDPI AG',,,core
267814382,2017-01-01T00:00:00,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.Distributed System",ANANKE: a Q-Learning-Based Portfolio Scheduler for Complex Industrial Workflows,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICAC.2017.21,,core
75987132,2016-06-01T00:00:00,"Nowadays, the miniaturization of many consumer products is extending the use of micro-milling operations with high-quality requirements. However, the impacts of cutting-tool wear on part dimensions, form and surface integrity are not negligible and part quality assurance for a minimum production cost is a challenging task. In fact, industrial practices usually set conservative cutting parameters and early cutting replacement policies in order to minimize the impact of cutting-tool wear on part quality. Although these practices may ensure part integrity, the production cost is far away to be minimized, especially in highly tool-consuming operations like mold and die micro-manufacturing. In this paper, an adaptive control optimization (ACO) system is proposed to estimate cutting-tool wear in terms of part quality and adapt the cutting conditions accordingly in order to minimize the production cost, ensuring quality specifications in hardened steel micro-parts. The ACO system is based on: (1) a monitoring sensor system composed of a dynamometer, (2) an estimation module with Artificial Neural Networks models, (3) an optimization module with evolutionary optimization algorithms, and (4) a CNC interface module. In order to operate in a nearly real-time basis and facilitate the implementation of the ACO system, different evolutionary optimization algorithms are evaluated such as particle swarm optimization (PSO), genetic algorithms (GA), and simulated annealing (SA) in terms of accuracy, precision, and robustness. The results for a given micro-milling operation showed that PSO algorithm performs better than GA and SA algorithms under computing time constraints. Furthermore, the implementation of the final ACO system reported a decrease in the production cost of 12.3 and 29 % in comparison with conservative and high-production strategies, respectively",Adaptive control optimization in micro-milling of hardened steels-evaluation of optimization approaches,https://core.ac.uk/download/75987132.pdf,'Springer Science and Business Media LLC',10.1007/s00170-015-7807-6,,core
197955670,2017-01-01T00:00:00,"Arguably the biggest strength of the functional programming language Erlang is how straightforward it is to implement concurrent and distributed programs with it. Numerical computing, on the other hand, is not necessarily seen as one of its strengths. The recent introduction of Federated Learning, a concept according to which edge devices are leveraged for decentralized machine learning tasks, while a central server only updates and distributes a global model, provided the motivation for exploring how well Erlang was suited to such a use case.We present a framework for Federated Learning in Erlang, written in a purely functional style, and compare two versions of it: one that has been exclusively written in Erlang, and one in which Erlang is relegated to coordinating client processes that rely on performing numerical computations in the programming language C. Initial results are promising, as we learnt that a real-world industrial use case of distributed data analytics can easily be tackled with a system purely written in Erlang.The novelty of our work is that we present the first implementation of a Federated Learning framework in a functional programming language, with the added benefit of being purely functional. In addition, we demonstrate that Erlang can not only be leveraged for message passing but also performs adequately for practical machine learning tasks",Purely Functional Federated Learning in Erlang,,,,,core
161640860,2017-01-01T00:00:00,"In line with 4th industrial revolution (Industry 4.0), the mechatronics and related areas
are fundamental to boost the developments of industry digitalization. However, it should not be
forgotten that artificial intelligence (AI) has a great preponderance on the development of
autonomous and intelligent systems incorporating the advances in mechatronics systems. It is
common in different industries the need to identify and recognize products or objects for different
purposes such as counts, quality control, selection of objects, among others. For these reasons,
pattern recognition is increasingly being used in systems on the shop floor, usually implemented in
computer vision systems with image processing in real time. This work focuses on automatic
detection and text recognition in unstructured images for use on shop floor mechatronic systems
with vision systems, to identify and recognize patterns in products. Unstructured images are
images that does not have a pre-defined image model or is not organized in a predefined manner.
Which means that there is no predefined calibration model, the system must identify and learn by
itself to recognize the text patterns. The techniques of character recognition, also known as OCR
(Optical Character Reader), are not new in the industry, however the use of machine learning
algorithms together with the existing techniques of OCR, allow endow the systems of greater
intelligence in the patterns recognition. The results achieved throughout the paper, demonstrates
that it is possible to identify and recognize text in objects based on unstructured images with a high
level of accuracy and that these algorithms can be used in real time applications.info:eu-repo/semantics/publishedVersio",Text recognition for objects identification in the industry,,,,"[{'title': 'International Journal of Mechatronics and Applied Mechanics', 'identifiers': ['issn:2559-6497', '2559-6497']}]",core
43007430,2016-06-15T00:00:00,"With the booming of large scale data related applications, cognitive systems that leverage modern data processing technologies, e.g., machine learning and data mining, are widely used in various industry fields. These application bring challenges to conventional computer systems on both semiconductor manufacturing and computing architecture. The invention of neuromorphic computing system (NCS) is inspired by the working mechanism of human-brain. It is a promising architecture to combat the well-known memory bottleneck in Von Neumann architecture. The recent breakthrough on memristor devices and crossbar structure made an important step toward realizing a low-power, small-footprint NCS on-a-chip. However, the currently low manufacturing reliability of nano-devices and circuit level constrains, .e.g., the voltage IR-drop along metal wires and analog signal noise from the peripheral circuits, bring challenges on scalability, precision and robustness of memristor crossbar based NCS.

In this dissertation, we quantitatively analyzed the robustness of memristor crossbar based NCS when considering the device process variations, signal fluctuation and IR-drop. Based on our analysis, we will explore deep understanding on hardware training methods, e.g., on-device training and off-device training. Then, new technologies, e.g., noise-eliminating training, variation-aware training and adaptive mapping, specifically designed to improve the training quality on memristor crossbar hardware will be proposed in this dissertation. A digital initialization step for hardware training is also introduced to reduce training time. The circuit level constrains

will also limit the scalability of a single memristor crossbar, which will decrease the efficiency of implementation of NCS. We also leverage system reduction/compression techniques to reduce the required crossbar size for certain applications. Besides, running machine learning algorithms on embedded systems bring new security concerns to the service providers and the users. In this dissertation, we will first explore the security concerns by using examples from real applications. These examples will demonstrate how attackers can access confidential user data, replicate a sensitive data processing model without any access to model details and how expose some key features of training data by using the service as a normal user. Based on our understanding of these security concerns, we will use unique property of memristor device to build a secured NCS",Neuromorphic System Design and Application,https://core.ac.uk/download/43007430.pdf,,,,core
81996112,2016-12-31,"AbstractSewer systems require regular inspection in order to ensure their satisfactory condition. As most sewer networks consist of pipes too small for engineers to traverse, CCTV footage is used to record the interior of these pipes. This footage is manually analysed by qualified engineers, to determine the condition of the pipe and the presence of any faults. We propose a methodology, which automatically detects faults within the CCTV footage. This has the potential to dramatically reduce the time required to process the large volume of CCTV footage produced during a survey. The proposed methodology first characterises localised regions of each video frame using multiscale GIST features. Extremely randomised trees are then used to learn a classifier that distinguishes between frames showing a fault and normal frames. The technique is tested on 670 video segments from real sewer inspections of a variety of pipes, supplied by Wessex Water. Detection performance is assessed by plotting receiver operating characteristics and quantifying the area under the curve. Preliminary results indicate high detection accuracy of 88% and an area under the ROC curve of 96%. The machine learning used reduces the footage to a selection of frames containing faults, which can be quickly identified (whether by an engineer or another piece of software), showing promise for use in industrial wastewater network surveys",Automated Detection of Faults in Wastewater Pipes from CCTV Footage by Using Random Forests ,https://core.ac.uk/download/pdf/81996112.pdf,Published by Elsevier Ltd.,10.1016/j.proeng.2016.07.416,,core
132618351,2017-09-01T00:00:00,"The advent of big data has created opportunities for firms to customize their products and services to unprecedented levels of granularity. Using big data to personalize an offering in real time, however, remains a major challenge. In the mobile advertising industry, once a customer enters the network, an ad-serving decision must be made in a matter of milliseconds. In this work, we describe the design and implementation of an ad-serving algorithm that incorporates machine-learning methods to make personalized ad-serving decisions within milliseconds. We developed this algorithm for Vungle Inc., one of the largest global mobile ad networks. Our approach also addresses other important issues that most ad networks face, such as user fatigue, budget restrictions, and campaign pacing. In an A/B test versus the company’s legacy algorithm, our algorithm generated a 23 percent increase in revenue per 1,000 impressions. Across the company’s network, this increase represents a $1 million increase in monthly revenue",Vungle Inc. Improves Monetization Using Big Data Analytics,https://core.ac.uk/download/132618351.pdf,'Institute for Operations Research and the Management Sciences (INFORMS)',10.1287/inte.2017.0903,,core
141671449,2016-01-01T08:00:00,"Deep learning consists of various machine learning algorithms that aim to learn multiple levels of abstraction from data in a hierarchical manner. It is a tool to construct models using the data that mimics a real world process without an exceedingly tedious modelling of the actual process. We show that deep learning is a viable solution to decision making in mechanical engineering problems and complex physical systems.
In this work, we demonstrated the application of this data-driven method in the design of microfluidic devices to serve as a map between the user-defined cross-sectional shape of the flow and the corresponding arrangement of micropillars in the flow channel that contributed to the flow deformation. We also present how deep learning can be used in the early detection of combustion instability for prognostics and health monitoring of a combustion engine, such that appropriate measures can be taken to prevent detrimental effects as a result of unstable combustion.
One of the applications in complex systems concerns robotic path planning via the systematic learning of policies and associated rewards. In this context, a deep architecture is implemented to infer the expected value of information gained by performing an action based on the states of the environment. We also applied deep learning-based methods to enhance natural low-light images in the context of a surveillance framework and autonomous robots. Further, we looked at how machine learning methods can be used to perform root-cause analysis in cyber-physical systems subjected to a wide variety of operation anomalies. In all studies, the proposed frameworks have been shown to demonstrate promising feasibility and provided credible results for large-scale implementation in the industry",Deep Learning for Decision Making and Autonomous Complex Systems,https://core.ac.uk/download/141671449.pdf,Iowa State University Digital Repository,,,core
129351474,2017-10-20T00:00:00,"In this paper, we present an automated feature engineering based approach to
dramatically reduce false positives in fraud prediction. False positives plague
the fraud prediction industry. It is estimated that only 1 in 5 declared as
fraud are actually fraud and roughly 1 in every 6 customers have had a valid
transaction declined in the past year. To address this problem, we use the Deep
Feature Synthesis algorithm to automatically derive behavioral features based
on the historical data of the card associated with a transaction. We generate
237 features (>100 behavioral patterns) for each transaction, and use a random
forest to learn a classifier. We tested our machine learning model on data from
a large multinational bank and compared it to their existing solution. On an
unseen data of 1.852 million transactions, we were able to reduce the false
positives by 54% and provide a savings of 190K euros. We also assess how to
deploy this solution, and whether it necessitates streaming computation for
real time scoring. We found that our solution can maintain similar benefits
even when historical features are computed once every 7 days","Solving the ""false positives"" problem in fraud prediction",http://arxiv.org/abs/1710.07709,,,,core
84327512,2017-07-03T00:00:00,"Artificial Intelligence methods to solve continuous- control tasks have made
significant progress in recent years. However, these algorithms have important
limitations and still need significant improvement to be used in industry and
real- world applications. This means that this area is still in an active
research phase. To involve a large number of research groups, standard
benchmarks are needed to evaluate and compare proposed algorithms. In this
paper, we propose a physical environment benchmark framework to facilitate
collaborative research in this area by enabling different research groups to
integrate their designed benchmarks in a unified cloud-based repository and
also share their actual implemented benchmarks via the cloud. We demonstrate
the proposed framework using an actual implementation of the classical
mountain-car example and present the results obtained using a Reinforcement
Learning algorithm.Comment: Accepted in 3rd IEEE International Forum on Research and Technologies
  for Society and Industry 201",OPEB: Open Physical Environment Benchmark for Artificial Intelligence,http://arxiv.org/abs/1707.00790,,10.1109/rtsi.2017.8065980,,core
198037982,2017-01-01T00:00:00,"The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical. Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners\u27 expectations. Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best",Predicting and evaluating software model growth in the automotive industry,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICSME.2017.41,,core
141495674,2017-01-01T00:00:00,"In the last years, the telecommunications industry has seen an increasing interest in the development of advanced solutions that enable communicating nodes to exchange large amounts of data. Indeed, well-known applications such as VoIP, audio streaming, video on demand, real-time surveillance systems, safety vehicular requirements, and remote computing have increased the demand for the efficient generation, utilization, management and communication of larger and larger data quantities. New transmission technologies have been developed to permit more efficient and faster data exchanges, including multiple input multiple output architectures or software defined networking: as an example, the next generation of mobile communication, known as 5G, is expected to provide data rates of tens of megabits per second for tens of thousands of users and only 1 ms latency. In order to achieve such demanding performance, these systems need to effectively model the considerable level of uncertainty related to fading transmission channels, interference, or the presence of noise in the data.

In this thesis, we will present how different approaches can be adopted to model these kinds of scenarios, focusing on wireless networking applications. In particular, the first part of this work will show how stochastic optimization models can be exploited to design energy management policies for wireless sensor networks. Traditionally, transmission policies are designed to reduce the total amount of energy drawn from the batteries of the devices; here, we consider energy harvesting wireless sensor networks, in which each device is able to scavenge energy from the environment and charge its battery with it. In this case, the goal of the optimal transmission policies is to efficiently manage the energy harvested from the environment, avoiding both energy outage (i.e., no residual energy in a battery) and energy overflow (i.e., the impossibility to store scavenged energy when the battery is already full).

In the second part of this work, we will explore the adoption of machine learning techniques to tackle a number of common wireless networking problems. These algorithms are able to learn from and make predictions on data, avoiding the need to follow limited static program instructions: models are built from sample inputs, thus allowing for data-driven predictions and decisions. In particular, we will first design an on-the-fly prediction algorithm for the expected time of arrival related to WiFi transmissions. This predictor only exploits those network parameters available at each receiving node and does not require additional knowledge from the transmitter, hence it can be deployed without modifying existing standard transmission protocols. Secondly, we will investigate the usage of particular neural network instances known as autoencoders for the compression of biosignals, such as electrocardiography and photo plethysmographic sequences. A lightweight lossy compressor will be designed, able to be deployed in wearable battery-equipped devices with limited computational power. Thirdly, we will propose a predictor for the long-term channel gain in a wireless network. Differently from other works in the literature, such predictor will only exploit past channel samples, without resorting to additional information such as GPS data. An accurate estimation of this gain would enable to, e.g., efficiently allocate resources and foretell future handover procedures. Finally, although not strictly related to wireless networking scenarios, we will show how deep learning techniques can be applied to the field of autonomous driving. This final section will deal with state-of-the-art machine learning solutions, proving how these techniques are able to considerably overcome the performance given by traditional approaches",Stochastic Optimization and Machine Learning Modeling for Wireless Networking,https://core.ac.uk/download/141495674.pdf,,,,core
293725865,2017-01-01T00:00:00,"Robotics has the potential to be one of the most revolutionary technologies in human history. The impact of cheap and
potentially limitless manpower could have a profound influence on our everyday life and overall onto our society. As
envisioned by Iain M. Banks, Asimov and many other science fictions writers, the effects of robotics on our society might
lead to the disappearance of physical labor and a generalized increase of the quality of life. However, the large-scale
deployment of robots in our society is still far from reality, except perhaps in a few niche markets such as manufacturing.
One reason for this limited deployment of robots is that, despite the tremendous advances in the capabilities of the
robotic hardware, a similar advance on the control software is still lacking. The use of robots in our everyday life is still
hindered by the necessary complexity to manually design and tune the controllers used to execute tasks. As a result,
the deployment of robots often requires lengthy and extensive validations based on human expert knowledge, which
limit their adaptation capabilities and their widespread diffusion. In the future, in order to truly achieve an ubiquitous
robotization of our society, it is necessary to reduce the complexity of deploying new robots in new environments and
tasks.
The goal of this dissertation is to provide automatic tools based on Machine Learning techniques to simplify and
streamline the design of controllers for new tasks. In particular, we here argue that Bayesian modeling is an important tool
for automatically learning models from raw data and properly capture the uncertainty of the such models. Automatically
learning models however requires the definition of appropriate features used as input for the model. Hence, we present
an approach that extend traditional Gaussian process models by jointly learning an appropriate feature representation
and the subsequent model. By doing so, we can strongly guide the features representation to be useful for the subsequent
prediction task.
A first robotics application where the use of Bayesian modeling is beneficial is the accurate learning of complex dynamics models. For highly non-linear robotic systems, such as in presence of contacts, the use of analytical system
identification techniques can be challenging and time-consuming, or even intractable. We introduce a new approach for
learning inverse dynamics models exploiting artificial tactile sensors. This approach allows to recognize and compensate
for the presence of unknown contacts, without requiring a spatial calibration of the tactile sensors. We demonstrate
on the humanoid robot iCub that our approach outperforms state-of-the-art analytical models, and when employed in
control tasks significantly improves the tracking accuracy.
A second robotics application of Bayesian modeling is automatic black-box optimization of the parameters of a controller. When the dynamics of a system cannot be modeled (either out of complexity or due to the lack of a full state
representation), it is still possible to solve a task by adapting an existing controller. The approach used in this thesis is
Bayesian optimization, which allows to automatically optimize the parameters of the controller for a specific task. We
evaluate and compare the performance of Bayesian optimization on a gait optimization task on the dynamic bipedal
walker Fox. Our experiments highlight the benefit of this approach by reducing the parameters tuning time from weeks
to a single day.
In many robotic application, it is however not possible to always define a single straightforward desired objective.
More often, multiple conflicting objectives are desirable at the same time, and thus the designer needs to take a decision
about the desired trade-off between such objectives (e.g., velocity vs. energy consumption). One framework that is
useful to assist in this decision making is the multi-objective optimization framework, and in particular the definition of
Pareto optimality. We propose a novel framework that leverages the use of Bayesian modeling to improve the quality
of traditional multi-objective optimization approaches, even in low-data regimes. By removing the misleading effects
of stochastic noise, the designer is presented with an accurate and continuous Pareto front from which to choose the
desired trade-off. Additionally, our framework allows the seamless introduction of multiple robustness metrics which can
be considered during the design phase. These contributions allow an unprecedented support to the design process of
complex robotic systems in presence of multiple objective, and in particular with regards to robustness.
The overall work in this thesis successfully demonstrates on real robots that the complexity of deploying robots to solve
new tasks can be greatly reduced trough automatic learning techniques. We believe this is a first step towards a future
where robots can be used outside of closely supervised environments, and where a newly deployed robot could quickly
and automatically adapt to accomplish the desired tasks",Bayesian Modeling for Optimization and Control in Robotics,,,,,core
267814378,2017-01-01T00:00:00,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduces operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for datacenters with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Reinforcement learning, based in this work on Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learningbased portfolio scheduler can perform better (5–20%) and cost less (20–35%) than the considered alternatives.Distributed System",ANANKE: a Q-Learning-Based Portfolio Scheduler for Complex Industrial Workflows: Technical Report DS-2017-001,,Delft University of Technology,,,core
211503605,2017-01-01T00:00:00,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases",IEEE Access special section editorial: health informatics for the developing world,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
203923069,2016-12-31,"AbstractTransferring predictive microbial models from research into real world food manufacturing or risk assessment applications is still a challenge for members of the food safety modelling community. Such knowledge transfer could be facilitated if publicly available food safety model repositories would exist.This research therefore aimed at identification of missing resources hampering the establishment of community driven food safety model repositories. Existing solutions in related scientific disciplines like Systems Biology and Data Mining were analyzed.On the basis of this analysis, some factors which would promote the establishment of community driven model repositories were identified – among them: a standardized information exchange format for models and rules for model annotation. As a consequence a proposal for a Predictive Modelling in Food Markup Language (PMF-ML) together with a prototypic implementation on the basis of the Systems Biology Markup Language (SBML) has been developed. In addition the adoption of MIRIAM guidelines for model annotation is proposed. In order to demonstrate the practicability of the proposed strategy, existing predictive models previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models are made publicly available in the first community Food Safety Model Repository called openFSMR (https://sites.google.com/site/openfsmr/).This work illustrates that a standardized information exchange format for predictive microbial models can be established by adoption of resources from Systems Biology. Harmonized description and annotation of predictive models will also contribute to increased transparency and quality of food safety models",Towards Community Driven Food Safety Model Repositories ,,The Author(s). Published by Elsevier Ltd.,10.1016/j.profoo.2016.02.098,,core
428380032,2017-05-01T07:00:00,"Report from a meeting held on the topic of disinformation, the Internet, and public diplomacy held at the Hoover Institution, Stanford University, in 2017.
Executive Summary
Scientific progress continues to accelerate, and while we’ve witnessed a revolution in communication technologies in the past ten years, what proceeds in the next ten years may be far more transformative. It may also be extremely disruptive, challenging long held conventions behind public diplomacy (PD) programs and strategies. In order to think carefully about PD in this ever and rapidly changing communications space, the Advisory Commission on Public Diplomacy (ACPD) convened a group of private sector, government, and academic experts at Stanford University’s Hoover Institution to discuss the latest trends in research on strategic communication in digital spaces. The results of that workshop, refined by a number of follow-on interviews and discussions, are included in this report. I encourage you to read each of the fourteen essays that follow, which are divided into three thematic sections: Digital’s Dark Side, Disinformation, and Narratives.
Digital’s Dark Side focuses on the emergence of social bots, artificial intelligence, and computational propaganda. Essays in this section aim to raise awareness regarding how technology is transforming the nature of digital communication, offer ideas for competing in this space, and raise a number of important policy and research questions needing immediate attention. The Disinformation section confronts Oxford English Dictionary’s 2016 word of the year – “post-truth” – with a series of compelling essays from practitioners, a social scientist, and philosopher on the essential roles that truth and facts play in a democratic society. Here, theory, research, and practice neatly align, suggesting it is both crucial and effective to double-down on fact-checking and evidence-based news and information programming in order to combat disinformation campaigns from our adversaries. The Narrative section concludes the report by focusing on how technology and facts are ultimately part of, and dependent on, strategic narratives. Better understanding how these narratives form, and what predicts their likely success, is necessary to think through precisely how PD can, indeed, survive the Internet. Below are some key takeaways from the report.
In Defense of Truth
• We are not living in a “post-truth” society. Every generation tends to think that the current generation is less honest than the previous generation. This is an old human concern, and should be seen today as a strategic narrative (see Hancock, p. 49; Roselle, p. 77). Defending the value and search for truth is crucial. As Jason Stanley notes (p. 71), “without truth, there is just power.”
• Humans are remarkably bad at detecting deception. Studies show that people tend to trust what others say, an effect called the truth bias. This bias is actually quite rational—most of the messages that a person encounters in a day are honest, so being biased toward the truth is almost always the correct response (see Hancock, p.49).
• At the same time people are also continuously evaluating the validity of their understanding of the world. This process is called “epistemic vigilance,” a continuous process checking that the information that a person believes they know about the world is accurate. While we have a difficult time detecting deception from interpersonal cues, people can detect lies when they have the time, resources, and motivation. Lies are often discovered through contradicting information from a third source, or evidence that challenges a deceptive account (see Hancock, p. 49).
• Fact checking can be effective, even in hyper-partisan settings (see Porter, p. 55), and is crucial for sustained democratic dialogue (Bennett, p. 61; Stanley, p. 71). Moreover, it is possible, using digital tools, to detect and effectively combat disinformation campaigns in real time (Henick and Walsh, p. 65).
Computational Propaganda
• Computational propaganda refers to the coordinated use of social media platforms, autonomous agents and big data directed towards the manipulation of public opinion.
• Social media bots (or “web robots”) are the primary tools used in the dissemination of computational propaganda. In their most basic form, bots provide basic answers to simple questions, publish content on a schedule or disseminate stories in response to triggers (e.g. breaking news). Bots can have a disproportionate impact because it is easy to create a lot of them and they can post a high-volume content at a high frequency (see Woolley, p.13).
• Political bots aim to automate political engagement in an attempt to manipulate public opinions. They allow for massive amplification of political views and can empower a small group of people to set conversation agenda’s online. Political bots are used over social media to manufacture trends, game hashtags, megaphone particular content, spam opposition and attack journalists. The noise, spam and manipulation inherent in many bot deployment techniques threaten to disrupt civic conversations and organization worldwide (see Chessen, p.19).
• Advances in artificial intelligence (AI) – an evolving constellation of technologies enabling computers to simulate cognitive processes – will soon enable highly persuasive machine-generated communications. Imagine an automated system that uses the mass of online data to infer your personality, political preferences, religious affiliation, demographic data and interests. It knows which news websites and social media platforms you frequent and it controls multiple user accounts on those platforms. The system dynamically creates content specifically designed to plug into your particular psychological frame and achieve a particular outcome (see Chessen, p. 39).
• Digital tools have tremendous advantages over humans. Once an organization creates and configures a sophisticated AI bot, the marginal cost of running it on thousands or millions of user accounts is relatively low. They can operate 24/7/365 and respond to events almost immediately. AI bots can be programmed to react to certain events and create content at machine speed, shaping the narrative almost immediately. This is critical in an information environment where the first story to circulate may be the only one that people recall, even if it is untrue (see Chessen, p. 39).
• PD practitioners need to consider the question of how they can create and sustain meaningful conversations and engagements with audiences if the mediums typically relied upon are becoming less trusted, compromised and dominated by intelligent machines.
• Challenging computational propaganda should include efforts to ensure the robustness and integrity of the marketplace of information online. Defensively, this strategy would focus on producing patterns of information exchange among groups that would make them difficult to sway using techniques of computational propaganda. Offensively, the strategy would seek to distribute the costs of counter-messaging broadly, shaping the social ecosystem to enable alternative voices to effectively challenge campaigns of misinformation (see Hwang, p. 27). In the persuasive landscape formed by social media and computational propaganda, it may be at times more effective to build tools, rather than construct a specific message.
• Practitioners are not alone in their concern about the escalating use of social bots by adversarial state actors. The private sector is, too. Social media platforms see this trend as a potentially existential threat to their business models, especially if the rise of bots and computational propaganda weakens users’ trust in the integrity of the platforms themselves. Coordination with private sector is key, as their policies governing autonomous bots will adapt and, thus, shape what is and isn’t feasible online.
Moving Past Folk Theories
• Folk theories, or how people think a particular process works, are driving far too many digital strategies. One example of a folk theory is in the prevalence of echo chambers online, or the idea that people are increasingly digitally walled off from one another, engaging only with content that fits cognitive predispositions and preferences.
• Research suggests that the more users rely on digital platforms (e.g. Twitter and Facebook) for their news and information, the more exposure they have to a multitude of sources and stories. This remains true even among partisans (though to a lesser extent than non-partisans). It turns out we haven’t digitally walled ourselves off after all (see Henick and Walsh, p. 65).
• Despite increased exposure to a pluralistic media ecosystem, we are becoming more and more ideological and partisan, and becoming more walled off at the interpersonal and physical layers. For example, marriages today are twice as likely to be between two people with similar political views than they were in 1960.
• Understanding this gap between a robustly diverse news environment and an increasingly “siloed” physical environment is crucial to more effectively engaging with target audiences around the world. Interpersonal and in-person engagement, including exchange programs, remain crucial for effective PD moving forward (see Wharton, p. 7).
• Despite this growing ideological divide, people are increasingly willing to trust one another, even complete strangers, when their goals are aligned (see the sharing economy, for example). This creates interesting opportunities for PD practitioners. Targeting strategies based on political attitudes or profiles may overshadow the possibility of aligned goals on important policy and social issues (see Hancock, p. 49).
Rethinking Our Digital Platforms and Metrics
Virality – the crown jewel in the social media realm – is overemphasized often at the expense of more important metrics like context and longevity. Many of the metrics used to measure the effectiveness of social media campaigns are vulnerable to manipulation, and more importantly, don’t measure engagement in any meaningful way. These metrics were built for an industry reliant on advertising for revenue generation, and as a result, may not be well-suited when applied to the context of PD (see Ford, p. 33; Woolley, p. 13).
• Overemphasizing certain metrics, such as reach or impressions, fails to account for the risks created by relaying on the same portals as other, less truthful and more nefarious actors. We need to be cautious and aware of the various ways in which the digital media business industries are shaping PD content, be aware of the risks, and think carefully about safeguarding the credibility U.S. Department of State PD programs operating in this space (see Wharton, p. 7; Ford, p. 33).
Strategic Narratives
• Strategic narratives—a means for political actors to construct a shared meaning of the past, present and future of politics in order to shape the behavior of other actors.” They provide the ideological backdrop for how audiences assess the meaning and significance of current events and breaking news. Put another way, they help people make sense of what would otherwise be a dizzying onslaught of news they are exposed to on a daily basis (see Roselle, p. 77; Kounalakis, p. 91).
• Crafting effective narratives require a genuine consensus--even if limited or temporary--on our policy priorities and their underlying values, as well as a detailed understanding and appreciation of local grievances and concerns about the related policy issue (see Wharton, p. 7; Roselle. P. 77). As such, effective strategic narratives must be mutually constructed.
• Rather than focusing on trending news topics and stories alone, we need to develop greater capacity to understand competing public narratives in foreign contexts and track how they adapt over time. Understanding distinctions between system (or governance), value, and identity narratives would allow PD practitioners to construct policy narratives that speak to, or at least acknowledge, the underlying pillars of belief in a given community (see Walker, p. 83; Roselle, p. 77).
• Every new administration creates new opportunities for foreign engagement. A shift towards a more transactional approach to PD, focused less on values but more on shared policy priorities, could allow for improved relations and cooperation with a number of countries previously hostile to American PD efforts and programs (see Kounalakis, p. 91)","Can Public Diplomacy Survive the Internet? Bots, Echo Chambers, and Disinformation",https://core.ac.uk/download/428380032.pdf,DigitalCommons@University of Nebraska - Lincoln,,,core
226725125,2017-01-01T00:00:00,"Managing intermittent demand is a vital task in several industrial contexts, and good forecasting ability is a fundamental prerequisite for an efficient inventory control system in stochastic environments. In recent years, research has been conducted on single-hidden layer feedforward neural networks, with promising results. In particular, back-propagation has been adopted as a gradient descent-based algorithm for training networks. However, when managing a large number of items, it is not feasible to optimize networks at item level, due to the effort required for tuning the parameters during the training stage. A simpler and faster learning algorithm, called the extreme learning machine, has been therefore proposed in the literature to address this issue, but it has never been tried for forecasting intermittent demand. On the one hand, an extensive comparison of single-hidden layer networks trained by back-propagation is required to improve our understanding of them as predictors of intermittent demand. On the other hand, it is also worth testing extreme learning machines in this context, because of their lower computational complexity and good generalisation ability. In this paper, neural networks trained by back-propagation and extreme learning machines are compared with benchmark neural networks, as well as standard forecasting methods for intermittent demand on real-time series, by combining different input patterns and architectures. A statistical analysis is then conducted to validate the best performance through different aggregation levels. Finally, some insights for practitioners are presented to improve the potential of neural networks for implementation in real environments",Single-hidden layer neural networks for forecasting intermittent demand,,'Elsevier BV',10.1016/j.ijpe.2016.10.021,,core
148673901,2017,"Arguably the biggest strength of the functional programming language Erlang is how straightforward it is to implement concurrent and distributed programs with it. Numerical computing, on the other hand, is not necessarily seen as one of its strengths. The recent introduction of Federated Learning, a concept according to which edge devices are leveraged for decentralized machine learning tasks, while a central server only updates and distributes a global model, provided the motivation for exploring how well Erlang was suited to such a use case.

We present a framework for Federated Learning in Erlang, written in a purely functional style, and compare two versions of it: one that has been exclusively written in Erlang, and one in which Erlang is relegated to coordinating client processes that rely on performing numerical computations in the programming language C. Initial results are promising, as we learnt that a real-world industrial use case of distributed data analytics can easily be tackled with a system purely written in Erlang.

The novelty of our work is that we present the first implementation of a Federated Learning framework in a functional programming language, with the added benefit of being purely functional. In addition, we demonstrate that Erlang can not only be leveraged for message passing but also performs adequately for practical machine learning tasks",Purely Functional Federated Learning in Erlang,,,,,core
79587102,2016-01-01T00:00:00,"abstract: The dawn of Internet of Things (IoT) has opened the opportunity for mainstream adoption of machine learning analytics. However, most research in machine learning has focused on discovery of new algorithms or fine-tuning the performance of existing algorithms. Little exists on the process of taking an algorithm from the lab-environment into the real-world, culminating in sustained value. Real-world applications are typically characterized by dynamic non-stationary systems with requirements around feasibility, stability and maintainability. Not much has been done to establish standards around the unique analytics demands of real-world scenarios.

This research explores the problem of the why so few of the published algorithms enter production and furthermore, fewer end up generating sustained value. The dissertation proposes a ‘Design for Deployment’ (DFD) framework to successfully build machine learning analytics so they can be deployed to generate sustained value. The framework emphasizes and elaborates the often neglected but immensely important latter steps of an analytics process: ‘Evaluation’ and ‘Deployment’. A representative evaluation framework is proposed that incorporates the temporal-shifts and dynamism of real-world scenarios. Additionally, the recommended infrastructure allows analytics projects to pivot rapidly when a particular venture does not materialize. Deployment needs and apprehensions of the industry are identified and gaps addressed through a 4-step process for sustainable deployment. Lastly, the need for analytics as a functional area (like finance and IT) is identified to maximize the return on machine-learning deployment.

The framework and process is demonstrated in semiconductor manufacturing – it is highly complex process involving hundreds of optical, electrical, chemical, mechanical, thermal, electrochemical and software processes which makes it a highly dynamic non-stationary system. Due to the 24/7 uptime requirements in manufacturing, high-reliability and fail-safe are a must. Moreover, the ever growing volumes mean that the system must be highly scalable. Lastly, due to the high cost of change, sustained value proposition is a must for any proposed changes. Hence the context is ideal to explore the issues involved. The enterprise use-cases are used to demonstrate the robustness of the framework in addressing challenges encountered in the end-to-end process of productizing machine learning analytics in dynamic read-world scenarios.Dissertation/ThesisDoctoral Dissertation Computer Science 201",Crossing the Chasm: Deploying Machine Learning Analytics in Dynamic Real-World Scenarios,https://core.ac.uk/download/79587102.pdf,,,,core
155243174,2017-12-15T00:00:00,"Dissertação (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Mecânica, 2017.Este trabalho apresenta vários esquemas para identiﬁcação, observação e controle adaptativos em tempo real de sistemas não lineares incertos usando redes neurais artiﬁciais. Com base na teoria de estabilidade de Lyapunov, e usando resultados já disponíveis na teoria de controle adaptativo, são propostos esquemas para identiﬁcação, observação e controle nos quais os erros de identiﬁcação, observação e rastreamento estão relacionados com parâmetros de projeto que podem ser ajustados diretamente pelo usuário. Entretanto, ao contrário das propostas usuais na literatura, este trabalho propõe algoritmos nos quais o desempenho transiente e em regime podem ser desacoplados e ajustados independentemente através de parâmetros de projeto independentes. Inicialmente, o caso de identiﬁcação em tempo real é considerado, uma vez que cada vez mais a resolução de sistemas caixa preta tem sido demandados. O identiﬁcador proposto apresenta as seguintes peculiaridades: 1) Possibilidade de controlar o tamanho do erro residual de estado a partir de matrizes de projeto; 2) Possibilidade de ajustar a duração do regime transiente a partir de um parâmetro de projeto que é independente do tamanho do erro em regime. A identiﬁcação de um sistema caótico de três estados foi considerada para validar o esquema. A seguir, o resultado é estendido para o caso nos quais alguns estados não estão disponíveis para medida. Para tanto, é necessário apenas fazer alguns ajustes no esquema de identiﬁcação proposto, para permitir agora estimar um ou mais estados não disponíveis para medição, a partir das entradas e saídas ao sistema. O observador proposto apresenta as mesmas peculiaridades do identiﬁcador. A observação de um sistema de Rössler foi implementada de forma a exempliﬁcar este observador. Na sequência, considera-se o caso de controle com realimentação do estado. Para tanto, se propôs o projeto do controlador empregando como base o caso de identiﬁcação de malha aberta. As principais peculiaridades do identiﬁcador ocorrem também no controlador. Finalmente, de modo a ressaltar a aplicabilidade e relevância dos algoritmos propostos, a identiﬁcação e controle de um sistema de soldagem foram realizados. A dissertação está organizada da seguinte forma. O capítulo 1 apresenta a introdução, motivação, objetivo, possíveis contribuições e estrutura do trabalho proposto. No capítulo 2 é apresentada uma revisão do estado da arte dos métodos de identiﬁcação, observação e controle baseados em redes neurais artiﬁciais. No capítulo 3, usando a teoria de estabilidade de Lyapunov, propõe-se um esquema de identiﬁcação neural adaptativo em tempo real para uma classe de sistemas não lineares na presença de distúrbios limitados. É importante ressaltar que nenhum conhecimento prévio sobre a dinâmica do erro de aproximação, pesos ideais ou perturbações externas é necessário. Mostra-se que o algoritmo de aprendizado baseado na teoria de estabilidade de Lyapunov leva o estado estimado a convergir assintoticamente para o estado de sistemas não lineares. O algoritmo proposto permite: 1) reduzir o erro residual de estimação de estado para valores pequenos por meio de matrizes de projeto; 2) controlar o tempo de transiente de maneira arbitraria a partir de um parâmetro de projeto. Foram feitas simulações para um sistema caótico de 3 estados e para um sistema hipercaótico de 4 estados para demonstrar a eﬁcácia e a eﬁciência do algoritmo de aprendizado proposto. Nessas simulações foram feitas análises do tamanho do erro residual de estado e da escolha do tempo de transiente. Finaliza-se o capítulo com uma aplicação: a identiﬁcação neural de um sistema caótico de soldagem na qual se analisou o ajuste do tamanho do erro residual de estado. Posteriormente, no capítulo 4, os resultados obtidos no capítulo anterior são estendidos para um sistema de observação neural. O caso de observação ocorre quando nem todos os estados estão disponíveis e um ou mais estados precisam ser estimados. A metodologia de projeto do algoritmo de aprendizado é semelhante ao caso do capítulo 3, sendo necessário fazer algumas adaptações próprias para um esquema de observação. Mais especiﬁcamente, a ideia principal consiste em expressar o erro de estimação de estado, que não é mais disponível para medida, em função do erro de estimação da saída. Para tanto, faz-se necessária a imposição de uma hipótese de detectabilidade e de uma outra condição matricial que devem ser satisfeitas simultaneamente para que o esquema de observação apresente características de estabilidade e convergência semelhantes ao esquema de identiﬁcação proposto. Realiza-se no ﬁnal do capítulo a observação de um sistema caótico de Rössler sob a presença de distúrbios externos com controle do tempo de transiente. No capítulo 5, os resultados do capítulo 3 são estendidos para controlar sistemas não lineares aﬁns no controle. O caso de controle ocorre quando se realiza uma identiﬁcação em malha fechada, ou seja, há uma realimentação no sistema. Mais exatamente, através da realimentação objetiva-se cancelar as não lineares desconhecidas no sistema que podem ser parametrizadas por uma rede neural artiﬁcial. Dessa maneira, a equação de erro de restreamento pode ser reescrita com uma estrutura similar à equação de erro de estimação do caso de identiﬁcação. Com a ﬁnalidade de ressaltar a aplicabilidade do esquema de controle proposto, para situações de interesse industrial, realiza-se a simulação do controle de um sistema de soldagem com tranferência globular-spray em um processo GMAW (Soldagem por arco elétrico com gás de proteção). Finalmente, no capítulo 6 resume-se as contribuições da pesquisa, os resultados obtidos, e sugestões para pesquisas futuras são discutidas. A fundamentação teórica das redes neurais artiﬁciais (incluindo suas propriedades), dos algoritmos de aprendizado e da teoria de estabilidade de Lyapunov são descritas no apêndice 1, assim como outras informações importantes que embasam os capítulos do trabalho. O apêndice 2 contém os códigos utilizados para implementação do identiﬁcador, observador e controlador propostos nesta dissertação.This work presents several schemes for online identiﬁcation, observation, and adaptive control of uncertain nonlinear systems by using artiﬁcial neural networks in which the transient and residual errors can be independently adjusted. Based on Lyapunov theory, and using results already available in adaptive control theory, schemes for identiﬁcation, observation, and control are proposed. However, unlike other works in the literature, the identiﬁcation model, learning algorithm, and control laws are designed to decouple the transient and residual state performance, which is accomplished through the manipulation of independent design parameters. Initially, the case of online identiﬁcation is considered, since black-box systems can be parameterized by neural networks and these parameterizations are needed to tackle more complex problems, such as observation and adaptive control. The proposed identiﬁer has the following peculiarities: 1) Possibility to control the size of residual state error from design matrices; 2) Possibility to adjust the duration of the transient regime from a design parameter regardless of the size of the regime error. The identiﬁcation of a chaotic system of three states was considered to apply the scheme. The result is then extended to the case where some states are not available for measurement. To do so, it is only necessary to make some adjustments in the identiﬁcation scheme. Basically, the state error is placed as a function of the output error, which is available for measurement. The main characteristic of the proposed observer is the preservation of the properties of the proposed identiﬁer. The observation of a Rössler system was implemented in order to exemplify this observer. In the sequence, the case of control with state feedback is considered. For this, a controller was proposed using the open loop identiﬁcation case as analogue to it. The main peculiarities of the identiﬁer also occur in the controller. Finally, in order to emphasize the applicability and relevance of the proposed algorithms, the identiﬁcation and control of a welding system were performed",Identificação de sistemas não lineares em malha aberta e fechada usando redes neurais artificiais,,,,,core
83110680,2017-06-13T00:00:00,"Informatika in informacijske tehnologije so že več desetletij gonilo inoviranja na vseh področjih poslovanja podjetij ter delovanja posameznikov. Odprti standardi in interoperabilnost ter vedno višja odzivnost informatikov vodijo k razvoju inteligentnih digitalnih storitvenih platform in inovativnih poslovnih modelov ter novih ekosistemov, kjer se povezujejo in sodelujejo ne le partnerji, temveč tudi konkurenti. Vse večja in pomembnejša je tudi vključenost končnih uporabnikov naših storitev in rešitev. Napredne informacijske tehnologije in sodobni pristopi k razvoju, vpeljavi in upravljanju omogočajo višjo stopnjo avtomatizacije in integracije doslej ločenih svetov, saj vzpostavljajo zaključeno zanko in zagotavljajo nenehne izboljšave, ki temeljijo na aktivnem sodelovanju in povratnih informacijah vseh vključenih akterjev.
 
Na strokovni konferenci OTS tudi letos naslavljamo aktualne izzive, tehnologije in pristope kot so BlockChain, BizDevOps, zabojniki, agilni procesi in neprekinjeno zagotavljanje kakovosti, brezstični uporabniški vmesniki, internet stvari, velepodatki in globoko učenje. V prispevkih so predstavljene izkušnje strokovnjakov, ki načela in principe digitalne preobrazbe že udejanjajo in pri tem ustrezno skrb posvečajo zagotavljanju informacijske varnosti in zasebnosti. In kar je še posebej pomembno – svoje znanje, storitve in rešitve uspešno tržijo na globalnem trgu vse od Avstralije, Filipinov, Kitajske, Rusije, Irana, držav Evropske unije in Švice do ZDA.For many decades, the field of Informatics and Information technologies has been the driving force and the enabler for innovations in business and industry, as well in society, improving the quality of life for everyone. Advanced technologies and tools, open standards, interoperability and the increased responsiveness of IS/IT experts are leading the way to the development of intelligent digital service platforms and innovative business models. In these new ecosystems, characterized by active involvement and the engagement of end-users, even competitors are becoming partners and allies.  

As in previous years, the papers from this year\u27s proceedings address actual challenges and technologies related to the development of advanced software and information solutions, namely BlockChain, BizDevOps, dockers, agile and continuous development, delivery and quality assurance, contactless HCI, Internet of Things, Big Data and Deep learning. We are delighted and thankful to all the authors for their willingness to share their experiences and best practices identified with real-life projects. There is no doubt that the authors have managed to demonstrate not only a successful application of the principles of digital transformation but also the ability to compete on the global market - from Australia, the Philippines, China, Russia, Iran, the EU and Switzerland to the USA",OTS 2017 Modern technologies and services,https://core.ac.uk/download/83110680.pdf,'University of Maribor',10.18690/978-961-286-040-0,,core
104448520,2016-08-17,"Abstract Automating the design of heuristic search methods is an active research field within computer science, artificial intelligence and operational research. In order to make these methods more generally applicable, it is important to eliminate or reduce the role of the human expert in the process of designing an effective methodology to solve a given computational search problem. Researchers developing such methodolo-gies are often constrained on the number of problem domains on which to test their adaptive, self-configuring algorithms; which can be explained by the inherent difficulty of implementing their corresponding domain specific software components. This paper presents HyFlex, a software framework for the development of cross-domain search methodologies. The framework features a common software interface for dealing with different combinatorial optimisation problems, and provides the al-gorithm components that are problem specific. In this way, the algorithm designer does not require a detailed knowledge the problem domains, and thus can concen-trate his/her efforts in designing adaptive general-purpose heuristic search algorithms. Four hard combinatorial problems are fully implemented (maximum satisfiability, one dimensional bin packing, permutation flow shop and personnel scheduling), each con-taining a varied set of instance data (including real-world industrial applications) and an extensive set of problem specific heuristics and search operators. The framework forms the basis for the first International Cross-domain Heuristic Search Challenge (CHeSC), and it is currently in use by the international research community. In sum-mary, HyFlex represents a valuable new benchmark of heuristic search generality, with which adaptive cross-domain algorithms are being easily developed, and reliably com-pared. Keywords hyper-heuristics · combinatorial optimisation · search methodologies, self-adaptation, adaptatio",Noname manuscript No. (will be inserted by the editor) HyFlex: A Benchmark Framework for Cross-domain Heuristic Search,,,,,core
74410704,2016-12-29T00:00:00,"Increased market demand for composite products and shortage of expert laminators is compelling the composite industry to explore ways to acquire layup skills from experts and transfer them to novices and eventually to machines. There is a lack of holistic methods in literature for capturing composite layup skills especially involving complex moulds. This research aims to develop an informatics-based method, enabled by consumer-grade gaming technology and machine learning, to capture and digitise manufacturing task knowledge from skill-intensive hand layup. The digitisation is underpinned by the proposed human-workpiece interaction theory and implemented to automatically extract and decode key knowledge constituents such as layup strategies, ply manipulation techniques, motion mechanics and problem-solving during hand layup, collectively categorised as layup skills. The significance of this research is its potential to facilitate cost-effective transfer of skills from experts to novices, real-time automated supervision of hand layup and automation of layup tasks in the future",Digitisation of manual composite layup task knowledge using gaming technology,https://core.ac.uk/download/74410704.pdf,'Elsevier BV',10.1016/j.compositesb.2016.12.050,"[{'title': 'Composites Part B Engineering', 'identifiers': ['1359-8368', 'issn:1359-8368']}]",core
45450607,2016-06-16T10:10:00,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.  While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.  The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg-Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg-Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg-Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.  Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H2O, O3, HNO3, CH4, N2O, NO2, CFC-11, CFC-12, N2O5 and ClONO2. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements. © Author(s) 2013.This work has been performed under the ESA study >Support to MIPAS Level 2 product validation>, contract ESA-ESRIN no. 21719/08/I-OL. The authors are grateful to the Astrium team that developed the industrial prototype ML2PP using the ORM code as reference, to Thorsten Fehr and Rolf von Kulhmann for the coordination of the MIPAS Quality Working Group activities and to Michael Kiefer for the work done within the MIPAS Quality Working Group.Peer Reviewe",Ten years of MIPAS measurements with ESA Level 2 processor V6 – Part 1: Retrieval algorithm and diagnostics of the products,https://core.ac.uk/download/45450607.pdf,'Copernicus GmbH',10.5194/amt-6-2419-2013,"[{'title': None, 'identifiers': ['issn: 1867-1381', ' 1867-1381']}]",core
212844020,2016-01-01T08:00:00,"Maintenance data can be used to make inferences about the reliability of system components. In industrial reliability applications, it is common that a fleet contains multiple systems. Within each system, there are multiple copies of a component installed in multiple locations (sockets). Examples are two automobile headlights, eight DIMM modules in a computing server, sixteen cylinders in a locomotive engine, etc. For each component replacement event, there is system-level information that a component was replaced. The socket-level information (which particular component was replaced), however, is usually unknown. The aggregated data for a system form a collection of superpositions of renewal processes (SRP). The reliability of the system component is of particular interest for future system design and maintenance planning. In this dissertation, statistical models and methods were developed for estimating the lifetime distribution of system components for the SRP data, which is motivated by some real applications.
In Chapter 2, we propose a parametric likelihood-based procedure for estimating the lifetime distribution of a component from the aggregated recurrence data for a fleet with multiple SRPs. We show how to formulate the likelihood function of the SRP data and compute the maximum likelihood (ML) estimator. The performance of the ML estimator is investigated by simulation studies, as well as real applications on two different data sets. Chapter 3 provides more extensive results of simulation studies on the performance of the ML estimator and two confidence interval methods for estimating quantiles of the component lifetime distribution.
Chapter 4 presents two graphical distributional assessment methods to evaluate how well the parametric models proposed in Chapter 2 fit the observed SRP data. The first method provides a flexible semi-parametric estimate of the lifetime distribution of the component, which is based on a piecewise exponential (PEX) model. The nonparametric simultaneous confidence bands (NPSCBs) based on a bootstrap procedure, are given to assess the amount of statistical uncertainty of the semi-parametric estimate. The second method is based on the use of the mean cumulative function (MCF) plot for the SRP data. The amount of sampling error can be evaluated by using NPSCBs, which show the departures from the fitted model.
Chapter 5 describes how the maximum likelihood estimation procedure (proposed in Chapter 2) for the SRP data is implemented in the SRPML R package. Given the required input, which includes the observed SRP data, the specified log-location-scale distribution and certain parameter specifications, the functions in the SRPML package return the parameter ML estimates, the maximized log likelihood, and confidence intervals based on Wald and/or likelihood-based procedures. A real application of the R package is illustrated, and the required computation time for this estimation procedure are summarized",Inference based on data from superpositions of identical renewal processes,https://core.ac.uk/download/212844020.pdf,Iowa State University Digital Repository,,,core
42710846,2016-04-21T00:00:00,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.Comment: Review of state of the art articulated hand pose estimation since
  2007, this paper was written in May 201",Articulated Hand Pose Estimation Review,http://arxiv.org/abs/1604.06195,,,,core
196643655,2017-01-01T00:00:00,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0Integrated Process Control based on Distributed In-Situ Sensors into Raw Material and Energy Feedstock, DISIR",Cloud computing for big data analytics in the Process Control Industry,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/MED.2017.7984310,,core
77073532,2017,"With the advent of 21st Century, we stepped into the fourth industrial revolution of cyber physical systems. There is the need of secured network systems and intrusion detection systems in order to detect network attacks. Use of machine learning for anomaly detection in industrial networks faces challenges which restricts its large-scale commercial deployment. ADIN Suite proposes a roadmap to overcome these challenges with multi-module solution. It solves the need for real world network traffic, an adaptive hybrid analysis to reduce error rates in diverse network traffic and alarm correlation for semantic description of detection results to the network operator",Anomaly detection in industrial networks using machine learning: A roadmap,,,10.1007/978-3-662-53806-7_8,,core
59422015,2016-04-07,"INTRODUCTION Cancer Research UK Formulation Unit The Formulation Unit based at the University of Strathclyde in Glasgow has a research and development history in excess of 25 years, being funded by, and working in partnership with, firstly Cancer Research Campaign, and since 2002, with Cancer Research UK. The Unit is based in an entirely academic University setting, and since 2004 has been licensed by the UK government Medicines and Healthcare products Regulatory Agency (MHRA) for research, development and manufacture of Phase I/II novel small molecule cancer therapeutics and diagnostics. Research programs have delivered new formulations to clinical trial as either sterile or non-sterile presentations. However, the Unit’s specialty is based around small volume parenteral product manufacture. Boronophenylalanine (L-BPA) in Boron Neutron Capture Therapy (BNCT) L-BPA is the premier pharmaceutical selection in BNCT in treatment of selected head and neck tumours. BNCT relies on localisation of boron 10 within a tumour mass, made possible by the amino acid carrier portion of the L-BPA molecule. Phenylalanine is selectively transported across the blood brain barrier and then into astrocytic cells by a LAT-1 transporter system that is up-regulated in tumour. A targeted external neutron beam activates the accumulated L-BPA. In brief, neutron capture by boron causes nuclear re-arrangement and formation of a high linear energy transfer alpha particle and lithium 7 nuclei. Thus the patient is dosed with localised radiotherapy. OLD FORMULATION Issues existed with the previous standard formulation of L-BPA in fructose. L-BPA complexed with fructose has low solubility of around 30mg/mL. Consequently, large administration volumes are required to achieve clinical dosing in tens of grams of drug per patient. Moreover, L-BPA in fructose solutions must be freshly prepared and administered within 48 hours for reasons of product instability (Henriksson et al, 2008). Although rare, hereditary fructose intolerance needs to be considered. Taken together, L-BPA production, preparation and patient dosing is highly challenging. NEW FORMULATION Restrictions The Formulation Unit developed a new improved formulation; the drug product was a lyophilized pH8 solution of L-BPA at 100mg/mL in 110mg/mL mannitol (Schmidt et al, 2011). When lyophilised, a shelf life of 48 months was supported for the drug product. Whilst a three times increase in solubility, and a significantly enhanced product lifetime were worthy formulation enhancements, a new restriction emerged; the solution for lyophilisation contained 21% w/v solids far exceeding the ‘normal’ region of 2% w/v to 5% w/v (Boylan and Nail, 2009). Moreover, the lyophilisation cycle of 6 days was considered commercially unfavourable. A shortened drying cycle of 1 to 3 days would be preferred. Research was therefore initiated to reduce drying cycle time utilising Manometric Temperature Measurement (MTM) technology. MTM Studies MTM controlled freeze drying systems were originally marketed in the first decade of the new millennium. The ability to use software to calculate the performance at the freeze-drying front in real time is scientifically and commercially appealing. The possibility to optimize processing conditions at that same time as data is being received invites the prospect of a reduced experimentation phase thereby rapidly reaching the goal of a maximally efficient freeze drying cycle. In theory, even a minimally experienced operator could achieve this outcome. In summary, MTM functions by taking pressure rise information at regular intervals (Giesler et al, 2007). Based on SMART® software (SP Scientific, Stone Ridge, NY, USA), hourly pressure rise data are taken at a rate of 10 samples per second. The system calculates the product temperature at the sublimation interface and mass transfer resistance of the product. Adjustments are then automatically made to the shelf temperature and system pressure to achieve a calculated target product temperature. The end of primary drying can be determined by comparing the vapour pressure of ice with the system chamber pressure. Input data is minimal, such as vial number, inner vial area, fill volume and weight, concentration, product critical temperature. MATERIALS AND METHODS Chemicals Syntagon AB, Södertälje, Sweden manufactured BPA raw material according to EU current Good Manufacturing Practice (cGMP). D-mannitol (Ph. Eur) was sourced from Sigma-Aldrich, Poole, UK, and fuming hydrochloric acid and sodium hydroxide pellets (both extra pure Ph. Eur., BP, JP, NF) were obtained from VWR International, Lutterworth, UK. Water for Irrigation (WFI) in bulk was acquired from Baxter’s Healthcare Ltd., Norfolk, UK. Type 1 clear glass 50mL vials with 20mm butyl rubber stoppers (proved clean), crimped with 20mm tear off aluminium overseals were all from Adelphi Healthcare Packaging, Haywards Heath, UK. Lyophilisation equipment MTM software (SMART®) was operated on an FTS Systems Lyostar II drier (Biopharma, Winchester, UK). CONCLUSION A new improved L-BPA formulation in mannitol has been developed and used in human clinical trial. Further research using MTM technology succeeded in reducing a 6 day drug product drying cycle to 53 hours. The formulation exhibited non-ideal behaviour, and MTM failed to predict drying parameters, e.g., base of vial temperature, that are more closely replicated in ‘ideal’ test articles such as a 5% mannitol comparator. Further test lyophilisations are required to reach ideal. ACKNOWLEDGMENTS This research is funded by Cancer Research UK. REFERENCES 1. Boylan, J.C. and Nail, S.L. Parenteral Products, in: Florence, A.T. and Siepman, J. (Eds.), Modern Pharmaceutics. Informa Healthcare, New York, 565-609 (2009). 2. Giesler, H.; Kramer, T. and Pikal, M. J. Use of manometric temperature measurement (MTM) and SMART freeze dryer technology for development of an optimised freeze drying cycle. J. Pharm Sci. 96(12), 3402-3418 (2007). 3. Henriksson, R.; Capala, J.; Michanek, A.; Lindahl, S.A.; Satford, L.G.; Franzen, L.; Blomquist, E.; Westlin, J.E. and Bergenheim, A.T. Boron neutron capture therapy (BNCT) for glioblastoma multiforme: A phase II study evaluating a prolonged high-dose of boronophenylalanine (BPA). Radiotherapy and Oncology 88, 183-191 (2008). 4. Schmidt, E.; Dooley, N.; Ford, S. J.; Elliott, M. and Halbert, G. W. Physicochemical investigation of the influence of saccharide based parenteral formulation excipients on L-p-boronphenylalanine solubilisation for Boron Neutron Capture Therapy. J. Pharm. Sci. 101(1), 223-232 (2011)",Manometric Temperature Measurement (MTM) lyophilisation of a challenging clinical trial pharmaceutical,,,,,core
44523753,2016-01-01T00:00:00,"This monograph opens up new horizons for engineers and researchers in academia and in industry dealing with or interested in new developments in the field of system identification and control. It emphasizes guidelines for working solutions and practical advice for their implementation rather than the theoretical background of Gaussian process (GP) models. The book demonstrates the potential of this recent development in probabilistic machine-learning methods and gives the reader an intuitive understanding of the topic. The current state of the art is treated along with possible future directions for research. Systems control design relies on mathematical models and these may be developed from measurement data. This process of system identification, when based on GP models, can play an integral part of control design in data-based control and its description as such is an essential aspect of the text. The background of GP regression is introduced first with system identification and incorporation of prior knowledge then leading into full-blown control. The book is illustrated by extensive use of examples, line drawings, and graphical presentation of computer-simulation results and plant measurements. The research results presented are applied in real-life case studies drawn from successful applications including: a gas–liquid separator control; urban-traffic signal modelling and reconstruction; and prediction of atmospheric ozone concentration. A MATLAB® toolbox, for identification and simulation of dynamic GP models is provided for download. Advances in Industrial Control aims to report and encourage the transfer of technology in control engineering. The rapid development of control technology has an impact on all areas of the control discipline. The series offers an opportunity for researchers to present an extended exposition of new work in all aspects of industrial control",Modelling and control of dynamic systems using gaussian process models,,'Springer Science and Business Media LLC',10.1007/978-3-319-21021-6,,core
140523295,2017-10-09,"2017-10-09Conservation agencies worldwide must make the most efficient use of their limited resources to protect natural resources from over-harvesting and animals from poaching. Predictive modeling, a tool to increase efficiency, is seeing increased usage in conservation domains such as to protect wildlife from poaching. Many works in this wildlife protection domain, however, fail to train their models on real-world data or test their models in the real world. My thesis proposes novel poacher behavior models that are trained on real-world data and are tested via first-of-their-kind tests in the real world. ❧ First, I proposed a paradigm shift in traditional adversary behavior modeling techniques from Quantal Response-based models to decision tree-based models. Based on this shift, I proposed an ensemble of spatially-aware decision trees, INTERCEPT, that outperformed the prior state-of-the-art and then also presented results from a one-month pilot field test of the ensemble's predictions in Uganda's Queen Elizabeth Protected Area (QEPA). This field test represented the first time that a machine learning-based poacher behavior modeling application was tested in the field. ❧ Second, I proposed a hybrid spatio-temporal model that led to further performance improvements. To validate this model, I designed and conducted a large-scale, eight-month field test of this model's predictions in QEPA. This field test, where rangers patrolled over 450 km in the largest and longest field test of a machine learning-based poacher behavior model to date in this domain, successfully demonstrated the selectiveness of the model's predictions; the model successfully predicted, with statistical significance, where rangers would find more snaring activity and also where rangers would not find as much snaring activity. I also conducted detailed analysis of the behavior of my predictive model. ❧ Third, beyond wildlife poaching, I also provided novel graph-aware models for modeling human adversary behavior in wildlife or other contraband smuggling networks and tested them against human subjects. Lastly, I examined human considerations of deployment in new domains and the importance of easily-interpretable models and results. While such interpretability has been a recurring theme in all my thesis work, I also created a game-theoretic inspection strategy application that generated randomized factory inspection schedules and also contained visualization and explanation components for users",Real-world evaluation and deployment of wildlife crime prediction models,,University of Southern California. Libraries,,,core
129351900,2017-10-23T00:00:00,"In the Canadian's lumber industry, simulators are used to predict the lumbers
resulting from the sawing of a log at a given sawmill. Giving a log or several
logs' 3D scans as input, simulators perform a real-time job to predict the
lumbers. These simulators, however, tend to be slow at processing large volume
of wood. We thus explore an alternative approximation techniques based on the
Iterative Closest Point (ICP) algorithm to identify the already processed log
to which an unseen log resembles the most. The main benefit of the ICP approach
is that it can easily handle 3D scans with a variable number of points. We
compare this ICP-based nearest neighbor predictor, to predictors built using
machine learning algorithms such as the K-nearest-neighbor (kNN) and Random
Forest (RF). The implemented ICP-based predictor enabled us to identify key
points in using the 3D scans directly for distance calculation. The long-term
goal of this ongoing research is to integrated ICP distance calculations and
machine learning.Comment: 7th Workshop on Service Orientation in Holonic and Multi Agent
  Manufacturing SOHOMA'17, Oct 2017, Nantes, Franc","An iterative closest point method for measuring the level of similarity
  of 3d log scans in wood industry",http://arxiv.org/abs/1710.08135,,,,core
148032139,2017-08-09T08:35:41,"The performance monitoring of computer systems is a complex affair, made even more challenging by the increasing gap between hardware and software. Methods that collect and feed data to performance analysis can usually be classified into one of two groups. Methods in the first group (e.g., software instrumentation), can be accurate but have unacceptable overheads and offer no insight into the interaction of the software and the hardware. Methods in the second group (e.g., hardware supported) run in real time and connect code to the response in the architecture, at a considerable decrease in accuracy, with a high degree of specificity to the hardware and therefore with limited actionable information made available. In this work, we focus on the latter group - the collection and analysis of raw performance data sourced from hardware Performance Monitoring Units, built into the majority of processors. The periodic collection of samples (Event Based Sampling), containing code locations, for instance, can be triggered on events such as retired instructions or cache misses. We quantify accuracy improvements to existing methods, propose a better performing method of performance data collection and a new architecture-agnostic method of performance data analysis. In our experiments, we analyze compiled, large-scale production workloads, industry standard benchmarks, and micro-benchmarks reproducing specific architectural behaviors. First, we focus on performance data collection methods and their accuracy, when establishing instruction retirement rates. These vary from least to most advanced and we examine the low level factors influencing accuracy. In particular, we employ a method based on Last Branch Records, a hardware facility present on some Intel architectures, which allows for the sampling of short records of last taken branches leading up to the sample. We compare the most commonly used method, its improved derivatives and the Last Branch Record based method. With respect to the first method, we observe accuracy improvements of up to 18x on synthetic benchmarks and 15x on real applications. Second, to further improve accuracy, we propose a new collection method named Hybrid Basic Block Profiling, fusing those based on Last Branch Records and the widely used Event Based Sampling. We apply simple machine learning techniques to determine the operating criteria. As a demonstration of capability, our method and a profiling tool are used to generate Instruction Mixes, which traditionally require good quality data at the most detailed level of granularity - basic blocks. Compared to software instrumentation, we observe an improvement in runtime of up to 76x, while keeping instruction attribution errors at 2.1% on average. In our third contribution we describe an architecture-agnostic performance analysis methodology called Hierarchical Cycle Accounting. The user is presented with a navigable hierarchy of microarchitectural issues, expressed in a metric that is universal and simple to compare - core cycles. We develop a reference implementation for the Intel Ivy Bridge microarchitecture and provide pointers for implementations on other processor architectures. Our approach and tool are successfully used by non-experts to improve and vectorize complex scientific code. Experts benefit as well - by being able to pinpoint software issues undetected by other methods, and identifying a bug in a family of Intel processors",Easy and Accurate Hardware-based Program Performance Monitoring,,"Lausanne, EPFL",10.5075/epfl-thesis-7917,,core
375549057,2017-11-08T00:00:00,"This paper will describe and present results for a local flood risk reduction system which utilises existing in-network storage capacity to attenuate flow peaks. The storage capacity is mobilised through active flow control automatically regulated by an Artificial Intelligence system using local level monitoring. The effects of climate change, population growth and urbanisation are putting increasing pressure on sewer and drainage networks both in the UK and overseas. The capacity of networks to cope with runoff at the required rate often falls short of requirements leading to localised floods and/or increased CSO spills to receiving waters. Smart Water/ Wastewater Network technologies have the potential to deliver improved service to customers and cost-effective performance improvements for the water industry. CENTAUR aims to provide an innovative, cost effective, local autonomous data driven in-sewer flow control system whose operation will attenuate peaks and reduce the risk of surface water flooding. The system enables the capacity of existing infrastructure to be utilised more efficiently as a very economical alternative to capital-intensive solutions, for example building extra storage capacity. The system is also quick to implement with virtually no enabling works prior to installation. CENTAUR comprises level monitors which relay data to an intelligent controller, which instructs a flow control device regulated by a novel and robust artificial intelligence routine based on Fuzzy Logic. The level monitors and intelligent controller are located locally and utilise real time data to provide effective real time control (RTC). The CENTAUR Fuzzy Logic control algorithm was developed in Matlab. The Matlab RTC algorithm was linked to a SWMM hydro-dynamic model of a test network to prove its efficiency. Further rigorous testing was carried out by the University of Sheffield on the full-scale test facility designed to replicate field conditions. The CENTAUR system has been further developed and it is now implemented and fully functional in trial site in Coimbra, Portugal. Results of successful testing in the laboratory and the Coimbra field trial will be presented",CENTAUR: Smart Utilisation of Wastewater storage capacity to prevent flooding,,CIWEM (The Chartered Institution of Water and Environmental Management),10.5281/zenodo.1051200,,core
390020517,2017-12-25T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and moreРассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что ИВ создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применение в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернет речей (ІР), місця в ньому інтелектуальної власності. Показано роль держави у сприянні розвитку ІР, існуючі проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал та невикористані можливості України у розвитку технологій ІР.Робиться висновок, що у широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,Науково-дослідний інститут інтелектуальної власності НAПрН України,,,core
199656793,2017-12-31,"본 연구의 목적은 인공지능을 비롯한 기술 변화, 그로 인하여 야기되는 노동시장 변화에 대응하기 위하여 고교 단계 직업교육이 변화해야할 방향을 모색하는 것이다.1. Research Purpose

Discussions about the rapid changes in society caused by  drsatic technological changes are vigorous. Currently, vocational education and training in Korea focuses on the development and application of national competence standards and the expansion of work-based learning. This study began with the question ""Is the direction of these high school vocational education policies compatible with the coming changes?"" The purpose of this study is to consider how vocational high school education should respond to technological change. 

2. Research Method

In order to find out the future direction of vocational high school education, we reviewed the existing discussions about technology change, job change, change in demand on competences and education change in the artificial intelligence era. 
In this study, 43 teachers engaged participated in a future workshop. The workshop was conducted three times. During the workshop, we recorded the discussions and conducted text mining and qualitative analysis.
Two surveys were conducted for teachers who participated in the workshop. We analyzed the teachers' perceptions about the direction and purpose of the ideal vocational high school education and analyzed the changes before and after the workshop. One month after the end of the workshop, teachers were asked to evaluate the future orientation of the current policy.

3. Technological Change, Job Change and Necessary Competences in the Era of Artificial Intelligence

We reviewed the development process of artificial intelligence technology and social changes due to technological change such as increasing dependence of task on machinery, platform-based industrial development, changes in production and consumption patterns, political change, and cultural change.
Several studies(WEF, 2016; Grace et al., 2017; Oh, Young et al., 2016; Kim, Seum-woo) differ in methodology and it is not appropriate to concretely synthesize the results of the research. There may be disagreements as to the magnitude and speed of the disappearance of jobs, but the vocational world will be largely reorganized in sooner or later. Jobs expected to have a relatively high possibility of replacement by machines were predicted as transportation, office and administrative support, service, sales, manufacturing, and construction. On the other hand, education, law, arts, social services, management and business, finance, health and computer related jobs are less likely to be automated.
Future Jobs changes involve not only the increase or decrease of jobs, but also the change in employment patterns. Flexible employment is expected to expand and short-term employment will increase. With the development of platform-based services, non-typical employment such as platform workers is expected to continue to expand.
As the future competences, problem solving ability, interpersonal ability (social intelligence), high level cognitive ability (agile thinking ability), creativity (design thinking) and communication ability were emphasized. These competencies are already emphasized as core competencies. Media competencies, virtual cooperation competencies, and digital competencies (data thinking) are added in recognition of technological change. We also considered Gardner's emphasis on the five minds for the future.
Given that there is more potential for future developments in jobs that require processes and strategies that can not be prescribed in advance, current vocational high school education policies that emphasize concrete and explicit skills can be seen as contrary to this trend.
We reviewed future prospects for education and examples of applying the latest technologies to education, and highlighted the following issues. First, the value of knowledge is still important in school education. Second, if artificial intelligence technology is actively used, it is more likely to realize the ideal of individualized customized learning. Third, we paid attention to the possibility that the application of advanced technology could be operated in a way that equality in education, respect for diversity, and social integration through it. Fourth, we need to pay attention to the advantages and limitations of hybrid education through MOOC. Finally, The teacher needs to develop a wide range of competencies, and changes in the curriculum of teacher training are needed.


4. Teachers' Perception on Future Society and Future Vocational high school Education

In order for teachers to visualize the future of vocational high school education, vision of future society should be premised. A total of 43 teachers participated in the future workshop, composed of 4 scenarios where teachers could experience one of them. 
The resuts of text mining on the teachers' narratives in the future workshop were presented for each scenarios. In all scenarios, ‘human’ is a major concept.  In blue society, there is a concept of ‘community’ and ‘environment friendly’, whereas in orange society, there is the concept of ‘individual’ and ‘polarization’. It is blue society and orange society that ‘life’ is closely related to ‘human’, an ‘work’ is especially emphasized only in orange society. In orange society, ‘work’ is closely connected with ‘human’and ‘robot’.
Teachers mentioned as contents of the future vocational high school education most frequently about liberal arts and personality education, reflection on human and life, emotional education, value and ethics education, and basic vocational compatences. There are teachers who have emphasized technical education, but it has also been predicted that the importance of technical education for specific functions will be lowered. Teachers envisioned that in future vocational education, individualized education reflecting the needs and characteristics of students will be carried out and high technology such as virtual reality, big data and robot technology will be applied to vocational high school education.
Teachers predict that future teachers will act as guiders, assistants, coaches, facilitators, coordinators, life designers, career guiders, and counselors. Teachers are predicting that the status of teachers will be lowered and their roles will be reduced in the future. 
There was a change in the awareness of the purpose of vocational high school education for teachers after the workshop. First, the tendency to respond to the needs of specific companies through specific skills acquisition and to emphasize employment after graduation has decreased. Instead, there has been a stronger agreement on broader skills, transferable capabilities, holistic development of students and access to broader career choices.
A month after the workshop, a questionnaire was sent to teachers to evaluate current vocational high school education policy. Teachers recognized that apprenticeship program, NCS based curriculum, and work-based training were not future oriented. 

5. Direction of 2030 Vocational High School Education

□ Moving focus from human resources to future competitiveness of students 
Instead of focusing too much on the need of the labor market, it is necessary to focus more on strengthening students' competencies in vocational high school education, so that they can adapt successfully to an uncertain future and live in a meaningful connection with society.

□ Turning in a direction to provide a wider experience than a specific skills
The NCS approach to break down job units into observable sub-elements emphasizes working in a predictable manner. Jobs to which this approach is applied is a job in which automation in the future can occur most efficiently. Competitive advantage in the current labor market, not only in the future, is to creatively and autonomously improve work and create new value. To help students become competitive in a changing future society, it is necessary to mitigate the current rigid curriculum and regime represented by NCS in  vocational high school education.

□ Transition from job preparation institutions to complex functional education institutions
It should be possible to set up the purpose and function of  vocational education more comprehensive and to provide flexible programs for various purposes. 

□ From basic vocational competences to conceptual thinking and higher mental functions
Emphasis on basic vocational competences in vocational high school education could imply a risk of accepting a standardized and fragmented approach, comforting that it is developing a very useful ability. Despite the fact that basic vocational competences are developed through various curricular activities, but they themselves can not be replaced with the purpose of the academic subjects. It is essential to learn conceptual and theoretical knowledge in school education so that they can understand the principles and apply them to newly developed situations.

□ Prioritization of cutting-edge teaching techniques 
Vocational high school policy has been changed in its direction whenever the regime changes. Policy was decided and implemented unilaterally without proper discussion. Now, it is necessary to look at vocational education as an object to be invested. It is necessary to push forward the high-technology of high school vocational education so that the most advanced technologies can be applied to the vocational high school education scene.

□ Meister and Makerspace
It is necessary to build makerspaces that provide the spaces where creativity and voluntary function can be demonstrated and to guide the path as a Meister.

□ From equality of access to equality of consequence and social integration
In high school vocational education, it is necessary to clearly set the minimum standards for individuals to live happily and healthily in future society, and to convert them into the direction of encouraging educational resources to reach all students. This emphasizes the equality of educational outcomes, not merely the equality of educational opportunities. Decisions should be made to prioritize resources so that students may be able to graduate from high school while meeting minimum standards by strengthening differentiated programs.

□ Change in teacher placement and teacher training
Teacher’s role should change drastically in order to lead future change and adapt to social change. Teachers should be proficient in technology-based teaching and learning methods for constructing class contents and supporting and evaluating learners using various technologies.

□ Build a platform for social consensus in vocational high school education
In the process of change of high school vocational education policy, a few experts or policy makers tried to make a large system change without sufficient consideration. I would like to propose a system for leading and implementing and seeking change and development of high school vocational education for the future. This proposal should be a critical review of the current situation and an active response to the future, and should be a process involving all stakeholders in vocational high school education.요약       ix

제1장 서 론       1
	제1절 연구 필요성과 목적       3
	제2절 연구 내용       7
	제3절 연구 방법       8

제2장 인공지능 기술 변화와 사회 변화       15
	제1절 인공지능 기술 변화       19
	제2절 기술 변화로 인한 사회 변화       38

제3장 인공지능 시대 직업 변화와 필요 역량       47
	제1절 자동화와 직업 변화       49
	제2절 미래 직업세계에서 요구되는 역량       57
	제3절 소결 및 시사점       69

제4장 기술 변화에 따른 교육 변화       79
	제1절 미래 학교교육에 대한 전망       82
	제2절 기술 변화가 반영된 교육 변화       87
	제3절 소결 및 시사점       90

제5장 미래사회와 직업교육에 대한 교사 인식       97
	제1절 미래 워크숍       99
	제2절 특성화고 교사들의 미래사회에 대한 인식       112
	제3절 미래사회의 고교 직업교육에 대한 교사 전망       125
	제4절 소결 및 시사점       133

제6장 고교 직업교육의 목적과 정책에 대한 교사 인식       139
	제1절 직업교육 목적에 대한 교사 인식       142
	제2절 주요 직업교육 정책 미래지향성에 대한 교사 평가       153
	제3절 소결 및 시사점       160

제7장 결론 및 제언       163
	제1절 주요 결론       165
	제2절 제언: 2030 고교 직업교육의 방향       168

SUMMARY       183

참고문헌       193

부록       207
	 미래 워크숍 진행자 역할 안내       209
	 미래 워크숍 1차 워크시트(미래사회)       212
	 미래 워크숍 2차 워크시트(미래 직업교육)       214
	 선호 미래사회/실현 가능성 설문조사       217
	 특성화고 교육의 목표에 대한 인식(사전, 사후)       219
	 미래 워크숍 만족도 조사       221
	 특성화고 정책의 미래지향성 설문조사       222
	 미래사회 시나리오별 텍스트마이닝 결과       22",Vocational educationin the Artificial Intelligence Era,,,,,core
196210066,2017,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0Integrated Process Control based on Distributed In-Situ Sensors into Raw Material and Energy Feedstock, DISIR",Cloud computing for big data analytics in the Process Control Industry,,"Piscataway, NJ",10.1109/MED.2017.7984310,,core
144874233,2017-11-08,"This paper will describe and present results for a local flood risk reduction system which utilises existing in-network storage capacity to attenuate flow peaks. The storage capacity is mobilised through active flow control automatically regulated by an Artificial Intelligence system using local level monitoring.

The effects of climate change, population growth and urbanisation are putting increasing pressure on sewer and drainage networks both in the UK and overseas. The capacity of networks to cope with runoff at the required rate often falls short of requirements leading to localised floods and/or increased CSO spills to receiving waters. Smart Water/ Wastewater Network technologies have the potential to deliver improved service to customers and cost-effective performance improvements for the water industry.

CENTAUR aims to provide an innovative, cost effective, local autonomous data driven in-sewer flow control system whose operation will attenuate peaks and reduce the risk of surface water flooding. The system enables the capacity of existing infrastructure to be utilised more efficiently as a very economical alternative to capital-intensive solutions, for example building extra storage capacity. The system is also quick to implement with virtually no enabling works prior to installation.

CENTAUR comprises level monitors which relay data to an intelligent controller, which instructs a flow control device regulated by a novel and robust artificial intelligence routine based on Fuzzy Logic. The level monitors and intelligent controller are located locally and utilise real time data to provide effective real time control (RTC).

The CENTAUR Fuzzy Logic control algorithm was developed in Matlab. The Matlab RTC algorithm was linked to a SWMM hydro-dynamic model of a test network to prove its efficiency. Further rigorous testing was carried out by the University of Sheffield on the full-scale test facility designed to replicate field conditions. The CENTAUR system has been further developed and it is now implemented and fully functional in trial site in Coimbra, Portugal. Results of successful testing in the laboratory and the Coimbra field trial will be presented",CENTAUR: Smart Utilisation of Wastewater storage capacity to prevent flooding,,,10.5281/zenodo.1051199,,core
287621920,2017-01-01T00:00:00,"An increasing number of workloads are moving to cloud data centers, including large-scale machine learning, big data analytics and back-ends for the Internet of Things. Many of these workloads are written in managed languages such as Java, Python or Scala. The performance and efficiency of managed-language workloads are therefore crucial in terms of hardware cost, energy efficiency and quality of service for these data centers.While managed-language issues such as garbage collection (GC) and JIT compilation have seen a significant amount of research on single-node deployments, data center workloads run across a large number of independent language virtual machines and face new systems challenges that were not previously addressed. At the same time, there has been a large amount of work on specialized systems software and custom hardware for data centers, but most of this work does not fundamentally address managed languages and does not modify the language runtime system, effectively treating it as a black box.In this thesis, we argue that we can substantially improve the performance, efficiency and responsiveness of managed applications in cloud data centers by treating the language runtime system as a fundamental part of the data center stack and co-designing it with both the software systems layer and the hardware layer. In particular, we argue that the cloud operators' full control over the software and hardware stack enables them to co-design these different layers to a degree that would be difficult to achieve in other settings. To support this thesis, we investigate two examples of co-designing the language runtime system with the remainder of the stack, spanning both the hardware and software layers.On the software side, we show how to better support distributed managed-language applications through a ""Holistic"" Language Runtime System, which treats the runtimes underpinning a distributed application as a distributed system itself. We first introduce the concept of a Holistic Runtime System. We then present Taurus, a prototype implementation of such a system, based on the OpenJDK Hotspot JVM. By applying Taurus to two representative real-world workloads, we show that it is effective both in reducing the overall runtime and resource consumption, as well as improving long tail-latencies.On the hardware side, we describe how custom data center SoCs provide an opportunity to revisit the old idea of hardware support for garbage collection. We first show that garbage collection is a suitable workload to be offloaded from the CPU to data-parallel accelerators, by demonstrating how integrated GPUs can be used to perform garbage collection for applications running on the CPU. We then generalize these ideas into a custom hardware accelerator for garbage collection that performs GC more efficiently than running the operation on a traditional CPU. We show this design in the context of a stop-the-world garbage collector, and describe how it could be extended to a fully concurrent, pause-free GC.Finally, we discuss how hardware-software research on managed languages requires new research infrastructure to achieve a higher degree of realism and industry adoption. We then present the foundation of a new research platform for this type of work, using open-source hardware based on the free and open RISC-V ISA combined with the Jikes Research Virtual Machine. Using this research infrastructure, we evaluate the performance and efficiency of our proposed hardware-assisted garbage collector design",Hardware and Software Support for Managed-Language Workloads in Data Centers,,"eScholarship, University of California",,,core
143953425,2017-11-01T00:00:00,"The goal of this research was to develop and optimise microfluidic systems and assays
for rapid detection of selected targets such as marine toxins and human IgG. The
measurement of IgG, as the major format of therapeutic antibodies, was used as a
prototype to develop a centrifugal-based microfluidic system for effectively monitoring
biopharmaceutical production. To achieve this, a sandwich immunoassay for human
IgG detection was developed and used to study the implementation of a new
microfluidic CD-based cartridge. The centrifugal-based microfluidic CD adopted a
serial siphon technique for implementation of automated sequential delivery of the
assay reagents. Surface-confined supercritical angel fluorescence (SAF)-based detection
was designed to sensitively measure the fluorescence signal from the microfluidic CDbased immunoassay. The CD substrate was functionalized with
aminopropyltriethoxysilane (APTES) using plasma enhanced chemical vapour
deposition (PECVD) for the immobilization of analyte capture protein. The developed
prototype microfluidic system could automatically run a microfluidic assay in less than
30 min, and accurately measure industrial bioprocess samples that contained 10 mg mL1 of human IgG. Additionally, computational simulations were performed to
fundamentally understand the kinetics of immunoassays in a microfluidic system. The
effects of varying assay parameters on the capture of analytes in microfluidic-based
heterogeneous immunoassays under real-world operating conditions, was examined by
using theoretical modeling and experimental binding assay results. A marine toxin,
saxitoxins (STX), was another potential target for analysis using the developed
centrifugal-based microfluidic CD. Work was carried out to generate recombinant
antibodies (scFv) to saxitoxin and its derivatives. After successful immunization of the
host animal with different STX-conjugates, an scFv antibody phage library against antiSTX was constructed by PCR amplification of scFv genes, cloning the genes library
into phagemid vector and transforming into E. coli cells. The phages expressing high
affinity antibody gene were isolated by “bio-panning”",Application of centrifugal microfluidics and fluorescence-based detection for rapid biological analysis,,Dublin City University. School of Biotechnology,,,core
84494094,2017-01-01T00:00:00,"Managing intermittent demand is a vital task in several industrial contexts, and good forecasting ability is a fundamental prerequisite for an efficient inventory control system in stochastic environments. In recent years, research has been conducted on single-hidden layer feedforward neural networks, with promising results. In particular, back-propagation has been adopted as a gradient descent-based algorithm for training networks. However, when managing a large number of items, it is not feasible to optimize networks at item level, due to the effort required for tuning the parameters during the training stage. A simpler and faster learning algorithm, called the extreme learning machine, has been therefore proposed in the literature to address this issue, but it has never been tried for forecasting intermittent demand. On the one hand, an extensive comparison of single-hidden layer networks trained by back-propagation is required to improve our understanding of them as predictors of intermittent demand. On the other hand, it is also worth testing extreme learning machines in this context, because of their lower computational complexity and good generalisation ability. In this paper, neural networks trained by back-propagation and extreme learning machines are compared with benchmark neural networks, as well as standard forecasting methods for intermittent demand on real-time series, by combining different input patterns and architectures. A statistical analysis is then conducted to validate the best performance through different aggregation levels. Finally, some insights for practitioners are presented to improve the potential of neural networks for implementation in real environments",Single-hidden layer neural networks for forecasting intermittent demand,,'Elsevier BV',10.1016/j.ijpe.2016.10.021,,core
141489661,2017-08-01T00:00:00,"Carbon fiber reinforced plastic (CFRP) parts for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Different existing path planning algorithms are evaluated and compared. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale",Full Automatic Path Planning of Cooperating Robots in Industrial Applications,,,10.1109/coase.2017.8256157,,core
234922210,2017,"Presently, garment fit evaluation mainly focuses on real try-on, and rarely deals with virtual try-on. With the rapid development of E-commerce, there is a profound growth of garment purchases through the internet. In this context, fit evaluation of virtual garment try-on is vital in the clothing industry. In this paper, we propose a Naive Bayes-based model to evaluate garment fit. The inputs of the proposed model are digital clothing pressures of different body parts, generated from a 3D garment CAD software; while the output is the predicted result of garment fit (fit or unfit). To construct and train the proposed model, data on digital clothing pressures and garment real fit was collected for input and output learning data respectively. By learning from these data, our proposed model can predict garment fit rapidly and automatically without any real try-on; therefore, it can be applied to remote garment fit evaluation in the context of e-shopping. Finally, the effectiveness of our proposed method was validated using a set of test samples. Test results showed that digital clothing pressure is a better index than ease allowance to evaluate garment fit, and machine learning-based garment fit evaluation methods have higher prediction accuracies",Fit evaluation of virtual garment try-on by learning from digital pressure data,https://core.ac.uk/download/pdf/234922210.pdf,'Elsevier BV',10.1016/j.knosys.2017.07.007,,core
201567711,2017-05-01T00:00:00,"Abstract Background Standardized animal-free components are required for manufacturing cell-based medicinal products. Human platelet concentrates are sources of growth factors for cell expansion but such products are characterized by undesired variability. Pooling together single-donor products improves consistency, but the minimal pool sample size was never determined. Methods Supernatant rich in growth factors (SRGF) derived from n = 44 single-donor platelet-apheresis was obtained by CaCl2 addition. n = 10 growth factor concentrations were measured. The data matrix was analyzed by a novel statistical algorithm programmed to create 500 groups of random data from single-donor SRGF and to repeat this task increasing group statistical sample size from n = 2 to n = 20. Thereafter, in created groups (n = 9500), the software calculated means for each growth factor and, matching groups with the same sample size, the software retrieved the percent coefficient of variation (CV) between calculated means. A 20% CV was defined as threshold. For validation, we assessed the CV of concentrations measured in n = 10 pools manufactured according to algorithm results. Finally, we compared growth rate and differentiation potential of adipose-derived stromal/stem cells (ASC) expanded by separate SRGF pools. Results Growth factor concentrations in single-donor SRGF were characterized by high variability (mean (pg/ml)–CV); VEGF: 950–81.4; FGF-b: 27–74.6; PDGF-AA: 7883–28.8; PDGF-AB: 107834–32.5; PDGF-BB: 11142–48.4; Endostatin: 305034–16.2; Angiostatin: 197284–32.9; TGF-β1: 68382–53.7; IGF-I: 70876–38.3; EGF: 2411–30.2). In silico performed analysis suggested that pooling n = 16 single-donor SRGF reduced CV below 20%. Concentrations measured in 10 pools of n = 16 single SRGF were not different from mean values measured in single SRGF, but the CV was reduced to or below the threshold. Separate SRGF pools failed to differently affect ASC growth rate (slope pool A = 0.6; R2 = 0.99; slope pool B = 0.7; R2 0.99) or differentiation potential. Discussion Results deriving from our algorithm and from validation utilizing real SRGF pools demonstrated that pooling n = 16 single-donor SRGF products can ameliorate variability of final growth factor concentrations. Different pools of n = 16 single donor SRGF displayed consitent capability to modulate growth and differentiation potential of expanded ASC. Increasing the pool size should not further improve product composition",Standardization of platelet releasate products for clinical applications in cell therapy: a mathematical approach,,'Springer Science and Business Media LLC',10.1186/s12967-017-1210-z,"[{'title': 'Journal of Translational Medicine', 'identifiers': ['issn:1479-5876', '1479-5876']}]",core
94909857,2017,"With the advent of 21st Century, we stepped into the fourth industrial revolution of cyber physical systems. The industrial components are modular and capable of taking decentralized decisions in real time. The processes can be virtualized and automated through inter-operable service oriented components connected in a network. Therefore, there is need of secured network systems and intrusion detection systems in order to detect network attacks. Use of machine learning for anomaly detection in industrial networks faces challenges which restricts its large-scale commercial deployment. A roadmap is proposed to overcome the challenges. Real world network traffic for an industrial production is generated by IT Security Laboratory at Fraunhofer IOSB. The various attack vectors can be implemented under these circumstances and an adaptive hybrid analysis would reduce the errors of an intrusion detection system. Alarm correlation could be performed for semantic descriptions of detected results to network operator",Anomaly Detection in Industrial Networks: An Introduction,,,,,core
162506848,2017-10-18T00:00:00,"Part 5: Intelligent Electronics and Systems for Industrial IoTInternational audienceInternet of Things (IoT) platforms applied to health promise to offer solutions to the challenges in healthcare systems by providing tools for lowering costs while increasing efficiency in diagnostics and treatment. Many of the works on this topic focus on explaining the concepts and interfaces between different parts of an IoT platform, including the generation of knowledge based on smart sensors gathering bio-signals from the human body which are processed by data mining and more recently, deep neural networks hosted on cloud computing infrastructure. These techniques are designed to serve as useful intelligent companions to healthcare professionals in their practice. In this work we present details about the implementation of an IoT Platform for real-time analysis and management of a network of bio-sensors and gateways, as well as the use of a cloud deep neural network architecture for the classification of ECG data into multiple cardiovascular conditions",IoT Platform for Real-Time Multichannel ECG Monitoring and Classification with Neural Networks,,'Springer Science and Business Media LLC',10.1007/978-3-319-94845-4_16,,core
150600268,2017-01-01T00:00:00,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases",IEEE Access special section editorial: health informatics for the developing world,https://core.ac.uk/download/150600268.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2017.2783118,,core
296203818,2017,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases",IEEE Access special section editorial: health informatics for the developing world,,IEEE,10.1109/ACCESS.2017.2783118,,core
92690630,2017-01-01T00:00:00,"Power grids are critical infrastructure assets that face non-technical losses (NTL) such as electricity theft or faulty meters. NTL may range up to 40% of the total electricity distributed in emerging countries. Industrial NTL detection systems are still largely based on expert knowledge when deciding whether to carry out costly on-site inspections of customers. Electricity providers are reluctant to move to large-scale deployments of automated systems that learn NTL profiles from data due to the latter's propensity to suggest a large number of unnecessary inspections. In this paper, we propose a novel system that combines automated statistical decision making with expert knowledge. First, we propose a machine learning framework that classifies customers into NTL or non-NTL using a variety of features derived from the customers' consumption data. The methodology used is specifically tailored to the level of noise in the data. Second, in order to allow human experts to feed their knowledge in the decision loop, we propose a method for visualizing prediction results at various granularity levels in a spatial hologram. Our approach allows domain experts to put the classification results into the context of the data and to incorporate their knowledge for making the final decisions of which customers to inspect. This work has resulted in appreciable results on a real-world data set of 3.6M customers. Our system is being deployed in a commercial NTL detection software",Identifying Irregular Power Usage by Turning Predictions into Holographic Spatial Visualizations,https://core.ac.uk/download/92690630.pdf,,10.1109/icdmw.2017.40,,core
132774464,2017-01-01T00:00:00,"The increasing attention devoted to air quality by legislative, scientific, industrial and public sectors has led to the development of different control strategies for the emission level monitoring. In this scenario, Predictive Emission Monitoring System (PEMS) is able to predict emission concentrations thanks to empirical or first principles models fed by real-time process data provided by measurement sensors. It follows that PEMS consistency (and, crucially, its acceptance from regulations-enforcing agencies) strictly depends on input accuracy and that reliable Sensor Validation (SV) strategies are fundamental. In this work, the capability of two different SV techniques, Feed Forward Neural Networks and Locally Weighted Regression, is tested exploiting a commercial software package (ABB's IMP) on actual field data from a fluid catalytic cracking unit. The results showed that both techniques are suitable as complement to PEMS applications, but Locally Weighted Regression results are preferable for performance, economic and operating reasons",Assessment and testing of sensor Validation algorithms for environmental monitoring applications,,Italian Association of Chemical Engineering - AIDIC,10.3303/CET1757056,,core
42685871,2017-07-25T00:00:00,"Non-technical losses (NTL) such as electricity theft cause significant harm
to our economies, as in some countries they may range up to 40% of the total
electricity distributed. Detecting NTLs requires costly on-site inspections.
Accurate prediction of NTLs for customers using machine learning is therefore
crucial. To date, related research largely ignore that the two classes of
regular and non-regular customers are highly imbalanced, that NTL proportions
may change and mostly consider small data sets, often not allowing to deploy
the results in production. In this paper, we present a comprehensive approach
to assess three NTL detection models for different NTL proportions in large
real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and
Support Vector Machine. This work has resulted in appreciable results that are
about to be deployed in a leading industry solution. We believe that the
considerations and observations made in this contribution are necessary for
future smart meter research in order to report their effectiveness on
imbalanced and large real world data sets.Comment: Proceedings of the Seventh IEEE Conference on Innovative Smart Grid
  Technologies (ISGT 2016",Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets,http://arxiv.org/abs/1602.08350,,,,core
88286731,2017-01-01T00:00:00Z,"Modern broadband hybrid optical-wireless access networks have gained the attention of academia and industry due to their strategic advantages (cost-efficiency, huge bandwidth, flexibility, and mobility). At the same time, the proliferation of Software Defined Networking (SDN) enables the efficient reconfiguration of the underlying network components dynamically using SDN controllers. Hence, effective traffic-aware schemes are feasible in dynamically determining suitable configuration parameters for advancing the network performance. To this end, a novel machine learning mechanism is proposed for an SDN-enabled hybrid optical-wireless network. The proposed architecture consists of a 10-gigabit-capable passive optical network (XG-PON) in the network backhaul and multiple Long Term Evolution (LTE) radio access networks in the fronthaul. The proposed mechanism receives traffic-aware knowledge from the SDN controllers and applies an adjustment on the uplink-downlink configuration in the LTE radio communication. This traffic-aware mechanism is capable of determining the most suitable configuration based on the traffic dynamics in the whole hybrid network. The introduced scheme is evaluated in a realistic environment using real traffic traces such as Voice over IP (VoIP), real-time video, and streaming video. According to the obtained numerical results, the proposed mechanism offers significant improvements in the network performance in terms of latency and jitter",DIANA: A Machine Learning Mechanism for Adjusting the TDD Uplink-Downlink Configuration in XG-PON-LTE Systems,,Hindawi Limited,10.1155/2017/8198017,"[{'title': None, 'identifiers': ['1574-017x', '1875-905x', 'issn:1574-017X', 'issn:1875-905X']}]",core
478869500,2017-01-01T00:00:00,"2017 annual report for the Blue Waters ProjectNSF OCI-0725070NSF ACI-12389932017 Blue Waters Annual Report Table of Contents
18.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence in Star-Forming Molecular Clouds on the Blue Waters System
20.	Tiziana Di Matteo, Supermassive Black Holes at the Cosmic Frontier
22.	Gilbert Holder, Theoretical Astrophysics and Data Analysis
24.	Eliu Huerta, Detection of Gravitational Wave Sources in Dense Stellar Environments
26.	Eliu Huerta, Deep Neural Networks to Enable Real-Time Multimessenger Astrophysics
28.	Thomas W. Jones, Toward Robust Magnetohydrodynamic Simulations of Galaxy Cluster Formation
30.	Eric J. Lentz, Exploring the Nature of Exploding Massive Stars with High Resolution
32.	Deborah Levin, Modeling Plasma Flows with Kinetic Approaches using Hybrid CPU-GPU Computing
34.	Yi-Hsin Liu, Three-Dimensional Nature of Collisionless Magnetic Reconnection at Earth’s Magnetopause
36.	Warren B. Mori, Transformative Petascale Particle-in-Cell Simulations
38.	Scott C. Noble, Mini-disk Dynamics about Supermassive Black Holes Binaries 
40.	Michael L. Norman, Realistic Simulations of the Intergalactic Medium:  The Search for Missing Physics
42.	Brian W. O’Shea, Simulating Galaxy Formation Across Cosmic Time
44.	Christian D. Ott, 3D General-Relativistic Radiation-Hydrodynamic Simulations of Core-Collapse Supernovae
46.	Nikolai Pogorelov, Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with a Multiscale Fluid-Kinetic Simulations Suite
48.	Thomas Quinn, Unified Modeling of Galaxy Populations in Clusters
50.	Vadim Roytershteyn, Kinetic Simulations of Large-Scale Plasma Turbulence
52.	Hsi-Yu Schive, GPU-Accelerated Adaptive Mesh Refinement
54.	Stuart L. Shapiro, Magnetorotational Collapse of Supermassive Stars:  Black Hole Formation, Gravitational Waves, and Jets
56.	Alexander Tchekhovskoy, GPU-Accelerated Simulations:  Black Holes, Spaghettified Stars, and Tilted Disks
58.	Gabor Toth, Advanced Space Weather Modeling
60.	Paul R. Woodward, 3D Simulations of i-Process Nucleosynthesis
64.	Marin Clark, High-Resolution Digital Surface Models of the 2016 Mw7.8 Kaikoura Earthquake, New Zealand
66.	Jennifer Corcoran, Image Processing to Build a Multi-Temporal Vegetation Elevation Model(MTVEM) of the Great Lakes Basin(GLB)
68.	Larry Di-Girolamo, The Terra Data Fusion Project
70.	Marcelo H. Garcia, Large-Eddy Simulation of Sediment Transport and Hydrodynamics at River Bifurcations:  Using a Highly Scalable Spectral Element-Based CFD Solver
72.	Ian Howat, The Reference Elevation Model of Antarctica
74.	Sonia Lasher-Trapp, Untangling Entrainment and Precipitation in Convective Clouds
76.	Lijun Liu, Understanding the 4-D Evolution of the Solid Earth Using Geodynamic Models with Data Assimilation
78.	Philip J. Maechling, Physics-Based Modeling of High-Frequency Ground Motions and Probabilistic Seismic Hazard Analysis
80.	Paul Morin, Enhanced Digital Elevation Model for the Artic
82.	Leigh Orf, Simulating the Most Devastating Tornadoes Embedded Within Supercell Thunderstorms
84.	Robert Rauber, Don Wuebbles, High-Resolution Earth System Modeling Using Blue Waters’ Capabilities
86.	Jamesina J. Simpson, Location-Specific Space Weather Hazards to Electric Power Grids Calculated on a Global Scale
88.	Ryan L. Sriver, Impact of Ocean Coupling on Simulated Tropical Cyclone Activity in the High-Resolution Community Earth System Mode
90.	Robert J. Trapp, Petascale Modeling of Convective Storms Under Climate Change and Variability
92.	Junshik Um, Impacts of Orientation and Morphology of Small Atmospheric Ice Crystals on in-situ Aircraft Measurements:  Scattering Calculations
94.	Albert J. Valocchi, Pore-Scale Simulation of Multiphase Flow in Porous Media with Applications to Geological Sequestration of Carbon Dioxide
96.	Matthew West, 3D Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
98.	Donald J. Wuebbles, Particulate Matter Prediction and Source Attribution for U.S. Air Quality Management in a Changing World
102.	David Ackerman, Exploring Confinement vs. Orientation Effects in Rigid and Semi-Flexible Polymers Using a Massively Parallel Framework
104.	Ange-Therese Akono, Multi-Scale and Multi-Physics Modeling of the Strength of Geopolymer Composites
106.	Jean Paul Allain, Harnessing Petascale Computing to Explain Fundamental Mechanisms Driving Nanopatterning of Multicomponent Surfaces by Directed Irradiation Synthesis
108.	Narayana R. Aluru, Study of DIBs with Functional Channels
110.	Jerzy Bernholc, Petaflops Simulation and Design of Nanoscale Materials and Devices
112.	Daniel Bodony, Reducing Jet Aircraft Noise
114.	David Ceperley, Properties of Dense Hydrogen
116.	Huck Beng Chew, Scalable Nanopatterning of Graphene by Hydrogen-Plasma Etching
118.	Davide Curreli, hPIC:  A Scalable Electrostatic Particle-In-Cell for Plasma-Material Interactions
120.	J. P. Draayer, Innovative Ab Initio Symmetry-Adapted No-Core Shell Model for Advancing Fundamental Physics and Astrophysics
122.	Lian Duan, DNS of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
124.	Said Elghobashi, Dispersion of Fully Resolved Liquid Droplets in Isotropic Turbulent Flow
126.	Elif Ertekin, QMCBD:  A Living Database to Accelerate Worldwide Development and use of Quantum Monte Carlo Methods
128.	Paul Fischer, Numerical Methods and Software for Computational Fluid Dynamics, NEK5000
130.	Marcelo H. Garcia, Direct Numerical Simulation of Turbulence and Sediment Transport in Oscillatory Boundary Layer Flows
132.	Paolo Gardoni, 3D Probabilistic Physics-Based Seismic Hazard Maps for Regional Risk Analysis
134.	Mattia Gazzola, Optimal Bio-Locomotion Strategies in Fluids
136.	Kathryn Huff, Coupled Multi-Physics of Advanced Molten Salt Nuclear Reactors
138.	Sohrab Ismail-Beigi, Understanding Hydrogen Storage in Metal Organic Frameworks using Massively-Parallel Electronic Structure Calculations
140.	Prashant K. Jain, Atomistic Modeling of Transformations in Nanocrystals
142.	Eric Johnsen, Numerical Simulations of Collapsing Cavitation Bubbles on Blue Waters
144.	Gerhard Klimeck, Leading Future Electronics into the Nano Regime Using Quantum Atomistic Simulations in NEMO5
146.	Deborah A. Levin, Kinetic Simulations of Unsteady Shock-Boundary Layer Interactions
148.	Paul Mackenzie, High Energy Physics on Blue Waters
150.	Burkhard Militzer, First-Principles Computer Simulations of Hydrocarbons Under Fusion Conditions
152.	Sarma L. Rani, Direct Numerical Simulations of the Relative Motion of High-Inertia Particles in Isotropic Turbulence
154.	Caroline Riedl, Mapping Proton Quark Structure Using Petabytes of COMPASS Data
156.	Andre Schleife, Optical Determination of Crystal Phase in Semiconductor Nanocrystals
158.	Ahmed Taha, Advanced Digital Technology for Materials and Manufacturing
160.	Brian G. Thomas, Transient Multiphase Flow Phenomena and Defect Formation in Steel Continuous Casting
162.	Rafael Tinoco Lopez, High Resolution Numerical Simulation of Oscillatory Flow and Sediment Transport through Aquatic Vegetation
164.	Lucas K. Wagner, Quantum Monte Carlo Simulations of Magnetism and Models in Condensed Matter
166.	P.K. Yeung, Intermittency, Resolution Effects and High Schmidt Number Mixing in Turbulence
170.	Donna Cox, CADENS NSF Project:  Digital Literacy, Data Visualization, and the Cinematic Presentation of Science
172.	William Gropp, Algorithms for Extreme-Scale Systems
174.	Levent Gurel, Parallelization of the Multilevel Fast Multipole Algorithm(MLFMA) on Heterogeneous CPU-GPU Architectures
176.	Ravishankar K. Iyer, Predicting Performance Degradation and Failure of Applications through System Activity Monitoring
178.	Rakesh Nagi, Parallel Algorithms for Solving Large Assignment Problems
180.	Luke Olson, Localizing Communication in Sparse Matrix Operations
182.	Edgar Solomonik, Performance Evaluation of New Algebraic Algorithms and Libraries
184.	Tandy Warnow, Parallel Algorithms for Big Data Phylogenomics, Proteomics, and Metagenomics
186.	Tao Xie, Hardware Acceleration of Deep Learning
188.	Kevin Olson, A Critical Evaluation of the OP2/OPS Parallel Meshing and Code Generation Software
192.	Aleksei Aksimentiev, DNA Origami Membrane Channels
194.	Aleksei Aksimentiev, Molecular Mechanism of Nuclear Transports
196.	Gregory Bauer, Improving NWChem Scalability Using the DataSpaces Framework
198.	Gustavo Caetano-Anolles, How Function Shapes Dynamics in Protein Evolution
200.	Isaac Cann, Cellulosome Structure Determination by Atomistic Simulations Combined with Experimental Assays
202.	Vincenzo Carnevale, Mechanism of Temperature Sensitivity in TRPV1 Channel
204.	Thomas E. Cheatham III, Exploring the Structure and Dynamics of Converged Ensembles of DNA and RNA Through Molecular Dynamics Simulations
206.	Christina C.H. Cheng, Structural Basis for Extreme Cold Tolerance in the Eye Lenses of Teleost Fishes
208.	Ken A. Dill, Predicting Protein Structures with Physical Petascale Molecular Simulations
210.	Ahmed Elbanna, Multiscale Modeling of Biofilm Dynamics in Drinking Water Distribution Systems:  Toward Predictive Modeling of Pathogen Outbreaks
212.	Peter Freddolino, Comprehensive in silico Mapping of DNA-Binding Protein Affinity Landscapes
214.	Sharon Hammes-Schiffer, Non-Born-Oppenheimer Effects Between Electrons and Protons
216.	So Hirata, Brueckner-Goldstone Quantum Monte Carlo
218.	Peter Kasson, How Membrane Organization Controls Influenza Infections
220.	Zaida Luthey-Schulten, A Hybrid Stochastic-Deterministic Simulation Method Enables Fast Simulation of Cellular Processes in Eukaryotes
222.	Nancy Makri, Quantum-Classical Path Integral Simulation of Charge Transfer Reactions
224.	Arif Masud, Patient-Specific HPC Models and Simulation-Based Imaging for Cardiovascular Surgical Planning
226.	James W. Mazzuca, Quantum Effects of Proton Transfer in Biological Systems
228.	Mahmoud Moradi, Thermodynamic Characterization of Conformational Landscape in Proton-Coupled Oligopeptide Transporters
230.	Vijay Pande, Machine Learning Reveals Ligand-Directed Conformational Change of u Opioid Receptor
232.	Benoit Roux, Elucidating the Molecular Mechanism of C-type Inactivation in Potassium Channels
234.	Klaus Schulten, Studying Cellular Processes through the Computational Microscope
236.	Diwakar Shukla, Understanding the Protein Allostery in Kinanses and GPCRs
238.	Ivan Soltesz, Data-Driven, Biologically Constrained Computational Model of the Hippocampal Network at Full Scale
240.	Marcos Sotomayor, Stretching the Cadherin Molecular Velcro of Cell-Cell Junctions
242.	Ahsok Srinivasan, Simulation of Viral Infection Propagation through Air Travel
244. 	Brad Sutton, High-Resolution Magnetic Resonance Imaging of Mechanical Properties of the Brain
246.	Ilias Tagkopoulos, A Crystal Ball of Bacterial Behavior:  from Data to Prediction using Genome-Scale Models
248.	Greg A. Voth, Large-Scale Coarse-Grained Molecular Simulations of the Viral Lifecycle of HIV-1
252.	Lars Hansen, Yongyang Cai, Policy Responses to Climate Change in a Dynamic Stochastic Economy
254.	Wendy K. Tam Cho, Enabling Redistricting Reform:  A Computational Study of Zoning Optimization
258.	Elizabeth Agee, Resolving Plant Functional Biodiversity to Quantify Forest Drought Resistance in the Amazon
260.	Maureen T. Brooks, Modeling Nonlinear Physical-Biological Interactions:  Eddies and Sargassum in the North Atlantic
262.	Iryna Butsky, The Role of Cosmic Rays in Isolated Disk Galaxies
264.	Jon Calhoun, Analyzing the Propagation of Soft Error Corruption in HPC Applications
266.	Justin Drake, Toward Developing a Thermodynamic Model of Binding-Induced Conformational Transitions in Short, Disordered Protein Regions
268. 	Paul Hime, Genomic Perspectives on the Amphibian Tree of Life
270.	Michael P. Howard, Multiscale Simulations of Complex Fluid Rheology
272.	Alexandra L. Jones, High Accuracy Radiative Transfer in Cloudy Atmospheres
274.	Andrew Kirby, High-Fidelity Blade-Resolved Wind Farm Simulations
276.	Sara Kokkila Schumacher, Reducing the Computational Cost of Coupled Clustery Theory
278.	Larisa Reames, Simulated Effects of Urban Environments on the Dynamics of a Supercell Thunderstorm
280.	Sherwood Richers, Monte Carlo Neutrino Closures in 3D GRMHD Simulations of Core-Collapse Supernovae and Neutron Star Mergers
282.	Sean L. Seyler, Understanding the Role of Hydrodynamic Fluctuations in Biomacromolecular Dynamics through the Development of Hybrid Atomistic-Continuum Simulation
284.	Ronald Stenz, The Impacts of Hydrometeor Centrifuging on Tornado Dynamics
286.	Erin Teich, Glassy Dynamics and Identity Crises in Hard-Particle Systems
288.	Samuel Totorica, Magnetic Reconnection in Laser-Driven Plasmas:  from Astrophysics to the Laboratory in silico






2017 Blue Waters Annual Report Table of Contents
18.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence in Star-Forming Molecular Clouds on the Blue Waters System
20.	Tiziana Di Matteo, Supermassive Black Holes at the Cosmic Frontier
22.	Gilbert Holder, Theoretical Astrophysics and Data Analysis
24.	Eliu Huerta, Detection of Gravitational Wave Sources in Dense Stellar Environments
26.	Eliu Huerta, Deep Neural Networks to Enable Real-Time Multimessenger Astrophysics
28.	Thomas W. Jones, Toward Robust Magnetohydrodynamic Simulations of Galaxy Cluster Formation
30.	Eric J. Lentz, Exploring the Nature of Exploding Massive Stars with High Resolution
32.	Deborah Levin, Modeling Plasma Flows with Kinetic Approaches using Hybrid CPU-GPU Computing
34.	Yi-Hsin Liu, Three-Dimensional Nature of Collisionless Magnetic Reconnection at Earth’s Magnetopause
36.	Warren B. Mori, Transformative Petascale Particle-in-Cell Simulations
38.	Scott C. Noble, Mini-disk Dynamics about Supermassive Black Holes Binaries 
40.	Michael L. Norman, Realistic Simulations of the Intergalactic Medium:  The Search for Missing Physics
42.	Brian W. O’Shea, Simulating Galaxy Formation Across Cosmic Time
44.	Christian D. Ott, 3D General-Relativistic Radiation-Hydrodynamic Simulations of Core-Collapse Supernovae
46.	Nikolai Pogorelov, Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with a Multiscale Fluid-Kinetic Simulations Suite
48.	Thomas Quinn, Unified Modeling of Galaxy Populations in Clusters
50.	Vadim Roytershteyn, Kinetic Simulations of Large-Scale Plasma Turbulence
52.	Hsi-Yu Schive, GPU-Accelerated Adaptive Mesh Refinement
54.	Stuart L. Shapiro, Magnetorotational Collapse of Supermassive Stars:  Black Hole Formation, Gravitational Waves, and Jets
56.	Alexander Tchekhovskoy, GPU-Accelerated Simulations:  Black Holes, Spaghettified Stars, and Tilted Disks
58.	Gabor Toth, Advanced Space Weather Modeling
60.	Paul R. Woodward, 3D Simulations of i-Process Nucleosynthesis
64.	Marin Clark, High-Resolution Digital Surface Models of the 2016 Mw7.8 Kaikoura Earthquake, New Zealand
66.	Jennifer Corcoran, Image Processing to Build a Multi-Temporal Vegetation Elevation Model(MTVEM) of the Great Lakes Basin(GLB)
68.	Larry Di-Girolamo, The Terra Data Fusion Project
70.	Marcelo H. Garcia, Large-Eddy Simulation of Sediment Transport and Hydrodynamics at River Bifurcations:  Using a Highly Scalable Spectral Element-Based CFD Solver
72.	Ian Howat, The Reference Elevation Model of Antarctica
74.	Sonia Lasher-Trapp, Untangling Entrainment and Precipitation in Convective Clouds
76.	Lijun Liu, Understanding the 4-D Evolution of the Solid Earth Using Geodynamic Models with Data Assimilation
78.	Philip J. Maechling, Physics-Based Modeling of High-Frequency Ground Motions and Probabilistic Seismic Hazard Analysis
80.	Paul Morin, Enhanced Digital Elevation Model for the Artic
82.	Leigh Orf, Simulating the Most Devastating Tornadoes Embedded Within Supercell Thunderstorms
84.	Robert Rauber, Don Wuebbles, High-Resolution Earth System Modeling Using Blue Waters’ Capabilities
86.	Jamesina J. Simpson, Location-Specific Space Weather Hazards to Electric Power Grids Calculated on a Global Scale
88.	Ryan L. Sriver, Impact of Ocean Coupling on Simulated Tropical Cyclone Activity in the High-Resolution Community Earth System Mode
90.	Robert J. Trapp, Petascale Modeling of Convective Storms Under Climate Change and Variability
92.	Junshik Um, Impacts of Orientation and Morphology of Small Atmospheric Ice Crystals on in-situ Aircraft Measurements:  Scattering Calculations
94.	Albert J. Valocchi, Pore-Scale Simulation of Multiphase Flow in Porous Media with Applications to Geological Sequestration of Carbon Dioxide
96.	Matthew West, 3D Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
98.	Donald J. Wuebbles, Particulate Matter Prediction and Source Attribution for U.S. Air Quality Management in a Changing World
102.	David Ackerman, Exploring Confinement vs. Orientation Effects in Rigid and Semi-Flexible Polymers Using a Massively Parallel Framework
104.	Ange-Therese Akono, Multi-Scale and Multi-Physics Modeling of the Strength of Geopolymer Composites
106.	Jean Paul Allain, Harnessing Petascale Computing to Explain Fundamental Mechanisms Driving Nanopatterning of Multicomponent Surfaces by Directed Irradiation Synthesis
108.	Narayana R. Aluru, Study of DIBs with Functional Channels
110.	Jerzy Bernholc, Petaflops Simulation and Design of Nanoscale Materials and Devices
112.	Daniel Bodony, Reducing Jet Aircraft Noise
114.	David Ceperley, Properties of Dense Hydrogen
116.	Huck Beng Chew, Scalable Nanopatterning of Graphene by Hydrogen-Plasma Etching
118.	Davide Curreli, hPIC:  A Scalable Electrostatic Particle-In-Cell for Plasma-Material Interactions
120.	J. P. Draayer, Innovative Ab Initio Symmetry-Adapted No-Core Shell Model for Advancing Fundamental Physics and Astrophysics
122.	Lian Duan, DNS of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
124.	Said Elghobashi, Dispersion of Fully Resolved Liquid Droplets in Isotropic Turbulent Flow
126.	Elif Ertekin, QMCBD:  A Living Database to Accelerate Worldwide Development and use of Quantum Monte Carlo Methods
128.	Paul Fischer, Numerical Methods and Software for Computational Fluid Dynamics, NEK5000
130.	Marcelo H. Garcia, Direct Numerical Simulation of Turbulence and Sediment Transport in Oscillatory Boundary Layer Flows
132.	Paolo Gardoni, 3D Probabilistic Physics-Based Seismic Hazard Maps for Regional Risk Analysis
134.	Mattia Gazzola, Optimal Bio-Locomotion Strategies in Fluids
136.	Kathryn Huff, Coupled Multi-Physics of Advanced Molten Salt Nuclear Reactors
138.	Sohrab Ismail-Beigi, Understanding Hydrogen Storage in Metal Organic Frameworks using Massively-Parallel Electronic Structure Calculations
140.	Prashant K. Jain, Atomistic Modeling of Transformations in Nanocrystals
142.	Eric Johnsen, Numerical Simulations of Collapsing Cavitation Bubbles on Blue Waters
144.	Gerhard Klimeck, Leading Future Electronics into the Nano Regime Using Quantum Atomistic Simulations in NEMO5
146.	Deborah A. Levin, Kinetic Simulations of Unsteady Shock-Boundary Layer Interactions
148.	Paul Mackenzie, High Energy Physics on Blue Waters
150.	Burkhard Militzer, First-Principles Computer Simulations of Hydrocarbons Under Fusion Conditions
152.	Sarma L. Rani, Direct Numerical Simulations of the Relative Motion of High-Inertia Particles in Isotropic Turbulence
154.	Caroline Riedl, Mapping Proton Quark Structure Using Petabytes of COMPASS Data
156.	Andre Schleife, Optical Determination of Crystal Phase in Semiconductor Nanocrystals
158.	Ahmed Taha, Advanced Digital Technology for Materials and Manufacturing
160.	Brian G. Thomas, Transient Multiphase Flow Phenomena and Defect Formation in Steel Continuous Casting
162.	Rafael Tinoco Lopez, High Resolution Numerical Simulation of Oscillatory Flow and Sediment Transport through Aquatic Vegetation
164.	Lucas K. Wagner, Quantum Monte Carlo Simulations of Magnetism and Models in Condensed Matter
166.	P.K. Yeung, Intermittency, Resolution Effects and High Schmidt Number Mixing in Turbulence
170.	Donna Cox, CADENS NSF Project:  Digital Literacy, Data Visualization, and the Cinematic Presentation of Science
172.	William Gropp, Algorithms for Extreme-Scale Systems
174.	Levent Gurel, Parallelization of the Multilevel Fast Multipole Algorithm(MLFMA) on Heterogeneous CPU-GPU Architectures
176.	Ravishankar K. Iyer, Predicting Performance Degradation and Failure of Applications through System Activity Monitoring
178.	Rakesh Nagi, Parallel Algorithms for Solving Large Assignment Problems
180.	Luke Olson, Localizing Communication in Sparse Matrix Operations
182.	Edgar Solomonik, Performance Evaluation of New Algebraic Algorithms and Libraries
184.	Tandy Warnow, Parallel Algorithms for Big Data Phylogenomics, Proteomics, and Metagenomics
186.	Tao Xie, Hardware Acceleration of Deep Learning
188.	Kevin Olson, A Critical Evaluation of the OP2/OPS Parallel Meshing and Code Generation Software
192.	Aleksei Aksimentiev, DNA Origami Membrane Channels
194.	Aleksei Aksimentiev, Molecular Mechanism of Nuclear Transports
196.	Gregory Bauer, Improving NWChem Scalability Using the DataSpaces Framework
198.	Gustavo Caetano-Anolles, How Function Shapes Dynamics in Protein Evolution
200.	Isaac Cann, Cellulosome Structure Determination by Atomistic Simulations Combined with Experimental Assays
202.	Vincenzo Carnevale, Mechanism of Temperature Sensitivity in TRPV1 Channel
204.	Thomas E. Cheatham III, Exploring the Structure and Dynamics of Converged Ensembles of DNA and RNA Through Molecular Dynamics Simulations
206.	Christina C.H. Cheng, Structural Basis for Extreme Cold",Blue Waters 2017 Annual Report,,,,,core
378419309,2017-01-01T00:00:00,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases",IEEE Access special section editorial: health informatics for the developing world,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
202094075,2017-12-01T00:00:00Z,"One of the most notable trends associated with the Fourth industrial revolution is a significant strengthening of the role played by semantic methods. They are engaged in artificial intelligence means, knowledge mining in huge flows of big data, robotization, and in the internet of things. Smart contracts also can be mentioned here, although the ’intelligence’ of smart contracts still needs to be seriously elaborated. These trends should inevitably lead to an increased role of logical methods working with semantics, and significantly expand the scope of their application in practice. However, there are a number of problems that hinder this process.

We are developing an approach, which makes the application of logical modeling efficient in some important areas. The approach is based on the concept of locally simple models and is primarily focused on solving tasks in the management of enterprises, organizations, governing bodies. The most important feature of locally simple models is their ability to replace software systems. Replacement of programming by modeling gives huge advantages, for instance, it dramatically reduces development and support costs. Modeling, unlike programming, preserves the explicit semantics of models allowing integration with artificial intelligence and robots. In addition, models are much more understandable to general people than programs.

In this paper we propose the implementation of the concept of locally simple modeling on the basis of so-called document models, which has been developed by us earlier. It is shown that locally simple modeling is realized through document models with finite submodel coverages. In the second part of the paper an example of using document models for solving a management problem of real complexity is demonstrated",Locally Simple Models Construction: Methodology and Practice,,Irkutsk State University,,"[{'title': None, 'identifiers': ['1997-7670', '2541-8785', 'issn:1997-7670', 'issn:2541-8785']}]",core
297633323,,"Gate location is one of the most important design elements of an injection mould.
Injection moulding is a very complex process with several parameters having interactive
effects on each otherhence, computer-aided engineering and artificial intelligence were
utilised to optimise mould design based on weld line. Therefore, the finite element
analysis, artificial neural network and genetic algorithm were linked to find optimum gate
location in a plastic product. A reliable numerical model was developed by Moldflow
software based on a real product to simulate injection process. Moldflow was used to
predict weld line length and position of the part. Weld lines are visually undesirable and
a plastic part is structurally weak at weld line positions. This study describes how the
weld line was formed on the part. Polyamide-6 (PA-66) was used as the plastic material.
To find optimum gate location, the finite element predictions were implemented to train
a neural network, which was used later in the genetic algorithm. The optimisation
objective was to find gate location which led to minimum weld line length. This research
concluded that the developed procedure can efficiently optimise complex manufacturing
processes and prevent flaws in products and thus, can be applied practically in the
injection moulding process","Optimisation of gate location based on weld line in plastic injection moulding using computer-aided engineering, artificial neural network, and genetic algorithm",,"International Journal of Automotive and Mechanical Engineering, Universiti Malaysia Pahang Publishing (UMP Publisher)",,,core
88566911,2017-01-01T00:00:00Z,"Recent advances in modern manufacturing industries have created a great need to track and identify objects and parts by obtaining real-time information. One of the main technologies which has been utilized for this need is the Radio Frequency Identification (RFID) system. As a result of adopting this technology to the manufacturing industry environment, RFID Network Planning (RNP) has become a challenge. Mainly RNP deals with calculating the number and position of antennas which should be deployed in the RFID network to achieve full coverage of the tags that need to be read. The ultimate goal of this paper is to present and evaluate a way of modelling and optimizing nonlinear RNP problems utilizing artificial intelligence (AI) techniques. This effort has led the author to propose a novel AI algorithm, which has been named “hybrid AI optimization technique,” to perform optimization of RNP as a hard learning problem. The proposed algorithm is composed of two different optimization algorithms: Redundant Antenna Elimination (RAE) and Ring Probabilistic Logic Neural Networks (RPLNN). The proposed hybrid paradigm has been explored using a flexible manufacturing system (FMS), and results have been compared with Genetic Algorithm (GA) that demonstrates the feasibility of the proposed architecture successfully",Introducing a Novel Hybrid Artificial Intelligence Algorithm to Optimize Network of Industrial Applications in Modern Manufacturing,,Hindawi-Wiley,10.1155/2017/8728209,"[{'title': None, 'identifiers': ['issn:1099-0526', 'issn:1076-2787', '1076-2787', '1099-0526']}]",core
93944077,2017-09-09T00:00:00,"Power grids are critical infrastructure assets that face non-technical losses
(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of
the total electricity distributed in emerging countries. Industrial NTL
detection systems are still largely based on expert knowledge when deciding
whether to carry out costly on-site inspections of customers. Electricity
providers are reluctant to move to large-scale deployments of automated systems
that learn NTL profiles from data due to the latter's propensity to suggest a
large number of unnecessary inspections. In this paper, we propose a novel
system that combines automated statistical decision making with expert
knowledge. First, we propose a machine learning framework that classifies
customers into NTL or non-NTL using a variety of features derived from the
customers' consumption data. The methodology used is specifically tailored to
the level of noise in the data. Second, in order to allow human experts to feed
their knowledge in the decision loop, we propose a method for visualizing
prediction results at various granularity levels in a spatial hologram. Our
approach allows domain experts to put the classification results into the
context of the data and to incorporate their knowledge for making the final
decisions of which customers to inspect. This work has resulted in appreciable
results on a real-world data set of 3.6M customers. Our system is being
deployed in a commercial NTL detection software.Comment: Proceedings of the 17th IEEE International Conference on Data Mining
  Workshops (ICDMW 2017","Identifying Irregular Power Usage by Turning Predictions into
  Holographic Spatial Visualizations",http://arxiv.org/abs/1709.03008,,,,core
391271841,2017-09-18T00:00:00,"Conference of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2017 ; Conference Date: 18 September 2017 Through 22 September 2017; Conference Code:209269International audienceWe present WHODID: a turnkey intuitive web-based interface for fault detection, identification and diagnosis in production units. Fault detection and identification is an extremely useful feature and is becoming a necessity in modern production units. Moreover, the large deployment of sensors within the stations of a production line has enabled the close monitoring of products being manufactured. In this context, there is a high demand for computer intelligence able to detect and isolate faults inside production lines, and to additionally provide a diagnosis for maintenance on the identified faulty production device, with the purpose of preventing subsequent faults caused by the diagnosed faulty device behavior. We thus introduce a system which has fault detection, isolation, and identification features, for retrospective and on-the-fly monitoring and maintenance of complex dynamical production processes. It provides real-time answers to the questions: "" is there a fault? "" , "" where did it happen? "" , "" for what reason? "". The method is based on a posteriori analysis of decision sequences in XGBoost tree models, using recurrent neural networks sequential models of tree paths. The particularity of the presented system is that it is robust to missing or faulty sensor measurements, it does not require any modeling of the underlying, possibly exogenous manufacturing process, and provides fault diagnosis along with confidence level in plain English formulations. The latter can be used as maintenance directions by a human operator in charge of production monitoring and control","WHODID: Web-based interface for Human-assisted factory Operations in fault Detection, Identification and Diagnosis",,'Springer Science and Business Media LLC',10.1007/978-3-319-71273-4_47,,core
144110905,2017-01-13T00:00:00,"O número de acidentes veiculares têm aumentado mundialmente e a principal causa associada a estes acidentes é a falha humana. O desenvolvimento de veículos autônomos é uma área que ganhou destaque em vários grupos de pesquisa do mundo, e um dos principais objetivos é proporcionar um meio de evitar estes acidentes. Os sistemas de navegação utilizados nestes veículos precisam ser extremamente confiáveis e robustos o que exige o desenvolvimento de soluções específicas para solucionar o problema. Devido ao baixo custo e a riqueza de informações, um dos sensores mais utilizados para executar navegação autônoma (e nos sistemas de auxílio ao motorista) são as câmeras. Informações sobre o ambiente são extraídas por meio do processamento das imagens obtidas pela câmera, e em seguida são utilizadas pelo sistema de navegação. O objetivo principal desta tese consiste do projeto, implementação, teste e otimização de um comitê de Redes Neurais Artificiais utilizadas em Sistemas de Visão Computacional para Veículos Autônomos (considerando em específico o modelo proposto e desenvolvido no Laboratório de Robótica Móvel (LRM)), em hardware, buscando acelerar seu tempo de execução, para utilização como classificadores de imagens nos veículos autônomos desenvolvidos pelo grupo de pesquisa do LRM. Dentre as contribuições deste trabalho, as principais são: um hardware configurado em um FPGA que executa a propagação do sinal em um comitê de redes neurais artificiais de forma rápida com baixo consumo de energia, comparado a um computador de propósito geral; resultados práticos avaliando precisão, consumo de hardware e temporização da estrutura para a classe de aplicações em questão que utiliza a representação de ponto-fixo; um gerador automático de look-up tables utilizadas para substituir o cálculo exato de funções de ativação em redes MLP; um co-projeto de hardware/software que obteve resultados relevantes para implementação do algoritmo de treinamento Backpropagation e, considerando todos os resultados, uma estrutura que permite uma grande diversidade de trabalhos futuros de hardware para robótica por implementar um sistema de processamento de imagens em hardware.The number of vehicular accidents have increased worldwide and the leading associated cause is the human failure. Autonomous vehicles design is gathering attention throughout the world in industry and universities. Several research groups in the world are designing autonomous vehicles or driving assistance systems with the main goal of providing means to avoid these accidents. Autonomous vehicles navigation systems need to be reliable with real-time performance which requires the design of specific solutions to solve the problem. Due to the low cost and high amount of collected information, one of the most used sensors to perform autonomous navigation (and driving assistance systems) are the cameras.Information from the environment is extracted through obtained images and then used by navigation systems. The main goal of this thesis is the design, implementation, testing and optimization of an Artificial Neural Network ensemble used in an autonomous vehicle navigation system (considering the navigation system proposed and designed in Mobile Robotics Lab (LRM)) in hardware, in order to increase its capabilites, to be used as image classifiers for robot visual navigation. The main contributions of this work are: a reconfigurable hardware that performs a fast signal propagation in a neural network ensemble consuming less energy when compared to a general purpose computer, due to the nature of the hardware device; practical results on the tradeoff between precision, hardware consumption and timing for the class of applications in question using the fixed-point representation; a automatic generator of look-up tables widely used in hardware neural networks to replace the exact calculation of activation functions; a hardware/software co-design that achieve significant results for backpropagation training algorithm implementation, and considering all presented results, a structure which allows a considerable number of future works on hardware image processing for robotics applications by implementing a functional image processing hardware system",Reconfigurable hardware system for autonomous vehicles visual navigation,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.55.2017.tde-13012017-164142,,core
146472430,2017-12-17T00:00:00,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals",Railway Track Specific Traffic Signal Selection Using Deep Learning,http://arxiv.org/abs/1712.06107,,,,core
93622113,2017-01-01T00:00:00,"The work presents new approaches to Machine Learning for Cyber Physical Systems, experiences and visions. It contains some selected papers from the international Conference ML4CPS – Machine Learning for Cyber Physical Systems, which was held in Karlsruhe, September 29th, 2016. Cyber Physical Systems are characterized by their ability to adapt and to learn: They analyze their environment and, based on observations, they learn patterns, correlations and predictive models. Typical applications are condition monitoring, predictive maintenance, image processing and diagnosis. Machine Learning is the key technology for these developments. The Editors Prof. Dr.-Ing. Jürgen Beyerer is Professor at the Department for Interactive Real-Time Systems at the Karlsruhe Institute of Technology. In addition he manages the Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB. Prof. Dr. Oliver Niggemann is Professor for Embedded Software Engineering. His research interests are in the field of Distributed Real-time Software and in the fields of analysis and diagnosis of distributed systems. He is a board member of the inIT and a senior researcher at the Fraunhofer Application Center Industrial Automation INA located in Lemgo. Dr. Christian Kühnert is a senior researcher at the Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB. His research interests are in the field of machine-learning, data-fusion and data-driven condition monitoring. ",International Conference ML4CPS 2016,,'Springer Science and Business Media LLC',10.1007/978-3-662-53806-7,,core
228125424,2017-01-01T00:00:00,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases",IEEE Access Special Section Editorial: Health Informatics for the Developing World,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2017.2783118,,core
200341287,2017-01-01T08:00:00Z,"The U.S. Securities and Exchange Commission (SEC) maintains a publicly-accessible database of all required filings of all publicly traded companies. Known as EDGAR (Electronic Data Gathering, Analysis, and Retrieval), this database contains documents ranging from annual reports of major companies to personal disclosures of senior managers. However, the common user and particularly the retail investor are overwhelmed by the deluge of information, not empowered. EDGAR as it currently functions entrenches the information asymmetry between these retail investors and the large financial institutions with which they often trade. With substantial research staffs and budgets coupled to an industry standard of “playing both sides” of a transaction, these investors “in the know” lead price fluctuations while others must follow.
In general, this thesis applies recent technological advancements to the development of software tools that will derive valuable insights from EDGAR documents in an efficient time period. While numerous such commercial products currently exist, all come with significant price tags and many still rely on significant human involvement in deriving such insights. Recent years, however, have seen an explosion in the fields of Machine Learning (ML) and Natural Language Processing (NLP), which show promise in automating many of these functions with greater efficiency. ML aims to develop software which learns parameters from large datasets as opposed to traditional software which merely applies a programmer’s logic. NLP aims to read, understand, and generate language naturally, an area where recent ML advancements have proven particularly adept.
Specifically, this thesis serves as an exploratory study in applying recent advancements in ML and NLP to the vast range of documents contained in the EDGAR database. While algorithms will likely never replace the hordes of research analysts that now saturate securities markets nor the advantages that accrue to large and diverse trading desks, they do hold the potential to provide small yet significant insights at little cost.
This study first examines methods for document acquisition from EDGAR with a focus on a baseline efficiency sufficient for the real-time trading needs of market participants. Next, it applies recent advancements in ML and NLP, specifically recurrent neural networks, to the task of standardizing financial statements across different filers. Finally, the conclusion contextualizes these findings in an environment of continued technological and commercial evolution",Realizing EDGAR: eliminating information asymmetries through artificial intelligence analysis of SEC filings,,UNI ScholarWorks,,,core
129143556,2017-03-29T19:00:00,"This presentation reports on high school students’ deep learning of fundamental concepts in programming during a pilot summer program organized to promote construction-related careers, especially construction technology careers, to high school students. The program introduced concepts from programming, virtual reality, and construction disciplines to students who had little to no experience in these fields. The two-week long program utilized a project-based learning approach to increase relevance and deep learning, in addition to exposing students to learning concepts using hands-on experiences. The program brought about interest in taking an interdisciplinary approach to solving problems in the field of construction. The project succeeded in informing and enticing the students of the possibilities of using technology to solving problems in other fields. Completion of a project was the focus of the program, and required the students to design and construct a building in a virtual environment using a programming language. To construct this building as specified in the project requirements, the students had to learn programming, use proficiently a virtual reality software, utilize their spatial visualization prior knowledge, and collaborate with each other. The objective of the program was to entice a new generation of tech-savvy students towards construction. By adding more technologically fluent minds in the field of construction, these minds will eventually expedite technological adaptations in the construction industry",A University-Based Summer Camp to Promote Construction Technology Career for High School Students,https://core.ac.uk/download/129143556.pdf,FIU Digital Commons,,,core
86419694,2017-08-09T00:00:00,"The size of a software artifact influences the software quality and impacts
the development process. In industry, when software size exceeds certain
thresholds, memory errors accumulate and development tools might not be able to
cope anymore, resulting in a lengthy program start up times, failing builds, or
memory problems at unpredictable times. Thus, foreseeing critical growth in
software modules meets a high demand in industrial practice. Predicting the
time when the size grows to the level where maintenance is needed prevents
unexpected efforts and helps to spot problematic artifacts before they become
critical.
  Although the amount of prediction approaches in literature is vast, it is
unclear how well they fit with prerequisites and expectations from practice. In
this paper, we perform an industrial case study at an automotive manufacturer
to explore applicability and usability of prediction approaches in practice. In
a first step, we collect the most relevant prediction approaches from
literature, including both, approaches using statistics and machine learning.
Furthermore, we elicit expectations towards predictions from practitioners
using a survey and stakeholder workshops. At the same time, we measure software
size of 48 software artifacts by mining four years of revision history,
resulting in 4,547 data points. In the last step, we assess the applicability
of state-of-the-art prediction approaches using the collected data by
systematically analyzing how well they fulfill the practitioners' expectations.
  Our main contribution is a comparison of commonly used prediction approaches
in a real world industrial setting while considering stakeholder expectations.
We show that the approaches provide significantly different results regarding
prediction accuracy and that the statistical approaches fit our data best","Predicting and Evaluating Software Model Growth in the Automotive
  Industry",http://arxiv.org/abs/1708.02884,,,,core
84966421,2017-01-01T00:00:00,"International audienceWood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (MC) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting MC from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the MC of wood. The relationship between the reflection coefficient measurements and the MC is studied. Based upon this relationship, MC predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible",Wood moisture content prediction using feature selection techniques and a kernel method,,'Elsevier BV',10.1016/j.neucom.2016.09.005,,core
220150267,2017-01-01T08:00:00,"Synchronous machines are useful in small and large applications such as compressors, fans, pumps, power generators, and many other applications where precise speed control is necessary. Modern brushless excitation synchronous motors utilize electronic components to start the motor as well as lock the rotor into a synchronous speed. The electrical component of a brushless exciter contains a full-wave rectifier, which may malfunction due to a short circuit or open circuit failure of a diode. These failures are not usually detected unless the machine is inspected by a certified technician offline. The best way to prevent major damage to a machine is the have frequent maintenance, which would consist of shutting down the motor and inspecting the electronic components. Having vital machinery shutdowns routinely can reduce production time and is a reason that frequent maintenance is not a common practice in industry. The purpose of this thesis is to extend the work done by previous students in designing and testing the feasibility of using artificial neural networks as a method for detecting faults occurring within the brushless exciter’s diode rectifier of a synchronous motor. The main objectives that this thesis aims to achieve are to create a real-time working fault detection system on a Simulink platform allowing for compilation of the fault detection system to a variety of hardware platforms including microcontrollers. Secondly, the use of ANSYS Maxwell/Simplorer software configured with specific synchronous machine and mechanical load parameters will be investigated as a means to generate and provide the data needed to train an artificial neural network to detect the rectifier failure modes. This would eliminate the current need to first obtain experimental data from the synchronous machine prior to neural network training and ultimate installation of a real-time system on the motor",Fault Detection of Brushless Exciter within Synchronous Motors,,'Purdue University (bepress)',,,core
153504321,2017-01-01T00:00:00,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0Integrated Process Control based on Distributed In-Situ Sensors into Raw Material and Energy Feedstock, DISIR",Cloud computing for big data analytics in the Process Control Industry,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/MED.2017.7984310,,core
226726445,2018,"The reliability estimation of engineered components is fundamental for many optimization policies in a production process. The main goal of this paper is to study how machine learning models can fit this reliability estimation function in comparison with traditional approaches (e.g., Weibull distribution). We use a supervised machine learning approach to predict this reliability in 19 industrial components obtained from real industries. Particularly, four diverse machine learning approaches are implemented: artificial neural networks, support vector machines, random forest, and soft computing methods. We evaluate if there is one approach that outperforms the others when predicting the reliability of all the components, analyze if machine learning models improve their performance in the presence of censored data, and finally, understand the performance impact when the number of available inputs changes. Our experimental results show the high ability of machine learning to predict the component reliability and particularly, random forest, which generally obtains high accuracy and the best results for all the cases. Experimentation confirms that all the models improve their performance when considering censored data. Finally, we show how machine learning models obtain better prediction results with respect to traditional methods when increasing the size of the time-to-failure datasets",On the use of machine learning methods to predict component reliability from data-driven industrial case studies,,,10.1007/s00170-017-1039-x,,core
144120132,2017-09-13T00:00:00,"Software testing is one of the most important Software Engineering processes, being the primary activity to check the conformance between the software requirements and its actual behavior. The automation of software testing activities is essential to certify productivity and effectiveness in such activities. Test automation leads testing activities to be conducted under systematic and accurate criteria, raising the chance of testers to reveal faults or inconsistencies. Test oracles are elementary members in software testing automation, being the mechanism responsible for indicating the correctness of software outputs. In testing environments, test oracles can be effectively implemented based on several sources of information about the Software Under Testing (SUT): software specifications, assertions, formal methods (Finite State Machines (FSM), formal specifications, etc, machine-learning methods, and metamorphic relations. Regardless of the implementation strategy, test oracles are vulnerable to false positive/negative verdicts, configuring what the literature describes as the oracle problem. Therefore, test oracles are a non-trivial and challenging object of studies of the software engineering research area. SUTs outputs in unusual formats make it harder the oracle problem. Audio, images, three-dimensional objects, virtual reality environments, complex statistical compositions, etc, are examples of non-trivial output formats. In the software testing context, SUTs with unusual outputs can be called complex-output systems. In this doctorate dissertation, we propose and evaluate a novel test oracle approach for complex-output systems called feature-based test oracles. The purpose of feature-based test oracles is the appropriation of a processing image technique called Content-Based Image Retrieval (CBIR) to collect information from features extracted from the SUTs outputs to compose test oracles. Given a query image, CBIR combines feature extraction and similarity functions to alleviate the problem of searching for digital images in large databases. In previous research, we have integrated CBIR concepts in a testing framework to support the automation of testing activities in processing image systems and systems with Graphical User Interfaces (GUI). In this doctorate dissertation, we extended that framework and its concepts to general complex-output systems, addressing the feature-based test oracle approach. We use Text-To-Speech (TTS) systems to validate empirically our test oracle technique. Through the results of five empirical analyses, three of them conducted in line with problems of a real-world industry TTS system, show the proposed technique is a valuable instrument to automate testing activities and alleviate practitioners efforts on testing complex output systems. We conclude the proposed test oracles are effective because they systematically evaluate the SUTs sensorial output rather than produce verdicts based on subjective specifications. As future work, we plan to conduct investigations towards the reduction of false positives/negatives and the association of the test oracles with machine learning techniques and metamorphic relations.Teste de Software é um dos processos mais importantes da Engenharia de Software, sendo a principal atividade para averiguar a conformidade de requisitos de software e suas saídas. A automatização das atividades de teste é essencial para conferir produtividade e efetividade em tais atividades. A automatização faz com que atividades de teste sejam conduzidas sob critérios sistemáticos e precisos, aumentando a chance dos testadores de revelarem falhas ou inconcistências. Oráculos de teste são membros elementares na automatização do teste de software, sendo o mecanismo responsável por indicar a corretude das saídas do softwre. Em ambientes de teste, oráculos de teste podem ser efetivamente implementados com base em diversos fontes de informação sobre o sistema em teste: especificações de software, assertivas, métodos formais (máquinas de estados finitas, especificações formais, etc), métodos de aprendizagem de máquina e relações metamórficas. Independente da estratégia de implementação, oráculos de teste são vulneráveis a veridictos de falsos positivos/negativos, configurando o que é apresentado na literatura como O problema do Oráculo. Então, na área de engenharia de software, oráculos de teste são objetos de estudo não-triviais e desafiadores. O problema de oráculo é potencializado quando as saídas do sistema em teste são dadas em formatos não triviais como, por exemplo, audio, imagens, objetos tridimensionais, ambientes de realidade virtual, composições estatísticas complexas, etc. No contexto do teste de software, sistemas com saídas não triviais podem ser chamados de sistemas com saídas complexas. Esta tese de doutorado propões e avalia uma nova estratégia de oráculo de teste para sistemas com saídas complexas. O propósito de tal estratégia é a apropriação da técnica de processamento de imagem conhecida como CBIR (Recuperação de Imagem Basead em Conteúdo  CBIR) para coletar informações de características extratídas do sistema em teste, compondo oráculos de teste. A partir de uma imagem de busca, o CBIR combina extração de características e funções de similaridade para aliviar problemas de busca em grandes based de imagens digitais. Em pesquisas anteriores, conceitos de CBIR foram integrados em um arcabouço de teste para apoiar a automatização de atividades de teste em systemas de processamento de imagens e sistemas com interfaces gráficas. Esta tese de doutorado estende o arcabouço e seus conceitos para sistemas com saídas complexas em geral. Sistemas Texto-Fala (TTS) foram utlizados para validações empíricas. Os resultados de seis análises empíricas, duas delas condizidas em consonância com problemas de um TTS industrial, revelam que a técnica proposta é um valioso instrumento para automatizar atividaes de teste e aliviar esforços de profissionais da indústria ao teste sistemas com saídas complexas. Conclui-se que a efetividade dos oráculos de teste propostos são devido às sistemáticas análises do conteúdo das saídas dos sistemas em teste, em vez da análises de especificações subjetivas. Os trabalhos futuros vislumbrados devem ser conduzidos no intuito de reduzir número de falsos positivos/negativos e a associação dos oráculos de teste com técnicas de aprendizado de máquina e relações metamórficas",Oracles de teste para sistemas com saídas complexas - o caso dos sistemas TTS,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.55.2017.tde-13092017-085208,,core
296894670,2018-09-03T22:34:18,"Orientadores: João Mauricio Rosario, Oscar Fernando AvilésTese (doutorado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: Esta tese contribui com a parametrização e caracterização dos sinais de eletromiografia de superfície usando a aprendizagem em profundidade (Deep Learning) como técnica avançada no reconhecimento de padrões para reproduzir os movimentos de preensão de uma mão robótica em ambientes industriais através da interação homem-máquina. A análise para reproduzir os movimentos da mão é realizada a partir da interação dos sinais de eletromiografia das ações, que geram uma resposta cognitiva com o objetivo de replica-lo para que o dispositivo robótico possa realizar o movimento de preensão de acordo com o movimento realizado pelo usuário. Nesta tese, parte-se da bancada experimental MUC-1 previamente desenvolvida no Laboratório de Automação Integrada e Robótica (LAIR) da Universidade Estadual de Campinas e acrescentam-se funções que aumentam o escopo e melhoram a exequibilidade dos testes. A técnica de obtenção dos valores experimentais dos dados é baseada na adaptação do sensor MYO armband® por meio dos oito bio-sensores de eletromiografia relacionando a cinemática e dinâmica da mão pela identificação dos músculos do braço correspondente aos métodos de preensão, os quais são aprimorados por médio do método baseado em redes neuronais convolucionais da aprendizagem em profundidade previamente investigado na literatura para o reconhecimento de padrões. Para validação do sistema proposto, foi construído três arquiteturas de redes convolucionais, viabilizando a execução do teste virtual por meio da mão implementada no Simmechanics de Matlab® e no modelo real MUC-1. Por fim, o procedimento experimental resultante é documentado e as etapas prévias de modelagem e filtragem são descritas de acordo com as condições de preensão de objetos de figuras geométricas preestabelecidas que são executadas no dispositivo robótico de forma naturalAbstract: This thesis contributes to the parametrization and characterization of surface electromyography signals using deep learning as an advanced technique in pattern recognition to reproduce the grip movements of a robotic hand in industrial environments through the man-machine interaction. The analysis to reproduce the movements of the hand is made from the interaction of the electromyography signals of the actions, which generate a cognitive response to replicate it so that the robotic device can perform the grip movement, in accordance with the movement made by the user. Some part of this thesis is experimental bench MUC-1 previously developed in the Laboratory of Automation and Robotics (LAIR) at the State University of Campinas and added functions that increase the scope and improve the feasibility of testing. The technique of obtaining the experimental values of the data is based on the adaptation of the MYO armband® sensor through the eight bio-sensors of electromyography relating the kinematics and dynamics of the hand by the identification of the muscles of the arm corresponding to the grasping methods, which are improved by the method based on convolutional neuronal networks of in-depth learning previously research in the literature for the recognition of patterns. For the validation of the proposed system, three convolutional network architectures were built, enabling the virtual test execution through the hand implemented in the Matlab® Simmechanics and in the real MUC-1 model. Finally, the resulting experimental process is documented and the previous stages of modeling and filtering are described according to the prehension conditions of objects of geometric figures that are executed in the robotic device in a natural wayDoutoradoMecatrônicaDoutor em Engenharia Mecânic","Movement Identification from the intention of grasping, based on deep learning, with signals EMGs for use as HMI in robotic devices",https://core.ac.uk/download/296894670.pdf,[s.n.],,,core
295379862,2017-07-14,"A perfilagem de poços é uma ferramenta essencial para a indústria de petróleo. Em poucas palavras, pode ser resumida como a caracterização de propriedades petrofísicas e geológicas representadas graficamente pela associação com a profundidade do poço. É através da análise dos perfis que é possível identificar os tipos de rochas, a localização de hidrocarbonetos, pode-se estimar a viabilidade do poço e permite o reaproveitamento de poços já explorados.Esse trabalho aponta uma das medidas alternativas utilizadas hoje em dia para otimizar o processo de interpretação das respostas da perfilagem: o software Interactive Petrophysics que conta com a metodologia de redes neurais artificiais utilizando um algoritmo interpretativo usado para treinar a rede e simular uma saída esperada. As redes neurais artificiais possuem a capacidade de associar as informações de entrada e ponderá-las através de pesos no processo de aprendizagem. A metodologia do processo consistiu em utilizar informações petrofísicas reais provindas de perfis geofísicos e também dados de análise de testemunho fornecidos pela ANP (Agência Nacional de Petróleo, Gás Natural e Bicombustíveis) de três poços produtores pertencentes ao campo de Namorado (campo escola), os mais próximos possíveis, e assim tais informações foram usadas como os dados de entrada e para treinamento da rede para estimar a saída esperada, que, para esse caso, foi a permeabilidade dos poços. E, no final do trabalho, foi feita uma análise comparativa com os dados reais obtidos pela análise de testemunho com a saída do simulador.Well logging is an essential tool for the oil industry. In a few words, it can be described as the characterization of petrophysics and geological properties registered in association with the depth of the well. It is through the analysis of the logs that it is possible to determinate the type of rocks, the hydrocarbons localization, it can be estimated the viability of the well and the return of wells already explored. The well logging activity occurs during the exploration phase of the well. In this study, a brief presentation of the qualitative and quantitative properties of the logs will be carried out, as well as their importance for the characterization of the wells; on top of that the main logs used in the study of the case of the Namorado Field will be presented. This paper shows one of the alternative measures to accelerate the well logging process: The Interactive Petrophysics software presents an artificial neural network logic that uses an interpretative algorithm to train and simulate an expected output. The artificial neural networks can associate the input data and weight them through the learning process. This complex logic will also be presented in this paper so it can be better understood. The methodology of the process consisted in taking the input data, the logs and the core analysis, provided by ANP (Nacional Petroleum Agency) of three wells specifically selected, and then train this input data to provide the output permeability. At the end of this study it was made a comparative analysis with the real data and the one that came out of the process",Aplicação de redes neurais artificiais na estimativa da permeabilidade usando perfis de poços do Campo de Namorado,,Niterói,,,core
144733647,2017-02-03,"Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.

Reference: Chen, Ning, Steven CH Hoi, and Xiaokui Xiao. ""Software process evaluation: A machine learning approach."" Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering. IEEE Computer Society, 2011",Defect Prediction: SPE,,,10.5281/zenodo.268487,,core
359064633,2018-06-01T00:00:00,"[Excerpt] The ideas and uses for Artificial Intelligence (AI) are abundant, and each business is seemingly ripe for disruption, including HR. As the hype surrounding AI continues to be championed by popular press, we began our research in order to determine whether the press’ biased view that AI was here and ready to implement was accurate. We found that in reality, AI programs were far behind the progress discussed, as the software was slower, more expensive, and there was a general lack of amalgamation throughout the industry. From there, we asked CAHRS partners to tell us where AI was used in their company, and how it helped them deliver HR differently. Our research focused on how AI technology will disrupt, change, or bolster the HR function, specifically in Talent Acquisition and Learning and Development (L&D) spaces. We found our CAHRS partners dove into AI, and represented three key points along a spectrum of AI implementation. Of the 59 participants at 32 companies, 26% are Observers, 48% are Explorers, and 26% are Implementers. Observers were companies that did not believe AI fits with their strategy, and therefore do not intend to implement AI right now. Explorers are companies that have begun to actively explore AI through industry research, vendor exploration, and piloting AI and machine learning (ML) technologies. Implementers are companies that have either built in house or worked with an external vendor to implement an AI or machine learning technology. The CAHRS partners represented such a wide range along this spectrum because there are no best practices for AI implementation. However, each of our partners that leveraged AI understood the tool, while also understanding their business needs, people, and technology, which allowed them to utilize AI technology.White_Paper_2018.pdf: 436 downloads, before Oct. 1, 2020",CAHRS Partners' Implementation of Artificial Intelligence,https://hdl.handle.net/1813/73724,,,,core
199298351,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",Digitalization of the power business: How to make this work?,,,,,core
237178114,2018-07-12T00:00:00,"Illustration as a mode of cultural production is an exponentially expanding phenomenon. Illustration students form a significant and substantial constituency within the Art and Design education sector. The future of Illustration disciplinarians is assured, or so it may seem. It is conceivable that the volume of Illustration graduates entering the ‘job (less) market’ exceeds real demand for conventional, transactional, freelance, commercial Illustration work. This constitutes an existential threat for the discipline and increased precarity for Illustration graduates. Add to this the imminence of wide scale Automation (Chui et al), the deployment of Artificial Intelligence in to the creative sector and the normalization of Self-Entrepreneurship (Deresiewicz) and we have a potent and destabilizing ‘mix’ that poses serious ethical and operational questions to educators, institutions and the industry at large",>doom jolt-ante,http://ualresearchonline.arts.ac.uk/15048/3/draft%232_DC_draftDOOMJOLT_ANTEV2.pdf,,,,core
151040580,2017-01-01T00:00:00,"Nowadays real-time industrial applications are generating a huge amount of data continuously every day. To process these large data streams, we need fast and efficient methodologies and systems. A useful feature desired for data scientists and analysts is to have easy to visualize and understand machine learning models. Decision trees are preferred in many real-time applications for this reason, and also, because combined in an ensemble, they are one of the most powerful methods in machine learning.
In this paper, we present a new system called STREAMDM-C++, that implements decision trees for data streams in C++, and that has been used extensively at Huawei. Streaming decision trees adapt to changes on streams, a huge advantage since standard decision trees are built using a snapshot of data, and can not evolve over time. STREAMDM-C++ is easy to extend, and contains more powerful ensemble methods, and a more efficient and easy to use adaptive decision trees. We compare our new implementation with VFML, the current state of the art implementation in C, and show how our new system outperforms VFML in speed using less resources",Extremely fast decision tree mining for evolving data streams,https://core.ac.uk/download/151040580.pdf,'Association for Computing Machinery (ACM)',10.1145/3097983.3098139,,core
162306906,2018-01-01T00:00:00,"This thesis presents a novel synthetic environment for supporting advanced explorations of user interfaces and interaction modalities for future transport systems. The main goal of the work is the definition of novel interfaces solutions designed for increasing trust in self-driving vehicles. The basic idea is to provide insights to the passengers concerning the information available to the Artificial Intelligence (AI) modules on-board of the car, including the driving behaviour of the vehicle and its decision making. Most of currently existing academic and industrial testbeds and vehicular simulators are designed to reproduce with high fidelity the ergonomic aspects associated with the driving experience. However, they have very low degrees of realism for what concerns the digital components of the various traffic scenarios. These includes the visuals of the driving simulator and the behaviours of both other vehicles on the road and pedestrians.  High visual testbed fidelity becomes an important pre-requisite for supporting the design and evaluation of future on-board interfaces. An innovative experimental testbed based on the hyper-realistic video game GTA V, has been developed to satisfy this need. To showcase its experimental flexibility, a set of selected user studies, presenting novel self-driving interfaces and associated user experience results, are described. These explore the capabilities of inducing trust in autonomous vehicles and explore Heads-Up Diplays (HUDs), Augmented Reality (ARs) and directional audio solutions. The work includes three core phases focusing on the development of software for the testbed, the definition of relevant interfaces and experiments and focused testing with panels comprising different user demographics. Specific investigations will focus on the design and exploration of a set of alternative visual feedback mechanisms (adopting AR visualizations) to gather information about the surrounding environment and AI decision making. The performances of these will be assessed with real users in respect of their capability to foster trust in the vehicle and on the level of understandability of the provided signals. Moreover, additional accessory studies will focus on the exploration of different designs for triggering driving handover, i.e. the transfer vehicle control from AI to human drivers, which is a central problem in current embodiments of self-driving vehicles.QC 20181010</p",Novel synthetic environment to design and validate future onboard interfaces for self-driving vehicles,,,,,core
34646243,2015,"What You Will Learn

* Select and configure camera systems to see invisible light, fast motion, and distant objects
* Build a “camera trap”, as used by nature photographers, and process photos to create beautiful effects
* Develop a facial expression recognition system with various feature extraction techniques and machine learning methods
* Build a panorama Android application using the OpenCV stitching module in C++ with NDK support
* Optimize your object detection model, make it rotation invariant, and apply scene-specific constraints to make it faster and more robust
* Create a person identification and registration system based on biometric properties of that person, such as their fingerprint, iris, and face
* Fuse data from videos and gyroscopes to stabilize videos shot from your mobile phone and create hyperlapse style videosComputer vision is becoming accessible to a large audience of software developers who can leverage mature libraries such as OpenCV. However, as they move beyond their first experiments in computer vision, developers may struggle to ensure that their solutions are sufficiently well optimized, well trained, robust, and adaptive in real-world conditions. With sufficient knowledge of OpenCV, these developers will have enough confidence to go about creating projects in the field of computer vision.

This book will help you tackle increasingly challenging computer vision problems that you may face in your careers. It makes use of OpenCV 3 to work around some interesting projects. Inside these pages, you will find practical and innovative approaches that are battle-tested in the authors’ industry experience and research. Each chapter covers the theory and practice of multiple complementary approaches so that you will be able to choose wisely in your future projects. You will also gain insights into the architecture and algorithms that underpin OpenCV’s functionality.

We begin by taking a critical look at inputs in order to decide which kinds of light, cameras, lenses, and image formats are best suited to a given purpose. We proceed to consider the finer aspects of computational photography as we build an automated camera to assist nature photographers. You will gain a deep understanding of some of the most widely applicable and reliable techniques in object detection, feature selection, tracking, and even biometric recognition. We will also build Android projects in which we explore the complexities of camera motion: first in panoramic image stitching and then in video stabilization.

By the end of the book, you will have a much richer understanding of imaging, motion, machine learning, and the architecture of computer vision libraries and applications!edition: 1st editionnrpages: 382status: publishe",Birmingham,OpenCV 3 Blueprints: Expand your knowledge of computer vision by building amazing projects with OpenCV 3,,,,core
296622026,2015-11-26T14:15:06Z,"The authors present a new approach using genetic algorithms, neural networks and nanorobotics concepts applied to the problem of control design for nanoassembly automation and its application in medicine. As a practical approach to validate the proposed design, we have elaborated and simulated a virtual environment focused on control automation for nanorobotics teams that exhibit collective behavior. This collective behavior is a suitable way to perform a large range of tasks and positional assembly manipulation in a complex 3D workspace. We emphasize the application of such techniques as a feasible approach for the investigation of nanorobotics system design in nanomedicine. Theoretical and practical analyses of control modelling is one important aspect that will enable rapid development in the emerging field of nanotechnology.2:00 AM95104Adleman, L.M., On constructing a molecular computer (1995) DNA Based Computers, , http://olymp.wuwien.ac.at/usr/ai/frisch/local.htmlBachand, G.D., Montemagno, C.D., Constructing organic/inorganic NEMS devices powered by biomolecular motors (2000) Biomedical Microdevices, 2, pp. 179-184Baraff, D., (1992) Dynamic Simulation of Non-penetrating Rigid Bodies, , PhD Thesis, Department of Computer Science, Cornell University, Ithaca, NYBerg, H.C., Dynamic properties of bacterial flagellar motors (1974) Nature, 249, pp. 77-79. , 3 MayBojinov, H., Casai, A., Hogg, T., Multiagent control of modular self-reconfigurable robots (2002) Artificial Intelligence, 142, pp. 99-120Brooks, R., (1992) Artificial Life and Real Robots, , MIT pressBrutzman, D.P., Kanayama, Y., Zyda, M.J., Integrated simulation for rapid development of autonomous underwater vehicles (1992) IEEE Autonomous Underwater Vehicle Conference, pp. 3-10. , IEEE Oceanic Engineering Society, Washington DC, JuneCasai, A., Hogg, T., Cavalcanti, A., Nanorobots as cellular assistants in inflammatory responses (2003) IEEE BCATS Biomedical Computation at Stanford 2003 Symposium, , IEEE Computer Society, Stanford CA, OctoberCavalcanti, A., Assembly automation with evolutionary nanorobots and sensor-based control applied to nanomedicine (2003) IEEE Transactions on Nanotechnology, 2 (2), pp. 82-87. , www.nanorobotdesign.com, JuneCavalcanti, A., Freitas Jr., R.A., Autonomous multi-robot sensor-based cooperation for nanomedicine (2002) Int'l J. Nonlinear Science Numerical Simulation, 3 (4), pp. 743-746. , www.rfreitas.com/NanoPubls.htm, AugustChen, W., Lewis, K., A robust design approach for achieving flexibility in multidisciplinary design (1999) AIAA Jounal, 37 (8), pp. 982-990Downing, H.A., Jeanne, R.L., Nest construction by the paperwasp, Plistes: A test of stigmergy theory (1988) Animal Behavior, 36, pp. 1729-1739Drexler, K.E., Forrest, D., Freitas Jr., R.A., Hall, J.S., Jacobstein, N., McKendree, T., Merkle, R., Peterson, C., (2001) A Debate about Assemblers, , www.imm.org/SciAmDebate2/whitesides.html, Institute for Molecular ManufacturingDrexler, K.E., (1992) Nanosystems: Molecular Machinery, Manufacturing, and Computation, , John Wiley & SonsDrucker, H.D., Wu, D., Vapnik, V., Support vector machines for spam categorization IEEE Transactions on Neural Networks, 10 (5), pp. 1048-1054Fishbine, G., (2001) The Investor's Guide to Nanotechnology & Micromachines, , John Wiley & SonsFreitas Jr., R.A., Phoenix, C.J., Vasculoid: A personal nanomedical appliance to replace human blood (2002) J. Evol. Technol., 11. , www.jetpress.org/volume11/vasculoid.htmlFreitas Jr., R.A., Nanomedicine (1999) Vol. I: Basic Capabilities, 1. , Landes BioscienceNanomedicine (2003) IIA: Biocompatibility, , www.nanomedicine.com, Landes BioscienceFukuda, T., Arai, T., Prototyping design and automation of micro/nano manipulation system (2000) Proc. of IEEE Int 'l Conf. on Robotics and Automation (ICRA '00), pp. 192-197Geppert, L., The amazing vanishing transistor act (2002) IEEE Spectrum Magazine, pp. 28-33. , Cover story, OctoberGrefenstette, J.J., Schultz, A., An evolutionary approach to learning in robots (1994) Machine Learning Workshop on Robot Learning, , www.citidel.org, New Brunswick, NJGrzeszczuk, R., Terzopoulos, D., Hinton, G., NeuroAnimator: Fast neural network emulation and control of physics-based models (1998) Proc. of ACM SIGGRAPH 98 Conf., pp. 142-148. , M. Cohen, edGuthold, M., Controlled manipulation of molecular samples with the nano-manipulator (2000) IEEE/ASME Transactions on Mechatronics, 5 (2), pp. 189-198Hagan, M.T., Demuth, H.B., Jesús, O.D., An introduction to the use of neural networks in control systems (2002) International Journal of Robust and Nonlinear Control, 12 (11), pp. 959-985. , John Wiley & Sons, SeptemberHagiya, M., From molecular computing to molecular programming (2000) Proc. 6th DIMACS Workshop on DNA Based Computers, pp. 198-204. , Leiden, NetherlandsHaykin, S., (1999) ""Neural Networks A Comprehensive Foundation"", 2nd Edition, , Prentice Hall, New Jersey, USAHellemans, A., German team creates new type of transistor-like device (2003) IEEE Spectrum Magazine, pp. 20-21. , News Analysis, JanuaryHelmer, G., Wong, J.S.K., Honavar, V., Miller, L., Intelligent agents for intrusion detection (1998) Proceedings, IEEE Information Technology Conference, pp. 121-124. , Syracuse, NY, SeptemberKatz, E., Riklin, A., Heleg-Shabtai, V., Willner, I., Bückmann, A.F., Glucose oxidase electrodes via reconstitution of the apo-enzyme: Tailoring of novel glucose biosensors (1999) Anal. Chim. Acta., 385, pp. 45-58Khatib, M., Bouilly, B., Simeon, T., Chatila, R., Indoor navigation with uncertainty using sensor based motions (1997) Proc. 1997 IEEE Int'l Conf. on Robotics and Automation, pp. 3379-3384. , Albuquerque, New Mexico, USAKretly, L.C., Almeida, A.F.L., Oliveira, R.S., Sasaki, J.M., Sombra, A.S., Electrical and optical properties of CaCu3 Ti4 O12 (CCTO) substrates for microwave devices and antennas (2003) Microwave and Optical Technology Letters, 39 (2), pp. 145-150. , New York, EU A, OctoberKrishnamurthy, B., Rosemblum, D.S., Yeast: A general purpose event-action system (1995) IEEE Transactions on Software Engineering, 21 (10), pp. 845-857. , OctoberKube, C.R., Zhang, H., Task modelling in collective robotics (1997) Autonomous Robots, 4 (1), pp. 53-72Kumar, M.N.V.R., Nano and microparticles as controlled drug delivery devices (2000) J. Pharmacy Parmaceutical Science, 3 (2), pp. 234-258Lehoczky, J., Sha, L., Ding, Y., (1989) The Rate Monotonic Scheduling Algorithm, pp. 166-171. , IEEE Computer Society Press, Santa Monica, California, USA, DecemberLewis, M.A., Bekey, G.A., The behavioral self-organization of nanorobots using local rules (1992) Proc. of IEEE Int'l Conf. on Intelligent Robots and Systems, , Raleigh, NCLyons, K., Wang, Y., An open architecture for virtual reality in nano-scale manipulation, measurement and manufacturing (M3) (2000) 8th Foresight Conference on Molecular Nanotechnology, , USAMakaliwe, J.H., Requicha, A.A.G., Automatic planning of nanoparticle assembly tasks (2001) Proc. IEEE Int'l Symp. on Assembly and Task Planning, pp. 288-293. , Fukuoka, JapanMartel, S., Madden, P., Sosnowski, L., Hunter, I., Lafontaine, S., NanoWalker: A fully autonomous highly integrated miniature robot for nano-scale measurements (1999) Proc. of the European Optical Society and SPIE Int'l Symposium on Envirosense, Microsystems Metrology and Inspection, 3825, pp. 64-76. , Munich, GermanyMatellan, V., Fernandez, C., Molina, J.M., Genetic learning of fuzzy reactive controllers (1998) Robotics and Autonomous Systems, 225 (2), pp. 33-41Menezes, A.J., Kapoor, V.J., Goel, V.K., Cameron, B.D., Lu, J.-Y., Within a nanometer of your Life (2001) Mechanical Engineering Magazine, , www.memagazine.org/backissues/aug01/features/nmeter/nmeter.html, AugustMerkle, R.C., Nanotechnology and medicine (1996) Advances in AntiAging Medicine, 1, pp. 277-286. , Mary Ann Liebert PressMok, A., Chen, D., A multiframe model for real-time tasks (1997) IEEE Transactions on Software Engineering, 23 (10), pp. 635-645Mokhoff, N., (2003) Education Overhaul Urged for Nanotech Revolution, , www.theworkcircuit.com/news/OEG20030206S0026, EE Times, FebruaryMoore, S.K., Just one word - Plastics (2002) IEEE Spectrum Magazine, pp. 55-59. , Special R&D Report, Organic Electronics, SeptemberOkamoto, T., Ishida, Y., A performance analysis of a mobile anti-virus system (2002) Proc. of AROB 02, 1, pp. 132-135www.eng.nsf.gov/sbirPetrie, C.J., (1996) Agent-based Engineering, the Web, and Intelligence, , www-cdr.stanford.edu/NextLink/Expert.html, IEEE Expert, decemberRamia, M., Tullock, D.L., Thien, N.P., The role of hydrodynamic interaction in the locomotion of microorganisms (1993) Biophys. J., 65, pp. 755-778Reppesgaard, L., Nanobiotechnologie: Die Feinmechaniker der Zukunft nutzen Biomaterial als Werkstoff (2002) Computer Zeitung, 36, p. 22. , 2 SeptemberRequicha, A.A.G., Nanorobots, NEMS and nanoassembly (2003) IEEE ICRA International Conference on Robotics and Automation, 91 (11), pp. 1922-1933. , special issue on Nanoelectronics and Nanoprocessing, Taipei, Taiwan, NovemberRequicha, A.A.G., Resch, R., Montoya, N., Koel, B.E., Madhukar, A., Will, P., Towards hierarchical nanoassembly (1999) IEEE/RSJ Int'l Conf. on Intelligent Robots & Systems, , Kyongju, KoreaRietman, E.A., (2001) Molecular Engineering of Nanosystems, , Biological Physics Series, Springer-Verlag, New York, USARoco, M.C., (2003) Government Nanotechnology Funding: An International Outlook, , www.nano.gov/html/res/IntlFundingRoco.htm, National Science Foundation, JuneSeely, T.D., Camazine, S., Sneyd, J., Collective Decision-making in honey bees: How colonies choose among nectar sources (1991) Behavioral Ecology and Sociobiology, 28, pp. 277-290Simon, J., Frank, R.K., McDevitt, M.R., Ma, D., Lai, L.T., Borchardt, P., Wu, K., Bander, N., Tumor therapy with targeted atomic nano-scale generators (2001) Science Magazine, , Nov. 16Sitti, M., Hashimoto, K., Teleoperated nano scale object manipulation (1999) Recent Advances on Mechatronics, pp. 172-178. , Springer Verlag Pub., Ed. O. KaynakStracke, R., Böhm, K.J., Burgold, J., Schacht, H., Unger, E., Physical and technical parameters determining the functioning of a kinesin-based cell-free motor system (2000) Nanotechnology, 11, pp. 52-56. , UKSun, J., Gao, M., Feldmann, J., Electric field directed layer-by-layer assembly of highly fluorescent CdTe nanoparticles (2001) Journal of Nanoscience and Nanotechnology, 1 (2), pp. 21-27. , American Scientific PublishersToth-Fejel, T., Agents, assemblers, and ANTS: Scheduling assembly with market and biological software mechanisms (2000) Nanotechnology, 11, pp. 133-137Voss, S., Meta-heuristics: Advances and trends in local search paradigms for optimization (1998) Meta-Heuristics International Conference, , Kluwer Academic PubWieland III, C.F., Is the US nanotechnology investiment paying off? (2004) Small Times Magazine, , www.burnsdoane.com/pubs/wieland2.pdf, Intellectual Property, January/FebruaryWhitcomb, L.L., Underwater Robotics: Out of the research laboratory and into the Field (2000) IEEE Int'l Conf. on Robotics and Automation, pp. 85-90. , San Francisco, CA, USAWurll, C., Henrich, D., Wörn, H., Parallel on-line motion planning for industrial robots (1998) 3rd ASCE Specialty Conf. on Robotics for Challenging Environments, Robotics, pp. 308-314. , New Mexico, US",,Nanorobotics Control Design: A Practical Approach Tutorial,,,,core
296662226,2015-11-26T15:37:01Z,"The productivity and quality of a continuous caster depend mainly on process parameters, i.e. casting speed, casting temperature, steel composition and cleanliness of the melt, water flow rates in the different cooling zones, etc. This work presents the development of an algorithm, which incorporates heuristic search techniques for direct application in metallurgical industries, particularly those using continuous casting process for the production of steel billets and slabs. This is done to determine the casting objectives of maximum casting rate as a function of casting constraints. These constraints are evaluated with the aid of a heat transfer and solidification model based on the finite difference technique, which has been developed and integrated with a genetic algorithm. The essential parts of continuous casting equipment, which must be subjected to monitoring, as well as a methodology of mathematical model and physical settlements in each cooling region, are presented. The efficiency of the intelligent system is assured by the optimisation of the continuous casting operation by maximum casting rate and defect-free products. This approach is applied to the real dimension of a steel continuous caster, in real conditions of operation, demonstrating that good results can be attained by using heuristic search, such as: smaller temperature gradients between sprays zones, reduction in water consumption and an increase in casting speed. © 2002 Elsevier Science Inc. All rights reserved.261110771092Williamson, C.Q., Process control in continuous casting a trend or must (1988) Continuous Casting, 4, pp. 281-287Irving, W.R., On line quality control for continuously cast semis (1990) Ironmaking and Steelmaking, 17 (3), pp. 197-202Kumar, S., Samarasekera, I.V., Brimacombe, J.K., Mould thermal response and formation of defects in the continuous casting of steel billets-laps and bleeds (1997) Iron and Steelmaker, pp. 53-69Samarasekera, I.V., Brimacombe, J.K., Wilder, K., The pursuit of steel billet quality (1994) Iron and Steelmaker, pp. 53-63Kumar, S., Walker, B.N., Samarasekera, I.V., Brimacombe, J.K., Chaos at the meniscus-the genesis of defects in continuously cast steel billets (1993) 13th PTD Conference Proceeding, pp. 119-141Brimacombe, J.K., Empowerment with knowledge--toward the intelligent mould for the continuous casting of steel billets (1993) Iron and Steelmaker, pp. 35-47Kumar, S., Meech, J.A., Samarasekera, I.V., Brimacombe, J.K., Knowledge engineering an expert systems to troubleshoot quality problems in the continuous casting of steel billets (1993) Iron and Steelmaker, pp. 29-36Larreq, M., Birat, J.P., Optimization of casting and cooling conditions on steel continuous casters--implementation of optimal strategies on slab and bloom casters (1982) Application of Mathematical and Physical Models in the Iron and Steel Industry, Iron and Steel Society of ASMEFilipic, B., Sarler, B., Continuous casting simulator--a tool for improved quality and productivity (1997) Proceedings of the 2nd International Metallurgical Conference Continuous Casting of Billets, pp. 161-168. , Trinec, Czech RepublicFilipic, B., Sarler, B., Evolving parameter setting for continuous casting of steel (1998) Proceedings of the 6th European Congress on Intelligent Techniques and Soft Computing--EUFIT'98, 1, pp. 444-449. , Aachen, Germany, Sept 7-10Lally, B., Biegler, L.T., Henein, H., Optimisation and continuous casting: Part I. Problem formulation and solution strategy (1991) Metallurgical Transactions B, 22 B, pp. 641-648Cheung, N., Garcia, A., The use of a heuristic search technique for the optimization of quality of steel billets produced by continuous casting (2001) Engineering Applications of Artificial Intelligence, 14, pp. 229-238Osman, I.H., Kelly, J.P., Meta-heuristics: An overview (1996) Meta-Heuristics: Theory - Applications, , Kluwer Academic PublishersYagiura, M., Ibaraki, T., Genetic and local search algorithms as robust and simple optimization tools (1996) Meta-Heuristics: Theory - Applications, , Kluwer Academic PublishersRasheed, K., Hirsh, H., Gelsey, A., A genetic algorithm for continuous design space search (1997) Artificial Intelligence in Engineering, 11, pp. 295-305Spim, J.A., Garcia, A., An optimisation of the finite difference method for modeling solidification of complex shapes (1997) Journal of the Brazilian Society of Mechanical Sciences, 19, pp. 392-409Brown, D.E., White, C.C., (1990) Operations Research and Artificial Intelligence: The Integration of Problem-solving Strategies, , Kluwer Academic PublisherApelian, D., Meysel, A., Intelligent processing of materials (1990) The Minerals, pp. 427-434. , H. N. Wadley, & W. E. Eckhart (Eds.), Metals - Materials SocietyBohmer, J.R., Fett, F.N., Modelling of casting welding and advanced solidification process V (1991) The Minerals, p. 337. , M. Rappaz, M. R. Ozgu, & K. Mahin (Eds.), Metals - Materials SocietyMizikar, E.A., Sprays cooling investigation for continuous casting of billets and blooms (1970) Iron and Steel Institute, pp. 53-60Bolle, E., Moureau, J.C., Sprays cooling of hot surfaces: A description of the dispersed phase and a parametric study of heat transfer results (1946) Proceedings of Two Phase Flows and Heat Transfer, 3, pp. 1327-1346. , NATO Advanced Study InstituteKominami, H., Neural network system for breakout prediction in continuous casting process (1991) Nippon Steel Technical Report, 49, pp. 34-38Voller, V.R., Swaminathan, C.R., General source-based method for solidification phase change (1991) Numerical Heat Transfer Part B, 19, p. 175Welty, J.R., (1976) Engineering Heat Transfer, , New York: J. Wiley and Sons In",,The Use Of Artificial Intelligence Technique For The Optimisation Of Process Parameters Used In The Continuous Casting Of Steel,,10.1016/S0307-904X(02)00062-8,,core
23735876,2014-01-15,"This paper assesses the possibility of using Rapid Manufacturing (RM) as a final manufacturing route through a comparison of RM processes capabilities vs. conventional manufacturing processes. This is done by means of a computer-aided system (RMADS) intended to guide the designer in the selection of optimum production parameters according to typical requirements of the first design stages. A number of Artificial Intelligence (AI) tools are applied namely: fuzzy inference, relational databases and rule-based decision making. A pilot application developed in Matlab ® is presented to illustrate a case study for a real mechanical part. Furthermore the proposed system makes use of two different costing approaches: Parametric and Empirical. An empirical cost model has been implemented for those widely studied RM processes whose parameters and their participation in the final cost can be clearly defined. On the other hand Empirical costing has been adapted trough the use of neural networks in order to get rough estimations of part cost for those RM processes whose parameters and their cost implications are not yet defined",,AN EXPERT RULE-BASED SYSTEM FOR ADDITIVE MANUFACTURING SELECTION,,,,core
60693686,2015-06-01T00:00:00,"National audienceIn machine learning, empirical performance on real data are crucial in the success of a method. Recent years have seen the emergence of a large number of machine learning competitions. These challenges are motivated by industrial (Netflix prize) or academic (HiggsML challenge) applications and put in competition researchers and data scientists to obtain the best performance. We wanted to expose students to this reality bysubmitting a challenge in the context of the machine learning course. The leaderboard is displayed on an automatically updated web page allowing emulation among students. The history of the results also allows them to visualize their progress through the submissions. In addition, the challenge can continue outside of the supervised sessions promoting independence and exploration of new learning techniques and computer tools. The system we have implemented is available as an R package for reuse by other teachers. Building on the R Markdown and Dropbox tools, it requires no network configuration and can be deployed very easily on a personal computer.En apprentissage automatique, les performances empiriques obtenues sur données réelles sont déterminantes dans le succès d'une méthode. Ces dernières années ont vu l'apparition d'un grand nombre de compétitions d'apprentissage automatique. Ces challenges sont motivés par des applications industrielles (prix Netflix) ou académiques (challenge HiggsML) et mettent en compétition chercheurs et data scientists pour obtenir les meilleures performances. Nous avons souhaité confronter les étudiants à cette réalité en leur soumettant un challenge dans le cadre du cours d'apprentissage automatique. Leur classement est affiché sur une page web mise à jour automatiquement permettant une émulation parmi les étudiants. L'historique des résultats leur permet également de visualiser leur progression au fil des soumissions. De plus, le challenge peut se poursuivre en dehors des sessions encadrées favorisant l'autonomie et l'exploration de nouvelles techniques d'apprentissage et outils informatiques. Le système que nous avons mis en œuvre est disponible sous forme de package R afin d'être réutilisé par d'autres enseignants. S'appuyant sur les outils R Markdown et Dropbox, il ne nécessite aucune configuration réseau et peut être déployé très facilement sur un ordinateur personnel",HAL CCSD,Compétitions d'apprentissage automatique avec le package R rchallenge,,,,core
87661950,2014-01-01T00:00:00Z,"Artificial intelligence methodologies, as the core of discrete control and decision support systems, have been extensively applied in the industrial production sector. The resulting tools produce excellent results in certain cases; however, the NP-hard nature of many discrete control or decision making problems in the manufacturing area may require unaffordable computational resources, constrained by the limited available time required to obtain a solution. With the purpose of improving the efficiency of a control methodology for discrete systems, based on a simulation-based optimization and the Petri net (PN) model of the real discrete event dynamic system (DEDS), this paper presents a strategy, where a transformation applied to the model allows removing the redundant information to obtain a smaller model containing the same useful information. As a result, faster discrete optimizations can be implemented. This methodology is based on the use of a formalism belonging to the paradigm of the PN for describing DEDS, the disjunctive colored PN. Furthermore, the metaheuristic of genetic algorithms is applied to the search of the best solutions in the solution space. As an illustration of the methodology proposal, its performance is compared with the classic approach on a case study, obtaining faster the optimal solution",Hindawi Limited,Control of Discrete Event Systems by Means of Discrete Optimization and Disjunctive Colored PNs: Application to Manufacturing Facilities,,10.1155/2014/821707,"[{'title': None, 'identifiers': ['issn:1085-3375', '1687-0409', 'issn:1687-0409', '1085-3375']}]",core
217337005,2014-08-01T07:00:00,"Modelling rainfall-runoff processes enables hydrologists to plan their response to flooding events. Urban drainage catchment modelling requires rainfall-runoff models as a prerequisite. In the UK, one of the main software tools used for drainage modelling is InfoWorks CS, based on relatively simple methods which are relatively robust in predicting runoff. This paper presents an alternative approach to modelling runoff that will allow for the complex inter-relation of runoff that occurs from impermeable areas, permeable areas, local surface storage and variation in rainfall induced infiltration. Apart from the uncertainties associated with the measurement of connected surfaces to the drainage system, the physical processes involved in runoff are nonlinear, making artificial neural networks (ANNs) an ideal candidate for modelling them. ANNs have been used for runoff prediction in natural catchments, and recently on a study for predicting the performance of urban drainage systems. This study seeks to determine an input set that predicts sewerage flow in urban catchments where the runoff is dominated by infiltration, a major issue for the water industry. A framework is proposed in which an ANN is trained by an evolutionary algorithm, which optimises ANN weights; results are assessed using the Nash-Sutcliffe Efficiency Coefficient. The model is demonstrated on a real-world case study site for which rainfall, flow, air temperature and groundwater levels in three boreholes have been measured. Various combinations of these data are used as model inputs, examining a mixture of daily and sub-daily timesteps. The best predictions are generated from daily linearly combined antecedent rainfall and air temperature, although sub-daily information improves the worst-case performance of the model. Although infiltration is affected by groundwater levels, incorporating groundwater into the model does not improve predictions. The proposed ANN model is capable of producing acceptable predictions, thus avoiding many of the uncertainties involved in traditional infiltration modelling",CUNY Academic Works,An Artificial Neural Network-Based Rainfall Runoff Model For Improved Drainage Network Modelling,https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1333&context=cc_conf_hic,,,core
143124446,2015-07-07,"In this paper an ethanol reformer based on catalytic steam reforming with a catalytic honeycomb loaded with RhPd/CeO2 and palladium separation membranes with an area of 30.4 cm2 has been used to generate a pure hydrogen stream of up to 100 ml/min to feed a PEM fuel cell with an active area of 5 cm2. The fuel reformer behavior has been extensively
							studied under different temperature, ethanolewater flow rate and gas pressure at a fixed S/C ratio of 1.6 (molar). The hydrogen yield has been controlled by acting upon the ethanol-water fuel flow and gas pressure.
							A mathematical model of the ethanol reformer has been developed and an adaptive and predictive control has been implemented on a real time system to take account of its nonlinear behavior. With this control the response time of the reformer can be reduced by a factor of 7 down to 8 s.
							The improved dynamics of the controlled reformer match better the quickly changing hydrogen demands of fuel cells. They reached a magnitude where costly hydrogen buffers between the reformer and the fuel cell can be omitted and an electric buffer at the output of the fuel cell is sufficient.Fil: Koch, Reinhold. Universitat Technical Zu Munich;Fil: Lopez, Eduardo. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico - CONICET - Bahia Blanca. Planta Piloto de Ingenieria Quimica (i); Argentina;Fil: Divins, Núria J.. Institut de Têcniques Energetiques, Universitat Politecnica de Catalunya, España;Fil: Allué, Miguel. Institut de Robótica i Informática Industrial; España;Fil: Jossen, Andreas. Institute for Electrical Energy Storage Technology. Technische Universitat Munchen; Alemania;Fil: Riera, Jordi. Institut de Robótica i Informática Industrial; España;Fil: Llorca, Jordi. Institut de Têcniques Energetiques, Universitat Politecnica de Catalunya, España",Pergamon-elsevier Science Ltd,Ethanol catalytic membrane reformer for direct PEM FC feeding,,,,core
102562106,2015-08-26,"Flight simulators have been part of aviation history since its beginning. With the development of modern aeronautics industry, flight simulators have gained an important place and the industry devoted to their manufacture has become significant. In the case of transportation aircraft, accurate mathematical models based on extensive experimental data have been developed by their manufacturers to optimise their aerodynamic and propulsive characteristics and to design efficient flight control systems. However, in the case of small general aviation aircraft this kind of knowledge is not commonly available and the design of accurate flight simulators can result in a tedious try and modify process until the simulator presents a qualitative behaviour close to the one of the real aircraft. This communication proposes through the use of neural networks a method to perform a direct estimation of the aerodynamic forces acting on aircraft. Artificial Neural networks appear to be an appropriate numerical technique to achieve the mapping of these continuous relationships and detailed aerodynamics and thrust models should become no more mandatory to produce accurate flight simulation software. 1",,A Neural Approach for Fast Simulation of Flight Mechanics,,,,core
211478482,2014-01-01T00:00:00,"Fruits and vegetables are considered as part of a healthy diet and lifestyle. However, concerns have arisen regarding the microbiological safety of Ready To Eat (RTE) produces due to a number of foodborne outbreaks associated with pathogens. Although strict practices for controlling the safety of RTE produce have been implemented in the fresh produce industry, the current commercial operations rely on a wash treatment with water or with an antimicrobial agent as the only step for reducing microbial populations on fresh produce. However, washing with common sanitizers has been demonstrated to achieve no more than 1-2 log10 reduction in pathogen populations. Recently, much research effort has been put into development to provide multiple-hurdle techniques which enhance produce safety. Thus, non-thermal technologies for the inactivation of microorganisms are of increasing interest to the food industry for the control of spoilage and safety, thus for assuring public health.In this study, the effects of non-thermal disinfection processes, Near UV-Visible light (NUV-Vis), Continuous Ultraviolet Light (UV 254 nm), High Intensity Light Pulses (HILP), Ultrasound (US), as well as conventional sodium hypochlorite (SH) disinfection solutions were used. The effect of the above technologies was tested against bacteria (Escherichia coli, Staphylococcus aureus, Salmonella Enteritidis and Listeria innocua) and viruses (Human Adenovirus). More precisely, the bacteria that were used were: E. coli K12, E. coli NCTC 9001 (representative microorganisms for the Enterohaemorrhagic foodborne pathogen E. coli O157:H7), S. aureus NCTC 6571, L. innocua NCTC 11288 (as a surrogate microorganism for the common foodborne pathogen L. monocytogenes), S. Enteritidis NCTC 6676 and HAdV (indicator virus selected as a surrogate of HAV and norovirus). The main scope of this work was to study the efficacy of three light technologies on liquid suspensions. Then, the effect of UV, US, SH and combined technologies were evaluated on their efficiency to disinfect inoculated romaine lettuce, strawberries and cherry tomatoes. Furthermore, the effect of the above technologies on quality (color) and physicochemical characteristics of the RTE produces was evaluated. The physicochemical characteristics tested were Total Antioxidant Capacity (TAC), Total Phenolic Content (TPC) and Ascorbic Acid (AA) concentration.This study demonstrates that the use of alternative non-thermal technologies is effective for inactivation of microorganisms in fresh RTE foods and could be used as an alternative to traditional chlorine immersions. However, the effect of UV and US on quality and nutritional quality retention of RTE foods should be considered before its use as a disinfection technique.As far as non-thermal light technologies are concerned, HIPL treatment inactivated both E. coli and L. innocua more rapidly and effectively than either continuous UV-C or NUV-vis treatments. With HILP at a distance of 2.5 cm from the lamp, E. coli and L. innocua populations were reduced by 3.07 and 3.77 log10 CFU/mL respectively after a 5 sec treatment time, and were shown to be below the limit of detection ( 0.05) change the color of RTE foods tested.  Moreover, it was indicated that no significant differences (p>0.05) were observed as far as TAC is concerned when conventional treatments at different treatment times were used. However, when alternative disinfection treatments were used, an increase in TAC concentration was obvious from the first minutes of treatment. TPC concentration remained constant or was slightly decreased when RTE foods were immersed in NaOCl solutions. However, TPC increased significantly (p<0.05) in all RTE foods when UV and US alternative disinfection technologies were used. The Vit.C content of RTE foods did not exhibit any significant changes during different treatments. However, Vit.C was slightly decreased (p<0.05) when treatments of more than 30 minutes for US, UV and combinations of UV+US occurred.Furthermore, a computerized model was proposed based on critical points which are important during the production of lettuce. More precisely the development of a Decision Support System (DSS) using the theory of Fuzzy Cognitive Maps (FCMs), in order to diagnose the importance of critical control points (concepts) for the food safety and hygiene during the production of salad vegetables (lettuce), was implemented. The methodology described, extracts the knowledge from experts with different scientific background and exploits their experience on the process of lettuce production. The results of this study show that the present software tool can be explored and problems that can arise during the food production chain can be prevented.Generally, it was noted that the effect of each disinfection method is dependent upon the treatment time tested and the type of food. Treatment with UV and US reduced the numbers of selected inoculated bacteria on lettuce, strawberries and cherry tomatoes, which could be good alternatives to other traditional and commonly used technologies such as chlorine and hydrogen peroxide solutions. These results suggest that UV and US might be promising, non-thermal and environmental friendly disinfection technologies for fresh RTE produce industry. Taking everything into consideration, disinfection technologies play an important role in commercial practice in order to prevent the survival of pathogens and lower the risk of contamination thus assuring public health. However, nutritional and quality properties are essential as they can provide a protective role against the development and progression of many diseases and must be considered for the selection of disinfection process parameters.Η κατανάλωση φρούτων και λαχανικών αποτελεί μέρος μίας υγιεινούς δίαιτας και διατροφικού προφίλ γενικότερα. Η Μεσογειακή διατροφή αποτελεί ένα μοντέρνο τρόπο διατροφής η οποία έχει τις ρίζες της στις χώρες της Μεσογείου, όπως η Ελλάδα, η Ισπανία, η Πορτογαλία και η Νότια Ιταλία. Τα βασικά συστατικά που την απαρτίζουν είναι το λάδι, τα λαχανικά, τα δημητριακά, τα φρούτα, τα ψάρια, τα γαλακτοκομικά, και η μικρή κατανάλωση κρέατος (Noah and Truswell, 2001). Ως εκ τούτου, λαχανικά όπως το μαρούλι, οι τομάτες είναι κύρια συστατικά μιας ισορροπημένης διατροφής. Επίσης, φρούτα όπως οι φράουλες προτιμώνται από εκείνους που θέλουν να ακολουθούν την Μεσογειακή Διατροφή αλλά και από αυτούς που θέλουν να προσέχουν την υγεία τους.Το μαρούλι (Lactuca sativa L.) καταναλώνεται κυρίως ως σαλάτα και αποτελεί μία πλούσια πηγή συστατικών ευεργετικών για την υγεία όπως τα φαινολικά, η βιταμίνη C, τα καροτενοειδή και οι χλωροφύλλες (Nicolle et al., 2004). Περιλαμβάνει πολλά μακροστοιχεία (π.χ K, Na, Ca και Mg) και ιχνοστοιχεία (π.χ Fe, Mn, Cu, Zn και Se), τα οποία αποτελούν σημαντικά συστατικά μια σωστής διατροφής (Kawashima & Soares, 2003). Το μαρούλι αποτελεί επίσης μία καλή πηγή φωτοσυνθετικών χρωστικών και άλλων φυτοχημικών τα οποία ωφελούν την διατροφή και διαδραματίζουν σπουδαίο ρόλο στην παρεμπόδιση πολλών οξειδωτικών- σχετιζόμενων με το στρες- ασθενειών (Llorach et al., 2008). Οι φράουλες, είναι πλούσιες σε μία σειρά φυτοχημικών, ιδιαίτερα των φαινολικών συστατικών, κατέχοντας υψηλή αντιοξειδωτική ικανότητα (Häkkinen et al., 1999, Koponen et al., 2007). Επίσης έχουν μεγάλη περιεκτικότητα σε βιταμίνη C (60-100 mg/100 g τροφίμου) και σε ανθοκυανίνες, ειδικά πελαργονιδίνη-3-γλυκοζίτη (pg-3-gluc) και κυανιδίνη-3-γλυκοζίτη (CyA-3-gluc). Ως εκ τούτου, η φράουλα θεωρείται ως μια σημαντική διαιτητική πηγή ενώσεων που προάγουν την υγεία (Koponen et al., 2007). Οι τομάτες αποτελούν μία καλή πηγή βιταμινών (βιταμίνη Α, βιταμίνη C, και άλλων βιταμινών) καθώς και μεταλλικών στοιχείων (νάτριο, ασβέστιο, φώσφορος, σίδηρος), και ινών, πρωτεϊνών και λιπών. Η τομάτα είναι μία καλή πηγή αντιοξειδωτικών όπως το λυκοπένιο. Είναι γνωστό ότι το λυκοπένιο και οι ίνες είναι ευεργετικές στην ανθρώπινη υγεία, όταν καταναλώνονται ως μέρος μια ισορροπημένης διατροφής (Canene-Adams et al., 2005). Σύμφωνα με μελέτες το λυκοπένιο έχει χαρακτηριστεί για τις αντιφλεγμονώδεις, αντιμεταλλαξιγόνες και αντικαρκινικές ιδιότητές του (Boon et al., 2010). Επιπλέον, το λυκοπένιο είναι γνωστό για τη μείωση του κινδύνου αδενώματος, και την προώθηση λειτουργικότητας του ανοσοποιητικού συστήματος (Kun et al., 2006). Συνιστάται, 6-15 mg πρόσληψη λυκοπενίου για την βελτίωση της υγείας (Kun et al., 2006). Οι διαλυτές φυτικές ίνες ρυθμίζουν τη γλυκόζη στο αίμα και τα επίπεδα χοληστερόλης (Weickert and Pfeifer, 2008). Ενώ οι αδιάλυτες φυτικές ίνες προάγουν την κάθαρση και βοηθούν εναντίον πολλών καρκίνων όπως ο καρκίνος του παχέος εντέρου (Alvarado et al., 2001). Στα προϊόντα τομάτας, η βιταμίνη C και οι πολυφαινόλες έχουν αναφερθεί ότι είναι τα κύρια υδρόφιλα αντιοξειδωτικά συστατικά, ενώ η βιταμίνη Ε και τα καροτενοειδή αποτελούν κυρίως το υδρόφοβο κλάσμα (Hsu, 2008).Πολλές επιδημιολογικές μελέτες έχουν συσχετίσει την κατανάλωση φρούτων και λαχανικών με τον προστατευτικό ρόλο τους ενάντια σε πολλές ασθένειες (Hannum, 2004). Ρίζες οξυγόνου, μπορεί να αντιδράσουν με λίπη, πρωτεΐνες και DNA. Ο ρόλος των αντιοξειδωτικών που υπάρχουν στα φρούτα και στα λαχανικά είναι να διατηρούν τα χαμηλά επίπεδα των ελευθέρων ριζών είτε παρεμποδίζοντας την εμφάνισή τους, είτε ευνοώντας την αποσύνθεσή τους (Hancock et al., 2007). Παρόλα αυτά, το τελευταίο διάστημα, λόγω του αυξανόμενου αριθμού τροφιμογενών ασθενειών σε όλο τον κόσμο, επικρατεί ανησυχία σχετικά με την μικροβιολογική ασφάλεια των τροφίμων αυτών. Τρόφιμα «έτοιμα προς κατανάλωση  (ready-to-eat)», θεωρούνται ότι ανήκουν στην κατηγορία «υψηλού κινδύνου». Τα συγκεκριμένα τρόφιμα δεν επιδέχονται κάποια θερμική ή άλλη επεξεργασία θανάτωσης παθογόνων μικροοργανισμών. Η μικροβιολογική ασφάλεια των τροφίμων και των τροφιμογενών ασθενειών αποτελούν περίπλοκα ζητήματα, καθώς περισσότερες από 200 γνωστές ασθένειες είναι γνωστό ότι μεταδίδονται μέσω των τροφίμων. Οι κύριοι λόγοι μετάδοσης τροφιμογενών ασθενειών είναι η επιμόλυνση με βακτήρια, ιούς, παράσιτα, μύκητες.Στις Η.Π.Α, το μέσο ετήσιο κόστος που σχετίζεται με βακτηριακές και παρασιτικές τροφιμογενείς λοιμώξεις εκτιμάται στα 6.5 δισεκατομμύρια δολάρια (Buzby & Roberts, 1996, Tauxe, 2002). Ο Tauxe (2002) αναφέρει ότι ανάμεσα στις καταγεγραμμένες τροφιμογενείς λοιμώξεις, οι βακτηριακές λοιμώξεις ευθύνονται για ένα περίπου 30% των περιπτώσεων, οι ιολογικές για το 67% και οι παρασιτικές για το 3%. Νοσηλεία στο νοσοκομείο πραγματοποιήθηκε λόγω λοιμώξεων από βακτήρια (60%), από ιούς (35%) και από παράσιτα (5%). Τέλος, θάνατος είναι δυνατό να προέλθει από βακτήρια (72%), ιούς (7%) και παράσιτα (21%). Έχει αναφερθεί ότι πέντε τροφιμογενείς παθογόνοι μκροοργανισμοί (E.coli O157:H7, Salmonella, Campylobacter, Listeria, and Toxoplasma) είναι υπεύθυνοι για 3.5 εκατομμύρια κρούσματα, 33.000 νοσηλείες και 1.600 θανάτους ετησίως στις Η.Π.Α (Tauxe, 2002).Μέχρι σήμερα στις βιομηχανίες τροφίμων εφαρμόζονται μία σειρά αυστηρών πρακτικών απολύμανσης για τον έλεγχο της ασφάλειας των έτοιμων προς κατανάλωση τροφίμων. Οι πρακτικές αυτές περιλαμβάνουν ξεπλύματα με τρεχούμενο νερό ή με αντιμικροβιακά διαλύματα. Επιστημονικές μελέτες όμως καταδεικνύουν την ανικανότητα επαρκούς απολύμανσης παθογόνων μικροοργανισμών που υπάρχουν στα τρόφιμα, με τις συνήθεις πρακτικές που εφαρμόζονται σήμερα. Για το λόγο αυτό, νέες τεχνικές πολλαπλών εμποδίων έχουν αρχίσει να εφαρμόζονται με σκοπό την διασφάλιση της δημόσιας υγείας.Τα τρόφιμα επεξεργάζονται με διάφορες τεχνολογίες με σκοπό την μείωση και την απομάκρυνση πιθανών παθογόνων ή άλλων βιολογικών κινδύνων που μπορεί να υπάρχουν στα τρόφιμα. Οι κλασικές τεχνολογίες απολύμανσης όπως παστερίωση ή αποστείρωση, χρησιμοποιούνται με σκοπό την απενεργοποίηση ή τη θανάτωση των μικροβίων. Η απολύμανση με χλώριο αποτελεί μία ευρέως διαδεδομένη και οικονομική μέθοδο απολύμανσης η οποία χρησιμοποιείται για την απολύμανση νερού και τροφίμων (EPA, 1999c). Διαφορετικές μορφές χλωρίου μπορούν να χρησιμοποιηθούν στην απολύμανση των τροφίμων όπως: διοξείδιο του χλωρίου, υποχλωριώδες νάτριο, υποχλωριώδες ασβέστιο, κ.α (Park et al., 2008). Παρόλα αυτά το χλώριο μπορεί να αντιδράσει με οργανικές ουσίες σχηματίζοντας έτσι τοξικές χημικές ουσίες όπως οργανοχλωρoπαράγωγα, δηλαδή ενώσεις που ανήκουν στην κατηγορία των τριαλογονοµεθανίων (THM’s)  (McDonnel and Russell, 1999).Οι εναλλακτικές, μη-θερμικές τεχνολογίες απολύμανσης έχουν αποδειχθεί ότι είναι ικανές να επιτύχουν απολύμανση μικροβίων χωρίς την έκθεση των τροφίμων σε θερμότητα. Έχει επίσης βρεθεί ότι οι τεχνολογίες αυτές διατηρούν τα διατροφικά και οργανοληπτικά χαρακτηριστικά των τροφίμων, επεκτείνοντας τον χρόνο ζωής τους και διατηρώντας την εξωτερική τους εμφάνιση (Butz and Tauscher, 2002). Τέτοιες τεχνολογίες είναι τα παλλόμενα ηλεκτρικά πεδία (PEF), η υπεριώδης ακτινοβολία (UV), το παλλόμενο φως υψηλής έντασης (HILP), υπέρηχοι (US), εγγύς υπεριώδης φως (NUV light), ιονίζουσα ακτινοβολία, όζων, υψηλή υδροστατική πίεση (HPP)κ.α (Mohd. Adzahan and Benchamaporn, 2007).Η ακτινοβολία στο εγγύς υπεριώδες 395± 5 nm, δρα διεγείροντας ενδογενή μόρια πορφυρίνης παράγοντας μονήρες οξυγόνο (1O2), που καταστρέφει τα κύτταρα και έτσι θανατώνονται οι μικροοργανισμοί (Elman and J. Lebzelter, 2004, Feuerstein et al. 2005, Maclean et al. 2008b, Murdoch et al., 2012, Lipovsky et al. 2010). Η υπεριώδης ακτινοβολία όταν διαπερνά την κυτταρική μεμβράνη των μικροοργανισμών και απορροφάται από τα κυτταρικά συστατικά τους (DNA, RNA),  τους καθιστά ανίκανους να πολλαπλασιαστούν. Το κατάλληλο μήκος κύματος το οποίο μπορεί να προκαλέσει ζημιά στο μικροβιακό DNA ή RNA είναι περίπου 254 nm. Όταν το γενετικό υλικό των κυττάρων απορροφά την ενέργεια από την υπεριώδη ακτινοβολία σχηματίζονται διμερή πυριμιδίνης μεταξύ γενετικών βάσεων πυριμιδίνης στην ίδια  αλυσίδα DNA. Χάρη σε αυτό το δεσμό διμερών στην αλυσίδα του DNA, οι μικροοργανισμοί προσβάλλονται με τέτοιο τρόπο ώστε ο διαχωρισμός των κυττάρων και επομένως ο πολλαπλασιασμός τους να είναι αδύνατος. Έτσι, ο μικροοργανισμός γίνεται αβλαβής και θανατώνεται (Guerrero- Beltrán and Barbosa-Cánovas, 2004). Αν και οι περισσότεροι μικροοργανισμοί προσβάλλονται από την υπεριώδη ακτινοβολία, η ευαισθησία τους ποικίλλει, καθώς εξαρτάται από την αντίσταση στη διείσδυση της υπεριώδους ακτινοβολίας. Η χημική σύνθεση του κυτταρικού τοιχώματος και το πάχος του καθορίζουν την αντίσταση των μικροοργανισμών στην υπεριώδη ακτινοβολία. Η αποτελεσματικότητα της απολύμανσης με υπεριώδη ακτινοβολία  επηρεάζεται από την ποσότητα – δόση της υπεριώδους ενέργειας που απορροφάται από το μικροοργανισμό. Η δόση της ακτινοβολίας εξαρτάται από την ένταση της παρεχόμενης ακτινοβολίας (ενέργεια, mW), τον χρόνο κατά τον οποίο ο μικροοργανισμός εκτίθεται σε αυτήν (διάρκεια ακτινοβολίας, sec) και είναι αντιστρόφως ανάλογη με την επιφάνεια του υγρού στο οποίο εφαρμόζεται (cm2). Το παλλόμενο φως υψηλής έντασης (HILP) αποτελεί μία αναδυόμενη μη-θερμική τεχνολογία απολύμανσης, η οποία χρησιμοποιεί μικρής διάρκειας (100–400 s) αλλά υψηλής έντασης φως (200–1100 nm) (Marquenie et al., 2003, Woodling and Moraru, 2007, Gomez-Lopez et al., 2007). Ο τρόπος δράσης βασίζεται στην φωτοχημική δράση της υπεριώδους ακτινοβολίας η οποία προκαλεί διμερισμό της θυμίνης οδηγώντας στον θάνατο των κυττάρων (Muňoz et al., 2012, Gomez-Lopez et al., 2007, Rajkovic et al., 2010). Εξ’ ορισμού οι υπέρηχοι συνιστούν κύματα υψηλής συχνότητας που μεταφέρουν πίεση κατά τη διέλευσή τους σε ένα μέσο. Αυτό έχει ως αποτέλεσμα τη δημιουργία  περιοχών χαμηλής και υψηλής πίεσης. Η διακύμανση αυτή της πίεσης  αναφέρεται ως πλάτος πίεσης (amplitude) και είναι ανάλογο της ποσότητας ενέργειας που εφαρμόζεται στο σύστημα. Στην περίπτωση που οι διακυμάνσεις της πίεσης είναι αρκετά υψηλές (3.000 ΜΡa), τότε ένα υγρό μέσο μπορεί να αποδομηθεί και να έχουμε το σχηματισμό  μικροφυσαλίδων αερίου και ατμού. Το φαινόμενο αυτό είναι γνωστό ως σπηλαίωση (cavitation), ενώ οι φυσαλίδες είναι δυνατόν να διασπώνται και να επαναδημιουργούνται συνεχώς επιφέροντας αλλαγές στη δομή του μέσου που υφίσταται την επίδραση των υπερηχητικών κυμάτων, απενεργοποιώντας έτσι τους μικροοργανισμούς από την επιφάνεια των τροφίμων (Bilek and Turantas, 2013). Το κύριο πλεονέκτημα των υπερήχων για τη βιομηχανία τροφίμων  είναι ότι θεωρούνται μια ευρέως αποδεκτή τεχνολογία από το ευρύ καταναλωτικό κοινό, λόγω της ασφάλειάς τους, της μη τοξικότητάς τους και της φιλικότητάς τους προς το περιβάλλον.Σκοπός της παρούσας εργασίαςΠολλές επιδημίες που προέρχονται από τροφιμογενείς λοιμώξεις έχουν καταγραφεί τον τελευταίο καιρό, καθώς επίσης πολλές ανακλήσεις προϊόντων συμβαίνουν. Αποτελεί λοιπόν αναγκαιότητα η εξάλειψη των παθογόνων από τρόφιμα λόγω του υψηλού κινδύνου, του υψηλού ποσοστού θνησιμότητας καθώς και της οικονομικής επιβάρυνσης που προκαλούν οι ασθένειες (π.χ λιστερίωση, σαλμονέλλωση). Στην παρούσα μελέτη μη-θερμικές, εναλλακτικές τεχνολογίες απολύμανσης ελέγχθηκαν όπως: Φως κοντά στην υπεριώδη ακτινοβολία σε μήκος κύματος 395±5 nm (NUV-Vis light), συνεχής υπεριώδης ακτινοβολία σε μήκος κύματος 254nm (Continuous UV light), υψηλής έντασης παλμοί φωτός (HILP), υπέρηχοι (ultrasound). Επίσης, η συμβατική και κλασική μέθοδος της εμβάπτισης σε υποχλωριώδες νάτριο (sodium hypochlorite solution) εφαρμόστηκε. Τέλος, συνδυασμοί εναλλακτικών, καθώς και εναλλακτικών με κλασικές μεθόδους πραγματοποιήθηκαν. Ο σκοπός ήταν ο έλεγχος της εφαρμογής των τεχνολογιών αυτών στα τρόφιμα με σκοπό τη διασφάλιση της δημόσιας υγείας των καταναλωτών.Όλες οι παραπάνω μέθοδοι εφαρμόστηκαν σε τρόφιμα έτοιμα προς κατανάλωση όπως μαρούλι, φράουλες και τοματίνια τύπου cherry, τα οποία αγοράστηκαν από τοπικό σουπερμάρκετ και εμβολιάστηκαν με παθογόνους μικροοργανισμούς οι οποίοι αποτέλεσαν το αρχικό μικροβιακό φορτίο. Οι μικροοργανισμοί οι οποίοι εμβολιάστηκαν ήταν βακτήρια που έχουν συναντηθεί στα εν λόγω τρόφιμα όπως  E. coli, S. aureus, S. enteritidis και L. innocua καθώς και ο αδενοιός (HAdV35). Πιο συγκεκριμένα τα στελέχη που χρησιμοποιήθηκαν ήταν: E. coli K12, E. coli NCTC 9001 (ως μικροοργανισμοί δείκτες για το εντεροαιμοραγικό παθογόνο E. coli O157:H7), S. aureus NCTC 6571, L. innocua NCTC 11288 (ως μικροοργανισμοί δείκτες για το παθογόνο Listeria monocytogenes), S. Enteritidis NCTC 6676 και HAdV (ιός δείκτης για τους ιούς HAV και norovirus). Πιο συγκεκριμένα, στο πρώτο μέρος της διατριβής, τρεις τεχνολογίες απολύμανσης (NUV-Vis, Continuous UV, HILP) χρησιμοποιήθηκαν όσον αφορά την απολύμανση των μικροοργανισμών δεικτών (E. coli και L. innocua), τα οποία εμβολιάστηκαν σε υγρά διαλύματα  (MRD Buffer). Ο σκοπός ήταν να ελεγχθεί η απολυμαντική δράση των τεχνολογιών αυτών, χρησιμοποιώντας διαφορετικές εντάσεις φωτός.Στο δεύτερο μέρος της διατριβής, τα τρόφιμα εμβολιάστηκαν με διάφορα βακτήρια (E. coli, S. aureus, S. Εnteritidis, L. innocua) και έναν αδενοϊό με σκοπό να ελεγχθεί η απολύμανσή τους, έπειτα από τη χρήση του χλωρίου, του υπεριώδους φωτός και των υπερήχων, καθώς επίσης και συνδυασμών τους. Επίσης, πραγματοποιήθηκαν πειράματα με διαφορετικές αρχικές συγκεντρώσεις βακτηρίων, με σκοπό τον περεταίρω έλεγχο της απολυμαντικής δράσης των παραπάνω τεχνολογιών. Τέλος, τα αποτελέσματα της συγκέντρωσης των ιών που προήλθαν από τη χρήση αλυσιδωτής αντίδρασης πολυμεράσης σε πραγματικό χρόνο (Real-Time PCR), επιβεβαιώθηκαν με τη χρήση καλλιεργειών κυττάρων. Τέλος,  έπειτα από την επεξεργασία των τροφίμων με τις μεθόδους απολύμανσης, αποθηκεύτηκαν τα τρόφιμα στο ψυγείο για διάστημα 15 ημερών, και ελέγχθηκε το μικροβιακό τους φορτίο έπειτα από 3, 7 και 15 ημέρες.Στο τρίτο μέρος της διατριβής, η επίδραση των παραπάνω μεθόδων σε επιλεγμένες διατροφικές παραμέτρους και παραμέτρους ποι",'National Documentation Centre (EKT)',Εναλλακτικές τεχνολογίες απολύμανσης τροφίμων και εκτίμηση κινδύνου για την δημόσια υγεία,,10.12681/eadd/36272,,core
103131457,2015-11-01,"The aim of this study was modeling of ambient air pollutants through ANN in industrial area of Ujjain city in India and the study was carried out on modeling of air pollutants like SPM and RSPM using Artificial Neural Network. Artificial neural networks (ANN), whose performances to deal with pattern recognition problems is well known, are proposed to identify air pollution sources. The ANN system was run by giving the inputs of meteorological data’s and giving the outputs of concentration of various pollutants and accordingly the estimation of Errors was done by this study. The monthly data’s in year from 2009-2012 of meteorological data’s like Temperature, Humidity, wind pressure and rainfall and the pollutants concentration were collected from the State Pollution Control Board. The ANN system used, as shown in figure 1, analyses all these data’s and finds the error coming during the experiment. The study estimated the Mean Square Error (MSE) from the inputs and outputs which were given to ANN in the industrial area of Ujjain City in India was found satisfactory being in the range of 0.001-0.003. The results shown here indicate that the neural network techniques can be useful tool in the hands of practitioners of air quality management and prediction. The models studied in this study are easily implemented, and they can deliver prediction in real time, unlike other modeling techniques",,Modeling of Ambient Air Pollutats for RSPM and SPM Through Artifical Neural Network In Industrial Area of Ujjain City,,,,core
228059107,2015-11-11T00:00:00,"International audienceThe International Workshop on Principles of Diagnosis (DX) is an annual event that started in 1989 and is rooted in the Artificial Intelligence (AI) community. This annual international workshop has been uniting researchers and practitioners with diverse backgrounds (Artificial Intelligence (AI), verification, software engineering, debugging, ...) in order to leverage research in the global context of diagnosis, that is, identifying the root causes for encountered issues. The DX Workshop series has been offering a forum to present current research and experience reports, exchange and discuss emerging ideas, as well as debate current issues and envisioned future challenges.Papers presented at the workshop cover a variety of theories, principles, and computational techniques for diagnosis, monitoring, testing, reconfiguration, fault- adaptive control, and repair of complex systems. Applications of these theories, principles, and techniques to industry-related disciplines and other real-world problems are also important topics of the workshop.Like the previous workshops in this series, DX-2015 encourages the interactions and the exchange of theories, techniques, applications, and experiences amongst researchers and practitioners from diverse backgrounds – artificial intelligence, control theory, verification, software and systems engineering, and other related areas – who share an interest in different aspects of diagnosis, and the related fields of testing, reconfiguration, maintenance, prognosis, and fault-adaptive control.DX is a lively forum that has traditionally adopted a single track program with a limited number of participants in order to promote detailed technical exchange and debate while at the same time making efforts to develop synergistic approaches to solving real-world problems",HAL CCSD,Proceedings of the 26th International Workshop on Principles of Diagnosis (DX-2015),,,,core
56611312,2014,"Artículo de publicación ISIThe capability to generate complex geometry features
at tight tolerances and fine surface roughness is a key
element in implementation of Creep Feed grinding process in
specialist applications such as the aerospace manufacturing
environment. Based on the analysis of 3D cutting forces,
this paper proposes a novel method of predicting the profile
deviations of tight geometrical features generated using
Creep Feed grinding. In this application, there are several
grinding passes made at varying depths providing an incremental
geometrical change with the last cut generating the
final complex feature. With repeatable results from coordinate
measurements, both the radial and tangential forces can
be gauged versus the accuracy of the ground features. The
results of the tangential force were found more sensitive to
the deviation of actual cut depth from the theoretical one.
However, to make a more robust prediction on the profile
deviation, its values were considered as a function of both
force components. In addition, the power signals were obtained
as these signals are also proportional to force and
deviation measurements. Genetic programming (GP), an
evolutionary programming technique, has been used to
compute the prediction rules of part profile deviations based
on the extracted radial and tangential force and correlated
with the initial “gauging” methodology. It was found that
using this technique, complex rules can be achieved and
used online to dynamically control the geometrical accuracy
of the ground features. The GP complex rules are based on
the correlation between the measured forces and recorded
deviation of the theoretical profile. The mathematical rules
are generated from Darwinian evolutionary strategy which
provides the mapping between different output classes. GP works from crossover recombination of different rules, and
the best individual is evaluated in terms of the given best
fitness value so far which closes on an optimal solution.
Once the best rule has been generated, this can be further
used independently or in combination with other close-tobest
rules to control the evolution of output measures of
machining processes. The best GP terminal sets will be
realised in rule-based embedded coded systems which will
finally be implemented into a real-time Simulink simulation.
This realisation gives a view of how such a control
regime can be utilised within an industrial capacity. Neural
networks were also used for GP rule verification.The experimental work was carried out at The University of
Nottingham funded by EPSRC",Springer-Verlag,The prediction of profile deviations when Creep Feed grinding complex geometrical features by use of neural networks and genetic programming with real-time simulation,,10.1007/s00170-014-5829-0,,core
296623211,2015-11-26T14:19:14Z,"This work presents the development of a predictive hybrid controller (PHC) based in fuzzy systems for polymerization processes. These reactions have typically a highly non linear dynamic behavior, thus making the performance of controllers based on conventional internal models to be poor or to require a lot of effort in controller tuning. The solution copolymerization of methyl methacrylate and vinyl acetate in a continuous stirred tank reactor is used to illustrate the performance of the proposed controller. It is introduced the development of a methodology for the design of the predictive controller based on functional fuzzy dynamic models of Takagi-Sugeno type. These models present an excellent capacity to represent dynamic data and this feature is explored in the proposed hybrid controller. Moreover, they allow the inclusion of qualitative or operational information of the process. Gaussian membership functions are used for the fuzzy sets and model determination (rules number and model parameters) is obtained from the process database. The treatment of these data for the fuzzy model determination is carried out by means of algorithms of subtractive clustering and least squares. The kinetic parameters and reactor operating conditions are obtained from the literature and a mathematical model is considered as a virtual plant for data generation and process identification. The modeling by the fuzzy approach showed to have a good potential for the processes representation. The PHC controller was compared to the dynamic matrix controller (DMC) to the regulatory and servo problems. The obtained results showed that the proposed control is robust and it requires less computational time than the conventional predictive controllers, being an interesting alternative to attack control problems in complex chemical processes.ABDELAZIM, T., MALIK, O.P., Identification of nonlinear systems by Takagi-Sugeno logic grey box modeling for real-time control (2005) Control Engineering Practice, 13, pp. 1489-1498ALEXANDRIDIS, A.P., SIETTOS, C.I., SARIMVEIS, H.K., BOUDOUVIS, A.G., BAFAS, G.V., Modelling of nonlinear process dynamics using Kohonen's neural networks, fuzzy systems and Chebyshev series (2002) Computers and Chemical Engineering, 26, pp. 479-486CERRADA, M., AGUILAR, J., COLINA, E., TITLI, A., Dynamical membership functions: An approach for adaptive fuzzy modeling (2005) Fuzzy Sets and Systems, 152, pp. 513-533CHEN, B., LIU, X., Reliable control design of fuzzy dynamic systems with time-varying delay (2003) Fuzzy Sets and Systems, pp. 1-26CHIU, S., A cluster estimation method with extension to fuzzy model identification (1994) IEEE, pp. 1240-1245CHIU, S., Method and software for extracting fuzzy classification rules by subtractive clustering (1996) IEEE, pp. 461-465CONGALIDIS, J.P., RICHARDS, J.R., RAY, W.H., Feedforward and feedback control of a solution copolymerization reactor (1989) AIChe Journal, 35 (6), pp. 891-907. , JuneDOUGHERTY, D., COOPER, D.A., Practical Multiple Model Adaptive Strategy for Multivariable Model Predictive Control (2003) Control Engineering Practice, 11, pp. 649-664GUIAMBA, I.R.F., MULHOLLAND, M., Adaptive Linear Dynamic Matrix Control Applied to an Integrating Process (2004) Computers and Chemical Engineering, 28, pp. 2621-2633HABBI, H., ZELMAT, M., BOUAMAMA, B.O., A dynamic fuzzy model for a drum-boiler-turbine system (2003) Automatica, 39, pp. 1213-1219HAERI, M., BEIK, H.Z., Application of Extended DMC for Nonlinear MIMO Systems (2005) Computers and Chemical Engineering, 29, pp. 1867-1874MANER, B.R., DOYLE III, F.J., Polymerization reactor control using autoregressive-plus volterra-based MPC (1997) AIChe Journal, 43 (7), pp. 1763-1784. , JulyPARK, M., RHEE, H., Property Evaluation and Control in a Semibatch MMA/MA Solution Copolymerization Reactor (2003) Chemical Engineering Science, 58, pp. 603-611PASSINO, K.M., YURKOVICH, S., (1998) Fuzzy Control, , Addison-Wesley-Longman, Menlo Park, CARAMASWAMY, S., CUTRIGHT, T.J., QAMMAR, H.K., Control of a Continuous Bioreactor Using Model Predictive Control (2005) Process Biochemistry, 40, pp. 2763-2770ROSS, T.J., (2004) Fuzzy Logic with Engineering Applications, , John Wiley & Sons Ltd, Second EditionSALA, A., GUERRA, T.M., BABUSKA, R., Perspectives of fuzzy systems and control (2005) Fuzzy Sets and Systems, 156, pp. 432-444SANTOS, L.O., AFONSO, P.A., CASTRO, J.A., OLIVEIRA, N.M., BIEGLER, L.T., On-line Implementation of Nonlinear MPC: An Experimental case study (2001) Control Engineering Practice, 9, pp. 847-857SCHNELLE, P.D., ROLLINS, D.L., Industrial Model Predictive Control Technology as Applied to Continuous Polymerization Processes (1998) ISA Transactions, 36 (4), pp. 281-292SILVA, J. E. L. Simulação e Controle Preditivo Linear (com Modelo de Convolução) e Não-Linear (com Modelo Baseado em Redes Neurais Artificiais) de Colunas Recheadas de Absorção com Reação Química. MSc. Thesis, DESQ/FEQ/UNICAMP, Campinas, São Paulo, Brazil, 1997;TAKAGI, T., SUGENO, M., Fuzzy identification of systems and its applications to modeling and Control (1985) IEEE Transactions on Systems, Man, and Cybernetics, 15, pp. 116-133TOLEDO, E.C., Modelagem, V., Simulação e Controle de Reatores Catalíticos de Leito Fixo. DSc. Thesis (1999) DPQ/FEQ/UNICAMP, , Campinas, São Paulo, BrazilZADEH, L., Outline of a new approach to the analysis of complex systems and decision process (1973) IEEE Transactions on Systems, Man, and Cybernetics, 1, pp. 28-4",,Development Of Dynamic Models And Predictive Control By Fuzzy Logic For Polymerization Processes,,,,core
296649846,2015-11-26T15:13:52Z,"For the correct simulation of solidification and temperature evolution in the continuous casting of steel, the determination of boundary conditions describing the heat-transfer phenomena through the strand surface, in each cooling zone of the casting machine, is extremely important. These boundary conditions are usually expressed as heat fluxes or heat transfer coefficients. In the present study, the surface temperature of the steel billet was experimentally determined in a steelmaking plant by infrared pyrometers positioned along the secondary cooling zone during real operation of a continuous casting machine. These data were used as input information into an Inverse Heat Transfer Code, implemented in this work, in order to permit the heat transfer coefficients of each spray cooling zone to be determined. The resulting simulations of temperature evolution during continuous casting have shown that the solidification was not complete at the unbending point and that there was a risk of breakout at the mold exit under the adopted operating conditions.730-732841846 Portuguese Materials Society- SPM,School of Engineering of the University of Minho,CT2M - Centre for Mechanical and Materials Technologies,Institute for Polymers and Composites/I3N,3B's Res. Group on Biomater., Biodegradables and BiomimeticsSantos, C.A., Spim, J.A., Ierardi, M.C.F., Garcia, A., The use of artificial intelligence technique for the optimisation of process parameters used in the continuous casting of steel (2002) Appl. Math. Modell, 26, pp. 1077-1092Santos, C.A., Spim, J.A., Garcia, A., Modeling of solidification in twin-roll strip casting (2000) J. Mater. Proc. Technol, 102, pp. 33-39Brimacombe, J.K., Design of continuous casting machines based on a heat-flow analysis: Stateof-the-art review (1976) Can. Metall. Q, 15, pp. 163-175Zheng, X.S., Sha, M.H., Jin, J.Z., Experimental research and numerical simulation of mold temperature field in continuous casting of steel (2006) Acta Metall. Sin, 19, pp. 176-182Chaudhuri, S., Singh, R.K., Patwari, K., Majumdar, S., Ray, A.K., Prasad, S.A.K., Neogi, N., Design and implementation of an automated secondary cooling system for the continuous casting of billets (2010) ASA Trans, 49, pp. 121-129Brimacombe, J.K., Sorimachi, K., Crack formation in the continuous casting of steel (1977) Metall. Mater. Trans. B, 8, pp. 489-505Cheung, N., Garcia, A., The use of a heuristic search technique for the optimization of quality of steel billets produced by continuous casting (2001) Eng. Appl. Artif. Intel., 14, pp. 229-238Meng, Y., Thomas, B.G., Heat transfer and solidification model of continuous slab casting: Con1d, Metall (2003) Mater. Trans. B, 34, pp. 685-705Ramírez-López, A., Aguilar-López, R., Palomar-Pardavé, M., Romero-Romo, M.A., Muñoz-Negrón, D., Simulation of heat transfer in steel billets during continuous casting (2010) Int. J. Miner. Metall. Mater, 17, pp. 403-416Barcellos, V.K., Ferreira, C.R.F., Spim, J.A., Santos, C.A., Garcia, A., The interrelation between casting size, steel grade, and temperature evolution along the mold length and the strand surface during continuous casting of steel, Mater (2011) Manuf. Proc., 26, pp. 113-126Santos, C.A., Fortaleza, E.L., Ferreira, C.R.F., Spim, J.A., Garcia, A., A solidification heat transfer model and a neural network based algorithm applied to the continuous casting of steel billets and blooms, Model (2005) Simul. Mater. Sc., 13, pp. 1071-1087Incropera, F.P., Dewitt, D.P., (1990) Fundamentals of Heat and Mass Transfer, , John Wiley & Sons, New YorkVoller, V.R., Swaminathan, C.R., General Source-Based Method for Solidification Phase Change, Numer (1991) Heat Transfer, Part B, 19, pp. 175-189Beck, J.V., Nonlinear estimation applied to the nonlinear inverse heat conduction problem (1970) Int. J. Heat Mass Tran., 13, pp. 703-716Larreq, M., Birat, J.P., Optimization of casting and cooling conditions on steel continuous casters (1982) Proceedings of the Third Process Technology Conference, Iron & Steel Society of AIME, Pittsburgh, PA, 1, pp. 273-282Lally, B., Biegler, L.T., Henein, H., Optimization and continuous casting: Part II. application to industrial casters (1991) Metall. Mater. Trans. B, 22, pp. 649-65",,Assessment Of Cooling Conditions Of A Continuous Casting Machine For Steel Billets Based On Surface Temperature Measurements,,10.4028/www.scientific.net/MSF.730-732.841,,core
224450426,2015-01-01T00:00:00,"This dissertation investigates the communication optimization for customizable domain-specific computing at different levels in a customizable heterogeneous platform (CHP) to improve the system performance and energy efficiency. Fabric-level optimization driven by emerging devices. Programmable fabrics (e.g., FPGAs) can be used to improve domain-specific computing by &gt;10x in energy efficiency over CPUs since FPGAs can be customized to the application kernels in the target domain. But the programmable interconnects inside FPGAs occupy &gt;50% of the FPGA area, delay and power. We propose a novel architecture of programmable interconnects based on resistive RAM (RRAM), a type of emerging device with high density and low power. We optimize the layout and the programming circuit of the new architecture. We also extend RRAM benefits to routing buffers. We observe the high defect rate in the emerging RRAM manufacturing and further develop a defect-aware communication mechanism. Conventional defect avoidance leaves a large portion of the chip in the new architecture unusable. So we propose new defect utilization methodologies by treating stuck-closed defects as shorting constraints in the routing of signals. We develop a scalable algorithm to perform timing-driven routing under these extra constraints and successfully suppress the impact of defects. Chip-level optimization driven by accelerator-centric architectures. A chip can also be customized to an application domain by integrating a sea of accelerators designed for the frequently used kernels in the domain. The design of interconnects among customized accelerators and shared resources (e.g., shared memories) is a serious challenge in chip design. Accelerators run 100x faster than CPUs and post a high data demand on the communication infrastructure. To address this challenge, we develop a novel design of interconnects between accelerators and shared memories and exploit several optimization opportunities that emerge in accelerator-rich computing platforms. Experiments show that our design outperforms prior work that was optimized for CPU cores or signal routing. Another design challenge lies in the data reuse optimization within an accelerator to minimize its off-chip accesses and on-chip buffer usage. Since the fully pipelined computation kernel consumes large amounts of data every clock cycle, and the data access pattern is the major difference among applications, existing accelerators use ad hoc data reuse schemes that are carefully tuned per application to fit the data demand. To reduce the engineering cost of accelerator-rich architectures, we develop a data reuse infrastructure that is generalized for the stencil computation domain and can be instantiated to the optimal design for any application in the domain. We demonstrate the robustness of our method over a set of real-life benchmarks. Server-level and cluster-level optimization driven by big data. In the era of big data, workloads can no longer fit into a single chip. Most data are stored in disks, and we can only load a small part of it into main memories during computation. Due to the low access speed of disks, our primary design goal becomes minimization of the data transfer between disks and the main memory. We select a popular big data application, convolutional neural network (CNN), as a case study. We analyze the linear algebraic properties of CNN, and propose algorithmic modifications to reduce the total computational workload and the disk access. Furthermore, when the application data become even larger, it needs to be distributed among a cluster of server nodes. This motivates us to develop an accelerator-centric computing cluster. We test two machine learning applications, logistic regression and artificial neural network (ANN), on our prototyping cluster and try to minimize the total data transfer incurred during the computation in this cluster. We select the distributed stochastic gradient descent (dSGD) as our training algorithm to eliminate the inter-node communication within a training iteration. We also deploy an in-memory cluster computing infrastructure, Spark, to eliminate the inter-node communication across training iterations. The baseline Spark only supports CPUs, and we develop a software layer to allow Spark tasks to offload their major computation to accelerators which are equipped by each server node. During the computation offloading, we group multiple tasks into a batch and transfer it to the target accelerator in one transaction to minimize the setup overhead of the data transfer between accelerators and host servers. We further realize accelerator data caching to eliminate the unnecessary data transfer of training data based on the properties of iterative machine learning applications","eScholarship, University of California",Communication Optimization for Customizable Domain-Specific Computing,,,,core
296662864,2015-11-26T15:38:18Z,"This paper suggests a multiagent system (MAS) approach for market simulation. This is achieved through analysis, modeling, implementation and simulation of artificial markets populated by software agents that represent economic self interested agents. Software agents are the constructs of a complex system, an artificial market that model a real existing market or an outline of a market design. The interest in simulating a market is multiple: exploiting existing market rules, searching for market design flaws and loopholes, and supporting decision making during a market mechanism design process. The main aim of the suggested approach is to analyze the behavior that emerges from the interaction of self interested agents acting in an artificial market. AEMAS (Artificial Economy MultiAgent System), a multiagent system architecture inspired by the Market Oriented Programming (MOP) approach is defined. In different economical sectors, e.g. energy markets, there is no consensus about which structures lead to social welfare maximization outcomes. An approach to find adequate architectures allows different market structure instances to be created and simulated, to ease the design and analysis of alternative structures. These alternatives can then be compared and potential design flaws eventually risen by simulation identified. Taking the electricity market as an example, two instances of the proposed architecture are presented, corresponding to the centralized dispatch arrangement common to non restructured markets, and the auction based pool, common to restructured markets. Copyright 2008 ACM.3438Al-Agtash, S., Evolutionary negotiation strategies in emerging electricity markets (2004) Lecture Notes in Artificial Intelligence, 3070, pp. 1099-1104. , ICAISC 2004Batten, D.F., (2000) Discovering Artificial Economics: How Agents Learn and Economies Evolve, , Westview Press, Boulder, ColoradoBower, J., Bunn, D.W., Experimental analysis of the efficiency of uniform-price versus discriminatory auctions in the England and Wales electricity market (2001) Journal of Economic Dynamics and Control, 25 (3-4), pp. 561-592. , MarBunn, D.W., Oliveira, F.S., Agent-based simulation - An application to the New Electricity Trading Arrangements of England and Wales (2001) IEEE Transactions on Evolutionary Computing, 5 (5), pp. 493-503. , OctF. S. Carvalho and C. D. N. Vinhal. Temporal difference methods applied to thermoelectric energy markets: A distributed multi-agents approach. In XV Congresso Brasileiro de Automática (CBA 2004), Gramado, RS, Sept.21-24 2004(2000) FIPA 2000, , http://www.fipa.org/repository/fipa2000.html, Foundation for Intelligent Physical AgentsGenoud, C., Regulation as a game: The role of independent regulatory agencies in the regulatory process (2003) CARR Risk and Regulation Research Student Conference, , London, UK, Sept, London School of Economics and Political ScienceM. Griss and R. Letsinger. Games at work - Agent mediated e-commerce simulation. In Autonomous Agents 2000, Barcelona, Spain, June 2000. HP Laboratories Technical Report HPL-2000-52Harp, S.A., Brignone, S., Wollenberg, B.F., Samad, T., SEPIA: A simulator for electric power industry agents (2000) IEEE Control Systems Magazine, 20 (4), pp. 53-69. , AugLima, W., Freitas, E.N.A., A multi agent based simulator for Brazilian wholesale electricity energy market (2006) Lecture Notes in Computer Science, 4140, pp. 68-77. , X Ibero-American Artificial Intelligence Conference, XVIII Brazilian Artificial Intelligence Symposium, IBERAMIA '2006/SBIA '2006, of, Ribeirão Preto, SP, Oct, SpringerMonclar, F.-R., Quatrain, R., Simulation of electricity markets: A multi-agent approach (2001) International Conference on Intelligent System Application to Power Systems, pp. 207-212. , Budapest, Hungary, June, IEEE Power Engineering SocietyPraça, I., Ramos, C., Vale, Z., Cordeiro, M., MASCEM: A multiagent system that simulates competitive electricity markets (2003) IEEE Intelligent Systems, 18 (6), pp. 54-60. , Nov-DecSimon, H.A., From substantive to procedural rationality (1979) Philosophy and Economic Theory, pp. 65-86. , F. Hahn and M. Hollis, editors, Oxford University PressSycara, K., Decker, K., Williamson, M., Matchmaking and brokering (1996) Proceedings of the Second International Conference on Multi-Agent SystemsTesfatsion, L., Agent-based computational economics: Growing economies from the bottom up (2002) Artificial Life, 8 (1), pp. 55-82Handbook of Computational Economics: Agent-Based Computational Economics (2006) Handbooks in Economics, 2. , L. Tesfatsion and K. L. Judd, editors, of, North HollandVeit, D., Matchmaking in electronic markets: An agent-based approach towards matchmaking in electronic negotiations (2003) Lecture Notes in Artificial Intelligence, 2882. , J. G. Carbonell and J. Siekmann, editors, SpringerWalter, I., (2006) Sistemas Multiagentes em Mercados de Energia Elétrica, , PhD thesis, Faculdade de Engenharia Elétrica e de Computação, Universidade Estadual de Campinas, DecWalter, I., Gomide, F., Simulação de mercados de energia elétrica: Abordagem multi-agentes (2005) VII SBAI Simpósio Brasileiro de Automação Inteligente, , O. R. Saavedra et al, editors, São Luís, MA, SeptI. Walter and F. Gomide. Design of coordination strategies in multiagent systems via genetic fuzzy systems. Soft Computing, 10(10):903-915, Aug. 2006. Special Issue: New Trends in the Design of Fuzzy SystemsWalter, I., Gomide, F., Genetic fuzzy systems to evolve coordination strategies in multiagent systems (2007) International Journal of Intelligent Systems, 22 (9), pp. 971-991. , Special Issue on Genetic Fuzzy SystemsWellman, M.P., A market-oriented programming environment and its application to distributed multicommodity flow problems (1993) Journal of Artificial Intelligence Research, 1 (1), pp. 1-23. , AugWooldridge, M., Jennings, N.R., Intelligent agents: Theory and practice (1995) The Knowledge Engineering Review, 10 (2), pp. 115-15",,Electricity Market Simulation: Multiagent System Approach,,10.1145/1363686.1363695,,core
296644591,2015-11-26T15:04:48Z,"This paper proposes the use of a nonparametric permutation test to assess the presence of trends in the residuals of multivariate calibration models. The permutation test was applied to the residuals of models generated by principal component regression (PCR), partial least squares (PLS) regression and support vector regression (SVR). Three datasets of real cases were studied: the first dataset consisted of near-infrared spectra for animal fat biodiesel determination in binary blends, the second one consisted of attenuated total reflectance infrared spectra (ATR-FTIR) for the determination of kinematic viscosity in petroleum and the third one consisted of near infrared spectra for the determination of the flash point in diesel oil from an in-line blending optimizer system of a petroleum refinery. In all datasets, the residuals of the linear models presented trends that have been satisfactorily diagnosed by a permutation test. Additionally, it was verified that 500,000 permutations were enough to produce reliable test results. © 2014 Elsevier B.V.1333341Pesarin, F., Salmaso, L., A review and some new results on permutation testing for multivariate problems (2012) Stat. Comput., 22, pp. 639-646Wu, W., Roberts, S.L.L., Armitage, J.R., Tookeb, P., Cordingley, H.C., Wildsmith, S.E., Validation of consensus between proteomic and clinical chemistry datasets by applying a new randomisation F-test for generalised procrustes analysis (2003) Anal. Chim. Acta., 490, pp. 365-378Dejaegher, B., Capron, X., Smeyers-Verbeke, J., Vander Heyden, Y., Randomization tests to identify significant effects in experimental designs for robustness testing (2006) Anal. Chim. Acta., 564, pp. 184-200Van der Voet, H., Comparing the predictive accuracy of models using a simple randomization test (1994) Chemom. Intell. Lab. Syst., 25, pp. 313-323Xu, H., Liu, Z., Cai, W., Shao, X., A wavelength selection method based on randomization test for near-infrared spectral analysis (2009) Chemom. Intell. Lab. Syst., 97, pp. 189-193Lindgren, F., Hansen, B., Karcher, W., Sjöström, M., Eriksson, L., Model validation by permutation tests: applications to variable selection (1996) J. Chemom., 10, pp. 521-532Wold, S., Esbensen, K., Geladi, P., Principal component analysis (1987) Chemom. Intell. Lab. Syst., 2, pp. 37-52Wold, S., Sjöströma, M., Eriksson, L., PLS-regression: a basic tool of chemometrics (2001) Chemom. Intell. Lab. Syst., 58, pp. 109-130Santos, V.O., Oliveira, F.C.C., Lima, D.G., Petry, A.C., Garcia, E., Suarez, P.A.Z., Rubim, J.C., A comparative study of diesel analysis by FTIR, FTNIR and FT-Raman spectroscopy using PLS and artificial neural network analysis (2005) Anal. Chim. Acta., 547, pp. 188-196López-Sánchez, M., Domínguez-Vidal, A., Ayora-Cañada, M.J., Molina-Díaz, A., Assessment of dentifrice adulteration with diethylene glycol by means of ATR-FTIR spectroscopy and chemometrics (2008) Anal. Chim. Acta., 620, pp. 113-119Pinzi, S., Alonso, F., Olmo, J.G., Dorado, M.P., Near infrared reflectance spectroscopy and multivariate analysis to monitor reaction products during biodiesel production (2012) Fuel, 92, pp. 354-359Nicolaou, N., Xu, Y., Goodacre, R., Fourier transform infrared and Raman spectroscopies for the rapid detection, enumeration, and growth interaction of the bacteria Staphylococcus aureus and Lactococcus lactis ssp. cremoris in milk (2011) Anal. Chem., 83, pp. 5681-5687Teixeira, L.S.G., Oliveira, F.S., Santos, H.C., Cordeiro, P.W.L., Almeida, S.Q., Multivariate calibration in fourier transform infrared spectrometry as a tool to detect adulterations in Brazilian gasoline (2008) Fuel, 87, pp. 346-352Cortes, C., Vapnik, V., Support-vector networks (1995) Mach. Learn., 20, pp. 273-297Smola, A.J., Schölkopf, B., A tutorial on support vector regression (2004) Stat. Comp., 14, pp. 199-222Schölkopf, B., Sung, K.K., Burges, C.J.C., Girosi, F., Niyogi, P., Poggio, T., Vapnik, V., Comparing support vector machines with gaussian kernels to radial basis function classifiers (1997) IEEE Trans. Neural Networks, 11, pp. 2758-2765Filgueiras, P.R., Alves, J.C.L., Poppi, R.J., Quantification of animal fat biodiesel in soybean biodiesel and B20 diesel blends using near infrared spectroscopy and synergy interval support vector regression (2014) Talanta, 119, pp. 582-589Geladi, P., MacDougall, D., Martens, H., Linearization and Scatter-Correction for Near-Infrared Reflectance Spectra of Meat (1985) Appl. Spectrosc., 39, pp. 491-500Filgueiras, P.R., Sad, C.M.S., Loureiro, A.R., Santos, M.F.P., Castro, E.V.R., Dias, J.C.M., Poppi, R.J., Determination of API gravity, kinematic viscosity and water content in petroleum by ATR-FTIR spectroscopy and multivariate calibration (2014) Fuel, 116, pp. 123-130(2007) Standard Test Method for Dynamic Viscosity and Density of Liquids by Stabinger Viscometer (and the Calculation of Kinematic Viscosity), D7042-04, Vol. 05.06, , ASTM International, West Conshohocken, Pennsylvania, USA, Annual Book of ASTM StandardsAlves, J.C.L., Henriques, C.B., Poppi, R.J., Determination of diesel quality parameters using support vector regression and near infrared spectroscopy for an in-line blending optimizer system (2012) Fuel, 97, pp. 710-717(2010) Standard Test Method for Flash Point by Tag Closed Cup Tester, D56-05, , ASTM International, West Conshohocken, Pennsylvania, USA, Annual Book of ASTM StandardsMelvik, B.H., Cederkvist, H.R., Mean squared error of prediction (MSEP) estimates for principal component regression (PCR) and partial least squares regression (PLSR) (2004) J. Chemom., 18, pp. 422-429Wise, B.M., Gallagher, N.B., Bro, R., Shaver, J.M., Windig, W., Koch, R.S., (2006) PLS toolbox version 6.7 for use with Matlab, , Eigenvector research Inc., WenatcheeChang, C.C., Lin, C.J., LIBSVM: a library for support vector machines, software, , http://www.csie.ntu.edu.tw/cjlin/libsvm, available atValderrama, P., Braga, J.W.B., Poppi, R.J., Variable selection, outlier detection, and figures of merit estimation in a partial least-squares regression multivariate calibration model. A case study for the determination of quality parameters in the alcohol industry by near-infrared spectroscopy (2007) J. Agric. Food Chem., 55, pp. 8331-8338(2005) Standards Practices for Infrared, Multivariate, Quantitative Analysis, E1655-05, vol. 03.06, , ASTM International, West Conshohocken, Pennsylvania, USA, Annual Book of ASTM Standard",Elsevier,Evaluation Of Trends In Residuals Of Multivariate Calibration Models By Permutation Test,,10.1016/j.chemolab.2014.02.002,,core
56773884,2014,"Manufacturing sites in developed countries can only exist in the long run if their production processes are automated. Prerequisite for this automation is the existence of reliable monitoring systems to detect and fix unfavorable process states. Monitoring systems capture sensor data from the observed process. The current process state is deduced from this data, either based on fixed rules (thresholds, envelopes...) or by the means of artificial intelligence. Intelligent process monitoring systems are considered to be the more powerful type. Currently the creation of intelligent process monitoring systems for a given manufacturing method is time-consuming and requires a high degree of expert knowledge. This is a major barrier for a comprehensive application of such systems. This thesis presents a method for the automatic creation of intelligent process monitoring systems for arbitrary cyclic production processes. These production processes can be described by a set of training data. Each data record in this training data set contains both: data that has been measured in a single cycle of the production process, and the desired prediction result for this production cycle. Based on these training data, the new method generates a process monitoring system that is able to assign predictions even to such data records that have not been part of the training data set. For the realization of the proposed system, a generic process monitoring system was designed and implemented. This system provides the infrastructure for data acquisition and the data streams required for the generated monitoring systems. In the first stage, the generic system does not hold any program logic for processing and evaluating the acquired data. This logic is provided by an external analysis model. Such a model is a processing chain integrating methods for signal preprocessing, feature generation, feature selection and classification. By setting the analysis model, the generic process monitoring system can be adapted to any manufacturing process. With the concept described above, the creation of a process monitoring system for a manufacturing process can be reduced to an optimization problem. The goal is to find the analysis model that adapts best the generic monitoring system to the given manufacturing process. To solve this optimization problem, a heuristic optimization algorithm named Artificial Bee Colony Optimization is applied. For the method proposed in this thesis, the original Artificial Bee Colony Optimization was adapted to handle non-real valued problem spaces","Fraunhofer Verlag, Stuttgart",Ein Verfahren zur automatischen Erzeugung intelligenter Prozessüberwachungssysteme,,,,core
42981056,2015-04-01T00:00:00,"Building Information Modeling (BIM) is playing a significant role in the development of Construction industry.Evaluation of BIM software selection is one of the key roles in successfully BIM adoption.Currently, there is limited study on BIM software selection. With a great potential for integration of MADM and the current Web 2.0

technology, the development of Web DSS based on TOPSIS is desired to solve this problem. In order to develop an effective DSS, the development of subsystem which is TOPSIS would be integrated with fuzzy element.The proposed of this integration is to deal with the vagueness of decision makers in order to evaluate and rating the software and attributes of BIM software selection.Inteads of use crips value, the decision maker will asked to weight and rating through linguistics.For example Very Low (VL), Low (L), Medium Low (ML), Medium (M), Medium High (MH), High (H) and Very High

(VH) will used for wighting asessement in BIM software selection.In order to demostrade this proposed DSS, a real construction project which UTHM Multipurpose hall will be deploy",AENSI Journals,Conceptual design of fuzzy TOPSIS DSS for building information modeling (BIM),https://core.ac.uk/download/42981056.pdf,,,core
296634503,2015-11-26T14:46:19Z,"Current high-performance multicore processors provide users with a non-uniform memory access model (NUMA). These systems perform better when threads access data on memory banks next to the core where they run. However, ensuring data locality is difficult. In this paper, we propose compiler analyses and code generation methods to support a lightweight runtime system that dynamically migrates memory pages to improve data locality. Our technique combines static and dynamic analyses and is capable of identifying the most promising pages to migrate. Statically, we infer the size of arrays, plus the amount of reuse of each memory access instruction in a program. These estimates rely on a simple, yet accurate, trip count predictor of our own design. This knowledge lets us build templates of dynamic checks, to be filled with values known only at runtime. These checks determine when it is profitable to migrate data closer to the processors where this data is used. Our static analyses are quadratic on the number of variables in a program, and the dynamic checks are O(1) in practice. Our technique does not require any form of user intervention, neither the support of a third-party middleware, nor modifications in the operating system's kernel. We have applied our technique on several parallel algorithms, which are completely oblivious to the asymmetric memory topology, and have observed speedups of up to 4x, compared to static heuristics. We compare our approach against Minas, a middleware that supports NUMA-aware data allocation, and show that we can outperform it by up to 50% in some cases. © 2014 ACM.369380ACM SIGARCH,IEEE Computer Society,IFIPAppel, A.W., Palsberg, J., (2002) Modern Compiler Implementation in Java, 2nd Ed., , Cambridge University PressAwasthi, M., Nellans, D.W., Sudan, K., Balasubramonian, R., Davis, A., Handling the problems and opportunities posed by multiple on-chip memory controllers (2010) PACT. ACM, pp. 319-330Basu, S., Pollack, R., Roy, M.-F., (2006) Algorithms in Real Algebraic Geometry, , SpringerBlagodurov, S., Zhuravlev, S., Fedorova, A., Kamali, A., A case for NUMA-aware contention management on multicore systems (2010) PACT. ACM, pp. 557-558Borin, E., Devloo, P., Programming finite element methods for ccNUMA processors (2013) Int. Conference on Parallel, Distributed, Grid and Cloud Computing for Engineering, , Civil-Comp PressBroquedis, F., Furmento, N., Goglin, B., Wacrenier, P.-A., Namyst, R., ForestGOMP: An efficient OpenMP environment for NUMA architectures (2010) Inter. J. Parallel Programming, 38 (5-6), pp. 418-439Broquedis, F., Clet-Ortega, J., Moreaud, S., Furmento, N., Goglin, B., Mercier, G., Thibault, S., Namyst, R., hwloc: A generic framework for managing hardware affinities in HPC applications (2010) PDP. IEEE, pp. 180-186Castro, M., Góes, L.F.W., Ribeiro, C.P., Cole, M., Cintra, M., Méhaut, J.-F., A machine learning-based approach for thread mapping on transactional memory applications High Performance Computing Conference (HiPC). Bangalore, India: IEEE, 2011, pp. 1-10Chatterjee, S., Parker, E., Hanlon, P.J., Lebeck, A.R., Exact analysis of the cache behavior of nested loops (2001) PLDI. ACM, pp. 286-297Cruz, E., Diener, M., Navaux, P., Using the translation lookaside buffer to map threads in parallel applications based on shared memory IPDPS, 2012, pp. 532-543Cytron, R., Ferrante, J., Rosen, B.K., Wegman, M.N., Zadeck, F.K., Efficiently computing static single assignment form and the control dependence graph (1991) TOPLAS, 13 (4), pp. 451-490Diener, M., Cruz, E., Navaux, P., Communication-based mapping using shared pages IPDPS, 2013, pp. 700-711Dijkstra, E.W., A note on two problems in connexion with graphs (1959) Numerische Mathematik, 1, pp. 269-271Dupros, F., Ribeiro, C.P., Carissimi, A., Méhaut, J.-F., Parallel simulations of seismic wave propagation on NUMA architectures (2009) PARCO. IOS, pp. 67-74Dykema, G.L., Bassett, D.H., Lach, J.L., Mechanisms for synchronizing data transfers between non-uniform memory architecture computers (2012), 8 (244), p. 930. , US PatentFerrante, J., Ottenstein, J., Warren, D., The program dependence graph and its use in optimization (1987) TOPLAS, 9 (3), pp. 319-349Goglin, B., Furmento, N., Enabling high-performance memory-migration in Linux for multithreaded applications (2009) MTAAP. IEEEJoyner, D., Čertík, O., Meurer, A., Granger, B.E., Open source computer algebra systems: SymPy Commun. Comput. Algebra, 45 (3-4), pp. 225-234+2012. , ACMKale, L.V., Bhatele, A., (2013) Parallel Science and Engineering Applications: The Charm++ Approach, , Taylor & Francis Group, CRC Press, NovLameter, C., An overview of non-uniform memory access (2013) Commun. ACM, 56 (9), pp. 59-154Lattner, C., Adve, V.S., LLVM: A compilation framework for lifelong program analysis & transformation (2004) CGO. IEEE, pp. 75-88Li, Y., Melhem, R., Abousamra, A., Jones, A., Compiler-assisted data distribution for chip multiprocessors (2010) PACT. ACM, pp. 501-512Löf, H., Holmgren, S., Affinity-on-next-touch: Increasing the performance of an industrial PDE solver on a cc-NUMA system (2005) ICS. ACM, pp. 387-392Pilla, L.L., Ribeiro, C.P., Coucheney, P., Broquedis, F., Gaujal, B., Navaux, P.O., Méhaut, J.-F., A topology-aware load balancing algorithm for clustered hierarchical multi-core machines Future Generation Computer Systems, 30, pp. 191-201+2014. , no. 0Ribeiro, C.P., (2011) Contributions on Memory Affinity Management for Hierarchical Shared Memory Multi-core Platforms, , Ph.D. dissertation, University of GrenobleRibeiro, C.P., Méhaut, J.-F., Carissimi, A., Memory affinity management for numerical scientific applications over multi-core multiprocessors with hierarchical memory IPDPS Workshops. IEEE, 2010, pp. 1-4Tang, L., Mars, J., Zhang, X., Hagmann, R., Hundt, R., Tune, E., Optimizing Google's warehouse scale computers: The NUMA experience (2013) HPCA. IEEETikir, M.M., Hollingsworth, J.K., Using hardware counters to automatically improve memory performance (2004) Supercomputing. IEEE, pp. 46-46Wittmann, M., Hager, G., (2011) Optimizing CcNUMA Locality for Task-parallel Execution under OpenMP and TBB on Multicore-based SystemsWolf, M.E., Lam, M.S., A data locality optimizing algorithm (1991) PLDI. ACM, pp. 30-44Wolfe, M., (1996) High Performance Compilers for Parallel Computing, 1st Ed., , Adison-Wesle",Institute of Electrical and Electronics Engineers Inc.,Compiler Support For Selective Page Migration In Numa Architectures,,10.1145/2628071.2628077,,core
30733152,2015-04-01T00:00:00,"Ontology organises the things that was used to

consist of corpus for the real world. Ontology

constructs the model of information systems in

term of taxonomy in a wide range of subject

areas from social science and natural science.

Ontology defines a large number of objects for

a wide range of applications, such as education,

healthcare, medicine, engineering and

manufacturing. Ontology is underpinned by the

theories of formal language, classification and

automata languages, and can be implemented

by the natural language process, particularly

involving the tools and technologies in artificial

intelligence. Ontology made significant

contribution to the computational science, especially

in information retrieval/extraction and

visualisation from the theory to practice. The

challenge ahead for ontology is to prove even

more useful and effective in an even broader

range of application domains. It follows that

ontology made this issue special",'IGI Global',Editorial : Special Issue on Ontology and Innovation: Part1,,,,core
250576392,2015-11-11T00:00:00,"International audienceThe International Workshop on Principles of Diagnosis (DX) is an annual event that started in 1989 and is rooted in the Artificial Intelligence (AI) community. This annual international workshop has been uniting researchers and practitioners with diverse backgrounds (Artificial Intelligence (AI), verification, software engineering, debugging, ...) in order to leverage research in the global context of diagnosis, that is, identifying the root causes for encountered issues. The DX Workshop series has been offering a forum to present current research and experience reports, exchange and discuss emerging ideas, as well as debate current issues and envisioned future challenges.Papers presented at the workshop cover a variety of theories, principles, and computational techniques for diagnosis, monitoring, testing, reconfiguration, fault- adaptive control, and repair of complex systems. Applications of these theories, principles, and techniques to industry-related disciplines and other real-world problems are also important topics of the workshop.Like the previous workshops in this series, DX-2015 encourages the interactions and the exchange of theories, techniques, applications, and experiences amongst researchers and practitioners from diverse backgrounds – artificial intelligence, control theory, verification, software and systems engineering, and other related areas – who share an interest in different aspects of diagnosis, and the related fields of testing, reconfiguration, maintenance, prognosis, and fault-adaptive control.DX is a lively forum that has traditionally adopted a single track program with a limited number of participants in order to promote detailed technical exchange and debate while at the same time making efforts to develop synergistic approaches to solving real-world problems",HAL CCSD,Proceedings of the 26th International Workshop on Principles of Diagnosis (DX-2015),,,,core
54655008,2015,"Managing and optimising cloud services is one of the main challenges faced by industry and academia. A possible solution is resorting to self-management, as fostered by autonomic computing. However, the abstraction layer provided by cloud computing obfuscates several details of the provided services, which, in turn, hinders the effectiveness of autonomic managers. Data-driven approaches, particularly those relying on service clustering based on machine learning techniques, can assist the autonomic management and support decisions concerning, for example, the scheduling and deployment of services. One aspect that complicates this approach is that the information provided by the monitoring contains both continuous (e.g. CPU load) and categorical (e.g. VM instance type) data. Current approaches treat this problem in a heuristic fashion. This paper, instead, proposes an approach, which uses all kinds of data and learns in a data-driven fashion the similarities and resource usage patterns among the services. In particular, we use an unsupervised formulation of the Random Forest algorithm to calculate similarities and provide them as input to a clustering algorithm. For the sake of efficiency and meeting the dynamism requirement of autonomic clouds, our methodology consists of two steps: (i) off-line clustering and (ii) on-line prediction. Using datasets from real-world clouds, we demonstrate the superiority of our solution with respect to others and validate the accuracy of the on-line prediction. Moreover, to show the applicability of our approach, we devise a service scheduler that uses the notion of similarity among services and evaluate it in a cloud test-bed",'Institute of Electrical and Electronics Engineers (IEEE)',Service Clustering for Autonomic Clouds Using Random Forest,,10.1109/CCGrid.2015.41,,core
199468428,2016-12,"4??? ???????????? ????????? ???????????????????????? ?????? ???????????? ????????? ????????? ???????????? ??????????????? ????????? ????????? ????????? ??????????????? ???????????? ????????? ???????????? ???????????? ??????????????? ??????. ?????? ?????????????????? ??????????????? ?????? ??????(innovation)?????????, ?????? ??????????????? ????????? ????????????????????? ????????? ?????? ????????? ?????? ????????? ????????? ???????????? ?????? ????????????????????????????????? ????????? ????????? ???????????? ?????????. ??????????????? ????????? ?????? ???????????? ?????? &lsquo;????????????&rsquo;??? &lsquo;???????????????&rsquo;(posthuman)??? ?????????, ????????? &lsquo;??????&rsquo;(human) ????????? ????????? ???????????? ????????? ??????????????? ?????? ????????? ??????????????? ????????? ???????????? ???????????? ????????? ?????? ????????? ?????? ?????? ????????? ?????? ???????????? ?????? ????????? ?????? ????????? ????????? ????????? ???????????? ??????.
???????????????????????? ??????????????? ???????????? ????????????, ???????????? ??? ?????? ????????? ????????? ??????????????? ?????? ????????????. ?????? ????????? ????????? ???????????? ???????????? ??? ?????? ??????, ?????????????????? ????????? ????????? ??????????????? ????????? ????????????. ????????? ????????? ??? ?????? ???????????? ???????????? ?????? ???????????? ??? ??? ?????????, ?????? ???????????????????????? ?????? ????????? ??????????????? ???????????? ????????? ??? ?????????, ???????????? ???????????? ????????? ???????????? ???????????? ?????? ????????? ????????? ???????????? ????????? ???????????? ?????? ?????? ????????????, ?????? ???????????? ????????? ????????? ?????? ??????????????? ?????? ????????? ??? ?????? ????????? ???????????? ??????, ?????? ??? ????????? ??????????????? ???????????? ?????? ????????? ???????????? ???????????? ????????? ????????? ????????? ????????? ???????????? ??????&middot;???????????? ????????? ???????????? ??????.
????????? ???????????? ???????????????????????? ??? ??????????????? ??????????????? ?????? ????????? ????????? ??????(device)??? ??????????????? ?????? ?????????, ???????????????????????? ?????? ????????? ????????? ????????? ???????????? ?????? ??????????????? ???????????? ???????????????, ????????? ?????????????????? ???????????? ?????? ????????????(connected) ?????? ??? ????????? ??????????????? ????????????????????????????????? ???????????? ?????? ?????????, ???????????? ????????? ????????? ????????? ???????????? ???????????? ?????? ????????? ????????? ???????????? ????????? ??? ??? ??????. ????????? ???????????? ?????? ????????? ????????? ????????? ?????????????????? ??????????????? ????????? ????????? ????????? ????????? ???????????? ????????? ??????????????? ????????? ??????????????? ??????????????? ??????????????? ?????????????????? ????????? ????????? ???????????? ??????. ?????? ???????????? ?????????????????? ??????????????? ??? ?????? ??????, ????????????, ??????????????? ????????? ?????? ??????, ????????? ?????? ??????????????? ?????????, ????????????????????? ???????????? ??????????????? ????????? ????????????, ?????? ??????????????? ????????????, ???????????? ????????? ???????????? ????????? ????????? ?????? ????????? ????????? ??????????????? ???????????? ??? ??????. ????????? ????????? ?????? ????????? ??? ???????????? ????????? ????????? ?????? ???????????? ?????? ????????? ????????? ??? ??????, ??????????????? ??????????????? ??????????????? ?????? ??????????????? ????????? ??? ?????? ?????????, ????????? ????????? ?????? ?????? ???????????? ????????? ???????????? ????????? ????????? ????????? ????????? ?????? ???????????? ??????????????? ????????? ????????? ????????????.
????????? ???????????? ????????? ??????????????? ???????????? ??????????????? ??????. ??????????????? ?????? ????????? ????????? ????????? ????????? ???????????? ????????? ??????????????? ??? ??????????????? ????????? ????????? ???????????? ???????????? ???????????????????????? ????????? ????????? ????????? ?????? ????????? ????????? ?????????  ????????? ????????? ????????? ????????? ??? ?????? ?????????.


Concept of Automated Vehicle in the era of the fourth industrial revolution is transforming from the `vehicle` that had its focal point in the hardware to the `portable computer` that has its focus in the software that processes broad range of digital information. The phenomenon in one aspect is innovation through scientific technology. However on the other hand, it is raising humanistic, philosophical and legal questions about how human life will change according to the technological and industrial transition. Age of `Artificial Intelligence` and `Posthuman` emerging through the development of technology is creating new concept of human and bringing forward the issue of new danger that is occurring when reason and physical abilities thought as the essence of `human` is being substituted by scientific technology and computer. The advocates of automated vehicle assert that it provides more safety and convenience to human. Such argument cannot be completely be false, and I wish to reach the same conclusion eventually. However, as other process of introducing new technology like nuclear energy tell us, to consider whether automated vehicle is more convenient and better than the previous vehicle system and whether there are no new danger that new technology is bringing about or overlooked elements of danger is severely important. Therefore, if certain danger is expected, technological prevention should be devised as a first step, and for the sections where technology and industry is not self-regulating, it is legislations and policies` role to lead and force to a human-friendly direction. Automated vehicle that is being discussed nowadays can be neutral itself and seem to be no more than a convenient device. However, since each automated vehicle is activated by entering massive amount of digital information and since it collects, stores, and distributes a welter of personal information when connected to Internet of Things(IoT), whole new danger from the previous era is created in terms of personal information and privacy. The ironic reality of surveillance society since the modern times that has assured freedom of an individual on the one hand and has reinforced utilitarian society control on the grounds of fully protecting one`s right and expanding welfare on the other lies beneath this near future. Especially, personal information is more being exposed and distributed and collection and surveillance of digital information is becoming easier in the information age. Automated vehicle system cannot be free from such exposure and danger of violating since it is operated with geolocation, video, and communication information as well. Moreover, since violation of personal information can lead to violation of privacy and human dignity, and damage the basic order of liberal democracy, arranging the frame of public regulation relating to personal information protection by researching U.S. and European Union legislation and seems more than necessary. Being awake is what sovereign needs in the digital era. Likewise, to secure right to personal information, we should remember to turn the monitoring lamp on in advance not only as an individual, but also as a group in order to obtain convenience and prevent danger of new era of automated vehicle.??? ????????? ??????????????? ?????????????????????????????? ???????????????(HY-2014??????)",????????????????????????,Public Law Perspectives on Personal Information Protection in the Era of Automated Vehicle,,,"[{'title': None, 'identifiers': ['1227-0954', 'issn:1227-0954']}]",core
103085415,2015-11-01,"Abstract- In the modern world class manufacturing it is of prime importance for modern management to make crucial decisions quickly and accurately to stand at the global competitive cutting edge. In many real life situations the main interest concern the prediction of how a system will perform under various conditions of change in the environment as well as with in the system. Experimenting on real system is not always feasible, so it is carried on some representative unit of the system. This unit is called a model and the process modeling. Simulation is a technique of systems modeling and analysis that involves mathematical models of a dynamic nature which are solved numerically. Simulation is a powerful and scientific method, which is widely, applied methodology for studying the behaviour of a variety of systems in order to develop solutions to problems in their design and operation. An overview of simulation modeling and analysis is recent advancements in this field, recommendations for selecting right simulation software, related technologies like artificial intelligence techniques, how they are integrated with computer simulation modeling and benefits due to development of these hybrid technologies.Computer simulation is one of the popular experimental investigation techniques as it involved reduced costs, time and risks compared to experimenting decision alternatives with real world system in real time. In the computer simulation, developing the models of the real systems on the computer has carried out experimentation. The use of simulation mathematical models has been proposed to reduce the computer costs of simulation while making use of its potential of predicting the performance of complex system. Key words: Simulation, modeling, manufacturing process I",,Simulation and Modeling Analysis in Manufacturing Process R.Uday Kumar,,,,core
77521365,2016-08-09T21:15:00,"With support from DARPA and the Operationally Responsive Space (ORS) Office a new approach to conventional spacecraft mission assurance has been developed that has the potential to revolutionize current practices. The goal of this new approach, coined Digital Assurance (DA), is to provide decision makers with real-time, quantified information at any level of the program, including continuous, live custody of its comprising components. Digital Assurance will enable reduction in program costs and time associated with conventional mission assurance approaches. DA represents the intelligent integration of the digital design environment with the digital manufacturing environment to achieve unprecedented levels of knowledge about the physical configuration of a satellite.
The foundation for the successful implementation of DA is the concept of Continuous Custody and a Graph Database. Continuous Custody takes advantage of the physical reality that the majority of spacecraft Assembly, Integration, and Test (AI&T) takes place in a very well-defined and well-controlled physical environment, allowing capture of a broad scope of environmental factors",DigitalCommons@USU,Digital Assurance: Empowering Decision Makers in the Digital Age,https://core.ac.uk/download/77521365.pdf,,,core
155733826,2016,"Deep learning has been applied for processing programs in recent years and gains extensive attention on the academic and industrial communities. In analogous to process natural language data based on word embeddings, embeddings of tokens (e.g. classes, variables, methods etc.) provide an important basis for processing programs with deep learning. Nowadays, lots of real-world programs rely on API libraries for implementation. They contain numbers of API tokens (e.g. API related classes, interfaces, methods etc.), which indicate notable semantics of programs. However, learning embeddings of API tokens is not exploited yet. In this paper, we propose a neural model to learn embeddings of API tokens. Our model combines a recurrent neural network with a convolutional neural network. And we use API documents as training corpus. Our model is trained on documents of five popular API libraries and evaluated on a description selecting task. To our best knowledge, this paper is the first to learn embeddings of API tokens and takes a meaningful step to facilitate deep learning based program processing.CPCI-S(ISTP)luyy@pku.edu.cn; lige@pku.edu.cn; miaorui@pku.edu.cn; zhijin@pku.edu.cn527-539998","9th International Conference on Knowledge Science, Engineering, and   Management (KSEM)",Learning Embeddings of API Tokens to Facilitate Deep Learning Based   Program Processing,,10.1007/978-3-319-47650-6_42,"[{'title': None, 'identifiers': ['0302-9743', 'issn:0302-9743']}]",core
234828572,2015,"Mastering the finest art of ‘mechatronics’ currently looks one of the most attractive task of modern engineering technology and

science. Many are the applications which resort to the interdisciplinary approach of mechatronics to enhance the performance, quality

and safety of either product or process. Some are very traditional, like hard disk drives, biomedical, automotive and aerospace

systems, other are fairly new like micro and nano electromechanical systems, unmanned air vehicles, intelligent machining and

manufacturing systems or bioinspired devices. A first generation of mechatronic products was conceived to embed a suitable

‘smartness’ to improve the skill of self–adapting to any abrupt variation of operating conditions, by resorting to the ‘synergistic

integration of mechanical engineering with electronics and control in the design and manufacturing of product process’ as

mechatronics was brightly defined. Nowadays, a mechatronic design is surely based on its interdisciplinary nature, but its real

meaning was harmonized with an effective contamination among different application domains, methodologies and technologies,

being smartly applied to reach the highest result in any product, system and process development. A recent experience within the

frame of the EMEA District of the American Society of Mechanical Engineers (ASME) was a chance to get an impression of the

scientific and industrial research activity performed in some fields of mechatronics. Some exciting examples describing how different

competences, disciplines, technologies met in an innovative mechatronic system are herein exposed by some researchers of the EMEA

area of the world. They deal with several domains, like the hard disk drive technology, biomedical prostheses, fluidic automation,

UAV Vision System, vibration monitoring and suppression in steelmaking plants, materials machining and smart composites. These

examples will narrate to the reader who is still looking for the meaning of mechatronics how some approaches, as neural network

positioning control, chaos prevention, myoelectric stimulation of prosthesis, human detection by vision system, multi-physics

modeling and control of dynamics are currently implemented in a sort of artificial intelligence in small scale device, as in a finger of a

biotronic hand or in a large equipment like an electric arc furnace. Moreover, the reader will realize how intensively this goal is

achieved by exploiting the available technologies as additive manufacturing or fiber optics embedded into composite structures to

reduce the cost, weight or volume of the product or to improve the quality and accuracy of a material processing like in rolling or in

turning against the risk of self–excited chatter vibration. This scenario is covering a wide range of mechatronic applications, although

many others are currently developed in several fileds of engineering","NOVA Science Publishers, Inc","Mechatronics: Principles, Technologies and  Applications",,,,core
101124176,2015-01-12,"industry innovation, centralized computer consoles as the operating base for beneficiation plants. Soon after, a development was started for bringing a new concept of on-stream analysis into practice to supply real-time data on rougher flotation BPL to the control consoles enabling quick recognition of process changes. &quot;BPL &quot; is &quot;bone phosphate of lime&quot;, the industry term for mineral grade. Both programs have become successfully implemented. The pay-off is shown in graphical illustrations below-- up to five per cent gains in production. Improved product value can be measured in several million dollars per year for a typical mine beneficiation plant. Continued beneficiation plant control improvements are planned at PCS Phosphate- White Springs toward applying expert systems control techniques linked to artificial intelligence and other advantageous technologies available in current state-of-the-art. High-points of operating efficiencies-illustrated by peak values in Figure 3, &quot;Increased Recovery &quot;- will be approached more consistently with use of advanced control techniques to produce further gains. KEY FACTORS IN ACHIEVING PRODUCTIVITY GAINS A first factor essential to implementing an innovative productivity improvement program is recognition by management that risks implied in the work are worth taking because gains are potentiall",,PCS PHOSPHATE WHITE SPRINGS AUTOMATIC CONTROL AND ON-STREAM ANALYSIS INNOVATIONS HAVE PAY-OFF IN BIG GAINS,,,,core
33736225,2015-09-11T00:00:00,"Drug discovery and development is a long process: it takes usually 12 to 15 years before a drug candidate reaches the market. The pharmacokinetics of the drug is an important aspect of drug discovery and development, because the drug must reach its target site and exert the therapeutic response. The pharmacokinetic parameters of new compounds should be investigated early in drug discovery. Pharmacokinetic predictions can be made with Quantitative Structure-Property Relationships (QSPR) which are computational models that correlate chemical features with pharmacokinetic properties. The correlations are based on in vivo or in vitro pharmacokinetic data and molecular descriptors. QSPR models can be used to predict the pharmacokinetic parameters even before any actual drug synthesis and can be exploited to guide drug discovery. Pharmacokinetic models can also simulate concentration profiles of drugs during the drug discovery and development process. It was decided to develop QSPR models of pharmacokinetic parameters of drugs to be delivered by the systemic or ocular routes. A combination of Principal Component Analysis and Partial Least Square multivariate statistical methods was used to obtain QSPR equations for volume of drug distribution and fraction of unbound drug in plasma. Parallel modelling of these parameters resulted in acceptable R2 (0.58 - 0.77) and Q2 values (0.55 - 0.58). These models are based on a large set of structurally unrelated compounds, they are open and they have a defined applicability domain. Charge and lipophilicity related descriptors were the relevant ones which influenced the volume of distribution and free fraction of drug in plasma. Pharmacokinetics is an important factor in the development of ocular medications, because the ocular drug targets are difficult to reach, particularly in the posterior tissues such as retina and choroid. Therefore, drugs need to be injected intravitreally in the treatment of retina and choroid diseases (e.g. in exudative age-related macular degeneration) and thus prediction of intravitreal pharmacokinetics would be especially advantageous in ocular drug discovery and development. The first comprehensive collection of intravitreal volume of distribution and clearance values of compounds was collated based on extensive rabbit eye data from the literature. Moreover, predictive QSPR models for intravitreal clearance and half-life were created which had R2 and Q2 values of 0.62   0.84 for clearance and 0.61 - 0.80 for half-life. LogD7.4 and hydrogen bonding capacity defined the intravitreal clearance and half-life of compounds with a molecular weight below 1500 Da. The intravitreal volumes of drug distribution lay within a narrow range (80% within 1.18 - 2.28 ml). The QSPR models for intravitreal clearance and the typical values for intravitreal volumes of distribution were implemented in pharmacokinetic simulation models; the simulated profiles based on the real and predicted pharmacokinetic parameter values were similar. Thus, a combination of QSPR and pharmacokinetic models can be used in drug discovery and development to aid in the design of drugs and drug delivery systems. A comprehensive comparison of intravitreal pharmacokinetic data between rabbit and human was carried out to clarify the translational value of the rabbit model. The analysis revealed that the rabbit can be considered as a clinically predictive animal model for intravitreal pharmacokinetics of small molecules (18 Da - 1500 Da) and macromolecules (7.1 kDa - 149 kDa). There was a correlation between the intravitreal clearance values in human patients and healthy rabbits; they showed similar, but not identical, absolute values. The intravitreal pharmacokinetics of small molecules is mainly governed by permeability-limited clearance across blood-ocular barriers and occurs via the posterior route, whereas large molecules are cleared mostly via the anterior route. Although the literature contains some claims about the significance of the viscosity of the vitreous, it seems that this is not a major factor in drug elimination from the eye. In conclusion, new in silico tools were generated for systemic and ocular pharmacokinetics and drug delivery. These models can be exploited in industrial drug discovery and will hopefully speed up the development of new medications.Silmätaudeissa lääkehoitoa vaikeuttaa se, että lääkehoitoa on vaikea saattaa perille silmänpohjaan verkkokalvon soluihin, joissa näkövammaisuuteen ja sokeutumiseen johtavat muutokset tavallisesti tapahtuvat.  Näin ollen lääkkeitä joudutaan antamaan silmän sisään toistuvina injektioina esimerkiksi verkkokalvon ikärappeuman hoidossa. Lääkkeiden kulkeutumisen ymmärtäminen ja ennustaminen tietokoneella auttaa pitkävaikutteisten injektioiden ja vaihtoehtoisten lääkkeen antotapojen kehittämistä. Väitöskirjassa kehitettiin tällaisia tietokonemalleja pohjautuen julkaistuihin tutkimuksiin",'University of Helsinki Libraries',Ocular and systemic pharmacokinetic models for drug discovery and development,https://core.ac.uk/download/33736225.pdf,,,core
54526124,2015-01-01T00:00:00,"Le analisi per la selezione degli investimenti aziendali segnano uno dei momenti più critici per la vita di ogni azienda in quanto si è in procinto di scelte di impiego durevole del capitale disponibile in progetti rilevanti per l’impresa e che, solitamente, la condizioneranno per periodi non brevi. Nella prospettiva finanziaria, gli investimenti sono ricondotti ai fabbisogni finanziari che generano, ai flussi di cassa attesi e alle possibili variazioni dei livelli di rischiosità aziendale: in definitiva, ogni investimento interessa per l’apporto positivo al valore aziendale. Eventuali errori valutativi in fase di analisi dei progetti di investimento possono essere addirittura fatali per l’impresa: in caso di fallimento di un progetto, le risorse impiegate non sono facilmente recuperabili.

La selezione degli investimenti aziendali, riguardino il rinnovo o l’acquisizione di nuovi cespiti industriali, commerciali o amministrativi, deve sempre avvenire secondo criteri e modelli di valutazione finanziari. I piani finanziari dell’impresa comunicati al mercato hanno già fissato le coordinate finanziarie nell’ambito delle quali il management deve muoversi (rendimento atteso proporzionale ai rischi quotati dai mercati; orizzonte di pianificazione e di impiego del capitale; struttura dell’attivo e struttura finanziaria; equilibri patrimoniali e finanziari di massima).  

In questo quadro, il capital budgeting conduce a decisioni operative (non più strategiche) e mira ad un uso razionale del capitale che è o sarà disponibile, nell’ambito di risultati attesi già ponderati e premi di rischio coerenti. Malgrado i vincoli sembrino stringenti, l’attività contribuisce alla creazione di valore aziendale quando, a parità di altre condizioni, si riesce ad individuare una specifica soluzione di investimento: particolarmente conveniente per il risparmio di risorse economiche che può generare rispetto altre combinazioni produttive; che richiede minori impieghi di capitale; che consente di generare più pingui flussi di cassa in ragione della maggiore qualità o produttività; in grado di contenere i rischi connessi all’uso più di altre soluzioni; dotata  di tempi di obsolescenza prolungati rispetto alternative tecnologiche. Su tali specifici aspetti fanno luce corrispondenti famiglie di indicatori: indicatori di efficienza; indicatori di redditività economica; indicatori di durata di recupero degli esborsi; indicatori di rischiosità.

Seguendo ciascun tipo di vantaggio, e rispettivo indicatore, la graduatoria degli investimenti preferibili – tra quelli comparabili e perciò alternativi – sembra essere molto chiara; talvolta però le indicazioni sono discordanti e ogni investimento ha diversi motivi di preferibilità.

Così procedendo, la prassi operativa annovera, di fatto, l’uso di tecniche alternative di valutazione degli investimenti: occorre rendersi conto della valenza di ciascun suggerimento. 

Solo il valore attuale netto (VAN) è coerente con l’obiettivo fondamentale delle politiche di investimento: la massimizzazione del valore dell’impresa.

Pur essendo il criterio elettivo per la selezione dei progetti di investimento nell’ambito del capital budgeting, in taluni casi specifici, il valore attuale netto risulta inapplicabile per la stima del valore delle opportunità di investimento.

Si pensi agli investimenti richiesti dalla ricerca di nuovi prodotti. In molti casi non è possibile stimare la probabilità di successo della ricerca, e si può solo ragionare per estremi: la ricerca andrà a buon fine e l’impiego del capitale frutterà; conseguentemente l’investimento deve avere un valore finanziario. O non andrà a buon fine: quindi il capitate non frutterà, e l’investimento non ha alcun valore. Si tratta, quindi, di casi che richiedono investimenti connotati da totale incertezza rispetto all’esito; per questo motivo, si considerano solo i casi limite ammissibili razionalmente. Negli investimenti rischiosi si discute sul grado di successo e gli investimenti hanno un valore anche in relazione a situazioni di successo parziale.

Nelle circostanze caratterizzate da una valorizzazione del capitale investito dipendenti dalla eventuale manifestazione di specifici eventi (contingency claims), la logica delle opzioni reali sembra poter essere utile: per tali ragioni rientra nei tools della capital budgeting analysis.he analysis for the selection of corporate investment mark one of the most critical moments in the life of every business as it is about choices of durable use of available capital in projects relevant to the company and that, usually, will condition for the periods not short. In the financial perspective, the investments are written down to the financial needs they generate, the expected cash flows and the possible changes in the levels of risk business: ultimately, every investment interests for the positive contribution to the business value. Valuation errors in the analysis of investment projects can even be fatal for the company: in case of failure of a project, the resources used are not easily recoverable.

The selection of corporate investment, concern the renewal or acquisition of new assets, industrial, commercial or administrative, must always be in accordance with criteria and models of financial evaluation. The financial plans of the company disclosed to the market have already established the coordinates within which the financial management must move (expected return proportional to the risks listed by the markets, planning horizon and capital deployment, asset structure and financial structure equilibria and financial maximum).

In this framework, the capital budgeting leads to operational decisions (non-strategic) and aims for a rational use of capital that is or will be available as part of the expected results already weighted and risk premiums consistent. Despite the constraints seem stringent, the activity contributes to the creation of business value when, in the same conditions, you can not find a specific investment solution: particularly convenient for saving resources that can generate than other productive combinations; requiring minor uses of capital; allowing you to generate more fertile cash flows due to the higher quality or productivity; able to contain the risks of using more than other solutions; with times of obsolescence prolonged than alternative technologies. Shed light on those specific aspects relevant families of indicators: indicators of efficiency; indicators of economic viability; life indicators recovery of disbursements; risk indicators.

Following each type of benefit, and the respective indicator, the list of preferred investments - those between comparable and therefore alternative - seems to be very clear; But sometimes the signs are conflicting and every investment has several reasons preferable.

Thus proceeding, the operational practice includes, in fact, the use of alternative valuation techniques of investment: we need to realize the importance of each suggestion.

Only the net present value (NPV) is consistent with the fundamental objective of the investment policies: the maximization of the value of the firm.

Although the election rules for the selection of investment projects within the capital budgeting, in certain specific cases, the net present value is inapplicable to estimate the value of the investment opportunity.

Think of the investment required by the search for new products. In many cases it is not possible to estimate the probability of success of the research, and you can only think in extremes: the search will fail, and the use of capital will yield; consequently, the investment must have a financial value. Or will fail: thus will yield not happened, and the investment has no value. It is, therefore, of cases that require investments characterized by total uncertainty in the outcome; For this reason, they consider only the extreme cases eligible rationally. In risky investments discussed on the degree of success and investments have a value in relation to situations of partial success.

In circumstances characterized by enhancement of capital invested by the employees any manifestation of specific events (contingency claims), the logic of real options seem to be useful for such situations is within the tools of capital budgeting analysis",McGraw-Hill Education,Decisioni di investimento e capital budgeting,,,,core
290067106,2016-06,"Nowadays, the miniaturization of many consumer products is extending the use of micro-milling operations with high-quality requirements. However, the impacts of cutting-tool wear on part dimensions, form and surface integrity are not negligible and part quality assurance for a minimum production cost is a challenging task. In fact, industrial practices usually set conservative cutting parameters and early cutting replacement policies in order to minimize the impact of cutting-tool wear on part quality. Although these practices may ensure part integrity, the production cost is far away to be minimized, especially in highly tool-consuming operations like mold and die micro-manufacturing. In this paper, an adaptive control optimization (ACO) system is proposed to estimate cutting-tool wear in terms of part quality and adapt the cutting conditions accordingly in order to minimize the production cost, ensuring quality specifications in hardened steel micro-parts. The ACO system is based on: (1) a monitoring sensor system composed of a dynamometer, (2) an estimation module with Artificial Neural Networks models, (3) an optimization module with evolutionary optimization algorithms, and (4) a CNC interface module. In order to operate in a nearly real-time basis and facilitate the implementation of the ACO system, different evolutionary optimization algorithms are evaluated such as particle swarm optimization (PSO), genetic algorithms (GA), and simulated annealing (SA) in terms of accuracy, precision, and robustness. The results for a given micro-milling operation showed that PSO algorithm performs better than GA and SA algorithms under computing time constraints. Furthermore, the implementation of the final ACO system reported a decrease in the production cost of 12.3 and 29 % in comparison with conservative and high-production strategies, respectively",Springer,Adaptive control optimization in micro-milling of hardened steels-evaluation of optimization approaches,,10.1007/s00170-015-7807-6,,core
144105164,2016-08-03T00:00:00,"Nesta pesquisa, é proposta uma metodologia para detectar e classificar os distúrbios observados em um Sistema Elétrico Industrial (SEI), além de estimar de forma não intrusiva, o torque eletromagnético e a velocidade associada ao Motor de Indução Trifásico (MIT) em análise. A metodologia proposta está baseada na utilização da Transformada Wavelet (TW) para a detecção e a localização no tempo dos afundamentos e interrupções de tensão, e na aplicação da Função Densidade de Probabilidade (FDP) e Correlação Cruzada (CC) para a classificação dos eventos. Após o processo de classificação dos eventos, a metodologia como implementada proporciona a estimação do torque eletromagnético e a velocidade do MIT por meio das tensões e correntes trifásicas via Redes Neurais Artificiais (RNAs). As simulações computacionais necessárias sobre um sistema industrial real, assim como a modelagem do MIT, foram realizadas utilizando-se do software DIgSILENT PowerFactory. Cabe adiantar que a lógica responsável pela detecção e a localização no tempo detectou corretamente 93,4% das situações avaliadas. Com relação a classificação dos distúrbios, o índice refletiu 100% de acerto das situações avaliadas. As RNAs associadas à estimação do torque eletromagnético e à velocidade no eixo do MIT apresentaram um desvio padrão máximo de 1,68 p.u. e 0,02 p.u., respectivamente.This study proposes a methodology to detect and classify the disturbances observed in an Industrial Electric System (IES), in addition to, non-intrusively, estimate the electromagnetic torque and speed associated with the Three-Phase Induction Motor (TPIM) under analysis. The proposed methodology is based on the use of the Wavelet Transform WT) for the detection and location in time of voltage sags and interruptions, and on the application of the Probability Density Function (PDF) and Cross Correlation (CC) for the classification of events. After the process of events classification, the methodology, as implemented, provides the estimation of the electromagnetic torque and the TPIM speed through the three-phase voltages and currents via Artificial Neural Networks (ANN). The necessary computer simulations of a real industrial system, as well as the modeling of the TPIM, were performed by using the DIgSILENT PowerFactory software. The logic responsible for the detection and location in time correctly detected 93.4% of the assessed situations. Regarding the classification of disturbances, the index reflected 100% accuracy of the assessed situations. The ANN associated with the estimation of the electromagnetic torque and speed at the TPIM shaft showed a maximum standard deviation of 1.68 p.u. and 0.02 p.u., respectively","'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",Methodology for the diagnosis and analysis of influence of voltage sags and interruptions in three-phase induction motors,,10.11606/T.18.2016.tde-29072016-164050,,core
103609730,2016-01-08,"Abstract — Planning is becoming increasingly prevalent as a tool for high-level reasoning in real-world robotics systems. This paper discusses the implementation of a high-level ‘mission planner ’ that utilises AI planning techniques to find initial plans for a fleet of robots acting in a manufacturing factory. The paper introduces the system architecture, and then focuses on the ROS-based mission planning component, which requires the translation of low-level robot ‘skills ’ and a world model to a high-level planning domain. This paper also introduces a new algorithm for decomposition-based planning that can find ‘balanced ’ plans in large multi-robot domains where current state-of-the-art techniques fail. I",,Mission Planning for a Robot Factory Fleet,,,,core
145029566,2016-07-01T07:00:00Z,"Within the fields of visual effects and animation, humans have historically spent countless painstaking hours mastering the skill of drawing frame-by-frame animations. One such animation technique that has been widely used in the animation and visual effects industry is called \u22rotoscoping\u22 and has allowed uniquely stylized animations to capture the motion of real life action sequences, however it is a very complex and time consuming process. Automating this arduous technique would free animators from performing frame by frame stylization and allow them to concentrate on their own artistic contributions. This thesis introduces a new artificial system based on an existing neural style transfer method which creates artistically stylized animations that simultaneously reproduce both the motion of the original videos that they are derived from and the unique style of a given artistic work. This system utilizes a convolutional neural network framework to extract a hierarchy of image features used for generating images that appear visually similar to a given artistic style while at the same time faithfully preserving temporal content. The use of optical flow allows the combination of style and content to be integrated directly with the apparent motion over frames of a video to produce smooth and visually appealing transitions. The implementation described in this thesis demonstrates how biologically-inspired systems such as convolutional neural networks are rapidly approaching human-level behavior in tasks that were once thought impossible for computers. Such a complex task elucidates the current and future technical and artistic capabilities of such biologically-inspired neural systems as their horizons expand exponentially. Further, this research provides unique insights into the way that humans perceive and utilize temporal information in everyday tasks. A secondary implementation that is explored in this thesis seeks to improve existing convolutional neural networks using a biological approach to the way these models adapt to their inputs. This implementation shows how these pattern recognition systems can be greatly improved by integrating recent neuroscience research into already biologically inspired systems. Such a novel hybrid activation function model replicates recent findings in the field of neuroscience and shows significant advantages over existing static activation functions",RIT Scholar Works,A Temporally Coherent Neural Algorithm for Artistic Style Transfer,,10.1109/icpr.2016.7900142,,core
43097967,2016-03-31T08:38:01,"This Presentation is brought to you for free and open access by the City College of New York at CUNY Academic Works. It has been accepted for
inclusion in International Conference on Hydroinformatics by an authorized administrator of CUNY Academic Works. For more information, please
contact AcademicWorks@cuny.edu.11
th International Conference on Hydroinformatics
HIC 2014, New York City, USAModelling rainfall-runoff processes enables hydrologists to plan their response to flooding events. Urban drainage catchment modelling requires rainfall-runoff models as a prerequisite. In the UK, one of the main software tools used for drainage modelling is InfoWorks CS, based on relatively simple methods which are relatively robust in predicting runoff. This paper presents an alternative approach to modelling runoff that will allow for the complex inter-relation of runoff that occurs from impermeable areas, permeable areas, local surface storage and variation in rainfall induced infiltration. Apart from the uncertainties associated with the measurement of connected surfaces to the drainage system, the physical processes involved in runoff are nonlinear, making artificial neural networks (ANNs) an ideal candidate for modelling them. ANNs have been used for runoff prediction in natural catchments, and recently on a study for predicting the performance of urban drainage systems. This study seeks to determine an input set that predicts sewerage flow in urban catchments where the runoff is dominated by infiltration, a major issue for the water industry. A framework is proposed in which an ANN is trained by an evolutionary algorithm, which optimises ANN weights; results are assessed using the Nash-Sutcliffe Efficiency Coefficient. The model is demonstrated on a real-world case study site for which rainfall, flow, air temperature and groundwater levels in three boreholes have been measured. Various combinations of these data are used as model inputs, examining a mixture of daily and sub-daily timesteps. The best predictions are generated from daily linearly combined antecedent rainfall and air temperature, although sub-daily information improves the worst-case performance of the model. Although infiltration is affected by groundwater levels, incorporating groundwater into the model does not improve predictions. The proposed ANN model is capable of producing acceptable predictions, thus avoiding many of the uncertainties involved in traditional infiltration modelling",City University of New York (CUNY): CUNY Academic Works,An artificial neural network-based rainfall runoff model for improved drainage network modelling,https://core.ac.uk/download/43097967.pdf,,,core
42594288,2016-04-07T00:00:00,"INTRODUCTION Cancer Research UK Formulation Unit The Formulation Unit based at the University of Strathclyde in Glasgow has a research and development history in excess of 25 years, being funded by, and working in partnership with, firstly Cancer Research Campaign, and since 2002, with Cancer Research UK. The Unit is based in an entirely academic University setting, and since 2004 has been licensed by the UK government Medicines and Healthcare products Regulatory Agency (MHRA) for research, development and manufacture of Phase I/II novel small molecule cancer therapeutics and diagnostics. Research programs have delivered new formulations to clinical trial as either sterile or non-sterile presentations. However, the Unit’s specialty is based around small volume parenteral product manufacture. Boronophenylalanine (L-BPA) in Boron Neutron Capture Therapy (BNCT) L-BPA is the premier pharmaceutical selection in BNCT in treatment of selected head and neck tumours. BNCT relies on localisation of boron 10 within a tumour mass, made possible by the amino acid carrier portion of the L-BPA molecule. Phenylalanine is selectively transported across the blood brain barrier and then into astrocytic cells by a LAT-1 transporter system that is up-regulated in tumour. A targeted external neutron beam activates the accumulated L-BPA. In brief, neutron capture by boron causes nuclear re-arrangement and formation of a high linear energy transfer alpha particle and lithium 7 nuclei. Thus the patient is dosed with localised radiotherapy. OLD FORMULATION Issues existed with the previous standard formulation of L-BPA in fructose. L-BPA complexed with fructose has low solubility of around 30mg/mL. Consequently, large administration volumes are required to achieve clinical dosing in tens of grams of drug per patient. Moreover, L-BPA in fructose solutions must be freshly prepared and administered within 48 hours for reasons of product instability (Henriksson et al, 2008). Although rare, hereditary fructose intolerance needs to be considered. Taken together, L-BPA production, preparation and patient dosing is highly challenging. NEW FORMULATION Restrictions The Formulation Unit developed a new improved formulation; the drug product was a lyophilized pH8 solution of L-BPA at 100mg/mL in 110mg/mL mannitol (Schmidt et al, 2011). When lyophilised, a shelf life of 48 months was supported for the drug product. Whilst a three times increase in solubility, and a significantly enhanced product lifetime were worthy formulation enhancements, a new restriction emerged; the solution for lyophilisation contained 21% w/v solids far exceeding the ‘normal’ region of 2% w/v to 5% w/v (Boylan and Nail, 2009). Moreover, the lyophilisation cycle of 6 days was considered commercially unfavourable. A shortened drying cycle of 1 to 3 days would be preferred. Research was therefore initiated to reduce drying cycle time utilising Manometric Temperature Measurement (MTM) technology. MTM Studies MTM controlled freeze drying systems were originally marketed in the first decade of the new millennium. The ability to use software to calculate the performance at the freeze-drying front in real time is scientifically and commercially appealing. The possibility to optimize processing conditions at that same time as data is being received invites the prospect of a reduced experimentation phase thereby rapidly reaching the goal of a maximally efficient freeze drying cycle. In theory, even a minimally experienced operator could achieve this outcome. In summary, MTM functions by taking pressure rise information at regular intervals (Giesler et al, 2007). Based on SMART® software (SP Scientific, Stone Ridge, NY, USA), hourly pressure rise data are taken at a rate of 10 samples per second. The system calculates the product temperature at the sublimation interface and mass transfer resistance of the product. Adjustments are then automatically made to the shelf temperature and system pressure to achieve a calculated target product temperature. The end of primary drying can be determined by comparing the vapour pressure of ice with the system chamber pressure. Input data is minimal, such as vial number, inner vial area, fill volume and weight, concentration, product critical temperature. MATERIALS AND METHODS Chemicals Syntagon AB, Södertälje, Sweden manufactured BPA raw material according to EU current Good Manufacturing Practice (cGMP). D-mannitol (Ph. Eur) was sourced from Sigma-Aldrich, Poole, UK, and fuming hydrochloric acid and sodium hydroxide pellets (both extra pure Ph. Eur., BP, JP, NF) were obtained from VWR International, Lutterworth, UK. Water for Irrigation (WFI) in bulk was acquired from Baxter’s Healthcare Ltd., Norfolk, UK. Type 1 clear glass 50mL vials with 20mm butyl rubber stoppers (proved clean), crimped with 20mm tear off aluminium overseals were all from Adelphi Healthcare Packaging, Haywards Heath, UK. Lyophilisation equipment MTM software (SMART®) was operated on an FTS Systems Lyostar II drier (Biopharma, Winchester, UK). CONCLUSION A new improved L-BPA formulation in mannitol has been developed and used in human clinical trial. Further research using MTM technology succeeded in reducing a 6 day drug product drying cycle to 53 hours. The formulation exhibited non-ideal behaviour, and MTM failed to predict drying parameters, e.g., base of vial temperature, that are more closely replicated in ‘ideal’ test articles such as a 5% mannitol comparator. Further test lyophilisations are required to reach ideal. ACKNOWLEDGMENTS This research is funded by Cancer Research UK. REFERENCES 1. Boylan, J.C. and Nail, S.L. Parenteral Products, in: Florence, A.T. and Siepman, J. (Eds.), Modern Pharmaceutics. Informa Healthcare, New York, 565-609 (2009). 2. Giesler, H.; Kramer, T. and Pikal, M. J. Use of manometric temperature measurement (MTM) and SMART freeze dryer technology for development of an optimised freeze drying cycle. J. Pharm Sci. 96(12), 3402-3418 (2007). 3. Henriksson, R.; Capala, J.; Michanek, A.; Lindahl, S.A.; Satford, L.G.; Franzen, L.; Blomquist, E.; Westlin, J.E. and Bergenheim, A.T. Boron neutron capture therapy (BNCT) for glioblastoma multiforme: A phase II study evaluating a prolonged high-dose of boronophenylalanine (BPA). Radiotherapy and Oncology 88, 183-191 (2008). 4. Schmidt, E.; Dooley, N.; Ford, S. J.; Elliott, M. and Halbert, G. W. Physicochemical investigation of the influence of saccharide based parenteral formulation excipients on L-p-boronphenylalanine solubilisation for Boron Neutron Capture Therapy. J. Pharm. Sci. 101(1), 223-232 (2011)",,Manometric Temperature Measurement (MTM) lyophilisation of a challenging clinical trial pharmaceutical,https://core.ac.uk/download/42594288.pdf,,,core
79584235,2016-01-01T00:00:00,"abstract: Feature learning and the discovery of nonlinear variation patterns in high-dimensional data is an important task in many problem domains, such as imaging, streaming data from sensors, and manufacturing. This dissertation presents several methods for learning and visualizing nonlinear variation in high-dimensional data. First, an automated method for discovering nonlinear variation patterns using deep learning autoencoders is proposed. The approach provides a functional mapping from a low-dimensional representation to the original spatially-dense data that is both interpretable and efficient with respect to preserving information. Experimental results indicate that deep learning autoencoders outperform manifold learning and principal component analysis in reproducing the original data from the learned variation sources.

A key issue in using autoencoders for nonlinear variation pattern discovery is to encourage the learning of solutions where each feature represents a unique variation source, which we define as distinct features. This problem of learning distinct features is also referred to as disentangling factors of variation in the representation learning literature. The remainder of this dissertation highlights and provides solutions for this important problem.

An alternating autoencoder training method is presented and a new measure motivated by orthogonal loadings in linear models is proposed to quantify feature distinctness in the nonlinear models. Simulated point cloud data and handwritten digit images illustrate that standard training methods for autoencoders consistently mix the true variation sources in the learned low-dimensional representation, whereas the alternating method produces solutions with more distinct patterns. 

Finally, a new regularization method for learning distinct nonlinear features using autoencoders is proposed. Motivated in-part by the properties of linear solutions, a series of learning constraints are implemented via regularization penalties during stochastic gradient descent training. These include the orthogonality of tangent vectors to the manifold, the correlation between learned features, and the distributions of the learned features. This regularized learning approach yields low-dimensional representations which can be better interpreted and used to identify the true sources of variation impacting a high-dimensional feature space. Experimental results demonstrate the effectiveness of this method for nonlinear variation pattern discovery on both simulated and real data sets.Dissertation/ThesisDoctoral Dissertation Industrial Engineering 201",,Distinct Feature Learning and Nonlinear Variation Pattern Discovery Using Regularized Autoencoders,https://core.ac.uk/download/79584235.pdf,,,core
203158594,2016-12-31,"AbstractSewer systems require regular inspection in order to ensure their satisfactory condition. As most sewer networks consist of pipes too small for engineers to traverse, CCTV footage is used to record the interior of these pipes. This footage is manually analysed by qualified engineers, to determine the condition of the pipe and the presence of any faults. We propose a methodology, which automatically detects faults within the CCTV footage. This has the potential to dramatically reduce the time required to process the large volume of CCTV footage produced during a survey. The proposed methodology first characterises localised regions of each video frame using multiscale GIST features. Extremely randomised trees are then used to learn a classifier that distinguishes between frames showing a fault and normal frames. The technique is tested on 670 video segments from real sewer inspections of a variety of pipes, supplied by Wessex Water. Detection performance is assessed by plotting receiver operating characteristics and quantifying the area under the curve. Preliminary results indicate high detection accuracy of 88% and an area under the ROC curve of 96%. The machine learning used reduces the footage to a selection of frames containing faults, which can be quickly identified (whether by an engineer or another piece of software), showing promise for use in industrial wastewater network surveys",Published by Elsevier Ltd.,Automated Detection of Faults in Wastewater Pipes from CCTV Footage by Using Random Forests ,,10.1016/j.proeng.2016.07.416,,core
100958395,2015-01-04,"The diesel hydrotreating (HDT) process in refining oil plants is a conversion process responsible for the specification of this product in oil industry. In this work, the objective was to estimate sulfur content in the outlet stream of the unit, using inferences based on heuristic modeling. Neural networks (NN) were used to correlate the sulfur content, measured offline in laboratories, with variables measured on-line (as temperature and flow rates) in the reaction section of the HDT unit. Historical data was loaded from Petrobras (Brazilian Oil Company) Duque de Caxias Refinery (REDUC) in Rio de Janeiro and treated in order to remove outliers and reduce dimensionality. After that, twenty-four different designs of neural networks were trained to find out the best fit to real data. The chosen neural network was implemented in the refinery’s data storing and acquisition system. Very good predicitons of sulfur content were obtained indicating the use of this inference for advanced process control. 1",,Design and Implementation of a Neural Network Based Soft Sensor to Infer Sulfur Content in a Brazilian Diesel Hydrotreating Unit,,,,core
87562050,2016-09-01T00:00:00Z,"Introduction

Automation of agricultural and machinery construction has generally been enhanced by intelligent control systems due to utility and efficiency rising, ease of use, profitability and upgrading according to market demand. A broad variety of industrial merchandise are now supplied with computerized control systems of earth moving processes to be performed by construction and agriculture field vehicle such as grader, backhoe, tractor and scraper machines. A height control machine which is used in measuring base thickness is consisted of two mechanical and electronic parts. The mechanical part is consisted of conveyor belt, main body, electrical engine and invertors while the electronic part is consisted of ultrasonic, wave transmitter and receiver sensor, electronic board, control set, and microcontroller. The main job of these controlling devices consists of the topographic surveying, cutting and filling of elevated and spotted low area, and these actions fundamentally dependent onthe machine's ability in elevation and thickness measurement and control. In this study, machine was first tested and then some experiments were conducted for data collection. Study of system modeling in artificial neural networks (ANN) was done for measuring, controlling the height for bases by input variable input vectors such as sampling time, probe speed, conveyer speed, sound wave speed and speed sensor are finally the maximum and minimum probe output vector on various conditions. The result reveals the capability of this procedure for experimental recognition of sensors' behavior and improvement of field machine control systems. Inspection, calibration and response, diagnosis of the elevation control system in combination with machine function can also be evaluated by some extra development of this system.

Materials and Methods

Designing and manufacture of the planned apparatus classified in three dissimilar, mechanical and electronic module, courses of action. The mechanical parts were computer-generated by engineering software in assembled, exploded and standard two-dimensional drawing required for the manufacturing process. Carrier and framework of control unit and actuator mainly designed to have the capability to support and hold the hardware and sensor assembly in an easy mountable fashion. This arrangement performed feasibility of the movement and allocating of control unit along the travel length of belt above the conveyor unit.

In this work a multilayer perceptron network with different training algorithm was used and it is found that the backpropagation algorithm with Levenberge-Marquardt learning rule was the best choice for this analysis because of the accurate and faster training procedure. The Levenberg-Marquardt algorithm was an iterative technique that locates the minimum of a multivariate function that was expressed as the sum of squares of nonlinear real-valued functions. It has become a standard technique for non-linear least-squares problems, widely adopted in a broad spectrum of disciplines. LM can be thought of as a combination of steepest descent and the Gauss-Newton method. When the current solution was far from the correct one, the algorithm behaves like a steepest descent method: slow, but guaranteed to converge. When the current solution is close to the correct solution, it becomes a Gauss-Newton method. The Levenberg algorithm is:

1. Do an update as directed by the rule above.

2. Evaluate the error at the new parameter vector.

3. If the error has increased as a result the update, then retract the step (i.e. reset the weights to their previous values) and increase l by a factor of 10 or some such significant factor, then goes to (1) and try an update again.

4. If the error has decreased as a result of the update, then accept the step (i.e. keep the weights at their new values) and decrease l by a factor of 10 or so.

Results and Discussion

 The study of multi artificial neural network learning algorithm by using base Levenberg–Marquardt was the best choice to estimate function experimental data convergence. Artificial neural networks databases were generated by experimental measurement data condition scales.

It has been observed that the artificial neural networks could be used in height control. The function estimation problem with parameters in Levenberg–Marquardt algorithm showed a high performance and has a high speed, the error in the most cases were decrease and show a high convergence. Sum square error between ANN predictions and experimental measurements was less than 0.001 and correlation coefficient is above 0.99.

Conclusions

ANN method was capable to predict and capture the behavior of experimental measurements.

ANN method can easily be used to determine new results with considerably less computational cost and time. Results show that the back-propagation method with Levenberg-Marquardt learning rule was suitable for training the networks.

 The Sum square error between ANN predictions and experimental measurements was less than 0.001 and the correlation coefficient is above 0.99.

Replacement of the identity matrix with the diagonal of the Hessian in Levenberge-Marquardt update equation has great advantages in convergence and computation time",Ferdowsi University of Mashhad,Modeling of the height control system using artificial neural networks,,,"[{'title': None, 'identifiers': ['2228-6829', '2423-3943', 'issn:2228-6829', 'issn:2423-3943']}]",core
148683794,2015-06-03T00:00:00,"Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup",'Elsevier BV',Artificial cognitive control with self-x capabilities: A case study of a micro-manufacturing process,https://core.ac.uk/download/148683794.pdf,10.1016/j.compind.2015.05.001,,core
161502228,2016,"The paper provides a state of the art review of guided wave based structural health monitoring (SHM). First, the fundamental concepts of guided wave propagation and its implementation for SHM is explained. Following sections present the different modeling schemes adopted, developments in the area of transducers for generation, and sensing of wave, signal processing and imaging technique, statistical and machine learning schemes for feature extraction. Next, a section is presented on the recent advancements in nonlinear guided wave for SHM. This is followed by section on Rayleigh and SH waves. Next is a section on real-life implementation of guided wave for industrial problems. The paper, though briefly talks about the early development for completeness,. is primarily focussed on the recent progress made in the last decade. The paper ends by discussing and highlighting the future directions and open areas of research in guided wave based SHM",IOP PUBLISHING LTD,Guided wave based structural health monitoring: A review,,10.1088/0964-1726/25/5/053001,,core
129712012,2016-07-01T00:00:00,"This research work presents a novel Cognitive Task Planning framework for Smart Industrial Robots. The framework makes an industrial mobile manipulator robot Cognitive by applying Semantic Web Technologies. It also introduces a novel Navigation Among Movable Obstacles algorithm for robots navigating and manipulating inside a ﬁrm. 



The objective of Industrie 4.0 is the creation of Smart Factories: modular ﬁrms provided with cyber-physical systems able to strong customize products under the condition of highly ﬂexible mass-production. Such systems should real-time communicate and cooperate with each other and with humans via the Internet of Things. They should intelligently adapt to the changing surroundings and autonomously navigate inside a ﬁrm while moving obstacles that occlude free paths, even if seen for the ﬁrst time. At the end, in order to accomplish all these tasks while being eﬃcient, they should learn from their actions and from that of other agents. 



Most of existing industrial mobile robots navigate along pre-generated trajectories. They follow ectriﬁed wires embedded in the ground or lines painted on th eﬂoor. When there is no expectation of environment changes and cycle times are critical, this planning is functional. When workspaces and tasks change frequently, it is better to plan dynamically: robots should autonomously navigate without relying on modiﬁcations of their environments. Consider the human behavior: humans reason about the environment and consider the possibility of moving obstacles if a certain goal cannot be reached or if moving objects may signiﬁcantly shorten the path to it. This problem is named Navigation Among Movable Obstacles and is mostly known in rescue robotics. This work transposes the problem on an industrial scenario and tries to deal with its two challenges: the high dimensionality of the state space and the treatment of uncertainty. 



The proposed NAMO algorithm aims to focus exploration on less explored areas. For this reason it extends the Kinodynamic Motion Planning by Interior-Exterior Cell Exploration algorithm. The extension does not impose obstacles avoidance: it assigns an importance to each cell by combining the eﬀorts necessary to reach it and that needed to free it from obstacles. The obtained algorithm is scalable because of its independence from the size of the map and from the number, shape, and pose of obstacles. It does not impose restrictions on actions to be performed: the robot can both push and grasp every object. Currently, the algorithm assumes full world knowledge but the environment is reconﬁgurable and the algorithm can be easily extended in order to solve NAMO problems in unknown environments. The algorithm handles sensor feedbacks and corrects uncertainties. 



Usually Robotics separates Motion Planning and Manipulation problems. NAMO forces their combined processing by introducing the need of manipulating multiple objects, often unknown, while navigating. Adopting standard precomputed grasps is not suﬃcient to deal with the big amount of existing diﬀerent objects. A Semantic Knowledge Framework is proposed in support of the proposed algorithm by giving robots the ability to learn to manipulate objects and disseminate the information gained during the fulﬁllment of tasks. The Framework is composed by an Ontology and an Engine. The Ontology extends the IEEE Standard Ontologies for Robotics and Automation and contains descriptions of learned manipulation tasks and detected objects. It is accessible from any robot connected to the Cloud. It can be considered a data store for the eﬃcient and reliable execution of repetitive tasks; and a Web-based repository for the exchange of information between robots and for the speed up of the learning phase. No other manipulation ontology exists respecting the IEEE Standard and, regardless the standard, the proposed ontology diﬀers from the existing ones because of the type of features saved and the eﬃcient way in which they can be accessed: through a super fast Cascade Hashing algorithm. The Engine lets compute and store the manipulation actions when not present in the Ontology. It is based on Reinforcement Learning techniques that avoid massive trainings on large-scale databases and favors human-robot interactions. 



The overall system is ﬂexible and easily adaptable to diﬀerent robots operating in diﬀerent industrial environments. It is characterized by a modular structure where each software block is completely reusable. Every block is based on the open-source Robot Operating System. Not all industrial robot controllers are designed to be ROS-compliant. This thesis presents the method adopted during this research in order to Open Industrial Robot Controllers and create a ROS-Industrial interface for them",,Cognitive Task Planning for Smart Industrial Robots,https://core.ac.uk/download/129712012.pdf,,,core
49519786,2015-06-01T00:00:00,"National audienceIn machine learning, empirical performance on real data are crucial in the success of a method. Recent years have seen the emergence of a large number of machine learning competitions. These challenges are motivated by industrial (Netflix prize) or academic (HiggsML challenge) applications and put in competition researchers and data scientists to obtain the best performance. We wanted to expose students to this reality bysubmitting a challenge in the context of the machine learning course. The leaderboard is displayed on an automatically updated web page allowing emulation among students. The history of the results also allows them to visualize their progress through the submissions. In addition, the challenge can continue outside of the supervised sessions promoting independence and exploration of new learning techniques and computer tools. The system we have implemented is available as an R package for reuse by other teachers. Building on the R Markdown and Dropbox tools, it requires no network configuration and can be deployed very easily on a personal computer.En apprentissage automatique, les performances empiriques obtenues sur données réelles sont déterminantes dans le succès d'une méthode. Ces dernières années ont vu l'apparition d'un grand nombre de compétitions d'apprentissage automatique. Ces challenges sont motivés par des applications industrielles (prix Netflix) ou académiques (challenge HiggsML) et mettent en compétition chercheurs et data scientists pour obtenir les meilleures performances. Nous avons souhaité confronter les étudiants à cette réalité en leur soumettant un challenge dans le cadre du cours d'apprentissage automatique. Leur classement est affiché sur une page web mise à jour automatiquement permettant une émulation parmi les étudiants. L'historique des résultats leur permet également de visualiser leur progression au fil des soumissions. De plus, le challenge peut se poursuivre en dehors des sessions encadrées favorisant l'autonomie et l'exploration de nouvelles techniques d'apprentissage et outils informatiques. Le système que nous avons mis en œuvre est disponible sous forme de package R afin d'être réutilisé par d'autres enseignants. S'appuyant sur les outils R Markdown et Dropbox, il ne nécessite aucune configuration réseau et peut être déployé très facilement sur un ordinateur personnel",HAL CCSD,Compétitions d'apprentissage automatique avec le package R rchallenge,https://core.ac.uk/download/49519786.pdf,,,core
232140814,2016-07-01T07:00:00,"Within the fields of visual effects and animation, humans have historically spent countless painstaking hours mastering the skill of drawing frame-by-frame animations. One such animation technique that has been widely used in the animation and visual effects industry is called  rotoscoping  and has allowed uniquely stylized animations to capture the motion of real life action sequences, however it is a very complex and time consuming process. Automating this arduous technique would free animators from performing frame by frame stylization and allow them to concentrate on their own artistic contributions. This thesis introduces a new artificial system based on an existing neural style transfer method which creates artistically stylized animations that simultaneously reproduce both the motion of the original videos that they are derived from and the unique style of a given artistic work. This system utilizes a convolutional neural network framework to extract a hierarchy of image features used for generating images that appear visually similar to a given artistic style while at the same time faithfully preserving temporal content. The use of optical flow allows the combination of style and content to be integrated directly with the apparent motion over frames of a video to produce smooth and visually appealing transitions. The implementation described in this thesis demonstrates how biologically-inspired systems such as convolutional neural networks are rapidly approaching human-level behavior in tasks that were once thought impossible for computers. Such a complex task elucidates the current and future technical and artistic capabilities of such biologically-inspired neural systems as their horizons expand exponentially. Further, this research provides unique insights into the way that humans perceive and utilize temporal information in everyday tasks. A secondary implementation that is explored in this thesis seeks to improve existing convolutional neural networks using a biological approach to the way these models adapt to their inputs. This implementation shows how these pattern recognition systems can be greatly improved by integrating recent neuroscience research into already biologically inspired systems. Such a novel hybrid activation function model replicates recent findings in the field of neuroscience and shows significant advantages over existing static activation functions",RIT Scholar Works,A Temporally Coherent Neural Algorithm for Artistic Style Transfer,https://core.ac.uk/download/232140814.pdf,,,core
73439723,2016-11-29T00:00:00,"This paper proposes a maintenance platform for business vehicles which
detects failure sign using IoT data on the move, orders to create repair parts
by 3D printers and to deliver them to the destination. Recently, IoT and 3D
printer technologies have been progressed and application cases to
manufacturing and maintenance have been increased. Especially in air flight
industry, various sensing data are collected during flight by IoT technologies
and parts are created by 3D printers. And IoT platforms which improve
development/operation of IoT applications also have been appeared. However,
existing IoT platforms mainly targets to visualize ""things"" statuses by batch
processing of collected sensing data, and 3 factors of real-time, automatic
orders of repair parts and parts stock cost are insufficient to accelerate
businesses. This paper targets maintenance of business vehicles such as
airplane or high-speed bus. We propose a maintenance platform with real-time
analysis, automatic orders of repair parts and minimum stock cost of parts. The
proposed platform collects data via closed VPN, analyzes stream data and
predicts failures in real-time by online machine learning framework Jubatus,
coordinates ERP or SCM via in memory DB to order repair parts and also
distributes repair parts data to 3D printers to create repair parts near the
destination.Comment: 5 pages, 3 figures, 5th International Conference on Software and
  Information Engineering (ICSIE 2016), May 201",,"Proposal of Real Time Predictive Maintenance Platform with 3D Printer
  for Business Vehicles",http://arxiv.org/abs/1611.09944,,,core
103396003,2015-11-21,"Healthcare is costing lot of money and resources all over the world in all countries; it consumes almost 30-40 percent of their budget in the healthcare industry. The only solution is to make this health care industry automated and online. In this paper we are proposing a software design framework for health care systems which will be based on agent technologies. The proposed system is based on Artificial Intelligence techniques that can support the user for selection and making choices. The proposed software framework suggested that Multi-Agent Systems (MAS) is the most suitable technique for designing such systems. MAS collaborate intelligently for solving this complex problem. Patients can be supported remotely using the proposed framework so that it can reduce the patient load on hospitals, the proposed software framework operate on real time framework. Computing can help in improving the communication process between follow-up doctors and nurses with patients by making appointments the process easier, according to patient preference with a reminder on necessary actions such as taking scheduled prescribed medicine, engaging in exercises, avoiding some kinds of food and harmful habits such as smoking before and after patient",,Software Design Framework for Healthcare Systems,,10.5120/20328-2507,,core
101501367,2015-02-15,"Abstract: This paper deals withneural based image processing and solutions developed for noise reduction, image enhancement and visual probe mark inspection in VLSI production using the ZISC-036 neuro-processor, an IBM hardware processor which implements Restricted Coulomb Energy (RCE) and the K-Nearest Neighbor (KNN) neural models. The main characteristics of such systems are real-time control and high reliability in detection and classification tasks. Experimental results, validating the exposed concepts, have been reported showing quantitative and qualitative improvement as well as our solutions efficiency",,On RCE Like Neural Networks Based Image Processing and it’s ZISC-036 Based Fully Parallel Implementation Solving Real World and Real Complexity Problems. (Part-Two: industrial applications),,,,core
160630798,2015-01-01T00:00:00,"Introduction
The goal of this work is to advance the production and use of 52Mn (t½ = 5.6 d, β+: 242 keV, 29.6%) as a radioisotope for in vivo preclinical nuclear imaging. More specifically, the aims of this study were: (1) to measure the excitation function for the natCr(p,n)52Mn reaction at low energies to verify past results [1–4]; (2) to measure binding constants of Mn(II) to aid the design of a method for isolation of Mn from an irradiated Cr target via ion-exchange chromatography, building upon previously published methods [1,2,5–7]; and (3) to perform phantom imaging by positron emission tomography/magnetic resonance (PET/MR) imaging with 52Mn and non-radioactive Mn(II), since Mn has potential dual-modality benefits that are beginning to be investigated [8].

Material and Methods
Thin foils of Cr metal are not available commercially, so we fabricated these in a manner similar to that reported by Tanaka and Furukawa [9]. natCr was electroplated onto Cu discs in an industrial-scale electroplating bath, and then the Cu backing was digested by nitric acid (HNO3). The remaining thin Cr discs (~1 cm diameter) were weighed to determine their thickness (~ 75–85 μm) and arranged into stacked foil targets, along with ~25 μm thick Cu monitor foils. These targets were bombarded with ~15 MeV protons for 1–2 min at ~1–2 μA from a CS-15 cyclotron (The Cyclotron Corporation, Berkeley, CA, USA). The beamline was perpendicular to the foils, which were held in a machined 6061-T6 aluminum alloy target holder. The target holder was mounted in a solid target station with front cooling by a jet of He gas and rear cooling by circulating chilled water (T ≈ 2–5 °C). Following bombardment, these targets were disassembled and the radioisotope products in each foil were counted using a high-purity Ge (HPGe) detector. Cross-sections were calculated for the natCr(p,n)52Mn reaction.
Binding constants of Mn(II) were measured by incubating 54Mn(II) (t½ = 312 d) dichloride with anion- or cation-exchange resin (AG 1-X8 (Cl− form) or AG 50W-X8 (H+ form), respectively; both: 200–400 mesh; Bio-Rad, Hercules, CA) in hydrochloric acid (HCl) ranging from 10 mM-8 M (anion-exchange) and from 1 mM-1 M (cation-exchange) or in sulfuric acid (H2SO4) ranging from 10 mM-8 M on cation-exchange resin only. The amount of unbound 54Mn(II) was measured using a gamma counter, and binding constants (KD) were calculated for the various concentrations on both types of ion-exchange resin.
We have used the unseparated product for preliminary PET and PET/MR imaging. natCr metal was bombarded and then digested in HCl, resulting in a solution of Cr(III)Cl3 and 52Mn(II)Cl2. This solution was diluted and imaged in a glass scintillation vial using a microPET (Siemens, Munich, Germany) small animal PET scanner. The signal was corrected for abundant cascade gamma-radiation from 52Mn that could cause random false coincidence events to be detected, and then the image was reconstructed by filtered back-projection. Additionally, we have used the digested target to spike non-radioactive Mn(II)Cl2 solutions for simultaneous PET/MR phantom imaging using a Biograph mMR (Siemens) clinical scanner. The phantom consisted of a 4×4 matrix of 15 mL conical tubes containing 10 mL each of 0, 0.5, 1.0, and 2.0 mM concentrations of non-radioactive Mn(II)Cl2 with 0, 7, 14, and 27 μCi (at start of PET acquisition) of 52Mn(II)Cl2 from the digested target added. The concentrations were based on previous MR studies that measured spin-lattice relaxation time (T1) versus concentration of Mn(II), and the activities were based on calculations for predicted count rate in the scanner. The PET/MR imaging consisted of a series of two-dimensional inversion-recovery turbo spin echo (2D-IR-TSE) MR sequences (TE = 12 ms; TR = 3,000 ms) with a wide range of inversion times (TI) from 23–2,930 ms with real-component acquisition, as well as a 30 min. list-mode PET acquisition that was reconstructed as one static frame by 3-D ordered subset expectation maximization (3D-OSEM). Attenuation correction was performed based on a two-point Dixon (2PD) MR sequence. The DICOM image files were loaded, co-registered, and windowed using the Inveon Research Workplace software (Siemens)",Helmholtz-Zentrum Dresden - Rossendorf,Cyclotron Production and PET/MR Imaging of 52Mn,,,,core
71496489,2016-06-15,"With the booming of large scale data related applications, cognitive systems that leverage modern data processing technologies, e.g., machine learning and data mining, are widely used in various industry fields. These application bring challenges to conventional computer systems on both semiconductor manufacturing and computing architecture. The invention of neuromorphic computing system (NCS) is inspired by the working mechanism of human-brain. It is a promising architecture to combat the well-known memory bottleneck in Von Neumann architecture. The recent breakthrough on memristor devices and crossbar structure made an important step toward realizing a low-power, small-footprint NCS on-a-chip. However, the currently low manufacturing reliability of nano-devices and circuit level constrains, .e.g., the voltage IR-drop along metal wires and analog signal noise from the peripheral circuits, bring challenges on scalability, precision and robustness of memristor crossbar based NCS.\ud
In this dissertation, we quantitatively analyzed the robustness of memristor crossbar based NCS when considering the device process variations, signal fluctuation and IR-drop. Based on our analysis, we will explore deep understanding on hardware training methods, e.g., on-device training and off-device training. Then, new technologies, e.g., noise-eliminating training, variation-aware training and adaptive mapping, specifically designed to improve the training quality on memristor crossbar hardware will be proposed in this dissertation. A digital initialization step for hardware training is also introduced to reduce training time. The circuit level constrains\ud
will also limit the scalability of a single memristor crossbar, which will decrease the efficiency of implementation of NCS. We also leverage system reduction/compression techniques to reduce the required crossbar size for certain applications. Besides, running machine learning algorithms on embedded systems bring new security concerns to the service providers and the users. In this dissertation, we will first explore the security concerns by using examples from real applications. These examples will demonstrate how attackers can access confidential user data, replicate a sensitive data processing model without any access to model details and how expose some key features of training data by using the service as a normal user. Based on our understanding of these security concerns, we will use unique property of memristor device to build a secured NCS",,Neuromorphic System Design and Application,,,,core
300004244,2016-01-01T00:00:00,"With the increasing demand for safer and better driving experience, driver assistance systems are receiving more and more attention in car industry. There are many data sources available to recognize driver’s brain state, such as brainwave (electroencephalography, EEG), image processing, and eye movement pattern analysis. EEG has several advantages over other data sources including directly and early detection of the brain state. However, EEG is very sensitive to artifacts such as eye blink and head movement. Traditional EEG artifact removal needs human experts to identify the artifact segment. In this FYP project, we aimed at removing EEG artifacts in real-time without human inspection.
The objective of this project is to design scenarios that induce different kinds of EEG artifacts in a simulated driving, as well as an online real-time interface using machine learning algorithms to automatically recognize and remove artifacts. The project is interdisciplinary and includes research on driver’s safety, brain-computer interfaces, machine learning and signal processing. It is expected that the results of the FYP project could become a prototype for future car driver assistance systems.
This interim report summarized the theory study and program implementation of Independent Component Analysis and Extreme Learning Machine algorithms for EEG artifact removal application. In addition, the report highlighted the progress so far and recognized the future project plan for the upcoming semester.Bachelor of Engineerin",,Real-time EEG artifact removal in simulated driving,,,,core
296620568,2015-11-26T14:09:29,"Flight simulators have been part of aviation history since its beginning. With the development of modern aeronautics industry, flight simulators have gained an important place and the industry devoted to their manufacture has become significant. In the case of transportation aircraft, accurate mathematical models based on extensive experimental data have been developed by their manufacturers to optimise their aerodynamic and propulsive characteristics and to design efficient flight control systems. However, in the case of small general aviation aircraft this kind of knowledge is not commonly available and the design of accurate flight simulators can result in a tedious try and modify process until the simulator presents a qualitative behaviour close to the one of the real aircraft. This communication proposes through the use of neural networks a method to perform a direct estimation of the aerodynamic forces acting on aircraft. Artificial Neural networks appear to be an appropriate numerical technique to achieve the mapping of these continuous relationships and detailed aerodynamics and thrust models should become no more mandatory to produce accurate flight simulation software. © 2005 IEEE.168172Jategaonkar, R., Flight vehicule system identification in time domain (2003) DINCON03, , Courses notes, São José dos CamposRaisinghani, S.C., Gosh, A.K., Parameter estimation of an aeroelastic aircraft using neural networks (2000) Sadhanã, 25 (2 PART), pp. 181-191. , AprilSteven, B.L., Lewis, F.L., (1992) Aircraft Control and Simulation, , John Wiley & Sons, IncHaykin, S., (1994) Neural Networks: A Comprehensive Foundation, , McMillan, New YorkCybenko, G., Approximation by superpositions of a sigmoïd function (1989) Mathematics of Control, Signals and Systems, 2, pp. 303-314Shazad, M., Slama, J.G., Mora-Camino, F., A new approach for the automation of relative guidance of aircraft (1999) 13 th International Conference on Systems Engineering, , Las Vegas, USAHagan, M.T., Menhaj, M.B., Training feedforward networks with marquardt algorithm (1994) IEEE Transactions on Neural Networks, 5 (6)Freeman, J.A., Skapura, D.M., (1997) Neural Networks Algorithms, Applications and Programming Techniques, , Addison-Wesley, New-Yor",'Institute of Electrical and Electronics Engineers (IEEE)',A Neural Approach For Fast Simulation Of Flight Mechanics,,10.1109/ANSS.2005.8,,core
296641034,2015-11-26T14:57:00Z,"In order to ensure good power quality for modern power systems and/or industrial installations, power conditioning devices have been extensively applied. However, the data analysis for the installation of a determined compensator mainly considers a particular power quality index or disturbance and it is usually based on human expertise. Therefore, this paper proposes a novel expert system that automatically suggests the most appropriate and cost-effective solution for compensating reactive, harmonic and unbalanced current through a careful analysis of several power quality indices and some grid characteristics. Such an expert system is an important tool in real-world applications, where there is a complex scenario in choosing, designing and applying power quality compensators in modern power grids. Since there are no strict boundaries for voltage and current non idealities at distribution level or clear correlation between them and possible solutions, a fuzzy decision-maker was developed to deal with such uncertainties and to embed human expertise in the system. The approach is based on analyzing data from a given time window and providing off-line recommendations for the design and installation of proper compensators. Therefore, the application of the proposed expert system may result in enhanced and faster projects when compared to the traditional design methods for power conditioning. A computational study consisting on applying the suggested compensators for a 5-node network and different load configurations shows the effectiveness of the proposed expert system.42735623570Akagi, H., Watanabe, E.H., Aredes, M., (2007) Instantaneous Power Theory and Applications to Power Conditioning, , 1st ed. Wiley-IEEE PressArghandeh, R., Von Meier, A., Broadwater, R., Phasor-based approach for harmonic assessment from multiple distributed energy resources (2014) Proceedings of IEEE PES General Meeting Conference & Exposition, pp. 1-5Banerjee, A., Mukherjee, V., Ghoshal, S.P., Intelligent fuzzy-based reactive power compensation of an isolated hybrid power system (2014) International Journal of Electrical Power and Energy Systems, 57, pp. 164-177Bellman, R.E., Zadeh, L.A., Decision-making in a fuzzy environment (1970) Management Science, Application Series, 17, pp. B141-B164Bhattacharya, S.K., Goswami, S.K., A new fuzzy based solution of the capacitor placement problem in radial distribution system (2009) Expert Systems with Applications, 36, pp. 4207-4212Bisanovic, S., Hajro, M., Samardzic, M., One approach for reactive power control of capacitor banks in distribution and industrial networks (2014) International Journal of Electrical Power and Energy Systems, 60, pp. 67-73Bollen, M.H.J., What is power quality? (2003) Electric Power Systems Research, 66, pp. 5-14Caia, Y.P., Huangc, G.H., Lina, Q.G., Niea, X.H., Tana, Q., An optimization-model-based interactive decision support system for regional energy management systems planning under uncertainty (2009) Expert Systems with Applications, 36, pp. 3470-3482Chang, Y., Integration of SQP and PSO for optimal planning of harmonic filters (2010) Expert Systems with Applications, 37, pp. 2522-2530Chang, Y., Low, C., Hung, S., Integrated feasible direction method and genetic algorithm for optimal planning of harmonic filters with uncertainty conditions (2009) Expert Systems with Applications, 36, pp. 3946-3955Cheng, C., Design of output filter for inverters using fuzzy logic (2011) Expert Systems with Applications, 38, pp. 8639-8647Cheng, P.T., Chen, C., Lee, T.L., Kuo, S., A cooperative imbalance compensation method for distributed-generation interface converters (2009) IEEE Transactions on Industry Applications, 45, pp. 805-815Chompoo-Inwai, C., Mungkornassawakul, J., A smart recording power analyzer prototype using lab view and low-cost data acquisition (DAQ) in being a smart renewable monitoring system (2013) Proceedings of IEEE Green Technologies Conference, pp. 49-56Corasaniti, V.F., Barbieri, M.B., Arnera, P.L., Valla, M.I., Hybrid active filter for reactive and harmonics compensation in a distribution network (2009) IEEE Transactions on Industrial Electronics, 56, pp. 670-677Currence, E.J., Plizga, J.E., Nelson, H.N., Harmonic resonance at a medium-sized industrial plant (1995) IEEE Transactions on Industry Applications, 31, pp. 682-690Faisal, M., Mohamed, A., Shareef, H., Hussain, A., Power quality diagnosis using time frequency analysis and rule based techniques (2011) Expert Systems with Applications, 38, pp. 12592-12598Galvão T. ., M., Belchior F. ., N., Silveira P. ., M., Ribeiro P. ., F., Comparative analysis of power quality instruments measuring voltage and power (2014) Proceedings of IEEE 16th International Conference on Harmonics and Quality of Power (ICHQP), pp. 762-767Golovanov, N., Lazaroiu G. ., C., Porumb, R., Wind generation assessment proposal by experimental harmonic and distortion factor analysis (2013) Proceedings of 48th International Universities' Power Engineering Conference (UPEC), pp. 1-4Gunal, S., Gerek, O.N., Ece, D.G., Edizkan, R., The search for optimal feature set in power quality event classification (2009) Expert Systems with Applications, 36, pp. 10266-10273Gyugy, L., Otto, R.A., Putman, T.H., Principles and applications of static, thyristor-controlled shunt compensators (1978) IEEE Transactions on Power Apparatus and Systems, 97, pp. 1935-1945Hashemi, S., Aghamohammad, M.R., Wavelet based feature extraction of voltage profile for online voltage stability assessment using RBF neural network (2013) International Journal of Electrical Power and Energy Systems, 49, pp. 86-94Hassan, L.H., Moghavvemi, M., Almurib, H.A.F., Steinmayer, O., Current state of neural networks applications in power system monitoring and control (2013) International Journal of Electrical Power and Energy Systems, 51, pp. 134-144IEEE - Institute of Electrical and Electronics Engineers (1993) IEEE Recommended Practices and Requirements for Harmonic Control in Electrical Power Systems in IEEE Standard, pp. 519-1992(2010) IEEE Standard Definitions for the Measurement of Electrical Power Quantities under Sinusoidal, Nonsinusoidal, Balanced or Unbalanced Conditions, , IEEE - Institute of Electrical and Electronics Engineers. IEEE Std 1459-2010Jegathesan, V., Jerome, J., Elimination of lower order harmonics in voltage source inverter feeding an induction motor drive using evolutionary algorithms (2011) Expert Systems with Applications, 38, pp. 692-699Jintakosonwit, P., Srianthumrong, S., Jintagosonwit, P., Implementation and performance of an anti-resonance hybrid delta-connected capacitor bank for power factor correction (2007) IEEE Transactions on Power Electronics, 22, pp. 2543-2551Kriukov, A., Grigoras, G., Scarlatache, F., Ivanov, O., Vicol, B., Use of fuzzy techniques in reliability assessment of electric distribution systems (2014) Proceedings of IEEE 16th International Conference on Harmonics and Quality of Power (ICHQP), pp. 29-33Liberado, E.V., Souza, W.A., Pomilio, J.A., Paredes, H.K.M., Marafão, F.P., Design of static var compensators using a general reactive energy definition (2013) Przeglad Elektrotechniczny (Electrical Review), 89, pp. 233-238Low, C., Chang, Y., Hung, S., An application of sequential neural-network approximation for sitting and sizing passive harmonic filters (2009) Expert Systems with Applications, 36, pp. 2910-2920Marafão, F.P., Souza, W.A., Liberado, E.V., Silva, L.C.P., Paredes, H.K.M., Load analyser using conservative power theory (2013) Przeglad Elektrotechniczny (Electrical Review), 12, pp. 1-6Morsi, W.G., El-Hawary, M.E., A new fuzzy-based representative quality power factor for nonsinusoidal situations (2008) IEEE Transactions on Power Delivery, 23, pp. 930-936Morsi, W.G., El-Hawary, M.E., A new fuzzy-based representative quality power factor for unbalanced three-phase systems with nonsinusoidal situations (2008) IEEE Transactions on Power Delivery, 23, pp. 2426-2438Obulesu, Y.P., Reddy, M.V., Kusumalatha, Y., A %THD analysis of industrial power distribution systems with active power filter-case studies (2014) International Journal of Electrical Power and Energy Systems, 60, pp. 107-120Paredes, H.K.M., Costabeber, A., Tenti, P., Application of conservative power theory to cooperative control of distributed compensators in smart grids (2011) Przeglad Elektrotechniczny (Electrical Review), 87, pp. 1-7Paredes, H.K.M., Silva, L.C.P., Brandão, D., Marafão, F.P., Possible shunt compensation strategies based on conservative power theory (2011) Przeglad Elektrotechniczny (Electrical Review), 87, pp. 34-39Phipps, J.K., Nelson, J.P., Sen, P.K., Power quality and harmonics on distribution system (1994) IEEE Transactions on Industry Applications, 30, pp. 476-484Piltan, M., Mehmanchi, E., Ghaderi, S.F., Proposing a decision-making model using analytical hierarchy process and fuzzy expert system for prioritizing industries in installation of combined heat and power systems (2012) Expert Systems with Applications, 39, pp. 1124-1133Rahmani, S., Hamadi, A., Al-Haddad, K., Dessaint, L.A., A combination of shunt hybrid power filter and thyristor-controlled reactor for power quality (2014) IEEE Transactions on Industrial Electronics, 61, pp. 2152-2164Ray, R.N., Chatterjee, D., Goswami, S.K., A PSO based optimal switching technique for voltage harmonic reduction of multilevel inverter (2010) Expert Systems with Applications, 37, pp. 7796-7801Sahin, S., Tolun, M.R., Hassanpour, R., Hybrid expert systems: A survey of current approaches and applications (2012) Expert Systems with Applications, 39, pp. 4609-4617Salem, M., Mohamed, A., Samad, S.A., Rule based system for power quality disturbance classification incorporating S-transform features (2010) Expert Systems with Applications, 37, pp. 3229-3235Singh, B., Al-Haddad, K., Chandra, A., A review of active filters for power quality improvement (1999) IEEE Transactions on Industrial Electronics, 46, pp. 960-971Su H. ., J., Chang G. ., W., Hsu L. ., Y., Lu H. ., J., Lee Y. ., D., Chang Y. ., R., Lin J. ., H., Development of power quality analysis platform for INER Microgrid (2013) Proceedings of IEEE Power and Energy Society General Meeting (PES), pp. 1-5Suslov K. ., V., Solonina N. ., N., Smirnov A. ., S., Distributed power quality monitoring (2014) Proceedings of IEEE 16th International Conference on Harmonics and Quality of Power (ICHQP), pp. 517-520Tutkun, N., Improved power quality in a single-phase PWM inverter voltage with bipolar notches through the hybrid genetic algorithms (2010) Expert Systems with Applications, 37, pp. 5614-5620Vallée, F., Klonari, V., Lisiecki, T., Durieux, O., Moiny, F., Lobry, J., Development of a probabilistic tool using Monte Carlo simulation and smart meters measurements for the long term analysis of low voltage distribution grids with photovoltaic generation (2013) International Journal of Electrical Power and Energy Systems, 53, pp. 468-477Wang, M., Tseng, Y., A novel analytic method of power quality using extension genetic algorithm and wavelet transform (2011) Expert Systems with Applications, 38, pp. 12491-12496Yang, Z., Cao, J., Xu, Y., Zhang, H., Yu, P., Yao, S., Data Cleaning for Power Quality Monitoring (2013) Proceedings of Fourth International Conference on Networking and Distributed Computing (ICNDC), pp. 111-115Zhang, J., Yan, W., Yang, Q., Bao, Z., Sun, R., Power quality measurement and analysis of offshore wind farm based on PSCAD/EMTDC models (2013) Proceedings of Sixth International Conference on Advanced Computational Intelligence (ICACI), pp. 371-37",Elsevier Ltd,Novel Expert System For Defining Power Quality Compensators,,10.1016/j.eswa.2014.12.032,,core
52991772,2016-01-01T00:00:00,"International audienceTo satisfy the increasing demand for wireless systems capacity, the industry is dramatically increasing the density of the deployed networks. Like other wireless technologies, Wi-Fi is following this trend, particularly because of its increasing popularity. In parallel, Wi-Fi is being deployed for new use cases that are atypically far from the context of its first introduction as an Ethernet network replacement. In fact, the conventional operation of Wi-Fi networks is not likely to be ready for these super dense environments and new challenging scenarios. For that reason, the high efficiency wireless local area network (HEW) study group (SG) was formed in May 2013 within the IEEE 802.11 working group (WG). The intents are to improve the “real world” Wi-Fi performance especially in dense deployments. In this context, this work proposes a new centralized solution to jointly adapt the transmission power and the physical carrier sensing based on artificial neural networks. The major intent of the proposed solution is to resolve the fairness issues while enhancing the spatial reuse in dense Wi-Fi environments. This work is the first to use artificial neural networks to improve spatial reuse in dense WLAN environments. For the evaluation of this proposal, the new designed algorithm is implemented in OPNET modeler. Relevant scenarios are simulated to assess the efficiency of the proposal in terms of addressing starvation issues caused by hidden and exposed node problems. The extensive simulations show that our learning-based solution is able to resolve the hidden and exposed node problems and improve the performance of high-density Wi-Fi deployments in terms of achieved throughput and fairness among contending nodes. © 2016, Jamil et al",'Springer Science and Business Media LLC',Novel learning-based spatial reuse optimization in dense WLAN deployments,,10.1186/s13638-016-0632-2,,core
290007827,2015-09-13,"سامانه های ابزار دقیق و خودکار در ماشین آلات کشاورزی، میدانی و عمرانی به شکل فزآینده موجب افزایش بهره وری، دقت و توسعه کاربردی گردیده است. دستگاه کنترل ارتفاع جهت اندازه گیری ارتفاع نمونه های مختلف، از بخش های مکانیکی شامل مجموعه محرکه تسمه نقاله، بدنه اصلی، موتور الکتریکی، مبدل و بخش الکترونیکی شامل حسگر فراصوت فرستنده و گیرنده امواج، مجموعه الکترونیکی، مجموعه کنترل و میکروکنترلر ای تی مگا 32 ساخته شده است. در ابتدا عملکرد دستگاه به کمک شبکه عصبی مصنوعی مورد بررسی قرار گرفته و سپس آزمایش هایی جهت جمع آوری و معتبرسازی داده ها انجام گردیده است. در این مطالعه مشاهده گردید که استفاده از شبکه عصبی مصنوعی چند لایه همراه با الگوریتم یادگیری لونبرگ-مارکواردت دارای بهترین معیار برای برآورد و همگرایی نتایج داده های آزمایشگاهی می باشد و جایگزینی ماتریس واحد با ماتریس هسیان در معادله بهینه سازی شده قانون لونبرگ-مارکواردت تأثیر به سزایی در همگرایی و کاهش زمان محاسبات دارد. همچنین مطالعه حاضر جهت توسعه فرآیندهای کشاورزی، مهندسی و افزایش تأثیر دستگاه های تولید شده مورد نیاز در آموزش واحدهای اندازه گیری هوشمند بسیار مفید بوده و مسائل تخمین تابع در الگوریتم لونبرگ- مارکواردت کارایی، سرعت، دقت و همگرایی بالایی را از خود نشان داده و خطای کمینه4-10× 7/77 را دارا می باشد.Introduction
Automation of agricultural and machinery construction has generally been enhanced by intelligent control systems due to utility and efficiency rising, ease of use, profitability and upgrading according to market demand. A broad variety of industrial merchandise are now supplied with computerized control systems of earth moving processes to be performed by construction and agriculture field vehicle such as grader, backhoe, tractor and scraper machines. A height control machine which is used in measuring base thickness is consisted of two mechanical and electronic parts. The mechanical part is consisted of conveyor belt, main body, electrical engine and invertors while the electronic part is consisted of ultrasonic, wave transmitter and receiver sensor, electronic board, control set, and microcontroller. The main job of these controlling devices consists of the topographic surveying, cutting and filling of elevated and spotted low area, and these actions fundamentally dependent onthe machine's ability in elevation and thickness measurement and control. In this study, machine was first tested and then some experiments were conducted for data collection. Study of system modeling in artificial neural networks (ANN) was done for measuring, controlling the height for bases by input variable input vectors such as sampling time, probe speed, conveyer speed, sound wave speed and speed sensor are finally the maximum and minimum probe output vector on various conditions. The result reveals the capability of this procedure for experimental recognition of sensors' behavior and improvement of field machine control systems. Inspection, calibration and response, diagnosis of the elevation control system in combination with machine function can also be evaluated by some extra development of this system.
Materials and Methods
Designing and manufacture of the planned apparatus classified in three dissimilar, mechanical and electronic module, courses of action. The mechanical parts were computer-generated by engineering software in assembled, exploded and standard two-dimensional drawing required for the manufacturing process. Carrier and framework of control unit and actuator mainly designed to have the capability to support and hold the hardware and sensor assembly in an easy mountable fashion. This arrangement performed feasibility of the movement and allocating of control unit along the travel length of belt above the conveyor unit.
In this work a multilayer perceptron network with different training algorithm was used and it is found that the backpropagation algorithm with Levenberge-Marquardt learning rule was the best choice for this analysis because of the accurate and faster training procedure. The Levenberg-Marquardt algorithm was an iterative technique that locates the minimum of a multivariate function that was expressed as the sum of squares of nonlinear real-valued functions. It has become a standard technique for non-linear least-squares problems, widely adopted in a broad spectrum of disciplines. LM can be thought of as a combination of steepest descent and the Gauss-Newton method. When the current solution was far from the correct one, the algorithm behaves like a steepest descent method: slow, but guaranteed to converge. When the current solution is close to the correct solution, it becomes a Gauss-Newton method. The Levenberg algorithm is:
1. Do an update as directed by the rule above.
2. Evaluate the error at the new parameter vector.
3. If the error has increased as a result the update, then retract the step (i.e. reset the weights to their previous values) and increase l by a factor of 10 or some such significant factor, then goes to (1) and try an update again.
4. If the error has decreased as a result of the update, then accept the step (i.e. keep the weights at their new values) and decrease l by a factor of 10 or so.
Results and Discussion
 The study of multi artificial neural network learning algorithm by using base Levenberg–Marquardt was the best choice to estimate function experimental data convergence. Artificial neural networks databases were generated by experimental measurement data condition scales.
It has been observed that the artificial neural networks could be used in height control. The function estimation problem with parameters in Levenberg–Marquardt algorithm showed a high performance and has a high speed, the error in the most cases were decrease and show a high convergence. Sum square error between ANN predictions and experimental measurements was less than 0.001 and correlation coefficient is above 0.99.
Conclusions
ANN method was capable to predict and capture the behavior of experimental measurements.
ANN method can easily be used to determine new results with considerably less computational cost and time. Results show that the back-propagation method with Levenberg-Marquardt learning rule was suitable for training the networks.
 The Sum square error between ANN predictions and experimental measurements was less than 0.001 and the correlation coefficient is above 0.99.
Replacement of the identity matrix with the diagonal of the Hessian in Levenberge-Marquardt update equation has great advantages in convergence and computation time",Ferdowsi University of Mashhad Press,"Modeling of the height control system using artificial neural networks
				دوره6 شماره2 سال1394",,10.22067/jam.v6i2.45162,,core
275728661,2016-01-01T00:00:00,"International audienceTo satisfy the increasing demand for wireless systems capacity, the industry is dramatically increasing the density of the deployed networks. Like other wireless technologies, Wi-Fi is following this trend, particularly because of its increasing popularity. In parallel, Wi-Fi is being deployed for new use cases that are atypically far from the context of its first introduction as an Ethernet network replacement. In fact, the conventional operation of Wi-Fi networks is not likely to be ready for these super dense environments and new challenging scenarios. For that reason, the high efficiency wireless local area network (HEW) study group (SG) was formed in May 2013 within the IEEE 802.11 working group (WG). The intents are to improve the “real world” Wi-Fi performance especially in dense deployments. In this context, this work proposes a new centralized solution to jointly adapt the transmission power and the physical carrier sensing based on artificial neural networks. The major intent of the proposed solution is to resolve the fairness issues while enhancing the spatial reuse in dense Wi-Fi environments. This work is the first to use artificial neural networks to improve spatial reuse in dense WLAN environments. For the evaluation of this proposal, the new designed algorithm is implemented in OPNET modeler. Relevant scenarios are simulated to assess the efficiency of the proposal in terms of addressing starvation issues caused by hidden and exposed node problems. The extensive simulations show that our learning-based solution is able to resolve the hidden and exposed node problems and improve the performance of high-density Wi-Fi deployments in terms of achieved throughput and fairness among contending nodes. © 2016, Jamil et al",'Springer Science and Business Media LLC',Novel learning-based spatial reuse optimization in dense WLAN deployments,,10.1186/s13638-016-0632-2,,core
197921418,2015-11-11T00:00:00,"International audienceThe International Workshop on Principles of Diagnosis (DX) is an annual event that started in 1989 and is rooted in the Artificial Intelligence (AI) community. This annual international workshop has been uniting researchers and practitioners with diverse backgrounds (Artificial Intelligence (AI), verification, software engineering, debugging, ...) in order to leverage research in the global context of diagnosis, that is, identifying the root causes for encountered issues. The DX Workshop series has been offering a forum to present current research and experience reports, exchange and discuss emerging ideas, as well as debate current issues and envisioned future challenges.Papers presented at the workshop cover a variety of theories, principles, and computational techniques for diagnosis, monitoring, testing, reconfiguration, fault- adaptive control, and repair of complex systems. Applications of these theories, principles, and techniques to industry-related disciplines and other real-world problems are also important topics of the workshop.Like the previous workshops in this series, DX-2015 encourages the interactions and the exchange of theories, techniques, applications, and experiences amongst researchers and practitioners from diverse backgrounds – artificial intelligence, control theory, verification, software and systems engineering, and other related areas – who share an interest in different aspects of diagnosis, and the related fields of testing, reconfiguration, maintenance, prognosis, and fault-adaptive control.DX is a lively forum that has traditionally adopted a single track program with a limited number of participants in order to promote detailed technical exchange and debate while at the same time making efforts to develop synergistic approaches to solving real-world problems",HAL CCSD,Proceedings of the 26th International Workshop on Principles of Diagnosis (DX-2015),,,,core
81111337,2016-12-31,"AbstractTransferring predictive microbial models from research into real world food manufacturing or risk assessment applications is still a challenge for members of the food safety modelling community. Such knowledge transfer could be facilitated if publicly available food safety model repositories would exist.This research therefore aimed at identification of missing resources hampering the establishment of community driven food safety model repositories. Existing solutions in related scientific disciplines like Systems Biology and Data Mining were analyzed.On the basis of this analysis, some factors which would promote the establishment of community driven model repositories were identified – among them: a standardized information exchange format for models and rules for model annotation. As a consequence a proposal for a Predictive Modelling in Food Markup Language (PMF-ML) together with a prototypic implementation on the basis of the Systems Biology Markup Language (SBML) has been developed. In addition the adoption of MIRIAM guidelines for model annotation is proposed. In order to demonstrate the practicability of the proposed strategy, existing predictive models previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models are made publicly available in the first community Food Safety Model Repository called openFSMR (https://sites.google.com/site/openfsmr/).This work illustrates that a standardized information exchange format for predictive microbial models can be established by adoption of resources from Systems Biology. Harmonized description and annotation of predictive models will also contribute to increased transparency and quality of food safety models",'Elsevier BV',Towards Community Driven Food Safety Model Repositories ,https://core.ac.uk/download/pdf/81111337.pdf,10.1016/j.profoo.2016.02.098,,core
43559521,2015-06-25T00:00:00,"In the gaming industry, it has long been popular to combine genres with the aim of
creating games that bring together the best of several worlds. However, there are still
many unexplored combinations with good potential.
This report addresses the planning and development of a game prototype that combines
two game genres, turn-based and real-time strategy. In addition to presenting the results,
the report also discusses the di culties that were encountered and how they were handled.
The prototype contains a working combat system and a basic AI. The prototype is
modular, which means that it is easy for an end user to extend the game with extra content.
Most elements that were considered important from the two genres were implemented, but
there is still room for improvement and further development.
The result is evaluated by means of quality assurance, a process that is commonly used
in the gaming industry. The response has been generally positive, which indicates that
the combination has potential and should be explored further",,Experimentell spelmekanik - Realtidsaction m oter turbaserad strategi,https://core.ac.uk/download/43559521.pdf,,,core
80745813,2016-03-28T00:00:00,"Tese (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Mecânica, 2016.O processo de soldagem GMAW é um dos mais utilizado na produção industrial, devido, entre outras características, a seu alto grau de automação e a vantagem de se poder utilizar em diversas configurações com a maioria dos metais e ligas comerciais existentes. No caso da geometria dos cordões de solda no processo GMAW, diferentes pesquisas têm sido encaminhadas ao controle dos parâmetros operacionais que garantam as características geométricas requeridas, entre as mais importantes, a largura, o reforço e a penetração. Atualmente, diferentes modelos baseados em modelamento empírico ou em inteligência artificial são utilizados para controlar um parâmetro geométrico à vez. Este trabalho propõe uma estratégia que, independente de modelos predefinidos do processo, permite controlar simultaneamente a largura e o reforço dos cordões de solda no processo de soldagem GMAW no modo de transferência metálica por curto-circuito (GMAW-S). O controlador proposto é baseado em agentes inteligentes focados diretamente nas medições de largura e reforço dos cordões de solda. O monitoramento dos parâmetros geométricos é realizado em tempo real utilizando uma única câmera e diferentes metodologias de processamento digital de imagens. A avaliação da estabilidade do processo é realizada em tempo real e emprega-se para sair das regiões de instabilidade nas quais possa incorrer o processo durante a etapa de controle. A metodologia de monitoramento é avaliada como satisfatória utilizando o teste “t” para diferentes combinações de parâmetros de entrada. O tempo de processamento de cada imagem não supera os 3 ms, considerando-se adequado visando etapas de controle com uma taxa de amostragem de 100 Hz. Os resultados experimentais mostram que a implementação da estratégia de controle proposta é viável e consegue atingir simultaneamente diferentes valores de referência de largura e reforço dos cordões de solda.The GMAW process is widely used in industry due to, among others, its easer automation and high productivity. In the case of weld bead geometry in GMAW processes, different researches have been conducted to control operating parameters and to ensure required geometrical characteristics, among the most important: the width, the height and the penetration. Currently, different models, based on empirical modeling or artificial intelligence methodologies, are used to control individual geometric parameters. This work proposes a strategy that, regardless of predefined models, can simultaneously control the width and the height of the weld beads in GMAW-S process. The proposed control system is based on intelligent agents focus on measurements of the weld bead width and height. The geometric parameters monitoring is performed in real time using a single camera and different methods of digital image processing. The evaluation of process stability is performed in real time and employed to avoid the regions of instability in which may incur this process during the control stage. The monitoring methodology is assessed as satisfactory using the “t” test for different combinations of input parameters. The time of the image processing does not exceed 3 ms for each image and is considered appropriate to control steps, which use a 100 Hz sampling rate. The experimental results show that the implementation of the proposed control strategy is feasible in systems control without predefined model, achieving different width and height reference bead values",'Biblioteca Central da UNB',Intelligent agents for simultaneously control of width and height of weld beads of GMAW-S process,,10.26512/2016.03.T.20815,,core
48147934,2016-01-01T00:00:00,"International audienceTo satisfy the increasing demand for wireless systems capacity, the industry is dramatically increasing the density of the deployed networks. Like other wireless technologies, Wi-Fi is following this trend, particularly because of its increasing popularity. In parallel, Wi-Fi is being deployed for new use cases that are atypically far from the context of its first introduction as an Ethernet network replacement. In fact, the conventional operation of Wi-Fi networks is not likely to be ready for these super dense environments and new challenging scenarios. For that reason, the high efficiency wireless local area network (HEW) study group (SG) was formed in May 2013 within the IEEE 802.11 working group (WG). The intents are to improve the “real world” Wi-Fi performance especially in dense deployments. In this context, this work proposes a new centralized solution to jointly adapt the transmission power and the physical carrier sensing based on artificial neural networks. The major intent of the proposed solution is to resolve the fairness issues while enhancing the spatial reuse in dense Wi-Fi environments. This work is the first to use artificial neural networks to improve spatial reuse in dense WLAN environments. For the evaluation of this proposal, the new designed algorithm is implemented in OPNET modeler. Relevant scenarios are simulated to assess the efficiency of the proposal in terms of addressing starvation issues caused by hidden and exposed node problems. The extensive simulations show that our learning-based solution is able to resolve the hidden and exposed node problems and improve the performance of high-density Wi-Fi deployments in terms of achieved throughput and fairness among contending nodes. © 2016, Jamil et al",'Springer Science and Business Media LLC',Novel learning-based spatial reuse optimization in dense WLAN deployments,,10.1186/s13638-016-0632-2,,core
296620763,2015-11-26T14:10:09,"Artificial Neural Networks (ANN) have demonstrated to be powerful tools to model non linear systems, such as high solid content latexes produced by emulsion polymerisation. This system has a great importance in the polymeric industry, essentially for environmental reasons, since they usually have water as continuous phase. In order to propose technical and economically feasible alternatives to control polymeric structure, this work is aimed to develop a new methodology based on artificial neural networks associated with calorimetry to preview polymeric structure. The designed artificial neural networks presented excellent results when tested with process condition variations as well as when they were submitted to test concerning to the variation on the proportion of monomers in the latex formulation. Hence, it was possible to conclude that artificial neural networks, associated to calorimetry, lead to an efficient method to preview the polymer composition in emulsion copolymerizations. © 2005 IEEE.422372242Asua, J.M., Urretabizkaia, A., Arzamendi, G., Unzué, M.J., (1994) J. Polym. Sci.: Part A: Polym. Chem., 32, p. 1779El-Aasser, M.S., Leiza, J.R., Sudol, E.D., (1997) J. Appl. Polym. Sci., 64, p. 1797Lovell, P.A., El-Aasser, M.S., (1999) Emulsion Polymerization and Emulsion Polymers, , Ed. John Wiley and Sons, New YorkMcKenna, T.F., Othman, S., Fevotte, G., Santos, A.M., Hamnmouri, H., (1998) DECHEMA Monographien, 134, p. 567. , Wiley-VCH, BerlinZeaiter, J., Gomes, V.G., Romagnoli, J.A., Barton, G.W., Inferential conversion monitoring and control in emulsion polymerisation through calorimetric measurements (2002) Chemical Engineering Journal, 89, p. 37Vieira, R.A.M., Embiruçu, M., Sayer, C., Pinto, J.C., Lima, E.L., Control strategies for complex chemical processes. Applications in polymerization processes (2003) Computers and Chemical Engineering, 27, p. 1307Vicente, M., Leiza, J.R., Asua, J.M., Maximizing production and polymer quality (MWD and composition) in emulsion polymerization reactors with limited capacity of heat removal (2003) Chemical Engineering Science, 58, p. 215Zorzetto, L.F.M., Maciel Filho, R., Wolf-Maciel, M.R., Process modelling development through artificial neural networks and hybrid models (2000) Computers and Chemical Engineering, 24, p. 1355Fernandes, F.A.N., Lona, L.M.F., Development of polymer resins using neural networks (2002) Polímeros: Ciência e Tecnologia, 12 (3), p. 164Zhang, Z., Friedrich, K., Artificial neural networks applied to polymer composites: A review (2003) Composites Science and Technology., 63, p. 1Haykin, S., (1999) Neural Networks - A Comprehensivefoundation -2nd. Ed., , New York: Macmillan College Publishing CompanyRamirez-Beltran, N.D., Jackson, H., Application of neural networks to chemical process control (1999) Computers & Industrial Engineering, 37, p. 387Yu, D.L., Gomm, J.B., Implementation of neural network predictive control to a multivariable chemical reactor (2003) Control Engineering Practice, 11 (11), p. 1315Laugier, S., Richon, D., Use of artificial neural networks for calculating derived thermodynamic quantities from volumetric property data (2003) Fluid Phase Equilibria, 210, p. 247Boillereaux, L.A., Cadet, C.B., Le Bail, A., Thermal properties estimation during thawing via real-time neural network learning (2003) Journal OfFood Engineering, 57, p. 17Svozil, D., Kvasničva, V., Pospíchal, J., (1997) Chemometrics and Intelligent Laboratory Systems, 39, p. 43Dubé, M.A., Penlidis, A., A systematic approach to the study of multicomponent polymerization kinetics - The butyl acrylate /methyl methacrylate/vinyl acetate example: 1. Bulk Polymerization (1995) Polymer, 36, p. 587Févotte, G., Barudio, J., Guillot, J., An adaptive inferential measurement strategy for on-line monitoring of conversion in polymerization processes (1996) Thermochimica Acta, 289, p. 223Garson, G.D., Interpreting neural network connection weights (1991) Artificial Inteligence Expert, 6, p. 47Othman, N., Santos, A.M., Févotte, G., McKenna, T.F., Evaluation of emulsion polymerisation kinetics using non-linear state estimator (2000) Macromol. Symp., 150, pp. 109-11",'Institute of Electrical and Electronics Engineers (IEEE)',Artificial Neural Networks Associated To Calorimetry To Preview Polymer Composition Of High Solid Content Emulsion Copolymerizations,,10.1109/IJCNN.2005.1556249,,core
78370895,2016-01-01T00:00:00,"Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets",,Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets,https://core.ac.uk/download/78370895.pdf,10.1109/isgt.2016.7781159,,core
161105792,2016-01-01T00:00:00,"Optimisation theory is at the heart of any control process, where we seek to control the behaviour of a system through a set of actions. Linear control problems have been extensively studied, and optimal control laws have been identified. But the world around us is highly non-linear and unpredictable. For these dynamic systems, which don’t possess the nice mathematical properties of the linear counterpart, the classic control theory breaks and other methods have to be employed. But nature thrives by optimising non-linear and over-complicated systems. Evolutionary Computing (EC) methods exploit nature’s way by imitating the evolution process
and avoid to solve the control problem analytically.

Reinforcement Learning (RL) from the other side regards the optimal control problem as a sequential one. In every discrete time step an action is applied. The transition of the system to a new state is accompanied by a sole numerical value, the “reward” that designate the quality of the control action. Even though the amount of feedback information is limited into a sole
real number, the introduction of the Temporal Difference method made possible to have accurate predictions of the value-functions. This paved the way to optimise complex structures, like the Neural Networks, which are used to approximate the value functions.

In this thesis we investigate the solution of continuous Reinforcement Learning control problems by EC methodologies. The accumulated reward of such problems throughout an episode suffices as information to formulate the required measure, fitness, in order to optimise a population of candidate solutions. Especially, we explore the limits of applicability of a specific branch of EC, that of Genetic Programming (GP). The evolving population in the GP case is comprised
from individuals, which are immediately translated to mathematical functions, which can serve
as a control law.

The major contribution of this thesis is the proposed unification of these disparate Artificial Intelligence paradigms. The provided information from the systems are exploited by a step by step basis from the RL part of the proposed scheme and by an episodic basis from GP. This makes possible to augment the function set of the GP scheme with adaptable Neural Networks. In the quest to achieve stable behaviour of the RL part of the system a modification of the Actor-Critic
algorithm has been implemented.

Finally we successfully apply the GP method in multi-action control problems extending the spectrum of the problems that this method has been proved to solve. Also we investigated the capability of GP in relation to problems from the food industry. These type of problems exhibit also non-linearity and there is no definite model describing its behaviour",,Computation Approaches for Continuous Reinforcement Learning Problems,https://core.ac.uk/download/161105792.pdf,,,core
161678092,2015,"Introduction
The goal of this work is to advance the production and use of 52Mn (t½ = 5.6 d, β+: 242 keV, 29.6%) as a radioisotope for in vivo preclinical nuclear imaging. More specifically, the aims of this study were: (1) to measure the excitation function for the natCr(p,n)52Mn reaction at low energies to verify past results [1–4]; (2) to measure binding constants of Mn(II) to aid the design of a method for isolation of Mn from an irradiated Cr target via ion-exchange chromatography, building upon previously published methods [1,2,5–7]; and (3) to perform phantom imaging by positron emission tomography/magnetic resonance (PET/MR) imaging with 52Mn and non-radioactive Mn(II), since Mn has potential dual-modality benefits that are beginning to be investigated [8].

Material and Methods
Thin foils of Cr metal are not available commercially, so we fabricated these in a manner similar to that reported by Tanaka and Furukawa [9]. natCr was electroplated onto Cu discs in an industrial-scale electroplating bath, and then the Cu backing was digested by nitric acid (HNO3). The remaining thin Cr discs (~1 cm diameter) were weighed to determine their thickness (~ 75–85 μm) and arranged into stacked foil targets, along with ~25 μm thick Cu monitor foils. These targets were bombarded with ~15 MeV protons for 1–2 min at ~1–2 μA from a CS-15 cyclotron (The Cyclotron Corporation, Berkeley, CA, USA). The beamline was perpendicular to the foils, which were held in a machined 6061-T6 aluminum alloy target holder. The target holder was mounted in a solid target station with front cooling by a jet of He gas and rear cooling by circulating chilled water (T ≈ 2–5 °C). Following bombardment, these targets were disassembled and the radioisotope products in each foil were counted using a high-purity Ge (HPGe) detector. Cross-sections were calculated for the natCr(p,n)52Mn reaction.
Binding constants of Mn(II) were measured by incubating 54Mn(II) (t½ = 312 d) dichloride with anion- or cation-exchange resin (AG 1-X8 (Cl− form) or AG 50W-X8 (H+ form), respectively; both: 200–400 mesh; Bio-Rad, Hercules, CA) in hydrochloric acid (HCl) ranging from 10 mM-8 M (anion-exchange) and from 1 mM-1 M (cation-exchange) or in sulfuric acid (H2SO4) ranging from 10 mM-8 M on cation-exchange resin only. The amount of unbound 54Mn(II) was measured using a gamma counter, and binding constants (KD) were calculated for the various concentrations on both types of ion-exchange resin.
We have used the unseparated product for preliminary PET and PET/MR imaging. natCr metal was bombarded and then digested in HCl, resulting in a solution of Cr(III)Cl3 and 52Mn(II)Cl2. This solution was diluted and imaged in a glass scintillation vial using a microPET (Siemens, Munich, Germany) small animal PET scanner. The signal was corrected for abundant cascade gamma-radiation from 52Mn that could cause random false coincidence events to be detected, and then the image was reconstructed by filtered back-projection. Additionally, we have used the digested target to spike non-radioactive Mn(II)Cl2 solutions for simultaneous PET/MR phantom imaging using a Biograph mMR (Siemens) clinical scanner. The phantom consisted of a 4×4 matrix of 15 mL conical tubes containing 10 mL each of 0, 0.5, 1.0, and 2.0 mM concentrations of non-radioactive Mn(II)Cl2 with 0, 7, 14, and 27 μCi (at start of PET acquisition) of 52Mn(II)Cl2 from the digested target added. The concentrations were based on previous MR studies that measured spin-lattice relaxation time (T1) versus concentration of Mn(II), and the activities were based on calculations for predicted count rate in the scanner. The PET/MR imaging consisted of a series of two-dimensional inversion-recovery turbo spin echo (2D-IR-TSE) MR sequences (TE = 12 ms; TR = 3,000 ms) with a wide range of inversion times (TI) from 23–2,930 ms with real-component acquisition, as well as a 30 min. list-mode PET acquisition that was reconstructed as one static frame by 3-D ordered subset expectation maximization (3D-OSEM). Attenuation correction was performed based on a two-point Dixon (2PD) MR sequence. The DICOM image files were loaded, co-registered, and windowed using the Inveon Research Workplace software (Siemens)",Helmholtz-Zentrum Dresden - Rossendorf,Cyclotron Production and PET/MR Imaging of 52Mn,,,,core
296643354,2015-11-26T15:02:15,"Euclidean distance geometry is the study of Euclidean geometry based on the concept of distance. This is useful in several applications where the input data consist of an incomplete set of distances and the output is a set of points in Euclidean space realizing those given distances. We survey the theory of Euclidean distance geometry and its most important applications, with special emphasis on molecular conformation problems. © 2014 Society for Industrial and Applied Mathematics.561369Alexandrov, A., (1950) Convex Polyhedra, Gosudarstv. Izdat. Tekhn.-Theor. Lit., , MoscowAlfakih, A., Khandani, A., Wolkowicz, H., Solving Euclidean distance matrix completion problems via semidefinite programming (1999) Comput. Optim. Appl., 12, pp. 13-30Alves, R., Cassioli, A., Mucherino, A., Lavor, C., Liberti, L., Adaptive branching in iBP with Clifford algebra (2013) Proceedings of the Workshop on Distance Geometry and Applications, pp. 65-69. , A. Andrioni, C. Lavor, L. Liberti, A. Mucherino, N. Maculan, and R. Rodriguez, eds., Universidade Federal do Amazonas, ManausAnderson, B., Belhumeur, P., Eren, T., Goldenberg, D., Morse, S., Whiteley, W., Yang, R., Graphical properties of easily localizable sensor networks (2009) Wireless Networks, 15, pp. 177-191Arabie, P., Was Euclid an unnecessarily sophisticated psychologist? (1991) Psychometrika, 56, pp. 567-587Asimow, L., Roth, B., The rigidity of graphs (1978) Trans. Amer. Math. Soc., 245, pp. 279-289Asimow, L., Roth, B., The rigidity of graphs II (1979) J. Math. Anal. Appl., 68, pp. 171-190Aspnes, J., Eren, T., Goldenberg, D., Morse, S., Whiteley, W., Yang, R., Anderson, B., Belhumeur, P., A theory of network localization (2006) IEEE Trans. Mobile Comput., 5, pp. 1663-1678Aspnes, J., Goldenberg, D., Yang, R., On the computational complexity of sensor network localization (2004) Algorithmic Aspects of Wireless Sensor Networks, pp. 32-44. , S. Nikoletseas and J. Rolim, eds., Lecture Notes in Comput. Sci. 3121, Springer, BerlinAuslander, L., Mackenzie, R., (1977) Introduction to Differentiable Manifolds, , Dover, New YorkBahr, A., Leonard, J., Fallon, M., Cooperative localization for autonomous underwater vehicles (2009) Internat. J. Robotics Res., 28, pp. 714-728Barvinok, A., Problems of distance geometry and convex properties of quadratic maps (1995) Discrete Comput. Geom., 13, pp. 189-202Belkin, M., Niyogi, P., Laplacian eigenmaps for dimensionality reduction and data representation (2003) Neural Comput., 15, pp. 1373-1396Belotti, P., Lee, J., Liberti, L., Margot, F., Wächter, A., Branching and bounds tightening techniques for non-convex MINLP (2009) Optim. Methods Softw., 24, pp. 597-634Ben-Israel, A., Mond, B., What is invexity? (1986) J. Aust. Math. Soc. Ser. B, 28, pp. 1-9Benedetti, R., Risler, J.-J., (1990) Real Algebraic and Semi-algebraic Sets, , Hermann, ParisBerger, B., Kleinberg, J., Leighton, T., Reconstructing a three-dimensional model with arbitrary errors (1999) J. ACM, 46, pp. 212-235Berman, H., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T., Weissig, H., Shindyalov, I., Bourne, P., The protein data bank (2000) Nucleic Acid Res., 28, pp. 235-242Biggs, N., (1974) Algebraic Graph Theory, , Cambridge University Press, Cambridge, UKBiggs, N., Lloyd, E., Wilson, R., (1976) Graph Theory 1736-1936, , Oxford University Press, OxfordBiswas, P., (2007) Semidefinite Programming Approaches to Distance Geometry Problems, , Ph.D. thesis, Stanford University, Stanford, CABiswas, P., Lian, T., Wang, T., Ye, Y., Semidefinite programming based algorithms for sensor network localization (2006) ACM Trans. Sensor Networks, 2, pp. 188-220Biswas, P., Liang, T.-C., Toh, K.-C., Wang, T.-C., Ye, Y., Semidefinite programming approaches for sensor network localization with noisy distance measurements (2006) IEEE Trans. Automation Sci. Engrg., 3, pp. 360-371Biswas, P., Toh, K.-C., Ye, Y., A distributed SDP approach for large-scale noisy anchorfree graph realization with applications to molecular conformation (2008) SIAM J. Sci. Comput., 30, pp. 1251-1277Biswas, P., Ye, Y., Semidefinite programming for ad hoc wireless sensor network localization (2004) Proceedings of the 3rd International Symposium on Information Processing in Sensor Networks (IPSN04), pp. 46-54. , ACM, New YorkBiswas, P., Ye, Y., A distributed method for solving semidefinite programs arising from ad hoc wireless sensor network localization (2006) Multiscale Optimization Methods and Applications, pp. 69-84. , W. Hager et al., eds., Nonconvex Optim. Appl. 82, Springer, New YorkBjörner, A., Las Vergnas, M., Sturmfels, B., White, N., Ziegler, G., (1993) Oriented Matroids, , Cambridge University Press, Cambridge, UKBlumenthal, L., (1953) Theory and Applications of Distance Geometry, , Oxford University Press, OxfordBohr, H., Brunak, S., (1996) Protein Folds: A Distance Based Approach, , CRC Press, Boca Raton, FLBokowski, J., Sturmfels, B., On the coordinatization of oriented matroids (1986) Discrete Comput. Geom., 1, pp. 293-306Borg, I., Groenen, P., (2010) Modern Multidimensional Scaling, , 2nd ed., Springer, New YorkBoyd, S., El Ghaoui, L., Feron, E., Balakrishnan, V., (1994) Linear Matrix Inequalities in System and Control Theory, , SIAM, PhiladelphiaBreu, H., Kirkpatrick, D., Unit disk graph recognition is NP-hard (1998) Comput. Geom., 9, pp. 3-24Canny, J., Emiris, I., A subdivision-based algorithm for the sparse resultant (2000) J. ACM, 47, pp. 417-451Carroll, J., Chang, J., Analysis of individual differences in multidimensional scaling via an n-way generalization of ""eckart-Young"" decomposition (1970) Psychometrika, 35, pp. 283-319Carvalho, R., Lavor, C., Protti, F., Extending the geometric build-up algorithm for the molecular distance geometry problem (2008) Inform. Process. Lett., 108, pp. 234-237Cauchy, A.-L., Sur les polygones et les polyèdres (1813) J. Ecole Polytechnique, 16, pp. 87-99Cayley, A., A theorem in the geometry of position (1841) Cambridge Math. J., 2, pp. 267-271Chen, H., (2012) Distance Geometry for Kissing Balls, , preprint, arXiv:1203.2131v2Chevalley, C., (1955) The Construction and Study of Certain Important Algebras, , The Mathematical Society of Japan, TokyoClark, B., Colburn, C., Johnson, D., Unit disk graphs (1990) Discrete Math., 86, pp. 165-177Clore, G., Gronenborn, A., Determination of three-dimensional structures of proteins and nucleic acids in solution by nuclear magnetic resonance spectroscopy (1989) Critical Reviews in Biochemistry and Molecular Biology, 24, pp. 479-564Connelly, R., A counterexample to the rigidity conjecture for polyhedra (1978) Inst. Hautes Études Sci. Publ. Math., 47, pp. 333-338Connelly, R., On generic global rigidity (1991) Applied Geometry and Discrete Mathematics, DIMACS Ser. Discrete Math. Theoret. Comput. Sci. 4, pp. 147-155. , AMS, Providence, RIConnelly, R., Generic global rigidity (2005) Discrete Comput. Geom., 33, pp. 549-563Conway, J., Sloane, N., (1993) Sphere Packings, , Lattices and Groups, Springer, BerlinCoope, I., Reliable computation of the points of intersection of n spheres in Rn (2000) Aust. N. Z. Indust. Appl. Math. J., 42, pp. C461-C477Costa, V., Lavor, C., Mucherino, A., Cassioli, A., Carvalho, L., Maculan, N., Discretization orders for protein side chains J. Global Optim., , to appearCremona, L., (1872) Le Figure Reciproche Nella Statica Grafica, , G. Bernardoni, MilanoCremona, L., (1874) Elementi di Calcolo Grafico, , Paravia, TorinoCrippen, G., Distance geometry for realistic molecular conformations (2013) Distance Geometry: Theory, Methods, and Applications, pp. 315-328. , A. Mucherino, C. Lavor, L. Liberti, and N. Maculan, eds., Springer, New YorkCrippen, G., Havel, T., (1988) Distance Geometry and Molecular Conformation, , Wiley, New YorkCrum Brown, A., On the theory of isomeric compounds (1864) Trans. Roy. Soc. Edinburgh, 23, pp. 707-719Cucuringu, M., Lipman, Y., Singer, A., Sensor network localization by eigenvector synchronization over the Euclidean group (2012) ACM Trans. Sensor Networks, 8, pp. 1-42Cucuringu, M., Singer, A., Cowburn, D., Eigenvector synchronization, graph rigidity and the molecule problem (2012) Inform. Inference, 1, pp. 21-67Dattorro, J., (2005) Convex Optimization and Euclidean Distance Geometry, , Mß oo, Palo AltoDattorro, J., Equality relating Euclidean distance cone to positive semidefinite cone (2008) Linear Algebra Appl., 428, pp. 2597-2600De Leeuw, J., Heiser, W., Theory of multidimensional scaling (1982) Classification Pattern Recognition and Reduction of Dimensionality, pp. 285-316. , P. Krishnaiah and L. Kanal, eds., Handbook of Statist. 2, ElsevierDemaine, E., Gomez-Martin, F., Meijer, H., Rappaport, D., Taslakian, P., Toussaint, G., Winograd, T., Wood, D., The distance geometry of music (2009) Comput. Geom., 42, pp. 429-454Deza, M., Deza, E., (2009) Encyclopedia of Distances, , Springer, BerlinDiestel, R., (2005) Graph Theory, , Springer, New YorkDirac, G., On rigid circuit graphs (1961) Abh. Math. Sem. Univ. Hamburg, 25, pp. 71-76Doherty, L., Pister, K., El Ghaoui, L., Convex position estimation in wireless sensor networks (2001) Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies, Vol. 3 of INFOCOM, IEEE, pp. 1655-1663Donald, B., (2011) Algorithms in Structural Molecular Biology, , MIT Press, BostonDong, Q., Wu, Z., A linear-time algorithm for solving the molecular distance geometry problem with exact inter-atomic distances (2002) J. Global Optim., 22, pp. 365-375Dong, Q., Wu, Z., A geometric build-up algorithm for solving the molecular distance geometry problem with sparse distance data (2003) J. Global Optim., 26, pp. 321-333Dress, A., Havel, T., Distance geometry and geometric algebra (1993) Found. Phys., 23, pp. 1357-1374Dzemyda, G., Kurasova, O., Žilinskas, J., (2013) Multidimensional Data Visualiation: Methods and Applications, , Springer, New YorkDzhafarov, E., Colonius, H., Reconstructing distances among objects from their discriminability (2006) Psychometrika, 71, pp. 365-386Eaton, J., (2002) GNU Octave Manual, , Network Theory LimitedEckart, C., Young, G., The approximation of one matrix by another of lower rank (1936) Psychometrika, 1, pp. 211-218Emiris, I., Mourrain, B., Computer algebra methods for studying and computing molecular conformations (1999) Algorithmica, 25, pp. 372-402Eren, T., Goldenberg, D., Whiteley, W., Yang, Y., Morse, A., Anderson, B., Belhumeur, P., Rigidity, computation, and randomization in network localization (2004) IEEE Infocom Proc., 4, pp. 2673-2684Everitt, B., Rabe-Hesketh, S., (1997) The Analysis of Proximity Data, , Arnold, LondonFeferman, S., Dawson, J., Kleene, S., Moore, G., Solovay, R., Van Heijenoort, J., (1986) Kurt Gödel: Collected Works, 1. , Oxford University Press, OxfordFekete, Z., Jordán, T., Uniquely localizable networks with few anchors (2006) Algorithmic Aspects of Wireless Sensor Networks, pp. 176-183. , S. Nikoletseas and J. Rolim, eds., Lecture Notes in Comput. Sci. 4240, Springer, BerlinForman, G., Zahorjan, J., The challenges of mobile computing (1994) IEEE Comput., 27, pp. 38-47Fudos, I., Hoffmann, C., A graph-constructive approach to solving systems of geometric constraints (1997) ACM Trans. Graphics, 16, pp. 179-216Garey, M., Johnson, D., (1979) Computers and Intractability: A Guide to the Theory of NPCompleteness, , Freeman and Company, New YorkGibson, K., Scheraga, H., Energy minimization of rigid-geometry polypeptides with exactly closed disulfide loops (1997) J. Comput. Chem., 18, pp. 403-415Gluck, H., Almost all simply connected closed surfaces are rigid (1975) Geometric Topology, pp. 225-239. , A. Dold and B. Eckmann, eds., Lecture Notes in Math. 438, Springer, BerlinGlunt, W., Hayden, T.L., Hong, S., Wells, J., An alternating projection algorithm for computing the nearest Euclidean distance matrix (1990) SIAM J. Matrix Anal. Appl., 11, pp. 589-600Gortler, S., Healy, A., Thurston, D., Characterizing generic global rigidity (2010) Amer. J. Math., 132, pp. 897-939Gower, J., Some distance properties of latent root and vector methods in multivariate analysis (1966) Biometrika, 53, pp. 325-338Gower, J., Euclidean distance geometry (1982) Math. Sci., 7, pp. 1-14Gramacho, W., Mucherino, A., Lavor, C., Maculan, N., A parallel BP algorithm for the discretizable distance geometry problem (2012) Proceedings of the Workshop on Parallel Computing and Optimization, Shanghai, pp. 1756-1762. , IEEEGraver, J., Rigidity matroids (1991) SIAM J. Discrete Math., 4, pp. 355-368Graver, J., Servatius, B., Servatius, H., (1993) Combinatorial Rigidity, , AMS, Providence, RIGrippo, L., Sciandrone, M., On the convergence of the block nonlinear Gauss-Seidel method under convex constraints (2000) Oper. Res. Lett., 26, pp. 127-136Grone, R., Johnson, C., De Sá, E., Wolkowicz, H., Positive definite completions of partial Hermitian matrices (1984) Linear Algebra Appl., 58, pp. 109-124Grosso, A., Locatelli, M., Schoen, F., Solving molecular distance geometry problems by global optimization algorithms (2009) Comput. Optim. Appl., 43, pp. 23-27Havel, T., Metric matrix embedding in protein structure calculations (2003) Magnetic Resonance Chem., 41, pp. 537-550Havel, T., Kuntz, I., Crippen, G., The theory and practice of distance geometry (1983) Bull. Math. Biol., 45, pp. 665-720Hendrickson, B., Conditions for unique graph realizations (1992) SIAM J. Comput., 21, pp. 65-84Hendrickson, B., The molecule problem: Exploiting structure in global optimization (1995) SIAM J. Optim., 5, pp. 835-857Henneberg, L., (1886) Statik der Starren Systeme, , Bergstræsser, DarmstadtHenneberg, L., (1911) Die Graphische Statik der Starren Systeme, , Teubner, LeipzigHoai An, L., Solving large scale molecular distance geometry problems by a smoothing technique via the Gaussian transform and D.C. Programming (2003) J. Global Optim., 27, pp. 375-397Hoai An, L.T., Dinh Tao, P., Large-scale molecular optimization from distance matrices by a d.c. Optimization approach (2003) SIAM J. Optim., 14, pp. 77-114Huang, H.-X., Liang, Z.-A., Pardalos, P., Some properties for the Euclidean distance matrix and positive semidefinite matrix completion problems (2003) J. Global Optim., 25, pp. 3-21Hunt, K., (1990) Kinematic Geometry of Mechanisms, , Oxford University Press, OxfordIzrailev, S., Zhu, F., Agrafiotis, D., A distance geometry heuristic for expanding the range of geometries sampled during conformational search (2006) J. Comput. Chem., 26, pp. 1962-1969Jackson, B., Jordán, T., Connected rigidity matroids and unique realization of graphs (2005) J. Combin. Theory Ser. B, 94, pp. 1-29Jackson, B., Jordán, T., On the rigidity of molecular graphs (2008) Combinatorica, 28, pp. 645-658Jackson, B., Jordán, T., Graph theoretic techniques in the analysis of uniquely localizable sensor networks (2009) Localization Algorithms and Strategies for Wireless Sensor Networks: Monitoring and Surveillance Techniques for Target Tracking, pp. 146-173. , G. Mao and B. Fidan, eds., IGI GlobalJohnson, C., Kroschel, B., Wolkowicz, H., An interior-point method for approximate positive semidefinite completions (1998) Comput. Optim. Appl., 9, pp. 175-190Jolliffe, I., (2010) Principal Component Analysis, , 2nd ed., Springer, BerlinKostrowicki, J., Piela, L., Diffusion equation method of global minimization: Performance for standard test functions (1991) J. Optim. Theory Appl., 69, pp. 269-284Krishnaiah, P., Kanal, L., (1982) Theory of Multidimensional Scaling, 2. , North-HollandKrislock, N., (2010) Semidefinite Facial Reduction for Low-Rank Euclidean Distance Matrix Completion, , Ph.D. thesis, University of WaterlooKrislock, N., Wolkowicz, H., Explicit sensor network localization using semidefinite representations and facial reductions (2010) SIAM J. Optim., 20, pp. 2679-2708Kruskal, J., Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis (1964) Psychometrika, 29, pp. 1-27Kruskal, J., Nonmetric multidimensional scaling: A numerical method (1964) Psychometrika, 29, pp. 115-129Kucherenko, S., Belotti, P., Liberti, L., Maculan, N., New formulations for the kissing number problem (2007) Discrete Appl. Math., 155, pp. 1837-1841Kucherenko, S., Sytsko, Y., Application of deterministic low-discrepancy sequences in global optimization (2004) Comput. Optim. Appl., 30, pp. 297-318Laman, G., On graphs and rigidity of plane skeletal structures (1970) J. Engrg. Math., 4, pp. 331-340Laurent, M., Cuts, matrix completions and graph rigidity (1997) Math. Program., 79, pp. 255-283Laurent, M., Polynomial instances of the positive semidefinite and Euclidean distance matrix completion problems (2000) SIAM J. Matrix Anal. Appl., 22, pp. 874-894Laurent, M., Matrix completion problems (2009) Encyclopedia of Optimization, pp. 1967-1975. , 2nd ed., C. Floudas and P. Pardalos, eds., Springer, New YorkLavor, C., On generating instances for the molecular distance geometry problem (2006) Global Optimization: From Theory to Implementation, pp. 405-414. , L. Liberti and N.Maculan, eds., Springer, BerlinLavor, C., Lee, J., Lee-St. John, A., Liberti, L., Mucherino, A., Sviridenko, M., Discretization orders for distance geometry problems (2012) Optim. Lett., 6, pp. 783-796Lavor, C., Liberti, L., Maculan, N., Grover's algorithm applied to the molecular distance geometry problem (2005) Proceedings of the 7th Brazilian Congress of Neural Networks, Natal, BrazilLavor, C., Liberti, L., Maculan, N., Computational experience with the molecular distance geometry problem (2006) Global Optimization: Scientific and Engineering Case Studies, pp. 213-225. , J. Pintér, ed., Springer, BerlinLavor, C., Liberti, L., Maculan, N., (2006) The Discretizable Molecular Distance Geometry Problem, , preprint, arXiv:q-bio/0608012Lavor, C., Liberti, L., Maculan, N., Molecular distance geometry problem (2009) Encyclopedia of Optimization, pp. 2305-2311. , 2nd ed., C. Floudas and P. Pardalos, eds., Springer, New YorkLavor, C., Liberti, L., Maculan, N., A note on ""a Branch-and-Prune Algorithm for the Molecular Distance Geometry Problem"" (2011) Internat. Trans. Oper. Res., 18, pp. 751-752Lavor, C., Liberti, L., Maculan, N., Mucherino, A., The discretizable molecular distance geometry problem (2012) Comput. Optim. Appl., 52, pp. 115-146Lavor, C., Liberti, L., Maculan, N., Mucherino, A., Recent advances on the discretizable molecular distance geometry problem (2012) European J. Oper. Res., 219, pp. 698-706Lavor, C., Liberti, L., Mucherino, A., The interval branch-and-prune algorithm for the discretizable molecular distance geometry problem with inexact distances (2013) J. Global Optim., 56, pp. 855-871Lavor, C., Liberti, L., Mucherino, A., Maculan, N., On a discretizable subclass of instances of the molecular distance geometry problem (2009) Proceedings of the 24th Annual ACM Symposium on Applied Computing, pp. 804-805. , D. Shin, ed., ACM, New YorkLavor, C., Mucherino, A., Liberti, L., Maculan, N., An artificial backbone of hydrogens for finding the conformation of protein molecules (2009) Proceedings of the Computational Structural Bioinformatics Workshop, pp. 152-155. , Washington D.C., IEEELavor, C., Mucherino, A., Liberti, L., Maculan, N., Computing artificial backbones of hydrogen atoms in order to discover protein backbones (2009) Proceedings of the International Multiconference on Computer Science and Information Technology, Mragowo, Poland, IEEE, pp. 751-756Lavor, C., Mucherino, A., Liberti, L., Maculan, N., Discrete approaches for solving molecular distance geometry problems using NMR data (2010) Internat. J. Comput. Biosci., 1, pp. 88-94Lavor, C., Mucherino, A., Liberti, L., Maculan, N., On the solution of molecular distance geometry problems with interval data (2010) Proceedings of the International Workshop on Computational Proteomics, Hong Kong, IEEE, pp. 77-82Lavor, C., Mucherino, A., Liberti, L., Maculan, N., On the computation of protein backbones by using artificial backbones of hydrogens (2011) J. Global Optim., 50, pp. 329-344Lavor, C., Mucherino, A., Liberti, L., Maculan, N., Finding low-energy homopolymer conformations by a discrete approach (2012) Proceedings of the Global Optimization Workshop, , D. Aloise, P. Hansen, and C. Rocha, eds., Universidade Federal do Rio Grande do Norte, NatalLe Grand, S., Elofsson, A., Eisenberg, D., The effect of distance-cutoff on the performance of the distance matrix error when used as a potential function to drive conformational search (1996) Protein Folds: A Distance Based Approach, pp. 105-113. , H. Bohr and S. Brunak, eds., CRC Press, Boca Raton, FLLee, J., Verleysen, M., (2010) Nonlinear Dimensionality Reduction, , Springer, BerlinLeung, N.-H.Z., Toh, K.-C., An SDP-based divide-and-conquer algorithm for large-scale noisy anchor-free graph realization (2009) SIAM J. Sci. Comput., 31, pp. 4351-4372Liberti, L., (2004) Reformulation and Convex Relaxation Techniques for Global Optimization, , Ph.D. thesis, Imperial College London, LondonLiberti, L., Reformulations in mathematical programming: Definitions and systematics (2009) RAIRO Oper. Res., 43, pp. 55-85Liberti, L., Dražic, M., Variable neighbourhood search for the global optimization of constrained NLPs (2005) Proceedings of GO Workshop, Almeria, SpainLiberti, L., Kucherenko, S., (2004) Comparison of Deterministic and Stochastic Approaches to Global Optimization, , Tech. Rep. 2004.25, DEI, Politecnico di MilanoLiberti, L., Lavor, C., On a relationship between graph realizability and distance matrix completion (2013) Optimization Theory, Decision Making, and Operational Research Applications, pp. 39-48. , A. Migdalas, A. Sifaleras, C. Georgiadis, J. Papathanaiou, and E. Stiakakis, eds., Proc. Math. Statist. 31, Springer, BerlinLiberti, L., Lavor, C., Maculan, N., A branch-and-prune algorithm for the molecular distance geometry problem (200",'Society for Industrial & Applied Mathematics (SIAM)',Euclidean Distance Geometry And Applications,,10.1137/120875909,,core
296635711,2015-11-26T14:48:22Z,"In the last years, some large companies have been involved in scandals related to financial mismanagement, which represented a large financial damage to their stockholders. To recover market confidence, certifications for best practices of governance were developed, and in some cases, harder laws were implemented. Companies adhered to these changes as a response to the market, deploying process aware systems (PAS) and adopting the best practices of governance. However, companies demand a rapid response to strategic changes or changes in business models between partners, which may impose serious drawbacks to the adoption of normative PAS to the competitiveness of these companies. Thus, while companies need flexible PAS, flexibility may compromise security. To re-balance the trade-off between security and flexibility, we present in this work an anomaly detection algorithm for PAS. The identification of anomalous events can help the adoption of flexible PAS without the loss of security properties. Copyright © 2011 Inderscience Enterprises Ltd.52121129Agarwal, D.K., An empirical Bayes approach to detect anomalies in dynamic multidimensional arrays (2005) ICDM, pp. 26-33Agrawal, R., Gunopulos, D., Leymann, F., Mining process models from workflow logs (1998) EDBT '98: Proceedings of the 6th International Conference on Extending Database Technology, (1377), pp. 469-483. , Advances in Database Technology - EDDT'98Bezerra, F., Wainer, J., Towards detecting fraudulent executions in business process aware systems (2007) WfPM 2007 - Workshop on Workflows and Process Management, in Conjunction with SYNASC 2007, , Timisoara, RomaniaBezerra, F., Wainer, J., Um método de detec cão de anomalias em logs de processos de negócios (2007) I Brazilian Workshop on Business Process Management, SBC, in Conjunction with Webmedia 2007, , de Toledo, M. B. F. and Madeira, E. M. Eds.: Gramado, RS, BrazilBezerra, F., Wainer, J., Anomaly detection algorithms in business process logs (2008) 10th International Conference on Enterprise Information Systems, pp. 11-18. , Barcelona, SpainBezerra, F., Wainer, J., Anomaly detection algorithms in logs of process aware systems (2008) SAC '08: Proceedings of the 2008 ACM Symposium on Applied Computing, pp. 951-952. , ACM, New York, NY, USACook, J.E., Wolf, A.L., Discovering models of software processes from event-based data (1998) ACM Transactions on Software Engineering and Methodology, 7 (3), pp. 215-249Cook, J.E., Du, Z., Liu, C., Wolf, A.L., Discovering models of behavior for concurrent workflows (2004) Computers in Industry, 53 (3), pp. 297-319De Medeiros, A.K.A., (2006) Genetic Process Mining, , PhD thesis, Technische Universiteit Eindhoven, Eindhoven, ISBN 978-90-386-0785-6De Medeiros, A.K.A., Van Der Aalst, W.M.P., Weijters, A., Workflow mining: Current status and future directions (2003) On the Move to Meaningful Internet Systems, LNCS, 2888. , Meersman, R., Tari, Z. and Schmidt, D. EdsDe Medeiros, A.K.A., Weijters, A.J.M.M., Van Der Aalst, W.M.P., Genetic process mining: A basic approach and its challenges (2006) Lecture Notes in Computer Science, 3812, pp. 203-215. , ISSN 0302-9743Donoho, S., Early detection of insider trading in option markets (2004) KDD-2004 - Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 420-429. , KDD-2004 - Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningFawcett, T., (2004) Roc Graphs: Notes and Practical Considerations for ResearchersFawcett, T., Provost, F., Adaptive fraud detection (1997) Data Mining and Knowledge Discovery, 1, pp. 291-316Hammori, M., Herbst, J., Kleiner, N., Interactive workflow mining-requirements, concepts and implementation (2006) Data and Knowledge Engineering, 56 (1), pp. 41-63. , DOI 10.1016/j.datak.2005.02.006, PII S0169023X05000273, Business Process ManagementHerbst, J., Karagiannis, D., Workflow mining with involve (2004) Computers in Industry, 53 (3), pp. 245-264Lee, W., Xiang, D., Information-theoretic measures for anomaly detection (2001) IEEE Symposium on Security and PrivacyMaruster, L., Van Der Aalst, W.M.P., Weijters, T., Van Den Bosch, A., Daelemans, W., Automated discovery of workflow models from hospital data (2001) Proceedings of the 13th Belgium-Netherlands Conference on Artificial Intelligence (BNAIC 2001), pp. 183-190. , Krse, B., Rijke, M., Schreiber, G. and Someren, M. EdsNoble, C.C., Cook, D.J., Graph-based anomaly detection (2003) KDD '03: Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 631-636. , ACM Press, New York, NY, USAPandit, S., Chau, D.H., Wang, S., Faloutsos, C., Netprobe: A fast and scalable system for fraud detection in online auction networks (2007) 16th International World Wide Web Conference, WWW2007, pp. 201-210. , DOI 10.1145/1242572.1242600, 16th International World Wide Web Conference, WWW2007Pinter, S.S., Golani, M., Discovering workflow models from activities' lifespans (2004) Computers in Industry, 53 (3), pp. 283-296Rozinat, A., Van Der Aalst, W.M.P., Conformance checking of processes based on monitoring real behavior (2008) Information Systems, 33 (1), pp. 64-95. , DOI 10.1016/j.is.2007.07.001, PII S030643790700049XRozinat, A., Van Der Aalst, W.M.P., Conformance testing: Measuring the fit and appropriateness of event logs and process models (2005) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 3812, pp. 163-176. , Business Process Management Workshops - BPM 2005 International Workshops, BPI, BPD, ENEI, BPRM, WSCOBPM, BPS, Revised Selected PapersRozinat, A., Medeiros, A.A., Günther, C., Weijters, A., Van Der Aalst, W.M.P., Towards an evaluating framework for process mining algorithms (2007) Technical Report, Technische Universiteit Eindhoven, , BETA Research School for Operations Management and LogisticsSabhnani, R., Neill, D., Moore, A., Detecting anomalous patterns in pharmacy retail data (2005) Proceedings of the KDD 2005 Workshop on Data Mining Methods for Anomaly DetectionSchimm, G., Mining exact models of concurrent workflows (2004) Comput. Ind., 53 (3), pp. 265-281Van Der Aalst, W.M.P., De Medeiros, A.K.A., Process mining and security: Detecting anomalous process executions and checking process conformance (2005) Electronic Notes in Theoretical Computer Science, 121 (SPEC. ISS.), pp. 3-21. , DOI 10.1016/j.entcs.2004.10.013, PII S1571066105000228Van Der Aalst, W.M.P., Weijters, A.J.M.M., Process mining: A research agenda (2004) Computers in Industry, 53Van Der Aalst, W.M.P., Dongen, B.F., Herbst, J., Maruster, L., Schimm, G., Weijters, A.J.M.M., Workflow mining: A survey of issues and approaches (2003) Data Knowl. Eng., 47 (2), pp. 237-267Van Der Aalst, W.M.P., Weijters, T., Maruster, L., Workflow mining: Discovering process models from event logs (2004) IEEE Trans. Knowl. Data Eng., 16 (9), pp. 1128-1142Wainer, J., Kim, K., Ellis, C.A., A workflow mining method through model rewriting (2005) Groupware: Design, Implementation, and Use: 11th International Workshop, CRIWG 2005, 3706, pp. 184-191. , Fuks, H., Lukosch, S. and Salgado, A. C. Eds.:, Porto de Galinhas, BrazilYang, W.-S., Hwang, S.-Y., A process-mining framework for the detection of healthcare fraud and abuse (2006) Expert Systems with Applications, 31 (1), pp. 56-6",,Fraud Detection In Process Aware Systems,,10.1016/j.datak.2005.02.006,,core
296629277,2015-11-26T14:39:20Z,"Before promising oil basins are effectively developed into productive oil wells, it is necessary to conduct several drilling, completion and interconnection activities at such sites. The scheduling of these activities must satisfy several conflicting restrictions and also try to maximize the oil production within a given time frame. The present study describes a Greedy Randomized Adaptive Search Procedure (GRASP) to schedule the development of offshore oil wells. The results are compared to a Constraint Program (CP) tool developed by Petrobras, used and well accepted. Comparative results conducted in real instances indicate that the implementation of GRASP exceeds the RP tool, and produces significant production increase solutions.21217239Aiex, R.M., Binato, S., Resende, M.G.C., Parallel GRASP with pathrelinking for job shop scheduling (2003) Parallel Computing, 29 (4), pp. 393-430. , AprAiex, R.M., Resende, M.G.C., Pardalos, P.M., Toraldo, G., Grasp with path relinking for the three-index assignment problem (2001) AT&T Labs Research Technical Report, p. 43. , Florham Park, NJBard, J.F., Feo, T.A., Operations sequencing in discrete parts manufacturing (1989) Management Science Hannover, 35 (2), pp. 249-255. , FebBinato, S., Hery, W.J., Loewenstern, D., Resende, E.M.G.C., A greedy randomized adaptive search procedure for job shop scheduling (2002) Essays and Surveys on Metaheuristics, pp. 59-79. , Dordrecht: Kluwer AcademicBresina, J.L., Heuristic-biased stochastic sampling (1996) National Conference on Artificial Intelligence, (13). , Portland. Proceedings of the-K [S.l.: s.n.], 1996. p. 271-278Canuto, S.A., Resende, M.G.C., Ribeiro, E.C.C., Local search with perturbations for the prize-collecting steiner tree problem in graphs (2001) Networks [S.L.], 38, pp. 50-58Colorni, A., Dorigo, M., Maniezzo, V., Ant system for job-shop scheduling (1994) Belgian Journal of Operations Research, Statistics, and Computer Science, [S.L.], 1 (34), pp. 39-53Dorigo, M., Di Caro, G., Gambardella, L.M., Ant algorithms for discrete otimization (1998) Bruxelles: Universite Libre de Bruxelles, p. 36. , Technical Report IRIDIA/98-10). To appear in Artificial Life MIT Press 1999Feo, T.A., Bard, J.F., Holland, E.S., Facility-wide planning and scheduling of printed wiring board assembly (1995) Operations Research New York, 43, pp. 219-230Fleurent, C., Glover, F., Improved constructive multistart strategies for the quadratic assignment problem using adaptive memory (1999) Informs Journal on Computing, Cincinnati, 11 (2), pp. 198-204Glover, F., Tabu search and adaptive memory programming: Advances, applications and challenges (1996) Interfaces in Computer Science and Operations Research, p. 75. , BARR, R.S.HELGASON, R.V.KENNINGTON, J.L. (Ed.). [Dordrecht]: Kluwer AcademicGlover, F., Multi-start and strategic oscillation methods: Principles to exploit adaptive memory (2000) Interfaces in Computer Science and Operations Researchs, p. 24. , [Dordrecht]: BARR, R.S.HELGASON, R.V.KENNINGTON, J.L. (Ed.). Kluwer AcademicGlover, F., Laguna, M., (1997) Tabu Search, p. 382. , Boston: Kluwer AcademicGlover, F., Laguna, M., Marti, E.R., Fundamentals of scatter search and path relinking (2000) Control and Cybernetics Warszawa, 39 (3), pp. 653-684(1999) ILOG Solver 4.4 Reference Manual. [S.l.]: ILOG, , ILOGNascimento, J.M., (2002) Hybrid Computational Tools for the Optimization of the Production of Petroleum in Deep Waters, , Dissertacao (Mestrado) - Universidade de CampinasMarriott, K., Stuckey, P.J., (1998) Programming with constraints: an introduction, p. 467. , Cambridge: Massachusetts Institute of Technology PressResende, M.G.C., Festa, P., An updated bibliography of GRASP (2003) AT&T Labs Research Technical Report TD-5SB7BK, Florham Park, NJ, p. 10. , October 14Resende, M.G.C., Ribeiro, C.C., A GRASP with path-relinking for private virtual circuit routing (2001) AT&T Labs Research Technical Report, Florham Park, NJ, p. 19. , June 15Resende, M.G.C., Ribeiro, C.C., Greedy randomized adaptive search procedure (2001) AT&T Labs Research Technical Report, p. 29. , Florham Park, NJ Sept. Revision 2, Aug. 29, 2002. To appear in -State of the Art Handbook in Metaheuristics-, F. Glover and G. Kochenberger, eds. Kluwer 2002Rodrigo, M., Maniezzo, V., Colorni, A., The Ant System: Optimization by a colony of cooperating agents (1996) IEE Transactions on Systems Man and Cybernetics, 26 (1), pp. 1-13Roy, B., Sussmann, B., Les problemes d-Ordonnancement avec contraintes disjonctive (1964) Note Ds No. 9 Bis, , Paris: SEMAYannakakis, M., Computational complexity (1997) Local search in combinatorial optimization, pp. 19-55. , AARTS, E.LENSTRA, J.K. (Ed.). Chichester: J. Wile",,Grasp Strategy For The Scheduling Of Oil Well Development Activities [estratégia Grasp Para Escalonar Atividades De Desenvolvimento De Poços De Petróleo],,,,core
213151827,2015-09-01T00:00:00,"International audienceThis paper tackles the problem of testing production systems, i.e. systems that run in industrial environments, and that are distributed over several devices and sensors. Usually, such systems are not lacks of models, or are expressed with models that are not up to date. Without any model, the testing process is often done by hand, and tends to be an heavy and tedious task. This paper contributes to this issue by proposing a framework called Autofunk, which combines different fields such as model inference, expert systems, and machine learning. This framework, designed with the collaboration of our industrial partner Michelin, infers formal models that can be used as specifications to perform offline passive testing. Given a large set of production messages, it infers exact models that only capture the functional behaviours of a system under analysis. Thereafter, inferred models are used as input by a passive tester, which checks whether a system under test conforms to these models. Since inferred models do not express all the possible behaviours that should happen, we define conformance with two implementation relations. We evaluate our framework on real production systems and show that it can be used in practice",HAL CCSD,Passive testing of production systems based on model inference,,,,core
84296899,2016,"Neuroscience-based or neuroscience-informed design is a new application area of Brain-Computer Interaction (BCI). It takes its roots in study of human well-being in architecture, human factors study in engineering and manufacturing including neuroergonomics. In traditional human factors studies and/or well-being study, mental workload, stress, and emotion are obtained through questionnaires that are administered upon completion of some task and/or the whole experiment. Recent advances in BCI research allow for using Electroencephalogram (EEG) based brain state recognition algorithms to assess the interaction between brain and human performance. We propose and develop an EEG-based system CogniMeter to monitor and analyze human factors measurements of newly designed software/hardware systems and/or working places. Machine learning techniques are applied to the EEG data to recognize levels of mental workload, stress and emotions during each task. The EEG is used as a tool to monitor and record the brain states of subjects during human factors study experiments. We describe two applications of CogniMeter system: human performance assessment in maritime simulator and EEG-based human factors evaluation in Air Traffic Control (ATC) workplace. By utilizing the proposed EEG-based system, true understanding of subjects working patterns can be obtained. Based on the analyses of the objective real time EEG-based data together with the subjective feedback from the subjects, we are able to reliably evaluate current systems/hardware and/or working place design and refine new concepts and design of future systems",,Neuroscience based design: Fundamentals and applications,,10.1109/CW.2016.52,,core
148021646,2016-04-25T13:18:33,"The need for automation of the identity recognition process for a vast number of applications resulted in great advancement of biometric systems in the recent years. Yet, many studies indicate that these systems suffer from vulnerabilities to spoofing (presentation) attacks: a weakness that may compromise their usage in many cases. Face verification systems account for one of the most attractive spoofing targets, due to the easy access to face images of users, as well as the simplicity of the spoofing attack manufacturing process. Many counter-measures to spoofing have been proposed in the literature. They are based on different cues that are used to distinguish between real accesses and spoofing attacks. The task of detecting spoofing attacks is most often considered as a binary classification problem, with real accesses being the positive class and spoofing attacks being the negative class. The main objective of this thesis is to put the problem of anti-spoofing in a wider context, with an accent on its cooperation with a biometric verification system. In such a context, it is important to adopt an integrated perspective on biometric verification and anti-spoofing. In this thesis we identify and address three points where integration of the two systems is of interest. The first integration point is situated at input-level. At this point, we are concerned with providing a unified information that both verification and anti-spoofing systems use. The unified information includes the samples used to enroll clients in the system, as well as the identity claims of the client at query time. We design two anti-spoofing schemes, one with a generative and one with a discriminative approach, which we refer to as client-specific, as opposed to the traditional client-independent ones. The proposed methods are applied on several case studies for the face mode. Overall, the experimental results prove the integration to be beneficial for creating trustworthy face verification systems. At input-level, the results show the advantage of the client-specific approaches over the client-independent ones. At output-level, they present a comparison of the fusion methods. The case studies are furthermore used to demonstrate the EPS framework and its potential in evaluation of biometric verification systems under spoofing attacks. The source code for the full set of methods is available as free software, as a satellite package to the free signal processing and machine learning toolbox Bob. It can be used to reproduce the results of the face mode case studies presented in this thesis, as well as to perform additional analysis and improve the proposed methods. Furthermore, it can be used to design case studies applying the proposed methods to other biometric modes. At the second integration point, situated at output-level, we address the issue of combining the output of biometric verification and anti-spoofing systems in order to achieve an optimal combined decision about an input sample. We adopt a multiple expert fusion approach and we investigate several fusion methods, comparing the verification performance and robustness to spoofing of the fused systems. The third integration point is associated with the evaluation process. The integrated perspective implies three types of inputs for the biometric system: real accesses, zero-effort impostors and spoofing attacks. We propose an evaluation methodology for biometric verification systems under spoofing attacks, called Expected Performance and Spoofability (EPS) framework, which accounts for all the three types of input and the error rates associated with them. Within this framework, we propose the EPS Curve (EPSC), which enables unbiased comparison of systems","Lausanne, EPFL",Trustworthy Biometric Verification under Spoofing Attacks:Application to the Face Mode,https://core.ac.uk/download/148021646.pdf,10.5075/epfl-thesis-6879,,core
42675701,2016-01-25T00:00:00,"The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios",,Teaching Robots to Do Object Assembly using Multi-modal 3D Vision,http://arxiv.org/abs/1601.06473,,,core
76525985,2015,"Mastering the finest art of ‘mechatronics' currently looks one of the most attractive task of modern engineering technology and science. Many are the applications which resort to the interdisciplinary approach of mechatronics to enhance the performance, quality and safety of either product or process. Some are very traditional, like hard disk drives, biomedical, automotive and aerospace systems, other are fairly new like micro and nano electromechanical systems, unmanned air vehicles, intelligent machining and manufacturing systems or bioinspired devices. A first generation of mechatronic products was conceived to embed a suitable ‘smartness' to improve the skill of self-adapting to any abrupt variation of operating conditions, by resorting to the ‘synergistic integration of mechanical engineering with electronics and control in the design and manufacturing of product process' as mechatronics was brightly defined. Nowadays, a mechatronic design is surely based on its interdisciplinary nature, but its real meaning was harmonized with an effective contamination among different application domains, methodologies and technologies, being smartly applied to reach the highest result in any product, system and process development. A recent experience within the frame of the EMEA District of the American Society of Mechanical Engineers (ASME) was a chance to get an impression of the scientific and industrial research activity performed in some fields of mechatronics. Some exciting examples describing how different competences, disciplines, technologies met in an innovative mechatronic system are herein exposed by some researchers of the EMEA area of the world. They deal with several domains, like the hard disk drive technology, biomedical prostheses, fluidic automation, UAV Vision System, vibration monitoring and suppression in steelmaking plants, materials machining and smart composites. These examples will narrate to the reader who is still looking for the meaning of mechatronics how some approaches, as neural network positioning control, chaos prevention, myoelectric stimulation of prosthesis, human detection by vision system, multi-physics modeling and control of dynamics are currently implemented in a sort of artificial intelligence in small scale device, as in a finger of a biotronic hand or in a large equipment like an electric arc furnace. Moreover, the reader will realize how intensively this goal is achieved by exploiting the available technologies as additive manufacturing or fiber optics embedded into composite structures to reduce the cost, weight or volume of the product or to improve the quality and accuracy of a material processing like in rolling or in turning against the risk of self-excited chatter vibration. This scenario is covering a wide range of mechatronic applications, although many others are currently developed in several fileds of engineerin","NOVA Science Publishers, Inc","Mechatronics: Principles, Technologies and Applications",,,,core
31250254,2015-06-01T00:00:00,"National audienceIn machine learning, empirical performance on real data are crucial in the success of a method. Recent years have seen the emergence of a large number of machine learning competitions. These challenges are motivated by industrial (Netflix prize) or academic (HiggsML challenge) applications and put in competition researchers and data scientists to obtain the best performance. We wanted to expose students to this reality bysubmitting a challenge in the context of the machine learning course. The leaderboard is displayed on an automatically updated web page allowing emulation among students. The history of the results also allows them to visualize their progress through the submissions. In addition, the challenge can continue outside of the supervised sessions promoting independence and exploration of new learning techniques and computer tools. The system we have implemented is available as an R package for reuse by other teachers. Building on the R Markdown and Dropbox tools, it requires no network configuration and can be deployed very easily on a personal computer.En apprentissage automatique, les performances empiriques obtenues sur données réelles sont déterminantes dans le succès d'une méthode. Ces dernières années ont vu l'apparition d'un grand nombre de compétitions d'apprentissage automatique. Ces challenges sont motivés par des applications industrielles (prix Netflix) ou académiques (challenge HiggsML) et mettent en compétition chercheurs et data scientists pour obtenir les meilleures performances. Nous avons souhaité confronter les étudiants à cette réalité en leur soumettant un challenge dans le cadre du cours d'apprentissage automatique. Leur classement est affiché sur une page web mise à jour automatiquement permettant une émulation parmi les étudiants. L'historique des résultats leur permet également de visualiser leur progression au fil des soumissions. De plus, le challenge peut se poursuivre en dehors des sessions encadrées favorisant l'autonomie et l'exploration de nouvelles techniques d'apprentissage et outils informatiques. Le système que nous avons mis en œuvre est disponible sous forme de package R afin d'être réutilisé par d'autres enseignants. S'appuyant sur les outils R Markdown et Dropbox, il ne nécessite aucune configuration réseau et peut être déployé très facilement sur un ordinateur personnel",HAL CCSD,Compétitions d'apprentissage automatique avec le package R rchallenge,,,,core
54535597,2015-01-01T00:00:00,"Questo volume vuole evidenziare l’importanza di una modellizzazione

dei rischi bancari, in particolare di quelli associati all’attività di

credito e di negoziazione in titoli da parte della banca, che sia coerente

con le caratteristiche empiriche dei dati finanziari che, attualmente, non

sembrano confermare l’ipotesi tradizionale di normalità dei rendimenti.

Se la validità di un modello finanziario poggia sulla veridicità delle sue

ipotesi sottostanti, è anche vero che il contributo operativo di un modello

teorico è legato alla sua facilità d’implementazione e al suo basso sforzo

computazionale. Nella prima parte del volume si presentano, quindi,

dei modelli avanzati ma, allo stesso tempo, semplici da implementare

finalizzati ad una stima più realistica e “coerente” dell’ammontare dei

rischi di credito e di mercato dell’attività bancaria; nella seconda parte del

volume si descrivono dei modelli di gestione degli stessi rischi basati su un

approccio di portafoglio in cui le soluzioni del problema di minimizzazione

del rischio possono suggerire alla banca come riallocare coerentemente

il proprio capitale tra le diverse attività migliorando allo stesso tempo il

proprio profilo di rischio e rendimento. Successivamente questi modelli

sono stati implementati ai dati finanziari, reali e simulati, per testare il

grado di performance degli stessi in un’ottica comparativa con i modelli

più tradizionali utilizzati dall’industria finanziaria internazionale.This book aims to highlight the importance of a banking risk modeling, in particular of the financial risks associated with the activity of lending and trading in securities from the bank, which is consistent with the empirical characteristics of financial data that currently do not seem to confirm the traditional hypothesis of normality distribution. If the validity of a financial model is based on the truthfulness of his assumptions, it is also true that its operational contribution is related to the ease of implementation and its low computational efforts.

Therefore, in the first part of this volume we present some advanced models but, at the same time, simple to implement aimed at a more realistic and “coherent” estimate of the amount of credit and market risks of the banking activity. In the second part of this volume, we describe some advanced risk management models based on a portfolio approach. Le solutions of the portfolio risk minimization problem may suggest to the banking managers how consistently reallocate the total capital among the various financial assets improving the banking risk-return profile. Subsequently these same models have been implemented to financial data, real and simulated, in order to test the degree of performance of these models in a comparative perspective with some more traditional approaches used currently by the international financial industry.

In conclusion, we find in our experiments that the CVaR measure is a risk metric superior to the traditional VaR, overall in a perspective of financial risk management. In fact, only the CVaR is able to taking into account the diversification effects in terms of portfolio risk mitigation. In addition, we underline the utility of using copula functions for modeling the dependence structure among the assets in portfolio. In particular, this is true in the case of the Student’s t-copulas that by means of the tail dependence property are able to capture the “extreme” events of joint defaults. We think that the adoption of these new mathematical tools and of the advanced models based on these properties may enforce the grade of stability of the banking sector",McGraw-Hill Education,Modellizzazione e gestione dei rischi finanziari attraverso un approccio di portafoglio,,,,core
90251668,2016-09-01T00:00:00Z,"For decades, PID (Proportional + Integral + Derivative)-like controllers have been successfully used in academia and industry for many kinds of plants. This is thanks to its simplicity and suitable performance in linear or linearized plants, and under certain conditions, in nonlinear ones. A number of PID controller gains tuning approaches have been proposed in the literature in the last decades; most of them off-line techniques. However, in those cases wherein plants are subject to continuous parametric changes or external disturbances, online gains tuning is a desirable choice. This is the case of modular underwater ROVs (Remotely Operated Vehicles) where parameters (weight, buoyancy, added mass, among others) change according to the tool it is fitted with. In practice, some amount of time is dedicated to tune the PID gains of a ROV. Once the best set of gains has been achieved the ROV is ready to work. However, when the vehicle changes its tool or it is subject to ocean currents, its performance deteriorates since the fixed set of gains is no longer valid for the new conditions. Thus, an online PID gains tuning algorithm should be implemented to overcome this problem. In this paper, an auto-tune PID-like controller based on Neural Networks (NN) is proposed. The NN plays the role of automatically estimating the suitable set of PID gains that achieves stability of the system. The NN adjusts online the controller gains that attain the smaller position tracking error. Simulation results are given considering an underactuated 6 DOF (degrees of freedom) underwater ROV. Real time experiments on an underactuated mini ROV are conducted to show the effectiveness of the proposed scheme",MDPI AG,Neural Network-Based Self-Tuning PID Control for Underwater Vehicles,,10.3390/s16091429,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
103073867,2015-10-31,"Robots are gradually entering into diverse ap-plication domains such as home, office, and playing field. This article presents advanced re-search activities related to these domains. First is RoboCup which is an attempt to promote AI and robotics research by providing a common task for evaluation of various performance, the-ories, algorithms, and robot architectures. In order for robots (both physical robots and soft agents) to play a soccer game reasonably well, a wide range of technologies need to be inte-grated and a number of technical breakthrough must be accomplished. The recent results from the last two RoboCups are reviewed and future leagues are introduced. Second, the richer do-main of service robotics has also received signif-icant interest recently. The task here is to serve as a human assistant in an office or domestic en-vironment, for tasks like cleaning and delivery. The human-robot interaction is a key issue to success, which poses new challenges in terms of integration of spoken dialogue, gestures, body language, etc. In addition mobile manipulation and safe navigation around humans is essential to success. These two areas integrates many different disciplines including control, percep-tion, natural language processing, hybrid sys-tems and handling of uncertainty, and applied to tour guiding, mail delivery, domestic ser-vices, and rescue activities. 1 In t roduct ion Robotics offers a fertile ground for demonstration of ar-tificial intelligence techniques. The domain provides a basis for real-world evaluation of techniques under real-istic assumptions. Construction of robotic systems re-quires at the same time integration of a diverse range of expertise in order to provide operational systems. Tradi-tionally robot systems have been used in manufacturing, in particular in the car industry. The majority of robots sold today are still deployed for spot-welding and ca",,"office, and playing field Robotics in the home,",,,,core
80739260,2015-06-03T00:00:00,"Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup",'Elsevier BV',Artificial cognitive control with self-x capabilities: A case study of a micro-manufacturing process,http://oa.upm.es/43641/1/INVE_MEM_2015_235909.pdf,10.1016/j.compind.2015.05.001,,core
478705930,2016-01-01T00:00:00,"For many years, NTNU in Ålesund (formerly Aalesund University College) has maintained a close relationship with the maritime industrial cluster, centred in the surrounding geographical region, thus acting as a hub for both education, research, and innovation. Of many common relevant research topics, virtual prototyping is currently one of the most important. In this paper, we describe our first complete version of a generic and modular software framework for intelligent computer-automated product design. We present our framework in the context of design of offshore cranes, with easy extensions to other products, be it maritime or not. Funded by the Research Council of Norway and its Programme for Regional R&D and Innovation (VRI), the work we present has been part of two separate but related research projects (grant nos. 241238 and 249171) in close cooperation with two local maritime industrial partners. We have implemented several software modules that together constitute the framework, of which the most important are a server-side crane prototyping tool (CPT), a client-side web graphical user interface (GUI), and a client-side artificial intelligence for product optimisation (AIPO) module that uses a genetic algorithm (GA) library for optimising design parameters to achieve a crane design with desired performance. Communication between clients and server is achieved by means of the HTTP and WebSocket protocols and JSON as the data format. To demonstrate the feasibility of the fully functioning complete system, we present a case study where our computer-automated design was able to improve the existing design of a real and delivered 50-tonnes, 2.9 million EUR knuckleboom crane with respect to some chosen desired design criteria. Our framework being generic and modular, both clientside and server-side modules can easily be extended or replaced. We demonstrate the feasibility of this concept in an accompanying paper submitted concurrently, in which we create a simple product optimisation client in Matlab that uses readily available toolboxes to connect to the CPT and optimise various crane designs by means of a GA. In addition, our research team is currently developing a winch prototyping tool to which our existing AIPO module can connect and optimise winch designs with only small configuration changes. This work will be published in the near future",ECMS European Council for Modelling and Simulation,A Software Framework For Intelligent Computer-Automated Product Design,,,,core
42412712,2015-10-01T00:00:00,"© Emerald Group Publishing Limited. Purpose - The focus of this work is on the client-designer interface where decisions have significant impact over the lifecycle of the project. Therefore, the briefing stage is examined in the context of clients needs which is divided into project-based strategy and broader clients strategy. The purpose of this paper is to address the pitfalls in the briefing process which has been attributed to the shortcomings in the client-designer communication interfaces. This will be achieved by developing an automated brief generation framework. The research examines the efficiency of standard approaches to modelling and design, and the benefits that these methodologies have offered to the computer industry. The work reviews the similarities between the two industries and argues in support of the potential benefits in adopting a standard methodology in the construction industry. The structure upon which the framework is developed is based on system analysis and design methodology (SSADM) which has proven to be an effective platform used within the software development industry. Design/methodology/approach - SSADM is an established methodology within the software development industry. The paper will demonstrate that due to fundamental similarities between the construction and software development industries, SSADM is likely to offer a viable platform upon which an automated enhanced brief generation model is developed for use in the construction industry. The construction design and construction process will be mapped on SSADM high-level definition before focusing and honing on the design phase. The methodology for the development of the framework will be based on the rationalist approach of generating knowledge through reasoning leading to model-building. Findings - A model that is based on SSADM is proposed for the design development phase of construction projects. In order to shape the project strategy, the model considers the combined role of clients requirements with organisation strategy and environmental factors. The paper has shown that it is feasible to increase the automation of the briefing process and enhanced the briefing output. The model here does not diminish the importance of direct communication between the client and the design team. It provides a more structured way of doing so, while taking advantage of vast array of data and technology in order to improve the brief outcome. Research limitations/implications - From practical perspective, the proposed framework is in its formative stage, thus requiring incremental refinement through several case studies. This is particularly true about the AI components of the system which typically rely on extensive data representing the real-case scenarios. Therefore, the work invites further research into the examination of various parts as well as the overall system. Practical implications - There are several ways by which construction projects are procured. There may be fluctuation in their rate of usage, but while there is no indication of any procurement option fading, new ones such as PPP and PFI are periodically introduced. The existence of this diversity is indicative of the fact that the industry tends to respond to problems rather than attempting to instigate a measured solution supported by theoretical underpinning. Subsequently, there have been suggestions of a communication and information discourse between actors and within processes involved in project lifecycle. This project is aimed at addressing the gap in the client-designer communication. The automated approach to brief generation will lead to better briefs while reducing ambiguities as well as the overhead associated with brief generation. Social implications - The quality of project brief has a significant impact on decisions at the design stage. In turn, these decisions will influence all phases of construction project lifecycle. The briefing session and requirement analysis of a construction project can be very difficult for inexperienced clients particularly for complex projects. Therefore, there is potential for the process of client-requirement-analysis to be optimised. The work promises to improve the quality of the briefing process, thus helping clients to realise their intended objectives and minimise resource waste. Originality/value - The work builds on the commonalities of the construction and software development industries and takes advantage of the advancements in the latter. In doing so, project quality is defined quantitatively which is used to develop project strategy in a three-dimensional space. The development of the model was also contingent upon enhancement of artificial neural network structure",'Emerald',Enhanced project brief: Structured approach to client-designer interface,https://core.ac.uk/download/42412712.pdf,10.1108/ECAM-10-2014-0128,,core
296637354,2015-11-26T14:50:53,"Although grid technologies have been embraced by academia and industry as a viable solution to build integrated systems out of heterogeneous resources, a number of challenges still hamper their widespread acceptance and use. One particular challenge has proven difficult to overcome: the transition of legacy code into grid environments. The ability to adapt legacy applications to benefit from grid resources is vital to the success of grid technologies, since re-writing them is not a practical solution in many settings. Financial institutions are perhaps the most clear case, since they use complex, sensitive and resource-demanding software that can greatly benefit from grid technologies but cannot afford to be significantly re-written. This paper describes the efforts conducted to modify legacy financial applications to be executed under a commercial grid middleware named Sparsi Maestro. We describe the steps involved in the transition of a legacy application into the grid environment and present two examples of actual financial applications that have undergone that process. © 2011 ACM.3945ACM SIGARCH,IEEE Computer Society (IEEE CS)Amdahl, G., Validity of the single-processor approach to achieving large scale computing capabilities (1967) AFIPS Conference, pp. 483-485Baduel, L., Baude, F., Caromel, D., Contes, A., Huet, F., Morel, M., Quilici, R., Programming, deploying, composing, for the grid (2006) Grid Computing: Software Environments and Tools, , Springer LondonChandra, R., Dagum, L., Kohr, D., Maydan, D., Mcdonald, J., Menon, R., (2001) Parallel Programming in OpenMP, , Morgan Kaufmann Publishers Inc., San Francisco, CA, USAChien, A., Calder, B., Elbert, S., Bhatia, K., Entropia: Architecture, performance of an enterprise desktop grid system (2003) Journal of Parallel and Distributed Computing, 63 (5)Childers, L., Disz, T., Olson, R., Papka, M.E., Stevens, R., Udeshi, T., Access grid: Immersive group-to-group collaborative visualization Proceedings of the Fourth International Immersive Projection Technology Workshop, , June 19-20Delaitre, T., Kiss, T., Goyeneche, A., Terstyanszky, G., Winter, S., Kacsuk, P., GEMLCA: Running legacy code applications as Grid services (2005) Journal of Grid Computing, 3 (1-2), pp. 75-90. , DOI 10.1007/s10723-005-9002-8Foster, I., Globus toolkit version 4: Software for service-oriented systems (2006) Journal of Computer Science and Technology, 21 (4)Ho, Q., Ong, Y., Cai, W., Gridifying aerodynamic design problem using GridRPC Grid and Cooperative Computing(Lecture Notes in Computer Science, 3032, pp. 83-90. , Springer Berlin / HeidelbergKommineni, J., Abramson, D., GriddLeS enhancements and building virtual applications for the GRID with legacy components (2005) Lecture Notes in Computer Science, 3470, pp. 961-971. , Advances in Grid Computing - EGC 2005: European Grid Conference, Revised Selected PapersMateos, C., Zunino, A., Campo, M., A survey on approaches to gridification (2008) Software - Practice and Experience, (38)Natrajan, A., Humphrey, M.A., Grimshaw, A.S., The legion support for advanced parameter space studies on a grid (2002) Future Generation Computer Systems, 18 (8)Project, B.B.J., (2011) Reliable Multicasting with the Jgroups Toolkit, , http://www.jgroups.org/manual/, SeptemberSnir, M., Otto, S., Huss-Lederman, S., Walker, D., Dongarra, J., (1998) MPI - The Complete Reference, Volume 1: The MPI Core, , MIT Press, Cambridge, MA, USA, 2nd. (revised) editionSynapse, D., (2010) Gridserver: High Performance Application Infrastructure for Your Business Critical Applications, , http://www.datasynapse.com/gridserver, MayThain, D., Tanenbaum, T., Livny, M., Condor and the grid (2003) Grid Computing: Making the Global Infrastructure a Reality, , John Wiley & SonsVadhiyar, S., Dongarra, J., Grads gridification of numeric applications based on globus and mpi (2005) Self Adaptability in Grid Computing. Concurrency and Computation: Practice and Experience, 17 (2-4). , Special Issue on Grid PerformanceWang, B., Xu, Z., Xu, C., Yin, Y., Ding, W., Yu, H., A study of gridifying scientific computing legacy codes (2004) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 3251, pp. 404-412Willard, C.G., (2010) Univa UD's Strategy for Subduing the Complex Software Monster, , http://www.univaud.com/about/resources/files/wp-tabor-research.pdf, Ma",'Association for Computing Machinery (ACM)',Practical Experiences On The Gridification Of Financial Applications,,10.1145/2088256.2088269,,core
199270216,2016-01-01T00:00:00,"The paper provides a state of the art review of guided wave based structural health monitoring (SHM). First, the fundamental concepts of guided wave propagation and its implementation for SHM is explained. Following sections present the different modeling schemes adopted, developments in the area of transducers for generation, and sensing of wave, signal processing and imaging technique, statistical and machine learning schemes for feature extraction. Next, a section is presented on the recent advancements in nonlinear guided wave for SHM. This is followed by section on Rayleigh and SH waves. Next is a section on real-life implementation of guided wave for industrial problems. The paper, though briefly talks about the early development for completeness,. is primarily focussed on the recent progress made in the last decade. The paper ends by discussing and highlighting the future directions and open areas of research in guided wave based SHM",'IOP Publishing',Guided wave based structural health monitoring: A review,,10.1088/0964-1726/25/5/053001,"[{'title': 'Smart Materials and Structures', 'identifiers': ['issn:1361-665X', 'issn:0964-1726', '0964-1726', '1361-665x']}]",core
54527048,2015-01-01T00:00:00,"In-memory (transactional) data stores, also referred to as data grids, are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model.

On the other hand, determining how performance and reliability/availability of these systems vary as a function of configuration parameters, such as the amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on the amount of cloud resources that are planned for usage.

To cope with the issue of predicting/analysing the behavior of different configurations of cloud in-memory  data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily  specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific (distributed) protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being  not feasible to be modeled via white-box (namely purely simulative) approaches.

 We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures. Particularly,   our validation test-bed has been based on an industrial-grade open-source data grid, namely Infinispan by JBoss/Red-Hat, and a de-facto standard benchmark for NoSQL platforms, namely YCSB by Yahoo.

The validation study has been conducted by relying on both public and private cloud systems, scaling the underlying infrastructure up to 100 (resp. 140) Virtual Machines for the public (resp. private) cloud case. Further, we provide some experimental data related to a scenario where our framework is used for on-line capacity planning and reconfiguration of the data grid system",'Elsevier BV',A flexible framework for accurate simulation of cloud in-memory data stores,https://core.ac.uk/download/54527048.pdf,10.1016/j.simpat.2015.05.011,,core
213619757,2015-08-01T00:00:00,"RÉSUMÉ
Le matériel informatique graphique destiné aux ordinateurs de bureau ou aux systèmes embarqués traditionnels, ainsi que leur interface de programmation ne peuvent pas être utilisés dans les systèmes avioniques puisqu’ils ne se conforment pas aux règles de certifications DO-254 et DO-178B. Toutefois, on remarque le faible nombre d’outils de conceptions qui encadrent le développement d’applications graphiques avioniques, et ce malgré l’apparition de matériel graphique avionique de plus en plus performants. Suivant par exemple la méthode classique de conception en V, les ingénieurs doivent d’abord effectuer des choix de conception reliés à la sélection du matériel graphique avant de débuter une quelconque implémentation de code. Ainsi, il peut être difficile d’évaluer la pertinence de ces choix en évaluant les performances de traitement du matériel graphique puisque l’engin graphique n’aurait pas nécessairement été développé. Je propose donc un outil de conception permettant de prédire les performances de matériel graphique en termes d’images par secondes (FPS), basé sur OpenGL SC. L’outil crée des modèles non-paramétriques de performance du matériel en analysant, à l’aide d’algorithmes d’apprentissage, le temps de dessin de chaque image, lors du rendu d’une scène 3D synthétique. Cette scène est rendue à quelques reprises en faisant varié certaines de ses caractéristiques spatiales (nombre de sommets, taille de la scène, taille des textures, etc.) qui font parte intégrante du logiciel de vision synthétique habituellement développé dans ce domaine. Le nombre de combinaisons de ces caractéristiques utilisées durant l’entraînement supervisé des modèles de performance n’est qu’un très petit sous-ensemble de toutes les combinaisons, le but étant de prédire par extrapolation celles manquantes. Pour valider les modèles, une scène 3D fournie par un partenaire industriel est dessinée avec des caractéristiques non traitées durant la phase d’entraînement, puis le FPS de chaque image rendue est comparé au FPS prédit par le modèle. La tendance centrale de l’erreur de prédiction est ensuite démontrée comme étant moins de 4 FPS.----------ABSTRACT
Within the strongly regulated avionics engineering field, conventional graphical desktop hardware and software API cannot be used because they do not conform to the DO-254 and DO-178B certifications. We observe the need for better avionic graphical hardware, but system engineers lack system design tools related to graphical hardware. The endorsement of an optimal hardware architecture by estimating the performance of a graphical software, when a stable rendering engine does not yet exist, represents a major challenge.  There is also a high potential for development cost reduction, by enabling developers to have a first estimation of the performance of its graphical engine at a low cost. In this paper, we propose to replace expensive development platforms by a predictive software running on desktop. More precisely, we present a system design tool that helps predict the rendering performance of graphical hardware based on the OpenGL SC API. First, we create non-parametric models of the underlying hardware, with machine learning, by analyzing the instantaneous frames-per-second (FPS) of the rendering of a synthetic 3D scene and by drawing multiple times with various characteristics that are typically found in synthetic vision applications. The number of characteristic combinations used during this supervised training phase is a subset of all possible combinations, but performance predictions can be arbitrarily extrapolated. To validate our models, we render an industrial scene with characteristics combinations not used during the training phase and we compare the predictions to real values. We find a median prediction error of less than 4 FPS",,Prédiction de performance de matériel graphique dans un contexte avionique par apprentissage automatique,https://core.ac.uk/download/213619757.pdf,,,core
61817978,2016,"Maritime pine sawdust, a by-product from industry of wood transformation, has been investigated as a potential source of polyphenols which were extracted by ultrasound-assisted maceration (UAM). UAM was optimized for enhancing extraction efficiency of polyphenols and reducing time-consuming. In a first time, a preliminary study was carried out to optimize the solid/liquid ratio (6 g of dry material per mL) and the particle size (0.26 cm2) by conventional maceration (CVM). Under these conditions, the optimum conditions for polyphenols extraction by UAM, obtained by response surface methodology, were 0.67 W/cm2 for the ultrasonic intensity (UI), 40 °C for the processing temperature (T) and 43 min for the sonication time (t). UAM was compared with CVM, the results showed that the quantity of polyphenols was improved by 40% (342.4 and 233.5 mg of catechin equivalent per [br/]
100 g of dry basis, respectively for UAM and CVM). A multistage cross-current extraction procedure allowed evaluating the real impact of UAM on the solid–liquid extraction enhancement. The potential industrialization of this procedure was implemented through a transition from a lab sonicated reactor (3 L) to a large scale one with 30 L volume",,"Impact of ultrasound on solid–liquid extraction of phenolic compounds from maritime pine sawdust waste. Kinetics, optimization and large scale experiments",,10.1016/j.ultsonch.2015.07.022,,core
30733153,2015-07-01T00:00:00,"Ontology organises the things that was used to

consist of corpus for the real world. Ontology

constructs the model of information systems in

term of taxonomy in a wide range of subject

areas from social science and natural science.

Ontology defines a large number of objects for

a wide range of applications, such as education,

healthcare, medicine, engineering and

manufacturing. Ontology is underpinned by the

theories of formal language, classification and

automata languages, and can be implemented

by the natural language process, particularly

involving the tools and technologies in artificial

intelligence. Ontology made significant

contribution to the computational science, especially

in information retrieval/extraction and

visualisation from the theory to practice. The

challenge ahead for ontology is to prove even

more useful and effective in an even broader

range of application domains",'IGI Global',Editorial : Special Issue on Ontology and Innovation: Part 2,,,,core
289954980,2016-01-01T00:00:00,"Tetra Pak is a world leader in the food packaging industry and has been so for a very long time. In recent years however, they are experiencing increased competition from low-cost suppliers selling their previously patented paper as a commodity. This has forced Tetra Pak to focus more on selling complete systems and services. One such potential service is condition monitoring coupled with predictive maintenance of their packaging machines. In a packaging machine, there are electrical components called inductors that are used for sealing packages. In this thesis, a model for predicting the remaining useful life of an inductor is built. Around 8 months of high resolution data is analysed and processed. The primary tool for data processing is Matlab, and the predictive model is built using Machine Learning algorithms in Microsoft’s analytics software Azure. In the data there are clear and visible trends of the inductor degenerating, but the precision of the predictive model is far too low to be useful in any real world-world scenario - more data is probably needed",Lunds universitet/Produktionsekonomi,Lifetime prediction of sealing component using machine learning algorithms,https://core.ac.uk/download/pdf/289954980.pdf,,,core
296628806,2015-11-26T14:38:13Z,"In this paper, we discuss the compression of waveforms obtained from measurements of power system quantities and analyze the reasons why its importance is growing with the advent of smart grid systems. While generation and transmission networks already use a considerable number of automation and measurement devices, a large number of smart monitors and meters are to be deployed in the distribution network to allow broad observability and real-time monitoring. This situation creates new requirements concerning the communication interface, computational intelligence and the ability to process data or signals and also to share information. Therefore, a considerable increase in data exchange and in storage is likely to occur. In this context, one must achieve an efficient use of channel communication bandwidth and a reduced need of storage space for power system data. Here, we review the main compression techniques devised for electric signal waveforms providing an overview of the achievements obtained in the past decades. Additionally, we envision some smart grid scenarios emphasizing open research issues regarding compression of electric signal waveforms. We expect that this paper will contribute to motivate joint research efforts between electrical power system and signal processing communities in the area of signal waveform compression. © 2010-2012 IEEE.51291302Gu, I.Y.-H., Styvaktakis, E., Bridge the gap: Signal processing for power quality applications (2003) Signal processing, 66 (1), pp. 83-96. , JulRibeiro, M.V., Szczupak, J., Iravani, M.R., Gu, I.Y.-H., Dash, P.K., Mamishev, A.V., Emerging signal processing techniques for power quality applications (2007) EURASIP J. Adv. Signal Process, 2007 (2), pp. 16-16. , http://dx.doi.org/10.1155/2007/87425, Jun. [Online]. AvailableBollen, M.H.J., Gu, I.Y.-H., Santoso, S., McGranaghan, M.F., Crossley, P.A., Ribeiro, M.V., Ribeiro, P.F., Bridging the gap between signal and power (2009) IEEE Signal Process. Mag., 26 (4), pp. 12-31. , JulBollen, M.H.J., Ribeiro, P.F., Gu, I.Y.-H., Duque, C.A., Trends, challenges and opportunities in power quality research (2009) Eur. Trans. Electr. Power, 4 (1), pp. 2-18Bollen, M.H.J., (2000) Understanding Power Quality Problems-Voltage Sags and Interruptions, , Piscataway, NJ USA: IEEE PressBollen, M.H.J., Gu, I.Y.-H., (2006) Signal Processing of Power Quality Disturbances, , New York: Wiley-IEEE PressMehta Ketan, Data compression for digital data from power systems disturbances: Requirements and technique evaluation (1989) IEEE Transactions on Power Delivery, 4 (3), pp. 1683-1688. , DOI 10.1109/61.32659Toivonen, L., Morsky, J., Measurement and processing of distortion quantities in a portable, multi-purpose analyzer (1993) IEEE Trans. Power Del., 8 (4), pp. 1736-1746. , Oct(2011) Smart Grid, , http://www.oe.energy.gov/smartgrid.htm, U.S. Department of Energy, Jun. [Online]. AvailableAmin, S.M., Wollenberg, B., Toward a smart grid: Power delivery for the 21st century (2005) IEEE Power Energy Mag., 3 (5), pp. 34-41. , Sep.-OctVu, K., Begouic, M.M., Novosel, D., Grids get smart protection and control (1997) IEEE Comput. Apps. Power, 10 (4), pp. 40-44. , OctVojdani, A., Smart integration (2008) IEEE Power Energy Mag., 6 (6), pp. 71-79. , Nov.-DecIpakchi, A., Albuyeh, F., Grid of the future (2009) IEEE Power Energy Mag., 7 (2), pp. 52-62. , Mar.-Apr(2011) Smart Grid Demonstration-integration of Distributed Energy Resources, , http://www.smartgrid.epri.com/Demo.aspx, EPRI [Online]. Available JunAlbu, M.M., Neurohr, R., Apetrei, D., Silvas, I., Federenciuc, D., Monitoring voltage and frequency in smart distribution grids. A case study on data compression and accessibility (2010) Proc. IEEE PES Gen. Meet., pp. 1-6Abart, A., Lugmair, A., Schenk, A., Smart metering features for managing low voltage distribution grids (2009) Proc. 20th CIRED-Part, 2, p. 1Zhang, D., Bi, Y., Zhao, J., A new data compression algorithm for power quality online monitoring (2009) Proc. SUPERGEN'09, pp. 1-4Chicco, G., Challenges for smart distribution systems: Data representation and optimization objectives (2010) Proc. 12th OPTIM, pp. 1236-1244Albu, M.M., Neurohr, R., Apetrei, D., Silvas, I., Federenciuc, D., Monitoring voltage and frequency in smart distribution grids. A case study on data compression and accessibility (2010) Proc. PES Gen. Meet., pp. 1-6Ning, J., Wang, J., Gao, W., Liu, C., A wavelet-based data compression technique for smart grid (2011) IEEE Trans. Smart Grid, 2 (1), pp. 212-218. , MarDas, S., Rao, P., Principal component analysis based compression scheme for power system steady state operational data (2011) IEEE PES Innov. Smart Grid Technol.-India, pp. 95-100Parseh, R., Acevedo, S.S., Kansanen, K., Molinas, M., Ramstad, T.A., Real-time compression of measurements in distribution grids (2012) Proc. IEEE 3rd Int. Conf. Smart Grid Commun. (SmartGridComm.), pp. 223-228Recommended practice on monitoring electric power quality (2009) Ieee p1159/p6Bingham, R.P., Kreiss, D., Santoso, S., Advances in data reduction techniques for power quality instrumentation (1995) Proc. 3rd Eur. Power Quality Conf., , Bremen, GermanyWilkinson, W.A., Cox, M.D., Discrete wavelet analysis of power system transients (1996) IEEE Transactions on Power Systems, 11 (4), pp. 2038-2044Pillay, P., Bhattacharjee, A., Application of wavelets to model short-term power system disturbances (1996) IEEE Trans. Power Syst., 11 (4), pp. 2031-2037. , NovToivonen, L., Morsky, J., Measurement and processing of distortion quantities in a portable, multi-purpose analyzer (1993) IEEE Trans. Power Del., 8 (4), pp. 1736-1746. , OctKhan, A.K., Monitoring power for the future (2001) Power Engineering Journal, 15 (2), pp. 81-85Heydt, G.T., Gunther, E., Post-measurement processing of electric power quality data (1996) IEEE Trans. Power Del., 11 (4), pp. 1853-1859. , OctSantoso, S., Powers, E.J., Grady, W.M., Power quality disturbance data compression using wavelet transform methods (1997) IEEE Trans. Power Del., 12 (3), pp. 1250-1257. , JulHsieh, C.-T., Huang, S.-J., Huang, C.-L., Data reduction of power quality disturbances - A wavelet transform approach (1998) Electric Power Systems Research, 47 (2), pp. 79-86. , PII S0378779698000431Chung, J., Powers, E.J., Grady, W., Bhatt, S.C., Variable rate power disturbance signal compression using embedded zerotree wavelet transform coding (1999) Proc. 1999 IEEE Power Eng. Soc. Winter Meet., 2, pp. 1305-1309. , Jan.-4 FebLittler, T.B., Morrow, D.J., Wavelets for the analysis and compression of power system disturbances (1999) IEEE Trans. Power Del., 14 (2), pp. 358-364. , AprHeydt, G.T., Bhatt, S.C., Present and future trends and needs in electric power quality sensors and instrumentation (1999) Elect. Mach. Power Syst., 27 (7), pp. 691-700Hamid, E.Y., Kawasaki, Z.-I., Wavelet-based data compression of power system disturbances using the minimum description length criterion (2002) IEEE Transactions on Power Delivery, 17 (2), pp. 460-466. , DOI 10.1109/61.997918, PII S0885897702032776Panda, G., Dash, P.K., Pradhan, A.K., Meher, S.K., Data compression of power quality events using the slantlet transform (2002) IEEE Transactions on Power Delivery, 17 (2), pp. 662-667. , DOI 10.1109/61.997957, PII S0885897702011755Hsieh, C.-T., Huang, S.-J., Disturbance data compression of a power system using the huffman coding approach with wavelet transform enhancement (2003) IEE Proc. Gener., Transm., Distrib., 150 (1), pp. 7-14. , JanDash, P.K., Panigrahi, B.K., Sahoo, D.K., Panda, G., Power quality disturbance data compression, detection, and classification using integrated spline wavelet and s-transform (2003) IEEE Trans. Power Del., 18 (2), pp. 595-600. , AprWu, C.-J., Fu, T.-H., Huang, C.-P., Data compression technique in recording electric arc furnace voltage and current waveforms for tracking power quality (2003) Proc. IEEE PES Transm. Distrib. Conf. Expo., 1, pp. 383-388Shang, L., Krebs, J.J.R., Efficiency analysis of data compression of power system transients using wavelet transform (2003) Proc. IEEE Bologna Power Tech. Conf., 4, p. 6Meher, S.K., Pradhan, A.K., Panda, G., An integrated data compression scheme for power quality events using spline wavelet and neural network (2004) Elect. Power Syst. Res., 69 (2-3), pp. 213-220Gerek, O.N., Ece, D., 2-d analysis and compression of power-quality event data (2004) IEEE Trans. Power Del., 19 (2), pp. 791-798. , AprHuang, S.-J., Jou, M.-J., Application of arithmetic coding for electric power disturbance data compression with wavelet packet enhancement (2004) IEEE Trans. Power Syst., 19 (3), pp. 1334-1341. , AugLorio, F., Magnago, F., Analysis of data compression methods for power quality events (2004) 2004 IEEE Power Engineering Society General Meeting, 1, pp. 504-509. , 2004 IEEE Power Engineering Society General MeetingYuan, Y., Yu, X., Du, H., Power system fault data compression using the wavelet transform and vector quantification (2006) Proc. IEEE POWERCOM, Oct., pp. 1-6Gerek, O.N., Ece, D.G., Compression of power quality event data using 2D representation (2008) Electric Power Systems Research, 78 (6), pp. 1047-1052. , DOI 10.1016/j.epsr.2007.08.006, PII S0378779607001769Qing, A., Hongtao, Z., Zhikun, H., Zhiwen, C., A compression approach of power quality monitoring data based on two-dimension dct (2011) Proc. ICMTMA'11, 1, pp. 20-24Lovisolo, L., Da Silva, E.A.B., Rodrigues, M.A.M., Diniz, P.S.R., Efficient coherent adaptive representations of monitored electric signals in power systems using damped sinusoids (2005) IEEE Transactions on Signal Processing, 53 (10), pp. 3831-3846. , DOI 10.1109/TSP.2005.855400Tcheou, M.P., Lovisolo, L., Da Silva, E.A.B., Rodrigues, M.A.M., Diniz, P.S.R., Optimum rate-distortion dictionary selection for compression of atomic decompositions of electric disturbance signals (2007) IEEE Signal Processing Letters, 14 (2), pp. 81-84. , DOI 10.1109/LSP.2006.882117Lovisolo, L., Tcheou, M.P., Da Silva, E.A.B., Rodrigues, M.A.M., Diniz, P.S.R., Modeling of electric disturbance signals using damped sinusoids via atomic decompositions and its applications (2007) EURASIP J. Adv. Signal Process, 2007, p. 15. , Article ID 29 507Ribeiro, M.V., Romano, J.M.T., Duque, C.A., An enhanced data compression method for applications in power quality analysis (2001) IECON Proceedings (Industrial Electronics Conference), 3, pp. 676-681Ribeiro, M.V., Duque, C.A., The word length influence on waveform coding techniques based on wavelet transform applied to disturbance compression (2002) Proc. 10th IEEE ICHQP, 1, pp. 139-143Ramos, F.R., Riberto, M.V., Romano, J.M.T., Duque, C.A., On signal processing approach for event detection and compression applied to power quality evaluation (2002) Proc. 10th EEE ICHQP, 1, pp. 133-138Ribeiro, M.V., Romano, J.M.T., Duque, C.A., An improved method for signal processing and compression in power quality evaluation (2004) IEEE Trans. Power Del., 19 (2), pp. 464-471. , AprRibeiro, M.V., Park, S.H., Romano, J.M.T., Mitra, S.K., A novel MDL-based compression method for power quality applications (2007) IEEE Transactions on Power Delivery, 22 (1), pp. 27-36. , DOI 10.1109/TPWRD.2006.887091Yun, Z., Xiaoming, L., Lingxu, A., Jian, S., Lihui, W., Research on encoding/decoding method of electric physical information based on lms-adpcm algorithm (2011) Proc. Int. Conf. Adv. Power Syst. Autom. Protection, 1, pp. 795-800Kraus, J., Tobiska, T., Bubla, V., Looseless encodings and compression algorithms applied on power quality datasets (2009) Proc. 2nd IEEE CIRED-Part 1, pp. 1-4Kraus, J., Stepan, P., Kukacka, L., Optimal data compression techniques for smart grid and power quality trend data (2012) Proc. IEEE ICHQP, pp. 707-712Recommended Practice for the Transfer of Power Quality Data, , IEEEP1159.3/D9Tse, N.C.F., Chan, J.Y.C., Lai, L.L., Development of a smart metering scheme for building smart grid system (2009) Proc. 8th APSCOM, pp. 1-5Zhang, M., Li, K., Hu, Y., A high efficient compression method for power quality applications (2011) IEEE Trans. Instrum. Meas., 60 (6), pp. 1976-1985. , JunXu, W., Component modeling issues for power quality assessment (2001) IEEE Power Eng. Rev., 21 (11), pp. 12-15. , 17 NovRibeiro, M.V., Pereira, J.L.R., Classification of single and multiple disturbances in electric signals (2007) EURASIP J. Adv. Signal Process., 2007 (2), p. 18. , JunInterharmonics: Theory and modeling (2007) IEEE Trans. Power Del., 22 (4), pp. 2335-2348. , IEEE Task Force on Harmonics Modeling and Simulation OctIeee, , http://grouper.ieee.org/1433, IEEE PES Working Group 1433 Power Quality [Online]. AvailableSchweitzer Edmund, O., Hou Daqing, Filtering for protective relays (1993) Communications, Computers and Power in the Modern Environment, pp. 15-23Wiot, D., A new adaptive transient monitoring scheme for detection of power system events (2004) IEEE Trans. Power Del., 19 (1), pp. 42-48Lobos, T., Rezmer, J., Koglin, H.-J., Analysis of power systems transients using wavelets and prony method (2001) Proc. IEEE Porto Power Tech. Conf., 4, pp. 1-4Tawfik, M.M., Morcos, M.M., ANN-based techniques for estimating fault location on transmission lines using prony method (2001) IEEE Transactions on Power Delivery, 16 (2), pp. 219-224. , DOI 10.1109/61.915486, PII S0885897701015394Bujanowski, B.J., Pierre, J.W., Hietpas, S.M., Sharpe, T.L., Pierre, D.A., A comparison of several system identification methods with application to power systems (1993) Proc. 36th MWSCAS, 1, pp. 64-67Galli, A.W., Heydt, G.T., Ribeiro, P.F., Exploring the power of wavelet analysis (1996) IEEE Comput. Appl. Power, 9 (4), pp. 37-41. , OctChung, J., Powers, E.J., Grady, W.M., Bhatt, S.C., Electric power transient disturbance classification using wavelet-based hidden Markov models (2000) Proc. IEEE ICASSP, 6, pp. 3662-3665Pillay, P., Bhattachrjee, A., Application of wavelets to model shortterm power systemdisturbances (1996) IEEE Trans. Power Syst., 11 (4), pp. 2031-2037. , NovPoisson, O., Rioual, P., Meunier, M., Detection and measurement of power quality disturbances using wavelet transforms (2000) IEEE Trans. Power Del., 15 (3), pp. 1039-1044. , JulLiao, H.Y.C., A de-noising scheme for enhancing wavelet-based power quality monitoring systems (2001) IEEE Trans. Power Del., 16 (3), pp. 353-360. , JulSantoso, S., Grady, W.M., Powers, E.J., Lamoore, J., Bhatt, S.C., Characterization of distribution power quality events with fourier and wavelets transforms (2000) IEEE Trans. Power Del., 15 (1), pp. 247-254. , JanKarimi, M., Mokhtari, H., Iravani, M.R., Wavelet based on-line disturbance detection for power quality applications (2000) IEEE Transactions on Power Delivery, 15 (4), pp. 1212-1220. , DOI 10.1109/61.891505Anis Ibrahim, W.R., Morcos, M.M., Artificial intelligence and advanced mathematical tools for power quality applications: A survey (2002) IEEE Transactions on Power Delivery, 17 (2), pp. 668-673. , DOI 10.1109/61.997958, PII S0885897702027474Gosh, A.K., Lubkeman, D.L., The classification of power system disturbance waveforms using a neural network approach (1995) IEEE Trans. Power Del., 10 (1), pp. 109-115. , JanLovisolo, L., Figueiredo, K.T., Laporte Menezes, L.De., Neto, J.A.M., Dos Santos Rocha, J.C., Location of faults generating short duration voltage variations in distribution systems regions from records captured at one point and decomposed into damped sinusoids IET Gener., Transm., Distrib., , Accepted for publicationYang, Q., Wang, J., Sima, W., Chen, L., Yuan, T., Mixed over-voltage decomposition using atomic decompositions based on a damped sinusoids atom dictionary (2011) Energies, 4 (9), pp. 1410-1427Ribeiro, M.V., Marques, C.A.G., Duque, C.A., Cerqueira, A.S., Pereira, J.L.R., Detection of disturbances in voltage signals for power quality analysis using HOS (2007) EURASIP J. Adv. Signal Process., 2007 (2), p. 13. , JunSayood, K., (2000) Introduction to Data Compression, , 2nd ed. San Francisco, CA, USA: Morgan KaufmanBell, T.C., Witten, I.H., Cleary, J.G., (1990) Prentice Hall, , Text Compression/Timothy C. Bell, John G. Cleary, Ian H. Witten. Englewood Cliffs, NJ, USA: Prentice-HallQing, A., Hongtao, Z., Zhikun, H., Zhiwen, C., A compression approach of power quality monitoring data based on two-dimension dct (2011) Proc. 3rd ICMTMA, 1, pp. 20-24Tcheou, M.P., Miranda, A.L., Lovisolo, L., Da Silva, E.A., Rodrigues, M.A., Diniz, P.S., How far can one compress digital fault records analysis of a matching pursuit based algorithm (2012) Digit. Signal Process., 22 (2), pp. 288-297Nascimento, F.A.O., Data compression algorithm for transient recording system (1997) Proc. IEEE ISIE, 3, pp. 1126-1130Mallat, S., (1998) A Wavelet Tour of Signal Processing, , 2nd ed. San Diego, CA, USA: AcademicDaubechies, I., (1991) Ten Lectures on Wavelets, , Philadelphia PA USA: SIAMShapiro, J.M., Embedded image coding using zerotrees of wavelet coefficients (1993) IEEE Trans. Signal Process, 41 (12), pp. 3445-3462. , DecLiu, S., An adaptive Kalman filter for dynamic estimation of harmonic signals (1998) Proc. 8th IEEE ICHQP, 2, pp. 636-640Romano, J.M.T., Bellanger, M., Fast least squares adaptive notch filtering (1988) IEEE Trans. Acoust., Speech, Signal Process, 36 (9), pp. 1536-1540. , SepDiniz, P.S.R., (2008) Adaptive Filtering: Algorithms and Practical Implementations, , 3rd ed. Boston, MA, USA: SpringerCheng, Y.T., TMS320C62x Algorithm: Sine wave generation (2000) Texas Instruments, Dallas, TX, USA, Tech. Rep., , NovSaito, N., Simultaneous noise suppression and signal compression using a library of orthonormal bases and the minimum description length criterion (1994) Wavelets in Geophysics, pp. 299-324. , San Diego, CA, USA: AcademicKrim, H., Tucker, D., Mallat, S., Donoho, D., On denoising and best signal representation (1999) IEEE Transactions on Information Theory, 45 (7), pp. 2225-2238. , DOI 10.1109/18.796365Krim, H., Schick, I.C., Minimax description length for signal denoising and optimized representation (1999) IEEE Trans. Inf. Theory, 45 (3), pp. 898-908. , AprHansen, M., Yu, B., Wavelet thresholding via MDL for natural images (2000) IEEE Trans. Inf. Theory, 46 (5), pp. 1778-1788. , AugChang, S.G., Yu, B., Vitterli, M., Adaptive wavelet thresholding for image denoising and compression (2000) IEEE Trans. Image Process, 9 (9), pp. 1532-1546. , SepRissanen, J., Modeling by shortest data description (1978) Automatica, 14, pp. 465-471Barron, A., Rissanen, J., Yu, B., The minimum description length principle in coding and modeling (1998) IEEE Transactions on Information Theory, 44 (6), pp. 2743-2760. , PII S0018944898052845Mallat, S., Zhang, Z., Matching pursuitswith time-frequency dictionaries (1993) IEEE Trans. Signal Process, 41 (12), pp. 3397-3415. , DecGalli, S., Scaglione, A., Wang, Z., For the grid and through the grid: The role of power line communications in the smart grid (2011) Proc. IEEE, 99 (6), pp. 998-1027. , JunGharavi, H., Hu, B., Multigate communication network for smart grid (2011) Proc. IEEE, 99 (6), pp. 1028-1045. , JunSauter, T., Lobashov, M., End-to-end communication architecture for smart grids (2011) IEEE Trans. Ind. Electron., 58 (4), pp. 1218-1228. , AprGomez-Exposito, A., Abur, A., Jaen Villa A.De, La., Gomez Quiles, C., A multilevel state estimation paradigm for smart grids (2011) Proc. IEEE, 99 (6), pp. 952-976. , JunArnold, G.W., Challenges and opportunities in smart grid: A position article (2011) Proc. IEEE, 99 (6), pp. 922-927. , Junhttp://www.powermonitors.com, Power Monitors Inc. [Online]. Available(1998) Method for Objective Measurements of Perceived Audio Quality, , ITU-R Rec. BS.1387 Geneva, Switzerland ITUWang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: From error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612. , AprKezunovic, M., Rikalo, M., Automating the analysis of faults and power quality (1999) IEEE Comput. Appl. Power, 12 (1), pp. 46-50. , JanStankovic, V., Stankovic, L., Shuang, W., Cheng, S., Distributed compression for condition monitoring of wind farms (2013) IEEE Trans. Sustain. Energy, 4 (1), pp. 174-181. , Ja",,The Compression Of Electric Signal Waveforms For Smart Grids: State Of The Art And Future Trends,,10.1155/2007/87425,,core
296620820,2015-11-26T14:10:21Z,"The use of digital technology in the form of health care apps has been on the increase. In the nutrition area, apps are now available with a view to lead to behavior change, helping individuals to reflect on their food choices and identify weak points in their dietary routine. The article seeks to evaluate user perception regarding the usability of the Digital Food Guide (DFG), which is a mobile smartphone app with guidelines on healthy eating. A cross-sectional study evaluated the user perception of the app using the Likert scale, built with 24 assertions organized in three dimensions of analysis: the DFG as an intuitive and self-explanatory tool; the DFG as a promoter of healthy food choices; and the DFG as a promoter of the transition to the appropriate weight. The instrument was assessed regarding its reliability through the split-half and validity method in two stages. The 22 assertions were validated; the reliability was 0.93; the average of the assertions in each dimension was 3.10; of the 80 respondents, 58.75% considered the implementation of the DFG to be positive. The application has good usability as perceived by users, considering analysis of the dimensions relating to its performance.19514371446(2013) Obesity and overweight., , http://www.who.int/mediacentre/factsheets/fs311/en/index.html#, Word Health Organization (WHO). [Internet]. [acessado 2014 abr 2]. Disponível em: Disponível emNoronha, J., Hysen, E., Zhang, H., PlateMate: Crowdsourcing nutrition analysis from food photographs (2011) UIST'11 Proceedings og the 24th annual ACM symposium on user interface software and technology., pp. 1-12. , In: Washington: Harvard School of Engineering and Applied SciencesTeixeira, P.D.S., Reis, B.Z., Vieira, D.A.S., Costa, D., Costa, J.O., Raposo, O.F.F., Wartha, E.R.S.A., Netto, R.S.M., Intervenção nutricional educative como ferramenta eficaz para mudanças de hábitos alimentares e peso corporal entre praticantes de atividade física (2013) Cien Saude Colet, 18 (2), pp. 347-356Liu, C., Zhu, Q., Holroyd, K.A., Seng, E.K., Status and trends of mobile-health applications for iOS devices: A developer's perspective (2011) J Syst Software, 84 (11), pp. 2022-2033Parker, A.G., Harper, R., Grinter, R.E., Celebratory health technology (2011) J Diab Science Techny, 5 (2), pp. 319-324Penn, L., Boeing, H., Boushey, C.J., Dragsted, L.O., Kaput, J., Scalbert, A., Welch, A.A., Mathers, J.C., Assessment of dietary intake: NuGO symposium report (2010) Genes Nutr, 5 (3), pp. 205-213Thompson, F.E., Subar, A.F., Loria, C.M., Reedy, J.L., Baranowshi, T., Need for technological innovation in dietary assessment (2011) J Am Diet Assoc, 110 (1), pp. 48-51(2010) Brasil, redes: Linhas telefônicas e assinantes de telefonia celular., , http://www.ibge.gov.br/oaisesat/main.php, Instituto Brasileiro de Geografia e Estatística (IBGE). Brasil. [Internet]. [acessado 2013 jul 15]. Disponível em: Disponível em(2009) Sala de imprensa: Acesso à internet e posse de telefone móvel celular para uso pessoal., , http://www.ibge.gov.br/home/noticias/notica_visualiza.php?id_noticia=1517, Instituto Brasileiro de Geografia e Estatística (IBGE). Brasil. [acessado 2013 jul 13]. Disponível em: Disponível emKenney, M., Pon, B., Structuring the smartphone industry: Is the mobile internet OS platform the key? (2011) J Ind Compet Trade, 11 (3), pp. 239-261Moura, A.M., (2010) Apropriação do telemóvel como ferramenta de mediação em mobile learning-estudos de caso em contexto educativo [dissertação]., , Braga: Universidade do MinhoGrohmann, Z.M., Battistella, L.F., Homens e mulheres ""aceitam"" de maneira diferente? impacto do gênero no modelo (expandido) de aceitação da tecnologia-TAM (2011) Inf & Soc, 21 (1), pp. 175-189Likert, R., Roslow, S., Murphy, G., A simple and reliable method of scoring the thurstone attitude scales (1993) Pers Psychol, 46 (3), pp. 689-690Wakita, T., Ueshima, N., Noguchi, H., Psychological distance between categories in the Likert Scale: Comparing different numbers of options (2012) EPM, 72 (4), pp. 533-546Willett, W.C., Skerrett, P.J., (2002) Eat, drink, and be healthy: The Harvard Medical School Guide to Healthy Eating., , Washington: Harvard School of Medicine Free Press(2011) Guide to eating a healthy meal based on latest science., , http://www.hsph.harvard.edu/news/press-releases/healthy-eating-plate/, Harvard School of Public Health. HSPH News. [Internet]. [acessado 2014 abr 2]. Disponível em: Disponível emBrandalise, L.T., (2006) Modelos de medição de percepção e comportamento [dissertação]., , Cascavel: Universidade Estadual do Oeste do ParanáNata, R.N., Progress in Education (2012) Development and validation of a strategy to assess teaching methods in undergraduate disciplines., pp. 81-107. , In: Moraes SG, Justino ML, Jansen BF, Barbosa EP, Bruno, LFC, Pereira LAV. Nova York: Nova Science Publishers IncSchimidt, M.J., (1975) Understanding and using statistics basic concepts., , Massachusetts: Health and Company(2010) Censo demográfico 2010., , http://www.censo2010.ibge.gov.br/apps/mapa/, Instituto Brasileiro de Geografia e Estatística (IBGE). Brasil. [Internet]. [acessado 2014 abr 2]. Disponível em: Disponível emBoushey, C., Wright, J.K.D.L., Ebert, D.E.J.D., Use of technology in children's dietary assessment (2009) Eur J Clin Nutr, 63 (1), pp. S50-S57Sevick, M.A., Zickmund, S., Korytkowski, M., Piraino, B., Sereika, S., Mihalko, S., Snetselaar, L., Burke, L.E., Design, feasibility, and acceptability of an intervention using personal digital assistant-based self-monitoring in managing type 2 diabetes (2008) Contemp Clin Trials, 29 (3), pp. 396-409Toral, N., Slater, B., Abordagem do modelo transteórico no comportamento alimentar (2007) Cien Saude Colet, 12 (6), pp. 1641-1650Fonseca, A.B., Souza, T.S.N., Frozi, D.S., Pereira, R.A., Modernidade alimentar e consumo de alimentos: Contribuições sócio-antropológicas para a pesquisa e nutrição (2011) Cien Saude Colet, 16 (9), pp. 3853-3862Fabry, P., Hejl, Z., Fodor, J., Braun, T.K.Z., The frequency of meals: Its relation to overweight, hypercholesterolaemia, and decreased glucose tolerance (1964) Lancet, 284 (7360), pp. 614-615La Bounty, P.M., Campbell, B.I., Wilson, J., Galvan, E., Berardi, J., Kleiner, S.M., Kreider, R.B., Antonio, J., International society of sports nutrition position stand: Meal frequency (2011) J Int Soc Sports Nutri, 8 (4), pp. 1-12Neumark-sztainer, D., Wall, M., Fulkerson, J.A., Larson, N., Changes in the frequency of family meals from 1999 to 2010 in the homes of adolescents: Trends by sociodemographic characteristics (2013) J Adol Health, 52 (2), pp. 201-206Chisolm, D.J., Does online health information seeking act like a health behavior?: A test of the behavioral model (2010) Telemed E-Health, 16 (2), pp. 154-160Rowe, S., Alexander, N., Almeida, N., Black, R., Burns, R., Bush, L., Crawford, P., Horn, L.V., Food Science Challenge: Translating the dietary guidelines for americans to bring about real behavior change (2011) JFS, 76 (1), pp. 29-37Rusin, M., Arsand, E., Hartvigsen, G., Functionalities and input methods for recording food intake: A systematic review (2013) Int J Med Inform, 82 (8), pp. 653-66",Associacao Brasileira de Pos - Graduacao em Saude Coletiva,Evaluation Of The Usability Of A Mobile Digital Food Guide Based On User Perception [avaliação Da Usabilidade Do Guia Alimentar Digital Móvel Segundo A Percepção Dos Usuários],,10.1590/1413-81232014195.13932013,,core
144089377,2015-04-14T00:00:00,"O presente trabalho apresenta o desenvolvimento de um sistema de Visão Artificial inteligente visando uma maior velocidade de processamento, um menor custo e aumento na produtividade industrial. Para o desenvolvimento do sistema foi utilizado o computador paralelo SPP3 desenvolvido no LCAD (Laboratório de Computação de Alto Desempenho) do ICMSC (Instituto de Ciências e Matemática de São Carlos) que utiliza uma arquitetura paralela MIMD com memória distribuída e a uma rede de comunicação de alta velocidade do tipo Myrinet [TRINDADE, 1994].  Este trabalho de tese teve como proposta desenvolver um sistema de visão em tempo real. Para atender os objetivos propostos citados, realizou-se de forma inédita a utilização de métodos estatísticos na extração do mínimo de características naturais (textura) e artificiais (histograma), invariantes à transformações geométricas, que definam a cena (ou objeto), para formar os vetores de atributos destinados ao treinamento e aprendizagem de redes neurais, utilizando ainda a técnica de invariância pelo treinamento. E para melhorar ainda mais a eficiência recorreu-se a utilização do paralelismo de hardware e software, proporcionando uma aplicação para multicomputadoresThe present work presents the development of an intelligent Artificial Vision system seeking larger processing speed, smaller cost and increase in the industrial productivity. For the development of the system, a parallel computer was used, the SPP3 developed in LCAD (Laboratory of Computation of High Performance computing) of the ICMSC (Institute of Sciences and Mathematics of São Carlos). The SPP3 uses a parallel architecture MIMD with distributed memory and the a high-speed Myrinet communication network [TRINDADE, 1994]. This Thesis\'s work has with plan the development of a vision\'s sistem in real time.  To attend the objetives cited, it was realized of original form, the utilization of statistical methods in extraction of less natural feature (texture) and artificial (histogram) invariants for geometrics transformations, that define the scene (or objects), to form the attributes vectors destinated for training and learning of neural networks, still using the invariants technical for training. And to improve more the eficient have recourse to use parallelism of hardware and software, giving a application to multicomputer","'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",Not available,,10.11606/T.76.2002.tde-14042015-145042,,core
43980798,2016-06-01T00:00:00Z,"Energy integration plays a significant role in increasing energy efficiency and sustainability of production systems. In order to model real energy integrated systems, sometimes we don't need rigorous models for involved units, but easily implemented and fast ones instead. This study presents an approach based on Artificial Neural Networks (ANNs) for predicting the main parameters of industrial Autothermal Thermophilic Aerobic Digestion (ATAD) bioreactors that are crucial for their energy integration. To create such predictive ANN model, four architectures with different number of hidden layers and artificial neurons in each one of them have been investigated. The developed ANN architectures have been trained and validated with data samplings obtained through long-term measurements of the operational conditions of real ATAD bioreactors. To train the models, BASIC genetic algorithm has been implemented. Using three independent measures for validation of the models, the best ANN architectures were selected. It is shown that selected ANN models predict with sufficient accuracy these ATAD parameters and are suitable for the implementation in an energy integration framework",Bulgarian Academy of Sciences,Prediction of Temperature Conditions of Autothermal Thermophilic Aerobic Digestion Bioreactors at Wastewater Treatment Plants,,,"[{'title': None, 'identifiers': ['issn:1314-1902', '1314-1902', '1314-2321', 'issn:1314-2321']}]",core
229087136,2015-06-01T07:00:00,"Lead-Acid batteries continue to be the preferred choice for backup energy storage systems. However, the inherent variability in the manufacturing and component design processes affect the performance of the manufactured battery. Therefore, the developed Lead-Acid battery models are not very flexible to model this type of variability. In this paper, a new and flexible modeling of a Lead-Acid battery is presented. Using curve fitting techniques, the model parameters were derived as a function of the battery’s state of charge based on a modified Thevenin equivalent model. In addition, the charge and discharge characteristics of the derived model were investigated and validated using a real NP4-12 YUASA battery manufacturer\u27s data sheet to match performance at different capacity rates. Furthermore, an artificial neural network based learning system with back-propagation technique was used for estimating the model parameters using MATLAB software. The proposed neural model had the ability to predict values and interpolate between the learning curves data at various characteristics without the need of training. Finally, a closed-form analytical model that connects between inputs and outputs for neural networks was presented. It was validated by comparing the target and output and resulted in excellent regression factors",Digital Commons@Georgia Southern,Lead Acid Battery Modeling for PV Applications,https://core.ac.uk/download/229087136.pdf,,,core
78077059,2015-07-01T00:00:00,"Many digital humanities–taught programmes aim to engage undergraduate and postgraduate humanists with computational methods and practices (Hirsch, 2012; Cohen and Scheinfeldt, 2013). It is relatively rare, however, to routinely engage computer scientists with the needs, methods, and worldview of historians, literature scholars, librarians, and related researchers (Spiro, 2012). This poster describes an ongoing collaboration between British Library Digital Research and the UCL Department of Computer Science (UCLCS), facilitated by the UCL Centre for Digital Humanities (UCLDH), that enables and engages students in computer science with humanities research issues as part of their core assessed work. We demonstrate that CS students can provide an experimental test-bed for developing, exploring, and exploiting technical infrastructure and digital content in ways that may benefit humanities researchers within a library context. Encouraging students to develop skills in a new (and often foreign) domain encourages their critical thinking and provides real-world, complex issues that stretch and develop their technical abilities as well as their understanding of user requirements. Furthermore, from the problems, issues, and potentials such collaborative working raises, we learn more about the nature of computational infrastructure we rely on for research, and perceptions of the institutions’ core business in delivering digital content. As the British Library has a vision for transforming access to and research with its digital collections, the British Library Big Data Experiment forms an important complement to the British Library’s ongoing infrastructure activities through enabling the development of experimental services that offer unconventional engagement with its digital collections (Farquhar and Baker, 2014). All taught programmes in UCLCS require students to undertake an industry exchange 4 where they work in teams as clients to an industry partner. Though UCLCS has experience with developing student projects in partnership with digital humanists (Martin et al., 2012), industry partners have tended to come from the financial or manufacturing sectors. The British Library Big Data Experiment is an umbrella for a series of activities where the British Library is the client for assessed UCLCS project work, allowing for a rolling, responsive program of experimental design, development, and testing of infrastructure and systems. At agreed milestones during the project, the British Library provides access to required data, knowledge of data structures, and project requirements. UCLCS and UCLDH jointly provide technical and academic support to the student teams. In June 2014 the first British Library Big Data Experiment team was convened with a dissertation project, submitted in fulfilment of the MSc in Software Systems Engineering (Georgiou, Stavrou) and Computer Science (Alborzpour, Wong), using a collection of circa 68,000 17th- to 19th-century digitised volumes to underpin the design of a research-oriented web-based service. Microsoft Azure 5 APIs were implemented that functionally scale to the data, whilst the students worked in close consultation with humanities researchers who may wish to use the capabilities of such a system. The final public output (http://blpublicdomain.azurewebsites.net/) represents an attempt to capture the complex and multifaceted needs of humanities researchers whilst offering unconventional services such as bulk download of text based on metadata queries, word frequency lists, and OCR text previews. Following this successful pilot, the British Library Big Data Experiment is undertaking further collaborative work, including machine learning and mobile app development strands in autumn/winter 2014 and a second MSc dissertation project in summer 2015. Both UCLCS and its students have an appetite for embedding problems faced by memory institutions within CS learning outcomes. In partaking in such truly interdisciplinary project work, students develop new skill sets, question their assumptions about the role of library and humanities scholars, and provide useful experimental design within the institutional context. In addition, having CS students engage with humanities scholars as a routine part of their degree allows humanists to understand their research needs and institutional structures, from a different perspective. We present the British Library Big Data Experiment as a model ripe for reuse and we argue that the benefits of such collaborative programmes outweigh potential risks. The Big Data Experiment is, then, both an experiment in teaching and an experiment in involving and integrating those undertaking advanced study in computer science into memory institutions and humanities scholarship",,"The British Library Big Data Experiment: Experimental Interfaces, Experimental Teaching",https://core.ac.uk/download/78077059.pdf,,,core
162757668,2016,"The paper provides a state of the art review of guided wave based structural health monitoring (SHM). First, the fundamental concepts of guided wave propagation and its implementation for SHM is explained. Following sections present the different modeling schemes adopted, developments in the area of transducers for generation, and sensing of wave, signal processing and imaging technique, statistical and machine learning schemes for feature extraction. Next, a section is presented on the recent advancements in nonlinear guided wave for SHM. This is followed by section on Rayleigh and SH waves. Next is a section on real-life implementation of guided wave for industrial problems. The paper, though briefly talks about the early development for completeness,. is primarily focussed on the recent progress made in the last decade. The paper ends by discussing and highlighting the future directions and open areas of research in guided wave based SHM",IOP PUBLISHING LTD,Guided wave based structural health monitoring: A review,,10.1088/0964-1726/25/5/053001,,core
226144714,2015-01-01T00:00:00,"Introduction
The goal of this work is to advance the production and use of 52Mn (t½ = 5.6 d, β+: 242 keV, 29.6%) as a radioisotope for in vivo preclinical nuclear imaging. More specifically, the aims of this study were: (1) to measure the excitation function for the natCr(p,n)52Mn reaction at low energies to verify past results [1–4]; (2) to measure binding constants of Mn(II) to aid the design of a method for isolation of Mn from an irradiated Cr target via ion-exchange chromatography, building upon previously published methods [1,2,5–7]; and (3) to perform phantom imaging by positron emission tomography/magnetic resonance (PET/MR) imaging with 52Mn and non-radioactive Mn(II), since Mn has potential dual-modality benefits that are beginning to be investigated [8].

Material and Methods
Thin foils of Cr metal are not available commercially, so we fabricated these in a manner similar to that reported by Tanaka and Furukawa [9]. natCr was electroplated onto Cu discs in an industrial-scale electroplating bath, and then the Cu backing was digested by nitric acid (HNO3). The remaining thin Cr discs (~1 cm diameter) were weighed to determine their thickness (~ 75–85 μm) and arranged into stacked foil targets, along with ~25 μm thick Cu monitor foils. These targets were bombarded with ~15 MeV protons for 1–2 min at ~1–2 μA from a CS-15 cyclotron (The Cyclotron Corporation, Berkeley, CA, USA). The beamline was perpendicular to the foils, which were held in a machined 6061-T6 aluminum alloy target holder. The target holder was mounted in a solid target station with front cooling by a jet of He gas and rear cooling by circulating chilled water (T ≈ 2–5 °C). Following bombardment, these targets were disassembled and the radioisotope products in each foil were counted using a high-purity Ge (HPGe) detector. Cross-sections were calculated for the natCr(p,n)52Mn reaction.
Binding constants of Mn(II) were measured by incubating 54Mn(II) (t½ = 312 d) dichloride with anion- or cation-exchange resin (AG 1-X8 (Cl− form) or AG 50W-X8 (H+ form), respectively; both: 200–400 mesh; Bio-Rad, Hercules, CA) in hydrochloric acid (HCl) ranging from 10 mM-8 M (anion-exchange) and from 1 mM-1 M (cation-exchange) or in sulfuric acid (H2SO4) ranging from 10 mM-8 M on cation-exchange resin only. The amount of unbound 54Mn(II) was measured using a gamma counter, and binding constants (KD) were calculated for the various concentrations on both types of ion-exchange resin.
We have used the unseparated product for preliminary PET and PET/MR imaging. natCr metal was bombarded and then digested in HCl, resulting in a solution of Cr(III)Cl3 and 52Mn(II)Cl2. This solution was diluted and imaged in a glass scintillation vial using a microPET (Siemens, Munich, Germany) small animal PET scanner. The signal was corrected for abundant cascade gamma-radiation from 52Mn that could cause random false coincidence events to be detected, and then the image was reconstructed by filtered back-projection. Additionally, we have used the digested target to spike non-radioactive Mn(II)Cl2 solutions for simultaneous PET/MR phantom imaging using a Biograph mMR (Siemens) clinical scanner. The phantom consisted of a 4×4 matrix of 15 mL conical tubes containing 10 mL each of 0, 0.5, 1.0, and 2.0 mM concentrations of non-radioactive Mn(II)Cl2 with 0, 7, 14, and 27 μCi (at start of PET acquisition) of 52Mn(II)Cl2 from the digested target added. The concentrations were based on previous MR studies that measured spin-lattice relaxation time (T1) versus concentration of Mn(II), and the activities were based on calculations for predicted count rate in the scanner. The PET/MR imaging consisted of a series of two-dimensional inversion-recovery turbo spin echo (2D-IR-TSE) MR sequences (TE = 12 ms; TR = 3,000 ms) with a wide range of inversion times (TI) from 23–2,930 ms with real-component acquisition, as well as a 30 min. list-mode PET acquisition that was reconstructed as one static frame by 3-D ordered subset expectation maximization (3D-OSEM). Attenuation correction was performed based on a two-point Dixon (2PD) MR sequence. The DICOM image files were loaded, co-registered, and windowed using the Inveon Research Workplace software (Siemens)",Helmholtz-Zentrum Dresden - Rossendorf,Cyclotron Production and PET/MR Imaging of 52Mn,https://core.ac.uk/download/226144714.pdf,,,core
296621636,2015-11-26T14:13:33,"Planning of hydroelectric systems is a complex and difficult task once it involves non-linear production characteristics and depends on numerous variables. A key variable is the natural streamflow. Streamflow values covering the entire planning period must be accurately forecasted because they strongly influence energy production. Currently, streamflow prediction using Box & Jenkins methodology prevails in the electric power industry. This paper suggests a fuzzy prediction model based on fuzzy clustering as an alternative for streamflow forecast. The model uses fuzzy c-means clustering to explore past data structure, and a median and pattern recognition procedures to capture similarities between streamflow history and data used for prediction. Computational experiments with actual data suggest that the predictive clustering approach performs globally better than periodic autoregressive moving average models, the current streamflow forecasting methodology adopted by many hydroelectric systems worldwide, and a fuzzy neural network, a non-linear prediction model.313491354Maier, H.R., Dandy, G., Neural networks for prediction and forecasting of water resources variables: A review of modelling issues and applications (2000) Environmental Modelling & Software, 15, pp. 101-124Box, G., Jenkins, G., Reinsel, G.C., (1994) Time Series Analysis, Forecasting and Control, 3rd Ed., , Oakland, California: Holden DayWeigend, A.S., Gershenfeld, N.A., (1993) Time Series Prediction: Prediction De Future and Understanding the Past, , Perseus PublishingBallini, R., Figueiredo, M., Soares, S., Andrade, M., Gomide, F., A seasonal streamflow forecasting model using neurofuzzy network (2000) Information, Uncertainty and Fusion, pp. 257-276. , Kluwer Academic Publishers: B. Bouchon- Meunier and R. R. Yager and L. Zadeh, EdsSee, L., Openshaw, S., A hybrid multi-model approach to river level forecasting (2000) Hydrology Science Journal, 45, pp. 523-536Chang, F.J., Chen, Y.C., A counterpropagation fuzzy neural network modeling approach to real time streamflow prediction (2001) Journal of Hydrology, 245, pp. 153-164Oh, K.J., Han, I., An intelligent clustering forecasting system based on change-point detection and artificial neural networks: Application to financial economics (2001) Proceedings of the 34th Hawaii International Conference on System ScienceGeva, A.B., Non-stationary time series prediction using fuzzy clustering (1999) Proceedings of the 18th International Conference of the North American Fuzzy Information Processing Society, pp. 413-417Figueiredo, M., Gomide, F., Adaptive neuro fuzzy modelling (1997) Proceedings of FUZZ-IEEE'97, pp. 1567-1572Bezdek, J., (1981) Pattern Recognition with Fuzzy Objective Function Algorithms, , Plenum PressBallini, R., (2000) Streamflow Forecasting and Analysis Using Temporal Series, Neural Networks and Fuzzy Neural Networks (In Portuguese), , Ph.D. dissertation, State University of Campinas, Campinas, SP, Brazi",'Institute of Electrical and Electronics Engineers (IEEE)',Predictive Fuzzy Clustering Model For Natural Streamflow Forecasting,,10.1109/FUZZY.2004.1375365,,core
296767877,2016-07-01T12:52:01,"Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP)Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES)Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)Traditional methods for bacterial identification include Gram staining, culturing, and biochemical assays for phenotypic characterization of the causative organism. These methods can be time-consuming because they require in vitro cultivation of the microorganisms. Recently, however, it has become possible to obtain chemical profiles for lipids, peptides, and proteins that are present in an intact organism, particularly now that new developments have been made for the efficient ionization of biomolecules. MS has therefore become the state-of-the-art technology for microorganism identification in microbiological clinical diagnosis. Here, we introduce an innovative sample preparation method for nonculture-based identification of bacteria in milk. The technique detects characteristic profiles of intact proteins (mostly ribosomal) with the recently introduced MALDI SepsityperTM Kit followed by MALDI-MS. In combination with a dedicated bioinformatics software tool for databank matching, the method allows for almost real-time and reliable genus and species identification. We demonstrate the sensitivity of this protocol by experimentally contaminating pasteurized and homogenized whole milk samples with bacterial loads of 10(3)-10(8) colony-forming units (cfu) of laboratory strains of Escherichia coli, Enterococcus faecalis, and Staphylococcus aureus. For milk samples contaminated with a lower bacterial load (104 cfu mL-1), bacterial identification could be performed after initial incubation at 37 degrees C for 4 h. The sensitivity of the method may be influenced by the bacterial species and count, and therefore, it must be optimized for the specific application. The proposed use of protein markers for nonculture-based bacterial identification allows for high-throughput detection of pathogens present in milk samples. This method could therefore be useful in the veterinary practice and in the dairy industry, such as for the diagnosis of subclinical mastitis and for the sanitary monitoring of raw and processed milk products.121727392745Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP)FINEPFundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP)Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES)Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq",'Wiley',Nonculture-based identification of bacteria in milk by protein fingerprinting,,10.1002/pmic.201200053,"[{'title': 'PROTEOMICS', 'identifiers': ['issn:1615-9853', '1615-9853']}]",core
80527571,2015-06-01T00:00:00,"Solea senegalensis is a promising species for European Aquaculture, however individuals born in captivity display a reproductive failure that hinders the sustainable culture of this species. The reproductive problem has been focused on males reared in captivity. It has been demonstrated that wild males held with captivity breed females produced viable spawns, but non viable spawns were obtained in the opposite situation (Mañanós et al., 2007, Carazo 2013). The absence of courtship is not the only reproductive problem in this species. Sperm quality is in general low, and spermatozoa show low resistance to hyperosmotic shock, high levels of DNA fragmentation, high levels of apoptotic cells and also display different and heterogeneous sperm subpopulations within the same seminal sample (Beirao et al 2009; Beirao et al 2011). 
The aim of this study is to implement a sperm selection method for optimal sperm subpopulation recovery. In particular, a method to eliminate apoptotic spermatozoa has been used.  Annexin V binds to phosphatidylserine, which is externalized to the outer surface of the sperm membrane in early apoptosis. In this study Annexin V-MACS beads have been used for the separation of apoptotic spermatozoa. This technique has been used in sperm samples from wild and F1 individuals. In order to confirm the efficiency of the technique, apoptotic cell population was studied by flow cytometry using YOPRO-1 and a caspases detection kit.   
MATERIALS AND METHOS 
Sperm samples from adult Senegalese sole males (wild and F1 broodstocks from el Bocal, IEO, Santander) were obtained by gently pressing the testes on the fish pigmented side. Ejaculates were pooled (3-5 males/pool) attending to similar motility parameters to avoid the influence of the sperm quality, to get volume enough to perform the study and following the routine in Aquaculture companies prior to artificial fertilisation. 
Each pooled sample was split into two aliquots. One of them was directly cryopreserved following the only published protocol for this species so far (Rasines et al 2013). Sperm was diluted (1:2 ratio) in Mounib extender with cryoprotectants (10% BSA and 10% DMSO), loaded into 0.5 mL French straws, exposed to liquid nitrogen vapour during 7 min and rinsed into liquid nitrogen until used. This aliquot was considered as control in the experiment.
Magnetic activated cell sorting (MACS) was performed with the other aliquot using a MiniMACS separation unit (Miltenyi Biotec GmbH, Bergisch Gladbach, Germany) following manufacturer’s instructions. Magnetically labelled cells (apoptotic cells) flushed from the MS columns were discarded and the apopototic-reduced elution was cryopreserved following the same protocol as the control. 
Samples were thawed (37 ºC, 7 s) and washed prior to cytometer analysis. Two apoptosis detection protocols were carried out: 1) YOPRO-1 (150 nM) (Invitrogen, Leiden, The Netherlands) and 2) CaspGLOW Fluorescein Active Caspase Staining Kit (eBioscience, San Diego, USA) following manufacture’s instructions. Both of them were co-stained with PI (2 µg/mL) (Sigma, Madrid, Spain).
 After the incubation time samples were analyzed twice in a FACSort Plus Analyzer (Becton–Dickinson, USA) acquiring 10000 events per replicate. The red fluorescence emitted by PI was detected using a 610 nm filter and the green fluorescence emitted by the active caspase detection kit (FITC) and YOPRO-1 with a 516 nm filter.
For each staining, significant differences between the apoptotic cell percentage (green positive/red negative cells for each protocol) between control and MACS samples were evaluated by ANOVA with a SNK (Student- Newman-Kleus) multiple range test (P < 0.05). All statistical analyses were conducted with SPSS software (version 20.0). 

RESULTS 
Results demonstrated that Magnetic Activated Cell sorting eliminated apoptotic cells from Solea senegalensis seminal samples. In samples obtained from captured males, apoptotic cell population significantly decreased after the process. This decrease is observed independently of the assessment method used (caspases and YO-PRO-1). However, in F1 individuals, only caspase-positive-cells significantly decreased after the selection. Moreover, the comparative study between seminal samples from captured and F1 individuals showed significant differences only in caspase positive cells, being the percentage of YO-PRO-1 positive cells similar in both cases.

DISCUSSION AND CONCLUSIONS 
This study demonstrated that caspase determination is more specific than YO-PRO-1 in the calculation of apoptotic cells in S. senegalensis seminal samples. The percentage of apoptotic cells (caspase positive) is, as expected, significantly higher in F1 seminal samples. Magnetic activated cell sorting is applicable in this species for non-apoptotic spermatozoa recovery, but fertility trials must be done to confirm the real potential of the technique. 

ACKNOWLEDGEMENTS 
Authors would like to acknowledge AQUAGAMETE FA 1205 COST Action,Junta de Castilla y León (EDU1084/2012) and Fondo Social Europeo, Dr. I. Rasines, I. Martín, J.R. Gutierrez-Martín, J. Baines and M de la Hera.
REFERENCES 
Beirão, J.; Soares, F.; Herráez, M.P.; Dinis, M.T., Cabrita, E., 2009: Sperm quality evaluation in Solea senegalensis during the reproductive season at cellular level. Theriogenology 72 (9),1251-61.
Beirão, J.; Soares, F.; Herráez, M.P.; Dinis, M.T., Cabrita, E., 2011: Changes in Solea senegalensis sperm quality throughout the year. Animal Reproduction Science 126,122–129
Carazo, I., 2013. Reproductive behaviour and physiology of Senegalese sole, (Solea senegalensis) broodstock in captivity. PhD Thesis. University of Barcelona, Spain, 209 
Mañanós, E.; Ferreiro, I.; Bolón, D.; Guzmán, J.M.; Mylonas, C.C.; Riaza, A., 2007: Different responses of Senegalese sole (Solea senegalensis) broodstock to a hormonal spawning induction therapy, depending on their wild or captive-reared origin. Proceedings of Aquaculture Europe 07, Istanbul, Turkey, pp. 330-331.
Rasines, I.; Gómez, M.; Martín, I.; Rodríguez, C.; Mañanós, E.; Chereguini O., 2013: Artificial fertilisation of cultured Senegalese sole (Solea senegalensis): Effects of the time of day of hormonal treatment on inducing ovulation. Aquaculture, 392-395, 94-97",Centro Oceanográfico de Santander,OPTIMAL SPERM SUBPOPULATION SELECTION IN Solea senegalensis,,,,core
216509883,2012-01-01T00:00:00,"The robust popularization of 3D videos noticed along the last decade, allied to the omnipresence of smart mobile devices handling multimedia-capable features, has led to intense development and research focusing on efficient 3D-video encoding techniques, display technologies, and 3D-video capable mobile devices. In this scenario, the Multiview Video Coding (MVC) standard is key enabler of the current 3D-video systems by leading to meaningful data reduction through advanced encoding techniques. However, real-time MVC encoding for high definition videos demands high processing performance and, consequently, high energy consumption. These requirements are attended neither by the performance budget nor by the energy envelope available in the state-of-the-art mobile devices. As a result, the realization of MVC targeting mobile systems has been posing serious challenges to industry and academia. The main goal of this thesis is to propose and demonstrate energy-efficient MVC solutions to enable high-definition 3D-video encoding on mobile battery-powered embedded systems. To expedite high performance under severe energy constraints, this thesis proposes jointly considering energy-efficient optimizations at algorithmic and architectural levels. On the one hand, extensive application knowledge and data analysis was employed to reduce and control the MVC complexity and energy consumption at algorithmic level. On the other hand, hardware architectures specifically designed targeting the proposed algorithms were implemented applying low-power design techniques, dynamic voltage scaling, and application-aware dynamic power management. The algorithmic contribution lies in the MVC energy reduction by shorten the computational complexity of the energy-hungriest encoder blocks, the Mode Decision and the Motion and Disparity Estimation.  The proposed energy-efficient algorithms take advantage of the video properties along with the strong correlation available within the 3D-Neighborhood (spatial, temporal and disparity) space in order to efficiently reduce energy consumption. Our Multi-Level Fast Mode Decision defines two complexity reduction operation modes able to provide, on average, 63% and 71% of complexity reduction, respectively. Additionally, the proposed Fast ME/DE algorithm reduces the complexity in about 83%, for the average case. Considering the run-time variations posed by changing coding parameters and video content, an Energy-Aware Complexity Adaptation algorithm is proposed to handle the energy versus coding efficiency tradeoff while providing graceful quality degradation under severe battery draining scenarios by employing asymmetric video coding. Finally, to cope with eventual video quality losses posed by the energy-efficient algorithms, we define a video quality management technique based on our Hierarchical Rate Control. The Hierarchical Rate Control implements a frame-level rate control based on a Model Predictive Controller able to increase in 0.8dB (Bjøntegaard) the overall video quality. The video quality is increased in 1.9dB (Bjøntegaard) with the integration of the basic unit-level rate control designed using Markov Decision Process and Reinforcement Learning. Even though the energy-efficient algorithms drive to meaningful energy reduction, hardware acceleration is mandatory to reach the energy-efficiency demanded by the MVC. Aware of this requirement, this thesis brings architectural solutions for the Motion and Disparity Estimation unit focusing on energy reduction while attending real-time throughput requirements. To achieve the desired results, as shown along this volume, there is a need to reduce the energy related to the ME/DE computation and related to the intense memory communication.  Therefore, the ME/DE architectures incorporate the Fast ME/DE algorithm in order to reduce the computational complexity while the memory hierarchy was carefully designed to find the optimal energy tradeoff between external memory accesses and on-chip video memory size. Statistical analysis where used to define the size and organization of the on-chip cache memory while avoiding increased memory misses and the consequent data retransmission. A prefetching technique based on search window prediction also supports the reduction of external memory access. Moreover, a memory power gating technique based on dynamic search window formation and an application aware power management were proposed to reduce the static energy consumption related to on-chip video memory. To implement these techniques a SRAM memory featuring multiple power states was used. The architectural contribution contained in this thesis extends the state-of-the-art by achieving real-time ME/DE processing for 4-views HD1080p running at 300MHz and consuming 57mW",Energy-efficient algorithms and architectures for multiview video coding,,,,,core
54035319,2013-01-01T00:00:00,"International audienceOne remaining technological bottleneck to develop industrial Fuel Cell (FC) applications resides in the system limited useful lifetime. Consequently, it is important to develop failure diagnostic and prognostic tools enabling the optimization of the FC. Among all the existing prognostics approaches, datamining methods such as artificial neural networks aim at estimating the process' behavior without huge knowledge about the underlying physical phenomena. Nevertheless, this kind of approach needs huge learning dataset. Also, the deployment of such an approach can be long (trial and error method), which represents a real problem for industrial applications where realtime complying algorithms must be developed. According to this, the aim of this paper is to study the application of a reservoir computing tool (the Echo State Network) as a prognostics system enabling the estimation of the Remaining Useful Life of a Proton Exchange Membrane Fuel Cell. Developments emphasize on the prediction of the mean voltage cells of a degrading FC. Accuracy and time consumption of the approach are studied, as well as sensitivity of several parameters of the ESN. Results appear to be very promising",Fuel Cells prognostics using Echo State Network,,HAL CCSD,,,core
214601082,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake-holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished[1] Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons (Chichester, England) (2002). ISBN 0 47149691X

[2] Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

[3] Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 10–15. ACM Press (2006)

[4] Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-Architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

[5] Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

[6] Zhang, Z., Thangarajah, J., Padgham, L.: Automated unit testing for agent systems. In: Second International Working Conference on Evaluation of Novel Approaches to Software Engineering (ENASE), pp. 10–18 (2007)

[7] Ekinci, E.E., Tiryaki, A.M., Çetin, Ö.: Goal-oriented agent testing revisited. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 85–96 (2008)

[8] Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 133–144 (2008)

[9] Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 165–176 (2008)

[10] Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

[11] Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

[12] Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

[13] Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

[14] d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, pp. 155–176. Springer-Verlag, LNAI 1365 (1998)

[15] Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

[16] Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

[17] Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

[18] Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), pp. 42–55. Springer Verlag, LNAI 1038 (1996)

[19] Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

[20] Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

[21] Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemel ¨ a (eds.) Proceedings of the 23rd International Conference on Logic Programming (ICLP), pp. 302–316. Springer, LNCS 4670 (2007)

[22] Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http://www.math.upenn.edu/∼wilf/gfology2.pdf

[23] Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

[24] Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), industry track., pp. 37–44. IFAAMAS (2008)

[25] Parunak, H.V.D.: “go to the ant”: Engineering principles from natural multi-agent systems. Annals of Operations Research 75, 69–101 (1997). (Special Issue on Artificial Intelligence and Management Science)

[26] van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

[27] Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: The Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

[28] Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: International Conference on Software Engineering: Future of Software Engineering, pp. 120–136 (2007)

[29] Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

[30] Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

[31] Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007",On the testability of BDI agent systems,,'University of Otago Library',,,core
236403589,2011-11-01T00:00:00,"The computer gaming industry has begun to export powerful products and technologies from its initial entertainment roots to a number of ""serious"" industries. Game technologies are being adapted to defense, medicine, architecture, education, city planning, and government applications. Each of these industries is already served by an established group of companies that typically do not use computer games to serve their customers. The rapid growth in the power of game technologies and the growing social acceptance of these technologies has created an environment in which these are displacing other industry-specific computer hardware and software tools.

This dissertation proposes four hypotheses concerning the impact and acceptance of virtual reality and computer game technologies in education and training for laparoscopic surgery. It focuses on laparoscopic surgery because of the similarities between that form of surgery and virtual reality systems. The research indicates that the following four hypotheses are supported by the literature published in the field. Hypothesis 1: Training in laparoscopic surgery can be accomplished at a lower cost using virtual reality and game technology-based tools than through existing methods of training. Hypothesis 2: Virtual reality and game technology-based training environments provide better access to representative patient symptoms and allow more repetitive practice than existing forms of training. Hypothesis 3: Virtual reality and game technology-based training environments can reduce the training time required to achieve proficiency in laparoscopic procedures. Hypothesis 4: Virtual reality and game technology-based training can reduce the number of medical errors caused by residents and surgeons learning to perform laparoscopic procedures.

I also proposed a model of medical education in which virtual reality, including game technology, is the next major addition to or transformation of the medical education curriculum. The strong evidence collected in this study indicates that these systems are becoming much more accepted in medical education and that the technical limitations that existed when these devices were first introduced are already being overcome.ABSTRACT
Title of Dissertation: INVESTIGATING THE DISRUPTIVE
EFFECT OF COMPUTER GAME
TECHNOLOGIES ON MEDICAL
EDUCATION AND TRAINING
Roger D. Smith, Doctor of Management, 2008
Dissertation Directed By: Dr. Michael Evanchik, Graduate School of
Management and Technology,
University of Maryland University College
The computer gaming industry has begun to export powerful products and technologies
from its initial entertainment roots to a number of “serious” industries. Game technologies are
being adapted to defense, medicine, architecture, education, city planning, and government
applications. Each of these industries is already served by an established group of companies that
typically do not use computer games to serve their customers. The rapid growth in the power of
game technologies and the growing social acceptance of these technologies has created an
environment in which these are displacing other industry-specific computer hardware and
software tools.
This dissertation proposes four hypotheses concerning the impact and acceptance of
virtual reality and computer game technologies in education and training for laparoscopic
surgery. It focuses on laparoscopic surgery because of the similarities between that form of
surgery and virtual reality systems. The research indicates that the following four hypotheses are
supported by the literature published in the field.
• Hypothesis 1: Training in laparoscopic surgery can be accomplished at a lower cost
using virtual reality and game technology-based tools than through existing methods
of training.
• Hypothesis 2: Virtual reality and game technology-based training environments
provide better access to representative patient symptoms and allow more repetitive
practice than existing forms of training.
• Hypothesis 3: Virtual reality and game technology-based training environments can
reduce the training time required to achieve proficiency in laparoscopic procedures.
• Hypothesis 4: Virtual reality and game technology-based training can reduce the
number of medical errors caused by residents and surgeons learning to perform
laparoscopic procedures.
I also proposed a model of medical education in which virtual reality, including game
technology, is the next major addition to or transformation of the medical education curriculum.
The strong evidence collected in this study indicates that these systems are becoming much more
accepted in medical education and that the technical limitations that existed when these devices
were first introduced are already being overcome.
INVESTIGATING THE DISRUPTIVE EFFECT OF COMPUTER GAME TECHNOLOGIES
ON MEDICAL EDUCATION AND TRAINING
By
Roger D. Smith.
Dissertation submitted to the Faculty of the Graduate School of the
University of Maryland University College, in partial fulfillment
of the requirements for the degree of
Doctor of Management
2008
Advisory Committee:
Dr. Michael Evanchik, Chair
Dr. Monica Bolesta, Member
Dr. Joseph D’Mello, Member
© Copyright by
Roger D. Smith
2008
ii
Acknowledgements
I would like to thank the members of my dissertation committee: Dr. Michael Evanchik,
Dr. Monica Bolesta, and Dr. Joseph D’Mello for their guidance and help in structuring this
research so that it makes a valuable contribution toward understanding current and future
changes in the business of medical education. I also appreciate the opportunity that this degree
program and its faculty have given me to expand my understanding of business, management,
and innovation. This knowledge has been tremendously helpful to me in contributing to the
companies and government organizations with which I have been associated.
I am indebted to my mother and father for the foundations that they established in my
early years and for always believing that I could accomplish great things. I only wish that my
father had lived to see this newest accomplishment. Finally, my gratitude to my wife and
children for allowing me to spend years shut in my office working toward this goal. Your
understanding and support have been greatly appreciated.
iii
Table of Contents
Acknowledgements............................................................................................................. ii
Table of Contents............................................................................................................... iii
List of Tables ...................................................................................................................... v
List of Figures .................................................................................................................... vi
Chapter 1: Introduction and Research Problem.................................................................. 1
Virtual Reality................................................................................................................. 2
Computer Game Technologies ....................................................................................... 3
3D Engine ................................................................................................................... 5
Graphical User Interface ............................................................................................. 7
Physical Models.......................................................................................................... 8
Artificial Intelligence .................................................................................................. 8
Networking ................................................................................................................. 9
Persistence................................................................................................................. 10
Terminology Issues....................................................................................................... 11
Surgical Practice and Education ................................................................................... 12
Research Problem ......................................................................................................... 16
Chapter 2: Literature Review............................................................................................ 20
Medical Education with Virtual Reality ....................................................................... 20
Pioneers in Medical Simulation ................................................................................ 21
Simulation as a Tool for Education .......................................................................... 23
Cost Factors in Medical Education........................................................................... 26
Access to Patient Symptoms and Virtual Reality ..................................................... 28
Simulation and VR Impact on Training Time .......................................................... 31
Potential to Reduce Medical Errors .......................................................................... 33
Game Technology for Non-Entertainment Applications.............................................. 35
Historical Applications ............................................................................................. 35
Educational Applications .......................................................................................... 36
Business Aspects of Games ...................................................................................... 40
Games as Technology Products ................................................................................ 44
Social Acceptance of Games .................................................................................... 45
Dissertations on Computer Games................................................................................ 48
Social Impacts........................................................................................................... 48
Educational Applications .......................................................................................... 51
Business Aspects of Games ...................................................................................... 51
Technology in Games ............................................................................................... 52
Simulation ..................................................................................................................... 53
History of Technology .................................................................................................. 55
Disruptive Innovation and Creative Destruction .......................................................... 59
Chapter 3: Conceptual Framework and Research Method ............................................... 64
Conceptual Framework................................................................................................. 66
Rationale ....................................................................................................................... 72
The Hypotheses............................................................................................................. 74
iv
Research Method .......................................................................................................... 76
Reference Coding.......................................................................................................... 82
Chapter 4: Data Analysis, Results, and Conclusions........................................................ 86
Data Analysis ................................................................................................................ 86
Hypothesis 1: Lower Cost......................................................................................... 86
Hypothesis 2: Better Access ..................................................................................... 95
Hypothesis 3: Reduced Training Time ................................................................... 101
Hypothesis 4: Reduced Errors ................................................................................ 106
Results........................................................................................................................ 112
H1 Lower Cost: Supported ..................................................................................... 113
H2 Better Access: Supported.................................................................................. 113
H3 Reduced Training Time: Supported.................................................................. 114
H4 Reduced Errors: Supported ............................................................................... 115
Model of Medical Education: Supported................................................................ 116
Misleading Domain Assumptions............................................................................... 117
Assumption 1: Didactic Education is Effective ...................................................... 117
Assumption 2: Cost of Systems is Not an Issue ..................................................... 118
Assumption 3: Sufficient Access to Faculty and Patients is Possible .................... 119
Assumption 4: Practicing on Live Patients is Acceptable ...................................... 120
Discussion................................................................................................................... 121
Conclusion .................................................................................................................. 122
Chapter 5: Recommendations for Future Work............................................................. 125
Appendix 1. Medical VR Reference Coding Matrix ...................................................... 127
Appendix 2. Medical VR and Simulation Vendors in the Literature Reviewed ............ 154
Appendix 3. Personal Communication in Support of Dissertation Topic ...................... 158
References...................................................................................................................... 170
VR for Laparoscopic Surgical Education ................................................................... 170
General Medical Education and Virtual Reality......................................................... 176
Game Technology for Non-Entertainment Applications............................................ 185
Dissertations on Computer Games.............................................................................. 188
Simulation ................................................................................................................... 190
History of Technology ................................................................................................ 190
Disruptive Innovation and Creative Destruction ........................................................ 191
v
List of Tables
Table 1. MIST-VR implementations used in the literature............................................... 81
Table 2. Cost/benefit of an AccuTouch laparoscopic simulator....................................... 91
Table 3. Cost categories associated with each method of psychomotor training. ............ 92
Table 4. MIST-VR training program for laparoscopic instrument proficiency.............. 103
vi
List of Figures
Figure 1. Sim One computerized training mannequin in 1967........................................... 2
Figure 2. Six core game technologies that are disruptive to other industries. .................... 5
Figure 3. Visual comparison of 3D scenes from 1992 and 2005........................................ 6
Figure 4. Unique domains of simulations, virtual environments, and computer games..... 7
Figure 5. Denson (left) and Hoffman (right) demonstrate Sim One in 1967.................... 22
Figure 6. Medical education model .................................................................................. 66
Figure 7. Medical education model by example............................................................... 67
Figure 8. Minimally Invasive Surgical Trainer – Virtual Reality (MIST-VR) system .... 81
Figure 9. Medical VR coding matrix. ............................................................................... 83
Figure 10. Medical VR reference coding items. ............................................................... 83
Figure 11. Exercises in MIST-VR skills course ............................................................. 104
1
Chapter 1: Introduction and Research Problem
Medical education has traditionally been conducted on live patients, cadavers, live
animals, collections of tissue and organs, and inanimate mannequins. The “gold standard” for
perfecting operations has been the use of porcine subjects in place of humans. But for over 40
years researchers, surgeons, and scientists have been introducing computerized devices to
augment or replace many of the traditional tools for training. The “Sim One” computerized
mannequin is considered one of the first applications of computers to medical training. This
system was conceived at an aerospace company in 1964, funded with a $272,000 grant from the
Department of Education, and first demonstrated on March 17, 1967. Sim One delivered a
mechanically animated, computer controlled mannequin that could receive and respond to two
forms of gaseous anesthesia and four forms of injection. The “patient” breathed, had a heart beat,
presented temporal and carotid pulse, and maintained blood pressure. The mannequin opened
and closed its mouth, blinked its eyes, and changed these behaviors in response to anesthesia
administered through a mask or a tube (Abrahamson, 1997). The device was enhanced in 1971 to
deliver training in respirator application, endotrachael intubation, intramuscular injection,
recovery room care, and the measurement of pulse and respiration (Hoffman & Abrahamson,
1975). Figure 1 shows the system in a classroom as it would be used for education.
2
Figure 1. Sim One computerized training mannequin in 1967.
Source: Abrahamson, 1997
Hoffman and Abrahamson (1975) summarized the results of 15 different studies into the
effectiveness of the device in improving performance in medical practice. These studies
demonstrated improvements in “learning gain per unit of time, amount of student time required
to reach criterion levels of performance, and investment of faculty time necessary for student
learning.” The educational improvements that were achieved using what would today be
considered primitive computers and animatronics were very impressive and suggest that further
development of these devices could grow these advantages and add others that were not
achievable forty years ago.
Virtual Reality
With Sim One and many later computerized mannequins as a foundation, new computer
technologies have been introduced into medical training with the hope of carrying improvements
3
deeper into the educational curriculum. One group of these technologies includes virtual reality
and the software being created for modern computer games. The first medical virtual reality
system based on a head mounted display and a data glove was introduced by Richard Satava and
Jaron Lanier in 1991 (Satava, 1993). Lanier had coined the term “virtual reality” around 1984 to
refer to the use of electronic devices for immersing humans into a computer generated world (the
head mounted display) and to provide a tool with which to interact with that world (the data
glove). Satava applied these to medical training and demonstrated how such a system might be
employed to teach surgery. Satava’s assessment of that system was that the technology was no
where near good enough to be used in real training. He felt that it would take at least ten more
years for the technology to reach a useful state (R. Satava, personal communication, January 10,
2008).
Early definitions of “virtual reality” required that a system must immerse at least one of
the senses by cutting off access to the outside world and replacing it with a computer generated
stimuli. However, a less strict definition often allows that the visual, audible, or tactile stimuli
can be presented without totally eliminating external, non-computerized stimuli. This latter view
has proven to be more practical and less expensive to develop and to sell to customers. In
medical education, the term virtual reality is usually applied to any system where 3D computer
images are being presented and manipulated. This categorization leads to computer games being
referred to as virtual reality in most of the medical literature.
Computer Game Technologies
The computer gaming industry has begun to export powerful products and technologies
from its initial entertainment roots to a number of “serious” industries. Game technologies are
being adapted in defense, medical, architectural, educational, social, and governmental
4
applications. Each of these industries is already served by an established group of companies that
typically do not use computer games to serve their customers. The rapid growth in the power of
game technologies and the growing social acceptance of these technologies has created an
environment in which these are displacing other industry-specific computer hardware and
software tools.
Computer games provide a rich environment in which to train a wide variety of tasks.
The availability of the necessary computer hardware and game-based software technologies
makes these an attractive alternative to existing methods of training (Lane, 1995; Mayo, 2007).
This attraction is motivated by lower costs, higher effectiveness, and the in",Investigating the disruptive effect of computer game technologies on medical education and training,,University of Maryland University College (UMUC),,,core
76379984,2013-01-01T08:00:00,"Computational fluid dynamics (CFD) is one of the branches of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the millions of calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. Indoor airflow simulations are necessary for building emergency management, preliminary design of sustainable buildings, and real-time indoor environment control. The simulation should also be informative since the airflow motion, temperature distribution, and contaminant concentration is important. However, CFD computation is usually time-consuming, and not suitable for simulating real-time indoor air movement. Many researchers are concentrating on both hardware utilization and CFD algorithms, to make simulation much faster. Fast flow simulations are important for some applications in the building industry, such as the conceptual design of indoor environment, or they are coupled with energy simulation to provide deep analysis on the performance of the buildings. Such application does not require the same high level of accuracy as traditional CFD simulation because it only requires conceptual or semi-accurate distributions of the flow but within a short computing time. However, year round simulation is needed rather than the analysis of two or three extreme cases in order to help the designer investigate the problem clearly. To meet these special needs, an efficient and informative fluid simulation method is needed to provide fast airflow simulation with an inevitable but nominal compromise in accuracy. This research provides a comprehensive workflow for the designer to simulate and analyze the annual indoor environment. In addition to the hardware acceleration deployed, fast fluid simulation algorithm is developed, and a machine learning based interpolation is used to allow the simulation coverage to be conducted annually. The outcome of this research is a methodology that allows the annual simulation time similar to the one used to perform two or three extreme cases of simulation using current methods",A three layered framework for annual indoor airflow CFD simulation,,ScholarlyCommons,,,core
24972311,2013-10-19T00:00:00,"The effort invested in a software project is probably one of the most
important and most analyzed variables in recent years in the process of project
management. The limitation of algorithmic effort prediction models is their
inability to cope with uncertainties and imprecision surrounding software
projects at the early development stage. More recently attention has turned to
a variety of machine learning methods, and soft computing in particular to
predict software development effort. Soft computing is a consortium of
methodologies centering in fuzzy logic, artificial neural networks, and
evolutionary computation. It is important, to mention here, that these
methodologies are complementary and synergistic, rather than competitive. They
provide in one form or another flexible information processing capability for
handling real life ambiguous situations. These methodologies are currently used
for reliable and accurate estimate of software development effort, which has
always been a challenge for both the software industry and academia. The aim of
this study is to analyze soft computing techniques in the existing models and
to provide in depth review of software and project estimation techniques
existing in industry and literature based on the different test datasets along
with their strength and weaknesse",Soft computing techniques for software effort estimation,http://arxiv.org/abs/1310.5221,,,,core
100312931,2011,"I hereby confirm that my diploma thesis was independently authored by myself, using solely the referred sources and support. I also assert that this thesis has not been part of another examination process. Berlin, Date Signature (Behsaad Ramez) I The majority of AI components in video games are still heavily relying on static methods like finite state machines or goal-driven agent behaviour. Al-though these techniques will probably still prevail in the future of the in-dustry, dynamic, adapting, machine learning techniques can offer many im-provements and interesting game design options to game developers. Static techniques have the advantage that they are often intuitive to use and easier to debug, whereas dynamic methods can be used to reveal or cover AI and game design weaknesses and adapt to the player’s individual playing style. These positive factors outweigh their implementation effort in games where the AI has a major impact on the player’s experience and fun. Traditionally AI and unit balance in real time strategy (RTS) games is much more important than in other genres. Even mediocre, antiquated graphics in AAA industry titles like Starcraft TM can be forgiven if these components are well implemented. The recent boom of portable devices and smartphones that have the hard-ware requirements for complex real time 3D applications has also opened new game design and business possibilities for casual game developers on mobile platforms. In this paper I will describe the development of my own mulitplatform RTS game and an implementation of an online machine learning technique for this specific game. I",Diploma Thesis Mechs And Tanks A Multiplatform RTS Game with Machine Learning AI Author:,,,,,core
45104520,2011,"Objetivou-se com este trabalho avaliar a precisão das redes neurais artificiais (RNA) na estimativa das redes neurais artificiais (RNA) na predição de índices zootécnicos, com base em variáveis térmicas e fisiológicas de porcas gestantes. A pesquisa foi realizada entre janeiro e abril de 2005 em uma propriedade de produção industrial de suínos, no setor de gestação, com 27 matrizes primíparas, alojadas em baias individuais e posteriormente na maternidade em baias de parição, onde foram quantificados os índices de produção dos leitões provenientes do estudo. Para tanto, foi implementada uma RNA backpropagation, com uma camada de entrada, uma oculta e uma camada de saída com funções de transferência tangente sigmoidal. A temperatura do ar e a frequência respiratória foram consideradas variáveis de entrada e o peso ao nascimento dos leitões e número de leitões mumificados, como variáveis de saída. A rede treinada apresentou ótimo poder de generalização, o que possibilitou a predição das variáveis-respostas. A caracterização do ambiente da gestação e maternidade foi adequada se comparada aos dados reais, com poucas tendências de sub ou superestimação de alguns valores. A utilização desse sistema especialista para a previsão dos índices zootécnicos é viável, pois o sistema tem bom desempenho para esta aplicação.The objective of this work was to evaluate the precision of Artificial Neural Networks (ANNs) to estimate zootechnical indexes, based on thermal and physiological variables of pregnant sows. This study was carried out from January to April 2005, in a swine industrial production farm in the gestation section with 27 primiparous gilts, allocated in individual pens and after on farrowing pens where it was quantified animal production indexes of piglets from the study. Therefore, an ANN backpropagation was implemented, with one input layer, one hidden layer, and one output layer with tangent sigmoidal transference functions. Air temperature and respiratory frequency were considered as input variables and weight of piglet at birth and the number of mummified piglets as output variables. The trained ANN presented a great generalization power, which enabled the prediction of the answer-variables. Characterization of the environment of gestation and maternity was appropriated if compared to the real data, with few under or overestimated tendencies of some values. The use of this specialist system to predict zootechnical indexes is viable because the system shows a good performance for this use",Use of artificial neural networks on the prediction of zootechnical indexes on gestation and farrowing stages of swines,,Sociedade Brasileira de Zootecnia,,,core
301109176,2012-05-01T00:00:00,"Regulated Power System is widely accepted and practised in several countries. The entire electric utility of this traditional system is entirely owned and managed by one organization or commonly the government. The dictation right, monopoly concept and with no third party to ensure the efficiency of the management had caused this structure of industry become less competitive and less efficient. When this problem arises, the solution is not a better set of rules, but a structural change. With the ongoing liberalization of electricity markets, it is now moving towards the era of Deregulated Power System. This is a type of restructuring in Power Industry. The general mechanism of deregulation is to unbundle the Generation, Transmission and Distribution into generating companies (GENCOs), transmission companies (TRANSCOs) and distribution companies (DISTCOs). This unbundled system is very competitive as multiple GENCOs would compete among themselves to supply DISTCOs electric utility through short or long term contracts while the consumers are free to select any GENCOs that provide them with the best service and best price. Therefore, deregulation will be the future of realizing sustainable development at high efficiency. However, in open access environment where the consumers and distributors are free to choose their own generation supplier, transmission congestion is a major concern of this unbundled system. Transmission congestion is the condition where power that flows across transmission lines and transformers exceeds the physical limits of those lines. The main reasons for congestion management are due to the increase demand of electricity usage, the construction of transmission is expensive and the pressure from environmental groups that restrict construction of transmission. The chances of transmission lines getting over-loaded is comparatively higher under deregulated operation because vary parts of the system are owned by different companies and under varying service charges. Several conventional methods were used to manage congestion in transmission line. These methods are Linear Programming Method, Newton-Raphson Method, Quadratic Programming Method, Nonlinear Programming Method and Interior Point Method. The disadvantages of these conventional methods are complex mathematical formulation, unable to solve real-world large-scale power system problems, poor convergence and the system is slow when the variables are large. In recent years, Artificial Intelligence Method is frequently used as it can solve highly complex problems. Fuzzy Logic is one of the types under this Artificial Intelligence Method. Hence, in this paper, Fuzzy Logic approach is implemented for congestion management. This approach deals with approximation rather than precision. The simple rule-based of Fuzzy Logic is using “IF X AND Y THEN Z”. The load flow of the transmission line will be used to model Fuzzy Logic in controlling transmission congestion and tested using IEEE Reliability Test System-1996 (RTS-96). The results showed the congestion level for Weekly Load and Daily Load using the data in IEEE RTS-96. With the congestion level, the price can be further determined by the distributor according to Zonal Pricing Method and Nodal Pricing Method",Congestion Management of a Deregulated Power System Using Fuzzy Logic,https://core.ac.uk/download/301109176.pdf,Universiti Teknologi PETRONAS,,,core
16341499,2013-06-19T14:39:51,"9th AICS Conference, Irish Artificial Intelligence and Cognitive Science Conference, UCD, Dublin, 19 - 21 August, 1998This paper provides an overview Agent Factory, a rapid
prototyping environment for Multi-Agent Systems. It examines the agent
models and software tools used in this fabrication process. In particular, the
paper concentrates upon the Agent Factory Visualiser (AFV) tool – a tool that
supports the visualisation of agent communities through the Internet by way
of a VRML (Virtual Reality Modeling Language) tool. The functionality of
this tool is then animated through a simple virtual robotic demonstrator.3.7.2013. SB",Agent Factory : An Environment for Constructing and Visualising Agent Communities,,,,,core
143918384,2013-07-04T11:07:36,"9th AICS Conference, Irish Artificial Intelligence and Cognitive Science Conference, UCD, Dublin, 19 - 21 August, 1998This paper provides an overview Agent Factory, a rapid
prototyping environment for Multi-Agent Systems. It examines the agent
models and software tools used in this fabrication process. In particular, the
paper concentrates upon the Agent Factory Visualiser (AFV) tool – a tool that
supports the visualisation of agent communities through the Internet by way
of a VRML (Virtual Reality Modeling Language) tool. The functionality of
this tool is then animated through a simple virtual robotic demonstrator.3.7.2013. SB",Agent Factory : An Environment for Constructing and Visualising Agent Communities,,,,,core
142711035,2011,"Objetivou-se com este trabalho avaliar a precisão das redes neurais artificiais (RNA) na estimativa das redes neurais artificiais (RNA) na predição de índices zootécnicos, com base em variáveis térmicas e fisiológicas de porcas gestantes. A pesquisa foi realizada entre janeiro e abril de 2005 em uma propriedade de produção industrial de suínos, no setor de gestação, com 27 matrizes primíparas, alojadas em baias individuais e posteriormente na maternidade em baias de parição, onde foram quantificados os índices de produção dos leitões provenientes do estudo. Para tanto, foi implementada uma RNA backpropagation, com uma camada de entrada, uma oculta e uma camada de saída com funções de transferência tangente sigmoidal. A temperatura do ar e a frequência respiratória foram consideradas variáveis de entrada e o peso ao nascimento dos leitões e número de leitões mumificados, como variáveis de saída. A rede treinada apresentou ótimo poder de generalização, o que possibilitou a predição das variáveis-respostas. A caracterização do ambiente da gestação e maternidade foi adequada se comparada aos dados reais, com poucas tendências de sub ou superestimação de alguns valores. A utilização desse sistema especialista para a previsão dos índices zootécnicos é viável, pois o sistema tem bom desempenho para esta aplicação.The objective of this work was to evaluate the precision of Artificial Neural Networks (ANNs) to estimate zootechnical indexes, based on thermal and physiological variables of pregnant sows. This study was carried out from January to April 2005, in a swine industrial production farm in the gestation section with 27 primiparous gilts, allocated in individual pens and after on farrowing pens where it was quantified animal production indexes of piglets from the study. Therefore, an ANN backpropagation was implemented, with one input layer, one hidden layer, and one output layer with tangent sigmoidal transference functions. Air temperature and respiratory frequency were considered as input variables and weight of piglet at birth and the number of mummified piglets as output variables. The trained ANN presented a great generalization power, which enabled the prediction of the answer-variables. Characterization of the environment of gestation and maternity was appropriated if compared to the real data, with few under or overestimated tendencies of some values. The use of this specialist system to predict zootechnical indexes is viable because the system shows a good performance for this use",Use of artificial neural networks on the prediction of zootechnical indexes on gestation and farrowing stages of swines,,Sociedade Brasileira de Zootecnia,"[{'title': None, 'identifiers': ['1516-3598', 'issn:1516-3598']}]",10.1590/S1516-35982011000300028,core
23453003,2013-11-28,"Currently, embedded and real time systems are used in wide range of a related human applications to improve the quality of our lives such as embedded systems for communication ( Mobile, satellite, and avionics systems), and control systems such as (microwaves, refrigerators and embedded system in vehicles). Nevertheless, embedded and real time systems are still immature. The application of these systems is used for various devices. However, these systems are never used for a human body to complete human missing-part functionality; which means the embedded system can be used as part of natural neural networks in a human system nerve. This paper proposes a technical view to build an electronic glass (E-Glass) for the blind people. Moreover, this paper provides the complete E-Glass electronic circuit in which the electronic scanning system to tackle the objects and time signals are included. This E-Glass could be used by the blind to assist them in their ways without any human assistance. Moreover, it will be used by the blind to make them self confidence, to let them walk independently and to increase their morality. It is important to note that the hardware and software components of the E-Glass are not expensive. This work could be provided to the practitioner’s people in the industry or to the students of the department of electrical or biomedica",Build an E-Glass for the Blinds,,,,,core
22632538,2013-07-23,"Abstract — Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments. Keywords- activity recognition, industrial workflows, service management, cloud infrastructure, QoS. I",QoS-oriented Service Management in Clouds for Large Scale Industrial Activity Recognition,,,,,core
151575690,2012-07-05T07:00:00,"SHW systems are generally expected to last for at least 20 years with little or no maintenance. However, in many cases failures occur far sooner due to a variety of problems, many of which are undetected or detected long after the system has failed because the backup heater silently assumes the heating load. Some of the failures may cause the system to run inefficiently or even damage other system components, such as when a system loses fluid in the solar loop and the pump runs dry, eventually destroying itself.  In recent years there has been an increasing demand for SHW systems to become economic and reliable. Fault Detection and Diagnosis (FDD) in SHW systems is an important part of maintaining proper performance, reducing power consumption and unnecessary peak electricity demand. The aim of the current work is to develop anomaly detection system that can reliably detect both anticipated and unforeseen faults and can be implemented in commercial SHW systems without any additional sensors to the ones commonly needed for ordinary system control.  Adaptive Resonance Theory (ART)-based neural networks are chosen to perform this task, because the ART-based neural networks are fast, efficient learners and retain memory while learning new patterns. In particular, the ART networks can be incorporated into SHW system controller without any extra sensors and have the capability of an early detection of performance degradation faults. Other benefits of ART-based neural networks are on-line fault detection for its high computational efficiency and no involvement of faulty data for the training process.  A testbed for SHW system reliability is developed for the purposes of investigating the fault detection system. The input patterns of the fault detection system are generated from two sensors: collector plate temperature and water tank heat exchanger outlet temperature, which are normally installed in residential SHW systems installed by commercial operators. One of the strengths of the system is that only few data points are needed, meaning that it will not be necessary to instrument SHW systems with additional sensors, something which would not be acceptable in an aggressively competitive industry where reducing costs is paramount.  The training data for the fault detection system are generated from a verified SHW system TRNSYS (Transient Systems Simulation) model. The simulation and experimental results show that the ART-based anomaly detection has the capability to accurately and efficiently detect degradation and failure. Faults are detected at various levels depending on their severity. The ART-based anomaly detection can be used for SHW real-time reliability monitoring, as well as, eventually, in larger, more complex systems such as commercial building HVAC systems or subsystems",Monitoring and anomaly detection in solar thermal systems using adaptive resonance theory neural networks,https://core.ac.uk/download/151575690.pdf,UNM Digital Repository,,,core
227572386,2013-01-01T00:00:00,"This paper summarize the first results of the research project SOMMACT (Self Optimising Measuring MAChine Tools), funded by the Seventh Framework Program of the European Commission, which proposes an innovative method aimed at the numerical compensation of these errors. The objective is the development of a controller with self-learning capabilities that, starting from historical information acquired by sensors systems, artefacts and finished workpiece measurements associated to the machine operative conditions, accumulates knowledge on machine performances, is able to predict errors that a machining process would present in different conditions and, therefore,can adapt compensation tables. Starting from the storage of a large data amount, a supervised learning method like Support Vector Machine, together with an inferential Fuzzy Logic-based system, is able to manage correspondences between historical accumulated information and a potential current situation, evaluating the influence of non-comparable physical dimensions, whose effects can’t a priori be added. This method can be applied on different machine tools typologies, also aside from the number of axes and is implemented inside a more complex software system aimed at supporting measurement procedures and provided of graphical user interface, able to calculate the volumetric error with 3D representations, to integrate different sensors systems, providing models for error functions calculation starting from measurements data and integrating communication processes with the CNC. This allows its effective application in real production sites, introducing relevant improvements in machine tools manufacturing field",Quasi static error compensation of 5-axis large machine tools using on-board sensors and AI analysis,,,,,core
56847977,2012,"Real-time monitoring of laser beam welding (LBW) has increasingly gained importance in several manufacturing processes ranging from automobile production to precision mechanics. In the latter, a novel algorithm for the real-time detection of spatters was implemented in a camera based on cellular neural networks. The latter can be connected to the optics of commercially available laser machines leading to real-time monitoring of LBW processes at rates up to 15 kHz. Such high monitoring rates allow the integration of other image evaluation tasks such as the detection of the full penetration hole for real-time control of process parameters",A novel spatter detection algorithm based on typical cellular neural network operations for laser beam welding processes,,,,10.1088/0957-0233/23/1/015401,core
25883166,2011-03-01T00:00:00Z,"Objetivou-se com este trabalho avaliar a precisão das redes neurais artificiais (RNA) na estimativa das redes neurais artificiais (RNA) na predição de índices zootécnicos, com base em variáveis térmicas e fisiológicas de porcas gestantes. A pesquisa foi realizada entre janeiro e abril de 2005 em uma propriedade de produção industrial de suínos, no setor de gestação, com 27 matrizes primíparas, alojadas em baias individuais e posteriormente na maternidade em baias de parição, onde foram quantificados os índices de produção dos leitões provenientes do estudo. Para tanto, foi implementada uma RNA backpropagation, com uma camada de entrada, uma oculta e uma camada de saída com funções de transferência tangente sigmoidal. A temperatura do ar e a frequência respiratória foram consideradas variáveis de entrada e o peso ao nascimento dos leitões e número de leitões mumificados, como variáveis de saída. A rede treinada apresentou ótimo poder de generalização, o que possibilitou a predição das variáveis-respostas. A caracterização do ambiente da gestação e maternidade foi adequada se comparada aos dados reais, com poucas tendências de sub ou superestimação de alguns valores. A utilização desse sistema especialista para a previsão dos índices zootécnicos é viável, pois o sistema tem bom desempenho para esta aplicação.<br>The objective of this work was to evaluate the precision of Artificial Neural Networks (ANNs) to estimate zootechnical indexes, based on thermal and physiological variables of pregnant sows. This study was carried out from January to April 2005, in a swine industrial production farm in the gestation section with 27 primiparous gilts, allocated in individual pens and after on farrowing pens where it was quantified animal production indexes of piglets from the study. Therefore, an ANN backpropagation was implemented, with one input layer, one hidden layer, and one output layer with tangent sigmoidal transference functions. Air temperature and respiratory frequency were considered as input variables and weight of piglet at birth and the number of mummified piglets as output variables. The trained ANN presented a great generalization power, which enabled the prediction of the answer-variables. Characterization of the environment of gestation and maternity was appropriated if compared to the real data, with few under or overestimated tendencies of some values. The use of this specialist system to predict zootechnical indexes is viable because the system shows a good performance for this use",Uso de redes neurais artificiais para predição de índices zootécnicos nas fases de gestação e maternidade na suinocultura Use of artificial neural networks on the prediction of zootechnical indexes on gestation and farrowing stages of swines,,Sociedade Brasileira de Zootecnia,"[{'title': None, 'identifiers': ['1806-9290', 'issn:1806-9290', '1516-3598', 'issn:1516-3598']}]",10.1590/S1516-35982011000300028,core
56878045,2011,"Cellular Neural Networks (CNN) are more and more attractive for closed loop control systems based on image processing because they allow for the combination of highcomputational power and short feedback times. This combination enables new applications, which are not feasible for conventional image processing systems. Laser beam welding (LBW), which has been largely adopted in the industrial scenario,is an example for such processes. Concerning the latter, monitoring systems using conventional cameras are quite common, but they do a statistical post process evaluation of certain image features for quality control purposes. Earlier attempts to build closed loop control systems failed due to the lack of computational power. In order to increase controlling rates and decrease false detectio ns by a more robust evaluation of the image feature, strategies based on CNN operations have been implemented in a cellular architecture called Q-Eye. They allow enabling the first robust closed loop control system adapting the laser power by observing the full penetration hole (FPH) in the melt. In this paper, the algorithms adopted for the FPH detection in process images are described and compared. Furthermore, experimental results obtained in real time applications are also discussed",Real time control of laser beam welding processes: Reality,,,,10.1007/978-1-4419-6475-5_12,core
5240865,2012-04-07T00:00:00,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.Comment: EPAM, General Problem solve",Machine Cognition Models: EPAM and GPS,http://arxiv.org/abs/1204.1653,,,,core
21625885,2011-10-29,"This paper discusses the identification of trash objects in cotton using machine vision-based systems. Soft computing techniques such as neural networks and fuzzy inference systems can classify trash objects into individual categories such as bark, stick, leaf, and pepper trash types with great accuracies. High speed trash measurements, enables the implementation of these techniques for on-line identification of trash. This identification of trash objects to individual categories can be used for the dynamic allocation of trash extraction equipment during the ginning process. Such a system can be implemented in a modern gin, to configure an optimal set of equipment during ginning to produce quality cotton. Classification of cotton in real-time allows for an automated means for assignment of trash grades to cotton, and could have a significant impact on the cotton industry",High Speed Trash Measurements,,,,,core
216474667,2012-01-01T00:00:00,"Torna-se cada vez mais comum o uso de dispositivos eletrônicos não apenas como ferramenta de trabalho, mas também como auxílio à atividade a ser executada. Na área de logística da empresa ADiWa, diferentes partes interessadas desempenhando diferentes papeis devem lidar com a mesma informação em locais possivelmente distintos, como escritórios, áreas de estoques ou até em veículos, caracterizando um problema de multi-criteria. Essa informação deve ser mostrada pelo dispositivo adequado em termos de tamanho, autonomia e muitas outras características. Decidir qual o melhor dispositivo em relação a todas essas informações não é tarefa fácil, portanto o uso de um sistema de apoio à tomada de decisão torna-se fundamental. A ADiWa contratou a organização Fraunhofer para desenvolver o sistema de apoio. Fraunhofer é a maior organização de pesquisa aplicada da Europa, com mais de 80 institutos de pesquisa. Seus clientes incluem companhias da indústria, de serviços e também órgãos públicos. O Fraunhofer Institute for Experimental Software Engineering (IESE) usa métodos com base empírica e processos para software voltados para a indústria, assim como o desenvolvimento de sistemas. No contexto do Fraunhofer IESE foi desenvolvida uma ferramenta que, dadas várias características de uma empresa, da tarefa a ser executada, do ambiente dessa tarefa e do perfil do usuário do dispositivo, entre outros aspectos, retorna uma lista de dispositivos ordenados pela sua adequação àquelas características e auxilia o processo de decisão. Fraunhofer é a maior organização de pesquisa a aplicada da Europa. Isso foi feito usando-se Drools, uma plataforma de integração de lógica de negócios que oferece um sistema especialista baseado em regras e eventos.  Sistemas especialistas tentam imitar as habilidades de decisão de um humano sobre um domínio específico e podem ser considerados o primeiro grande sucesso da Inteligência Artificial. Surgiram na década de 70 e se popularizaram na década de 80, sendo reconhecidos como uma ferramenta capaz de resolver problemas reais. Uma das áreas de aplicação é o suporte à decisão em sistemas complexos, como é o caso deste trabalho. No primeiro capítulo o problema vai ser caracterizado (como multi-criteria) e soluções para problemas similares serão apresentadas. No segundo capítulo serão explicados os requerimentos e suas origens. O terceiro capítulo trata da modelagem dos requerimentos e dos dispositivos, que inclui o atributo “adequação” que será usado na solução. No quarto capítulo é descrito o desenvolvimento do programa, da escolha do sistema especialista até o desenvolvimento da interface gráfica com o usuário. No quinto capítulo é feita uma breve descrição do funcionamento do programa.It is becoming increasingly common the use of electronic devices not only as a working tool but also as an aid to the activity being performed. In the logistic of the company ADiWa different stakeholders playing different roles must deal with the same information in different places such as offices, areas of inventory or even vehicles, featuring a multi-criteria problem. This information should be displayed for the appropriate device in terms of size, autonomy and many other features. Deciding the best device for all this information is not an easy task, so the use of a support system for decision making is fundamental. ADiWa hired the Fraunhofer organization to develop the support system. Fraunhofer is the largest organization for applied research in Europe, with more than 80 research institutes. Its contractor and clients include industrial companies, service companies and the public sector. The Fraunhofer Institute for Experimental Software Engineering (IESE) stands for empirically based methods and processes for industrial software and systems development. In the environment of Fraunhofer IESE was developed a tool that, given various characteristics about the company, the task to be performed, the environment of the task and also about the user of the device, among other things, returns a device rank by the suitability with those characteristics and this rank will assist the decision-making. This was done using Drools, an integration platform that provides business logic of an expert system based on rules and events. Expert systems attempt to mimic the abilities of a human decision-making ability about a specific domain and can be considered the first great success of Artificial Intelligence. Emerged in the 70s and became popular in the 80s, being recognized as a tool to solve real-world problems. One area of application is the decision support in complex systems, such as in this study.  In the first chapter the problem will be characterized (as multi-criteria) and solutions to similar problems will be presented. In the second chapter will be explained the requirements and their origins. The third chapter deals with the modeling of applications and devices, which includes the attribute ""suitability"" to be used in the solution. The fourth chapter describes the development of the program, from the choice of the expert system to the development of graphical user interface. In the fifth chapter there is a brief description of the operation of the program",Sistema especialista para ordenamento de dispositivos,,,,,core
400071107,2011-07-12T00:00:00,"International audienceA major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real world. The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e. cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources - expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex (LPFC and ACC) reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human-robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to ""cheating"" by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real world",Robot cognitive control with a neurophysiologically inspired reinforcement learning model,,'Frontiers Media SA',,10.3389/fnbot.2011.00001,core
103076137,2013,"The efficient, distributed factorization of large matrices on clusters of commodity machines is crucial to applying latent factor models in industrial-scale recommender systems. We propose an efficient, data-parallel low-rank matrix factorization with Alternating Least Squares which uses a series of broadcast-joins that can be efficiently executed with MapReduce. We empirically show that the performance of our solution is suitable for real-world use cases. We present experiments on two publicly available datasets and on a synthetic dataset termed Bigflix, generated from the Netflix dataset. Bigflix contains 25 million users and more than 5 billion ratings, mimicking data sizes recently re-ported as Netflix ’ production workload. We demonstrate that our ap-proach is able to run an iteration of Alternating Least Squares in six minutes on this dataset. Our implementation has been contributed to the open source machine learning library Apache Mahout",Distributed matrix factorization with mapreduce using a series of broadcast-joins,,,,10.1145/2507157.2507195,core
102624212,2013,"Actual software development processes define the different steps developers have to perform during a development project. Usually these development steps are not described independently from each other—a more or less formal flow of development step is an essential part of the development process definition. In practice, we observe that often the process definitions are hardly used and very seldom “lived”. One reason is that the predefined general process flow does not reflect the specific constraints of the individual project. For that reasons we claim to get rid of the process flow definition as part of the development process. Instead we describe in this paper an approach to smartly assist developers in software process execution. The approach observes the developer’s actions and predicts his next development step based on the project process history. Therefore we apply machine learning resp. sequence learning approaches based on a general rule based process model and its semantics. Finally we show two evaluations of the presented approach: The data of the first is derived from a synthetic scenario. The second evaluation is based on real project data of an industrial enterprise",Open Access                                              JCC Smart Development Process Enactment Based on Context Sensitive Sequence Prediction,,,,,core
293486815,2012-01-01T00:00:00,"Torna-se cada vez mais comum o uso de dispositivos eletrônicos não apenas como ferramenta de trabalho, mas também como auxílio à atividade a ser executada. Na área de logística da empresa ADiWa, diferentes partes interessadas desempenhando diferentes papeis devem lidar com a mesma informação em locais possivelmente distintos, como escritórios, áreas de estoques ou até em veículos, caracterizando um problema de multi-criteria. Essa informação deve ser mostrada pelo dispositivo adequado em termos de tamanho, autonomia e muitas outras características. Decidir qual o melhor dispositivo em relação a todas essas informações não é tarefa fácil, portanto o uso de um sistema de apoio à tomada de decisão torna-se fundamental. A ADiWa contratou a organização Fraunhofer para desenvolver o sistema de apoio. Fraunhofer é a maior organização de pesquisa aplicada da Europa, com mais de 80 institutos de pesquisa. Seus clientes incluem companhias da indústria, de serviços e também órgãos públicos. O Fraunhofer Institute for Experimental Software Engineering (IESE) usa métodos com base empírica e processos para software voltados para a indústria, assim como o desenvolvimento de sistemas. No contexto do Fraunhofer IESE foi desenvolvida uma ferramenta que, dadas várias características de uma empresa, da tarefa a ser executada, do ambiente dessa tarefa e do perfil do usuário do dispositivo, entre outros aspectos, retorna uma lista de dispositivos ordenados pela sua adequação àquelas características e auxilia o processo de decisão. Fraunhofer é a maior organização de pesquisa a aplicada da Europa. Isso foi feito usando-se Drools, uma plataforma de integração de lógica de negócios que oferece um sistema especialista baseado em regras e eventos. Sistemas especialistas tentam imitar as habilidades de decisão de um humano sobre um domínio específico e podem ser considerados o primeiro grande sucesso da Inteligência Artificial. Surgiram na década de 70 e se popularizaram na década de 80, sendo reconhecidos como uma ferramenta capaz de resolver problemas reais. Uma das áreas de aplicação é o suporte à decisão em sistemas complexos, como é o caso deste trabalho. No primeiro capítulo o problema vai ser caracterizado (como multi-criteria) e soluções para problemas similares serão apresentadas. No segundo capítulo serão explicados os requerimentos e suas origens. O terceiro capítulo trata da modelagem dos requerimentos e dos dispositivos, que inclui o atributo “adequação” que será usado na solução. No quarto capítulo é descrito o desenvolvimento do programa, da escolha do sistema especialista até o desenvolvimento da interface gráfica com o usuário. No quinto capítulo é feita uma breve descrição do funcionamento do programa.It is becoming increasingly common the use of electronic devices not only as a working tool but also as an aid to the activity being performed. In the logistic of the company ADiWa different stakeholders playing different roles must deal with the same information in different places such as offices, areas of inventory or even vehicles, featuring a multi-criteria problem. This information should be displayed for the appropriate device in terms of size, autonomy and many other features. Deciding the best device for all this information is not an easy task, so the use of a support system for decision making is fundamental. ADiWa hired the Fraunhofer organization to develop the support system. Fraunhofer is the largest organization for applied research in Europe, with more than 80 research institutes. Its contractor and clients include industrial companies, service companies and the public sector. The Fraunhofer Institute for Experimental Software Engineering (IESE) stands for empirically based methods and processes for industrial software and systems development. In the environment of Fraunhofer IESE was developed a tool that, given various characteristics about the company, the task to be performed, the environment of the task and also about the user of the device, among other things, returns a device rank by the suitability with those characteristics and this rank will assist the decision-making. This was done using Drools, an integration platform that provides business logic of an expert system based on rules and events. Expert systems attempt to mimic the abilities of a human decision-making ability about a specific domain and can be considered the first great success of Artificial Intelligence. Emerged in the 70s and became popular in the 80s, being recognized as a tool to solve real-world problems. One area of application is the decision support in complex systems, such as in this study. In the first chapter the problem will be characterized (as multi-criteria) and solutions to similar problems will be presented. In the second chapter will be explained the requirements and their origins. The third chapter deals with the modeling of applications and devices, which includes the attribute ""suitability"" to be used in the solution. The fourth chapter describes the development of the program, from the choice of the expert system to the development of graphical user interface. In the fifth chapter there is a brief description of the operation of the program",Sistema especialista para ordenamento de dispositivos,,,,,core
214263389,2013-01-01T08:00:00,"Computational fluid dynamics (CFD) is one of the branches of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the millions of calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. Indoor airflow simulations are necessary for building emergency management, preliminary design of sustainable buildings, and real-time indoor environment control. The simulation should also be informative since the airflow motion, temperature distribution, and contaminant concentration is important. However, CFD computation is usually time-consuming, and not suitable for simulating real-time indoor air movement. Many researchers are concentrating on both hardware utilization and CFD algorithms, to make simulation much faster. Fast flow simulations are important for some applications in the building industry, such as the conceptual design of indoor environment, or they are coupled with energy simulation to provide deep analysis on the performance of the buildings. Such application does not require the same high level of accuracy as traditional CFD simulation because it only requires conceptual or semi-accurate distributions of the flow but within a short computing time. However, year round simulation is needed rather than the analysis of two or three extreme cases in order to help the designer investigate the problem clearly. To meet these special needs, an efficient and informative fluid simulation method is needed to provide fast airflow simulation with an inevitable but nominal compromise in accuracy. This research provides a comprehensive workflow for the designer to simulate and analyze the annual indoor environment. In addition to the hardware acceleration deployed, fast fluid simulation algorithm is developed, and a machine learning based interpolation is used to allow the simulation coverage to be conducted annually. The outcome of this research is a methodology that allows the annual simulation time similar to the one used to perform two or three extreme cases of simulation using current methods",A three layered framework for annual indoor airflow CFD simulation,,ScholarlyCommons,,,core
9324409,2012-08-16T00:00:00,"The process of sorting marble plates according to their surface texture is an
important task in the automated marble plate production. Nowadays some
inspection systems in marble industry that automate the classification tasks
are too expensive and are compatible only with specific technological equipment
in the plant. In this paper a new approach to the design of an Automated Marble
Plate Classification System (AMPCS),based on different neural network input
training sets is proposed, aiming at high classification accuracy using simple
processing and application of only standard devices. It is based on training a
classification MLP neural network with three different input training sets:
extracted texture histograms, Discrete Cosine and Wavelet Transform over the
histograms. The algorithm is implemented in a PLC for real-time operation. The
performance of the system is assessed with each one of the input training sets.
The experimental test results regarding classification accuracy and quick
operation are represented and discussed.Comment: 7 pages, 11 figures, 1 table, (IJARAI) International Journal of
  Advanced Research in Artificial Intelligence, Vol. 1, No. 2, 2012;
  ISSN:2165-4069(Online), ISSN:2165-4050 (Print","Automated Marble Plate Classification System Based On Different Neural
  Network Input Training Sets and PLC Implementation",http://arxiv.org/abs/1208.6310,,,,core
82262799,2011-12-31,"AbstractThe non-uniformity of Infrared Focal Plane Array (IRFPA) resulted from the limits of the detector's materials and the manufacturing process affects the performance of the staring IR imaging system. To address this problem, non-uniformity correction (NUC), applied for real-time resolution, is the important issue in the IR imaging information processing system. This thesis introduces method of non-uniformity correction. Considering the nonlinear character of IRFPA, the calibration-based polynomial NUC method is proposed in the hardware system. Comparing with the conventional NUC schemes, polynomial method can achieve better NUC performance and implement in real-time. The algorithm is designed based on System architecture for FPGA hardware, for which is the Xilinx ML402 platform dedicated for video processing, which consists of A/D and D/A converter, and Virtex-4 FPGA on the mother board. The polynomial method reduces the non-uniformity in the infrared image largely, implemented at real-time, as well as the advantage of wide dynamic range",Calibration-based NUC Method in Real-time Based on IRFPA ,https://core.ac.uk/download/pdf/82262799.pdf,Published by Elsevier B.V.,,10.1016/j.phpro.2011.11.058,core
22883649,2013-08-12,"Grid computing provides key infrastructure for distributed problem solving in dynamic virtual organizations. It has been adopted by many scientific projects, and industrial interest is rising rapidly. However, Grids are still the domain of a few highly trained programmers with expertise in networking, high-performance computing, and operating systems. This paper describes our initial work in capturing knowledge and heuristics about how to select application components and computing resources, and using that knowledge to generate automatically executable job workflows for the Grid. Our system is implemented and integrated with a Grid environment where it has generated dozens of workflows with hundreds of jobs in real time. The paper also discusses the prospects of using AI to improve current Grid infrastructure",Transparent Grid Computing: A Knowledge-Based Approach,,,,,core
26140331,2014-05-01T00:00:00Z,"The integration of computer applications into industrial enterprises is imposed by the necessity of real-time decision 

making in order to meet the frequent changes within a business environment. In this paper, we have tried to identify the 

impact that IT has on production management as well as the need for planned changes enabling effective 

implementation of integrated information systems. At present, the integration of software applications has developed 

into CIMOSA type (Computer Integrated Manufacturing Open System Architecture), which allow computer modelling 

of business processes, taking into account their knowledge bases. The implementation of such computer models 

facilitate the use of expert systems and other forms of artificial intelligence, capable of reasoning through a problem in 

order to reach a conclusion",THE IMPACT OF IT ON PRODUCTION MANAGEMENT,,Academica Brâncuşi,"[{'title': None, 'identifiers': ['1844-7007', 'issn:1844-7007']}]",,core
37506947,2012,"Traditional methods for bacterial identification include Gram staining, culturing, and biochemical assays for phenotypic characterization of the causative organism. These methods can be time-consuming because they require in vitro cultivation of the microorganisms. Recently, however, it has become possible to obtain chemical profiles for lipids, peptides, and proteins that are present in an intact organism, particularly now that new developments have been made for the efficient ionization of biomolecules. MS has therefore become the state-of-the-art technology for microorganism identification in microbiological clinical diagnosis. Here, we introduce an innovative sample preparation method for nonculture-based identification of bacteria in milk. The technique detects characteristic profiles of intact proteins (mostly ribosomal) with the recently introduced MALDI SepsityperTM Kit followed by MALDI-MS. In combination with a dedicated bioinformatics software tool for databank matching, the method allows for almost real-time and reliable genus and species identification. We demonstrate the sensitivity of this protocol by experimentally contaminating pasteurized and homogenized whole milk samples with bacterial loads of 10(3)-10(8) colony-forming units (cfu) of laboratory strains of Escherichia coli, Enterococcus faecalis, and Staphylococcus aureus. For milk samples contaminated with a lower bacterial load (104 cfu mL-1), bacterial identification could be performed after initial incubation at 37 degrees C for 4 h. The sensitivity of the method may be influenced by the bacterial species and count, and therefore, it must be optimized for the specific application. The proposed use of protein markers for nonculture-based bacterial identification allows for high-throughput detection of pathogens present in milk samples. This method could therefore be useful in the veterinary practice and in the dairy industry, such as for the diagnosis of subclinical mastitis and for the sanitary monitoring of raw and processed milk products.Brazilian Science foundation CNPqBrazilian Science foundation CNPqFAPESPFAPESP [09/12751-5]FINEPFINE",Nonculture-based identification of bacteria in milk by protein fingerprinting,,HOBOKEN,"[{'title': None, 'identifiers': ['issn:1615-9853', '1615-9853']}]",10.1002/pmic.201200053,core
147981631,2012-04-17T13:00:35,"Process Analytical Chemistry/Technology has tremendously evolved in the last decades due to the development of multivariate online sensors that are able to monitor the properties of industrial processes in real time [1, 2]. Online monitoring of product quality and the detection of process upsets are important for the pharmaceutical and fine chemical industry in order to maintain product specifications and meet their commitments regarding safety, health and environment.  Many methods exist to extract useful information from the vast amount of data produced by online sensors. Chemometric methods, such as Principal Component Regression (PCR) and Partial Least Squares (PLS) or Black Box modelling (e.g. Neural Networks) are commonly used during the monitoring of batch processes [3, 4]. However, for these data-driven methods, calibration conditions need to be maintained during the actual process and the calibration generally behaves poorly when extrapolated to different operating conditions. On the other hand, kinetic modelling techniques [5], based on first principal models, describing the kinetics of main and side products, do not encounter such drawbacks and can be adapted for the monitoring of highly fluctuating processes, e.g. under semi-batch conditions.  During batch and semi-batch processes, deviations from standard operating conditions can have various origins. Most frequent sources of deviations are due to slightly imprecise initial conditions (e.g. initial concentrations) or impurities in the initial reactants causing unexpected side reactions [6]. In this contribution, we propose a method for the online monitoring of semi-batch processes based on a kinetic modelling approach in order to optimise operating conditions and reduce “batch to batch” deviations. To our knowledge, this option has not yet been considered in literature.  The proposed method requires the kinetic model and the associated rate constants to be known, i.e. determined in an early phase of R&D. In the following, the different steps of the algorithm, currently implemented into Matlab, are outlined. The algorithm assumes a first small amount of reagent to be dosed into the reaction mixture inside the reactor. Corrected initial concentrations are then determined by fitting the kinetic model to measurements, such as UV-vis, IR or heat power, using the Newton-Gauss-Levenberg/Marquardt (NGL/M) optimiser. If the optimiser fails the operator has the option to dose more reagent. Possible failure can be due to an early process upset, or to the fact that too little reagent was dosed in order to follow the kinetics reliably.  The corrected initial concentrations are then fed back into the kinetic model and the algorithm optimises the flow rate for the dosed reagent or the operating temperature in order to maximise under constraints user-defined properties of the process, such as yield, selectivity or conversion. For this constrained optimisation, nonlinear programming (NLP) is employed (Matlab’s fmincon function). As soon as optimum operating conditions are obtained by the algorithm, the reactor will automatically run at these improved settings. As an option, flow rate and temperature can continuously be re-optimised to adapt to possible fluctuations in operating conditions. During the whole procedure, the algorithm also tests for possible process upset. If such an incident is detected the operator is asked to take appropriate action, for example a reactor shut down.  The algorithm will be demonstrated using simulated data from mid-IR and UV-vis spectroscopy as well as from calorimetry.  [1] P. Gemperline, G. Puxty, M. Maeder, D. Walker, F. Tarczynski, M. Bosserman, Analytical Chemistry 76 (2004) 2575-2582.  [2] J. Workman, M. Koch, D. Veltkamp, Analytical Chemistry 77 (2005) 3789-3806.  [3] M. Spear, Chemical Processing 70 (2007) 20-26.  [4] T.J. Thurston, R.G. Brereton, D.J. Foord, R.E.A. Escott, Journal of Chemometrics 17 (2003) 313-322.  [5] M. Maeder, Y.M. Neuhold, Practical Data Analysis in Chemistry, Elsevier, Amsterdam NL, 2007.  [6] E.N.M. van Sprang, H.J. Ramaker, H.F.M. Boelens, J.A. Westerhuis, D. Whiteman, D. Baines, I. Weaver, Analyst 128 (2003) 98-102",Online optimisation and detection of process upset in semi-batch reactors using a kinetic modelling approach,https://core.ac.uk/download/147981631.pdf,,,,core
88838648,2014-12-04T00:00:00,"In the past decades, embedded systems become more and more present in our daily life. They are present in our wallet, in our car, in our house appliances and the current tendency is to control more and more things by using them. Major companies already started to envision the future of internet which become known as the Internet of Things. In the next years, more and more of our things will be smart, connected... and subject to software flaws.Micro-controllers, very small devices with a few hundreds bytes of memory, are at the heart of this revolution. They become cheaper, smaller and more powerful. Yet, contrary to the technologies used in our desktop computers or our server farms, the goal of the industry that creates these devices is not power. Indeed, controlling temperature in your home or humidity in your wine cellar does not need Gigahertz core. It even does not need multiple cores. What these devices need is to be cheap, to be able to be produced in very high volumes and to use small amount of electrical energy. In this context, the moore's law does not help us to have more cores in one device, but it allows to produce more devices with the same amount of silicon. The research I made in the past years try to reduce the gap between cheap software and robust and efficient software for these devices. I think that, by providing smart tools and production toolchains we can help junior or non specialized developers so that they can produce good embedded software for a reasonable development cost. I also focus my work on optimizing and providing efficient and secure software and network protocols, so that the Internet of Things can become a reality while respecting user privacy and being sustainable from a energy point of view.I focused my research on three aspects. First, I focused on how to allow larger softwares do be developped in embedded systems. In particular, we proposed techniques to allow Java/JavaCard code to be executed from a non-addressable memory using a full software cache.  I also looked at how to use domain specific languages to ease the implementation of a collaborative network intrusions detection system. Thanks to adapted tools, the software, writen in the DSL, can be compiled for a wide range of probes while offering garanties on the produced software. Last, I focused on energy aware network protocols in the context of smart cities.Au cours des dernières années, les systèmes embarqués sont de plus en plus présents dans notre vie de tous les jours. Ils sont dans notre portefeuille, dans notre voiture ou dans nos appareils ménagers. La tendance actuelle est à contrôler de plus en plus d'objets à l'aide de ces systèmes. Les grands acteurs de l'industrie ont déjà commencé à envisager le futur de l'internet, l'internet des objets. Dans les années à venir, de plus en plus de nos objets seront « intelligents », connectés... et sujets à des fautes logicielles.Les micro contrôleurs, de minuscules ordinateurs possédant quelques centaines d'octets de mémoire, sont au coeur de cette révolution. Ils deviennent de moins en moins cher et de plus en plus puissant. Néanmoins, à la différence de nos ordinateurs de bureau ou des serveurs, le but de l'industrie des micro contrôleurs n'est pas la puissance. En effet, contrôler la température de notre maison et le taux d'humidité de notre cave à vin ne nécessite pas des processeurs cadencés à plusieurs gigahertz. Cela ne nécessite même pas plusieurs coeurs de calculs. Le véritable besoin de ces équipements est d'être bons marché, d'être produits en très grands volumes, d'êtres petits, facilement intégrables et d'utiliser une faible quantité d'énergie électrique. Le meilleur exemple de cette tendance est très certainement la carte à puce. Depuis le début des années 90, de plus en plus de cartes à puce sont produites, vendues et utilisées dans le monde. Cependant, elles n'en restent pas moins de tout petits équipements d'une puissance inférieure de plusieurs ordres de grandeur de nos ordinateurs de bureau. Quand il s'agit de développer du logiciel pour ces équipements, un développeur débutant n'est généralement pas suffisamment préparé et ne sera pas capable d'écrire du logiciel efficace pour ces cibles avant de longue années passées à gagner de l'expérience et de l'expertise. Si le logiciel que l'ont souhaite produire doit être efficace, sûr et correct au sens le plus strict du terme, l'industrie doit compter sur des développeurs très spécialisés, expérimentés et donc chers. Les acteurs industriels ont donc deux alternatives : produire du logiciel bon marché, mais assez inefficaceet défectueux ou dépenser une somme importante pour le développement et fournir un logiciel de bonne qualité. On peu aisément supposer que la tendance actuelle est au logiciel bon marché, et que la probabilité que le logiciel qui équipera les millions d'équipements formant l'internet des objets sera de piètre qualité et offrira quantité de failles que des acteurs malveillants se feront une joie d'exploiter si rien n'est fait pour rendre bon marché le développement correct de logiciels embarqués.Mes recherches de ces dernières années vont dans le sens de la réduction du fossé séparant logiciels bon marché et logiciels sûrs et performants. Je suis convaincu qu'en offrant des outils « intelligents » et des chaînes de production logicielle efficaces, nous pouvons aider les développeurs débutants, ou non spécialisés, à produire du logiciel embarqué de bonne qualité pour un coût de développement raisonnable. J'ai également porté mon attention sur l'optimisation et la sécurisation des logiciels et des protocoles réseau afin que l'internet des objets puisse devenir une réalité tout en respectant la vie privée des utilisateurs et en offrant une alternative durable sur le plan énergétique.Je me suis principalement intéressé à trois champs de recherche. Tout d'abord, j'ai cherché à permettre l'utilisation de techniques de développement standard (objets, composants, programmation en Java…) à la faible capacité mémoire des équipements embarqués. A l'inverse, je me suis également intéressé à l'utilisation de langages dédiés à une application afin de permettre à des spécialistes du domaine de la sécurité réseau d'exprimer des algorithmes de détection d'intrusions. A l'aide d'une suite d'outils dédiée, ces algorithmes sont compilés et optimisés automatiquement pour être utilisés dans une architecture distribuée de sondes embarquées. Enfin, je me suis intéressé aux protocoles réseau économes en énergie pour interconnecter les équipements embarqués dans le cadre des villes intelligentes",Élements de conception de systèmes embarqués fortement contraints,,HAL CCSD,,,core
211485932,2011-01-01T00:00:00,"This interdisciplinary research is based on the application of unsupervized connectionist architectures in conjunction with modelling systems and on the determining of the optimal operating conditions of a new high precision industrial process known as laser milling. Laser milling is a relatively new micro-manufacturing technique in the production of high-value industrial components. The industrial problem is defined by a data set relayed through standard sensors situated on a laser-milling centre, which is a machine tool for manufacturing high-value micro-moulds, micro-dies and micro-tools. The new three-phase industrial system presented in this study is capable of identifying a model for the laser-milling process based on low-order models. The first two steps are based on the use of unsupervized connectionist models. The first step involves the analysis of the data sets that define each case study to identify if they are informative enough or if the experiments have to be performed again. In the second step, a feature selection phase is performed to determine the main variables to be processed in the third step. In this last step, the results of the study provide a model for a laser-milling procedure based on low-order models, such as black-box, in order to approximate the optimal form of the laser-milling process. The three-step model has been tested with real data obtained for three different materials: aluminium, cooper and hardened steel. These three materials are used in the manufacture of micro-moulds, micro-coolers and micro-dies, high-value tools for the medical and automotive industries among others. As the model inputs are standard data provided by the laser-milling centre, the industrial implementation of the model is immediate. Thus, this study demonstrates how a high precision industrial process can be improved using a combination of artificial intelligence and identification techniques",Optimizing the operating conditions in a high precision industrial process using soft computing techniques,,Wiley-Blackwell,,,core
380763802,2011-01-01T00:00:00,"The impact of the initial dissolved oxygen, fermentation temperature, wort concentration and yeast pitching rate on the major fermentation process responses were evaluated by full factorial design and statistical analysis by JMP 5.01 (SAS software) software. Fermentation trials were carried out in 2L-EBC tall tubes using an industrial lager brewing yeast strain. The yeast viability, ethanol production, apparent extract and real degree of fermentation were monitored. The results obtained demonstrate that very high gravity worts at 22°P can be fermented in the same period of time as a 15°P wort, by raising the temperature to 18°C, the oxygen level to about 22 ppm, and increasing the pitching rate to 22 × 106 cell/mL. When diluting to obtain an 11.5°P beer extract, the volumetric brewing capacity increased 91% for the 22°P wort fermentation and 30% using the 15°P wort. After dilution, the fermentation of the 22°P wort resulted in a beer with higher esters levels, primarily the compound ethyl acetate.(undefined",Comparing the impact of environmental factors during very high gravity brewing fermentations,,'Wiley',"[{'title': 'Journal of the Institute of Brewing', 'identifiers': ['2050-0416', 'issn:0046-9750', 'issn:2050-0416', '0046-9750']}]",10.1002/j.2050-0416.2011.tb00480.x,core
360547026,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake- holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished1. Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons, Chichester, England (2002). ISBN 0 47149691X

2. Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

3. Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 10–15. ACM Press (2006)

4. Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

5. Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

6. Zhang, Z., Thangarajah, J., Padgham, L.: Model based testing for agent systems. In: J. Filipe, B. Shishkov, M. Helfert, L. Maciaszek (eds.) Software and Data Technologies, Communications in Computer and Information Science, vol. 22, pp. 399–413. Springer, Berlin/Heidelberg (2009)

7. Ekinci, E.E., Tiryaki, A.M., Çetin, Ö., Dikenelli: Goal-oriented agent testing revisited. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 173–186. Springer, Berlin/Heidelberg (2009)

8. Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 199–212. Springer, Berlin/Heidelberg (2009)

9. Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 187–198. Springer, Berlin/Heidelberg (2009)

10. Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

11. Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

12. Erol, K., Hendler, J.A., Nau, D.S.: HTN planning: Complexity and expressivity. In: Proceedings of the 12th National Conference on Artificial Intelligence (AAAI), pp. 1123–1128. AAAI Press (1994)

13. de Silva, L., Padgham, L.: A comparison of BDI based real-time reasoning and HTN based planning. In: G. Webb, X. Yu (eds.) AI 2004: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3339, pp. 1167–1173. Springer, Berlin/Heidelberg (2004)

14. Erol, K., Hendler, J., Nau, D.: Complexity results for HTN planning. Annals of Mathematics and Artificial Intelligence 18(1), 69–93 (1996)

15. Paolucci, M., Shehory, O., Sycara, K.P., Kalp, D., Pannu, A.: A planning component for RETSINA agents. In: N.R. Jennings, Y. Lespérance (eds.) Intelligent Agents VI, Agent Theories, Architectures, and Languages (ATAL), 6th International Workshop, ATAL ’99, Orlando, Florida, USA, July 15-17, 1999, Proceedings, Lecture Notes in Computer Science, vol. 1757, pp. 147–161. Springer, Berlin/Heidelberg (2000)

16. Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

17. Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

18. d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, Lecture Notes in Artificial Intelligence, vol. 1365, pp. 155–176. Springer, Berlin/Heidelberg (1998)

19. Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

20. Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

21. Lee, J., Huber, M.J., Kenny, P.G., Durfee, E.H.: UM-PRS: An implementation of the procedural reasoning system for multirobot applications. In: Proceedings of the Conference on Intelligent Robotics in Field, Factory, Service, and Space (CIRFFSS’94), pp. 842–849 (1994)

22. Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

23. Morley, D., Myers, K.: The SPARK agent framework. In: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 714–721. IEEE Computer Society, Washington, DC, USA (2004)

24. Pokahr, A., Braubach, L., Lamersdorf, W.: Jadex: A BDI reasoning engine. In: R.H. Bordini, M. Dastani, J. Dix, A. El Fallah Seghrouchni (eds.) Multi-Agent Programming: Languages, Platforms and Applications, pp. 149–174. Springer (2005)

25. Bratman, M.E., Israel, D.J., Pollack, M.E.: Plans and resource-bounded practical reasoning. Computational Intelligence 4, 349–355 (1988)

26. Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), Lecture Notes in Artificial Intelligence, vol. 1038, pp. 42–55. Springer, Berlin/Heidelberg (1996)

27. Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

28. Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

29. Dastani, M.: 2APL: a practical agent programming language. Autonomous Agents and Multi-Agent Systems 16(3), 214–248 (2008)

30. Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemelä (eds.) Logic Programming, Lecture Notes in Computer Science, vol. 4670, pp. 302–316. Springer, Berlin/Heidelberg (2007)

31. Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http: //www.math.upenn.edu/∼wilf/gfology2.pdf

32. Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

33. Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 37–44. IFAAMAS (2008)

34. Dorigo, M., Stützle, T.: Ant Colony Optimization. MIT Press (2004). ISBN 0-262-04219-3

35. van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

36. Thangarajah, J., Winikoff, M., Padgham, L., Fischer, K.: Avoiding resource conflicts in intelligent agents. In: F. van Harmelen (ed.) Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 18–22. IOS Press (2002)

37. Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: Proceedings of the Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

38. Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: Future of Software Engineering 2007, pp. 120–136. IEEE Computer Society, Los Alamitos, CA (2007)

39. Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

40. Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

41. Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007)

42. Burch, J., Clarke, E., McMillan, K., Dill, D., Hwang, J.: Symbolic model checking: 1020 states and beyond. Information and Computation 98(2), 142–170 (1992)

43. Fix, L., Grumberg, O., Heyman, A., Heyman, T., Schuster, A.: Verifying very large industrial circuits using 100 processes and beyond. In: D. Peled, Y.K. Tsay (eds.) Automated Technology for Verification and Analysis, Lecture Notes in Computer Science, vol. 3707, pp. 11–25. Springer, Berlin/Heidelberg (2005",On the testability of BDI agent systems,,'University of Otago Library',,,core
26046753,2011-07-01T00:00:00Z,"A major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real world.  The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e. cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources – expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex (LPFC and ACC) reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human-robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to cheating by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real world",Robot cognitive control with a neurophysiologically inspired reinforcement learning model,,Frontiers Media S.A.,"[{'title': None, 'identifiers': ['issn:1662-5218', '1662-5218']}]",10.3389/fnbot.2011.00001/full,core
188077043,2011-01-01T08:00:00,"Beef tenderness is an important palatability trait and is related to consumer satisfaction. In this dissertation, hyperspectral imaging (HSI) was implemented and tested for beef tenderness assessment. An acousto-optic tunable filter (AOTF)-based and a spectrograph-based portable HSI system (wavelength range: 400 nm to 1000 nm) were developed. These systems were used to acquire hyperspectral images of fresh beef ribeye muscle (longissimus dorsi) on hanging beef carcasses at 2-day postmortem in multiple commercial beef packing plants. The spectral dimension of the beef images was reduced, image features were extracted, and discriminant models were developed to forecast 14-day aged, cooked beef tenderness. Four different spectral dimensionality reduction methods (sample, chemometric, and mosaic principal component analyses, and partial least squares regression), seven different image feature sets (descriptive statistical features, wavelet features, gray level co-occurrence matrix features, Gabor transform features, Laws features, local binary pattern features, and pooled features), and three different discriminant models (Fisher\u27s linear discriminant analysis, support vector machines, and decision tree) were evaluated. A third-party true validation was conducted to evaluate the performance of the HSI systems. A new evaluation metric, called Accuracy Index (AI), was developed and used to compare beef tenderness prediction models. The AOTF system provided an AI value and accuracies for tender certification, tender classification, and tough classification of 70%, 91.7%, 84.1%, and 56.9%, respectively. The corresponding metrics for the spectrograph system were 66.8%, 86.7%, 65.8%, and 63.6%, respectively. In addition, a multispectral imaging approach was implemented by identifying and analyzing five key wavelengths. This approach provided an AI value and accuracies for tender certification, tender classification, and tough classification of 67.8%, 87.5%, 62%, and 68.2%, respectively, and took only eight seconds to assign a tenderness score for a beef carcass. Both hyperspectral and multispectral imaging show outstanding potential for real-time beef tenderness assessment. The successful adoption of this technology will lead to the development of a value-based system that benefits both consumers and the beef industry. Keywords: Beef tenderness, hyperspectral imaging, multispectral imaging, principal component analysis, and textural features",Development and evaluation of spectral imaging systems and algorithms for beef tenderness grading,,DigitalCommons@University of Nebraska - Lincoln,,,core
29047200,2014-06-01T00:00:00,"Design optimisation of electromagnetic and electromechanical devices is usually aided by numerical simulations, such as the finite element method, which often carry high computational costs, especially if three-dimensional transient modelling is required. Thus in addition to the task of finding the global optimum, while avoiding local minima traps, there is the additional requirement of achieving the final solution efficiently with as few objective function evaluations as possible. With this in mind several surrogate modelling techniques have been developed to replace, under controlled environment, the computationally expensive accurate field modelling by fast approximate substitutes. This thesis looks at a particular technique known as kriging, which in other applications has been demonstrated to provide accurate representations, even of complicated responses, based on a limited set of observations whilst providing an error estimate of the predictions and hence increasing the confidence in the answer. In the iterative optimisation process the critical issue is where to position the next point for evaluation to find a sensible compromise between conflicting goals to explore the search space thoroughly but at the same time exploit information already available. This thesis proposes several novel algorithms based on reinforcement learning theory using the concept of rewards for balancing exploration and exploitation automatically and adaptively. The performance of these algorithms has been assessed comprehensively using carefully selected test functions and real engineering problems (taken from TEAM workshops) and compared with the results published in literature. The kriging approach has generally been found to outperform significantly other available methods. One of the practical limitations, however, was found to be large-scale multi-dimensional or multi-objective tasks because of the need to create special correlation matrices for the kriging predictions to work. Several techniques have been developed and implemented to alleviate such problems and control the memory space occupied by such matrices. Finally, in practical design problems, the issue of robustness of the design has to be considered – related to manufacturing tolerances, material variability, etc – which requires the designer not only to find the theoretical optimum but also assess its quality (sensitivity) within specified uncertainties of variables. Several strategies for evaluation of design robustness assisted by kriging modelling have been developed and implemented in combination with commercial electromagnetic design software",Balancing exploration and exploitation in robust multiobjective electromagnetic design optimisation,,,,,core
9840187,2011-01-01T00:00:00,"It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presente",Robot self-preservation and adaptation to user preferences in game play : a preliminary study,,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/ROBIO.2011.6181679,core
378089883,2014-12-01T00:00:00,"Rails are key elements in railway superstructure since these elements receive directly the train load transmitted by the wheels. Simultaneously, rails must provide effective stress transference to the rest of the track elements. This track element often deteriorates as a consequence of the vehicle passing or manufacturing imperfections that cause in rail several defects. Among these rail defects, transverse cracks highlights and are considered a severe pathology because they can suddenly trigger the rail failure. This study is focused on UIC-60 rails with transverse cracks. A 3-D FEM model is developed in ANSYS for the flawless rail in which conditions simulating the crack presence are implemented. To account for the inertia loss of the rail as a consequence of the cracking, a reduction of the bending stiffness of the rail is considered. The numerical models have been calibrated using the first four bending vibration modes in terms of frequencies. These vibration frequencies have been obtained using the Experimental Modal Analysis technique, studying the changes in the modal parameters of the rails induced by the crack and comparing the results obtained by the model with experimental results. Finally, the calibrated and validated models for the single rail have been implemented in a complete railway ballasted track FEM model in order to study the static influence of the cracks on the rail deflection caused by a load passingMontalban Domingo, ML.; Baydal Giner, B.; Zamorano Martín, C.; Real Herráiz, JI. (2014). Experimental Modal Analysis of transverse-cracked rails. Influence of the cracks on the real track behavior. Structural engineering and mechanics. 52(5):1019-1032. doi:10.12989/sem.2014.52.5.1019S1019103252",Experimental Modal Analysis of transverse-cracked rails. Influence of the cracks on the real track behavior,,'Techno-Press',,10.12989/sem.2014.52.5.1019,core
20261237,2012-01-01T00:00:00,"Whey disposal is one of the main problems for dairy industry due to its content of lactose. An interesting way to upgrade this waste is its use as substrate for fermentation using lactic acid bacteria (LAB). Further improvement could derive from the use of probiotic LAB, in order to obtain a cell biomass available for other applications. This study aimed the development of a fermentation control system based on NIRS, for a fast and accurate process monitoring. 
Whey powder was reconstituted at 7% with distilled water to reach the final lactose concentration of 4,5%. After sterilization, whey was inoculated with strains of Lactobacillus plantarum (1%), and incubated at 30\ub0C \ub1 1\ub0C for 48 h. Samples were collected at 0, 6, 8, 11, 15, 18, 24, 28, 32 and 48 h. Lactose (g/100g) and lactic acid (g/100g) contents were determined by HPLC, and microbial count determined in MRS agar (log CFU/mL). Spectra were collected in trasflectance mode with a NIRFlex N-500 spectrometer and controlled by the NIRWare 1.2 Operator software (BUCHI Italy, Assago, Milan) using an optic probe (pathlength = 0.08 mm) in the whole NIR range (1100-2500 nm). Data were processed by the Unscrambler software v.9.2 (Camo Inc., OSLO, Norway) and calibrations were built up applying partial least squares regression (PLSR) method. 
Preliminary results, obtained by PLSR, were satisfactory for all parameters, after pre-processing data with smoothing. Good NIR prediction ability in validation was highlighted, for each assessed parameter: for lactose (Rval=0.9133, RMSEP=0.1505); for lactic acid (Rval=0.9335, RMSEP=0.1404) and finally for microbial count (Rval=0.8644, RMSEP=0.2676). Based on these statistical indexes, these predictions are in agreement with reference data. 
The evaluation of statistical parameters obtained in this work, for the determination of lactose, lactic acid and microbial count indices, can confirm the suitability of NIR for their direct quantification in whey. 
The use of NIR probes could allow a complete process control in real time with benefits in terms of possibilities for intervention, affecting also positively on the waste disposal",The use of NIR spectroscopy for monitoring milk-whey biotransformation processes using,,place:Cape Town,,,core
33547986,2014-03-07T00:00:00,"Dissertação (mestrado)—Universidade de Brasília, Faculdade de Tecnologia,
Departamento de Engenharia Mecânica, 2014.Novas metodologias que possibilitem a redução do tempo e custo na soldagem, além da melhoria da qualidade das soldas são de grande interesse para o meio científico e industrial. Ao longo dos anos, a necessidade de melhorar a produtividade e qualidade levou ao desenvolvimento e aperfeiçoamento de técnicas e sistemas automatizados para o monitoramento e controle dos processos de soldagem. No desenvolvimento das pesquisas relacionadas ao monitoramento e controle em tempo real da soldagem, observa-se a utilização de várias abordagens direcionadas à aplicação e melhor entendimento do monitoramento direto ou indireto da geometria do cordão de solda. Este trabalho apresenta uma metodologia para modelagem, otimização e controle da altura do reforço e da largura do cordão de solda, permitindo ajustar os parâmetros do processo em tempo real. Desenvolveu-se para cada parâmetro geométrico estudado um sistema integrado de aquisição de imagens, modelagem e controle do processo de soldagem, permitindo uma resposta em tempo real, através de redes neurais artificiais, em que os parâmetros de velocidade de soldagem, velocidade de alimentação do arame e a tensão são preditos em função de uma altura do reforço ou largura desejada. Projetou-se um controlador “fuzzy” para realizar o controle em malha fechada do sistema, onde se toma como referência a diferença entre o valor desejado do parâmetro geométrico que se pretende controlar e o valor real do parâmetro no cordão de solda. Este cordão é medido através de um sistema de aquisição e processamento de imagens usando-se uma câmera web de baixo custo. As ações de controle são exercidas preferencialmente na velocidade de alimentação do arame para a altura do reforço e na velocidade de soldagem para a largura do cordão, parâmetros escolhidos por apresentar a maior influência em cada um dos parâmetros geométricos do cordão estudados respectivamente. Obtiveram-se satisfatoriamente cordões de solda com altura do reforço e largura pré-definida, além de boa aparência e qualidade.         _______________________________________________________________________________________ ABSTRACTNew methodologies that make possible the reduction of time and cost in welding, besides the improvement of the welding bead quality have shown to be the great interest for scientists and industries. Along the years, the needs of improving the productivity and quality carry out to the development and improvement of techniques and automated systems for monitoring and control of welding processes. The use of several approaches in researches related to the monitoring and control in real time of the welding is addressed to the application and understanding of direct or indirect monitoring of weld bead geometry. This work presents a methodology for modeling, optimization and control of the weld height reinforcement and bead width, allowing adjusting the parameters of the process in real time. It was developed for each welding geometric parameter an integrated system of images acquisition, modeling and control of the welding process, allowing a response in real time, through artificial neural networks, where the parameters of welding speed, wire feed speed and the voltage are predicted in function of the desired height reinforcement or bead width. It was designed a controller ""fuzzy"" to accomplish the control of the system in closed loop, where it is taken as reference the difference between the desired value of the geometric parameter to be controlled and the actual value of the parameter in the weld bead produced. This weld bead is measured through an acquisition system and processing of images using a low cost web cam. The control actions are preferentially implemented in the wire feed speed for height reinforcement and in welding speed for bead width, parameters chosen for presenting the most influence in each one of the geometric parameters studied respectively. It was obtained weld bead with acceptably height reinforcement and predefined width, and good appearance and quality",Uma metodologia para modelagem e controle da altura do reforço e da largura do cordão de solda no processo GMAW,https://core.ac.uk/download/33547986.pdf,,,,core
45133740,2012,"Traditional methods for bacterial identification include Gram staining, culturing, and biochemical assays for phenotypic characterization of the causative organism. These methods can be time-consuming because they require in vitro cultivation of the microorganisms. Recently, however, it has become possible to obtain chemical profiles for lipids, peptides, and proteins that are present in an intact organism, particularly now that new developments have been made for the efficient ionization of biomolecules. MS has therefore become the state-of-the-art technology for microorganism identification in microbiological clinical diagnosis. Here, we introduce an innovative sample preparation method for nonculture-based identification of bacteria in milk. The technique detects characteristic profiles of intact proteins (mostly ribosomal) with the recently introduced MALDI SepsityperTM Kit followed by MALDI-MS. In combination with a dedicated bioinformatics software tool for databank matching, the method allows for almost real-time and reliable genus and species identification. We demonstrate the sensitivity of this protocol by experimentally contaminating pasteurized and homogenized whole milk samples with bacterial loads of 10(3)-10(8) colony-forming units (cfu) of laboratory strains of Escherichia coli, Enterococcus faecalis, and Staphylococcus aureus. For milk samples contaminated with a lower bacterial load (104 cfu mL-1), bacterial identification could be performed after initial incubation at 37 degrees C for 4 h. The sensitivity of the method may be influenced by the bacterial species and count, and therefore, it must be optimized for the specific application. The proposed use of protein markers for nonculture-based bacterial identification allows for high-throughput detection of pathogens present in milk samples. This method could therefore be useful in the veterinary practice and in the dairy industry, such as for the diagnosis of subclinical mastitis and for the sanitary monitoring of raw and processed milk products",Nonculture-based identification of bacteria in milk by protein fingerprinting,,WILEY-BLACKWELL,,10.1002/pmic.201200053,core
26223284,2011-09-01T00:00:00Z,"This paper proposes an intelligent evolutionary algorithm that can be applied in the design of optimal automation systems, and employs a multimodal six-bar mechanism optimization design, job shop production scheduling for the fishing equipment industry, and dynamic real-time production scheduling system design cases to show how the technique developed in this paper is highly effective at resolving optimal automation system design problems. Major breakthroughs in artificial intelligence continue to be made in the wake of advanced information technology developments, and the field of intelligent evolutionary algorithms has attracted a particularly large amount of attention from researchers and users in the artificial intelligence community. The successful optimization of automation system design requires interdisciplinary integration, and further requires the use of actual cases, verification, and improvement to ensure implementation in real-world applications",Applications of Intelligent Evolutionary Algorithms in Optimal Automation System Design,,Chinese Institute of Automation Engineers (CIAE) & Taiwan Smart Living Space Association (SMART LISA),"[{'title': None, 'identifiers': ['issn:2223-9766', '2223-9766']}]",10.5875/ausmt.v1i1.100,core
55762097,2014-01-01T00:00:00,"Because of tight power and energy constraints, industry is progressively shifting toward heterogeneous system-on-chip (SoC) architectures composed of a mix of general-purpose cores along with a number of accelerators. However, such SoC architectures can be very challenging to efficiently program for the vast majority of programmers, due to numerous programming approaches and languages. Libraries, on the other hand, provide a simple way to let programmers take advantage of complex architectures, which does not require programmers to acquire new accelerator-specific or domain-specific languages. Increasingly, library-based, also called algorithm-centric, programming approaches propose to generalize the usage of libraries and to compose programs around these libraries, instead of using libraries as mere complements.

In this article, we present a software framework for achieving performance portability by leveraging a generalized library-based approach. Inspired by the notion of a component, as employed in software engineering and HW/SW codesign, we advocate nonexpert programmers to write simple wrapper code around existing libraries to provide simple but necessary semantic information to the runtime. To achieve performance portability, the runtime employs machine learning (simulated annealing) to select the most appropriate accelerator and its parameters for a given algorithm. This selection factors in the possibly complex composition of algorithms used in the application, the communication among the various accelerators, and the tradeoff between different objectives (i.e., accuracy, performance, and energy).

Using a set of benchmarks run on a real heterogeneous SoC composed of a multicore processor and a GPU, we show that the runtime overhead is fairly small at 5.1% for the GPU and 6.4% for the multi-core. We then apply our accelerator selection approach to a simulated SoC platform containing multiple inexact accelerators. We show that accelerator selection together with hardware parameter tuning achieves an average 46.2% energy reduction and a speedup of 2.1× while meeting the desired application error target",Performance portability across heterogeneous SoCs using a generalized library-based approach,,'Association for Computing Machinery (ACM)',,10.1145/2608253,core
56688141,2011-01-01T08:00:00,"Trucking industry, the business of transporting products via trucks, is vital to the health of our economy for the sheer number of people it employs, the value of product it hauls, and the diversity of dispatching models it uses. One of these dispatching models is the spot dispatching. This form of dispatching went through a recent transformation as a result of the industry deregulation in the 80s and the emergence of the internet. The deregulation allowed easier establishment of new trucking companies and their access to the entire market; the internet allowed for spot freight to be posted on the on-line hosting sites where shippers, brokers and truckers can post their service.  This change, however, brings with it challenges and opportunities which is the focus of this dissertation.
In this dissertation, we give a broad introduction of the freight spot-market; we identify the challenges and the opportunities. Spot-market dispatching problem is formulated as a dynamic assignment problem, implemented as a Markov Decision Process (MDP), which has its objective as maximizing the operation profit at the end of the dispatching planning horizon. A freight spot-market loads generation platform is created to mimic the dynamics of trucks and loads in such markets. Platform allows generation of data representing different market settings. Approximate dynamic programming methods are proposed to solve the dispatching problem. To address the curse of MDP state-space dimensionality for real-world settings, Neural Networks were used to approximate the value function. We benchmark our methods with local and myopic policies typically used be dispatchers in the business. Load hosting sites offer information about available loads and trucks across the market. We also explore the use of such information into dispatching policies and study its effect on the overall performance",Trucking: novel spot-market dispatching models,,DigitalCommons@WayneState,,,core
275607597,2014-12-01T00:00:00,"Rails are key elements in railway superstructure since these elements receive directly the train load transmitted by the wheels. Simultaneously, rails must provide effective stress transference to the rest of the track elements. This track element often deteriorates as a consequence of the vehicle passing or manufacturing imperfections that cause in rail several defects. Among these rail defects, transverse cracks highlights and are considered a severe pathology because they can suddenly trigger the rail failure. This study is focused on UIC-60 rails with transverse cracks. A 3-D FEM model is developed in ANSYS for the flawless rail in which conditions simulating the crack presence are implemented. To account for the inertia loss of the rail as a consequence of the cracking, a reduction of the bending stiffness of the rail is considered. The numerical models have been calibrated using the first four bending vibration modes in terms of frequencies. These vibration frequencies have been obtained using the Experimental Modal Analysis technique, studying the changes in the modal parameters of the rails induced by the crack and comparing the results obtained by the model with experimental results. Finally, the calibrated and validated models for the single rail have been implemented in a complete railway ballasted track FEM model in order to study the static influence of the cracks on the rail deflection caused by a load passingMontalban Domingo, ML.; Baydal Giner, B.; Zamorano Martín, C.; Real Herráiz, JI. (2014). Experimental Modal Analysis of transverse-cracked rails. Influence of the cracks on the real track behavior. Structural engineering and mechanics. 52(5):1019-1032. doi:10.12989/sem.2014.52.5.1019S1019103252",Experimental Modal Analysis of transverse-cracked rails. Influence of the cracks on the real track behavior,http://hdl.handle.net/10251/57689,'Techno-Press',,10.12989/sem.2014.52.5.1019,core
226583803,2011-01-01T00:00:00,"Microporous membranes are an attractive alternative to circumvent the typical drawbacks associated to bead-based chromatography. In particular, the present work intends to evaluate different affinity membranes for antibody capture, to be used as an alternative to Protein A resins. To this aim, two Mimetic LigandsTM A2P and B14, were coupled onto different epoxide and azide group activated membrane supports using different spacer arms and immobilization chemistries. The spacer chemistries investigated were 1,2-diaminoethane (2LP), 3,6-dioxa-1,8-octanedithiol (DES) and [1,2,3] triazole [TRZ]. These new mimetic membrane materials were investigated by static and by dynamic binding capacity studies, using pure polyclonal human immunoglobulin G (h-IgG) solutions as well as a real cell culture supernatant containing monoclonal h-IgG1. The best results were obtained by combining the new B14 ligand with a TRZ-spacer and an improved Epoxy 2 membrane support material. The new B14-TRZ-Epoxy 2 membrane adsorbent provided binding capacities of approximately 3.1 mg/mL, besides i) a good selectivity towards IgG, ii), high IgG recoveries of above 90%, iii) a high Pluronic-F68 tolerance and iv) no B14-ligand leakage under harsh cleaning-in-place conditions (0.6 M sodium hydroxide). Furthermore, foreseeable improvements in binding capacity will promote the implementation of membrane adsorbers in antibody manufacturing",Influence of different spacer arms on Mimetic Ligand\u2122 A2P and B14 membranes for human IgG purification,,'Elsevier BV',,10.1016/j.jchromb.2011.03.059,core
33269275,2014-01-01T00:00:00,"U radu je dano istraživanje ABC klasifikacije zaliha koristeći različite višekriterijske metode (AHP metoda i klaster analiza) te neuronske mreže. Za definirani realni podatkovni model zaliha i prethodno postavljeni model ABC analize, istražene su i primjene navedenih metoda u klasifikaciji zaliha. Kroz ostvarene rezultate primijenjenih metoda, procijenjene su mogućnosti njihova korištenja u realnom proizvodnom okruženju. Provedena istraživanja u ovom radu stvaraju dobru pretpostavku za bolje upravljanje zalihama te implementaciju rezultata u modulu zaliha ERP sustava.The work presents a research on inventory ABC classification using various multi-criteria methods (AHP method and cluster analysis) and neural networks. For the real inventory sample data and previously conducted traditional ABC analysis the applications of the mentioned methods in inventory classification have also been investigated. The applied methods’ obtained results have been used to evaluate their usage possibilities in real manufacturing environment. The investigations carried out in the present work create real conditions for a better inventory control and implementation of the results in the ERP system inventory module","Inventory classification using multi-criteria ABC analysis, neural networks and cluster analysis",https://core.ac.uk/download/33269275.pdf,"Faculty of Mechanical Engineering in Slavonski Brod; Faculty of Electrical Engineering, Computer Science and Information Technology Osijek; Faculty of Civil Engineering in Osijek",,,core
37446517,2011,"Objetivou-se com este trabalho avaliar a precisão das redes neurais artificiais (RNA) na estimativa das redes neurais artificiais (RNA) na predição de índices zootécnicos, com base em variáveis térmicas e fisiológicas de porcas gestantes. A pesquisa foi realizada entre janeiro e abril de 2005 em uma propriedade de produção industrial de suínos, no setor de gestação, com 27 matrizes primíparas, alojadas em baias individuais e posteriormente na maternidade em baias de parição, onde foram quantificados os índices de produção dos leitões provenientes do estudo. Para tanto, foi implementada uma RNA backpropagation, com uma camada de entrada, uma oculta e uma camada de saída com funções de transferência tangente sigmoidal. A temperatura do ar e a frequência respiratória foram consideradas variáveis de entrada e o peso ao nascimento dos leitões e número de leitões mumificados, como variáveis de saída. A rede treinada apresentou ótimo poder de generalização, o que possibilitou a predição das variáveis-respostas. A caracterização do ambiente da gestação e maternidade foi adequada se comparada aos dados reais, com poucas tendências de sub ou superestimação de alguns valores. A utilização desse sistema especialista para a previsão dos índices zootécnicos é viável, pois o sistema tem bom desempenho para esta aplicação.The objective of this work was to evaluate the precision of Artificial Neural Networks (ANNs) to estimate zootechnical indexes, based on thermal and physiological variables of pregnant sows. This study was carried out from January to April 2005, in a swine industrial production farm in the gestation section with 27 primiparous gilts, allocated in individual pens and after on farrowing pens where it was quantified animal production indexes of piglets from the study. Therefore, an ANN backpropagation was implemented, with one input layer, one hidden layer, and one output layer with tangent sigmoidal transference functions. Air temperature and respiratory frequency were considered as input variables and weight of piglet at birth and the number of mummified piglets as output variables. The trained ANN presented a great generalization power, which enabled the prediction of the answer-variables. Characterization of the environment of gestation and maternity was appropriated if compared to the real data, with few under or overestimated tendencies of some values. The use of this specialist system to predict zootechnical indexes is viable because the system shows a good performance for this use",Use of artificial neural networks on the prediction of zootechnical indexes on gestation and farrowing stages of swines,https://core.ac.uk/download/pdf/37446517.pdf,Sociedade Brasileira de Zootecnia,"[{'title': None, 'identifiers': ['1516-3598', 'issn:1516-3598']}]",10.1590/S1516-35982011000300028,core
22963097,2013-08-20,"Second Life is a 3D virtual environment that is created by Linden Labs. It is currently continuing to grow and is widely used all over the world. It has already been adopted by organisations such as IBM, Sun Microsystems, ABC, Sony, and many others. The Sony AIBO (short for Artificial Intelligence roBOt) is an artificial intelligence robotic pet dog designed and manufactured by Sony. The AIBO is considered to be an autonomous robot, which is able to gain information about the environment and be able to function without human intervention. The main aim of this project is to use the Second Life online virtual space (virtual world) to simulate and control the movements of the Sony AIBO robot (real world) in a wireless environment using sound software engineering principles. This paper details the design of the teleoperation system and the rationale behind the design as well as proving that the aim of the project can be successfully met. The outcomes of the experiments performed to measure the success of the system are detailed in this paper. These outcomes include how successful the system was in terms of teleoperating the robot using the Second Life virtual world as well as some of the issues faced while performing the experiments and a dicussion of the possible solutions on how to solve the problems. This project could lead to future work in various application domains such as the smart home concept or in the mining industry",Connecting the Real World with the Virtual World,,,,,core
2210777,2011-07-27T00:00:00,"Automating the design of heuristic search methods is an active research field
within computer science, artificial intelligence and operational research. In
order to make these methods more generally applicable, it is important to
eliminate or reduce the role of the human expert in the process of designing an
effective methodology to solve a given computational search problem.
Researchers developing such methodologies are often constrained on the number
of problem domains on which to test their adaptive, self-configuring
algorithms; which can be explained by the inherent difficulty of implementing
their corresponding domain specific software components.
  This paper presents HyFlex, a software framework for the development of
cross-domain search methodologies. The framework features a common software
interface for dealing with different combinatorial optimisation problems, and
provides the algorithm components that are problem specific. In this way, the
algorithm designer does not require a detailed knowledge the problem domains,
and thus can concentrate his/her efforts in designing adaptive general-purpose
heuristic search algorithms. Four hard combinatorial problems are fully
implemented (maximum satisfiability, one dimensional bin packing, permutation
flow shop and personnel scheduling), each containing a varied set of instance
data (including real-world industrial applications) and an extensive set of
problem specific heuristics and search operators. The framework forms the basis
for the first International Cross-domain Heuristic Search Challenge (CHeSC),
and it is currently in use by the international research community. In summary,
HyFlex represents a valuable new benchmark of heuristic search generality, with
which adaptive cross-domain algorithms are being easily developed, and reliably
compared.Comment: 28 pages, 9 figure",HyFlex: A Benchmark Framework for Cross-domain Heuristic Search,http://arxiv.org/abs/1107.5462,,,,core
76692113,2014-02-28T17:00:00,"Please join us for a panel covering all aspects of patent assertion and non-practicing entities and their effect on the patent industry. Our distinguished panelists are as follows:
Michael D. Friedman is Managing Director at Ocean Tomo, overseeing its Investments practice, which is composed of Investment Banking, Asset Management and Investment Research.
Ocean Tomo’s Investment Banking practice brings IP financing, monetization and capital markets solutions to corporations and other intellectual property owners. Recent notable transactions include the leveraged buyout of Mosaid Technologies and the sale of MIPS Technologies’ IP portfolio. Ocean Tomo Asset Management, where Mr. Friedman serves as Chief Investment Officer, engages in public equity, special situations and private equity investing where intellectual property insight drives alpha creation. Investment Research works in parallel with institutional investors, hedge funds and private equity funds advising them on capital allocations to IP-themed investments.
Mr. Friedman holds a JD from the University of Chicago Law School, where he worked as Research Editor of the University of Chicago Legal Forum. He also holds a BS in marine engineering and nautical science from the U.S. Merchant Marine Academy. Mr. Friedman is a member of the board of directors of the Intellectual Property Exchange International, the world’s first IP-focused financial exchange, and a Lecturer in Law at the University of Chicago Law School.
Jay P. Kesan is a Professor at the University of Illinois, College of Lawwhere he is H. Ross & Helen Workman Research Scholar and Director of the Program in Intellectual Property and Technology Law. Professor Kesan received his J.D. summa cum laude from Georgetown University, where he received several awards including Order of the Coif and served as associate editor of the Georgetown Law Journal. After graduation, he clerked for Judge Patrick E. Higginbotham of the United States Court of Appeals for the Fifth Circuit. Prior to attending law school, Jay Kesan – who also holds a Ph.D. in electrical and computer engineering from the University of Texas at Austin – worked as a research scientist at the IBM T.J. Watson Research Center in New York. He is a registered patent attorney and practiced at the former firm of Pennie & Edmonds LLP in the areas of patent litigation and patent prosecution. In addition, he has published numerous scientific papers, and he has obtained several patents in the U.S. and abroad. His recent publications can be found on SSRN (Social Science Research Network) at http://www.ssrn.com. At the University of Illinois, Professor Kesan is appointed in the College of Law, the Institute of Genomic Biology, the Department of Electrical & Computer Engineering, the Information Trust Institute, the Coordinated Science Laboratory, the College of Business, and the Department of Agricultural & Consumer Economics. Professor Kesan continues to be professionally active in the areas of patent litigation and technology entrepreneurship. He has served as a special master in patent litigations, and has served as a technical and legal expert and/or counsel in patent matters. He also serves on the boards of directors/advisors of start-up technology companies.
Matthew Levy is Patent Counsel at the Computer and Communications Industry Association, where he handles legal, policy advocacy, and regulatory matters related to patents and is lead blogger for CCIA’s Patent Progress.
Matt joined the CCIA in 2013 from the IP boutique Cloudigy Law, PLLC. He has also been an associate at Finnegan, Henderson, Farabow, Garrett, & Dunner, LLP and at Hogan & Hartson LLP. He got first-hand experience in both patent prosecution and patent litigation, including defending clients against patent trolls.
Matt graduated from the Georgetown University Law Center magna cum laude with the Order of the Coif, winning the ABA/BNA Award for Excellence in Intellectual Property. He received a Master’s in Computer Science from the University of Kentucky, where he won the Presidential Fellowship twice. His research at UK was in computational complexity theory and artificial intelligence. He received a Bachelor’s degree in Computer Science from the University of Southern Maine.
Before law school, Matt was a software engineer at IBM in Lexington, KY, as part of the team that developed and maintained Lotus Sametime, IBM’s real-time messaging and conferencing product. He is co-inventor on U.S. Patent No. 8,521,830.
Matt is still a software developer in his spare time. He developed an app for the iPad, Federal Local Rules, which is available on the App Store.
Laura Beth Miller is a shareholder at Brinks Gilson & Lione, where she co-chairs the firm’s practice before the U.S. International Trade Commission (“ITC”). With over two decades of trial and arbitration experience, Ms. Miller has handled substantial first and second chair responsibilities. She focuses her practice on patent, trade secret and trademark issues, as well as client counseling on complex commercial issues, including licensing, anti-trust and contract issues affecting business operations, product services and technology. In addition to representing major Fortune 500 companies, she is an adjunct professor at The John Marshall Law School, in Chicago, Illinois. She is a frequent speaker on intellectual property issues both in the United States and abroad, and has written a number of articles on intellectual property topics.
Ms. Miller received her B.A. from the University of Virginia and her J.D. from The College of William and Mary Marshall Wythe School of Law. She is licensed to practice before the United States Supreme Court, the Supreme Court of Illinois, the United States Patent and Trademark Office, and numerous federal courts. She has been recognized as one of Illinois\u27 leading intellectual property lawyers by Chambers USA, and has been named a Leading Intellectual Property Lawyer and one of the Top 50 Women Business Litigation Lawyers in Illinois by the Leading Lawyers Network. She also serves on the management teams at Brinks Gilson & Lione.
K. McNeill Taylor, Jr., is General Counsel at Round Rock Research, LLC. Neill Taylor joined Round Rock in 2012 as Vice President Law and General Counsel responsible for supervising and administering all legal affairs for the company.
Before joining Round Rock, Neill was Corporate Vice President and Chief IP Counsel of Motorola Mobility, Inc., responsible for the intellectual property law and litigation functions. He managed the offensive and defensive patent litigation in support of MMI’s Android smart phones, and the preparation, prosecution and legal support for MMI’s patent portfolio of approximately 24,000 patents and applications worldwide. Neill had a leading role in setting the strategy for and negotiating MMI’s $12.5B acquisition agreement with Google in August 2011.
After joining Motorola, Inc. in 2002, Neill led its efforts to protect intellectual property for a number of Motorola businesses through patent operations, licensing, in-business counseling, defensive matters and litigation. He also served as general counsel in the integration of Symbol Technologies, Inc. after its $4B acquisition by Motorola in January 2007.
Prior to joining Motorola, Neill served as vice president, general counsel and assistant secretary of Corning Cable Systems and Siecor Corp. Before that he held patent counsel positions with Corning Inc. and Schlumberger Ltd., and began his patent law career as an associate with Fish & Neave, a patent litigation firm in New York.
Neill received his law degree from the University of Chicago and a bachelor’s degree in physics and philosophy from Duke University, where he graduated magna cum laude and was an Angier B. Duke scholar.
Andrew W. Williams is a partner with McDonnell Boehnen Hulbert & Berghoff LLP. Dr. Williams\u27 practice primarily consists of patent litigation, prosecution, and opinion work in the areas of biotechnology, pharmaceuticals, and chemistry. Dr. Williams is a contributing author to the Patent Docs weblog, a site focusing on biotechnology and pharmaceutical patent law. Dr. Williams earned his Ph.D. in Molecular Biophysics and Biochemistry at Yale University. He earned his J.D. from George Washington University Law School with highest honors, and was Managing Editor of the law review",Patent Assertion and Non-Practicing Entities Panel,,Northwestern Pritzker School of Law Scholarly Commons,,,core
275593026,2013-06-01T00:00:00,"During the past years, evolutionary testing research has reported encouraging results for automated functional (i.e. black-box) testing. However, despite promising results, these techniques have hardly been applied to complex, real-world systems and as such, little is known about their scalability, applicability, and acceptability in industry. In this paper, we describe the empirical setup used to study the use of evolutionary functional testing in industry through two case studies, drawn from serial production development environments at Daimler and Berner & Mattner Systemtechnik, respectively. Results of the case studies are presented, and research questions are assessed based on them. In summary, the results indicate that evolutionary functional testing in an industrial setting is both scalable and applicable. However, the creation of fitness functions is time-consuming. Although in some cases, this is compensated by the results, it is still a significant factor preventing functional evolutionary testing from more widespread use in industry.This work is supported by EU grant IST-33472 (EvoTest). For their support and help, we would like to thank Mark Harman, Kiran Lakhotia and Youssef Hassoun from Kings College London; Marc Schoenauer and Luis da Costa from INRIA; Jochen Hansel from Fraunhofer FIRST; Dimitar Dimitrov and Ivaylo Spasov from RILA; and Dimitris Togias from European Dynamics.Vos ., TE.; Lindlar, FF.; Wilmes, B.; Windisch, A.; Baars, AI.; Kruse, PM.; Gross, H.... (2013). Evolutionary functional black-box testing in an industrial setting. Software Quality Journal. 21(2):259-288. doi:10.1007/s11219-012-9174-yS25928821",Evolutionary functional black-box testing in an industrial setting,http://hdl.handle.net/10251/45571,'Springer Science and Business Media LLC',,10.1007/s11219-012-9174-y,core
211485894,2013-01-01T00:00:00,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study",Applying soft computing techniques to optimise a dental milling process,,'Elsevier BV',,,core
211485941,2011-01-01T00:00:00,"A Manufacturing Execution System (MES) consists of high-cost, large-scale, multi-task software systems. Companies and factories apply these complex applications for the purposes of production management to monitor and track all aspects of factory-based manufacturing processes. Nevertheless, companies seek to control the production process with even greater rigour. Improvements associated with an MES involve the identification of new knowledge within the data set and its integration in the system, which implies a step forward to Business Process Management (BPM) systems, from which the users of an MES may gain relevant information, not only on execution procedures but to decide on the best scheduled arrangement. This work studies the data gathered from a real MES that is used in a plastic products factory. Several Artificial Intelligence and Soft Computing modelling methods based on fuzzy rules assist in the calculation of manufacturing costs and decisions over shift work rotas: two decisions that are of relevance for the improvement of the execution system. The results of the study, which identify the most suitable models to facilitate execution-related decision-making, are presented and discussed",Optimising operational costs using Soft Computing techniques,,'IOS Press',,,core
232140520,2013-05-01T07:00:00,"Fault Analysis is the detection and diagnosis of malfunction in machine operation or process control. Early fault analysis techniques were reserved for high critical plants such as nuclear or chemical industries where abnormal event prevention is given utmost importance. The techniques developed were a result of decades of technical research and models based on extensive characterization of equipment behavior. This requires in-depth knowledge of the system and expert analysis to apply these methods for the application at hand. Since machine learning algorithms depend on past process data for creating a system model, a generic autonomous diagnostic system can be developed which can be used for application in common industrial setups. In this thesis, we look into some of the techniques used for fault detection and diagnosis multi-class and one-class classifiers. First we study Feature Selection techniques and the classifier performance is analyzed against the number of selected features. The aim of feature selection is to reduce the impact of irrelevant variables and to reduce computation burden on the learning algorithm. We introduce the feature selection algorithms as a literature survey. Only few algorithms are implemented to obtain the results. Fault data from a Radio Frequency (RF) generator is used to perform fault detection and diagnosis. Comparison between continuous and discrete fault data is conducted for the Support Vector Machines (SVM) and Radial Basis Function Network (RBF) classifiers. In the second part we look into one-class classification techniques and their application to fault detection. One-class techniques were primarily developed to identify one class of objects from all other possible objects. Since all fault occurrences in a system cannot be simulated or recorded, one-class techniques help in identifying abnormal events. We introduce four one-class classifiers and analyze them using Receiver-Operating Characteristic (ROC) curve. We also develop a feature extraction method for the RF generator data which is used to obtain results for one-class classifiers and Radial Basis Function Network two class classification. To apply these techniques for real-time verification, the RIT Fault Prediction software is built. LabView environment is used to build a basic data management and fault detection using Radial Basis Function Network. This software is stand alone and acts as foundation for future implementations",Fault analysis using state-of-the-art classifiers,https://core.ac.uk/download/232140520.pdf,RIT Scholar Works,,,core
464689506,2014-01-01T00:00:00,"Artificial intelligence methodologies, as the core of discrete control and decision support systems, have been extensively applied in the industrial production sector. The resulting tools produce excellent results in certain cases; however, the NP-hard nature of many discrete control or decision making problems in the manufacturing area may require unaffordable computational resources, constrained by the limited available time required to obtain a solution. With the purpose of improving the efficiency of a control methodology for discrete systems, based on a simulation-based optimization and the Petri net (PN) model of the real discrete event dynamic system (DEDS), this paper presents a strategy, where a transformation applied to the model allows removing the redundant information to obtain a smaller model containing the same useful information. As a result, faster discrete optimizations can be implemented. This methodology is based on the use of a formalism belonging to the paradigm of the PN for describing DEDS, the disjunctive colored PN. Furthermore, the metaheuristic of genetic algorithms is applied to the search of the best solutions in the solution space. As an illustration of the methodology proposal, its performance is compared with the classic approach on a case study, obtaining faster the optimal solution. © 2014 Juan-Ignacio Latorre-Biel et al",Control of discrete event systems by means of discrete optimization and disjunctive colored PNs: Application to manufacturing facilities,,'Hindawi Limited',,10.1155/2014/821707,core
202005127,2012-12-31,"AbstractThis paper presents an overview of Proportional Integral control (PI) and Artificial Intelligent control (AI) algorithms. AI and PI controller are analyzed using Matlab [Simulink] software. The DC motor is an attractive piece of equipment in many industrial applications requiring variable speed and load characteristics due to its ease of controllability. The main objective of this paper illustrates how the speed of the DC motor can be controlled using different controllers. The simulation results demonstrate that the responses of DC motor with an AI control which is Fuzzy Logic Control shows satisfactory well damped control performance. The results shows that Industrial DC Motor model develop using its physical parameters and controlled with an AI controller give better response, it means it can used as a controller to the real time DC Moto",Modelling and Simulation for Industrial DC Motor Using Intelligent Control ,,Published by Elsevier Ltd.,,10.1016/j.proeng.2012.07.193,core
21901992,2012-06-15,"Career Objectives and Research Interests My long term career objective is to work at the intersection of basic research and real-world applications, where I believe I can have the largest impact. I am interested in a variety of topics regarding the application of AI planning and knowledge representation techniques to real-world problems. These include planning in highly dynamic, “open-world ” environments, integrated planning and execution, execution monitoring, diagnosis, the design of robust plans, the integration of domain-expert knowledge into planning and control, and the application of relevance-based, symbolic reasoning techniques to verification and control of deployed model-based systems. Applications I have worked on or am interested in include autonomous robots, workflows, semantic web, ubiquitous computing, design automation, and manufacturing analysis. Appointments Research Scientist",,,,,,core
22549884,2013-07-12,"Simulation is often used in research and industry as a low cost, high efficiency alternative to real model testing. Simulation has also been used to develop and test powerful learning algorithms. However, parameters learned in simulation often do not translate directly to the application, especially because heavy optimization in simulation has been observed to exploit the inevitable simulator simplifications, thus creating a gap between simulation and application that reduces the utility of learning in simulation. This paper introduces Grounded Simulation Learning (GSL), an iterative optimization framework for speeding up robot learning using an imperfect simulator. In GSL, a behavior is developed on a robot and then repeatedly: 1) the behavior is optimized in simulation; 2) the resulting behavior is tested on the real robot and compared to the expected results from simulation, and 3) the simulator is modified, using a machine-learning approach to come closer in line with reality. This approach is fully implemented and validated on the task of learning to walk using an Aldebaran Nao humanoid robot. Starting from a set of stable, hand-coded walk parameters, four iterations of this three-step optimization loop led to more than a 25 % increase in the robot’s walking speed",Humanoid Robots Learning to Walk Faster: From the Real World to Simulation and Back,,,,,core
22877537,2013-08-12,"Abstract--Power quality (PQ) monitoring is an important issue to electric utilities and many industrial power customers. This paper presents a DSP-based hardware monitoring system based on a recently proposed PQ classification algorithm. The algorithm is implemented with a Texas Instruments (TI) TMS320VC5416 digital signal processor (DSP) with the TI THS1206 12-bit 6 MSPS analog to digital converter. A TI TMS320VC5416 DSP Starter Kit (DSK) is used as the host board with the THS1206 mounted on a daughter card. The implemented PQ classification algorithm is composed of two processes: feature extraction and classification. The feature exaction projects a PQ signal onto a time-frequency representation (TFR), which is designed for maximizing the separability between classes. The classifiers include a Heavisidefunction linear classifier and neural networks with feedforward structures. The algorithm is optimized according to the architecture of the DSP to meet the hard real-time constraints of classifying a 5-cycle segment of the 60 Hz sinusoidal voltage/current signals in power systems. The classification output can be transmitted serially to an operator interface or control mechanism for logging and issue resolution",Real-Time Power Quality Waveform Recognition with a Programmable Digital Signal Processor,https://core.ac.uk/download/pdf/22877537.pdf,,,,core
44349026,2013-01-01T00:00:00,"There are many methods of stable controller design for nonlinear systems. In seeking to go beyond the minimum requirement of stability, Adaptive Dynamic Programming for Control approaches the challenging topic of optimal control for nonlinear systems using the tools of  adaptive dynamic programming (ADP). The range of systems treated is extensive; affine, switched, singularly perturbed and time-delay nonlinear systems are discussed as are the uses of neural networks and techniques of value and policy iteration. The text features three main aspects of ADP in which the methods proposed for stabilization and for tracking and games benefit from the incorporation of optimal control methods: • infinite-horizon control for which the difficulty of solving partial differential Hamilton–Jacobi–Bellman equations directly is overcome, and  proof provided that the iterative value function updating sequence converges to the infimum of all the value functions obtained by admissible control law sequences; • finite-horizon control, implemented in discrete-time nonlinear systems showing the reader how to obtain suboptimal control solutions within a fixed number of control steps and with results more easily applied in real systems than those usually gained from infinte-horizon control; • nonlinear games for which  a pair of mixed optimal policies are derived for solving games both when the saddle point does not exist, and, when it does, avoiding the existence conditions of the saddle point. Non-zero-sum games are studied in the context of a single network scheme in which policies are obtained guaranteeing system stability and minimizing the individual performance function yielding a Nash equilibrium. In order to make the coverage suitable for the student as well as for the expert reader, Adaptive Dynamic Programming for Control: • establishes the fundamental theory involved clearly with each chapter devoted to a clearly identifiable control paradigm; • demonstrates convergence proofs of the ADP algorithms to deepen undertstanding of the derivation of stability and convergence with the iterative computational methods used; and • shows how ADP methods can be put to use both in simulation and in real applications. This text will be of considerable interest to researchers interested in optimal control and its applications in operations research, applied mathematics computational intelligence and engineering. Graduate students working in control and operations research will also find the ideas presented here to be a source of powerful methods for furthering their study. The Communications and Control Engineering series reports major technological advances which have potential for great impact in the fields of communication and control. It reflects research in industrial and academic institutions around the world so that the readership can exploit new possibilities as they become available",Adaptive Dynamic Programming for Control: Algorithms and Stability,,'Springer Science and Business Media LLC',,10.1007/978-1-4471-4757-2,core
78928527,May 2012,"SHW systems are generally expected to last for at least 20 years with little or no maintenance. However, in many cases failures occur far sooner due to a variety of problems, many of which are undetected or detected long after the system has failed because the backup heater silently assumes the heating load. Some of the failures may cause the system to run inefficiently or even damage other system components, such as when a system loses fluid in the solar loop and the pump runs dry, eventually destroying itself.

In recent years there has been an increasing demand for SHW systems to become economic and reliable. Fault Detection and Diagnosis (FDD) in SHW systems is an important part of maintaining proper performance, reducing power consumption and unnecessary peak electricity demand. The aim of the current work is to develop anomaly detection system that can reliably detect both anticipated and unforeseen faults and can be implemented in commercial SHW systems without any additional sensors to the ones commonly needed for ordinary system control.

Adaptive Resonance Theory (ART)-based neural networks are chosen to perform this task, because the ART-based neural networks are fast, efficient learners and retain memory while learning new patterns. In particular, the ART networks can be incorporated into SHW system controller without any extra sensors and have the capability of an early detection of performance degradation faults. Other benefits of ART-based neural networks are on-line fault detection for its high computational efficiency and no involvement of faulty data for the training process.

A testbed for SHW system reliability is developed for the purposes of investigating the fault detection system. The input patterns of the fault detection system are generated from two sensors: collector plate temperature and water tank heat exchanger outlet temperature, which are normally installed in residential SHW systems installed by commercial operators. One of the strengths of the system is that only few data points are needed, meaning that it will not be necessary to instrument SHW systems with additional sensors, something which would not be acceptable in an aggressively competitive industry where reducing costs is paramount.

The training data for the fault detection system are generated from a verified SHW system TRNSYS (Transient Systems Simulation) model. The simulation and experimental results show that the ART-based anomaly detection has the capability to accurately and efficiently detect degradation and failure. Faults are detected at various levels depending on their severity. The ART-based anomaly detection can be used for SHW real-time reliability monitoring, as well as, eventually, in larger, more complex systems such as commercial building HVAC systems or subsystems.Sandia National Laboratories.Mechanical EngineeringDoctoralUniversity of New Mexico.  Dept. of Mechanical EngineeringMammoli, Andrea A.Mammoli, Andrea A.Razani, ArsalanVorobieff, PeterCaudell, Thomas P",Monitoring and anomaly detection in solar thermal systems using adaptive resonance theory neural networks,,,,,core
103244048,2013,"Predicting ad click–through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learn-ing based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for as-sessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated manage-ment of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promis-ing results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical ad-vances and practical engineering in this industrial setting, and to show the depth of challenges that appear when ap-plying traditional machine learning methods in a complex dynamic system",Ad click prediction: a view from the trenches,,,,,core
9660956,2012-12-01T00:00:00,"Autonomous Learning Systems is the result of over a decade of focused research and studies in this emerging area which spans a number of well-known and well-established disciplines that include machine learning, system identification, data mining, fuzzy logic, neural networks, neuro-fuzzy systems, control theory and pattern recognition. The evolution of these systems has been both industry-driven with an increasing demand from sectors such as defence and security, aerospace and advanced process industries, bio-medicine and intelligent transportation, as well as research-driven – there is a strong trend of innovation of all of the above well-established research disciplines that is linked to their on-line and real-time application; their adaptability and flexibility. Providing an introduction to the key technologies, detailed technical explanations of the methodology, and an illustration of the practical relevance of the approach with a wide range of applications, this book addresses the challenges of autonomous learning systems with a systematic approach that lays the foundations for a fast growing area of research that will underpin a range of technological applications vital to both industry and society. Key features: • Presents the subject systematically from explaining the fundamentals to illustrating the proposed approach with numerous applications. • Covers a wide range of applications in fields including unmanned vehicles/robotics, oil refineries, chemical industry, evolving user behaviour and activity recognition. • Reviews traditional fields including clustering, classification, control, fault detection and anomaly detection, filtering and estimation through the prism of evolving and autonomously learning mechanisms • Accompanied by a website hosting additional material, including the software toolbox and lecture notes Autonomous Learning Systems provides a ‘one-stop shop’ on the subject for academics, students, researchers and practicing engineers. It is also a valuable reference for Government agencies and software developers",Autonomous Learning Systems:From Data to Knowledge in Real Time,,John Willey and Sons,,,core
352949308,2013-01-01T00:00:00,"The process chain for the production of profiled strips, beams and panels includes a number of processes as mould design, fixing the number of cutting passes, tool grinding, tool measurement, moulding, sanding, profile-wrapping, coating and different types of quality assessment. This chain is still not consistently connected by means of information technologies. The design and the setup for each process in the chain are mostly done manually, using the experience of the operator. Computer aided systems (CAD/CAM) as widely used in the furniture industry are not in this field. The procedures are more complex and could not be automatized by the state of the art feature based algorithms. One attempt to solve the problem is the creation of a product data model that contains all information for the whole production process - not only geometrical data and tolerances but also semantic information (e.g. material, coating, batch size, etc.). Every process in the chain is now transforming the profile in a systematic and stochastic way. Both can be computed by the use of rules of artificial intelligence and tolerance calculations by means of statistical methods. Special CAM modules, which generate the specific process data, give also back the transformed profile and other necessary information to the superordinate product data model. The actual specific process data, for example for the process of profiling (e.g. setting parameters, pitch) can then be computed by a post processer using actual inputs as the geometry of the specified machine. The whole process chain can so be simulated before the real setup. The tools and processes can so be designed simultaneously in advance. The time for the production process can be reduced. This systematic is generating the same potentials as feature based CAD/CAM systems in the carcase furniture industry. The implementation of IT in the profiling process chain is accelerated and coordinated by the introduction of the linking product model",A product data model and computer aided manufacturing for the process chain of profiling,,Luleå Univ. of Technology,,,core
46819344,2014-12-04T00:00:00,"In the past decades, embedded systems become more and more present in our daily life. They are present in our wallet, in our car, in our house appliances and the current tendency is to control more and more things by using them. Major companies already started to envision the future of internet which become known as the Internet of Things. In the next years, more and more of our things will be smart, connected... and subject to software flaws.Micro-controllers, very small devices with a few hundreds bytes of memory, are at the heart of this revolution. They become cheaper, smaller and more powerful. Yet, contrary to the technologies used in our desktop computers or our server farms, the goal of the industry that creates these devices is not power. Indeed, controlling temperature in your home or humidity in your wine cellar does not need Gigahertz core. It even does not need multiple cores. What these devices need is to be cheap, to be able to be produced in very high volumes and to use small amount of electrical energy. In this context, the moore's law does not help us to have more cores in one device, but it allows to produce more devices with the same amount of silicon. The research I made in the past years try to reduce the gap between cheap software and robust and efficient software for these devices. I think that, by providing smart tools and production toolchains we can help junior or non specialized developers so that they can produce good embedded software for a reasonable development cost. I also focus my work on optimizing and providing efficient and secure software and network protocols, so that the Internet of Things can become a reality while respecting user privacy and being sustainable from a energy point of view.I focused my research on three aspects. First, I focused on how to allow larger softwares do be developped in embedded systems. In particular, we proposed techniques to allow Java/JavaCard code to be executed from a non-addressable memory using a full software cache.  I also looked at how to use domain specific languages to ease the implementation of a collaborative network intrusions detection system. Thanks to adapted tools, the software, writen in the DSL, can be compiled for a wide range of probes while offering garanties on the produced software. Last, I focused on energy aware network protocols in the context of smart cities.Au cours des dernières années, les systèmes embarqués sont de plus en plus présents dans notre vie de tous les jours. Ils sont dans notre portefeuille, dans notre voiture ou dans nos appareils ménagers. La tendance actuelle est à contrôler de plus en plus d'objets à l'aide de ces systèmes. Les grands acteurs de l'industrie ont déjà commencé à envisager le futur de l'internet, l'internet des objets. Dans les années à venir, de plus en plus de nos objets seront « intelligents », connectés... et sujets à des fautes logicielles.Les micro contrôleurs, de minuscules ordinateurs possédant quelques centaines d'octets de mémoire, sont au coeur de cette révolution. Ils deviennent de moins en moins cher et de plus en plus puissant. Néanmoins, à la différence de nos ordinateurs de bureau ou des serveurs, le but de l'industrie des micro contrôleurs n'est pas la puissance. En effet, contrôler la température de notre maison et le taux d'humidité de notre cave à vin ne nécessite pas des processeurs cadencés à plusieurs gigahertz. Cela ne nécessite même pas plusieurs coeurs de calculs. Le véritable besoin de ces équipements est d'être bons marché, d'être produits en très grands volumes, d'êtres petits, facilement intégrables et d'utiliser une faible quantité d'énergie électrique. Le meilleur exemple de cette tendance est très certainement la carte à puce. Depuis le début des années 90, de plus en plus de cartes à puce sont produites, vendues et utilisées dans le monde. Cependant, elles n'en restent pas moins de tout petits équipements d'une puissance inférieure de plusieurs ordres de grandeur de nos ordinateurs de bureau. Quand il s'agit de développer du logiciel pour ces équipements, un développeur débutant n'est généralement pas suffisamment préparé et ne sera pas capable d'écrire du logiciel efficace pour ces cibles avant de longue années passées à gagner de l'expérience et de l'expertise. Si le logiciel que l'ont souhaite produire doit être efficace, sûr et correct au sens le plus strict du terme, l'industrie doit compter sur des développeurs très spécialisés, expérimentés et donc chers. Les acteurs industriels ont donc deux alternatives : produire du logiciel bon marché, mais assez inefficaceet défectueux ou dépenser une somme importante pour le développement et fournir un logiciel de bonne qualité. On peu aisément supposer que la tendance actuelle est au logiciel bon marché, et que la probabilité que le logiciel qui équipera les millions d'équipements formant l'internet des objets sera de piètre qualité et offrira quantité de failles que des acteurs malveillants se feront une joie d'exploiter si rien n'est fait pour rendre bon marché le développement correct de logiciels embarqués.Mes recherches de ces dernières années vont dans le sens de la réduction du fossé séparant logiciels bon marché et logiciels sûrs et performants. Je suis convaincu qu'en offrant des outils « intelligents » et des chaînes de production logicielle efficaces, nous pouvons aider les développeurs débutants, ou non spécialisés, à produire du logiciel embarqué de bonne qualité pour un coût de développement raisonnable. J'ai également porté mon attention sur l'optimisation et la sécurisation des logiciels et des protocoles réseau afin que l'internet des objets puisse devenir une réalité tout en respectant la vie privée des utilisateurs et en offrant une alternative durable sur le plan énergétique.Je me suis principalement intéressé à trois champs de recherche. Tout d'abord, j'ai cherché à permettre l'utilisation de techniques de développement standard (objets, composants, programmation en Java…) à la faible capacité mémoire des équipements embarqués. A l'inverse, je me suis également intéressé à l'utilisation de langages dédiés à une application afin de permettre à des spécialistes du domaine de la sécurité réseau d'exprimer des algorithmes de détection d'intrusions. A l'aide d'une suite d'outils dédiée, ces algorithmes sont compilés et optimisés automatiquement pour être utilisés dans une architecture distribuée de sondes embarquées. Enfin, je me suis intéressé aux protocoles réseau économes en énergie pour interconnecter les équipements embarqués dans le cadre des villes intelligentes",Élements de conception de systèmes embarqués fortement contraints,,HAL CCSD,,,core
33222858,,"[[abstract]]This paper proposes an intelligent evolutionary algorithm that can be applied in the design of optimal automation systems, and employs a multimodal six-bar mechanism optimization design, job shop production scheduling for the fishing equipment industry, and dynamic real-time production scheduling system design cases to show how the technique developed in this paper is highly effective at resolving optimal automation system design problems. Major breakthroughs in artificial intelligence continue to be made in the wake of advanced information technology developments, and the field of intelligent evolutionary algorithms has attracted a particularly large amount of attention from researchers and users in the artificial intelligence community. The successful optimization of automation system design requires interdisciplinary integration, and further requires the use of actual cases, verification, and improvement to ensure implementation in real-world applications",Applications of Intelligent Evolutionary Algorithms in Optimal Automation System Design,,'International Journal of Automation and Smart Technology',,10.5875/ausmt.v1i1.100,core
354463891,2014-01-01T00:00:00,"2014 Fall.Includes illustrations (some color).Includes bibliographical references (pages 170-179).The structure of many first principle engineering models is in the form of non-linear differential algebraic equations (DAE). Standard system theory, however, pre-assumes that the system model is described by ordinary differential equations (ODE) and hence can not accommodate DAE models unless if they can be transformed to an equivalent ODE form. However, such transformation, even if possible, can become cumbersome and the descriptive representation of the model will be lost. The size of these models is typically in the order of 1000's of equations for systems with multiple units or for systems described by discretized partial differential algebraic equations. This demands numerically robust and efficient methods to use these models for real time applications. The focus of this study is to develop estimation techniques that can be used with linear and non-linear differential algebraic equations that are robust and numerically efficient. Estimation of DAE systems can be used for monitoring and control applications and will exploit the modelling software capabilities that are becoming prevalent in the industry. The first part of this dissertation examines the problem of state estimation of linear discrete time descriptor systems from new perspectives. First, the available theory on differential algebraic equations has been used to examine the problem of stochastically modelling a linear differential algebraic equation to avoid non-causality of the solution. Second, the Baysian paradigm has been used to find the Maximum a Posteriori (MAP) estimate for index 1 and higher index descriptor systems with the utility of Kronecker canonical transformation of a matrix pencil. This analysis indicated that state estimation of high index descriptor systems can be conducted without the need of any model transformations provided that the high index model is causal. This also showed that the MAP estimate is identical to the Maximum Likelihood (ML) estimate in the usual sense. Third, MAP estimation of descriptor systems was utilized for addressing problems of practical interest; namely state estimation with truncated Gaussian distributions, state estimation with measurement outliers and state estimation of singularly perturbed systems using the quasi-steady state model approximation. The second part of this dissertation addresses the need to find stable and efficient algorithms to solve the minimization problems presented in the theory section of this dissertation. The first algorithm solves the MAP estimation problem when mixed deterministic and stochastic equations are involved. The second algorithm solves the MAP estimation problem when inequality constraints are involved using a new strategy called Multiple Window Moving Horizon Estimation (MW-MHE) that enhances the performance of conventional Moving Horizon Estimation (MHE). This is achieved by exploiting periods of constraint inactivity in sliding window minimization problems by adaptively changing the objective function in response to the activity of constraints. In other words, the 'sparsity' of active constraints is exploited to enable efficient long horizon estimation. Demonstration of the efficiency of the technique was made with problems involving unknown input estimation and filtering subject to outliers in measurements and impulsive process disturbances. The third part of this dissertation serves the dual objective of examining the effectiveness of descriptor state estimation and addressing the practical need for estimating gas mole fractions in catalytic partial oxidation in real time. This process is critical for producing H2 for portable fuel cell applications and accurate on-line estimation of mole fractions is important for system operability and reliability. The residence time of the reactor is in the order of 10 milliseconds, imposing stringent real time operational constraints. A detail analysis of this estimation problem in terms of process dynamics, model reduction and observability analysis has been conducted with the utility of descriptor system state estimation techniques. A descriptor MHE has been developed successfully with update rates faster than 0.02 seconds",Efficient descriptor state estimation with a case study in catalytic partial oxidation reforming,,Colorado School of Mines. Arthur Lakes Library,,,core
47136616,2013-04-15T14:49:47,"Artificial intelligence techniques are being widely used to face the new reality and to provide solutions that can make power systems undergo all the changes while assuring high quality power. In this way, the agents that act in the power industry are gaining access to a generation of more intelligent applications, making use of a wide set of AI techniques. Knowledge-based systems and decision-support systems have been applied in the power and energy industry. This article is intended to offer an updated overview of the application of artificial intelligence in power systems. This article paper is organized in a way so that readers can easily understand the problems and the adequacy of the proposed solutions. Because of space constraints, this approach can be neither complete nor sufficiently deep to satisfy all readers’ needs. As this is amultidisciplinary area, able to attract both software and computer engineering and power system people, this article tries to give an insight into themost important concepts involved in these applications. Complementary material can be found in the reference list, providing deeper and more specific approaches",Intelligent power system,,'Wiley',,10.1002/9780470050118.ecse196,core
147779882,2012,"Traditional methods for bacterial identification include Gram staining, culturing, and biochemical assays for phenotypic characterization of the causative organism. These methods can be time-consuming because they require in vitro cultivation of the microorganisms. Recently, however, it has become possible to obtain chemical profiles for lipids, peptides, and proteins that are present in an intact organism, particularly now that new developments have been made for the efficient ionization of biomolecules. MS has therefore become the state-of-the-art technology for microorganism identification in microbiological clinical diagnosis. Here, we introduce an innovative sample preparation method for nonculture-based identification of bacteria in milk. The technique detects characteristic profiles of intact proteins (mostly ribosomal) with the recently introduced MALDI SepsityperTM Kit followed by MALDI-MS. In combination with a dedicated bioinformatics software tool for databank matching, the method allows for almost real-time and reliable genus and species identification. We demonstrate the sensitivity of this protocol by experimentally contaminating pasteurized and homogenized whole milk samples with bacterial loads of 10(3)-10(8) colony-forming units (cfu) of laboratory strains of Escherichia coli, Enterococcus faecalis, and Staphylococcus aureus. For milk samples contaminated with a lower bacterial load (104 cfu mL-1), bacterial identification could be performed after initial incubation at 37 degrees C for 4 h. The sensitivity of the method may be influenced by the bacterial species and count, and therefore, it must be optimized for the specific application. The proposed use of protein markers for nonculture-based bacterial identification allows for high-throughput detection of pathogens present in milk samples. This method could therefore be useful in the veterinary practice and in the dairy industry, such as for the diagnosis of subclinical mastitis and for the sanitary monitoring of raw and processed milk products.Brazilian Science foundation CNPqBrazilian Science foundation CNPqFAPESPFAPESP [09/12751-5]FINEPFINE",Nonculture-based identification of bacteria in milk by protein fingerprinting,,HOBOKEN,"[{'title': None, 'identifiers': ['issn:1615-9853', '1615-9853']}]",10.1002/pmic.201200053,core
29175627,2012-02-28T00:00:00,"This paper reports on a current project to develop a prototype system

for the automatic identification and quantification of potato defects based on

machine vision. The system developed uses off-the-shelf hardware, including a

low-cost vision sensor and a standard desktop computer with a graphics processing

unit (GPU), together with software algorithms to enable detection, identification

and quantification of common defects affecting potatoes at near-real-time frame

rates. The system uses state-of-the-art image processing and machine learning

techniques to automatically learn the appearance of different defect types. It also

incorporates an intuitive graphical user interface (GUI) to enable easy set-up of the

system by quality control (QC) staff working in the industry",A prototype low-cost machine vision system for automatic identification and quantification of potato defects,https://core.ac.uk/download/29175627.pdf,Proceedings Crop Protection in Northern Britain 2012,,,core
211485892,2013-01-01T00:00:00,"The twelve papers included in this special issue represent a selection of extended contributions presented at the Sixth International Conference on Soft Computing Models in Industrial and Environmental Applications, held in Salamanca, Spain, 6–8th April, 2011. Papers were selected on the basis of fundamental ideas and concepts rather than the direct usage of well-established techniques. This special issue is then aimed at practitioners, researchers and post-graduate students, who are engaged in developing and applying advanced Soft Computing Models to solving real-world problems in the Industrial and Environmental fields. The papers are organized as follows. In the first contribution, Graña and Gonzalez-Acuña develop a formulation of dendritic classifiers based on lattice kernels and train them using a direct Monte Carlo approach and a Sparse Bayesian Learning. The results of both kinds of training are compared with the Relevance Vector Machines on a collection of benchmark datasets. In the second contribution by Irigoyen and Miñano, the authors present the results of the identification of the relationship in time, between the required exercise (machine resistance) and the heart rate of the patient in medical effort tests, using a NARX neural network model. In the experimental stage, test data have been obtained by exercising with a cyclo-ergometer in two different tests: Power Step Response and Conconi. Carneiro et al. in the third contribution present a biologically inspired method to deal with the problem in which genetic algorithms are used to create possible solutions for a given dispute. The approach presented is able to generate a broad number of diverse solutions that cover virtually the whole search space for a given problem. The results of this work are being applied in a negotiation tool that is part of the UMCourt conflict resolution platform. In the fourth contribution by Donate et al., they propose a novel Evolutionary Artificial Neural Networks (EANN) approach, where a weighted n-fold validation fitness scheme is used to build an ensemble of neural networks, under four different combination methods: mean, median, softmax and rank-based combinations. Several experiments were held, using six real-world time series with different characteristics and from distinct domains. Overall, the proposed approach achieved competitive results when compared with non weighted n-fold EANN ensembles, the simpler 0-fold EANN and also the popular Holt–Winters statistical method. Dan Burdescu et al. in the fifth contribution, present a system used in the medical domain for three distinct tasks: image annotation, semantic based image retrieval and content based image retrieval. An original image segmentation algorithm based on a hexagonal structure was used to perform the segmentation of medical images. Image's regions are described using a vocabulary of blobs generated from image features using the K-means clustering algorithm. The annotation and semantic based retrieval task is evaluated for two annotation models: Cross Media Relevance Model and Continuous-space Relevance Model. Semantic based image retrieval is performed using the methods provided by the annotation models. The ontology used by the annotation process was created in an original manner starting from the information content provided by the Medical Subject Headings (MeSH). The experiments were made using a database containing colour images retrieved from medical domain using an endoscope and related to digestive diseases. In sixth paper by Pedraza et al., they develop a face recognition system based on soft computing techniques, which complies with privacy-by-design rules and defines a set of principles that context-aware applications (including biometric sensors) should contain to conform to European and US law. This research deals with the necessity to consider legal issues concerning privacy or human rights in the development of biometric identification in ambient intelligence systems. Clearly, context-based services and ambient intelligence (and the most promising research area in Europe, namely ambient assisted living, ALL) call for a major research effort on new identification procedures. The aim of the research by Redel-Macías et al. in paper seven is to develop a novel model which can be used in pass-by noise test in vehicles based on ensembles of hybrid Evolutionary Product Unit or Radial Basis function Neural Networks (EPUNN or ERBFNNs) at high frequencies. Statistical models and ensembles of hybrid EPUNN and ERBFNN approaches have been used to develop different noise identification models. The results obtained using different ensembles of hybrid EPUNNs and ERBFNNs show that the functional model and the hybrid algorithms proposed provide a very accurate identification compared to other statistical methodologies used to solve this regression problem. In the eighth paper, Wu et al. analyse the existence criterion of loop strategies, and then present some corollaries and theorems, by which the loop strategies and chain strategies can be found, also superfluous strategies and inconsistent strategies. It presents a ranking model that indicates the weak node in strategy set and it also introduces a probability-based model which is the basis of evaluation of strategy. Additionally, this research proposes a method to generate offensive strategy, and the statistic results of simulation game prove the validity of the method. Pop et al. in the ninth paper present an efficient hybrid heuristic algorithm obtained by combining a genetic algorithm (GA) with a local–global approach to the generalized vehicle routing problem (GVRP) and a powerful local search procedure. The computational experiments on several benchmarks instances show that the hybrid algorithm is competitive to all of the known heuristics published to date. In the tenth paper Kramer et al. illustrate how methods from neural computation can serve as forecasting, and monitoring techniques, contributing to a successful integration of wind into sustainable, and smart energy grids. The study is based on the application of kernel methods like support vector regression and kernel density estimation as prediction methods. Furthermore, dimension reduction techniques like self-organizing maps for monitoring of high-dimensional wind time series are applied. The methods are briefly introduced, related work is presented, and experimental case studies are exemplarily described. The experimental parts are based on real wind energy time series data from the NREL western wind resource dataset. Vera et al. in the eleventh contribution present a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. The novel proposed approach was tested under real dental milling processes using a high precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study. The final contribution, by Sakalauskas and Kriksciuniene, presents a research about financial market efficiency and to recognize major reversal points of long-term trend of stock market index, which could indicate forthcoming crisis or market raise periods. The study suggests a computational model of financial time series analysis, which combines several approaches of soft computing, including information efficiency evaluation methods (Shannon's entropy, Hurst exponent), neural networks and sensitivity analysis. The model aims to derive the aggregated measure for evaluating efficiency of the financial market and to find its interrelationships with the reversal of long-term trend. The radial basis function neural network was designed for forecasting moments of cardinal changes in stock market behaviour, expressed by its entropy values derived from the symbolized time series of stock market index. The performance of neural network model is explored by applying sensitivity analysis and resulted in selecting smoothing parameters of the input variables. The experimental research investigates behaviour of the long-term trend of the three emerging financial markets within NASDAQ OMX Baltic stock exchange. Introduction of information efficiency measures improve ability of the model to recognize the approaching reversal of long-term trend from temporary market “nervousness” and can be useful for calibrating stock trading strategy. First, we would like to thank all the authors for their valuable contributions, which made this special issue possible. We also like to thank our peer-reviewers for their timely diligent work and efficient efforts. We are also grateful to the Editor-in-Chief of Neurocomputing Journal, Prof. Tom Heskes, for his continued support for the SOCO series of conferences and for this Special Issue on this prestigious journal. Finally, we hope the reader will share our joy and find this special issue very useful",New trends on soft computing models in industrial and environmental applications,,'Elsevier BV',,,core
10673236,2013-01-01T00:00:00,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study.Web of Science1091049",Applying soft computing techniques to optimise a dental milling process,,'Elsevier BV',"[{'title': 'Neurocomputing', 'identifiers': ['0925-2312', 'issn:0925-2312']}]",10.1016/j.neucom.2012.04.033,core
293503102,2012-01-01T00:00:00,"The robust popularization of 3D videos noticed along the last decade, allied to the omnipresence of smart mobile devices handling multimedia-capable features, has led to intense development and research focusing on efficient 3D-video encoding techniques, display technologies, and 3D-video capable mobile devices. In this scenario, the Multiview Video Coding (MVC) standard is key enabler of the current 3D-video systems by leading to meaningful data reduction through advanced encoding techniques. However, real-time MVC encoding for high definition videos demands high processing performance and, consequently, high energy consumption. These requirements are attended neither by the performance budget nor by the energy envelope available in the state-of-the-art mobile devices. As a result, the realization of MVC targeting mobile systems has been posing serious challenges to industry and academia. The main goal of this thesis is to propose and demonstrate energy-efficient MVC solutions to enable high-definition 3D-video encoding on mobile battery-powered embedded systems. To expedite high performance under severe energy constraints, this thesis proposes jointly considering energy-efficient optimizations at algorithmic and architectural levels. On the one hand, extensive application knowledge and data analysis was employed to reduce and control the MVC complexity and energy consumption at algorithmic level. On the other hand, hardware architectures specifically designed targeting the proposed algorithms were implemented applying low-power design techniques, dynamic voltage scaling, and application-aware dynamic power management. The algorithmic contribution lies in the MVC energy reduction by shorten the computational complexity of the energy-hungriest encoder blocks, the Mode Decision and the Motion and Disparity Estimation.  The proposed energy-efficient algorithms take advantage of the video properties along with the strong correlation available within the 3D-Neighborhood (spatial, temporal and disparity) space in order to efficiently reduce energy consumption. Our Multi-Level Fast Mode Decision defines two complexity reduction operation modes able to provide, on average, 63% and 71% of complexity reduction, respectively. Additionally, the proposed Fast ME/DE algorithm reduces the complexity in about 83%, for the average case. Considering the run-time variations posed by changing coding parameters and video content, an Energy-Aware Complexity Adaptation algorithm is proposed to handle the energy versus coding efficiency tradeoff while providing graceful quality degradation under severe battery draining scenarios by employing asymmetric video coding. Finally, to cope with eventual video quality losses posed by the energy-efficient algorithms, we define a video quality management technique based on our Hierarchical Rate Control. The Hierarchical Rate Control implements a frame-level rate control based on a Model Predictive Controller able to increase in 0.8dB (Bjøntegaard) the overall video quality. The video quality is increased in 1.9dB (Bjøntegaard) with the integration of the basic unit-level rate control designed using Markov Decision Process and Reinforcement Learning. Even though the energy-efficient algorithms drive to meaningful energy reduction, hardware acceleration is mandatory to reach the energy-efficiency demanded by the MVC. Aware of this requirement, this thesis brings architectural solutions for the Motion and Disparity Estimation unit focusing on energy reduction while attending real-time throughput requirements. To achieve the desired results, as shown along this volume, there is a need to reduce the energy related to the ME/DE computation and related to the intense memory communication.  Therefore, the ME/DE architectures incorporate the Fast ME/DE algorithm in order to reduce the computational complexity while the memory hierarchy was carefully designed to find the optimal energy tradeoff between external memory accesses and on-chip video memory size. Statistical analysis where used to define the size and organization of the on-chip cache memory while avoiding increased memory misses and the consequent data retransmission. A prefetching technique based on search window prediction also supports the reduction of external memory access. Moreover, a memory power gating technique based on dynamic search window formation and an application aware power management were proposed to reduce the static energy consumption related to on-chip video memory. To implement these techniques a SRAM memory featuring multiple power states was used. The architectural contribution contained in this thesis extends the state-of-the-art by achieving real-time ME/DE processing for 4-views HD1080p running at 300MHz and consuming 57mW",Energy-efficient algorithms and architectures for multiview video coding,,,,,core
187722098,2014-01-01T00:00:00,"Artificial intelligence methodologies, as the core of discrete control and decision support systems, have been extensively applied
in the industrial production sector. The resulting tools produce excellent results in certain cases; however, the NP-hard nature of
many discrete control or decision making problems in the manufacturing area may require unaffordable computational resources,
constrained by the limited available time required to obtain a solution. With the purpose of improving the efficiency of a control
methodology for discrete systems, based on a simulation-based optimization and the Petri net (PN) model of the real discrete
event dynamic system (DEDS), this paper presents a strategy, where a transformation applied to the model allows removing the
redundant information to obtain a smaller model containing the same useful information. As a result, faster discrete optimizations
can be implemented.This methodology is based on the use of a formalism belonging to the paradigmof thePNfor describingDEDS,
the disjunctive colored PN. Furthermore, the metaheuristic of genetic algorithms is applied to the search of the best solutions in
the solution space. As an illustration of the methodology proposal, its performance is compared with the classic approach on a case
study, obtaining faster the optimal solution",Control of discrete event systems by means of discrete optimization and disjunctive colored PNs: application to manufacturing facilities,https://core.ac.uk/download/187722098.pdf,'Hindawi Limited',,10.1155/2014/821707,core
214144901,2013-01-01T08:00:00,"Computational fluid dynamics (CFD) is one of the branches of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the millions of calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. Indoor airflow simulations are necessary for building emergency management, preliminary design of sustainable buildings, and real-time indoor environment control.
The simulation should also be informative since the airflow motion, temperature distribution, and contaminant concentration is important. However, CFD computation is usually time-consuming, and not suitable for simulating real-time indoor air movement. Many researchers are concentrating on both hardware utilization and CFD algorithms, to make simulation much faster. Fast flow simulations are important for some applications in the building industry, such as the conceptual design of indoor environment, or they are coupled with energy simulation to provide deep analysis on the performance of the buildings. Such application does not require the same high level of accuracy as traditional CFD simulation because it only requires conceptual or semi-accurate distributions of the flow but within a short computing time. However, year round simulation is needed rather than the analysis of two or three extreme cases in order to help the designer investigate the problem clearly. To meet these special needs, an efficient and informative fluid simulation method is needed to provide fast airflow simulation with an inevitable but nominal compromise in accuracy.
This research provides a comprehensive workflow for the designer to simulate and analyze the annual indoor environment. In addition to the hardware acceleration deployed, fast fluid simulation algorithm is developed, and a machine learning based interpolation is used to allow the simulation coverage to be conducted annually. The outcome of this research is a methodology that allows the annual simulation time similar to the one used to perform two or three extreme cases of simulation using current methods",A Three Layered Framework for Annual Indoor Airflow CFD Simulation,https://core.ac.uk/download/214144901.pdf,ScholarlyCommons,,,core
23197378,2013-09-23,"Predicting ad click–through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system",Ad Click Prediction: a View from the Trenches,,,,,core
85532601,2014-01-01T00:00:00,"Nowadays the maritime operational picture is characterised by a growing number of entities whose interactions and activities are constantly changing. To provide timely support in this dynamic environment, automated systems need to be equipped with tools— lacking in existing systems—for real-time prioritisation of the application tasks (missions), selection and alignment of relevant information, and efficient reasoning at a situation level. In this paper, we present METIS—an industrial prototype system for supporting real-time, actionable maritime situational awareness. In particular, we focus on the innovation of METIS, which lies in the employment and integration of several state-of-the-art AI technologies to build the overall system’s intelligence. These include reconfiguration of multi-context systems, natural language processing of heterogeneous (un)structured data and probabilistic reasoning of uncertain information. The capabilities of the system have been demonstrated in a proof of concept, which is deployed as a situational awareness plugin in the Tacticos command-and-control platform of our industrial partner. The principles exploited by METIS are giving valuable insights into what is considered to become the next generation of situational awareness systems",An Integrated Reconfigurable System for Maritime Situational Awareness,,,,,core
100019666,2014-11-26,"Abstract. A Manufacturing Execution System (MES) consists of high-cost, large-scale, multi-task software systems. Compa-nies and factories apply these complex applications for the purposes of production management to monitor and track all as-pects of factory-based manufacturing processes. Nevertheless, companies seek to control the production process with even greater rigour. Improvements associated with an MES involve the identification of new knowledge within the data set and its integration in the system, which implies a step forward to Business Process Management (BPM) systems, from which the users of an MES may gain relevant information, not only on execution procedures but to decide on the best scheduled arrangement. This work studies the data gathered from a real MES that is used in a plastic products factory. Several Artificial Intelligence and Soft Computing modelling methods based on fuzzy rules assist in the calculation of manufacturing costs and decisions over shift work rotas: two decisions that are of relevance for the improvement of the execution system. The results of the study, which identify the most suitable models to facilitate execution-related decision-making,  are presented and discusse",Optimising operational costs using Soft Computing techniques,,,,,core
100309493,2014-12-11,"Harmonic estimation is the foundation of every active noise canceling method in low-voltage power systems. It generates and re-injects reference currents in phase opposition through an active power line conditioner. Active Power Filters (APFs) are today the most widely used systems to compensate harmonics in industrial power plants. We propose to improve the performances of conventional APFs by using artificial neural networks (ANNs) for harmonics estimation. This new method combines both the advantages of conventional APF to compute instantaneous real and imaginary powers and the learning capabilities of ANNs to adaptively choose the parameters of the power system. In fact, the separation of the powers is implemented with an Adaline neural network which uses a priori known frequencies as inputs. Furthermore, multilayer feedforward networks are used to approximate the instantaneous powers and to compute the reference currents. Simulation results show the reliability of the method and better performances than conventional APFs. I",Artificial Neural Networks for Harmonic Estimation in Low-Voltage Power Systems,,,,,core
145029254,2013-05-01T07:00:00Z,"Fault Analysis is the detection and diagnosis of malfunction in machine operation or process control. Early fault analysis techniques were reserved for high critical plants such as nuclear or chemical industries where abnormal event prevention is given utmost importance. The techniques developed were a result of decades of technical research and models based on extensive characterization of equipment behavior. This requires in-depth knowledge of the system and expert analysis to apply these methods for the application at hand. Since machine learning algorithms depend on past process data for creating a system model, a generic autonomous diagnostic system can be developed which can be used for application in common industrial setups. In this thesis, we look into some of the techniques used for fault detection and diagnosis multi-class and one-class classifiers. First we study Feature Selection techniques and the classifier performance is analyzed against the number of selected features. The aim of feature selection is to reduce the impact of irrelevant variables and to reduce computation burden on the learning algorithm. We introduce the feature selection algorithms as a literature survey. Only few algorithms are implemented to obtain the results. Fault data from a Radio Frequency (RF) generator is used to perform fault detection and diagnosis. Comparison between continuous and discrete fault data is conducted for the Support Vector Machines (SVM) and Radial Basis Function Network (RBF) classifiers. In the second part we look into one-class classification techniques and their application to fault detection. One-class techniques were primarily developed to identify one class of objects from all other possible objects. Since all fault occurrences in a system cannot be simulated or recorded, one-class techniques help in identifying abnormal events. We introduce four one-class classifiers and analyze them using Receiver-Operating Characteristic (ROC) curve. We also develop a feature extraction method for the RF generator data which is used to obtain results for one-class classifiers and Radial Basis Function Network two class classification. To apply these techniques for real-time verification, the RIT Fault Prediction software is built. LabView environment is used to build a basic data management and fault detection using Radial Basis Function Network. This software is stand alone and acts as foundation for future implementations",Fault analysis using state-of-the-art classifiers,,RIT Scholar Works,,,core
42337406,2012-08-24T00:00:00,"New embedded predictive control applications call for more efficient ways of solving quadratic programs (QPs) in order to meet demanding real-time, power and cost requirements. A single precision QP-on-a-chip controller is proposed, implemented in a field-programmable gate array (FPGA) with an iterative linear solver at its core. A novel offline scaling procedure is introduced to aid the convergence of the reduced precision solver. The feasibility of the proposed approach is demonstrated with a real-time hardware-in-the-loop (HIL) experimental setup where an ML605 FPGA board controls a nonlinear model of a Boeing 747 aircraft running on a desktop PC through an Ethernet link. Simulations show that the quality of the closed-loop control and accuracy of individual solutions is competitive with a conventional double precision controller solving linear systems using a Riccati recursion.This work was supported by the EPSRC (Grants EP/G031576/1, EP/G030308/1 and EP/I012036/1) and the EU FP7 Project EMBOCON, as well as industrial support from Xilinx, the Mathworks, and the European Space Agency.IFAC Conference on Nonlinear Model Predictive Control 2012 (NMPC'12), Noordwijkerhout, the Netherlands on August 23 - 27, 2012",Predictive Control of a Boeing 747 Aircraft using an FPGA,https://core.ac.uk/download/42337406.pdf,,,,core
46827307,2013-11-28T00:00:00,"This report presents the bulk of my research work from the completion of my PhD, in late 2004, until the present day. The overall aim of my research is the development of mathematically sound and practically efficient methods to check the correctness of computer software. Efficiency is achieved using approximations, while soundness is guaranteed by employing over-approximations of program behaviors. My research is grounded in the theory of abstract interpretation, a powerful mathematical framework facilitating the development, use, comparison, and composition of approximations in a sound way. I am mainly interested in developing new reusable abstraction components (so called abstract domains) that can be readily implemented, and in using them to develop static analyzers, which are computer programs able to check automatically the safety of software. While my early research was focused on inferring the values of variables in sequential programs, my current interest and latest results concern the analysis of concurrent programs, hence the title of this report. The first two chapters of this report constitute an introduction. The first chapter is an informal introduction to the problem at hand, existing solutions, their strengths and their shortcomings. The second chapter presents prior mathematical and formal tools on which our work is based, including some notions of abstract interpretation, a description of existing abstract domains and their application to the static analysis of sequential programs. It also recalls some results I obtained during my PhD and that will be useful in the rest of the report. The subsequent chapters describe the work I performed after completing my PhD. The third chapter is devoted to aspects of static analyzers that are specific to concurrent programs. This topic of personal research has led to the construction of a generic analysis method for concurrent programs, parametrized by the choice of abstract domains. The method is based on a notion of ''interference'' that abstracts thread interleavings in a sound way in order to achieve a thread-modular analysis. It is related to Jones' rely-guarantee proof method, and we make this connection formal in a first part. Then, we present an interference-based analysis in big-step form that is efficient and easy to implement. In a third part, we study the interaction of the analysis with weakly consistent memory models, found in modern processors and language specifications. The last part discusses how to adapt the analysis to exploit some properties of the scheduling (such as the use of real-time thread priorities and synchronization primitives). The fourth and fifth chapters are devoted to the design of abstract domains. Although some of them found their application in the analysis of concurrent programs, they are actually generic and could be exploited in any kind of static analysis, for concurrent or sequential programs. The fourth chapter concerns numeric domains to infer linear equality and inequality relations, developed in collaboration with Liqian Chen while he visited ENS during his PhD. The initial motivation was to revise the classic polyhedra domain using sound floating-point arithmetic to improve its efficiency, but it unexpectedly yielded the construction of new, more expressive domains based on interval affine relations, which we also present. The fifth chapter concerns the abstraction of realistic data-types as found in the C programming language, including machine integers, floating-point numbers, and structured blocks of memory (structs, unions, and arrays). We design abstractions that are aware of the low-level memory representation of data-types, to support the analysis of programs that rely on assumptions about this representation (such as ''type punning'' constructions in C). The need for such abstractions was motivated by the analysis, in the scope of the Astrée and AstréeA static analyzers, of industrial C programs, where such low-level constructions are widespread. The sixth chapter is devoted to the application of these methods to the design of static analyzer tools. It mainly reports on my experience with the Astrée analyzer, a team effort initiated during my PhD in 2001 that extended well beyond it and culminated in its industrialization in 2009. Much of my theoretical work could find some application in Astrée, as Astrée fuelled my research with not only practical problems to solve, but also concrete problems that could only be overcome by theoretical developments. This part also reports my own ongoing effort on AstréeA, an extension of Astrée that incorporates the interference abstraction presented above and aims at proving the absence of run-time error in concurrent embedded programs (while Astrée only considers synchronous programs). Additionally, this chapter presents the Apron abstract domain library, another, more academic, team effort, which aims at encouraging the research on numeric abstract domains. The report concludes with some perspectives for future researches.Ce mémoire d'habilitation résume la majeure partie de mes recherches, depuis la fin de mon doctorat, fin 2004, jusqu'à aujourd'hui. Le but essentiel de mes recherches est le développement de méthodes fondées sur des bases mathématiques et performantes en pratique pour s'assurer de la correction des logiciels. J'utilise des approximations pour permettre une bonne performance, tandis que la validité des résultats est garantie par l'emploi exclusif de sur-approximations des ensembles des comportements des programmes. Ma recherche est basée sur l'interprétation abstraite, une théorie très puissante des approximations de sémantiques permettant aisément de les développer, les comparer, les combiner. Je m'emploie en particulier au développement de nouveaux composants réutilisables d'abstraction, les domaines abstraits, qui sont directement implantables en machine, ainsi qu'à leur utilisation au sein d'analyseurs statiques, qui sont des outils de vérification automatique de programmes. Mes premières recherches concernaient l'inférence de propriétés numériques de programmes séquentiels, tandis que mes recherches actuelles se tournent vers l'analyse de programmes concurrents, d'où le titre de ce mémoire. Les deux premiers chapitres de ce mémoire constituent une introduction, tandis que les suivants présentent mon travail d'habilitation proprement dit. Le premier chapitre est une introduction informelle à la problématique de l'analyse de programmes, aux méthodes existantes, leurs forces et leurs faiblesses. Le deuxième chapitre présente de manière formelle les outils dont nous aurons besoin par la suite : les bases de l'interprétation abstraite, quelques domaines abstraits existants et la construction d'analyses statiques par interprétation abstraite, ainsi que quelques résultats utiles que j'ai obtenu en doctorat. Le troisième chapitre est consacré aux aspects spécifiques de l'analyse de programmes concurrents. Cette recherche, très personnelle, a abouti à la construction d'une méthode d'analyse de programmes concurrents, paramétrée par le choix de domaines abstraits, et basée sur une notion d'interférence abstrayant les interactions entre threads. Ainsi, l'analyse construite est modulaire pour les threads. Cette méthode est reliée aux preuves rely-guarantee proposées par Jones, ce que nous montrons formellement dans une première partie. Nous construisons ensuite une analyse à grands pas basée sur les interférences, efficace et facile à implanter. Les deux dernière parties étudient les liens entre l'analyse et les modèles mémoires faiblement cohérents (désormais incontournables) ainsi que le raffinement de l'analyse pour tenir compte des propriétés spécifiques des ordonnanceurs temps-réels (nous étudions en particulier l'effet des priorités des threads et l'emploi d'objets de synchronisation). Le quatrième et le cinquième chapitres sont consacrés à la constructions de domaines abstraits. Ceux-ci ne sont pas spécifiquement liés au problème de la concurrence ; ils sont utiles à l'analyse de tous programmes, séquentiels comme concurrents. Le chapitre 4 étudie des domaines numériques inférant des égalités et inégalités affines, développés en collaboration avec Liqian Chen, alors doctorant en visite à l'ENS. La motivation première était l'emploi de nombres à virgule flottante afin d'améliorer l'efficacité du domaine des polyèdres, mais ces travaux ont également débouché sur la découverte de nouveaux domaines, basés sur les relations affines à coefficients intervalles, que nous présentons également. Le chapitre 5 étudie les abstractions de types de données réalistes, comme ceux rencontrés dans le langage C : les entiers machines, les nombres à virgule flottante, et les blocs structurés (tableaux, structures, unions). Nos abstractions modélisent finement les détails de l'encodage en mémoire des données afin de permettre l'analyse de programmes qui en dépendent (par exemple, ceux utilisant le type-punning). Ces abstractions sont motivées par nos expériences d'analyses, avec les outils Astrée et AstréeA, de programmes C industriels ; ceux-ci employant fréquemment ce type de constructions de bas niveau. Le sixième chapitre est consacré aux applications des méthodes présentées ci-dessus à la construction d'outils d'analyse statique. Il décrit en particulier mon travail sur l'outil Astrée que j'ai co-développé avec l'équipe Abstraction pendant et après mon doctorat, et qui a été industrialisé en 2009. Mes résultats théoriques et appliqués ont contribué au succès d'Astrée, tandis que celui-ci m'a fourni de nouveaux thèmes de recherches, sous la forme de problèmes concrets dont la résolution n'a pu se faire que grâce à des développements théoriques. Ce chapitre décrit également AstréeA, une extension d'Astrée utilisant l'abstraction d'interférences proposée plus haut pour l'analyse de programmes concurrents (Astrée étant limité aux programmes séquentiels). Il décrit également Apron, une bibliothèque de domaines abstraits numériques que j'ai co-développée. Il s'agit d'un outil plus académique, dont le but est d'encourager la recherche sur les domaines numériques abstraits. Le mémoire se conclue par quelques perspectives sur des recherches futures",Analyse statique par interprétation abstraite de programmes concurrents,,HAL CCSD,,,core
41665747,2014-06-06T08:19:20Z,"In this article, a new and efficient multilayer neural networks learning algorithm is presented. The key concept of this new algorithm is the two-stage implementation of the steepest descendant method. At the first stage, the steepest descendant method is used to search the optimal learning constant eta and momentum term alpha for each weights updating process. At the second stage, the Delta learning rule is then employed to modify the connecting weights in terms of the optimal eta and alpha. Computer simulations show that the proposed new algorithm outmatches other learning algorithms both in converging speed and success rate. On real industrial application, we first apply the new neural network learning algorithm to the identification of a highly nonlinear injection molding barrel system. Experimental results demonstrate that the new algorithm can precisely identify the complicate injection molding barrel system. Further more, a self-tuning PID controller for precise temperature control based on the trained neural network barrel model is developed. Real experiments show that the self-tuning PID controller can precisely control the barrel temperature within +/-0.5 degrees C",Cascade steepest descendant learning algorithm for multilayer feedforward neural network,,,"[{'title': None, 'identifiers': ['issn:1340-8062', '1340-8062']}]",,core
45245579,2013,"Traditional methods for bacterial identification include Gram staining, culturing, and biochemical assays for phenotypic characterization of the causative organism. These methods can be time-consuming because they require in vitro cultivation of the microorganisms. Recently, however, it has become possible to obtain chemical profiles for lipids, peptides, and proteins that are present in an intact organism, particularly now that new developments have been made for the efficient ionization of biomolecules. MS has therefore become the state-of-the-art technology for microorganism identification in microbiological clinical diagnosis. Here, we introduce an innovative sample preparation method for nonculture-based identification of bacteria in milk. The technique detects characteristic profiles of intact proteins (mostly ribosomal) with the recently introduced MALDI SepsityperTM Kit followed by MALDI-MS. In combination with a dedicated bioinformatics software tool for databank matching, the method allows for almost real-time and reliable genus and species identification. We demonstrate the sensitivity of this protocol by experimentally contaminating pasteurized and homogenized whole milk samples with bacterial loads of 10(3)-10(8) colony-forming units (cfu) of laboratory strains of Escherichia coli, Enterococcus faecalis, and Staphylococcus aureus. For milk samples contaminated with a lower bacterial load (104 cfu mL-1), bacterial identification could be performed after initial incubation at 37 degrees C for 4 h. The sensitivity of the method may be influenced by the bacterial species and count, and therefore, it must be optimized for the specific application. The proposed use of protein markers for nonculture-based bacterial identification allows for high-throughput detection of pathogens present in milk samples. This method could therefore be useful in the veterinary practice and in the dairy industry, such as for the diagnosis of subclinical mastitis and for the sanitary monitoring of raw and processed milk products",Nonculture-based identification of bacteria in milk by protein fingerprinting,,Wiley-Blackwell,,,core
54575012,2012,"This paper presents an application of data-derived approaches for analyzing and monitoring industrial processes. The discussed methods are used in visualizing process measurements, extracting operational information, and designing estimation models for primary process variables otherwise difficult to measure in real-time. Emphasis is given to the modeling of the data with two classical machine learning paradigms; the self-organizing map (SOM) and the multi-layer perceptron (MLP). The effectiveness of the proposed approach is validated on an industrial deethanizer, where the goal is to identify operational modes and most sensitive variables for this full-scale unit, as well as design an inferential model for a critical process variable, the bottom ethane concentration. The study led to the definition of a fully automated monitoring tool to be implemented online in the plant's distributed control system. The results confirmed the potential of the data-derived approach, and based on the analysis, the existing control configuration of the unit could be redefined toward more consistent operations. Because it is general and modular by design, the tool can be easily used for other processes",Data-derived analysis and inference for an industrial deethanizer,,'American Chemical Society (ACS)',,10.1021/ie202854b,core
231037332,2014-02-28T17:00:00,"Please join us for a panel covering all aspects of patent assertion and non-practicing entities and their effect on the patent industry. Our distinguished panelists are as follows:
Michael D. Friedman is Managing Director at Ocean Tomo, overseeing its Investments practice, which is composed of Investment Banking, Asset Management and Investment Research.
Ocean Tomo’s Investment Banking practice brings IP financing, monetization and capital markets solutions to corporations and other intellectual property owners. Recent notable transactions include the leveraged buyout of Mosaid Technologies and the sale of MIPS Technologies’ IP portfolio. Ocean Tomo Asset Management, where Mr. Friedman serves as Chief Investment Officer, engages in public equity, special situations and private equity investing where intellectual property insight drives alpha creation. Investment Research works in parallel with institutional investors, hedge funds and private equity funds advising them on capital allocations to IP-themed investments.
Mr. Friedman holds a JD from the University of Chicago Law School, where he worked as Research Editor of the University of Chicago Legal Forum. He also holds a BS in marine engineering and nautical science from the U.S. Merchant Marine Academy. Mr. Friedman is a member of the board of directors of the Intellectual Property Exchange International, the world’s first IP-focused financial exchange, and a Lecturer in Law at the University of Chicago Law School.
Jay P. Kesan is a Professor at the University of Illinois, College of Lawwhere he is H. Ross & Helen Workman Research Scholar and Director of the Program in Intellectual Property and Technology Law. Professor Kesan received his J.D. summa cum laude from Georgetown University, where he received several awards including Order of the Coif and served as associate editor of the Georgetown Law Journal. After graduation, he clerked for Judge Patrick E. Higginbotham of the United States Court of Appeals for the Fifth Circuit. Prior to attending law school, Jay Kesan – who also holds a Ph.D. in electrical and computer engineering from the University of Texas at Austin – worked as a research scientist at the IBM T.J. Watson Research Center in New York. He is a registered patent attorney and practiced at the former firm of Pennie & Edmonds LLP in the areas of patent litigation and patent prosecution. In addition, he has published numerous scientific papers, and he has obtained several patents in the U.S. and abroad. His recent publications can be found on SSRN (Social Science Research Network) at http://www.ssrn.com. At the University of Illinois, Professor Kesan is appointed in the College of Law, the Institute of Genomic Biology, the Department of Electrical & Computer Engineering, the Information Trust Institute, the Coordinated Science Laboratory, the College of Business, and the Department of Agricultural & Consumer Economics. Professor Kesan continues to be professionally active in the areas of patent litigation and technology entrepreneurship. He has served as a special master in patent litigations, and has served as a technical and legal expert and/or counsel in patent matters. He also serves on the boards of directors/advisors of start-up technology companies.
Matthew Levy is Patent Counsel at the Computer and Communications Industry Association, where he handles legal, policy advocacy, and regulatory matters related to patents and is lead blogger for CCIA’s Patent Progress.
Matt joined the CCIA in 2013 from the IP boutique Cloudigy Law, PLLC. He has also been an associate at Finnegan, Henderson, Farabow, Garrett, & Dunner, LLP and at Hogan & Hartson LLP. He got first-hand experience in both patent prosecution and patent litigation, including defending clients against patent trolls.
Matt graduated from the Georgetown University Law Center magna cum laude with the Order of the Coif, winning the ABA/BNA Award for Excellence in Intellectual Property. He received a Master’s in Computer Science from the University of Kentucky, where he won the Presidential Fellowship twice. His research at UK was in computational complexity theory and artificial intelligence. He received a Bachelor’s degree in Computer Science from the University of Southern Maine.
Before law school, Matt was a software engineer at IBM in Lexington, KY, as part of the team that developed and maintained Lotus Sametime, IBM’s real-time messaging and conferencing product. He is co-inventor on U.S. Patent No. 8,521,830.
Matt is still a software developer in his spare time. He developed an app for the iPad, Federal Local Rules, which is available on the App Store.
Laura Beth Miller is a shareholder at Brinks Gilson & Lione, where she co-chairs the firm’s practice before the U.S. International Trade Commission (“ITC”). With over two decades of trial and arbitration experience, Ms. Miller has handled substantial first and second chair responsibilities. She focuses her practice on patent, trade secret and trademark issues, as well as client counseling on complex commercial issues, including licensing, anti-trust and contract issues affecting business operations, product services and technology. In addition to representing major Fortune 500 companies, she is an adjunct professor at The John Marshall Law School, in Chicago, Illinois. She is a frequent speaker on intellectual property issues both in the United States and abroad, and has written a number of articles on intellectual property topics.
Ms. Miller received her B.A. from the University of Virginia and her J.D. from The College of William and Mary Marshall Wythe School of Law. She is licensed to practice before the United States Supreme Court, the Supreme Court of Illinois, the United States Patent and Trademark Office, and numerous federal courts. She has been recognized as one of Illinois\u27 leading intellectual property lawyers by Chambers USA, and has been named a Leading Intellectual Property Lawyer and one of the Top 50 Women Business Litigation Lawyers in Illinois by the Leading Lawyers Network. She also serves on the management teams at Brinks Gilson & Lione.
K. McNeill Taylor, Jr., is General Counsel at Round Rock Research, LLC. Neill Taylor joined Round Rock in 2012 as Vice President Law and General Counsel responsible for supervising and administering all legal affairs for the company.
Before joining Round Rock, Neill was Corporate Vice President and Chief IP Counsel of Motorola Mobility, Inc., responsible for the intellectual property law and litigation functions. He managed the offensive and defensive patent litigation in support of MMI’s Android smart phones, and the preparation, prosecution and legal support for MMI’s patent portfolio of approximately 24,000 patents and applications worldwide. Neill had a leading role in setting the strategy for and negotiating MMI’s $12.5B acquisition agreement with Google in August 2011.
After joining Motorola, Inc. in 2002, Neill led its efforts to protect intellectual property for a number of Motorola businesses through patent operations, licensing, in-business counseling, defensive matters and litigation. He also served as general counsel in the integration of Symbol Technologies, Inc. after its $4B acquisition by Motorola in January 2007.
Prior to joining Motorola, Neill served as vice president, general counsel and assistant secretary of Corning Cable Systems and Siecor Corp. Before that he held patent counsel positions with Corning Inc. and Schlumberger Ltd., and began his patent law career as an associate with Fish & Neave, a patent litigation firm in New York.
Neill received his law degree from the University of Chicago and a bachelor’s degree in physics and philosophy from Duke University, where he graduated magna cum laude and was an Angier B. Duke scholar.
Andrew W. Williams is a partner with McDonnell Boehnen Hulbert & Berghoff LLP. Dr. Williams\u27 practice primarily consists of patent litigation, prosecution, and opinion work in the areas of biotechnology, pharmaceuticals, and chemistry. Dr. Williams is a contributing author to the Patent Docs weblog, a site focusing on biotechnology and pharmaceutical patent law. Dr. Williams earned his Ph.D. in Molecular Biophysics and Biochemistry at Yale University. He earned his J.D. from George Washington University Law School with highest honors, and was Managing Editor of the law review",Patent Assertion and Non-Practicing Entities Panel,,Northwestern Pritzker School of Law Scholarly Commons,,,core
53344724,2014-01-01T00:00:00,"Computer generated images have always drawn the attention of millions of people in their different roles: students, professors, researchers, designers, movie directors, etc. It is not only the magic of generating synthetic, static and/or dynamic images, emulations or simulations of reality, but of capturing all the creative abilities of the human being to narrate events, from the memory of the past, descriptions of current reality and projections towards the future. In other words, they may be timeless images and malleable towards the infinite.

In them rests the freedom of expression in a chromatic and three-dimensional way where sometimes it is difficult to see whether we are in the face of something real or virtual. It is the charm of the viewing power of communication where an image is always worth more than a thousand words. Not for nothing when the first graphics appeared to depict the numerical information in the offices, it marked the beginning of the use of computers in the businesses with professional purposes until they reached the home. It was these synthetic images which collaborated to change the meaning of the PC initials, that is, from professional computer to personal computer.

The risk lies in the fact that those who manage those images respect the final viewer, at the moment they are presented to his/her eyes. That is, a transparent communication avoiding manipulation through them. If this doesn\u2019t happen, the written word takes again a prevailing place in the communicative process, and one goes back to the time of the appearance of print. It is in this regressive period where the human and social factors may be inserted, which generate different types of structures which do not change over time.

Although we are in one of the most dynamic sectors of the current interactive systems, we may come across in some places of the Old Continent with dogmas and their supporters, who slow down the whole creative process which entails working with these computer-made images. Now if the human and social factors are positive, in view of the constant advances of the graphic hardware and software, the only limit that exists is the imagination or originality at the moment of generating static and/or dynamic images in 2D and 3D.

Computer graphics and computer animation along with text, audio and video have been key in the momentum of the Internet phenomenon of the 20th century. In the new millennium they have been the main means of drawing the attention of the users to the screens of a great variety of devices of classical computing.

There are millions of users who decide to purchase computer equipment because of the images that they can see on the screens. These images gain in quality thanks to their generation from scratch and thanks also to myriad techniques, methods, algorithms, etc. related to computer graphics down to the access to the information stored in the database, such as decompressed files with multimedia information available via the interaction of the user through the various peripherals.

Virtual images are generated in 2D and/or 3D, thanks to democratization of the pixels in 1990 \u2013 2000. Users are now capable of generating images in movement for 3D starting from photographs with digital cameras or the commercial applications to generate films with computer animations.

In this whole productive process, whether it is at a personal or an industrial level the cost factor is always present, in the equation of maximum quality in the least possible time. To reach that goal it is necessary to resort to several quality attributes that have an influence from the design stage right until the programming of the applications used in the context of graphic informatics.

Our intention in the current handbook is to show an essential part of those strategies and also the latest breakthroughs in all that which is directly or indirectly related to computer graphics, computer animation, database, software quality, hypermedia, design, communicability, interfaces, cloud computing, augmented reality, human-computer interaction, computer-aided design, mixed reality, models, techniques and methods of computer science, etc.

Some of the works that make up the current compendium have been presented orally by their authors in the following international conference in Venice, Italy: SETECEC 2012 \u2013First International Conference on Software and Emerging Technologies for Education, Culture, Entertainment, and Commerce: New Directions in Multimedia Mobile Computing, Social Networks, Human-Computer Interaction and Communicability, and international symposium in Valle d'Aosta, Italy: CCGIDIS 2012 \u2013Second International Symposium on Communicability, Computer Graphics and Innovative Design for Interactive Systems.

The innovation and originality of those proposals has been the reason for which the authors have been invited to enlarge their works submitting again the new versions to an assessment process of said works by the members of the scientific committee. Consequently, these are works that have gone satisfactorily through a double process of international selection. In the next section a short introductory presentation of the research works that make up the current compendium is given:

The authors Luigi Barazzetti and  Marco Scaioni present an interesting work \u201cTransforming Images and Laser Scans into 3D Models\u201d where theory and practice converge in the context of 3D images. They introduce all details of the use of a laser scanner to obtain high quality three-dimensional images and accuracy in the maintenance and the virtual reconstruction of cultural heritage, for instance. Besides, they comprehensively describe computer vision and photogrammetric 3D modeling techniques that are mainly based on images or laser scans. The description further covers the integration of global navigation satellite system (GNSS) or theodolite data allowing precise geo-referencing. The main goal they have set themselves is to indicate the reliability and robustness of their combined use for real surveys. Simultaneously a set of real examples serves to illustrate each one of the advantages and disadvantages of the used methods and techniques. In these examples the resolution of complex problems and with reduced costs can be verified. Each one of the issues is approached in a didactic way which facilitates the understanding even of the diverse devices used for the measurements.

Chih-Fang Huang, Chih-Hsiang Liang, and En-Ju Lin are the authors of the chapter \u201cSound-Color Synaesthetic Effect Using Algorithmic Composition with Image for Emotional Release.\u201d In said research they present the qualities of music from an emotive point of view. This is one of the motivations why they spread their study to the field of colors. In this sense, they develop an emotional model for music based on studies of color theory. Another main goal is to present the emotions of the users of a special software starting from the color and the electronic sounds that these represent from the psychological point of view, an approach informed by studies belonging to the set of the music color synesthesis and emotion releasing effect. Throughout the research work the authors explain in a gradual, simple and thorough way each one of the presented concepts with their matching theoretical examples and also the use of a special software for the experiments they have carried out.

The research work \u201cUser-created Interior Design Service Concepts, Interfaces and Outlines for Augmented Reality\u201d has been made by the following authors Tiina Kym\ue4l\ue4inen and Sanni Siltanen. This research work presents us masterfully a design process and the implementation requirements of an interactive interior design system. In said system the use of two focus groups can be detected where the knowledge and the experiences of designers, bloggers and serious amateurs in the field of interior design interact transversally. In that transversal interaction of diverse professionals the presence of user-driven innovation can be seen. The strategies of the conformation of the professionals and the transversal knowledge to the different areas of knowledge are explained with examples. Examples are given, which describe all the aspects of the new technologies applied to augmented reality, the results of tests with real users, the realization of a special software for the three-dimensional visualization of interiors, etc. Each one of the stages of the research project is accompanied by its corresponding textual and graphical explanations. Finally the authors make a wide-ranging reflection about the learned lessons signaling the positive and negative aspects of the used technology and also the future lines of research before their conclusions.

In the context of artificial intelligence, Damijan Novak and Domen Verber show us in their research \u201cNew Generation of Artificial Intelligence for Real-time Strategy Games\u201d the importance of applying it to computer games. The text starts with a complete state of the art overview, where the main notions to which the current research work refers are defined. In it is a constant interrelation among real-time strategy, games and artificial intelligence. The authors signal the importance of these issues within the academic field since they have detected new fields for research, whether in the present or the immediate future. Besides, in this work an excellent comparison example in the open-source real time strategy game development tools can be seen. The main and secondary goals of the text are gradually developed answering to a set of rhetorical questions with practical examples. All of this makes the reading and understanding easier to the potential readers. Their proposal of an enhanced combat artificial intelligence algorithm is striking in a very positive way.

The authors of the chapter \u201cTowards Smart Mobile System for Public Bus Transportation\u201d are Mitja Krajnc, Vili Podgorelec, and Marjan Heri\u10dko. In it converge the use of multimedia mobile phones and the geographical information systems (GPS satellite) for the localization of public buses in the town of Maribor, Slovenia. The system allows the user to see the movement of the buses in the road network through a real-time visualization on a mobile phone. The digital information which replaces the analogical one such as the timetables of the buses on paper support increases the available options of those users to access the public transportation system such as the combination of buses to reach a given destination. Besides, ease of use of the system can be seen since it is based on Windows Phone. Finally a quality attribute of the interactive systems such as the prediction has been incorporated to the system. The goal pursued by its authors is to increase the alternatives the user has at the moment of accessing the interactive information without this having negative repercussions on the visualization of the information inside the interface of the mobile phone.

Under the title \u201cA Learning Environment based on Movement and Sound Interaction\u201d its authors Serena Zanolla, Sergio Canazza, Antonio Rod\ue0, Giovanni De Poli, and Gian Luca Foresti present an Interactive Multimodal Environment for E-learning called \u201cStanza Logo-Motoria.\u201d The experiments carried out with children for the learning of a second language such as English have allowed the authors to carry out several experiments with positive results. A special module has been developed and tested in this sense called English as a Second Language (ESL). The motivation for learning languages in an environment of these characteristics is presented in a detailed way with a special stress on other aspects such as the socialization of early age users through new interactive technologies. Along with this the authors introduce technological aspects of interactive design which have been evolving as the experiments were carried out.

Michele Argiolas, Claudia Loggia, Vittorio Tramontin, and Cristina Trois are the authors of the chapter called \u201cA Web Application to Support Decision Making Process Based on a Bottom-up Approach in Public Buildings\u2019 Green Retrofit.\u201d They introduce a GIS web application related to such topics as green retrofit, public buildings, cost management just to mention three examples. The proposal starts with a comprehensive state of the art in legislation issues as technique of the main and secondary topics approached by its authors. The work shows how the implemented system online may serve to curtail the costs of management of green retrofitting public buildings. The study shows a constant interrelation among several disciplines of the sciences. Each of the issues is very well referenced in the bibliography section and they are developed in a pleasant way for the reader non-specialized in the issues approached at the start. Besides, there is a comparative study of real cases which entail additional advantages to the work developed by each one of the authors of the current text. Finally we can state that we are in front of an excellent triadic example of interrelations among databases, access to the stored information and the management of public administration.

The wide range of the phenomenon of the social networks requires in many of the offered services a set of recommendations. In this sense the authors Sa\u161o Karakati\u10d, Vili Podgorelec, and Marjan Heri\u10dko show in their research work \u201cMerging Social Networks and Semantic Databases to Build Recommendation Service\u201d the potential of linking semantic databases and social networks. Starting from Facebook and Freebase they have fashioned a prototype that uses Java technology. The prototype takes into consideration each one of the key elements for the recommendation of an online service to the potential users of contents in TV format, videogames, music, etc. In this sense we see a novel low cost solution is little by little brought forward by the authors along the pages. Besides, they have made a series of tests with users at the moment of interacting with the software and the hardware used. The results obtained back up the transcendence of the current proposal, and also the future developments they will make in a short time.

The three-dimensional reconstruction of the religious-historical heritage in the province of Avila (Spain) is the main objective of \u201cComputer Graphics to Encourage the Participation in the Virtual Reconstruction of Cultural Heritage: The Example of the Monastery of  Nuestra Se\uf1ora del Risco\u201d author of the chapter whose title is Gonzalo Mart\uedn S\ue1nchez. Starting from a reality which reveals the passing of time and the scarce remains of the walls of the monastery, the author resorts to several techniques using static images in 2D and 3D, such as historical cartography, digital photography, commercial software and open source, etc., for three-dimensional reconstruction. Now the author points out that these techniques from the field of computer graphics must be accompanied by a previous historical study of that is intended to be rebuilt. In this sense the illustrations, as well as the used methodology can serve as a guide for other analogous works.

In \u201cDeveloping Design Guidelines for E-Learning Environments: A War Story\u201d its authors Laura Benvenuti, Maria Menendez Blanco, and Gerrit C. van der Veer, denote the importance of the design, development, evaluation of a system oriented at university e-learning. A comprehensive study of the educational system to be developed with Moodle highlights the intersection between pedagogical theory, interactive design computer programming, among other areas of knowledge when it refers to blended academic education. A course on webculture has allowed the authors to carry out analyses of the web design. The university students belonging to the social sciences (culture, psychology, etc.) have taken part in several processes of heuristic evaluation to reach the goals proposed by the authors of the current research work. The learned lessons, future lines of research and the conclusions are interesting sections to understand the acquired experiences and for new research in the context of the blended education, distance learning, Internet based learning environments to mention a few examples.

The author presents a heuristic study of the evolution of human factors with their educational and social consequences in the context of the pixel of Southern Europe during the years 1990 \u2013 2012, under the title \u201cPixels: Educational Structures and Power Systems.\u201d Simultaneously, it is used for the first time a special language where diachronism interacts with the synchronism of the real examples that are presented, related to the graphics software. Besides, in this first research work are combined different techniques of the social sciences to establish the nodes and links which make up the human and functional structure of all that related to the commercial and educational pixel. Lastly, a series of strategies is put forward to analyse the educational quality of some training centers with regard to computer graphics and its derivations","A Learning Environment
Based on Movement and Sound Interaction",,place:Bergamo,,,core
52476673,2013-02-01T00:00:00,"In this paper an ethanol reformer based on catalytic steam reforming with a catalytic honeycomb loaded with RhPd/CeO2 and palladium separation membranes with an area of 30.4 cm2 has been used to generate a pure hydrogen stream of up to 100 ml/min to feed a PEM fuel cell with an active area of 5 cm2. The fuel reformer behavior has been extensively
studied under different temperature, ethanolewater flow rate and gas pressure at a fixed S/C ratio of 1.6 (molar). The hydrogen yield has been controlled by acting upon the ethanol-water fuel flow and gas pressure.
A mathematical model of the ethanol reformer has been developed and an adaptive and predictive control has been implemented on a real time system to take account of its nonlinear behavior. With this control the response time of the reformer can be reduced by a factor of 7 down to 8 s.
The improved dynamics of the controlled reformer match better the quickly changing hydrogen demands of fuel cells. They reached a magnitude where costly hydrogen buffers between the reformer and the fuel cell can be omitted and an electric buffer at the output of the fuel cell is sufficient.Fil: Koch, Reinhold. Universitat Technical Zu Munich;Fil: Lopez, Eduardo. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico - CONICET - Bahia Blanca. Planta Piloto de Ingenieria Quimica (i); Argentina;Fil: Divins, Núria J.. Institut de Têcniques Energetiques, Universitat Politecnica de Catalunya, España;Fil: Allué, Miguel. Institut de Robótica i Informática Industrial; España;Fil: Jossen, Andreas. Institute for Electrical Energy Storage Technology. Technische Universitat Munchen; Alemania;Fil: Riera, Jordi. Institut de Robótica i Informática Industrial; España;Fil: Llorca, Jordi. Institut de Têcniques Energetiques, Universitat Politecnica de Catalunya, España",Ethanol catalytic membrane reformer for direct PEM FC feeding,https://core.ac.uk/download/52476673.pdf,Pergamon-elsevier Science Ltd,"[{'title': 'International Journal of Hydrogen Energy', 'identifiers': ['0360-3199', 'issn:0360-3199']}]",,core
212803197,2012-12-01T00:00:00,"Generating and representing knowledge about heuristics for repair-based scheduling is a key issue in any rescheduling strategy to deal with unforeseen events and disturbances. Resorting to a feature-based propositional representation of schedule states is very inefficient and generalization to unseen states is highly unreliable whereas knowledge transfer to similar scheduling domains is difficult. In contrast, first-order relational representations enable the exploitation of the existence of domain objects and relations over these objects, and enable the use of  quantification over objectives (goals), action effects and properties of states. In this work, a novel approach which formalizes the re-scheduling problem as a Relational Markov Decision Process integrating first-order (deictic)representations of (abstract) schedule states is presented. Task rescheduling is solved using a relational reinforcement learning algorithm implemented in a real-time prototype system which makes room for an interactive scheduling strategy that successfully handle different repair goals and disruption scenarios. An industrial case study vividly shows how relational abstractions provide compact repair policies with minor computational efforts.Fil: Palombarini, Jorge Andrés. Universidad Tecnologica Nacional. Facultad Regional Villa Maria; ArgentinaFil: Martínez, Ernesto Carlos. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Santa Fe. Instituto de Desarrollo y Diseño. Universidad Tecnológica Nacional. Facultad Regional Santa Fe. Instituto de Desarrollo y Diseño; Argentin",Task Rescheduling using Relational Reinforcement Learning,,'IBERAMIA: Sociedad Iberoamericana de Inteligencia Artificial',"[{'title': 'INTELIGENCIA ARTIFICIAL', 'identifiers': ['1137-3601', 'issn:1988-3064', '1988-3064', 'issn:1137-3601']}]",,core
44337735,2013-01-01T00:00:00,"Stochastic Recursive Algorithms for Optimization presents algorithms for constrained and unconstrained optimization and for reinforcement learning. Efficient perturbation approaches form a thread unifying all the algorithms considered. Simultaneous perturbation stochastic approximation and smooth fractional estimators for gradient- and Hessian-based methods are presented. These algorithms: • are easily implemented; • do not require an explicit system model; and • work with real or simulated data. Chapters on their application in service systems, vehicular traffic control and communications networks illustrate this point. The book is self-contained with necessary mathematical results placed in an appendix. The text provides easy-to-use, off-the-shelf algorithms that are given detailed mathematical treatment so the material presented will be of significant interest to practitioners, academic researchers and graduate students alike. The breadth of applications makes the book appropriate for reader from similarly diverse backgrounds: workers in relevant areas of computer science, control engineering, management science, applied mathematics, industrial engineering and operations research will find the content of value",Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods,,'Springer Science and Business Media LLC',,10.1007/978-1-4471-4285-0,core
76383851,2013-01-01T08:00:00,"Computational fluid dynamics (CFD) is one of the branches of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the millions of calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. Indoor airflow simulations are necessary for building emergency management, preliminary design of sustainable buildings, and real-time indoor environment control.
The simulation should also be informative since the airflow motion, temperature distribution, and contaminant concentration is important. However, CFD computation is usually time-consuming, and not suitable for simulating real-time indoor air movement. Many researchers are concentrating on both hardware utilization and CFD algorithms, to make simulation much faster. Fast flow simulations are important for some applications in the building industry, such as the conceptual design of indoor environment, or they are coupled with energy simulation to provide deep analysis on the performance of the buildings. Such application does not require the same high level of accuracy as traditional CFD simulation because it only requires conceptual or semi-accurate distributions of the flow but within a short computing time. However, year round simulation is needed rather than the analysis of two or three extreme cases in order to help the designer investigate the problem clearly. To meet these special needs, an efficient and informative fluid simulation method is needed to provide fast airflow simulation with an inevitable but nominal compromise in accuracy.
This research provides a comprehensive workflow for the designer to simulate and analyze the annual indoor environment. In addition to the hardware acceleration deployed, fast fluid simulation algorithm is developed, and a machine learning based interpolation is used to allow the simulation coverage to be conducted annually. The outcome of this research is a methodology that allows the annual simulation time similar to the one used to perform two or three extreme cases of simulation using current methods",A Three Layered Framework for Annual Indoor Airflow CFD Simulation,https://core.ac.uk/download/76383851.pdf,ScholarlyCommons,,,core
82735414,2012-12-31,"AbstractThis paper presents an overview of Proportional Integral control (PI) and Artificial Intelligent control (AI) algorithms. AI and PI controller are analyzed using Matlab [Simulink] software. The DC motor is an attractive piece of equipment in many industrial applications requiring variable speed and load characteristics due to its ease of controllability. The main objective of this paper illustrates how the speed of the DC motor can be controlled using different controllers. The simulation results demonstrate that the responses of DC motor with an AI control which is Fuzzy Logic Control shows satisfactory well damped control performance. The results shows that Industrial DC Motor model develop using its physical parameters and controlled with an AI controller give better response, it means it can used as a controller to the real time DC Moto",Modelling and Simulation for Industrial DC Motor Using Intelligent Control ,https://core.ac.uk/download/pdf/82735414.pdf,Published by Elsevier Ltd.,,10.1016/j.proeng.2012.07.193,core
198967451,2012-01-01T00:00:00,"International audiencePurpose– Different machines are already present in the human environment, easing human beings' daily life. In the future, this tendency will be accentuated by integration of numerous robots (e.g. wheeled robots, legged robots, humanoid robots, network sensors, etc.) in the human environment. A wide range of applications, such as those dealing with warehouse management, industrial assembling, military applications, daily‐life tasks, can benefit from multi‐robot systems. The purpose of this paper is to propose an intelligent system for industrial robotics in the logistic field, based on collaboration between heterogeneous robots.Design/methodology/approach– The proposed infrastructure for this multi‐robot system is composed of a robots' network including one humanoid robot, wheeled robots, cameras, and remote computer. All devices can communicate between them by using wireless network. The goal of the humanoid robot is to lead the wheeled robots according to the environment and wheeled robots are used to carry a load. The camera allows providing complementary information about the environment; and thanks to machine learning, this control strategy allows complex tasks to be perormed for these logistic applications.Findings– This concept is implemented on real robots within the frame of a demonstrator including the above‐mentioned kind of robots. The preliminary results, obtained during experimentations, prove the feasibility of the presented strategy for real applications.Originality/value– The main originalities of this work are, on the one hand, the use of an heterogeneous multi‐robots system for logistic tasks, and on the other hand, the proposed machine learning allows a collaboration task between heterogeneous robots in an autonomous manner",Intelligent systems for Industrial Robotics: Application in Logistic Field,,'Emerald',,10.1108/01439911211217071,core
44623782,2014-01-01T00:00:00,"Artículo de publicación ISIThe capability to generate complex geometry features
at tight tolerances and fine surface roughness is a key
element in implementation of Creep Feed grinding process in
specialist applications such as the aerospace manufacturing
environment. Based on the analysis of 3D cutting forces,
this paper proposes a novel method of predicting the profile
deviations of tight geometrical features generated using
Creep Feed grinding. In this application, there are several
grinding passes made at varying depths providing an incremental
geometrical change with the last cut generating the
final complex feature. With repeatable results from coordinate
measurements, both the radial and tangential forces can
be gauged versus the accuracy of the ground features. The
results of the tangential force were found more sensitive to
the deviation of actual cut depth from the theoretical one.
However, to make a more robust prediction on the profile
deviation, its values were considered as a function of both
force components. In addition, the power signals were obtained
as these signals are also proportional to force and
deviation measurements. Genetic programming (GP), an
evolutionary programming technique, has been used to
compute the prediction rules of part profile deviations based
on the extracted radial and tangential force and correlated
with the initial “gauging” methodology. It was found that
using this technique, complex rules can be achieved and
used online to dynamically control the geometrical accuracy
of the ground features. The GP complex rules are based on
the correlation between the measured forces and recorded
deviation of the theoretical profile. The mathematical rules
are generated from Darwinian evolutionary strategy which
provides the mapping between different output classes. GP works from crossover recombination of different rules, and
the best individual is evaluated in terms of the given best
fitness value so far which closes on an optimal solution.
Once the best rule has been generated, this can be further
used independently or in combination with other close-tobest
rules to control the evolution of output measures of
machining processes. The best GP terminal sets will be
realised in rule-based embedded coded systems which will
finally be implemented into a real-time Simulink simulation.
This realisation gives a view of how such a control
regime can be utilised within an industrial capacity. Neural
networks were also used for GP rule verification.The experimental work was carried out at The University of
Nottingham funded by EPSRC",The prediction of profile deviations when Creep Feed grinding complex geometrical features by use of neural networks and genetic programming with real-time simulation,,'Springer Science and Business Media LLC',,10.1007/s00170-014-5829-0,core
49715393,2013-11-28T00:00:00,"This report presents the bulk of my research work from the completion of my PhD, in late 2004, until the present day. The overall aim of my research is the development of mathematically sound and practically efficient methods to check the correctness of computer software. Efficiency is achieved using approximations, while soundness is guaranteed by employing over-approximations of program behaviors. My research is grounded in the theory of abstract interpretation, a powerful mathematical framework facilitating the development, use, comparison, and composition of approximations in a sound way. I am mainly interested in developing new reusable abstraction components (so called abstract domains) that can be readily implemented, and in using them to develop static analyzers, which are computer programs able to check automatically the safety of software. While my early research was focused on inferring the values of variables in sequential programs, my current interest and latest results concern the analysis of concurrent programs, hence the title of this report. The first two chapters of this report constitute an introduction. The first chapter is an informal introduction to the problem at hand, existing solutions, their strengths and their shortcomings. The second chapter presents prior mathematical and formal tools on which our work is based, including some notions of abstract interpretation, a description of existing abstract domains and their application to the static analysis of sequential programs. It also recalls some results I obtained during my PhD and that will be useful in the rest of the report. The subsequent chapters describe the work I performed after completing my PhD. The third chapter is devoted to aspects of static analyzers that are specific to concurrent programs. This topic of personal research has led to the construction of a generic analysis method for concurrent programs, parametrized by the choice of abstract domains. The method is based on a notion of ''interference'' that abstracts thread interleavings in a sound way in order to achieve a thread-modular analysis. It is related to Jones' rely-guarantee proof method, and we make this connection formal in a first part. Then, we present an interference-based analysis in big-step form that is efficient and easy to implement. In a third part, we study the interaction of the analysis with weakly consistent memory models, found in modern processors and language specifications. The last part discusses how to adapt the analysis to exploit some properties of the scheduling (such as the use of real-time thread priorities and synchronization primitives). The fourth and fifth chapters are devoted to the design of abstract domains. Although some of them found their application in the analysis of concurrent programs, they are actually generic and could be exploited in any kind of static analysis, for concurrent or sequential programs. The fourth chapter concerns numeric domains to infer linear equality and inequality relations, developed in collaboration with Liqian Chen while he visited ENS during his PhD. The initial motivation was to revise the classic polyhedra domain using sound floating-point arithmetic to improve its efficiency, but it unexpectedly yielded the construction of new, more expressive domains based on interval affine relations, which we also present. The fifth chapter concerns the abstraction of realistic data-types as found in the C programming language, including machine integers, floating-point numbers, and structured blocks of memory (structs, unions, and arrays). We design abstractions that are aware of the low-level memory representation of data-types, to support the analysis of programs that rely on assumptions about this representation (such as ''type punning'' constructions in C). The need for such abstractions was motivated by the analysis, in the scope of the Astrée and AstréeA static analyzers, of industrial C programs, where such low-level constructions are widespread. The sixth chapter is devoted to the application of these methods to the design of static analyzer tools. It mainly reports on my experience with the Astrée analyzer, a team effort initiated during my PhD in 2001 that extended well beyond it and culminated in its industrialization in 2009. Much of my theoretical work could find some application in Astrée, as Astrée fuelled my research with not only practical problems to solve, but also concrete problems that could only be overcome by theoretical developments. This part also reports my own ongoing effort on AstréeA, an extension of Astrée that incorporates the interference abstraction presented above and aims at proving the absence of run-time error in concurrent embedded programs (while Astrée only considers synchronous programs). Additionally, this chapter presents the Apron abstract domain library, another, more academic, team effort, which aims at encouraging the research on numeric abstract domains. The report concludes with some perspectives for future researches.Ce mémoire d'habilitation résume la majeure partie de mes recherches, depuis la fin de mon doctorat, fin 2004, jusqu'à aujourd'hui. Le but essentiel de mes recherches est le développement de méthodes fondées sur des bases mathématiques et performantes en pratique pour s'assurer de la correction des logiciels. J'utilise des approximations pour permettre une bonne performance, tandis que la validité des résultats est garantie par l'emploi exclusif de sur-approximations des ensembles des comportements des programmes. Ma recherche est basée sur l'interprétation abstraite, une théorie très puissante des approximations de sémantiques permettant aisément de les développer, les comparer, les combiner. Je m'emploie en particulier au développement de nouveaux composants réutilisables d'abstraction, les domaines abstraits, qui sont directement implantables en machine, ainsi qu'à leur utilisation au sein d'analyseurs statiques, qui sont des outils de vérification automatique de programmes. Mes premières recherches concernaient l'inférence de propriétés numériques de programmes séquentiels, tandis que mes recherches actuelles se tournent vers l'analyse de programmes concurrents, d'où le titre de ce mémoire. Les deux premiers chapitres de ce mémoire constituent une introduction, tandis que les suivants présentent mon travail d'habilitation proprement dit. Le premier chapitre est une introduction informelle à la problématique de l'analyse de programmes, aux méthodes existantes, leurs forces et leurs faiblesses. Le deuxième chapitre présente de manière formelle les outils dont nous aurons besoin par la suite : les bases de l'interprétation abstraite, quelques domaines abstraits existants et la construction d'analyses statiques par interprétation abstraite, ainsi que quelques résultats utiles que j'ai obtenu en doctorat. Le troisième chapitre est consacré aux aspects spécifiques de l'analyse de programmes concurrents. Cette recherche, très personnelle, a abouti à la construction d'une méthode d'analyse de programmes concurrents, paramétrée par le choix de domaines abstraits, et basée sur une notion d'interférence abstrayant les interactions entre threads. Ainsi, l'analyse construite est modulaire pour les threads. Cette méthode est reliée aux preuves rely-guarantee proposées par Jones, ce que nous montrons formellement dans une première partie. Nous construisons ensuite une analyse à grands pas basée sur les interférences, efficace et facile à implanter. Les deux dernière parties étudient les liens entre l'analyse et les modèles mémoires faiblement cohérents (désormais incontournables) ainsi que le raffinement de l'analyse pour tenir compte des propriétés spécifiques des ordonnanceurs temps-réels (nous étudions en particulier l'effet des priorités des threads et l'emploi d'objets de synchronisation). Le quatrième et le cinquième chapitres sont consacrés à la constructions de domaines abstraits. Ceux-ci ne sont pas spécifiquement liés au problème de la concurrence ; ils sont utiles à l'analyse de tous programmes, séquentiels comme concurrents. Le chapitre 4 étudie des domaines numériques inférant des égalités et inégalités affines, développés en collaboration avec Liqian Chen, alors doctorant en visite à l'ENS. La motivation première était l'emploi de nombres à virgule flottante afin d'améliorer l'efficacité du domaine des polyèdres, mais ces travaux ont également débouché sur la découverte de nouveaux domaines, basés sur les relations affines à coefficients intervalles, que nous présentons également. Le chapitre 5 étudie les abstractions de types de données réalistes, comme ceux rencontrés dans le langage C : les entiers machines, les nombres à virgule flottante, et les blocs structurés (tableaux, structures, unions). Nos abstractions modélisent finement les détails de l'encodage en mémoire des données afin de permettre l'analyse de programmes qui en dépendent (par exemple, ceux utilisant le type-punning). Ces abstractions sont motivées par nos expériences d'analyses, avec les outils Astrée et AstréeA, de programmes C industriels ; ceux-ci employant fréquemment ce type de constructions de bas niveau. Le sixième chapitre est consacré aux applications des méthodes présentées ci-dessus à la construction d'outils d'analyse statique. Il décrit en particulier mon travail sur l'outil Astrée que j'ai co-développé avec l'équipe Abstraction pendant et après mon doctorat, et qui a été industrialisé en 2009. Mes résultats théoriques et appliqués ont contribué au succès d'Astrée, tandis que celui-ci m'a fourni de nouveaux thèmes de recherches, sous la forme de problèmes concrets dont la résolution n'a pu se faire que grâce à des développements théoriques. Ce chapitre décrit également AstréeA, une extension d'Astrée utilisant l'abstraction d'interférences proposée plus haut pour l'analyse de programmes concurrents (Astrée étant limité aux programmes séquentiels). Il décrit également Apron, une bibliothèque de domaines abstraits numériques que j'ai co-développée. Il s'agit d'un outil plus académique, dont le but est d'encourager la recherche sur les domaines numériques abstraits. Le mémoire se conclue par quelques perspectives sur des recherches futures",Analyse statique par interprétation abstraite de programmes concurrents,,HAL CCSD,,,core
20713808,2008-04-03,"Abstract: Changes in the natural environment affect our quality of life. Thus, government, industry, and the public call for integrated environmental management systems capable of supplying all parties with validated, accurate and timely information. The ‘near real-time ’ constraint reveals two critical problems in delivering such tasks: the low quality or absence of data, and the changing conditions over a long period. These problems are common in environmental monitoring networks and although harmless for off-line studies, they may be serious for near real-time systems. In this work, we discuss the problem space of near real-time reporting Environmental Management Systems and present a methodology for applying agent technology this area. The proposed methodology applies powerful tools from the IT sector, such as software agents and machine learning, and identifies the potential use for solving real-world problems. An experimental agent-based prototype developed for monitoring and assessing air-quality in near real time is presented. A community of software agents is assigned to monitor and validate measurements coming from several sensors, to assess air-quality, and, finally, to deliver air quality indicators and alarms to appropriate recipients, when needed, over the web. The architecture of the developed system is presented and the deployment of a real-world test case is demonstrated. Keywords: Agent-based systems; Environmental monitoring systems; Decision support systems ",Applying agent technology in Environmental Management Systems under real-time constraints,,,,,core
15351820,2010,"Software used by architectural and industrial designers has shifted from becoming a tool for drafting, towards use in verification, simulation, project management and remote project sharing. In more advanced models, design parameters for the designed object can be adjusted so that a family of variations can be produced rapidly. With the advances in computer aided design (CAD) technology, design options can now be generated and analyzed in real time. However the use of digital tools to support design as an activity is still at an early stage and has largely been limited in functionality with regard to the design process. To date, major CAD vendors have not developed an integrated tool that is able to leverage specialised design knowledge from various discipline domains (known as expert knowledge systems) as well as to support the creation of design alternatives that satisfy different forms of constraints. We propose that evolutionary computing and machine learning be linked with parametric design techniques in order to monitor a designeris cognition and intent based on their design history. This will lead to results that impact future work on design support systems which are capable of supporting implicit constraint and problem definition for wicked problems that are difficult to quantify","Patterns, heuristics for architectural design support: making use of evolutionary modelling in design",,,,,core
215212198,2009-01-01T00:00:00,"Recent developments in computing and technology, along with the availability of large amounts of raw data, have contributed to the creation of many effective techniques and algorithms in the fields of pattern recognition and machine learning.  Some of the main objectives for developing these algorithms are to identify patterns within the available data or to make predictions, or both. Great success has been achieved with many classification techniques in real-life applications.  Concerning binary data classification in particular, analysis of data containing rare events or disproportionate class distributions poses a great challenge to industry and to the machine learning community.  This study examines rare events (REs) with binary dependent variables containing many times more non-events (zeros) than events (ones).  These variables are difficult to predict and to explain as has been demonstrated in the literature.  This research combines rare events corrections on Logistic Regression (LR) with truncated-Newton methods and applies these techniques on Kernel Logistic Regression (KLR).  The resulting model, Rare-Event Weighted Kernel Logistic Regression (RE-WKLR) is a combination of weighting, regularization, approximate numerical methods, kernelization, bias correction, and efficient implementation, all of which enable RE-WKLR to be at once fast, accurate, and robust",Robust Weighted Kernel Logistic Regression in Imbalanced and Rare Events Data,https://core.ac.uk/download/215212198.pdf,,,,core
24329227,2007-11-22,"This paper presents an AI-planning-based framework to support the activities of a human operator in a supervisory control system. The framework uses an AI planning and learning substrate architecture and is designed for integration within a general third-party real time software. Our goal is to build a test-bed architecture for research in AI planning applications, such as electrical and industrial processes. AI planning techniques, as opposed to the more traditionally used rule-based systems, can be useful in the automation of the supervision of process systems, as they provide rich planning representations and algorithms. We present our work developing an AI planning system for a boiler power plant domain. We develop a set of planning operators from an extended multilevel flow modeling (MFM) of the plant. Our planner reasons about goals and its subgoals, generates plans for different scenarios, including the sequence for start-up the plant. We show our approach to acquire the domain ..",AI Planning in Supervisory Control Systems,,,,,core
49976180,2009-09-24T00:00:00,"In this habilitation report, I present a synthesis of my research on three themes. The results for each topic depends on both the difficulty of the issues studied, the time devoted to them and opportunities for student supervision. These topics are on scheduling problems and mainly focus on the flowshop workshops that take into account additional constraints, close to the industrial reality, namely, (i) taking into account the constraints of grouping jobs, i.e., batch scheduling, (ii) consideration of time constraints on the sequence of jobs, known as time-lag, (iii) taking into account the deterioration of jobs. Our contribution to these three themes is on one hand the study of the complexity of the combinatorial structure of these problems, and on the other hand the implementation of optimization methods for efficient solving of these problems. This habilitation concludes with a general conclusion and the perspectives and research directions that we want involved in the near future and some thoughts on new direction of research.Dans ce mémoire, je présente une synthèse de mes travaux de recherche ainsi que le choix des thèmes étudiés. J'ai choisi de présenter trois thèmes. Les résultats obtenus pour chaque thème dépendent à la fois de la difficulté des problématiques étudiées, du temps qui leur est imparti et des circonstances et des opportunités d'encadrement des étudiants. Ces thèmes sont essentiellement sur les problèmes d'ordonnancement et principalement sont axées sur les ateliers de type flowshop avec prise en compte de contraintes supplémentaires, proche de la réalité industrielle, à savoir, (i) prise en compte de contraintes de groupement des tâches, connues sous le terme anglais, batch scheduling, (ii) prise en compte de contraintes temporelles sur la succession d'exécution des tâches, connues sous le nom de time-lags, (iii) prise en compte de la détérioration des tâches. Notre contribution à ces trois thèmes concerne d'une part l'étude de la complexité de la structure combinatoire de ces problèmes, et d'autre part la mise en œuvre de méthodes d'optimisation efficaces pour la résolution. Ce mémoire se termine par une conclusion générale, ainsi que les perspectives et les orientations de recherche que nous souhaitons engagé dans un avenir proche ainsi que quelques réflexions sur de nouvelles voies de recherche",Contribution à l'étude des problèmes d'ordonnancement flowshop avec contraintes supplémentaires : Complexité et méthodes de résolution,,HAL CCSD,,,core
22613912,2007-11-22,"A novel neural chip SAND (Simple Applicable Neural Device) is described. It is highly usable for massive parallel neural network computations. The chip is optimized for a high input data rate (50 MHz, 16 bit data) at a very low cost basis. The performance of a single SAND chip is 200 MOPS due to four parallel 16 bit multipliers and 40 bit adders working in one clock cycle. The chip is able to implement feedforward neural networks with a maximum of 512 input neurons and three hidden layers. Kohonen feature maps and radial basis function networks may be also calculated. Up to four chips are implemented on a PCI-board for industrial applications and on a VME board for trigger analysis in particle physics.  1. Real-Time Applications with Neural Networks  The Research Center Karlsruhe (FZK), in collaboration with IMS, designed the neuro-chip SAND (Simple Applicable Neural Device) for industrial and physical real time applications. In the following some of these applications are introduced t..",High Speed Neural Network Chip on PCI-Board,,,,,core
21130401,2010-01-22,"Abstract — This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], and initiated the preliminary design of a simple system prototype [6], we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. I",A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities- Part 1: Prototype Design and Development,,,,,core
21128338,2008,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], and initiated the preliminary design of a simple system prototype [?], we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The second-part paper addresses the ICAM system prototype design verification and its logical behavior during sensor faults in the plant",A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities --  Part 2: Prototype Design Verification,,,10.1109/isic.2008.4635951,,core
61870513,2007,"This paper focuses on the results of different consumer surveys conducted between 2004 and 2006 with regard to consumers’ perceptions and reactions concerning AI in Vietnam, (mainly in Hanoi). The main results observed are as follows: A high proportion of consumers consider AI to be a food-related risk. However, over time,there has been a slight shift from a fear of consuming poultry to a fear of preparing it (slaughtering it). AI has had a profound effect on poultry consumption, even outside peak crisis times, more in terms of the quantity consumed (approximately a third less in 2006) than in terms of the number of consumers (6% less). Blood and internal organs are considered particularly risky, while eggs are viewed as being safer. Poultry from industrial farms is considered to be more risky than poultry from small farms. Purchasing practices have also been affected by AI: in Hanoi, consumers declare that they prefer to buy poultry directly from producers that they know, or from supermarkets in the case of the wealthiest consumers. A high proportion still buy live poultry from market traders, but more consumers now ask sellers to slaughter it for them. With a view to lessening market shocks in the wake of the crisis while maintaining the priority of consumer safety, a number of measures should nevertheless be implemented: Risk communication should not over-emphasize AI as a food-related risk. Reliable safe distribution channels should be promoted (with reliable quality signs and controls) in order to encourage safe production and poultry consumption. Otherwise, a market recovery will only benefit supermarkets and large-scale farmers capable of supplying supermarkets. As numerous live birds are still slaughtered in urban market places, facilities should be provided for safe slaughter. At the same time, more attention should be paid to the provision of a real “cold chain” with a view to promoting the sale of slaughtered poultry",Consumer perceptions and reactions concerning AI,,,,,core
82663801,2010-04-30,"Background/PurposeThe postweaning multisystemic wasting syndrome, caused by the porcine circovirus type 2 (PCV-2), is a major disease that poses a significant threat to the global swine industry. The purpose of this study was to establish a real-time polymerase chain reaction (PCR) method for the quantification of PCV-2 and to enable the rapid differentiation of porcine circoviruses type 1 and 2 (PCV-1 and PCV-2). Such a method would significantly speed up the process of clinical diagnosis, and could also be used to study the pathogenic mechanisms of diseases associated with PCV-2.MethodsMultiplex real-time PCR, together with LightCycler PCR data analysis software, was used for the quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2. A 263-bp DNA fragment was amplified from the 3′ end of the open reading frame-2 of PCV-2 by nested PCR, and its DNA sequence was verified as having 100% identity with a PCV-2 standard (NCBI accession number: AF055394). The 263-bp DNA fragment was cloned into the pGEM-T easy vector, and the recombinant plasmid was serially diluted and quantified using real-time PCR. A standard curve was then constructed for quantification of the PCV-2 levels in field samples. The differentiation of PCV-1 and PCV-2 was carried out by analyzing the melting temperatures of the genotype-specific PCR products.ResultsTo quantify the PCV-2 levels in field samples, a standard curve (1 × 102 −1 × 109 copies/μL) was constructed. PCV-2 concentrations as low as 1 × 102 copies/mL could be detected in specimens taken from the lymph nodes or infected tissues in samples of PCV-2-infected pigs. The diagnosis of PCV-1 and PCV-2 infections and the quantification of the viral load in the field samples could be completed within 45 minutes after extracting the viral DNA using a commercial extraction kit.ConclusionThis study demonstrate that real-time PCR is a clinically feasible method for the accurate quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2",Fast Diagnosis and Quantification for Porcine Circovirus Type 2 (PCV-2) Using Real-Time Polymerase Chain Reaction ,https://core.ac.uk/download/pdf/82663801.pdf,Taiwan Society of Microbiology. Published by Elsevier Taiwan LLC.,10.1016/S1684-1182(10)60014-X,,core
161119204,2009-01-01T00:00:00,"The 1st ACM International Conference on Management of Emergent Digital EcoSystems (MEDES) has been accommodated by INSA-Lyon, France. MEDES is conceived as a new perspective and light shedding into the chronic issues of complexity, scalability, flexibility, stability, diversity and distribution in computing, software engineering, computer science, as well as in digital systems engineering and communications in general.

At the cross-roads of Nature Inspired Computing, Theoretical Biology, Social Sciences, Computer Science and other disciplines, this volume and conference aspires to contribute some answers to the quests of the nature of a Digital Ecosystem as well as its relationship with real ecosystems conceived as complex adaptive systems. Since real ecosystems already exhibit properties useful to drive complex systems and problems towards higher order of complexity, scalability, diversity, the digital counterparts as manifested by, e.g., interconnected business systems, infrastructures of the Internet and Web, distributed systems, computing and heterogeneous data and knowledge management, multi-sensored networks and mobile computing, are some subject areas which can be perceived as digital ecosystems with their own equilibrium and dynamics, evolutionary aspects, complexities, stabilities, diversities.

To this extent, ACM MEDES 2009 brought together a diverse community from academia, research laboratories and industry interested in exploring the manifold challenges and issues related to Digital Ecosystems and how current approaches and technologies can be evolved and adapted to this end.

The International Program Committee secured an interesting mixture of as many as possible aspects of Digital Ecosystems, which have been presented by a series of sessions of regular, short papers, as well as posters and demonstrations of industrial, business and societal case studies.

The contributions addressed a wide spectrum and variety of subjects areas and current challenges in digital systems and computing such as interoperability and data integration, ontology engineering, information retrieval, business intelligence, digital watermarking, machine learning, image processing, multimedia, service oriented architectures, social networks, social Web, modelling languages, security, trust, privacy, infrastructures, networks, etc. The contributions as presented in this volume state 41 full papers, together with some short and poster-demonstration papers. These have been selected out of 102 papers submitted to the main conference","Proceedings of the International Conference on Management of Emergent Digital EcoSystems, Lyon, France, October 27 - 30, 2009",,'American College of Medical Physics (ACMP)',,,core
10858184,2008-01-01T00:00:00,"Preface
The history of automated pattern recognition can be traced back to the advent of modern computing mid-way through the 20th century. Since that time, the popularity and growth of the pattern recognition field has been fueled by its scientific significance and its applicability to the real world. Pattern recognition is a very challenging and multidisciplinary research area attracting researchers and practitioners from many fields, including computer science, computational intelligence, statistics, engineering, and medical sciences, to mention just a few. Pattern recognition is a process described as retrieving a pattern from a database of known patterns. It has numerous real-world applications in areas such as security, medicine, information processing, and retrieval. Some pattern recognition applications in areas such as handwriting recognition, document retrieval, speech recognition, signature verification, and face recognition are the main focus of the current research activities in the pattern recognition and computational intelligence communities around the globe. Researchers and developers are facing many challenges to applying pattern recognition techniques in many real-world applications. This book consists of 17 peer-reviewed chapters that describe theoretical and applied research work in this challenging area. The state of the art in areas such as handwriting recognition, signature verification, speech recognition, human detection, gender classification, morphological structures for image classification, logic synthesis for image and signal processing, occlusion sequence mining, probabilistic neural networks for EMG patterns, multi-objective clustering ensembles, evolutionary ensembles, support vector machines for biomedical data, and unified support vector machines is presented in various chapters of this book.
The first two chapters focus on off-line cursive handwriting recognition. In Chapter I, Verma and Blumenstein review existing handwriting recognition techniques and present the current state of the art in cursive handwriting recognition. Standard handwriting recognition processes are presented, and each process is described in detail. Some novel segmentation strategies and a segmentation-based approach for automated recognition of unconstrained cursive handwriting are also presented.
In Chapter II, Uchida investigates the theoretical and practical importance of elastic matching for handwriting recognition. He argues that the use of elastic matching techniques instead of rigid matching
techniques improves the robustness of handwriting recognition systems. In addition, the optimized matching represents the deformation of handwritten characters and, thus, is useful for statistical analysis of the deformation. Elastic matching is formulated as an optimization problem of planar matching, or pixel-to-pixel correspondence, between two character images under a certain matching model such as affine and nonlinear.
The next two chapters focus on off-line signature verification. In Chapter III, Batista, Rivard, Sabourin,
and Granger present the current state of art in automatic signature verification. Automatic signature verification is a biometric method that can be applied in all situations where handwritten signatures are used, such as cashing a check, signing a credit card, and authenticating a document. They review existing
approaches in the literature and present a survey of the most important techniques used for feature extraction and verification in this field. They also present strategies used for problems such as limited amounts of data and show important challenges and some new research directions.
xiv
In Chapter IV, Madasu and Lovell present an off-line signature verification and forgery detection system based on fuzzy modeling. The various handwritten signature characteristics and features are first studied and encapsulated to devise a robust verification system. The verification of genuine signatures and detection of forgeries is achieved via angle features extracted using a grid method. The derived features are fuzzified by an exponential membership function, which is modified to include two structural
parameters. The structural parameters are devised to take into account the possible variations due to handwriting styles and to reflect other factors affecting the scripting of a signature. The proposed system has been tested on a large database of signatures comprising more than 1,200 signature images obtained from 40 volunteers.
Chapters V and VI focus on speech recognition. In Chapter V, Suárez-Guerra and Oropeza-Rodriguez present the state of the art in automatic speech recognition. Speech recognition is very challenging for researchers in many fields, including computer science, mathematical statistics, applied artificial intelligence,
and linguistics. The unit of essential information used to characterize the speech signal in the most widely used ASR systems is the phoneme. However, several researchers recently have questioned this representation and demonstrated the limitations of the phonemes, suggesting that ASR with better performance can be developed replacing the phoneme by triphones and syllables as the unit of essential information used to characterize the speech signal. This chapter presents an overview of the most successful
techniques used in ASR systems, together with some recently proposed ASR systems that intend to improve the characteristics of conventional ASR systems.
In Chapter VI, Leedham, Pervouchine, and Zhong investigate features of handwriting and speech and their effectiveness at determining whether the identity of a writer or speaker can be identified from handwriting or speech. For handwriting, some of the subjective and qualitative features used by document
examiners are investigated in a scientific and quantitative manner based on the analysis of three characters (d, y, and f) and the grapheme th. For speech, several frequently used features are compared for their strengths and weaknesses in distinguishing speakers. The results show that some features do have good discriminative power, while others are less effective. Acceptable performance can be obtained in many situations using these features. However, the effect of handwriting forgery/disguise or conscious speech imitation/alteration on these features is not investigated. New and more powerful features are needed in the future if high accuracy person identification can be achieved in the presence of disguise or forgery.
In Chapter VII, Yu, Pham, and Yan present a new pattern recognition method using morphological structure. First, smooth linearization is introduced based on various chain codes. Second, morphological structural points are described in terms of smooth followed contours and linearized lines, and then the patterns of morphological structural points and their properties are given. Morphological structural points are basic tools for pattern recognition-based morphological structure. Furthermore, how the morphological
structure can be used to recognize and classify images is presented. One application is document image processing and recognition, analysis, and recognition of broken handwritten digits. Another one is dynamic analysis and recognition of cell-cycle screening based on morphological structures.
In Chapter VIII, Shan, Bigdeli, Lovell, and Chen propose a variability compensation technique that synthesizes realistic frontal face images from nonfrontal views. It is based on modeling the face via active
appearance models and estimating the pose through a correlation model. The proposed technique is coupled with adaptive principal component analysis (APCA), which was previously shown to perform well in the presence of both lighting and expression variations. The proposed recognition techniques, although advanced, are not computationally intensive. So they are quite well suited to the embedded system
environment. Indeed, the authors have implemented an early prototype of a face recognition module on a mobile camera phone so the camera could be used to identify the person holding the phone.
xv
In Chapter IX, Guha, Mukerjee, and Venkatesh present complex multi-object interactions resulting in occlusion sequences that are a visual signature for the event. In this chapter, multi-object interactions are tracked using a set of qualitative occlusion primitives derived on the basis of the persistence hypothesis—
objects continue to exist even when hidden from view. Variable length temporal sequences of occlusion primitives are shown to be well correlated with many classes of semantically significant events. In surveillance applications, determining occlusion primitives is based on foreground blob tracking and requires no prior knowledge of the domain or camera calibration. New foreground blobs are identified as putative objects that may undergo occlusions, split into multiple objects, merged back again, and so forth. Significant activities are identified through temporal sequence mining, which bear a high correlation
with semantic categories. Thus, semantically significant event categories can be recognized without assuming camera calibration or any environmental/object/action model prior.
In Chapter X, Jia and Zhang review human detection techniques. Human detection is the first step for a number of applications such as smart video surveillance, driving assistance systems, and intelligent digital content management. It is a challenging problem due to the variance of illumination, color, scale, pose, and so forth. This chapter reviews various aspects of human detection in static images and focuses on learning-based methods that build classifiers using training samples. There are usually three modules for these methods: feature extraction, classifier design, and merging of overlapping detections. The chapter reviews most of the existing methods for each module and analyzes their respective pros and cons. The contribution includes two aspects: first, the performance of existing feature sets on human detection are compared; second, a fast human detection system based on the histogram of oriented gradients features and a cascaded Adaboost classifier is proposed. This chapter is useful for both algorithm researchers and system designers in the computer vision and pattern recognition communities.
In Chapter XI, Tivive and Bouzerdoum present a brain-inspired pattern recognition architecture. With the ever-increasing utilization of imagery in scientific, industrial, civilian, and military applications, visual pattern recognition has been thriving as a research field and has become an essential enabling technology for many applications. In this chapter, a brain-inspired pattern recognition architecture that easily can be adapted to solve various real-world visual pattern recognition tasks is presented. The architecture has the ability to extract visual features from images and classify them within the same network structure; in other words, it integrates the feature extraction stage with the classification stage, and both stages are optimized with respect to one another. The main processing unit for feature extraction is governed by a nonlinear biophysical mechanism known as shunting inhibition, which plays a significant role in visual information processing in the brain. The proposed architecture is applied to four real-world visual pattern
recognition problems; namely, handwritten digit recognition, texture segmentation, automatic face detection, and gender recognition. Experimental results demonstrate that the proposed architecture is very competitive with and sometimes outperforms existing state-of-the-art techniques for each application.
In Chapter XII, Rawski, Selvaraj, Falkowski, and Luba present the discussion on efficiency of various implementation methodologies of DSP algorithms targeting modern FPGA architectures. Nowadays,
programmable technology provides the possibility of implementing digital systems with the use of specialized embedded DSP blocks. In the first place, however, this technology gives the designer the possibility to increase the efficiency of designed systems by exploitation of parallelisms of implemented algorithms. Moreover, it is possible to apply special techniques such as distributed arithmetic (DA). Since in this approach general-purpose multipliers are replaced by combinational LUT blocks, it is possible to construct digital filters of very high performance. Additionally, application of the functional decomposition-
based method to LUT block optimization and mapping has been investigated. The chapter presents results of the comparison of various design approaches in these areas.
xvi
In Chapter XIII, Zhou and Wang present an approach to class-dependent feature selection and a novel support vector machine (SVM). The relative background and theory are presented for describing the proposed method, and real applications of the method on several biomedical datasets are demonstrated. The authors hope that this chapter can provide readers with a different view of the feature selection method and also the classifier so as to promote more promising methods and applications.
In Chapter XIV, Shilton and Palaniswami present a unified introduction to support vector machine (SVM) methods for binary classification, one-class classification, and regression. The SVM method for binary classification (binary SVC) is introduced first and then extended to encompass one-class classification (clustering). Next, using the regularized risk approach as a motivation, the SVM method for regression (SVR) is described. These methods are then combined to obtain a single, unified SVM formulation that encompasses binary classification, one-class classification, and regression (as well as some extensions of these), and the dual formulation of this unified model is derived. A mechanical analogy
for the binary and one-class SVCs is given to provide an intuitive explanation of the operation of these two formulations. Finally, the unified SVM is extended to implement general cost functions, and an application of SVM classifiers to the problem of spam e-mail detection is considered.
In Chapter XV, Faceli, Carvalho, and Souto investigate multi-objective clustering ensembles for clustering techniques. Clustering is an important tool for data exploration. Several clustering algorithms exist, and new algorithms are frequently proposed in the literature. These algorithms have been very successful
in a large number of real-world problems. However, there is no clustering algorithm, optimizing only a single criterion, able to reveal all types of structures (homogeneous or heterogeneous) present in a dataset. In order to deal with this problem, several multi-objective clustering and cluster ensemble methods have been proposed in the literature, including a multi-objective clustering ensemble algorithm. In this chapter, an overview of these methods, which, to a great extent, are based on the combination of various aspects from traditional clustering algorithms, is presented.
In Chapter XVI, Duell and Yao present negative correlation learning in evolutionary ensembles with suitable speciation techniques. Negative correlation learning (NCL) is a technique that attempts to create an ensemble of neural networks whose outputs are accurate but negatively correlated. The motivation for such a technique can be found in the bias-variance-covariance decomposition of an ensemble of the learner’s generalisation error. NCL is also increasingly used in conjunction with an evolutionary process, which gives rise to the possibility of adapting the structures of the networks at the same time as learning the weights. This chapter examines the motivation and characteristics of the NCL algorithm. Some recent work relating to the implementation of NCL in a single objective evolutionary framework for classification tasks is presented, and the authors examine the impact of two different speciation techniques: implicit fitness sharing and an island model population structure. The choice of such speciation techniques can have a detrimental effect on the ability of NCL to produce accurate and diverse ensembles and should, therefore, be chosen carefully. This chapter also provides an overview of other researchers’ work with NCL and gives some promising future research directions.
In Chapter XVII, Tsuji, Bu, and Fukuda present a recurrent probabilistic neural network for EMG pattern recognition. In the field of pattern recognition, probabilistic neural networks (PNNs) have been proven as an important classifier. For pattern recognition of EMG signals, the characteristics usually used are amplitude, frequency, and space. However, significant temporal characteristics exist in the transient
and nonstationary EMG signals, which cannot be considered by traditional PNNs. In this chapter, a recurrent PNN called recurrent log-linearized Gaussian mixture network (R-LLGMN) is introduced for EMG pattern recognition, with the emphasis on utilizing temporal characteristics. The structure of R-LLGMN is based on the algorithm of a hidden Markov model (HMM), which is a routinely used technique for modeling stochastic time series. Since R-LLGMN inherits advantages from both HMM and
xvii
neural computation, it is expected to have a higher representation ability and show better performance when dealing with time series such as EMG signals. Experimental results show that R-LLGMN can achieve high discriminant accuracy in EMG pattern recognition.
Brijesh Verma, Central Queensland University, Australia
Michael Blumenstein, Griffith University, Australia
Editor",Pattern Recognition Technologies and Applications,,,,,core
358991,2009-01-01T00:00:00,"This study investigated the potential application of mid-infrared spectroscopy (MIR 4,000–900 cm−1) for the determination of milk coagulation properties (MCP), titratable acidity (TA), and pH in Brown Swiss milk samples (n = 1,064). Because MCP directly influence the efficiency of the cheese-making process, there is strong industrial interest in developing a rapid method for their assessment. Currently, the determination of MCP involves time-consuming laboratory-based measurements, and it is not feasible to carry out these measurements on the large numbers of milk samples associated with milk recording programs. Mid-infrared spectroscopy is an objective and nondestructive technique providing rapid real-time analysis of food compositional and quality parameters. Analysis of milk rennet coagulation time (RCT, min), curd firmness (a30, mm), TA (SH°/50 mL; SH° = Soxhlet-Henkel degree), and pH was carried out, and MIR data were recorded over the spectral range of 4,000 to 900 cm−1. Models were developed by partial least squares regression using untreated and pretreated spectra. The MCP, TA, and pH prediction models were improved by using the combined spectral ranges of 1,600 to 900 cm−1, 3,040 to 1,700 cm−1, and 4,000 to 3,470 cm−1. The root mean square errors of cross-validation for the developed models were 2.36 min (RCT, range 24.9 min), 6.86 mm (a30, range 58 mm), 0.25 SH°/50 mL (TA, range 3.58 SH°/50 mL), and 0.07 (pH, range 1.15). The most successfully predicted attributes were TA, RCT, and pH. The model for the prediction of TA provided approximate prediction (R2 = 0.66), whereas the predictive models developed for RCT and pH could discriminate between high and low values (R2 = 0.59 to 0.62). It was concluded that, although the models require further development to improve their accuracy before their application in industry, MIR spectroscopy has potential application for the assessment of RCT, TA, and pH during routine milk analysis in the dairy industry. The implementation of such models could be a means of improving MCP through phenotypic-based selection programs and to amend milk payment systems to incorporate MCP into their payment criteria","Prediction of coagulation properties, titratable acidity and pH of bovine milk using mid-infrared spectroscopy",,'American Dairy Science Association',10.3168/jds.2008-1163,,core
24619459,2008-04-01,"Abstract — A growing issue in the video game industry is the artificial intelligence that controls the non-player agents in their worlds; as visual and audio quality increases, the simple scripted behavior of the agents is made more obvious. This paper sets out to show how agent control systems can be evolved to provide truly dynamic agent behavior that adapts to the player. A demonstration of the power of this idea was built to model a simple space simulation game. The system allows the player to create and position targets for the game agents to swarm. rtNEAT (a real-time implementation of the NeuroEvolution of Augmenting Topologies method) is used to evolve the neural networks that control the individuals, and this paper details the results. In the future this technique could significantly advance the field of artificial intelligence in games and open up new genres for designers and players alike to explore",Real-Time NeuroEvolution for Game Agent Control,,,,,core
44254124,2010-01-01T00:00:00,"Electronics is under development in this country in an organized and institutional way since the beginning of 30-ties of the previous century. It grew up from electrical engineering of weak currents and its first name used popularly was communications. It was time when television was born and the radio was maturing. Electronics is a branch of research and technology which deals with generation and processing of electrical and electromagnetic signals. A subject of telecommunications is signal transmission for a distance. Electronics and telecommunications (ET) includes or is combined with other branches like: microelectronics, radioelectronics, optoelectronics, photonics, acoustoelectronics, magnetronics, bioelectronics, energoelectronics, material engineering, semiconductor physics, automation and robotics, mechatronics and microsystems, informatics, teleinformatics, software engineering and other. Devices and functional systems of ET such as computers, data warehouses, cell phones, TV sets, Internet, GPS are build of electronic components and circuits. ET is a branch which belongs to hi-tech area, where the products gather a large load of knowledge of value overcoming frequently the price of work and material. ET has recently turned to an active participant of the processes of generation, storing, processing, transportation, distribution and usage of knowledge in the society. ET started to create artificial intelligence, co-creates intellectual property, searches for knowledge in big data sets, aids medicine, extends virtual/augmented reality, builds Internet of persons and things, strengthens security, protects natural environment, facilitates our life, aids our decisions, activates individuals, equalizes chances, provides convenient personal communications and access to data, starts building a penetrating ubiquitous infrastructure, ceases to be only a branch of technology, grows into the social space, touches culture, sociology, psychology and art. Such an important role of ET is combined with the existence in the society of an adequate infrastructure which recreates the full development cycle of high technology embracing: people, institutions, finances and logistics, in this also science, higher education, education, continuous training, dissemination and outreach, professional social environment, legal basis, political support and lobbying, innovation structures, applications, industry and economy. The digest of chosen development tendencies in ET was made here from the academic perspective, in a wider scale and on this background the national one, trying to situate this branch in the society, determine its changing role to build a new technical infrastructure of a society based on knowledge, a role of builder of many practical gadgets facilitating life, a role of a big future integrator of today’s single bricks into certain more useful unity. This digest does not have a character of a systematic analysis of ET. It is a kind of an arbitrary utterance of the authors inside their field of competence. The aim of this paper is to take an active part in the discussion of the academic community in this country on the development strategy of ET, choice of priorities for cyclically rebuilding economy, in competitive environments. The review paper was initiated by the Committee of Electronics and Telecommunications of Polish Academy of Sciences and was published in Polish as introductory chapter of a dedicated expertise, printed in a book format. This version makes the included opinions available for a wider community",Electronics and telecommunications in Poland - issues and perspectives,,Proc. SPIE,,,core
73882988,2009-01-28,"Three dimensional graphics processing requires many complex algebraic and matrix based operations to be performed in real-time. In early stages of graphics processing, such tasks were delegated to a Central Processing Unit (CPU). Over time as more complex graphics rendering was demanded, CPU solutions became inadequate. To meet this demand, custom hardware solutions that take advantage of pipelining and massive parallelism become more preferable to CPU software based solutions. This fact has lead to the many custom hardware solutions that are available today. Since real time graphics processing requires extreme high performance, hardware solutions using Application Specific Integrated Circuits (ASICs) are the standard within the industry. While ASICs are a more than adequate solution for implementing high performance custom hardware, the design, implementation and testing of ASIC based designs are becoming cost prohibitive due to the massive up front verification effort needed as well as the cost of fixing design defects.Field Programmable Gate Arrays (FPGAs) provide an alternative to the ASIC design flow. More importantly, in recent years FPGA technology have begun to improve in performance to the point where ASIC and FPGA performance has become comparable. In addition, FPGAs address many of the issues of the ASIC design flow. The ability to reconfigure FPGAs reduces the upfront verification effort and allows design defects to be fixed easily. This thesis demonstrates that a 3-D graphics processor implementation on and FPGA is feasible by implementing both a two dimensional and three dimensional graphics processor prototype. By using a Xilinx Virtex 5 ML506 FPGA development kit a fully functional wireframe graphics rendering engine is implemented using VHDL and Xilinx's development tools. A VHDL testbench was designed to verify that the graphics engine works functionally. This is followed by synthesizing the design and real hardware and developing test applications to verify functionality and performance of the design. This thesis provides the ground work for push forward the use of FPGA technology in graphics processing applications",Real Time 3-D Graphics Processing Hardware Design using Field-Programmable Gate Arrays.,,,,,core
144057189,2010-10-05T00:00:00,"A laminação a frio de aços planos é um  processo industrial de grande complexidade, executado num ambiente bastante agressivo e caracterizado por parâmetros incertos, grandes tempos de atraso, múltiplas entradas e múltiplas saídas e por forte interação entre as diversas variáveis (GUO, 2000). As referências necessárias para o controle de tal processo são obtidas por meio de um modelo matemático, responsável pela sua otimização. Dadas as particularidades do processo, o modelo deve ser do tipo adaptativo, ou seja, deve ter seus parâmetros continuamente ajustados com base nos resultados efetivamente obtidos durante a operação. A adaptação do modelo é essencial para a consecução dos requisitos de qualidade do produto e, em conseqüência, para a viabilidade da planta. Este trabalho primeiramente investiga as estratégias tradicionalmente empregadas no desenvolvimento de modelos adaptativos para o processo de laminação de aços, identificando os pontos fortes e deficiências de tais técnicas. Com o objetivo de minimizar essas deficiências e aperfeiçoar o desempenho do modelo, o trabalho propõe em seguida a utilização de ferramentas de Inteligência Computacional (mais especificamente, Redes Neurais Artificiais) para tornar mais eficiente a adaptação de dois  importantes parâmetros: o limite de  escoamento do material e o coeficiente de atrito entre a tira e os cilindros de trabalho. O texto  apresenta os fundamentos teóricos, a metodologia de  desenvolvimento e a implantação da solução, bem como os resultados preliminares obtidos com a aplicação da proposta num laminador real. Finalmente, discutem-se esses resultados, apresentam-se as conclusões, sumarizam-se as contribuições da pesquisa e se sugerem futuras linhas de pesquisa para extensão dos resultados.Flat steel cold rolling is a complex industrial process, carried out into a very harsh environment, characterized by uncertain parameters, large delay times, multiple inputs and outputs, and having a strong interaction  between the diverse variables (GUO, 2000).  The control system of such a process obtains its setpoints from a mathematical model, responsible for process optimization. Given process  peculiarity, the mathematical model must have adaptive features, that is, its parameters need to be continuously adjusted, on the basis of actual operational results. Adaptation is an essential characteristic for achieving  product quality requirements and, as a consequence, for the plant feasibility itself. First of all, this work surveys the strategies commonly used in the development of cold rolling adaptive models, pointing out their strengths and deficiencies. Following to this, aiming at the minimization of such deficiencies and by this way improving the performance of the model, this work proposes the application of  Computational Intelligence techniques (specifically Artificial Neural Networks) in order to bestow greater effectiveness to the adaptation of two important parameters, namely, the material flow stress and the friction coefficient between rolled strip and rolling cylinders. The text presents the theoretical foundations, the methodology and the implementation of the solution, as so the preliminary results after its application in a real steel mill plant. Finally, the text presents the conclusions, sums up the contributions of the research work and suggests further research to improve the  results",Computational intelligence applied to the adaptation of mathematical modeling in flat steel cold rolling process.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.3.2009.tde-29092010-155922,,core
23001037,2007,"Today, there has been a massive proliferation of huge databases storing valuable information. The opportunities of an effective use of these new data sources are enormous, however, the huge size and dimensionality of current large databases call for new ideas to scale up current statistical and computational approaches. This paper presents an application of Artificial Intelligence technology to the problem of automatic detection of candidate anomalous records in a large database. We build our approach with three main goals in mind: 1)An effective detection of the records that are potentially anomalous, 2)A suitable selection of the subset of attributes that explains what makes a record anomalous, and 3)An efficient implementation that allows us to scale the approach to large databases. Our algorithm, called Bayesian Network Anomaly Detector (BNAD), uses 2 the joint probability density function (pdf) provided by a Bayesian Network (BN) to achieve these goals. By using appropriate data structures, advanced caching techniques, the flexibility of Gaussian Mixture models, and the efficiency of BNs to model joint pdfs, BNAD manages to efficiently learn a suitable BN from a large dataset. We test BNAD using synthetic and real databases, the latter from the fields of manufacturing and astronomy, obtaining encouraging results",1 Unsupervised Anomaly Detection in Large Databases Using Bayesian Networks,,,,,core
211469947,2010-01-01T00:00:00,"Objective of the present study is the development of design methods for the control of products‘ architecture in order to obtain modular designs. Towards this target, an integrated approach is proposed, investigating the design architecture from two aspects: the -functions to parts- mapping as well as the point of view related to parts‘ interactions.
For the first aspect, an approach utilizing Axiomatic Design Theory is described in order to control the design architecture with regards to the -functions to parts- mapping. As far as the second aspect is concerned, two indexes are developed quantifying the design architecture in terms of the parts‘ interactions perspective. Furthermore, an algorithm for clustering of product‘s parts into clusters/modules is introduced. The algorithm utilizes Artificial Neural Networks (ANNs) and Design Structure Matrices (DSMs).
The aforementioned developments were incorporated into a CAD based software tool, having as objective the support of modular design. Its main functions are: (a) DSM generation from product CAD model, (b) calculation of the aforementioned indexes, (c) facilitation of clustering and (d) representation of clustered DSM in CAD form. Application of the tool to real case studies from the automotive industry, provide an evaluation of the developed methods.
The main outcome of the present work is the integrated approach that was proposed and realized through the software tool, which integrates methods for the handling of a product‘s design architecture. This process assists in real time (during the design process) design engineers to the generation of modular designs. The evaluation of the case studies reveals the efficiency of the proposed approach to produce such designs and validates its applicability to industry",Design methos for the control of products' design architecture,,'National Documentation Centre (EKT)',10.12681/eadd/28038,,core
33706694,2008,"English translation of chapters: 1. Knowledge management in enterprises functioning in the new economy - 2. Innovativeness as one of the knowledge-based economy pillars - 3. The approach to information management in small and medium sized enterprise - 4. Knowledge outsourcing III. The fluctuation of the knowledge and decision centers - 5. Implementation of declarative framework for decision support in scheduling problems - 6. Small-size and multi-product production flow planning - 7. Artificial intelligence methods in prediction of stock index values with usage of newspaper articles - 8. Information system supporting utilization of knowledge on movement and transport control - 9. The role of modern manager in knowledge management process - 10. Developing an organisational culture supporting knowledge management - 11. A method for evaluating organizational structure on the basis of social network analysis - 12. Developing sets of experience knowledge structure: Toward decisional DNA - 13. The language of communication in a multi-agent system for network monitoring - 14. A concept study of a multiagent system for maintaining the quality of service in future mobile ad hoc networks - 15. Reconstruction of attack propagation tree in multi-agent IDS system - 16. Data storage management: outline of practical methodology foreffectiveness assessment - 17. Applications of rough classification method in e-learning systems - 18. Description logic as software modeling language - 19. The concept of IT system for knowledge and experience management - 20. Knowledge acquisition for workflow systems - 21. Knowledge management embedded in software engineering processes - 22. E-document technology in the e-administration - 23. The application of PHP scripts for information extraction from newspaper announcements - 24. Knowledge management in an EU project on the example of MAYDAY project - 25. Outline of the contemporary trends of the net and their implications for ebusiness - 26. Providing learning components with learning schemata by means of the UDDI registry - 27. Pomeranian firms and the new technology - 28. Linear model approach to the computerization strategy of an organization - 29. The analysis of organization models and the choice of information management system for commercial and manufacturing enterprises - 30. Role of project management office in IT project management - 31. An intelligent platform for communication and control as well as management of modern companies - 32. Internet-based polling system as company's competitiveness improvement tool - 33. The UML model of an intelligent system for the management of industrial like processes in real-time - 34. Formal foundations of a knowledge management system supporting business process optimization - 35. Examples of tools used for business process modeling - 36. A quality model for UML tools - 37. Evaluation of the business processes modeling methods used in enterprises - 38. Fuzzy logic and logic-algebraic method for constraint programming-driven project prototyping - 39. Use of analytical and simulation methods for modeling of discrete and stochastic systems - 40. Architecture of distributed system for teletraffic monitoring - 41. Needle Desktop Search: A search engine for local internet documents - 42. An approach to composite web services evaluation for service oriented architectures - 43. Methodological converters for the evaluation of internet computer shops’ websites - 44. Knowledge management in IT project management - 45. Studies of the stage of the IT projects realization as the project management adjusting factor - 46. Evaluation of information technology as a part of R&D process optimization - 47. IT organization transformation modelling. SITAR – the mimplified model - 48. A proposal of metodology for testing clients requirements against employed information technology - 49. Knowledge resources management model in the information technology evaluation environment - 50. Ontology management model in an information technology evaluation environment - 51. IT evaluation using a functional prototype of multiagent systems - 52. Estimation of the IT technologies, using Mind Map techniques, modeling, and expert estimation",Zarządzanie wiedzą i technologiami informatycznymi,,Pomorskie Wydawnictwo Naukowo-Techniczne,,,core
288092442,2010-01-01T00:00:00,"Nowadays, the one of sections which is studied about is Artificial Neural Network (ANN) Models. ANN researches are related to most field like optimization, control, image processing, meaning and separating language, natural language and forecasting. The inspiration of the ANNs is the power, elasticity and sensitivity of the biological brain. ANN is the mathematical model of the nerve cells, synapse and dendrites which are the main biological components of the brain. ANN is formed from simple mathematical elements. There are two kinds of learning processes in ANN; supervised and unsupervised. In the supervised learning process, the output set necessary for each input set, and both of them form the learning set. Usually, learning is used to realize by introduced to these pairs (input/output sets) to ANN. In the learning process, firstly, the input sets are given to ANN, and the output of them are computed. Afterwards, ANN changes the weights, until the desired convergence criteria level between the computed outputs and the real outputs is proved. As a result, ANN is trained and the weights at the most suitable values. In this study, the existing cost information of the factory was provided as an input for the artificial neural network, and the network was asked to yield the amount of production as an output. The study deals with an implementation of artificial neural network to determine the amount of production using the cost information obtained from the firm",Lecture Notes in Engineering and Computer Science,,INT ASSOC ENGINEERS-IAENG,,"[{'title': None, 'identifiers': ['issn:2078-0958', '2078-0958']}]",core
61389021,2008-01-01T00:00:00,"One of the areas that needs further improvement
within E-Learning environments via Internet (A big effort is
required in this area if progress is to be made) is allowing students
to access and practice real experiments in a real laboratory,
instead of using simulations [1]. Real laboratories allow students
to acquire methods, skills and experience related to real
equipment, in a manner that is very close to the way they are
being used in industry. The purpose of the project is the study,
development and implementation of an E-Learning environment
to allow undergraduate students to practice subjects related to
Robotics and Artificial Intelligence. The system, which is now at a
preliminary stage, will allow the remote experimentation with real
robotic devices (i.e. robots, cameras, etc.). It will enable the
student to learn in a collaborative manner (remote participation
with other students) where it will be possible to combine the onsite
activities (performed “in-situ” within the real lab during the
normal practical sessions), with the “on-line” one (performed
remotely from home via the Internet). Moreover, the remote
experiments within the E-Laboratory to control the real robots
can be performed by both, students and even scientist. This
project is under development and it is carried out jointly by two
Universities (UPC and UJI). In this article we present the system
architecture and the way students and researchers have been able
to perform a Remote Programming of Multirobot Systems via web",Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,https://core.ac.uk/download/61389021.pdf,Westing Publishing Co.,,,core
71922296,2009-01-01T00:00:00,"In the past few years, the field of autonomous robot has been rigorously studied and non-industrial applications of robotics are rapidly emerging. One of the most interesting aspects of this field is the development of the learning ability which enables robots to autonomously adapt to given environments without human guidance. As opposed to the conventional methods of robots' control, where human logically design the behavior of a robot, the ability to acquire action strategies through some learning processes will not only significantly reduce the production costs of robots but also improves the applicability of robots in wider tasks and environments. However, learning algorithms usually require large calculation cost, which make them unsuitable for robots with limited resources. In this study, we propose a simple two-layered neural network that implements a novel and fast Reinforcement Learning. The proposed learning method requires significantly less calculation resources, hence applicable to small physical robots running in the real world environments.For this study, we built several simple robots and implemented the proposed learning mechanism to them. In the experiments, to evaluate the efficacy of the proposed learning mechanism, several robots were simultaneously trained to acquire obstacle avoidance strategies in a same environment, thus, forming a dynamic environment where the learning task is substantially harder than in the case of learning in a static environment",Fast reinforcement learning for simple physical robots,,'Springer Fachmedien Wiesbaden GmbH',10.1007/s12293-009-0015-x,,core
15619336,2010,"Software used by architectural and industrial designers - has moved from becoming a tool for drafting, towards use in verification, simulation, project management and project sharing remotely. In more advanced models, parameters for the designed object can be adjusted so a family of variations can be produced rapidly. With advances in computer aided design technology, numerous design options can now be generated and analyzed in real time. However the use of digital tools to support design as an activity is still at an early stage and has largely been limited in functionality with regard to the design process. To date, major CAD vendors have not developed an integrated tool that is able to both leverage specialized design knowledge from various discipline domains (known as expert knowledge systems) and support the creation of design alternatives that satisfy different forms of constraints. We propose that evolutionary computing and machine learning be linked with parametric design techniques to record and respond to a designer&#039;s own way of working and design history. It is expected that this will lead to results that impact on future work on design support systems - (ergonomics and interface) as well as implicit constraint and problem definition for problems that are difficult to quantify","Patterns, heuristics for architectural design support: Making use of evolutionary modelling in design",,"Association for Research in Computer-Aided Architectural Research in Asia (CAADRIA) (Hong Kong, China)",,,core
23271241,2007-11-22,"The development and deployment of multi-agent systems in real world settings raises a number of important research issues and problems which must be overcome if Distributed AI (DAI) is to become a widespread solution technology. Work undertaken in the context of the ARCHON project has provided a number of important insights into these issues. By providing an in depth analysis of ARCHON&apos;s electricity transportation management application, this paper draws together many of the experiences obtained when building one of the world&apos;s first operational DAI systems.  Introduction  In many industrial applications a substantial amount of time, effort and finance has been devoted to developing complex and sophisticated software systems. These systems are often viewed in a piecemeal manner as isolated islands of automation, when, in reality, they should be seen as components of a much larger business function (Jennings, 1994a). The main benefit of taking a holistic perspective is that the partial ..",(AL) and an application program (known as an,,,,,core
21129654,2010-01-22,"Abstract — This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], and initiated the preliminary design of a simple system prototype [6], we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The third-part paper addresses the ICAM system prototype validation in terms of system performance analysis and system behavior during unexpected situations. I",P P A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities- Part 3: Performance Analysis and System Limitations,,,,,core
211457606,2009-01-01T00:00:00,"Uncertainties in structural mechanics, and in particular in the phase of analysis and design, can play an extremely important role, affecting not only the safety and reliability of structures and their mechanical components, but also the quality of their performance. The response of a structural system may be very sensitive to uncertainties in the material properties, manufacturing conditions, external loading and analytical or numerical modeling. In order to account for these issues, stochastic analysis methods have been developed over the last decades. The optimum result obtained by a deterministic optimization formulation that ignores scatter of any kind of the parameters affecting its response has limited value and reliability, as it can be severely affected by the uncertainties that are inherent in the model. The deterministic optimum can be associated with unaccepted probabilities of failure, or it can be vulnerable to slight variations of some uncertain parameters. The development of probabilistic analysis methods over the last two decades has stimulated the interest for considering also randomness and uncertainty in the formulation of structural design optimization problems. In order to account for uncertainties in a structural optimization framework, probabilistic-based formulations of the optimization problem have to be used, utilizing stochastic simulation and probabilistic analysis. The goal of the thesis is to unify the concepts of probability-based safety analysis and structural optimization and provide the necessary numerical tools to deal with optimization problems considering uncertainties. This goal is addressed by developing algorithms for solving the probabilistic structural optimization problems encountered. In order to deal with these problems efficiently, various algorithms and methodologies have to be used, such as efficient single- and multi-objective optimizers and efficient stochastic problems formulations for the stochastic analysis process. Despite the advances on these issues, the computational cost for considering the uncertainties in a structural design optimization problem remains extremely large, especially for real-world large-scale problems with many design and/or random variables. To alleviate the computational burden, the implementation of Neural Network (NN) metamodels is also proposed in this thesis for further reducing the computational cost, providing acceptable numerical results at an affordable computational time. The dissertation consists of nine chapters in total, plus the bibliography and three appendices. It is organized as follows: following the introduction of Chapter 1, Chapter 2 deals with the concept of uncertainty in structural engineering in general. Chapter 3 presents the formulation of single objective optimization problems, while Chapter 4 discusses the multi-objective optimization problem. The basics of Neural Networks and their implementation in structural engineering are presented in Chapter 5. Chapter 6 discusses the problem of structural optimization considering uncertainties, where the basic problems of this kind, namely the Reliability-Based Design Optimization (RBDO), the Robust Design Optimization (RDO) and the combination Reliability-based Robust Design Optimization (RRDO) problems are presented, among others. The numerical applications of the dissertation are divided into two parts, A and B, presented in Chapters 7 and 8, respectively. Part A (Chapter 7) contains the deterministic optimization test examples, where uncertainties are not taken into account. In Part B (Chapter 8), the probabilistic optimization test examples are discussed, where uncertainties play a significant role. Chapter 9 contains the conclusions, the original contribution of the thesis, and directions for future research. Finally, the bibliography is presented followed by three appendices: Appendix A, containing the notation and symbols used in the dissertation; Appendix B with the acronyms and abbreviations used; and Appendix C with a listing of publications by the author.Οι αβεβαιότητες στη δομοστατική μηχανική, και ιδιαίτερα κατά τη φάση της ανάλυσης και του σχεδιασμού μιας κατασκευής, μπορούν να παίξουν σημαντικό ρόλο, επηρεάζοντας όχι μόνο την ασφάλεια και την αξιοπιστία της κατασκευής και των μερών από τα οποία αποτελείται, αλλά και την ποιότητα των επιδόσεών της. Η απόκριση ενός δομικού συστήματος μπορεί να είναι ιδιαίτερα ευαίσθητη στις αβεβαιότητες των ιδιοτήτων των υλικών, των συνθηκών της κατασκευής, των εξωτερικών φορτίων και των αναλυτικών ή αριθμητικών μεθόδων που χρησιμοποιήθηκαν για την προσομοίωση του φυσικού προβλήματος. Για να ληφθούν υπόψη αυτές οι αβεβαιότητες, έχουν αναπτυχθεί τις τελευταίες δεκαετίες κατάλληλες μέθοδοι στοχαστικής ανάλυσης των κατασκευών. Το βέλτιστο αποτέλεσμα που προκύπτει από μία προσδιοριστική (αιτιοκρατική) θεώρηση της διαδικασίας βελτιστοποίησης η οποία αγνοεί τη διασπορά των τιμών των παραμέτρων που επηρεάζουν την απόκριση της κατασκευής, έχει περιορισμένη αξία και αξιοπιστία, καθώς μπορεί να επηρεαστεί σημαντικά από εγγενείς αβεβαιότητες τόσο του φυσικού προβλήματος όσο και του αριθμητικού προσομοιώματος. Το προσδιοριστικό βέλτιστο μπορεί επομένως να σχετίζεται με μη αποδεκτή τιμή της πιθανότητας αστοχίας, ή μπορεί να είναι ιδιαίτερα ευαίσθητο σε σχετικά μικρές διακυμάνσεις κάποιων παραμέτρων. Η ανάπτυξη στοχαστικών-πιθανοτικών μεθόδων ανάλυσης κατά τις δύο τελευταίες δεκαετίες έχει κεντρίσει το ενδιαφέρον των ερευνητών για την εισαγωγή των εννοιών της αβεβαιότητας και της τυχηματικότητας στις διατυπώσεις των προβλημάτων βέλτιστου σχεδιασμού των κατασκευών. Για να ληφθούν υπόψη οι αβεβαιότητες στα πλαίσια ενός προβλήματος βελτιστοποίησης, πρέπει να χρησιμοποιηθούν διατυπώσεις βασισμένες στην πιθανοτική φύση του προβλήματος, χρησιμοποιώντας τη στοχαστική ανάλυση και τη θεωρία πιθανοτήτων. Ο στόχος της διατριβής είναι η ενοποιημένη αντιμετώπιση της στοχαστικής ανάλυσης και του βέλτιστου σχεδιασμού των κατασκευών και η παροχή των απαραίτητων υπολογιστικών εργαλείων για την επίλυση του προβλήματος της βελτιστοποίησης των κατασκευών με θεώρηση αβεβαιοτήτων. Ο στόχος αυτός επιτυγχάνεται με την ανάπτυξη κατάλληλων αλγορίθμων για την επίλυση του στοχαστικού προβλήματος βελτιστοποίησης. Για να αντιμετωπιστούν αυτά τα προβλήματα με αποδοτικό τρόπο, πρέπει να χρησιμοποιηθούν διάφοροι αλγόριθμοι και μεθοδολογίες βέλτιστου σχεδιασμού, τόσο για προβλήματα μίας όσο και για προβλήματα πολλών αντικειμενικών συναρτήσεων, καθώς και επαρκείς μεθοδολογίες για την αντιμετώπιση του στοχαστικού προβλήματος. Παρά την εφαρμογή προχωρημένων μεθοδολογιών για την αντιμετώπιση των παραπάνω προβλημάτων, το υπολογιστικό κόστος για τη θεώρηση των αβεβαιοτήτων σε ένα πρόβλημα βελτιστοποίησης παραμένει εξαιρετικά υψηλό, ειδικά για προβλήματα πραγματικών κατασκευών μεγάλης κλίμακας με πολλές μεταβλητές σχεδιασμού ή/και αβέβαιες παραμέτρους. Για τον περιορισμό του προβλήματος αυτού και τη μείωση του υπολογιστικού κόστους, στην παρούσα διατριβή προτείνεται η εφαρμογή Νευρωνικών Δικτύων (Neural Networks, NNs) ως μετα-μοντέλων (metamodels), τα οποία δίνουν ικανοποιητικές λύσεις με ιδιαίτερα χαμηλό υπολογιστικό κόστος. Η διατριβή αποτελείται συνολικά από εννέα κεφάλαια, τη βιβλιογραφία και τρία παραρτήματα. Η διάρθρωσή της έχει ως εξής: Μετά από την εισαγωγή του 1ου Κεφαλαίου, το 2° Κεφάλαιο εξετάζει το θέμα των αβεβαιοτήτων σε προβλήματα δομοστατικής μηχανικής. Το 3° Κεφάλαιο παρουσιάζει τη διατύπωση του προβλήματος βελτιστοποίησης με μία αντικειμενική συνάρτηση (single-objective optimization), ενώ το 4° Κεφάλαιο εξετάζει το πρόβλημα της βελτιστοποίησης με πολλές αντικειμενικές συναρτήσεις (multi-objective optimization). Τα βασικά στοιχεία των Νευρωνικών Δικτύων (Neural Networks) και οι εφαρμογές τους σε προβλήματα δομοστατικής μηχανικής παρουσιάζονται στο 5° Κεφάλαιο. Το 6° Κεφάλαιο εξετάζει το πρόβλημα της βελτιστοποίησης κατασκευών με θεώρηση αβεβαιοτήτων, στο οποίο παρουσιάζονται μεταξύ άλλων οι κυριότερες διατυπώσεις αυτών των προβλημάτων, το πρόβλημα του Βέλτιστου Σχεδιασμού με βάση την Αξιοπιστία (Reliability-Based Design Optimization, RBDO), το πρόβλημα του Εύρωστου Βέλτιστου Σχεδιασμού (Robust Design Optimization, RDO) και το συνδυασμένο πρόβλημα του Εύρωστου Σχεδιασμού με βάση την Αξιοπιστία (Reliability-based Robust Design Optimization, RRDO). Οι αριθμητικές εφαρμογές της διατριβής είναι χωρισμένες σε δύο ενότητες - Μέρη A και Β, τα οποία παρουσιάζονται στο 7° και 8° Κεφάλαιο, αντίστοιχα. Το Μέρος A (7° Κεφάλαιο) περιέχει τις προσδιοριστικές αριθμητικές εφαρμογές, όπου οι αβεβαιότητες δεν λαμβάνονται υπόψη στο αριθμητικό προσομοίωμα. Στο Μέρος Β (8° Κεφάλαιο) εξετάζονται οι πιθανοτικές αριθμητικές εφαρμογές, όπου οι αβεβαιότητες διαδραματίζουν δεσπόζοντα ρόλο. Το 9° Κεφάλαιο περιέχει τα συμπεράσματα της διατριβής, την πρωτότυπη συνεισφορά της και κατευθύνσεις για μελλοντική έρευνα. Τέλος, παρουσιάζεται η βιβλιογραφία και τρία παραρτήματα: Το Παράρτημα A περιέχει τη σημειογραφία και τους μαθηματικούς συμβολισμούς που υιοθετήθηκαν, το Παράρτημα Β περιέχει τα ακρωνύμια και τις συντμήσεις που χρησιμοποιήθηκαν, και το Παράρτημα C περιέχει μια αναλυτική λίστα με τις δημοσιεύσεις του συγγραφέα",Προηγμένες υπολογιστικές τεχνικές βέλτιστου σχεδιασμού κατασκευών με αβεβαιότητες,,Εθνικό Μετσόβιο Πολυτεχνείο (ΕΜΠ),,,core
229168767,2008-10-01T07:00:00,"Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC\u27s features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications",Real-Time Implementation of Intelligent Modeling and Control Techniques on a PLC Platform,https://core.ac.uk/download/229168767.pdf,Scholars\u27 Mine,,,core
20641120,2007-05-03T12:37:11,"Over the last decades, calibration techniques have been widely used in robotics since they represent a cost-effective solution for improving the accuracy of robots and machine-tools. They only involve software modification without the necessity of revising the robot design or tightening the manufacturing tolerances.  The goal of this thesis is to propose a procedure that guides the engineer through the calibration of a given multi-DOF flexure parallel robot within sub-µm accuracy. Two robots having 3 and 6 degrees of freedom have been considered as a case-study throughout the work. As in any calibration procedure, the work has been conducted on three different fronts: measurement, data processing and validation. The originality of this thesis in respect to published material lies in these three points.  Measurements were carried out in a chamber inside which the measuring environment was protected against mechanical and thermal perturbations. In particular, the temperature variations experienced by the different parts of the measuring loop during a typical measurement session were stabilized within less than ± 0.1 °C.  Proposed procedures allow the collection of reliable sets of data on the two robots. Delicate aspects of practical implementation are discussed. In particular, the problem of collecting a complete set of 6D data within accuracies in the nanometre range, for which there is still a lack of standard equipment, is solved using a procedure comprising several steps and making use of existing instrumentation.  Suggestions for future investigations are given, regarding either long-term research problems or short-term industrial implementation issues.  Data processing was performed using two different techniques in order to reach absolute accuracies after calibration better than ± 100 nm for translations and ± 3 arcsec for rotations (± 0.3 arcsec inside a more restricted range of ± 0.11°).  The first method is called the ""model-based approach"" and requires the use of a known analytical relationship between the motor and operational coordinates of the robot. This relationship involves a certain number of parameters that can be related to the geometry of the robot (physical models) or simply mathematical coefficients of an approximating mathematical function (behavioural models). In the case of high-precision multi-DOF flexure parallel robots, we show that polynomial-based behavioural models are preferable to physical models in terms of accuracy for data processing tasks.  In the second method, called the ""model-free approach"", the user does not need to model explicitly the main error sources (or their effect) affecting the robot accuracy. A model-free approach has been implemented using Artificial Neural Networks. We show that, using a heuristic search based on a decision-tree, the architecture of a network with satisfactory prediction capability can be found systematically. In particular, this algorithm can find a network able to predict the direct correspondence between the motor and operational coordinates (within the desired accuracy) without the help of the Inverse Geometric Model of the robot, i.e. even if the nominal geometry of the robot being calibrated remains unknown. This result contradicts conclusions reported by previous researchers.  It is claimed that any robot (not necessarily a high-precision flexure parallel mechanism) can be calibrated by means of a ""neural approach"" in which the architecture of an appropriate network is determined with the help of our algorithm. Two examples (other than the robots measured in this thesis) are given to illustrate this universality.  In the last part of this work, we provide a feasibility study on the use of indentation, a technique traditionally used for material testing, as a validation procedure to assess the accuracy of the calibrated degrees of freedom.  The industrial interest of this technique lies in the fact that the robot is asked to execute similar motions to those involved in a real micro-machining operation.Dans les dernières décennies, les techniques d'étalonnage ont connu un succès fulgurant en robotique. Elles sont en effet une solution attractive pour améliorer la précision d'un manipulateur industriel donné puisqu'elles ne demandent qu'une modification au niveau du logiciel. Le but de cette thèse est de proposer une procédure visant à guider l'ingénieur dans l'étalonnage des robots parallèles de précisions sub-µm à articulations flexibles et ayant plusieurs degrés de liberté. Deux robots à 3 et 6 degrés de liberté sont pris comme cas d'étude. Comme dans n'importe quelle procédure d'étalonnage, le travail a été conduit sur plusieurs fronts: procédés de mesure, traitement de données et validation. L'originalité de cette thèse réside dans ces trois aspects. Les mesures ont eu lieu à l'intérieur d'une chambre visant à garantir une bonne isolation mécanique et thermique de l'environnement métrologique. En particulier, les variations de température subies par les différentes parties de la boucle de mesure pendant la durée d'une mesure typique, ont été stabilisées avec une tolérance meilleure que ± 0.1 °C. Des procédures ont été proposées permettant l'acquisition de données fiables sur les deux robots. Les aspects délicats liés à l'implémentation pratique de ces procédures sont discutés. En particulier, le problème de l'acquisition de mesures 6D avec des précisions nanométriques (pour lequel il n'y a toujours pas d'équipement standard) est résolu par l'intermédiaire d'une procédure en plusieurs étapes et faisant intervenir des instruments existants. Des suggestions sont formulées pour la suite des travaux soit sur le long terme, au niveau de la recherche fondamentale, soit dans une perspective d'implémentation industrielle à court terme. Deux techniques ont été proposées pour le traitement des données permettant d'atteindre des précisions absolues après étalonnage meilleures que ± 100 nm pour les translations et ± 3 arcsec pour les rotations (± 0.3 arcsec à l'intérieur d'une plage plus restreinte de ± 0.11°). La première méthode requiert un modèle analytique décrivant la correspondance entre les coordonnées articulaires et les coordonnées opérationnelles du robot. Ce modèle fait intervenir un certain nombre de paramètres pouvant être liés à la géométrie du robot (modèles à représentation physique) ou simplement des coefficients d'une fonction mathématique (modèles de comportement). Dans le cas des robots parallèles à articulations flexibles, il a été démontré que des modèles de comportement basés sur des fonctions polynomiales priment sur des modèles à représentation physique en termes de précision dans le traitement de données. Dans une deuxième approche, l'utilisateur n'a pas besoin de modéliser explicitement les différentes sources d'erreur (ou leur effet). Une approche de ce type a été implémentée en utilisant des réseaux de neurones artificiels. On démontre qu'une heuristique de type ""arbre de décision"" permet de déterminer systématiquement l'architecture d'un réseau pouvant fournir une précision satisfaisante au problème. En particulier, il est possible d'obtenir un réseau pouvant prédire la correspondance directe entre coordonnées opérationnelles et articulaires avec la précision escomptée sans l'aide du modèle géométrique inverse du robot, c'est-à-dire même en ne connaissant pas a priori la géométrie nominale du robot à étalonner. Ce dernier résultat contredit les résultats obtenus par d'autres chercheurs. Il est dit que tout robot (pas nécessairement un robot parallèle de haute précision à articulations flexibles) peut être étalonné par le biais d'une ""approche neuronale"" dans laquelle l'architecture d'un réseau approprié est déterminée par l'algorithme proposé. Cette universalité est illustrée sur 2 exemples autres que les robots mesurés dans ce travail. Dans la dernière partie de cette thèse, on fait une étude de faisabilité sur l'utilisation de l'indentation, une technique employée traditionnellement dans la caractérisation des propriétés mécaniques des matériaux, en tant que procédé de validation pour contrôler la qualité de l'étalonnage effectué. L'intérêt industriel de cette technique réside dans le fait que le robot y exécute des mouvements semblables à ceux qu'il serait amené à faire lors d'un procédé typique de micro-usinage",Calibration of high-precision flexure parallel robots,,,,,core
4763058,2010-08-27T00:00:00,"Growing needs for precise manipulation in medical surgery, manufacturing automation and structural health monitoring have motivated development of high accuracy, bandwidth and cost-effective sensing systems. Among these is a class of multi-axis electromagnetic devices where embedded magnetic fields can be capitalized for compact position estimation eliminating unwanted friction, stiction and inertia arising from dedicated and separate sensing mechanisms. Using fields for position measurements, however, is a challenging 'inverse problem' since they are often modeled in the 'forward' sense and their inverse solutions are often highly non-linear and non-unique. A general method to design a multisensor system that capitalizes on the existing magnetic field in permanent magnet (PM) actuators is presented. This method takes advantage of the structural field symmetry and meticulous placement of sensors to discretize the motion range of a PM-based device into smaller magnetic field segments, thereby reducing the required characterization domain. Within these localized segments, unique field-position correspondence is induced using field measurements from a network of multiple-axis sensors. A direct mapping approach utilizing trained artificial neural networks to attain multi-DOF positional information from distributed field measurements is employed as an alternative to existing computationally intensive model based methods which are unsuitable for real-time control implementation. Validation and evaluation of this technique are performed through field simulations and experimental investigation on an electromagnetic spherical actuator. An inclinometer was used as a performance comparison and experimental results have corroborated the superior tracking ability of the field-based sensing system. While the immediate application is field-based orientation determination of an electromagnetic actuator, it is expected that the design method can be extended to develop other sensing systems that harnesses other scalar, vector and tensor fields.PhDCommittee Chair: Lee, Kok-Meng; Committee Member: Sadegh, Nader; Committee Member: Singhose, William; Committee Member: Wang, Yang; Committee Member: Zhang, Fumi",Development of magnetic field-based multisensor system for multi-DOF actuators,,Georgia Institute of Technology,,,core
295051976,2008-01-01T00:00:00,"A crescente utilização de conceitos de programação avançada e inteligência artificial tem criado numerosas e novas aplicações desses algoritmos em processos industriais, resultando em expressivos ganhos de produtividade e economia de recursos. Algoritmos baseados em redes neurais tem demonstrado ser altamente acertivos, e quando empregados em conjunto a sistemas modelados em lógica fuzzy incorporam ao banco de dados subjetividade e experiências adquiridas ao longo do tempo, dotando o programa de capacidade de reagir de forma mais adequada frente a diferentes condições de processo e situações inesperadas. No processo de lingotamento contínuo é realizada a solidificação do aço na forma de placas, as quais serão enviadas para processamentos posteriores, exigindo que sua aparência superficial seja livre de irregularidades. Elevados requisitos de qualidade visam garantir maior produtividade, estabilidade operacional e acabamento superficial das bobinas laminadas. O primeiro estágio da solidificação do aço se inicia no molde onde o aço líquido, em contato com as faces do molde, forma uma casca externa solidificada. Um dos fenômenos mais prejudiciais à produção e manutenção da regularidade superficial da placa é o colamento do aço na face cobreada do molde, que leva a ruptura da pele de aço em formação. O “rompimento de pele” como é denominado, gera parada imediata e emergencial do lingotamento contínuo, bem como o sucatamento das placas em produção. De forma a se evitar a ocorrência de rompimentos de pele, os moldes de cobre possuem duas linhas de termopares instalados em suas faces para monitoramento em tempo real das condições de troca de calor durante o lingotamento. Um programa faz a comparação contínua da evolução do perfil de temperatura com os existentes em seu banco de dados, referentes a eventos de colamento de aço no molde. O programa Mold Sticker Detection (MSD) tem o objetivo de inibir a ocorrência de rompimentos de pele. Quando o MSD entende que em um determinado momento está ocorrendo o colamento de aço no molde, este gera um alarme reduzindo instantânemente a velocidade de lingotamento para valores de segurança. Essa redução abrupta tem por objetivo recompor a pele rompida, mas em contra partida causa marcas profundas na superfície da placa, além das geradas pelo próprio colamento. Este trabalho objetiva desenvolver um modelo baseado em lógica fuzzy a ser incorporado ao sistema atual de detecção de rompimento – MSD, procurando reduzir a geração de alarmes de colamento através da determinação da velocidade de lingotamento ideal condizentes com as condições atuais de extração de calor no molde. Para medição da eficácia do modelo proposto foram avaliadas as perdas de produtividade, bem como o número de ocorrência de alarmes de colamento e rompimentos de pele.A growing application of advanced programming concepts and artificial intelligence, has created a numerous and new usages for these algorithms in industrial processes resulting in expressive gains on productivity and resource savings. Algorithms based in neural networks has been proving its higher sharpness, and when applied together with fuzzy conceived systems may embody to the data basis subjectivity and experiences acquired through time, dowering the program of capacity to react more properly under different process conditions and uncommon situations. During the continuous casting process, the steel is solidified under a slab shape, which will be forward to post processes, demanding a smooth and roughness free surface appearance. Higher quality requirements aim to guarantee productivity increase, operational stability and greater superficial finishing of the laminated coils. The very first stage of steel solidification has its start on the mould where the liquid steel, facing the mould copper plates, forms an external solidified shell. One of the most harmful phenomena to the production regularity maintenance and superficial evenness of the slab is the steel sticking to the copper mould plates, which leads to the rupture of the solidifying shell. The shell break-out, as nominated, causes instant and emergency stoppage of the continuous casting machine and inproduction slabs scrapping. In order to avoid break-out occurrences, the mould has two thermocouple lines installed on the plates for real time monitoring of the heat transfer conditions during casting. A software compares the actual thermal profile with the existent data base concerning sticker events on the mould. The program Mold Sticker Detection (MSD) is used to inhibit break-outs. When the MSD understands that in a specific time a sticking is progress, it generates an alarm reducing instantaneously the casting speed to a safety value. This sudden reduction is provided to give enough time to have the steel shell rebuilt, but also marks deeply the slab surface besides those deriving from the sticking itself. This work has the objective to develop a fuzzy based model to be incorporated to the existing break-out detect system – MSD, aiming to reduce the occurrences of sticker alarms by establishing the ideal casting speed concerning the actual heat extraction conditions on the mould. To measure the effectiveness of the proposed model, will be evaluated the production losses, number of sticker alarms and break-outs as well",Modelo para predição da ocorrência de alarmes de colamento de aço no molde utilizando lógica Fuzzy.,,"Programa de Pós-Graduação em Engenharia de Materiais. Rede Temática em Engenharia de Materiais, Pró-Reitoria de Pesquisa e Pós-Graduação, Universidade Federal de Ouro Preto.",,,core
23921774,2007-11-22,". This paper addresses the problem of building classifiers from large datasets stored in relational databases, in a client-server context. This situation is common in industry. We identify a large class of algorithms whose data access can be restricted to SQL queries of a particular form, which we call primitives. We propose to enhance the performance of these algorithms by optimizing at the primitive (query) level, show how this can be achieved, and describe a prototype that has been implemented at Essex. We present performance measurements for the prototype in conjunction with a version of the C4.5 decision tree algorithm. Results are given for two systems, one of them a parallel DBMS. We are particularly interested in the extent to which a system of this kind can exploit the parallelism of the server.  Keywords: client-server, scalability, primitives, C4.5, parallel  1. Introduction  Data mining, which is to say the application of automated machine learning techniques to large real-..",Optimizing Client-Server Performance When Mining Large Databases,,,,,core
24664872,2008-04-01,"Abstract – The Integrated Intelligent Decision-Making and Information Discovery (IIDI) system was created for data management command and control (C2) systems. This paper focuses on one IIDI technique, grid fuzzy neural networks (GFNNs), which can learn and support decision making, and information/knowledge discovery. The IIDI GFNNs were developed, trained, and tested on real sample datasets to detect or discover system attacks and abnormalities on a spacecraft. The IIDI prototypes were developed and demonstrated, including both client-server and Web-based software packages that accurately detected abnormalities and potential attacks in real sample datasets. Our experiments proved that the IIDI GFNNs require less training time, while being two to three times more accurate and 15 to 25 times faster than commercial neural net products. In addition, the IIDI system can be retrained to learn, to discover information, to and perform new tasks in new environments. Keywords: Intelligent system, Neural network, Decision-making, Information technology In recent years, technologies such as standard neural networks (SNNs) and evolutionary algorithms have been applied to pattern recognition, data mining, and knowledge discovery. However, no single technique alone, including SNNs and evolutionary algorithms, can solve real world problems in either industry or the military. For instance, SNNs exhibit slow learning and low accuracy [1]. These weaknesses pervade all commercial SN",Integrated Intelligent Decision-Making and Information Discovery,,,,,core
20708896,2008-04-03,"Abstract — This paper describes an effective medical claim fraud/abuse detection system based on data mining used by a Chilean private health insurance company. Fraud and abuse in medical claims have become a major concern within health insurance companies in Chile the last years due to the increasing losses in revenues. Processing medical claims is an exhausting manual task carried out by a few medical experts who have the responsibility of approving, modifying or rejecting the subsidies requested within a limited period from their reception. The proposed detection system uses one committee of multilayer perceptron neural networks (MLP) for each one of the entities involved in the fraud/abuse problem: medical claims, affiliates, medical professionals and employers. Results of the fraud detection system show a detection rate of approximately 75 fraudulent and abusive cases per month, making the detection 6.6 months earlier than without the system. The application of data mining to a real industrial problem through the implementation of an automatic fraud detection system changed the original non-standard medical claims checking process to a standardized process helping to fight against new, unusual and known fraudulent/abusive behaviors. I",A Medical Claim Fraud/Abuse Detection System based on Data Mining: A Case Study in Chile,,,,,core
21070740,2009-10-28,"The direct implementation of the control algorithm from the simulation environment to the industrial process controller is the aim of many current papers. The sequence of direct implementation usually consists of three steps: simple simulation experiment, real time communication with the real process and monitoring of the control algorithm after implementation. The direct implementation enables us to describe the quantization effect. The issue of quantization effect disturbance given by A/D and D/A converters is often forgotten. This paper shows the second and the third implementation step where the first one was described in the 10th Zittau Fuzzy Colloquium. The advantage of identification based on neural networks in adaptive LQ controller that overcomes presented problem is introduced below",The Issue of Quantization Effect in Direct Implementation of Adaptive LQ Controller with NN Identification into PLC,,,,,core
20828589,2008-08-14,"With the recent interest and enthusiasm in the industry toward smart wells, intelligent fields, and real-time analysis and interpretation of large amounts of data for process optimization, our industry’s need for powerful, robust, and intelligent tools has significantly increased. Operations such as asset evaluation; 3D- and 4D-seismicdata interpretation; complex multilateral-drilling design and implementation; log interpretation; building of geologic models; well-test design, implementation, and interpretation; reservoir modeling; and simulation are being integrated to result in comprehensive reservoir management. In recent years, artificial intelligence (AI), in its many integrated flavors from neural networks to genetic optimization to fuzzy logic, has made solid steps toward becoming more accepted in the mainstream of the oil and gas industry. In a recent set of JPT articles, 1–3 fundamentals of these technologies were discussed. This article covers some of the most recent and advanced uses of intelligent systems in our industry and discusses their potential role in our industry’s future",DISTINGUISHED AUTHOR SERIES Recent Developments in Application of Artificial Intelligence in Petroleum Engineering,,,,,core
24472772,2008-02-06,"Abstract: In spite of the growing number of KMS emanating from academia, only few of these systems seem to be actively used by their creators. We can only speculate about the reasons, but a lack of usability and support for real-life business processes is likely to be one of them. This paper presents the Infolayer, a KMS developed at the Artificial Intelligence unit at Dortmund university. Originally developed as an integrated information infrastructure for human users and software agents within a specific project, it has grown into a general KMS solution. Where other systems are built around complex reasoning capabilities based on description logics, the Infolayer’s focus is ease of use, customizability and integration of industry standards like UML. The system is being actively used to drive the intranet/internet presence of our unit as well as a number of other projects. ",The Infolayer – A Simple Knowledge Management System Put to Use in Academia,,,,,core
212780410,2009-01-01T00:00:00,"In this paper, referring to the Model for  General Knowledge Management within the 
Enterprise (MGKME), we emphasize two of the operating elements of this model, which are 
essential to insure the organizational learning process that leads people to appropriate and use 
concepts, methods and tools of KM considered as an innovative technology: the “Ad hoc 
Infrastructures” element, and the “Organizational Learning Processes” element. The Nonaka’s SECI 
models, and the Japanese concept of Ba, underlie these two elements. The case of the “Semi-opened 
Infrastructure” model implemented to deploy Artificial Intelligence and Knowledge-based Systems 
within a large industrial company illustrates what could be the application of these concepts in the 
real field. Meanwhile, we partially validate MGKME. Furthermore, we consolidate the “Semi-
opened Infrastructure” model, which becomes a pattern of reference allowing implementing an “Ad 
hoc Infrastructure” for innovative technologies deployment.ou",Establishing an Ad Hoc Infrastructure for Innovative Technology Deployment: the Case of Knowledge-Based Systems,,,,,core
300019728,2009-01-01T00:00:00,"Intra-cabin communication is a key challenge in building entertainment and operating systems for next generation aircraft, and is a challenge that has attracted significant re-search in recent years. Many potential systems are available commercially, and several systems are currently in service. For these existing wired systems, there are three signifi-cant challenges need to be solved: the weight of wires needed to connect the systems, the potential for electromagnetic interference caused by the systems, and the scalability of the systems for future needs as well as future aircraft. The anticipated adoption of wireless systems addresses the first two issues, but not the third. 
In this project, infrared-based communication has been adopted as an attractive wireless solution. The project focuses on peer-to-peer communication schemes, and en-compasses the use of redundant communication paths as a method to address the problem of line-of-sight blocking. This project also considers another practical issue, namely that the current centralised server architecture in aircraft entertainment systems has resulted in data congestion and scalability limitations.
This report proposes a network architecture, named the PEER-TO-POINT-TO-POINT-TO-PEER (P4) NETWORK, consisting of point-to-point infrared-based communication paths with an overall high level of redundancy. A machine-learning routing approach, Q-ROUTING, is employed to increase the data rates by combining several redundant commu-nication paths. The proposed realisation of this routing algorithm, Q-ROUTING PROTOCOL, is a distributed, dynamic, lightweight and adaptive routing protocol, making it possible to interconnect multiple LANs without the existence of a central server, as well as to in-crease the stability of the overall network to temporal individual link breakdowns. Furthermore, this report also describes a software infrastructure, namely the P4 PROTOCOL SUITE, which efficiently implements the proposed ideas on the Windows OS platform. The core component of this infrastructure is the P4-BRIDGE, which interconnects the TCP/IP and IrDA protocol stacks. P4-BRIDGE transparently intercepts TCP/IP packets and route them over the infrared-links, if necessary.
Throughout each short phase of the SCOUTING PROJECT, which is the industrial project that this work is part of, different versions have been developed: the P4-NETWORK SIMULATOR, the P4-ENGINE and the P4-DRIVER. Firstly, the P4-NETWORK SIMULATOR emu-lates the entire network and reveals the possibility of streaming high-definition movies across the network. Next, utilising the highest development platform, P4-ENGINE realises the proposed ideas and shows a video streaming application in the real network. Lastly, although being under continuing development, P4-DRIVER has enabled the availability of the network to a wide range of compatible applications. The network is tested with exist-ing application software such as pinging, file transferring and even Windows file sharing. In particular, a pinging test over a path of four hops takes less than a hundred millise-conds.
As a primitive but fundamental result, this work has shown different promising aspects of the proposed solution on a limited hardware platform, including stability, availability, scalability and high performance. Nevertheless, further research and experi-ments need to be carried out to fully exploit the redundancy architecture of the P4-NETWORK.Bachelor of Engineering (Computer Engineering",P4-network : a networking infrastructure for intra-aircraft cabin wireless communication,,,,,core
24581174,2008-02-07,"Abstract — The concepts and methodology needed for designing, developing, and implementing real life applications based on multi-agent systems are today still a challenge for researchers in Artificial Intelligence and Computer Science. Industrial-strength multi-agent systems require, among other things, reusability, i.e. the capability of not having to design and implement from scratch a new multi-agent system for every new application domain. In order to improve the reusability of multi-agent systems, we have taken a knowledge modelling stance coming from the research on knowledge engineering, and adapted its insights to deal with multi-agent systems. The resulting contributions are organized in the ORCAS framework, comprising conceptual guidelines to design, methodological commitments for development, and an implemented platform to make concrete these abstract notions and effectively experiment with them. This paper describes the ORCAS e-Institution, and agent platform for developing and deploying open, reusable and configurable cooperative multiagent systems. I","The ORCAS e-Institution: a Platform to Develop Open, Reusable and Configurable Multi-Agent Systems",,,,,core
290044901,2008,"One of the areas that needs further improvement
within E-Learning environments via Internet (A big effort is
required in this area if progress is to be made) is allowing students
to access and practice real experiments in a real laboratory,
instead of using simulations [1]. Real laboratories allow students
to acquire methods, skills and experience related to real
equipment, in a manner that is very close to the way they are
being used in industry. The purpose of the project is the study,
development and implementation of an E-Learning environment
to allow undergraduate students to practice subjects related to
Robotics and Artificial Intelligence. The system, which is now at a
preliminary stage, will allow the remote experimentation with real
robotic devices (i.e. robots, cameras, etc.). It will enable the
student to learn in a collaborative manner (remote participation
with other students) where it will be possible to combine the onsite
activities (performed “in-situ” within the real lab during the
normal practical sessions), with the “on-line” one (performed
remotely from home via the Internet). Moreover, the remote
experiments within the E-Laboratory to control the real robots
can be performed by both, students and even scientist. This
project is under development and it is carried out jointly by two
Universities (UPC and UJI). In this article we present the system
architecture and the way students and researchers have been able
to perform a Remote Programming of Multirobot Systems via web",Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,,Westing Publishing Co.,,,core
54315909,2007-01-01T00:00:00,"La geometria descrittiva è la scienza che insegna a rappresentare, modellare e ricostruire nello spazio le forme a tre dimensioni che sono oggetto della invenzione in architettura, nella ingegneria e nel disegno industriale. Benché abbia ricevuto il suo nome da Gaspard Monge, intorno al 1795, questa scienza è tra le più antiche, tra quante fanno parte del patrimonio culturale dell'umanità, e comprende al suo interno importanti teorie e applicazioni quali la prospettiva, la teoria delle ombre e del chiaroscuro, il disegno dell'ordine architettonico, il taglio delle pietre e dei legnami, il disegno degli ingranaggi e molte altre ancora che qui non occorre ricordare. La geometria descrittiva è perciò, da sempre, uno strumento formativo essenziale nei curricula degli studenti architetti, ingegneri e designer. Nell'ultimo quarto dello scorso secolo, con il rapido sviluppo delle tecnologie informatiche (hardware e software), i problemi che avevano prima una soluzione esclusivamente grafica hanno trovato una soluzione digitale, vale a dire una soluzione di natura essenzialmente matematica che però si manifesta nei modi della geometria descrittiva classica e cioè attraverso immagini. Lo sviluppo di questi algoritmi ha anche arricchito il novero delle teorie di carattere geometrico descrittivo estendendo, ad esempio, il repertorio delle curve e delle superfici impiegate nella progettazione dalle coniche e dalle quadriche alle NURBS, il repertorio degli effetti della luce sui corpi che è possibile rappresentare con cura, dal semplice chiaroscuro della legge di Lambert, ai riflessi, ai punti brillanti, alle trasparenze degli attuali rendering, etc. A fronte di questa evoluzione, tuttavia, gli studi e, conseguentemente, l'insegnamento della geometria descrittiva, restano radicati alle forme antiche e questo radicamento provoca una pericolosa dicotomia tra l'insegnamento tradizionale, non più attuale ma ricco della sua storia, e l'insegnamento delle tecniche informatiche, attuale, ma ridotto a mera esecuzione di comandi programmati, avulsi da qualsiasi contesto teorico e perciò anche incontrollati. Urge dunque un rinnovamento della geometria descrittiva che, considerando il forte impatto dell'informatica in questo settore, può essere visto come una vera e propria rifondazione.The descriptive geometry is the science that teaches how to represent, model, and reconstruct the three-dimensional space forms which are the subject of the invention in architecture, in engineering and industrial design. Although it has received its name from Gaspard Monge, around 1795, this is one of the oldest science, including how many are part of the cultural heritage of mankind, and includes in its interior significant theories and applications such as Outlook, the theory of shadows and chiaroscuro, of the architectural design, the cutting of stones and wood, the design of the gears and many others here who do not need to remember. The descriptive geometry is therefore always a training tool essential in the curricula of students architects, engineers and designer.Nell 'last quarter of the last century, with the rapid development of information technology (hardware and software), the problems they had before a solution exclusively graphics have found a digital solution, namely a solution of essentially mathematics but manifests itself in the manner of descriptive geometry and classical ie through images. The development of these algorithms has also graced the ranks of the theories of geometric character descriptive extending, for example, the repertoire of curves and surfaces used in the design of the taper and the quadric NURBS, the repertoire of the effects of light on bodies that can be act with care, from simple chiaroscuro of Lambert law, the reflections, the shining points, the transparency of the current rendering, etc.A face of this development, however, studies and, therefore, the teaching of descriptive geometry, remain rooted the old forms and rooting this causes a dangerous dichotomy between the traditional teaching, no longer current but rich in its history and the teaching of information technology, the current, but reduced to mere execution of programmed commands, divorced from any context and theoretical therefore also incontrollati.Urge then a renewal of descriptive geometry, that given the strong impact of information technology in this area, can be seen as a real overhaul",Per una geometria descrittiva attuale,,Gangemi Editore,,,core
4733373,2008-10-28T00:00:00,"The scientific domain of this thesis is optimization under uncertainty for discrete event stochastic systems. In particular, this thesis focuses on the practical implementation of the Dynamic Programming (DP) methodology to discrete event stochastic systems. Unfortunately DP in its crude form suffers from three severe computational obstacles that make its  imple-mentation to such systems an impossible task. This thesis addresses these obstacles by developing and executing practical Approximate Dynamic Programming (ADP) techniques. 

Specifically, for the purposes of this thesis we developed the following ADP techniques. The first one is inspired from the Reinforcement Learning (RL) literature and is termed as Real Time Approximate Dynamic Programming (RTADP). The RTADP algorithm is meant for active learning while operating the stochastic system. The basic idea is that the agent while constantly interacts with the uncertain environment accumulates experience, which enables him to react more optimal in future similar situations. While the second one is an off-line ADP procedure

These ADP techniques are demonstrated on a variety of discrete event stochastic systems such as: i) a three stage queuing manufacturing network with recycle, ii) a supply chain of the light aromatics of a typical refinery, iii) several stochastic shortest path instances with a single starting and terminal state and iv) a general project portfolio management problem.

Moreover, this work addresses, in a systematic way, the issue of multistage risk within the DP framework by exploring the usage of intra-period and inter-period risk sensitive utility functions. In this thesis we propose a special structure for an intra-period utility and compare the derived policies in several multistage instances.Ph.D.Committee Chair: Jay H. Lee; Committee Member: Martha Grover; Committee Member: Matthew J. Realff; Committee Member: Shabbir Ahmed; Committee Member: Stylianos Kavadia",Multistage decisions and risk in Markov decision processes: towards effective approximate dynamic programming architectures,https://core.ac.uk/download/4733373.pdf,Georgia Institute of Technology,,,core
56921514,2009,"In this paper new CNN based visual algorithms for the control of welding processes are proposed. The high dynamics of laser welding in several manufacturing processes ranging from automobile production to precision mechanics requires the introduction of new fast real time controls. In the last few years, analogic circuits like cellular neural networks (CNN) have obtained a primary place in the development of efficient electronic devices because of their real-time signal processing properties. Furthermore, several pixel parallel CNN based architectures are now included within devices like the family of EyeRis systems [1]. In particular, the algorithms proposed in the following have been implemented on the EyeRis system v1.2 with the aim to be run at frame rates up to 20 kHz",New CNN based algorithms for the full penetration hole extraction in laser welding processes,,,10.1109/ISCAS.2009.5118362,,core
20643496,2008-12-11T14:06:11,"There is a large number of possible applications in the field of mobile robotics: Mail delivery robots, domestic or industrial vacuum cleaners, surveillance robots, demining robots and many others could be very interesting products. Despite this potential market and the actual technology, only few simple systems are commercially available. This proves that there are several important and problematic issues in this field, mainly at the intelligence level. As a reaction to the failure of the classical artificial intelligence applied to the field of mobile robotics, several new approaches have been proposed. Artificial neural networks are one of these, and genetic algorithms, supported by the Artificial Life trend, are also getting more and more consideration. These two techniques have already been applied to mobile robotics, but mainly in simulation, and without a final test on a real mobile robot. The use of physical robots for this research seems to be still problematic due to the lack of efficient tools. Several neural structures for the control of mobile robots have been analysed in this work. All experiences have been carried out on physical robots. To reach this goal, an important effort has been made in order to design new efficient robotic tools. Together with Edo Franzi, André Guignard and Yves Cheneval, we have developed and built hardware and software tools that make an efficient research work possible. Along with several analysis software tools, the mobile robot Khepera has been a result of this development. Using this equipment, six experiences have been carried out, covering a large spectrum of the possible ways neural networks can be used for the control of mobile robots. These experiments have nevertheless been restricted to simple behaviours and small neural networks. The first two experiments show, with a very simple and manually adjusted behaviour, the important role of the interaction of the robot with its environment. The first experiment is based on a collective behaviour, the second on a collaborative one. The adaptation of the robot to the environment is introduced in the third experiment, in which a learning technique is applied. The result is a robot able to learn how to use visual stimuli to avoid particular obstacles. Despite its interesting results, this approach has turned out to be very limited, due to the rigid structure needed. The last three experiments demonstrate the possibilities of the use of genetic algorithms, which proved to be a very flexible adaptation mechanism. The first of these three experiments tests the feasibility of this approach. The second one takes advantage of the characteristics of genetic algorithms to achieve more complex behaviours. Finally, genetic algorithms and learning techniques are associated in the last experiment, showing a high adaptive structure. An important effort has been made to show both advantages and disadvantages of each technique, in order to provide the necessary elements for the continuation of this research activity.Les applications de la robotique mobile autonome pourraient être nombreuses: le robot facteur, le robot aspirateur domestique ou nettoyeur industriel, le robot surveillant, le robot démineur et bien d'autres applications ont un marché considérable. Malgré le marché potentiel et les possibilités technologiques actuelles, seules quelques applications très limitées sont exploitées commercialement, ce qui prouve la difficulté de ces développements du point de vue intelligence. Face à l'échec des méthodes de conception classiques liées à l'intelligence artificielle, plusieurs nouvelles approches ont été proposées. Parmi celles-ci on trouve les réseaux de neurones et, plus récemment, les algorithmes génétiques, qui ont émergé de l'intérêt suscité par la vie artificielle (Artificial Life). L'application de ces techniques aux robots mobiles a été étudiée principalement en simulation, et dans la plupart des cas sans validation sur des robots réels. L'implémentation sur un robot mobile physique pose en effet souvent des problèmes pratiques qui limitent fortement leur utilisation. Ce travail a permis d'analyser plusieurs structures neuronales utilisées pour le contrôle d'un robot mobile, et ceci entièrement sur un robot physique. Pour atteindre ce but, une partie de l'effort a été consacrée à la mise au point d'outils matériels et logiciels. La collaboration avec Edo Franzi, André Guignard et Yves Cheneval a permis de développer le robot mobile Khepera et ses accessoires, ainsi que le logiciel de contrôle associé. Ces outils ont permis de réaliser six expériences qui couvrent le spectre des possibilités d'utilisation des réseaux de neurones pour le contrôle d'un robot mobile autonome. Ces expériences se limitent toutefois à la réalisation de comportements simples réalisés avec des réseaux de petite taille. Les deux premières expériences permettent, par un comportement conçu manuellement et finement ajusté, de mettre en évidence le rôle des interactions entre le robot et son environnement. La tâche exécutée est collective dans la première expérience, et coopérative dans la deuxième. Afin de permettre une adaptation du robot à son environnement, la troisième expérience introduit l'utilisation d'algorithmes d'apprentissage. Avec cette technique, un système capable d'apprendre à utiliser de la vision artificielle pour effectuer de l'évitement d'obstacles a été réalisé. Cette approche se montre toutefois très limitée à cause des structures très rigides nécessaires à son fonctionnement. Les dernières trois expériences mettent en oeuvre les algorithmes génétiques, qui se révèlent être un mécanisme d'adaptation plus flexible. La première de ces trois expériences prouve la faisabilité de l'approche, tandis que la deuxième va plus en profondeur, en essayant de mieux exploiter les caractéristiques des algorithmes génétiques. Finalement algorithmes génétiques et apprentissage sont combinés dans la dernière expérience, donnant lieu à un système de contrôle d'un degré d'adaptation élevé. L'analyse des résultats a permis de mettre en évidence avantages et inconvénients de chaque approche dans l'optique d'une utilisation concrète, et ceci afin de permettre une meilleure orientation de la recherche dans ce domaine",Conception de structures neuronales pour le contrôle de robots mobiles autonomes,,,,,core
209578960,2010-04-30,"Background/PurposeThe postweaning multisystemic wasting syndrome, caused by the porcine circovirus type 2 (PCV-2), is a major disease that poses a significant threat to the global swine industry. The purpose of this study was to establish a real-time polymerase chain reaction (PCR) method for the quantification of PCV-2 and to enable the rapid differentiation of porcine circoviruses type 1 and 2 (PCV-1 and PCV-2). Such a method would significantly speed up the process of clinical diagnosis, and could also be used to study the pathogenic mechanisms of diseases associated with PCV-2.MethodsMultiplex real-time PCR, together with LightCycler PCR data analysis software, was used for the quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2. A 263-bp DNA fragment was amplified from the 3′ end of the open reading frame-2 of PCV-2 by nested PCR, and its DNA sequence was verified as having 100% identity with a PCV-2 standard (NCBI accession number: AF055394). The 263-bp DNA fragment was cloned into the pGEM-T easy vector, and the recombinant plasmid was serially diluted and quantified using real-time PCR. A standard curve was then constructed for quantification of the PCV-2 levels in field samples. The differentiation of PCV-1 and PCV-2 was carried out by analyzing the melting temperatures of the genotype-specific PCR products.ResultsTo quantify the PCV-2 levels in field samples, a standard curve (1 × 102 −1 × 109 copies/μL) was constructed. PCV-2 concentrations as low as 1 × 102 copies/mL could be detected in specimens taken from the lymph nodes or infected tissues in samples of PCV-2-infected pigs. The diagnosis of PCV-1 and PCV-2 infections and the quantification of the viral load in the field samples could be completed within 45 minutes after extracting the viral DNA using a commercial extraction kit.ConclusionThis study demonstrate that real-time PCR is a clinically feasible method for the accurate quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2",Fast Diagnosis and Quantification for Porcine Circovirus Type 2 (PCV-2) Using Real-Time Polymerase Chain Reaction ,,Taiwan Society of Microbiology. Published by Elsevier Taiwan LLC.,10.1016/S1684-1182(10)60014-X,,core
53285515,2010-01-01T00:00:00,"The purpose of the work described in this paper is to construct and implement prediction models for optimizing container handling in particular at Cagliari\u2019s Terminal Container.
Prediction models are based on heuristic algorithms such as neural networks and classification and regression trees and evolutionary algorithms such as Genetic Algorithms (GA) and Ant Colony Optimization (ACO). These models form part of a Web Oriented Decision Support System, for real time external data acquisition (GPS information, weather information, etc.), providing operators with the information processed in real time.
The most commonly used parameter for assessing terminal performance is productivity, namely the number of containers handled in the unit of time considered.
This parameter can be associated with the terminal as a whole, or with the ship, the stevedores, each vehicle used, the single operator and related to different time intervals (year, month, week, day, hour and shift). Usually the hourly average is considered for monitoring operations and identifying shortcomings.
The rate at which operations are performed can significantly reduce turn round time and thus minimize the loss of productivity associated with the ship\u2019s time in port.
Because of the complexity of analyzing decision-making processes two sublevels are defined, that differ for type of decision and time horizon:
- The first level, generally organized around a weekly time horizon (from 10 to 1 days prior to ship\u2019s arrival in port), for scheduling operations and activities in the different areas such as, ship, quay, yard, for making decisions that satisfy the different requirements;- The second level, aimed at specific resource allocation (personnel and equipment) on the basis of the decisions made at the first planning level in order to maximise productivity and minimize costs over a time horizon of roughly 24h.
Both levels of planning are characterized by temporal fragmentation and uncertain information. The information is received at undefined times and is continually updated, resulting in uncertain content. The strong dependence of the planning process on information flow, means it is necessarily dynamic and makes it difficult to effectively optimize and integrate decisions over sufficiently broad time horizons.
The aim of the study is to construct a model for predicting containership arrivals using heuristic-based evolutionary algorithms. The so-called \u2015Inspect Inspired Algorithms\u2016 are proving effective tools for solving industrial optimization issues.
In this study the different models proposed are implemented in a \u2015Decision Support System\u2016, while data are analysed from a temporal aspect adopting a \u2015learning from data\u2016 approach. Indeed the observation of real data (actual arrival time of ships and handling in port) form the knowledge base which relies on learning from the past. All discrepancies observed between prediction models and reality, along with other factors governing that condition prediction errors, create a historical base on which models are automatically recalibrated.
This approach has the dual purpose of analysing the causes (shortcomings) of prediction errors while refining models for future prediction; an analysis of the causes and effects of recalibrating the models.
The proposed DSS can also be used for simulation purposes. In fact the algorithms will be implemented for studying the effects of external variables taken individually or interacting with one another, thus providing a useful planning tool",Development of prediction models for container traffic,,,,,core
143311543,2007-03-20T19:31:09Z,"In a broad area of industry such as remote
sensing and medical diagnosing, imaging enhancement technology takes a leading role, where energy distribution of the light source depends not only on image coordinate but also
on wavelength. Both infrared (IR) and near-infrared (NIR) imaging techniques have a variety of applications in these fields. For instance, satellite images are taken via IR or NIR spectrometer and laser Doppler medical scanning is collaborated with NIR spectrometer. Matrix functions of any
image correspond to brightness or energy at each image pixel. The actual decision making must rely on detailed investigation of images being obtained. Therefore, image processing should be taken into account so as to enhance the results from real world. Segmentation is an image analysis approach to clarify
feature ambiguity and information noise, which divides an image into separate parts that correlate with the objects or areas of the particular object involved. This procedure can be conducted by clustering, which is a process of partitioning a set
of pattern vectors into subsets. Being a simple unsupervised learning algorithm, K-means clustering algorithm has the
potential to both simplify the computation and accelerate the convergence. In most cases optimization is closely related to clustering, which gives rise to the best way of problem solving. In this article, optimal approach is proposed to be implemented along with image segmentation. This methodology is to enhance both large scale and small scale IR and NIR image processing",Optimal Approach for Enhancement of Large and Small Scale Near-Infrared and Infrared Imaging,,,,,core
298902607,2008-07-11,"Biotechnology has presented, in the last years, a rapidly growing development. New biotechnological industrial processes are constantly introduced using different microorganisms and/or enzymes. In this context, the application of process control and optimization techniques has become a need for biotechnological based industry. A technological approach was developed in this dissertation to bioreactor monitoring and control operating in fed-batch model to produce the penicillin enzyme G acilase (PGA) by means of wild cepa of the microorganism Bacillus megaterium. This enzyme is of great industrial importance, being used in the manufacture of semi-synthetic &#946;-lactamic antibiotics. This case study presents the main difficulties faced in the control of biological processes in general: the variability of kinetic parameters and the limited availability of on-line information. In order overcome these difficulties, a unconventional architecture was proposed for a dynamic and adaptive controller using filters, some of them developed in this work, and applying Computational Intelligence (CI) methodologies both in direct and hybrid form. The inference for the microbial concentration (Cx) state variable, a very relevant objective for the logic of the controller, was performed by a softsensor that had as input the filtered values of the sensors signals of the molar fractions of CO2 (yco2) and O2 (yo2) in the effluent gases, of the air feeding flow and of agitation velocity. The respiratory quotient (RQ), calculated from these data, was also used by the algorithms of the software developed here. For the Cx inference, the softsensor employed a hybrid intelligent system (HIS) composed by a neural networks ensemble (RNE) and a fuzzy rule based system (FRBS). These techniques were structured to complement each other such that the RNE infers the microbial concentration (Cx) capturing real-time process data (empirical knowledge) and the FRBS corrects this inferred value using phenomenological based knowledge. The obtained results demonstrated a more robust inference by using this architecture, even supporting some degree of extrapolation. Another important operational parameter is the definition of the initial and final instants of feeding flow of supplemental. In order to meet this goal, a logic was employed that is able to accurately predict this moments, using the CO2 (yco2) molar fraction signal, filters and adaptive fuzzy sets.Financiadora de Estudos e ProjetosA Biotecnologia tem apresentado nos últimos anos um grande e rápido desenvolvimento. Constantemente novos processos biotecnológicos industriais são introduzidos, utilizando diferentes microrganismos e/ou enzimas. Neste contexto, a aplicação de técnicas de otimização e controle de processos tornou-se uma necessidade para a indústria de base biotecnológica. Uma abordagem tecnológica foi desenvolvida nesta dissertação para monitoração e controle de um biorreator, operando em batelada alimentada ( fed-batch ), para produção da enzima penicilina G acilase (PGA) por cepa selvagem do microrganismo Bacillus megaterium Essa enzima é de grande importância industrial, sendo empregada na manufatura de antibióticos &#946;-lactâmicos semi-sintéticos. Este estudo de caso apresenta as principais dificuldades encontradas no controle de processos biológicos em geral: variabilidade dos parâmetros cinéticos e limitada disponibilidade de informação on-line. Para superar estas dificuldades foi proposta uma arquitetura não convencional para um controlador dinâmico e adaptativo utilizando filtros, alguns desenvolvidos neste trabalho, e aplicando metodologias da Inteligência Computacional (IC), tanto de forma direta como híbrida. A inferência da variável de estado concentração microbiana (Cx), objetivo muito importante para a lógica do controlador, foi realizada por um softsensor que teve como entrada valores filtrados dos sinais dos sensores das frações molares de CO2 (yco2) e O2 (yo2) nos gases efluentes, da vazão de alimentação de ar e da velocidade de agitação. O quociente respiratório (RQ), calculado a partir desses dados, foi também utilizado pelos algoritmos do software aqui desenvolvido. Para a inferência de Cx, o softsensor empregou um sistema híbrido inteligente (SHI) composto por um comitê de redes neurais artificiais (CRNAs) e por um sistema fuzzy baseado em regras (SFBR). Essas técnicas foram estruturadas de modo a se complementarem, pois o CRNAs infere a concentração microbiana (Cx) capturando dados do processo em tempo real (conhecimento empírico), e o SFBR corrige esse valor inferido utilizando conhecimento com base fenomenológica. Os resultados obtidos mostraram uma inferência mais robusta ao se utilizar esta arquitetura, suportando inclusive algum grau de extrapolação. Outro parâmetro operacional importante é a definição dos momentos de início e fim da vazão de alimentação de meio suplementar. Para alcançar esse objetivo, foi empregada lógica que foi capaz de prever estes momentos com acuidade, utilizando o sinal de fração molar de CO2 (yco2), filtros e conjuntos fuzzy adaptativos",Inteligência computacional aplicada à automação de biorreator para produção de Penicilina G Acilase (PGA),,Programa de Pós-graduação em Biotecnologia,,,core
36984773,2008-01-01T08:00:00,"Artificial intelligence (AI) refers to intelligence artificially realized through computation. AI has emerged as one of the promising computer science discipline originated in mid-1950. Over the past few decades, AI based random search algorithms, namely, genetic algorithm, ant colony optimization, and so forth have found their applicability in solving various real-world problems of complex nature. This chapter is mainly concerned with the application of some AI based random search algorithms, namely, genetic algorithm (GA), ant colony optimization (ACO), simulated annealing (SA), artificial immune system (AIS), and tabu search (TS), to solve the machine loading problem in flexible manufacturing system. Performance evaluation of the aforementioned search algorithms have been tested over standard benchmark dataset. In addition, the results obtained from them are compared with the results of some of the best heuristic procedures in the literature. The objectives of the present chapter is to make the readers fully aware about the intricate solutions existing in the machine loading problem of flexible manufacturing systems (FMS) to exemplify the generic procedure of various AI based random search algorithms. Also, the present chapter describes the step-wise implementation of search algorithms over machine loading problem",Solving machine loading problem of FMS: an artificial intelligence (AI) based random search optimization approach,,'Sociological Research Online',,,core
214600395,2011-04-07T00:00:00,"The purpose of this document is to describe the key technology issues for distributed information access in New Zealand. It is written from an industrial and public sector perspective, representing the views and findings of a wide cross-section of institutions in public and private sectors. It is an output of Objective 2 of the Distributed Information Systems project funded under contract UO0621 by the New Zealand Foundation for Research, Science and Technology (FRST).

It complements other project material produced by the academic research team at the University of Otago and its collaborators.

It focuses on requirements and applications, and is intended to provide a real-world, New Zealand-oriented context for the research in distributed information technologies (DIST).

The report represents the culmination of a series of workshops, industrial consultations, a questionnaire, and the experiences of the authors' institutions during the project, and therefore it supplements any previously produced material.UnpublishedPurvis, M., Cranefield, S., and Nowostawski, M., “A Distributed Architecture for Environmental Information Systems” in Environmental Software Systems – Environmental Information and Decision Support, Kluwer Academic, Dordrecht, The Netherlands (2000) 49-56.

Bush, G., Purvis, M., and Cranefield, S., “Experiences in the Development of an Agent Architecture”, in Design and Applications of Intelligent Agents, C. Zhang & V-W Soo (eds.), Springer-Verlag Lecture Notes in Artificial Intelligence, vol. 1881, ISBN: 3540679111, Berlin, Germany (2000) 76-87.

Huang, Z., Cranefield, S., Purvis, P. and McDonald, J. R, “A Proposal for a New Generation Hypertext Transfer Protocol on ATM Networks”, Chapter 34, World Wide Web: Technologies and Applications for the New Millennium, Computer Science Research, Education, and Applications Press, Athens, GA, U.S.A. (2000) 255-259.

Cranefield, S. and Purvis, M., “An agent-based architecture for software tool coordination”, Intelligent Agent Systems: Theoretical and Practical Issues, L. Cavedon, A. Rao, W. Wobcke (eds.), Springer-Verlag Lecture Notes in Artificial Intelligence, vol. 1209, Berlin, Germany (1997) 44-58.

Purvis, M., Zhou, Q., Cranefield, S., Ward, R., Raykov, R., and Jessberger, D., “Spatial Information Modelling and Analysis in a Distributed Environment” accepted for publication in Environmental Modelling and Software (2001).

Diaz, A., Cranefield, S., and Purvis, M., “Planning and Matchmaking in a Multi-Agent System for Software Integration”, Proceedings of the 11th International Conference on Mathematical and Computer Modelling, International Association for Mathematical and Computer Modelling (1997)

Nowostawski, M., Purvis, M. and Cranefield, S. Agent-Oriented Modelling for Complex Systems, in Applied Complexity—from neural nets to managed landscapes, S. Halloy and T. Williams (eds.), Christchurch, New Zealand Institute for Crop and Food Research (2000) 205–220

Huang, Z., Sun, C., Purvis, M., Cranefield, S., ""Weak sequential consistency models for distributed shared memory"", to appear in Proceedings of the 10th International Conference on Computing and Information (ICCI '2000), Kuwait (2000).

Cranefield, S., Purvis, M., and Nowostawski, M., “Is it an Ontology or an Abstract Syntax? Modelling Objects, Knowledge and Agent Messages”, Proceedings of the Workshop on Applications of Ontologies and Problem-Solving Methods at the 14th European Conference on Artificial Intelligence (ECAI'00), Berlin, Germany (2000) 16.1–16.4.

Huang, Z., Sun, C., Purvis, M. and Cranefield, S. “View-based Consistency for Distributed Shared Memory”, in Proceedings of the Joint Meeting of the 4th World Multiconference on Systemics, Cybernetics and Informatics (SCI'2000) and the 6th International Conference on Information Systems Analysis and Synthesis (ISAS'2000), Volume VIII, Lee, J. K., Juric, M., Bruzzone, A., Klovshy, D. and Fujita, M. (eds.) International Institute of Informatics and Cybernetics (2000) 1–6

Bush, G., Nowostawski, M., Cranefield, S. and Purvis, P. Platforms for Agent-Oriented Software Engineering, in Proceedings of the 7th Asia Pacific Software Engineering Conference (APSEC 2000), Dong J. S., He, J. and Purvis, M. (eds.), Los Alamitos, CA, USA, IEEE Computer Society Press (2000) 480–488

Cranefield, S. and Purvis, M., “Integrating Environmental Information: Incorporating Metadata in a Distributed Information Systems Architecture”, to appear in Proceedings of the Workshop Symposium on Integration in Environmental Information Systems (ISESS 2000), Environmental Informatics Institute, Gaiberg, Germany (2000).

Nowostawski, M., Purvis, M., and Cranefield, S., “An architectural approach to messaging in multi-agent systems”, Proceedings of the Autonomous Agents 2000 Workshop on Infrastructure for Scalable Multi-agent Systems (W13), Barcelona, Spain (2000) 105-110.

Cranefield, S. and Purvis, M., “Extending Agent Messaging to Enable OO Information Exchange”, Cybernetics and Systems 2000, (Proceedings of the 15th European Meeting on Cybernetics and Systems Research), R.Trappl (ed.), Vienna, Austrian Society for Cybernetic Studies, Vienna, Austria (2000) 573-578.

Purvis, M., Cranefield. S., Bush, G., Carter, D., McKinlay, B., Nowostawski, M., and Ward, R., “The NZDIS Project: an Agent-based Distributed Information Systems Architecture”, Proceedings of the Hawai`i International Conference on System Sciences (HICSS-33), R. H. Sprague, Jr. (ed.), (CD ROM) IEEE Computer Society Press, Los Alamitos, CA (2000).

Huang, Z., Cranefield, S., Chee, V. K. M., Purvis, M., “A Java Networking API for ATM Networks”, in Proceedings of the 31th International Conference and Exhibition on Technology of Object-Oriented Languages and Systems (TOOLS Asia’99), Nanjing, China, (1999) 306-315.

Cranefield, S. and Purvis, M., “UML as an ontology modelling language” Proceedings of the Workshop on Intelligent Information Integration, 16th International Joint Conference on Artificial Intelligence (IJCAI-99), (1999) 46-53.

Cranefield, S.J.S., Moreale, E., McKinlay, B. and Purvis, M. K., “Automating the Interoperation of Information Processing Tools”, Proceedings of the 32nd Hawaii International Conference on System Sciences (HICSS-32). Maui, Hawaii, IEEE (CD-ROM) (1999) 10 pages.

Cranefield, S.J.S., McKinlay, B., Moreale, E. and Purvis, M.K. “Automating Information Processing Tasks: An Agent-based Architecture”, TZI Report 9/98: Proceedings of the Workshop on Intelligent Agents in Information and Process Management, 22. Deutsche Jahrestagung für Künstliche Intelligenz (22nd German Conference on Artificial Intelligence, KI-98). Bremen, Center for Computing Technologies (TZI), University of Bremen (1998).

Purvis, M., Cranefield, S., and Ward, R., “Distributed Software Systems: From Objects to Agents”, Software Engineering: Education & Practice, Proceedings of the 1998 International Conference, IEEE Computer Society Press, Los Alamitos, CA (1998) 158-165.

Yu, Byung Hyun, Design and Implementation of a Native Java Interface for ATM Network Applications, M. Sc. Thesis, University of Otago (2000).

Nowostawski, M., Bush, G., Purvis, M. K., and Cranefield, S., ""Platforms for Agent-oriented Software"" Information Science Discussion Paper Series, Number 2000/13, ISSN1172-6024 (2000).

Cranefield, S., Purvis, M., and Nowostawski, M., “Is it an Ontology or an Abstract Syntax? Modelling Objects, Knowledge and Agent Messages”, Information Science Discussion Paper Series, Number 2000/08, ISSN1172-6024 (2000).

Cranefield, S. and Purvis, M., “Extending Agent Messaging to Enable OO Information Exchange”, Information Science Discussion Paper Series, Number 2000/07, ISSN1172-6024 (2000).

Cranefield, S. and Purvis, M., “Integrating Environmental Information: Incorporating Metadata in a Distributed Information Systems Architecture”, Information Science Discussion Paper Series, Number 2000/02, ISSN1172-6024 (2000).

Purvis, M., Cranefield, S., Nowostawski, M., Bush, G., Carter, D., McKinlay, B., Ward, R., “The NZDIS Project: an Agent-based Distributed Information Systems Architecture”, Information Science Discussion Paper Series, Number 99/17, ISSN1172-6024 (1999).

Purvis, M., Cranefield. S., and Nowostawski, M., “A Distributed Architecture for Environmental Information Systems”, Information Science Discussion Paper Series, Number 99/06, ISSN 1172-6024 (1999).

Cranefield, S. and Purvis, M., “UML as an Ontology Modelling Language”, Information Science Discussion Paper Series, Number 99/01, ISSN 1172-6024 (1999).

Cranefield, S., McKinlay, B., Moreale, E., Purvis, M., “Automating Information Processing Tasks: an Agent-based Architecture”, Information Science Discussion Paper Series, Number 98/7, ISSN 1172-6024 (1998).

Brendon Cahoon, Kathryn S. McKinley and Zhihong Lu. “Evaluating the performance of distributed architectures for information retrieval using a variety of workloads”. ACM Trans. Inf. Syst. 18, 1 (Jan. 2000), Pages 1 – 43.

S.-H Gary Chan and Fouad Tobagi. “Distributed servers architecture for networked video services”. IEEE/ACM Trans. Networking 9, 2 (Apr. 2001), Pages 125 – 136.

Conrad T. K. Chang and Bruce R. Schatz. “Performance and implications of semantic indexing in a distributed environment”. Proceedings of the eighth international conference on Information knowledge management, 1999, Pages 391 – 398.

Waiman Cheung and Cheng Hsu. “The model-assisted global query system for multiple databases in distributed enterprises”. ACM Trans. Inf. Syst. 14, 4 (Oct. 1996), Pages 421 – 470.

Miyi Chung, Ruth Wilson, Kevin Shaw and Maria A. Cobb. “Distributing Mapping Objects with the Geospatial Information Database”. Proceedings of the International Symposium on Distributed Objects and Applications, published by the IEEE, 1998.

Paul Dourish. “Using metalevel techniques in a flexible toolkit for CSCW applications”. ACM Trans. Comput.-Hum. Interact. 5, 2 (Jun. 1998), Pages 109 – 155.

Jeri Edwards. “3-Tier Client/Server at Work”. Pub. John Wiley, 1997.

Jacob Harris and Vivek Sarkar. “Lightweight object-oriented shared variables for distributed applications on the Internet”. Proceedings of the ACM conference on Object-oriented programming, systems, languages, and applications, 1998, Pages 296 – 309.

Nigel Hinds, Chinya V. Ravishankar. “Managing Metadata for Distributed Information Servers”. Proceedings of the 31st Hawaii International Conference on System Sciences (HICSS'98), published by the IEEE, 1998.

Nicholas Kassem. “Designing Enterprise Applications with the Java 2 Platform, Enterprise Edition”. Pub. Addison-Wesley, 2000.

Oscar Luiz Monteiro de Farias and Luiz Má C. P. M. de Farias. “Distributed information system on an Internet/intranet environment (DISI2E)”.Proceedings of the eighth ACM symposium on Advances in geographic information systems, 2000, Pages 197 – 198.

Julio C. Navas and Michael Wynblatt. “The network is the database: data management for highly distributed systems”. To be published in: ACM SIGMOD international conference on Management of Data on Management of data, 2001.

Okada, R.; Eun-Seok Lee; Shiratori, N. “Agent-based approach for information gathering on highly distributed and heterogeneous environment”. Proceedings of the 1996 International Conference on Parallel and Distributed Systems (ICPADS '96), published by the IEEE.

Robert Orfali and Dan Harkey. “Client/Server Programming with JAVA and CORBA”. Pub. John Wiley, 1998.

Robert Orfali, Dan Harkey and Jeri Edwards. “Instant CORBA”. Pub. John Wiley, 1997.

Robert Orfali, Dan Harkey and Jeri Edwards. “The Essential Distributed Objects Survival Guide, 2nd Edition”. Pub. John Wiley, 1996.

Owen de Kretser and Alistair Moffat. “Methodologies for Distributed Information Retrieval”. Proceedings of the The 18th International Conference on Distributed Computing Systems, published by the IEEE, 1998.

Andreas Paepcke, Chen-Chuan K. Chang, Terry Winograd and Héctor García-Molina. “Interoperability for digital libraries worldwide”. Commun. ACM 41, 4 (Apr. 1998), Pages 33 – 42.

Ahmed Saleh and George R. R. Justo. “A configuration-oriented framework for distributed multimedia applications”. Proceedings of the 2000 ACM symposium on Applied computing 2000 (volume 1), 2000, Pages 278 -280.

Wolfgang Theilmann and Kurt Rothermel. “Disseminating Mobile Agents for Distributed Information Filtering”. Proceedings of the First International Symposium on Agent Systems and Applications Third International Symposium on Mobile Agents, published by the IEEE, 1998.

Edgar Weippl. “Building secure knowledge bases: combining Java agents and Dbagents”. To be published in: Proceedings of the fifth international conference on Autonomous agents, 2001.

Jinxi Xu and W. Bruce Croft. “Cluster-based language models for distributed retrieval”. Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 1999, Pages 254 – 261.

Zhili Zhang and William Perrizo. “Distributed query processing using active networks”. Proceedings of the 2000 ACM symposium on Applied computing 2000 (volume 1), 2000, Pages 374 – 380",Distributed information access in New Zealand,,'University of Otago Library',,,core
219372139,2009-01-01T00:00:00,"The purpose of this paper is to lay the foundations of a new generation of closed loop
optimal control laws based on the plant state space model and implemented using artificial neural
networks. The basis is the long established open loop methods of Bellman and Pontryagin, which
compute optimal controls off line and apply them subsequently in real time. They are therefore open
loop methods and during the period leading up to the present century, they have been abandoned by
the mainstream control researchers due to a) the fundamental drawback of susceptibility to plant
modelling errors and external disturbances and b) the lack of success in deriving closed loop versions
in all but the simplest and often unrealistic cases. The recent energy crisis, however, has promoted the
authors to revisit
the classical optimal control methods with a view to deriving new practicable
closed loop optimal control laws that could save terawatts of electrical energy by replacement of
classical controllers throughout industry. First Bellman’s and Pontryagin’s methods are compared
regarding ease of computation. Then a new optimal state feedback controller is proposed based on the
training of artificial neural networks with the computed optimal controls",A comparison of fixed final time optimal control computational methods with a view to closed loop implementation using artificial neural networks,https://core.ac.uk/download/219372139.pdf,,,,core
13604451,2010-01-01T08:00:00,"Agent-based modeling (ABM) is a relatively new tool for use in electric power market research. At heart are software agents representing real-world stakeholders in the industry: utilities, power producers, system operators, and regulators. Agents interact in an environment modeled after the real-world market and underlying physical infrastructure of modern power systems. Robust simulation laboratories will allow interested parties to stress test regulatory changes with agents motivated and able to exploit any weaknesses, before making these changes in the real world. Eventually ABM may help develop better understandings of electric market economic dynamics, clarifying both delineations and practical implications of market power.
The research presented here builds upon work done in collateral fields of machine learning and computational economics, as well as academic and industry literature on electric power systems. We build a simplified transmission model with agents having learning capabilities, in order to explore agent performance under several plausible scenarios. The model omits significant features of modern electric power markets, but is able to demonstrate successful convergence to stable profit-maximizing equilibria of adaptive agents competing in a quantity-based, available capacity model",Electric Power Market Modeling with Multi-Agent Reinforcement Learning,https://core.ac.uk/download/13604451.pdf,ScholarWorks@UMass Amherst,,,core
12209548,2009-01-28T00:00:00,"Three dimensional graphics processing requires many complex algebraic and matrix based operations to be performed in real-time. In early stages of graphics processing, such tasks were delegated to a Central Processing Unit (CPU). Over time as more complex graphics rendering was demanded, CPU solutions became inadequate. To meet this demand, custom hardware solutions that take advantage of pipelining and massive parallelism become more preferable to CPU software based solutions. This fact has lead to the many custom hardware solutions that are available today. Since real time graphics processing requires extreme high performance, hardware solutions using Application Specific Integrated Circuits (ASICs) are the standard within the industry. While ASICs are a more than adequate solution for implementing high performance custom hardware, the design, implementation and testing of ASIC based designs are becoming cost prohibitive due to the massive up front verification effort needed as well as the cost of fixing design defects.Field Programmable Gate Arrays (FPGAs) provide an alternative to the ASIC design flow. More importantly, in recent years FPGA technology have begun to improve in performance to the point where ASIC and FPGA performance has become comparable. In addition, FPGAs address many of the issues of the ASIC design flow. The ability to reconfigure FPGAs reduces the upfront verification effort and allows design defects to be fixed easily. This thesis demonstrates that a 3-D graphics processor implementation on and FPGA is feasible by implementing both a two dimensional and three dimensional graphics processor prototype. By using a Xilinx Virtex 5 ML506 FPGA development kit a fully functional wireframe graphics rendering engine is implemented using VHDL and Xilinx's development tools. A VHDL testbench was designed to verify that the graphics engine works functionally. This is followed by synthesizing the design and real hardware and developing test applications to verify functionality and performance of the design. This thesis provides the ground work for push forward the use of FPGA technology in graphics processing applications",Real Time 3-D Graphics Processing Hardware Design using Field-Programmable Gate Arrays.,https://core.ac.uk/download/12209548.pdf,,,,core
21057500,2009-08-24,"ARCHON ™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the characteristics which typify this important domain. Secondly, the ARCHON framework is detailed- with a special emphasis being placed upon the implementation architecture. Thirdly, a brief resumee and status report of the main applications is presented. Finally, the lessons learned and the future plans are presented. 1","Mile End Road,",,,,,core
360546180,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake-holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished[1] Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons (Chichester, England) (2002). ISBN 0 47149691X

[2] Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

[3] Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 10–15. ACM Press (2006)

[4] Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-Architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

[5] Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

[6] Zhang, Z., Thangarajah, J., Padgham, L.: Automated unit testing for agent systems. In: Second International Working Conference on Evaluation of Novel Approaches to Software Engineering (ENASE), pp. 10–18 (2007)

[7] Ekinci, E.E., Tiryaki, A.M., Çetin, Ö.: Goal-oriented agent testing revisited. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 85–96 (2008)

[8] Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 133–144 (2008)

[9] Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: J.J. Gomez-Sanz, M. Luck (eds.) Ninth International Workshop on Agent-Oriented Software Engineering, pp. 165–176 (2008)

[10] Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

[11] Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

[12] Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

[13] Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

[14] d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, pp. 155–176. Springer-Verlag, LNAI 1365 (1998)

[15] Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

[16] Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

[17] Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

[18] Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), pp. 42–55. Springer Verlag, LNAI 1038 (1996)

[19] Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

[20] Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

[21] Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemel ¨ a (eds.) Proceedings of the 23rd International Conference on Logic Programming (ICLP), pp. 302–316. Springer, LNCS 4670 (2007)

[22] Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http://www.math.upenn.edu/∼wilf/gfology2.pdf

[23] Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

[24] Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), industry track., pp. 37–44. IFAAMAS (2008)

[25] Parunak, H.V.D.: “go to the ant”: Engineering principles from natural multi-agent systems. Annals of Operations Research 75, 69–101 (1997). (Special Issue on Artificial Intelligence and Management Science)

[26] van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

[27] Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: The Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

[28] Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: International Conference on Software Engineering: Future of Software Engineering, pp. 120–136 (2007)

[29] Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

[30] Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

[31] Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007",On the testability of BDI agent systems,,'University of Otago Library',,,core
195900126,2011-09-05T00:00:00,"Software development processes are now essential for an organization to obtain acceptable
levels of productivity and quality. The integration of agile and traditional development
processes is an open and few explored research area, which has attracted the interest of
industrial and academic communities in order to take advantage of the strengths of both
approaches. However, little is known about the real benefits of existing proposals, as
studies are still preliminary and evidence is very sparse. This research aims to investigate
the best options for agile and traditional integration by defining a hybrid process that takes
advantage of both approaches. A proposal to integrate the practices of Scrum agile method
within a development process based on RUP   Rational Unified Process   was made based
on some indications and results in the literature. An empirical study aiming to evaluate the
productivity impact of that hybrid Scrum-RUP proposal was also carried out. Five groups
of similar projects from a CMMI-ML2 medium-sized company were compared with
respect to productivity, some of which were developed using the new Scrum-RUP process
and others were developed using the other RUP-based process the company was used to
employ. Also interviews were held with developers who participated in the projects to
identify the causes of productivity results. Quantitative results have shown that four out of
five project groups showed significant productivity increase in Scrum-RUP projects. The
results of the interviews have shown that the main causes of productivity increase were
related to process, of which the most frequent were communication, collaboration and
reduction of documentation. The study shows that it is possible to integrate Scrum
practices in the software development process without losing the rigor needed in the
desired subprocesses and still get real development productivity gain.Doutor em CiênciasProcessos de desenvolvimento de software são atualmente imprescindíveis para uma
organização obter níveis aceitáveis de produtividade e qualidade. A integração de
processos de desenvolvimento de software ágeis e tradicionais é uma área de pesquisa
aberta e pouco explorada que tem atraído o interesse das comunidades acadêmica e
industrial com o intuito de se aproveitar os pontos fortes das duas abordagens. Entretanto,
pouco ainda se sabe sobre os reais benefícios das propostas existentes, pois os estudos
ainda são preliminares e as evidências muito esparsas. Esta pesquisa tem o objetivo de
investigar as melhores opções de integração ágil e tradicional, definindo um processo
híbrido que aproveite os pontos fortes de ambas as abordagens. Foi elaborada uma
proposta de integração de práticas do método ágil Scrum dentro de um processo de
desenvolvimento baseado no processo RUP   Rational Unified Process com base em
algumas indicações e resultados encontrados na literatura. Também foi realizado um
estudo de caso comparativo multi-projeto com o intuito de avaliar o impacto em
produtividade com a adoção desta proposta híbrida Scrum-RUP. Foram comparadas as
produtividades de cinco grupos de projetos similares desenvolvidos em uma empresa
CMMI-ML2 de porte médio, dentre os quais alguns usaram o novo processo Scrum-RUP e
outros usaram um processo baseado em uma customização RUP que a empresa já utilizava.
Também foram realizadas entrevistas com desenvolvedores que participaram dos projetos
no intuito de investigar as possíveis causas dos resultados de produtividade. Os resultados
quantitativos mostraram que, dos cinco grupos comparados, quatro apresentaram aumento
significativo na produtividade dos projetos Scrum-RUP. Os resultados das entrevistas
mostraram que as principais causas de aumento de produtividade estavam relacionadas ao
processo Scrum-RUP, sendo comunicação, colaboração e diminuição da documentação as
mais frequentes. O estudo mostra que é possível inserir práticas Scrum no processo de
desenvolvimento de software sem eliminar o rigor nos subprocessos necessários e, mesmo
assim, obter ganhos reais de produtividade no desenvolvimento",Integração de princípios de desenvolvimento ágil de software ao RUP - um estudo empírico,https://core.ac.uk/download/195900126.pdf,'EDUFU - Editora da Universidade Federal de Uberlandia',,,core
300010399,2009-01-01T00:00:00,"Polymerization processes are important industrial processes where the polymer product can be made into many household and laboratory products. One limitation to this process is that there is no online indication of an important polymer quality, melt flow rate (MFR), as at least 2 hours of laboratory analysis is required to determine the polymer quality. To enable real-time monitoring of MFR, various works have been done to conduct prediction of the polymer quality. One approach is modeling based on fundamental principles governing the process which will require good knowledge of the polymerization process and much effort and time due to the complexity of the process. The other approach is to develop an artificial intelligence model using data mining tools such as artificial neural network, which has an advantage over the mechanistic approach due to its excellent ability to model nonlinear relationships. In this project, a software sensor using artificial neural network built-in in MATLAB is developed to predict the MFR of an industrial polymerization plant. A suitable network function, newrb, is recommended in this report. Historical data is found to have an influence on the model and update of model is necessary when training input gets outdated. The normalization method using the mean and standard deviation of the process variables can be further improvised by other univariate and multivariate analysis in order to removing outliers, reduced and classified to gain better prediction results.Bachelor of Engineering (Chemical and Biomolecular Engineering",Development of soft-sensors for polymerization processes,,,,,core
4733858,2009-10-06T00:00:00,"Receiver design, especially equalizer design, in communications is a major concern in both academia and industry.  It is a problem with both theoretical challenges and severe implementation hurdles. While  much research has been focused on reducing complexity for optimal or near-optimal schemes, it is still common practice in industry to use simple techniques (such as linear equalization) that are generally significantly inferior. Although digital signal processing (DSP) technologies have been applied to wireless communications to enhance the throughput, the users' demands for more data and higher rate have revealed new
challenges. For example, to collect the diversity and combat fading channels, in addition to the transmitter designs that enable the diversity, we also require the receiver to be able to collect the prepared diversity.


Most wireless transmissions can be modeled as a linear block transmission system. Given a linear block transmission model assumption, maximum likelihood equalizers (MLEs) or near-ML decoders have been adopted at the receiver to collect diversity which is an important metric for performance, but these decoders exhibit high complexity. To reduce the decoding complexity, low-complexity equalizers, such as linear equalizers (LEs) and
decision feedback equalizers (DFEs) are often adopted. These methods, however, may not utilize the diversity enabled by the transmitter and as a result have degraded performance compared to
MLEs.


In this dissertation, we will present efficient receiver designs that achieve low bit-error-rate (BER), high mutual information, and low decoding complexity. Our approach is
to first investigate the error performance and mutual information of existing low-complexity equalizers to reveal the fundamental condition to achieve full diversity with LEs. We show that the fundamental condition for LEs to collect the same (outage) diversity as MLE is that the channels need to be constrained within a certain distance from orthogonality. The orthogonality deficiency (od) is adopted to quantify the distance of channels to orthogonality while other existing metrics are also introduced and compared. To meet the fundamental condition and achieve full diversity, a hybrid equalizer framework is proposed. The performance-complexity trade-off of hybrid equalizers is quantified by deriving the distribution of od.

Another approach is to apply lattice reduction (LR) techniques to improve the ``quality' of channel matrices. We present two widely adopted LR methods in wireless communications, the Lenstra-Lenstra-Lovasz (LLL) algorithm [51] and Seysen's algorithm (SA), by providing detailed descriptions and pseudo codes. The properties of output matrices of the LLL algorithm and SA are also quantified. Furthermore, other LR algorithms are also briefly introduced.

After introducing LR algorithms, we show how to adopt them into the wireless communication decoding process by presenting LR-aided hard-output detectors and LR-aided soft-output detectors for coded systems, respectively. We also analyze the performance of proposed efficient receivers from the perspective of diversity, mutual information, and complexity. We prove that LR techniques help to restore the diversity of low-complexity equalizers without increasing the complexity significantly.

When it comes to practical systems and simulation tool, e.g.,  MATLAB, only finite bits are adopted to represent numbers. Therefore, we revisit the diversity analysis for finite-bit represented systems. We illustrate that the diversity of MLE for systems with finite-bit representation is determined by the number of non-vanishing eigenvalues. It is also shown that although theoretically LR-aided detectors collect the same diversity as MLE in the real/complex field, it may show different diversity orders when finite-bit representation exists. Finally, the VLSI implementation of the complex LLL algorithms is provided to verify the practicality of our proposed designs.Ph.D.Committee Chair: Ma, Xiaoli; Committee Member: Anderson, David; Committee Member: Barry, John; Committee Member: Chen, Xu-Yan; Committee Member: Kornegay, Kevi",Wireless receiver designs: from information theory to VLSI implementation,https://core.ac.uk/download/4733858.pdf,Georgia Institute of Technology,,,core
324112753,2008-04-28,"A qualidade microbiológica do leite cru tem recebido grande atenção em função da predominância de altas contagens de aeróbios mesófilos e de coliformes, que são indicativos de contaminação durante o processamento e armazenamento. Os coliformes também podem produzir enzimas que comprometem a qualidade dos derivados a que o leite cru é destinado. Os objetivos desse trabalho foram estudar a possibilidade de ocorrência de Escherichia coli O157 na região produtora de leite que abastece a cidade de Viçosa, isolar espécies de coliformes termotolerantes e de E. coli O157 de propriedades rurais de criação de gado leiteiro, avaliar a adesão de E. coli O157:H7 ATCC 43895 pela contagem padrão em placas (CPP) em diferentes superfícies adotadas em unidades de processamento de alimentos, avaliar e modelar matematicamente a formação de biofilmes de E. coli O157:H7 (ATCC 43895) nas diferentes superfícies por meio da implementação de um desenho composto central rotacional para os fatores temperatura de exposição e tempo de incubação e avaliar a resistência de E. coli O157:H7 (ATCC 43895) aos sanitizantes mais amplamente usados na desinfecção de equipamentos e utensílios de indústrias de processamento de alimentos, bem como de restaurantes e cozinhas industriais. Foi verificado uma contagem de coliformes termotolerantes para amostras de leite cru entre  11,0 NMP.mL-1 de coliformes termotolerantes foi obtida para a água. Isso indicou um perigo potencial para introdução de patógenos no leite por meio da contaminação de tetas, utensílios e equipamentos. Foram isoladas quinze estirpes de E. coli, porém, essas ertipes não foram confirmadas como pertencentes ao sorogrupo O157. A incidência de coliformes e de E. coli no leite cru tem recebido considerável atenção, em função de sua associação à contaminação de origem fecal e o conseqüente risco de que outros patógenos de origem fecal estejam presentes. Na avaliação da adesão de E. coli O157:H7 ATCC 43895 pela contagem padrão em placas (CPP) em diferentes superfícies adotadas em unidades de processamento de alimentos foi observada um aumento da adesão em função do tempo de contato para todas as superfícies avaliadas. Uma menor adesão ocorreu no aço inoxidável e uma maior adesão no PVCRF, após 10 horas de contato. Observou-se pelo teste de Scott-Knott que não houve diferença significativa na adesão do microrganismo (p > 0,05) apenas no aço inoxidável para os tempos 2 horas e 4 horas de contato. Nos tempos de contato igual e superior a 6 horas houve diferença significativa na intensidade de adesão em todas as superfícies avaliadas. Avaliando-se cada superfície ao longo dos diferentes tempos de contato, verificou-se diferença significativa (p  11,0 NMP.mL-1 of thermotolerant coliforms was achieved for water. That indicates a potential danger of pathogens introduction in the milk through udder, appliances and equipments contamination. Fifteen strains of E. coli were isolated; however, such strains did not show to belong to the serotyping O157. The incidence of coliforms and E. coli in raw milk has been given considerate attention due to its association to fecal contamination and the following risk of other fecal pathogen presence. In the evaluation of E. coli O157:H7 ATCC 43895 adhesion by means of the standard plate count (SPC) on several surfaces used in food processing units, it was observed an increase in the adhesion in function of the contact time to all evaluated surfaces. Lower adhesion occurred to the stainless steel and higher adhesion to PVCRF after 10-hour contact. According to the Scott-Knott test, there was no meaningful difference in the microorganism adhesion (p > 0,05) only for the stainless steel at 2 and 4 hour contact. There was meaningful differences on the adhesion intensity to all the evaluated surfaces for contact periods equals or higher then 6 hours. Meaningful differences (p < 0.05) to the microorganism adhesion when each surface was evaluated through different contact periods were shown. The stainless steel surface when analyzed under the microscope revealed microtopographic characteristics very different from the other surfaces, what may justify the differences among the adhesion rates found in this experiment. It was shown by means of the central composite rotational design implementation that the achieved models for the three surfaces were validated, once the predictions for all results were higher than the values found in the experiment, that is, predictions tended to the safer side where the predicted adhesion rates were higher to the real ones. The results achieved for E. coli O157:H7 ATCC 43895 resistance to the most used sanitizing products showed a variation on the efficiency based on decimal reductions in the antimicrobial effect performed by the different sanitizing products that were evaluated. Following the AOAC test, 30 seconds is adopted as the contact period between the microorganism and the sanitizing product being tested. According to these criteria, the sanitizing products sodium hypochlorite at 100 and 200 mg.L-1, sodium dichloroisocyanurate at 200 mg.L-1, 2 % hydrogen peroxide, peracetic acid at 30 mg.L-1, grapefruit seed extract at 1:1500 and 0.20 % quaternary ammonia answered the specifications. The results showed the importance of proper hygiene practices in the food industry. Thus, it is suggested to evaluate such hygiene procedures and the use of sanitizing agents which are the most adequate to the surfaces used in the food processing industries.Coordenação de Aperfeiçoamento de Pessoal de Nível Superio","Escherichia coli O157:H7: Occurrence in the area where milk that supplies Viçosa is produced, adhesion on several surfaces and resistance to the sanitizing products",,UFV,,,core
147721850,2011,"Objetivou-se com este trabalho avaliar a precisão das redes neurais artificiais (RNA) na estimativa das redes neurais artificiais (RNA) na predição de índices zootécnicos, com base em variáveis térmicas e fisiológicas de porcas gestantes. A pesquisa foi realizada entre janeiro e abril de 2005 em uma propriedade de produção industrial de suínos, no setor de gestação, com 27 matrizes primíparas, alojadas em baias individuais e posteriormente na maternidade em baias de parição, onde foram quantificados os índices de produção dos leitões provenientes do estudo. Para tanto, foi implementada uma RNA backpropagation, com uma camada de entrada, uma oculta e uma camada de saída com funções de transferência tangente sigmoidal. A temperatura do ar e a frequência respiratória foram consideradas variáveis de entrada e o peso ao nascimento dos leitões e número de leitões mumificados, como variáveis de saída. A rede treinada apresentou ótimo poder de generalização, o que possibilitou a predição das variáveis-respostas. A caracterização do ambiente da gestação e maternidade foi adequada se comparada aos dados reais, com poucas tendências de sub ou superestimação de alguns valores. A utilização desse sistema especialista para a previsão dos índices zootécnicos é viável, pois o sistema tem bom desempenho para esta aplicação.The objective of this work was to evaluate the precision of Artificial Neural Networks (ANNs) to estimate zootechnical indexes, based on thermal and physiological variables of pregnant sows. This study was carried out from January to April 2005, in a swine industrial production farm in the gestation section with 27 primiparous gilts, allocated in individual pens and after on farrowing pens where it was quantified animal production indexes of piglets from the study. Therefore, an ANN backpropagation was implemented, with one input layer, one hidden layer, and one output layer with tangent sigmoidal transference functions. Air temperature and respiratory frequency were considered as input variables and weight of piglet at birth and the number of mummified piglets as output variables. The trained ANN presented a great generalization power, which enabled the prediction of the answer-variables. Characterization of the environment of gestation and maternity was appropriated if compared to the real data, with few under or overestimated tendencies of some values. The use of this specialist system to predict zootechnical indexes is viable because the system shows a good performance for this use",Use of artificial neural networks on the prediction of zootechnical indexes on gestation and farrowing stages of swines,,Sociedade Brasileira de Zootecnia,10.1590/S1516-35982011000300028,"[{'title': None, 'identifiers': ['1516-3598', 'issn:1516-3598']}]",core
103198820,2011,"Abstract: This paper proposes an intelligent evolutionary algorithm that can be applied in the design of optimal automation systems, and employs a multimodal six-bar mechanism optimization design, job shop production scheduling for the fishing equipment industry, and dynamic real-time production scheduling system design cases to show how the technique developed in this paper is highly effective at resolving optimal automation system design problems. Major breakthroughs in artificial intelligence continue to be made in the wake of advanced information technology developments, and the field of intelligent evolutionary algorithms has attracted a particularly large amount of attention from researchers and users in the artificial intelligence community. The successful optimization of automation system design requires interdisciplinary integration, and further requires the use of actual cases, verification, and improvement to ensure implementation in real-world applications",SPOTLIGHT: Intelligent Automation           Guest edited by Min-Hsiung Hung Applications of Intelligent Evolutionary Algorithms in Optimal Automation System Design,,,,,core
13631690,2011-01-01T08:00:00,"This thesis develops robotic skills for manipulating novel articulated objects. The degrees of freedom of an articulated object describe the relationship among its rigid bodies, and are often relevant to the object\u27s intended function. Examples of everyday articulated objects include scissors, pliers, doors, door handles, books, and drawers. Autonomous manipulation of articulated objects is therefore a prerequisite for many robotic applications in our everyday environments. Already today, robots perform complex manipulation tasks, with impressive accuracy and speed, in controlled environments such as factory floors. An important characteristic of these environments is that they can be engineered to reduce or even eliminate perception. In contrast, in unstructured environments such as our homes and offices, perception is typically much more challenging. Indeed, manipulation in these unstructured environments remains largely unsolved. We therefore assume that to enable autonomous manipulation of objects in our everyday environments, robots must be able to acquire information about these objects, making as few assumption about the environment as possible. Acquiring information about the world from sensor data is a challenging problem. Because there is so much information that could be measured about the environment, considering all of it is impractical given current computational speeds. Instead, we propose to leverage our understanding of the task, in order to determine the relevant information. In our case, this information consists of the object\u27s shape and kinematic structure. Perceiving this task-specific information is still challenging. This is because in order to understand the object\u27s degrees of freedom, we must observe relative motion between its rigid bodies. And, as relative motion is not guaranteed to occur, this information may not be included in the sensor stream. The main contribution of this thesis is the design and implementation of a robotic system capable of perceiving and manipulating articulated objects. This system relies on Interactive Perception, an approach which exploits the synergies that arise when crossing the boundary between action and perception. In interactive perception, the emphasis of perception shifts from object appearance to object function. To enable the perception and manipulation of articulated objects, this thesis develops algorithms for perceiving the kinematic structure and shape of objects. The resulting perceptual capabilities are used within a relational reinforcement learning framework, enabling a robot to obtain general domain knowledge for manipulation. This composition enables our robot to reliably and efficiently manipulate novel articulated objects. To verify the effectiveness of the proposed robotic system, simulated and real-world experiments were conducted with a variety of everyday objects",Interactive perception of articulated objects for autonomous manipulation,,ScholarWorks@UMass Amherst,,,core
145023082,2008-03-01T08:00:00Z,"in the field of human activity recognition has existed for quite sometime, but has gained popularity in recent years for use in many areas of application. In the security industry, suspicious activities could be detected in high-profile areas. In the medical industry, systems could be trained to detect patterns of motion indicating distress or to detect a lack of motion if a person had fallen and was unable to move. However, algorithms with reliable accuracy are difficult to implement in a real-time environment due to computational complexity. This thesis developed a new way of extracting and using data from a human figure in a video frame to determine what type of activity the subject is performing. Following background subtraction, a thinning algorithm operating on the silhouette offered a more robust limb extraction method, while a six-segment representation of the human figure offered more accuracy in deriving limb parameters, or components, such as distance from torso, and angle of displacement from the vertical axis. Neural networks or nearest neighbor classifiers used the limb components to identify a number of activities, such as walking, running, waving and jumping. This entire human activity recognition system was tested with both a MATLAB implementation (non real-time) and a C++ implementation in OpenCV (real-time). The algorithm achieved 96% classification accuracy in video feeds, which is only slightly lower than that of intensive, non real-time systems",Human activity recognition using limb component extraction,,RIT Scholar Works,,,core
146931545,2010-04,"Software used by architectural and industrial designers – has moved from becoming a tool for drafting, towards use in verification, simulation, project management and project sharing remotely. In more advanced models, parameters for\ud
the designed object can be adjusted so a family of variations can be produced rapidly. With advances in computer aided design technology, numerous design\ud
options can now be generated and analyzed in real time. However the use of digital tools to support design as an activity is still at an early stage and has largely been\ud
limited in functionality with regard to the design process. To date, major CAD vendors have not developed an integrated tool that is able to both leverage specialized design knowledge from various discipline domains (known as expert\ud
knowledge systems) and support the creation of design alternatives that satisfy different forms of constraints.\ud
We propose that evolutionary computing and machine learning be linked with parametric design techniques to record and respond to a designer’s own way of working and design history. It is expected that this will lead to results that impact on future work on design support systems-(ergonomics and interface) as well as implicit constraint and problem definition for problems that are difficult to quantify","Patterns, heuristics for architectural design support: making use of evolutionary modelling in design",,Association for Research in Computer-Aided Architectural Research in Asia (CAADRIA,,,core
380229,2011-01-01T00:00:00,"The recently described human pathogen M. immunogenum (Wilson et al., 2001; Loots et al., 2005; Sampaio et al., 2006) has been implicated in the causation of hypersensitivity pneumonitis (HP) in automobile workers and other occupational workers exposed to metal working fluids (MWF; Moore et al., 2000; Falkinham, 2003; Beckett et al., 2005). The bacterium has as closest relatives M. chelonae, M. abscessus and collectively these bacteria form the M. chelonae complex. The early detection of M. immunogenum and other bacteria implicated in the causation of HP may be crucial to reducing its prevalence. As detection by conventional techniques such as cultured is severely limited, an M. immunogenum-specific quantitative real-time PCR system was developed (Rhodes et al., 2008).  This technique made use of a 5’ nuclease assay (Taqman) to detect fluorescence in real time upon specific amplification of target DNA. When initially validated laboratory strains and on industrial metal working fluids from the UK and USA from which no M. immunogenum CFU were recovered, the assay detected between 3.4 x101 and 1.9 x 104 cell equivalents (CE) per ml, and increased the detection rate over culture to 37.5% (12 of 32 samples; Rhodes et al., 2008). The initial description of the assay gave an indication as to its potential for routine assessment of coolant samples for contamination with this important organism. However, the true value of the technique will become apparent only after more in depth study and application to industrial systems.

The present report describes further testing of the assay and direct comparison against culture, with its implementation on a larger scale on a range of different contaminated coolant samples obtained from the USA and Europe over a 19 month period (from December 2006 to July 2008)",Implementation of a Quantitative Real-Time PCR Assay for the Detection of Mycobacterium immunogenum in Metalworking Fluids,,'Informa UK Limited',10.1080/15459624.2011.590737,,core
13621323,2011-09-01T07:00:00,"This thesis develops robotic skills for manipulating novel articulated objects. The degrees of freedom of an articulated object describe the relationship among its rigid bodies, and are often relevant to the object\u27s intended function. Examples of everyday articulated objects include scissors, pliers, doors, door handles, books, and drawers. Autonomous manipulation of articulated objects is therefore a prerequisite for many robotic applications in our everyday environments. Already today, robots perform complex manipulation tasks, with impressive accuracy and speed, in controlled environments such as factory floors. An important characteristic of these environments is that they can be engineered to reduce or even eliminate perception. In contrast, in unstructured environments such as our homes and offices, perception is typically much more challenging. Indeed, manipulation in these unstructured environments remains largely unsolved. We therefore assume that to enable autonomous manipulation of objects in our everyday environments, robots must be able to acquire information about these objects, making as few assumption about the environment as possible. Acquiring information about the world from sensor data is a challenging problem. Because there is so much information that could be measured about the environment, considering all of it is impractical given current computational speeds. Instead, we propose to leverage our understanding of the task, in order to determine the relevant information. In our case, this information consists of the object\u27s shape and kinematic structure. Perceiving this task-specific information is still challenging. This is because in order to understand the object\u27s degrees of freedom, we must observe relative motion between its rigid bodies. And, as relative motion is not guaranteed to occur, this information may not be included in the sensor stream. The main contribution of this thesis is the design and implementation of a robotic system capable of perceiving and manipulating articulated objects. This system relies on Interactive Perception, an approach which exploits the synergies that arise when crossing the boundary between action and perception. In interactive perception, the emphasis of perception shifts from object appearance to object function. To enable the perception and manipulation of articulated objects, this thesis develops algorithms for perceiving the kinematic structure and shape of objects. The resulting perceptual capabilities are used within a relational reinforcement learning framework, enabling a robot to obtain general domain knowledge for manipulation. This composition enables our robot to reliably and efficiently manipulate novel articulated objects. To verify the effectiveness of the proposed robotic system, simulated and real-world experiments were conducted with a variety of everyday objects",Interactive Perception of Articulated Objects for Autonomous Manipulation,https://core.ac.uk/download/13621323.pdf,ScholarWorks@UMass Amherst,,,core
343402878,2010-08-01T00:00:00,"Service-oriented computing is the new wave emerging from maturing Web services and the adoption of elements from Semantic Web technology. More sophistication, in response to business requirements, does of course not make it easier to use or to control. In particular, business processes demand resilience and real-time adaptation in the face of changing business requirements, incorporation of alternative services and finding suitable substitutes when those needed are unavailable. The European Union-funded Alive project is prototyping ideas, driven by commercial and industrial use cases, that utilize research in organizational modeling, software agents, model-driven engineering, artificial intelligence, the Semantic Web, and Web services to construct tools and demonstrators to address these needs. This article outlines the Alive architecture for service-oriented computing, describes some of the innovative tools we have developed and illustrates it all with a detailed run-through from one of our use cases","Adaptable, Organization-Aware, Service-Oriented Computing",,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/MIS.2010.93,,core
210300293,2011-12-31,"AbstractThe non-uniformity of Infrared Focal Plane Array (IRFPA) resulted from the limits of the detector's materials and the manufacturing process affects the performance of the staring IR imaging system. To address this problem, non-uniformity correction (NUC), applied for real-time resolution, is the important issue in the IR imaging information processing system. This thesis introduces method of non-uniformity correction. Considering the nonlinear character of IRFPA, the calibration-based polynomial NUC method is proposed in the hardware system. Comparing with the conventional NUC schemes, polynomial method can achieve better NUC performance and implement in real-time. The algorithm is designed based on System architecture for FPGA hardware, for which is the Xilinx ML402 platform dedicated for video processing, which consists of A/D and D/A converter, and Virtex-4 FPGA on the mother board. The polynomial method reduces the non-uniformity in the infrared image largely, implemented at real-time, as well as the advantage of wide dynamic range",Calibration-based NUC Method in Real-time Based on IRFPA ,,Published by Elsevier B.V.,10.1016/j.phpro.2011.11.058,,core
211455324,2008-01-01T00:00:00,"In industrial practice, rough machining of sculptured surface parts is planned on a CAM system; users choose the roughing strategy, the milling parameters and their values. According to these choices, the CAM system creates toolpaths, performs machining simulations, and extracts statistics of the created toolpaths, such as machining time, toolpath length, etc., as well as the machined product model. Thus, rough machining quality of a sculptured surface part depends on the user's choices, which are based on his/her personal experience of the machining process and his/her familiarity with the specific CAM system. The main objective of the current thesis is formulation and development of a methodology, which provides the optimal cutting conditions for sculptured surface part roughing, regardless of the user's experience, and actually improving on the current industrial practice. Application of such an optimisation methodology requires suitable modeling of the studied problem, as well as determination of quality goals regarding the machining process result. Development of the methodology is demonstrated on a typical and widely employed commercially available CAM system (PowerMill). Under the scope of full exploitation of the CAM system's routines and operations, software was developed in Visual Basic for Applications to automate argument passing to the CAM system and essentially drive it externally, so that it can interact with other software. A first approach into rough machining optimisation was formulated as follows: a typical Genetic Algorithm (GA) exploits the output of Artificial Neural Networks (ANN), which predict the rough machining Quality Characteristics, thus constituting the Objective Function of the GA, after appropriate weighing. The first step in this approach is to determine the statistically significant rough machining parameters, based on Design of Experiments (DoE) as introduced by Taguchi. DoE is followed by ANalysis Of VAriance (ANOVA), which points out the statistically influential process parameters according to the proposed Quality Characteristics (Machining Time and Total Remaining Volume at the end of rough machining). ANNs were trained by the data set - input and output values - gathered through the implementation of DoE; thus, the ANNs were used as surrogate models (meta-models) of the machining process, predicting Quality Characteristics' values. The proposed method was implemented for a certain sculptured surface part. It was proven that, despite ANN fast response, ANNs are not precise enough, when trained by a limited size data set, as the one resulting from DoE implementation, however appealing this notion originally appeared. Besides, ANN model performance depends strongly on the geometry of the part under study. In a second approach, a two-objective (Machining Time and Total Remaining Volume) rough machining optimisation problem was formulated, bypassing the use ANN prediction models. Optimisation was realized on evolutionary optimisation software (EASY). Three two-objective optimisation methods were applied - GA, GA with Inexact Pre-evaluation (IPE) phase and GA with a specially shaped Two Player Nash Game, in order to determine optimal rough machining parameter values. These methods were compared to each other as far as computational cost was concerned. Although the Nash Game was proven as fast computationally as the GA with IPE phase, it still presents a single optimal solution, by contrast to GAs offering a set of optimal solutions, and, in addition the optimal solution value depends on the variable distribution among the players of the Game. The notion of the third approach stems from the fact that the two objectives mentioned above lead to optimal solutions that ignore the local shape of the remaining volume of the roughed part. Thus, a new - original - optimisation objective is formulated as the Standard Deviation of Difference in ""Shape"" between roughed and designed part. At first, a calculation method of the Standard Deviation was developed utilizing projection routines available in the CAD software; projections were determined from the roughed part onto the designed part. Levene hypothesis test was used to prove that a small portion (3% in cases studied) of the triangles, which describe the roughed part, is sufficient to reliably calculate the Standard Deviation of Difference in ""Shape"". This projection method, however, is computationally very expensive owing to the repetitive usage of ""expensive"" CAD functions. A much faster method was developed instead, which takes advantage of a ""cloud"" of points taken from the designed part for the calculation of the difference in ""shape"". A three-objective (Machining Time, Total Remaining Volume and Standard Deviation of Difference in ""Shape"" between roughed and designed part) optimisation of rough machining parameters was ultimately developed conjugating two evolutionary algorithms: a commercially available GA (EASY) and a Micro-Genetic Algorithm (μ-GA) that was developed from scratch. Conjugation of the two algorithms was realized in such a way that no approximations were needed in modeling the rough machining problem, thus avoiding partially optimal solutions. The μ-GA was used to determine optimal Z height distribution of the rough machining strategy slices. The μ-GA performs single-objective optimisation and its Objective Function is the weighted geometric mean of the values of the objectives mentioned previously. The GA performs three-objective optimization, while it optimizes the values of the rough machining parameters, as they are defined in the used CAM system. The design variables of the GA are: Cutting Tool, Stepover, Thickness, Number of Stepdowns, Profiling switch, Raster Angle, Allowance switch, Infinite Range switch, Feed per Tooth of the Cutting Tool and Spindle Speed of the CNC machine tool. Conjugation of GA and μ-GA enables the method to cope with problems related to modeling parameters that vary in number and value at the same time, such as the Number of Stepdowns parameter of the proposed rough machining strategy. Thus, the value of Number of Stepdowns parameter determines the size of μ-GA population chromosome. The GA and the μGA exchange data concerning parameter and objective values via files. The optimization method of the conjugated GA and μ-GA was implemented for three sculptured surface parts: a simplified hip prosthesis, similar to the ones used in the Orthopedic Surgery, an industrial compressor fin and a turbo-machine fin. Different design techniques were used for each part and their real-world counterparts are functionally different. The resulting Pareto fronts consist of sets of optimal solutions, out of which solutions corresponding to different objective values may be selected, depending on the machining goals of the user or the machinist. The main contribution of the thesis in the field of planning rough machining of sculptured surface parts is that it proposes evolutionary methods as practically viable, it copes with a large number of optimization parameters, it proposes multi-objective optimization and alternative regions of optimal solutions and, last, but not least, it builds directly on the same practical tool as the one used in industrial practice, i.e. on CAM systems.Ο σχεδιασμός της κατεργασίας εκχόνδρισης τεμαχίων με γλυπτές επιφάνειες στην βιομηχανία γίνεται με τη χρήση λογισμικού CAM, στο περιβάλλον του οποίου επιλέγονται από το χρήστη η στρατηγική εκχόνδρισης, οι παράμετροι της και οι τιμές τους. Βάσει αυτών των επιλογών, το σύστημα στη συνέχεια δημιουργεί αυτόματα τη διαδρομή που θα ακολουθήσει το εργαλείο, πραγματοποιεί προσομοίωση της κατεργασίας, εξάγει στατιστικά στοιχεία της παραγόμενης διαδρομής εργαλείου (χρόνος κατεργασίας, μήκος διαδρομής, κτλ.) και το τεμάχιο-προϊόν της εκχόνδρισης. Στην πραγματικότητα, η ποιότητα της εκχόνδρισης εξαρτάται πάρα πολύ από τις επιλογές των παραμέτρων, που θα κάνει ο χρήστης του συστήματος CAM, επαφιέμενος στην εμπειρία του και στην εξοικείωση του με το συγκεκριμένο σύστημα CAM. Η παρούσα διδακτορική διατριβή αποσκοπεί στην ανάπτυξη και την υλοποίηση μιας μεθοδολογίας προσδιορισμού βέλτιστων συνθηκών εκχόνδρισης τεμαχίων, τα οποία αποτελούνται από γλυπτές επιφάνειες. Απώτερη επιδίωξη είναι η ανεξαρτητοποίηση από την εμπειρία του χρήστη, αλλά και η βελτίωση της σημερινής βιομηχανικής πρακτικής. Η εφαρμογή μιας τέτοιας μεθοδολογίας βελτιστοποίησης απαιτεί κατάλληλη μοντελοποίηση του υπό μελέτη προβλήματος, καθώς και τον καθορισμό στόχων (κριτηρίων) ποιότητας του αποτελέσματος. Για την υλοποίηση της μεθοδολογίας υιοθετείται ένα τυπικό και πολύ διαδεδομένο εμπορικά διαθέσιμο σύστημα CAM (PowerMill). Για τη διευκόλυνση της χρήσης του λογισμικού CAM, στο πλαίσιο της διατριβής αναπτύχθηκε κώδικας σε γλώσσα προγραμματισμού Visual Basic for Applications, ο οποίος αυτοματοποιημένα παρέχει τιμές για τις παραμέτρους κατεργασίας και πρακτικά οδηγεί το σύστημα εξωτερικά, δίνοντας, έτσι, τη δυνατότητα διασύνδεσής του με άλλα προγράμματα λογισμικού. Μια πρώτη προσέγγιση της βελτιστοποίησης της εκχόνδρισης διαμορφώνεται ως εξής: ένας τυπικός Γενετικός Αλγόριθμος (ΓΑ) χρησιμοποιεί τις εξόδους μετα-μοντέλων Τεχνητών Νευρωνικών Δικτύων, τα οποία προσεγγίζουν τα Χαρακτηριστικά Ποιότητας και, μετά από κατάλληλη στάθμιση, αποτελούν την Αντικειμενική Συνάρτηση του ΓΑ. Για την υλοποίηση της προσέγγισης αυτής προσδιορίζονται καταρχήν οι πιο σημαντικές παράμετροι των στρατηγικών εκχόνδρισης, σύμφωνα με τη μέθοδο του Σχεδιασμού Πειραμάτων (ΣΠ) κατά Taguchi. Η Ανάλυση Διακύμανσης (ANOVA), που ακολουθεί το ΣΠ, υποδεικνύει συγκεκριμένες παραμέτρους ως τις πιο σημαντικές για το συγκεκριμένο τεμάχιο, με βάση κάποια Χαρακτηριστικά Ποιότητας, που συνήθως είναι ο Χρόνος Κατεργασίας και ο Συνολικός Εναπομένων Όγκος μετά το πέρας της εκχόνδρισης. Το σύνολο των δεδομένων - εισόδου και εξόδου, που συγκεντρώνονται κατά το ΣΠ, χρησιμοποιούνται για την εκπαίδευση των Τεχνητών Νευρικών Δικτύων (ΤΝΔ), που λειτουργούν έτσι ως μετα-μοντέλα πρόβλεψης των Χαρακτηριστικών Ποιότητας. Η προσέγγιση αυτή επιδεικνύεται για ένα τυπικό τεμάχιο με γλυπτές επιφάνειες. Γίνεται φανερό ότι τα μοντέλα ΤΝΔ, παρά την μεγάλη ταχύτητα απόκρισής τους, δεν εξασφαλίζουν ικανοποιητική ακρίβεια, τουλάχιστον μετά από εκπαίδευση με ένα σύνολο δεδομένων, που προκύπτει βάσει του ΣΠ, το περιορισμένο μέγεθος του οποίου, όμως, είναι αυτό που καθιστούσε τη μέθοδο καταρχήν ελκυστική. Επιπλέον, η απόδοση των μετα-μοντέλων ΤΝΔ εξαρτάται και από τη γεωμετρία του εκάστοτε μελετούμενου τεμαχίου. Μια δεύτερη προσέγγιση αναδιαμορφώνει το πρόβλημα βελτιστοποίησης της εκχόνδρισης τεμαχίων σε πρόβλημα δύο στόχων (Χρόνος Κατεργασίας και Συνολικός Εναπομένων Όγκος) παρακάμπτοντας τη χρήση προσεγγιστικών μετα-μοντέλων ΤΝΔ. Η επίλυση του προβλήματος πραγματοποιείται σε περιβάλλον εξελικτικής βελτιστοποίησης (EASY). Τρεις μέθοδοι βελτιστοποίησης δύο στόχων - ΓΑ, ΓΑ με Προσεγγιστική Προ-Αξιολόγηση και ΓΑ με ειδικά διαμορφωμένο Παίγνιο Nash Δύο Παικτών - εφαρμόζονται στην εύρεση των βέλτιστων τιμών των παραμέτρων εκχόνδρισης και συγκρίνονται μεταξύ τους, όσον αφορά το υπολογιστικό κόστος. Το κατάλληλα διαμορφωμένο Παίγνιο Nash αποτελεί μια γρήγορη μέθοδο, εξίσου γρήγορη με το ΓΑ υποβοηθούμενο από ΤΝΔ για Προσεγγιστικές Προ-Αξιολογήσεις λύσεων. Όμως, καταλήγει σε μία βέλτιστη λύση, αντί σε ένα σύνολο βέλτιστων λύσεων, όπως οι συνήθεις ΓΑ, και η τιμή του βέλτιστου εξαρτάται έντονα από τον τρόπο ανάθεσης των μεταβλητών του προβλήματος μεταξύ των παικτών, που λαμβάνουν μέρος στο Παίγνιο. Μια τρίτη προσέγγιση εκκινεί από το ότι η χρήση των δύο προαναφερθέντων στόχων οδηγεί σε λύσεις, οι οποίες δεν λαμβάνουν υπόψη τους την τοπική μορφή του εναπομένοντος όγκου επάνω στην ιδεατή τελική επιφάνεια του τεμαχίου. Για αυτό το λόγο, εισάγεται ως νέο - πρωτότυπο - κριτήριο βελτιστοποίησης η Τυπική Απόκλιση της Διαφοράς «Σχήματος» μεταξύ εκχονδρισμένου και ιδεατού τελικού τεμαχίου. Για τον υπολογισμό της αναπτύχθηκε αρχικά μια μέθοδος, η οποία πραγματοποιεί προβολές από το εκχονδρισμένο στο ιδεατό τελικό τεμάχιο με τη χρήση λειτουργικότητας (ρουτινών) λογισμικού CAD. Μέσω του στατιστικού ελέγχου υποθέσεων Levene, αποδεικνύεται ότι ένα μικρό ποσοστό (3% για τα παραδείγματα, που εξετάσθηκαν) των τριγώνων, που αποτελούν την επιφάνεια του εκχονδρισμένου τεμαχίου, είναι αρκετά για το συνεπή υπολογισμό της Τυπικής Απόκλισης των Διαφορών «Σχήματος». Η μέθοδος των προβολών, όμως, αποδεικνύεται μεγάλου υπολογιστικού κόστους, λόγω των επαναληπτικών χρήσεων ρουτινών του λογισμικού CAD. Αντ' αυτής, αναπτύχθηκε άλλη, πολύ ταχύτερη, μέθοδος, η οποία υπολογίζει την τοπική διαφορά «σχήματος» του εκχονδρισμένου τεμαχίου από ένα νέφος σημείων, που αντιπροσωπεύει την επιφάνεια του ιδεατού τελικού τεμαχίου. Η βελτιστοποίηση των παραμέτρων εκχόνδρισης με τρεις στόχους (Χρόνος Κατεργασίας, Συνολικός Εναπομένων Όγκος και Τυπική Απόκλιση της Διαφοράς «Σχήματος» μεταξύ εκχονδρισμένου και ιδεατού τελικού τεμαχίου) έγινε με δύο συζευγμένους αλγόριθμους: ένας διαθέσιμο ΓΑ, που εκτελείται σε περιβάλλον εξελικτικής βελτιστοποίησης (EASY), και ένα Μικρο-Γενετικό Αλγόριθμο (μ-ΓΑ), που αναπτύχθηκε από το μηδέν. Η σύζευξη πραγματοποιείται, έτσι ώστε να είναι δυνατή η επίλυση του προβλήματος βελτιστοποίησης, χωρίς να γίνουν παραδοχές και προσεγγίσεις, οι οποίες οδηγούν σε μερικώς βέλτιστες λύσεις. Ο μ-ΓΑ χρησιμοποιείται για την εύρεση της βέλτιστης κατανομής των «υψών Ζ», στα οποία τοποθετούνται οι φέτες της στρατηγικής εκχόνδρισης (ή ισοδύναμα: της βέλτιστης κατανομής του πάχους των φετών). Αντικειμενική Συνάρτηση του μ-ΓΑ αποτελεί ο σταθμισμένος γεωμετρικός μέσος των τριών στόχων, που αναφέρθηκαν προηγούμενα. Ο ΓΑ εκτελεί βελτιστοποίηση τριών στόχων και διαχειρίζεται τις υπόλοιπες παραμέτρους κατεργασίας, όπως αυτές ορίζονται στο τυπικό λογισμικό CAM, που χρησιμοποιείται. Αυτές είναι το Εργαλείο, το Οριζόντιο Βήμα, η Χάρη Κοπής, ο Αριθμός των Κάθετων Βημάτων, ο διακόπτης της Δημιουργίας Προφίλ, η Γωνία Μοτίβου, ο διακόπτης του Ορίου μη Διείσδυσης, ο διακόπτης της Απόστασης Ένωσης Πάσων, η Πρόωση ανά δόντι και η Ταχύτητα Περιστροφής της κυρίας ατράκτου της εργαλειομηχανής CNC. Η σύζευξη των ΓΑ και μ-ΓΑ αναιρεί προβλήματα σχετιζόμενα με τη μοντελοποίηση παραμέτρων, οι οποίες μεταβάλλονται σε πλήθος και μέγεθος ταυτόχρονα, όπως είναι τα Κάθετα Βήματα μιας στρατηγικής εκχόνδρισης. Έτσι, η τιμή της παραμέτρου Αριθμός Κάθετων Βημάτων καθορίζει το μέγεθος του χρωμοσώματος του μ-ΓΑ. Η ανταλλαγή δεδομένων μεταξύ του ΓΑ και του μ-ΓΑ σχετικά με τις τιμές των παραμέτρων και των στόχων πραγματοποιείται μέσω αρχείων. Σαν παραδείγματα εφαρμογής της βελτιστοποίησης μέσω συζευγμένων ΓΑ και μ-ΓΑ μελετώνται τρία τεμάχια που αποτελούνται από γλυπτές επιφάνειες: ένα απλοποιημένο προσθετικό ισχίο, παρόμοιο με αυτά που χρησιμοποιούνται στην Ορθοπεδική Χειρουργική, ένα πτερύγιο βιομηχανικού συμπιεστή και ένα πτερύγιο στροβιλομηχανής. Τα τεμάχια αυτά σχεδιάζονται με διαφορετικές τεχνικές και τα φυσικά ανάλογά τους έχουν διαφορετική λειτουργικότητα. Τα μέτωπα Pareto, που προκύπτουν από την εφαρμογή της μεθοδολογίας, αποτελούνται από σύνολα βέλτιστων λύσεων από τα οποία μπορούν να επιλεγούν οι λύσεις, που έχουν τις κατάλληλες βέλτιστες τιμές των στόχων ανάλογα με τις προτεραιότητες του χρήστη ή του μηχανουργού. Η κύρια συμβολή της διατριβής στο πεδίο του σχεδιασμού κατεργασιών εκχόνδρισης για γλυπτές επιφάνειες είναι η πρόταση εξελικτικών μεθόδων ως πρακτικά βιώσιμων, η αντιμετώπιση μεγάλου πλήθους παραμέτρων, η πρόταση πολλαπλών στόχων και εναλλακτικών περιοχών βέλτιστων λύσεων και τέλος, ιδιαίτερα σημαντικό, η χρησιμοποίηση ως βάσης του ίδιου πρακτικού εργαλείου, που χρησιμοποιείται στη βιομηχανική πράξη, δηλ. των συστημάτων CAM",CNC rough milling optimization of complex sculptured surface parts using artificial intelligence algorithms,,Εθνικό Μετσόβιο Πολυτεχνείο (ΕΜΠ),,,core
46841524,2009-09-24T00:00:00,"In this habilitation report, I present a synthesis of my research on three themes. The results for each topic depends on both the difficulty of the issues studied, the time devoted to them and opportunities for student supervision. These topics are on scheduling problems and mainly focus on the flowshop workshops that take into account additional constraints, close to the industrial reality, namely, (i) taking into account the constraints of grouping jobs, i.e., batch scheduling, (ii) consideration of time constraints on the sequence of jobs, known as time-lag, (iii) taking into account the deterioration of jobs. Our contribution to these three themes is on one hand the study of the complexity of the combinatorial structure of these problems, and on the other hand the implementation of optimization methods for efficient solving of these problems. This habilitation concludes with a general conclusion and the perspectives and research directions that we want involved in the near future and some thoughts on new direction of research.Dans ce mémoire, je présente une synthèse de mes travaux de recherche ainsi que le choix des thèmes étudiés. J'ai choisi de présenter trois thèmes. Les résultats obtenus pour chaque thème dépendent à la fois de la difficulté des problématiques étudiées, du temps qui leur est imparti et des circonstances et des opportunités d'encadrement des étudiants. Ces thèmes sont essentiellement sur les problèmes d'ordonnancement et principalement sont axées sur les ateliers de type flowshop avec prise en compte de contraintes supplémentaires, proche de la réalité industrielle, à savoir, (i) prise en compte de contraintes de groupement des tâches, connues sous le terme anglais, batch scheduling, (ii) prise en compte de contraintes temporelles sur la succession d'exécution des tâches, connues sous le nom de time-lags, (iii) prise en compte de la détérioration des tâches. Notre contribution à ces trois thèmes concerne d'une part l'étude de la complexité de la structure combinatoire de ces problèmes, et d'autre part la mise en œuvre de méthodes d'optimisation efficaces pour la résolution. Ce mémoire se termine par une conclusion générale, ainsi que les perspectives et les orientations de recherche que nous souhaitons engagé dans un avenir proche ainsi que quelques réflexions sur de nouvelles voies de recherche",Contribution à l'étude des problèmes d'ordonnancement flowshop avec contraintes supplémentaires : Complexité et méthodes de résolution,,HAL CCSD,,,core
21161967,2010-03-20,"The paper presents design and implementation of the hybrid system, which is the part of investigation focused on application of multiscale modeling in simulation of real industrial processes. The hybrid system is dedicated to support production processes based on metal forming, by using artificial intelligence and optimization algorithms. The proposed system is based on the multilayer architecture and consists of several functional components responsible for management of production process, modeling and simulations and, finally, optimization. The latter module aims at searching for optimial parameters of selected production processes, which form production chain, i.e. rod rolling and cold forging. Optimization is performed using results obtained from multiscale modeling calculations. Then, the optimized approach is passed directly to the configuration and control centre of the real industrial process as a feedback to obtain better quality of products employing lower costs of manufacturing. Moreover, the hybrid system is designed to exchange information with other external systems implemented inside an enterprise e.g. ERP and its modules. The internal structure of presented system is described in the paper, as well as measurable advantages of hybrid system application to real environment",Hybrid system for modeling and optimization of production chain in metal forming,,,,,core
18197397,2011-01-01T08:00:00,"Beef tenderness is an important palatability trait and is related to consumer satisfaction. In this dissertation, hyperspectral imaging (HSI) was implemented and tested for beef tenderness assessment. An acousto-optic tunable filter (AOTF)-based and a spectrograph-based portable HSI system (wavelength range: 400 nm to 1000 nm) were developed. These systems were used to acquire hyperspectral images of fresh beef ribeye muscle (longissimus dorsi) on hanging beef carcasses at 2-day postmortem in multiple commercial beef packing plants. The spectral dimension of the beef images was reduced, image features were extracted, and discriminant models were developed to forecast 14-day aged, cooked beef tenderness. Four different spectral dimensionality reduction methods (sample, chemometric, and mosaic principal component analyses, and partial least squares regression), seven different image feature sets (descriptive statistical features, wavelet features, gray level co-occurrence matrix features, Gabor transform features, Laws features, local binary pattern features, and pooled features), and three different discriminant models (Fisher\u27s linear discriminant analysis, support vector machines, and decision tree) were evaluated. A third-party true validation was conducted to evaluate the performance of the HSI systems. A new evaluation metric, called Accuracy Index (AI), was developed and used to compare beef tenderness prediction models. The AOTF system provided an AI value and accuracies for tender certification, tender classification, and tough classification of 70%, 91.7%, 84.1%, and 56.9%, respectively. The corresponding metrics for the spectrograph system were 66.8%, 86.7%, 65.8%, and 63.6%, respectively. In addition, a multispectral imaging approach was implemented by identifying and analyzing five key wavelengths. This approach provided an AI value and accuracies for tender certification, tender classification, and tough classification of 67.8%, 87.5%, 62%, and 68.2%, respectively, and took only eight seconds to assign a tenderness score for a beef carcass. Both hyperspectral and multispectral imaging show outstanding potential for real-time beef tenderness assessment. The successful adoption of this technology will lead to the development of a value-based system that benefits both consumers and the beef industry. Keywords: Beef tenderness, hyperspectral imaging, multispectral imaging, principal component analysis, and textural features",Development and evaluation of spectral imaging systems and algorithms for beef tenderness grading,,DigitalCommons@University of Nebraska - Lincoln,,,core
232131864,2007-01-01T08:00:00,"In an upper level undergraduate elective course in vehicle dynamics, the author has developed some off-campus experimental-based assignments that involve the students (in pairs) designing the experiments and providing their own measuring tools and test vehicle to get results that they can compare with their calculated predictions. The students are free to design their own procedure or do some investigation and use an industry-standard approach. The students typically find that their experimental results vary considerably from their predictions.  While this can be due to simple student implementation errors, it is typically a result of more complex issues.  This leads to some deep learning (and a little frustration) for the students as they look into why their results, which they have clearly observed, should differ so much from those predicted by standard machine design and dynamics formulae they have previously used without question.  Students must dig into and understand the assumptions behind the standard formulas and also the assumptions they made in designing and executing their experiments.  (Texts and Internet articles are often misleading on this subject so students also get an appreciation for the nuances of interpreting what someone has written.)  To balance the frustration factor associated with the “reality” of the assignment, there is a fun factor in testing using real vehicles (in various states of conforming to the original manufacturers’ specifications) that pushes the students further in the assignment than they would go with a typical campus lab experiment.  The paper describes two of the experiments providing some sample student approaches with examples of experiment-calculation discrepancies and their likely causes",Using Off-campus Student-designed Experiments to Aid in Student Learning,https://core.ac.uk/download/232131864.pdf,RIT Scholar Works,,,core
24369444,2008-01-29,"Realization of an Integrated Remote Monitoring and Diagnostic (IRMAD) tool draws upon three domains, software engineering, artificial intelligence and parallel processing architecture to provide close-to-real-time data validation assistance to plant personnel. Activities linked to the third component are reported on. The use of the MIMD computer (Volvox-transputers) and of specific software environments (Trollius and Unix environments) are described, together with their interrelation with other components of IRMAD.  I. Introduction  As technology evolves, human endeavours expand to include more hazardous environments as part of routine industrial activity. As the environments become more hazardous, and as the associated technology becomes more sophisticated and complex, the need for automatic operation of equipment, both for process control and for process safety, becomes more important. Effective automatic control requires the retrieval and assimilation of large amounts of data. The d..",Implementation Of A Parallel Processing Architecture In An Integrated Remote Monitoring System,,,,,core
22942315,2007-11-22,": Intelligent controller integrates in its structure in addition to standard attempt also  principles of artificial intelligence, e.g. adaptive control, fuzzy logic, neural networks,  quantitative control, etc. Implementation of such intelligent controllers to real process brings  together some difficulties. Our approach, which is described as follows, allows testing of  intelligent controllers in user friendly graphical environment. Simulation program for  dynamic systems is based on C programming language, which enables to simulate discrete  and continuous systems (both linear or non-linear). The same graphical environment can be  used to evaluate quality of control algorithm in real process. After testing in graphical  environment, resulting control algorithm can be sent by serial link without any change to  industrial controller with real time operating system.  Key words: real-time system, intelligent controller, fuzzy controller, neural networks, real  process  Introduction  Nowa..",Development Tool For Implementation Of Intelligent Controllers In Real,,,,,core
10901494,2010-01-01T00:00:00,"Software used by architectural and industrial designers – has moved from becoming a tool for drafting, towards use in verification, simulation, project management and project sharing remotely. In more advanced models, parameters for the designed object can be adjusted so a family of variations can be produced rapidly. With advances in computer aided design technology, numerous design options can now be generated and analyzed in real time. However the use of digital tools to support design as an activity is still at an early stage and has largely been limited in functionality with regard to the design process. To date, major CAD vendors have not developed an integrated tool that is able to both leverage specialized design knowledge from various discipline domains (known as expert knowledge systems) and support the creation of design alternatives that satisfy different forms of constraints. We propose that evolutionary computing and machine learning be linked with parametric design techniques to record and respond to a designer’s own way of working and design history. It is expected that this will lead to results that impact on future work on design support systems-(ergonomics and interface) as well as implicit constraint and problem definition for problems that are difficult to quantify","Patterns, heuristics for architectural design support: making use of evolutionary modelling in design",https://core.ac.uk/download/10901494.pdf,The Association for Computer-Aided Architectural Design Research in Asia,,,core
53429709,2009-01-01T00:00:00,"This study investigated the potential application
of mid-infrared spectroscopy (MIR 4,000\u2013900 cm 121)
for the determination of milk coagulation properties
(MCP), titratable acidity (TA), and pH in Brown
Swiss milk samples (n = 1,064). Because MCP directly
influence the efficiency of the cheese-making process,
there is strong industrial interest in developing a rapid
method for their assessment. Currently, the determination
of MCP involves time-consuming laboratory-based
measurements, and it is not feasible to carry out these
measurements on the large numbers of milk samples
associated with milk recording programs. Mid-infrared
spectroscopy is an objective and nondestructive technique
providing rapid real-time analysis of food compositional
and quality parameters. Analysis of milk rennet
coagulation time (RCT, min), curd firmness (a30, mm),
TA (SH\ub0/50 mL; SH\ub0 = Soxhlet-Henkel degree), and
pH was carried out, and MIR data were recorded over
the spectral range of 4,000 to 900 cm 121. Models were
developed by partial least squares regression using untreated
and pretreated spectra. The MCP, TA, and pH
prediction models were improved by using the combined
spectral ranges of 1,600 to 900 cm 121, 3,040 to 1,700
cm 121, and 4,000 to 3,470 cm 121. The root mean square
errors of cross-validation for the developed models were
2.36 min (RCT, range 24.9 min), 6.86 mm (a30, range
58 mm), 0.25 SH\ub0/50 mL (TA, range 3.58 SH\ub0/50 mL),
and 0.07 (pH, range 1.15). The most successfully predicted
attributes were TA, RCT, and pH. The model
for the prediction of TA provided approximate prediction
(R2 = 0.66), whereas the predictive models developed
for RCT and pH could discriminate between high
and low values (R2 = 0.59 to 0.62). It was concluded
that, although the models require further development
to improve their accuracy before their application in
industry, MIR spectroscopy has potential application
for the assessment of RCT, TA, and pH during routine
milk analysis in the dairy industry. The implementation
of such models could be a means of improving MCP
through phenotypic-based selection programs and to
amend milk payment systems to incorporate MCP into
their payment criteria","Prediction of coagulation properties, titratable acidity, and pH of bovine milk using mid-infrared spectroscopy",,'American Dairy Science Association',10.3168/jds.2008-1163,,core
81027260,2009-01-01T00:00:00,"We describe the implementation and deployment of a software decision support tool for the maintenance planning of gas turbines. The tool is used to plan the maintenance for turbines manufactured and maintained by Siemens Industrial Turbomachinery AB (SIT AB) with the goal to reduce the direct maintenance costs and the often very costly production losses during maintenance downtime. The optimization problem is formally defined, and we argue that feasibility in it is NP-complete. We outline a heuristic algorithm that can quickly solve the problem for practical purposes, and validate the approach on a real-world scenario based on an oil production facility. We also compare the performance of our algorithm with results from using mixed integer linear programming, and discuss the deployment of the application. The experimental results indicate that downtime reductions up to 65% can be achieved, compared to traditional preventive maintenance. In addition, using our tool is expected to improve availability with up to 1% and reduce the number of planned maintenance days with 12%. Compared to a mixed integer programming approach, our algorithm not optimal, but is orders of magnitude faster and produces results which are useful in practice. Our test results and SIT AB’s estimates based on operational use both indicate that significant savings can be achieved by using our software tool, compared to maintenance plans with fixed intervals.Proceedings of the Twenty-First Conference on Innovative Applications of Artificial Intelligence (IAAI'09) published by IEEE Computer SocietyCam",A Tool for Gas Turbine Maintenance Scheduling,,IEEE Computer Society,,,core
153410494,2011-01-01T00:00:00,"Another two years have passed from the last HoloMAS conference held in Linz
in 2009. It is a pleasure to say that the R&D activities around holonic and
multi-agent systems for industrial applications have not faded during this period.
On the contrary, the number of scientific events aimed at the subject field
is growing steadily. Besides HoloMAS, which has been the pioneering event in
this field, there are multiple conferences such as the IEEE Conference on Industrial
Informatics (INDIN), the IEEE Conference on Emergent Technologies
and Factory Automation (ETFA) or the IFAC Symposium on Information Control
Problems in Manufacturing (INCOM) that aim their attention at advanced
industrial automation systems based on intelligent agents.
This year’s conference was the eighth in the sequence of HoloMAS events. The
first three (HoloMAS 2000 in Greenwich, HoloMAS 2001 in Munich and Holo-
MAS 2002 in Aix-en-Provence) were organized as workshops under the umbrella
of DEXA association. Starting with 2003, HoloMAS became an independent
conference organized biyearly on the odd years, still under the DEXA patronage
(HoloMAS 2003 in Prague, HoloMAS 2005 in Copenhagen, HoloMAS 2007 in Regensburg
and HoloMAS 2009 in Linz). On the even years the attention is focused
on specific events: the IEEE Workshop on Distributed Intelligent Systems (DIS
2006) with a special track covering the “obvious”HoloMAS topics was organized
in Prague in June 2006. Similarly, the IEEE Conference on Distributed Human–
Machine Systems (DHMS 2008), which has absorbed the HoloMAS field, was
held in Athens, Greece, in March 2008, and the IFAC Workshop on Intelligent
Manufacturing Systems (IMS 2010) in Lisbon, Portugal, in 2010. This approach
allows the HoloMAS community to be better integrated with both the information
society-oriented DEXA community as well as the IEEE Society aimed at
human–machine systems, cybernetics, and industrial informatics.
The research of holonic and agent-based systems receives constant support
from both the public sector and private institutions. There is an increased interest
from the IEEE System, Man, and Cybernetics (SMC) Society, namely,
from its Technical Committee on Distributed Intelligent Systems, which leverages
the experience gained in the former Holonic Manufacturing Systems consortium.
Another IEEE body - the Industrial Electronics Society - supports
the related R&D field through its Technical Committee on Industrial Agents
(http://tcia.ieee-ies.org/). Its mission is to provide a base for researchers and
application practitioners, to share their experiences with applications of holonic
and agent technologies in the industrial sector, especially in assembly and process
control, planning and scheduling, and supply chain managements. There
are number of impacted journals that provide space for articles dealing with industrial
agents such as the IEEE Transactions on SMC, Part C: Applications and Reviews, Journal of Engineering of Artificial Intelligence Applications (EAAI),
IEEE Transactions on Industrial Informatics, Computers in Industry or the
Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS). Let us
recall that the extended versions of selected best HoloMAS 2009 papers were
published in the special issue of the International Journal of Production
Research.
It is our pleasure to inform you that for HoloMAS 2011 there were 36 papers
submitted, from which the ProgramCommittee selected 25 papers to be included
in this volume, by authors from 13 countries all over the world. Among the key
trends that the accepted papers report on is the effort to shift the holonic/agentbased
control system from the personal computer (the prevalent hosting platform
in the past) closer to the “hardware.” The obvious reason is to increase even
more the distributiveness and thus the robustness and flexibility of the control
system. In this model the control application is designed as embedded software
running on a dedicated microcontroller. This brings new challenges related to
limited resources in terms of memory, processing power, battery life, etc. Related
topics discussed in this volume are the employment of simulation techniques for
modeling, designing and validating the control system prior to its deployment
on the real hardware. The boom in Web applications and smart mobile devices
like smart phones and tablets have recently drawn the attention of the industrial
sector as it brings new challenges and possibilities for building next–generation
user interfaces for SCADA and operator panels. Looking at the applications of
holonic and agent systems we collected quite an interesting portfolio this year,
including smart grids, supply chain and logistics, healthcare, mobile robots and
unmanned aerial vehicles.
There were two invited talks specifically targeted toward HoloMAS 2011:
• Peter Skobelev (Magenta Solutions): “Multi-agent Systems for Real-Time
Resource Allocation, Scheduling, Optimization and Controlling”
• Alois Zoitl (Vienna University of Technology): “A Control Architecture for
Self-Reconfigurable Production Systems”
Also, for the first time in the HoloMAS history, there was a special session
organized covering the topic of smart industrial systems.
The HoloMAS 2011 conference was a highly motivating environment, challenging
the future research and fostering integration in the subject field. It has
always served as a showcase of the holonic and agent-based manufacturing research
offering information on the state of the art to specialists in neighboring,
knowledge-processing research fields covered by the DEXA multi-conference
event. We are very grateful to the DEXA Association for providing us with this
excellent opportunity. We would like to express our many thanks to Gabriela
Wagner for all her organizational efforts which were of key importance for the
success of this event. We would like to thank the IEEE SMC Society, and especially the Technical
Committee on Distributed Intelligent Systems of this Society, for its technical
co-sponsorship",Holonic and multi-agent systems for manufacturing: proceedings of the 5th International Conference on Industrial Applications of Holonic and Multi-Agent Systems,,'Springer Science and Business Media LLC',10.1007/978-3-642-23181-0,,core
214602749,2011-04-07T00:00:00,"Before deploying a software system we need to assure ourselves (and stake- holders) that the system will behave correctly. This assurance is usually done by testing the system. However, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. In this paper we examine this “obvious intuition” in the case of Belief-Desire-Intention (BDI) agents. We analyse the size of the behaviour space of BDI agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be; specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. We also discuss the implications of these findings on the testability of BDI agents.Unpublished1. Wooldridge, M.: An Introduction to MultiAgent Systems. John Wiley & Sons, Chichester, England (2002). ISBN 0 47149691X

2. Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., Luck, M.: Crossing the agent technology chasm: Experiences and challenges in commercial applications of agents. Knowledge Engineering Review 21(4), 345–392 (2006)

3. Benfield, S.S., Hendrickson, J., Galanti, D.: Making a strong business case for multiagent technology. In: P. Stone, G. Weiss (eds.) Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 10–15. ACM Press (2006)

4. Rao, A.S., Georgeff, M.P.: Modeling rational agents within a BDI-architecture. In: J. Allen, R. Fikes, E. Sandewall (eds.) Principles of Knowledge Representation and Reasoning, Proceedings of the Second International Conference, pp. 473–484. Morgan Kaufmann (1991)

5. Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University Press, Cambridge, MA (1987)

6. Zhang, Z., Thangarajah, J., Padgham, L.: Model based testing for agent systems. In: J. Filipe, B. Shishkov, M. Helfert, L. Maciaszek (eds.) Software and Data Technologies, Communications in Computer and Information Science, vol. 22, pp. 399–413. Springer, Berlin/Heidelberg (2009)

7. Ekinci, E.E., Tiryaki, A.M., Çetin, Ö., Dikenelli: Goal-oriented agent testing revisited. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 173–186. Springer, Berlin/Heidelberg (2009)

8. Gomez-Sanz, J.J., Botía, J., Serrano, E., Pavón, J.: Testing and debugging of MAS interactions with INGENIAS. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 199–212. Springer, Berlin/Heidelberg (2009)

9. Nguyen, C.D., Perini, A., Tonella, P.: Experimental evaluation of ontology-based test generation for multi-agent systems. In: M. Luck, J.J. Gomez-Sanz (eds.) Agent-Oriented Software Engineering IX, Lecture Notes in Computer Science, vol. 5386, pp. 187–198. Springer, Berlin/Heidelberg (2009)

10. Padgham, L., Winikoff, M.: Developing Intelligent Agent Systems: A Practical Guide. John Wiley and Sons (2004). ISBN 0-470-86120-7

11. Shaw, P., Farwer, B., Bordini, R.: Theoretical and experimental results on the goal-plan tree problem. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1379–1382. IFAAMAS (2008)

12. Erol, K., Hendler, J.A., Nau, D.S.: HTN planning: Complexity and expressivity. In: Proceedings of the 12th National Conference on Artificial Intelligence (AAAI), pp. 1123–1128. AAAI Press (1994)

13. de Silva, L., Padgham, L.: A comparison of BDI based real-time reasoning and HTN based planning. In: G. Webb, X. Yu (eds.) AI 2004: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3339, pp. 1167–1173. Springer, Berlin/Heidelberg (2004)

14. Erol, K., Hendler, J., Nau, D.: Complexity results for HTN planning. Annals of Mathematics and Artificial Intelligence 18(1), 69–93 (1996)

15. Paolucci, M., Shehory, O., Sycara, K.P., Kalp, D., Pannu, A.: A planning component for RETSINA agents. In: N.R. Jennings, Y. Lespérance (eds.) Intelligent Agents VI, Agent Theories, Architectures, and Languages (ATAL), 6th International Workshop, ATAL ’99, Orlando, Florida, USA, July 15-17, 1999, Proceedings, Lecture Notes in Computer Science, vol. 1757, pp. 147–161. Springer, Berlin/Heidelberg (2000)

16. Busetta, P., Rönnquist, R., Hodgson, A., Lucas, A.: JACK Intelligent Agents - Components for Intelligent Agents in Java. AgentLink News (2) (1999). URL http://www.agentlink.org/newsletter/2/newsletter2.pdf

17. Huber, M.J.: JAM: A BDI-theoretic mobile agent architecture. In: Proceedings of the Third International Conference on Autonomous Agents (Agents’99), pp. 236–243. ACM Press (1999)

18. d’Inverno, M., Kinny, D., Luck, M., Wooldridge, M.: A formal specification of dMARS. In: M. Singh, A. Rao, M. Wooldridge (eds.) Intelligent Agents IV: Proceedings of the Fourth International Workshop on Agent Theories, Architectures, and Languages, Lecture Notes in Artificial Intelligence, vol. 1365, pp. 155–176. Springer, Berlin/Heidelberg (1998)

19. Georgeff, M.P., Lansky, A.L.: Procedural knowledge. Proceedings of the IEEE, Special Issue on Knowledge Representation 74(10), 1383–1398 (1986)

20. Ingrand, F.F., Georgeff, M.P., Rao, A.S.: An architecture for real-time reasoning and system control. IEEE Expert 7(6), 33–44 (1992)

21. Lee, J., Huber, M.J., Kenny, P.G., Durfee, E.H.: UM-PRS: An implementation of the procedural reasoning system for multirobot applications. In: Proceedings of the Conference on Intelligent Robotics in Field, Factory, Service, and Space (CIRFFSS’94), pp. 842–849 (1994)

22. Bordini, R.H., Hübner, J.F., Wooldridge, M.: Programming multi-agent systems in AgentSpeak using Jason. Wiley (2007). ISBN 0470029005

23. Morley, D., Myers, K.: The SPARK agent framework. In: Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 714–721. IEEE Computer Society, Washington, DC, USA (2004)

24. Pokahr, A., Braubach, L., Lamersdorf, W.: Jadex: A BDI reasoning engine. In: R.H. Bordini, M. Dastani, J. Dix, A. El Fallah Seghrouchni (eds.) Multi-Agent Programming: Languages, Platforms and Applications, pp. 149–174. Springer (2005)

25. Bratman, M.E., Israel, D.J., Pollack, M.E.: Plans and resource-bounded practical reasoning. Computational Intelligence 4, 349–355 (1988)

26. Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: W.V. de Velde, J. Perrame (eds.) Agents Breaking Away: Proceedings of the Seventh European Workshop on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW’96), Lecture Notes in Artificial Intelligence, vol. 1038, pp. 42–55. Springer, Berlin/Heidelberg (1996)

27. Winikoff, M., Padgham, L., Harland, J., Thangarajah, J.: Declarative & procedural goals in intelligent agent systems. In: Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning (KR2002), pp. 470–481. Morgan Kaufmann, Toulouse, France (2002)

28. Georgeff, M.: Service orchestration: The next big challenge. DM Review Special Report (2006). URL http://www.dmreview.com/specialreports/20060613/1056195-1.html. (2006)

29. Dastani, M.: 2APL: a practical agent programming language. Autonomous Agents and Multi-Agent Systems 16(3), 214–248 (2008)

30. Naish, L.: Resource-oriented deadlock analysis. In: V. Dahl, I. Niemelä (eds.) Logic Programming, Lecture Notes in Computer Science, vol. 4670, pp. 302–316. Springer, Berlin/Heidelberg (2007)

31. Wilf, H.S.: generatingfunctionology, second edn. Academic Press Inc., Boston, MA (1994). URL http: //www.math.upenn.edu/∼wilf/gfology2.pdf

32. Sloane, N.J.A.: The on-line encyclopedia of integer sequences. http://www.research.att.com/∼njas/sequences/ (2007)

33. Burmeister, B., Arnold, M., Copaciu, F., Rimassa, G.: BDI-agents for agile goal-oriented business processes. In: Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 37–44. IFAAMAS (2008)

34. Dorigo, M., Stützle, T.: Ant Colony Optimization. MIT Press (2004). ISBN 0-262-04219-3

35. van Riemsdijk, M.B., Dastani, M., Winikoff, M.: Goals in agent systems: A unifying framework. In: Proceedings of the Seventh Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 713–720. IFAAMAS (2008)

36. Thangarajah, J., Winikoff, M., Padgham, L., Fischer, K.: Avoiding resource conflicts in intelligent agents. In: F. van Harmelen (ed.) Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 18–22. IOS Press (2002)

37. Nguyen, C.D., Perinirini, A., Tonella, P.: Automated continuous testing of multi-agent systems. In: Proceedings of the Fifth European Workshop on Multi-Agent Systems (EUMAS) (2007)

38. Dwyer, M.B., Hatcliff, J., Pasareanu, C., Robby, Visser, W.: Formal software analysis: Emerging trends in software model checking. In: Future of Software Engineering 2007, pp. 120–136. IEEE Computer Society, Los Alamitos, CA (2007)

39. Wooldridge, M., Fisher, M., Huget, M.P., Parsons, S.: Model checking multi-agent systems with MABLE. In: Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 952–959. ACM Press (2002)

40. Bordini, R.H., Fisher, M., Pardavila, C., Wooldridge, M.: Model checking AgentSpeak. In: Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 409–416. ACM Press (2003)

41. Raimondi, F., Lomuscio, A.: Automatic verification of multi-agent systems by model checking via ordered binary decision diagrams. J. Applied Logic 5(2), 235–251 (2007)

42. Burch, J., Clarke, E., McMillan, K., Dill, D., Hwang, J.: Symbolic model checking: 1020 states and beyond. Information and Computation 98(2), 142–170 (1992)

43. Fix, L., Grumberg, O., Heyman, A., Heyman, T., Schuster, A.: Verifying very large industrial circuits using 100 processes and beyond. In: D. Peled, Y.K. Tsay (eds.) Automated Technology for Verification and Analysis, Lecture Notes in Computer Science, vol. 3707, pp. 11–25. Springer, Berlin/Heidelberg (2005",On the testability of BDI agent systems,,'University of Otago Library',,,core
360546057,2011-04-07T00:00:00,"The purpose of this document is to describe the key technology issues for distributed information access in New Zealand. It is written from an industrial and public sector perspective, representing the views and findings of a wide cross-section of institutions in public and private sectors. It is an output of Objective 2 of the Distributed Information Systems project funded under contract UO0621 by the New Zealand Foundation for Research, Science and Technology (FRST).

It complements other project material produced by the academic research team at the University of Otago and its collaborators.

It focuses on requirements and applications, and is intended to provide a real-world, New Zealand-oriented context for the research in distributed information technologies (DIST).

The report represents the culmination of a series of workshops, industrial consultations, a questionnaire, and the experiences of the authors' institutions during the project, and therefore it supplements any previously produced material.UnpublishedPurvis, M., Cranefield, S., and Nowostawski, M., “A Distributed Architecture for Environmental Information Systems” in Environmental Software Systems – Environmental Information and Decision Support, Kluwer Academic, Dordrecht, The Netherlands (2000) 49-56.

Bush, G., Purvis, M., and Cranefield, S., “Experiences in the Development of an Agent Architecture”, in Design and Applications of Intelligent Agents, C. Zhang & V-W Soo (eds.), Springer-Verlag Lecture Notes in Artificial Intelligence, vol. 1881, ISBN: 3540679111, Berlin, Germany (2000) 76-87.

Huang, Z., Cranefield, S., Purvis, P. and McDonald, J. R, “A Proposal for a New Generation Hypertext Transfer Protocol on ATM Networks”, Chapter 34, World Wide Web: Technologies and Applications for the New Millennium, Computer Science Research, Education, and Applications Press, Athens, GA, U.S.A. (2000) 255-259.

Cranefield, S. and Purvis, M., “An agent-based architecture for software tool coordination”, Intelligent Agent Systems: Theoretical and Practical Issues, L. Cavedon, A. Rao, W. Wobcke (eds.), Springer-Verlag Lecture Notes in Artificial Intelligence, vol. 1209, Berlin, Germany (1997) 44-58.

Purvis, M., Zhou, Q., Cranefield, S., Ward, R., Raykov, R., and Jessberger, D., “Spatial Information Modelling and Analysis in a Distributed Environment” accepted for publication in Environmental Modelling and Software (2001).

Diaz, A., Cranefield, S., and Purvis, M., “Planning and Matchmaking in a Multi-Agent System for Software Integration”, Proceedings of the 11th International Conference on Mathematical and Computer Modelling, International Association for Mathematical and Computer Modelling (1997)

Nowostawski, M., Purvis, M. and Cranefield, S. Agent-Oriented Modelling for Complex Systems, in Applied Complexity—from neural nets to managed landscapes, S. Halloy and T. Williams (eds.), Christchurch, New Zealand Institute for Crop and Food Research (2000) 205–220

Huang, Z., Sun, C., Purvis, M., Cranefield, S., ""Weak sequential consistency models for distributed shared memory"", to appear in Proceedings of the 10th International Conference on Computing and Information (ICCI '2000), Kuwait (2000).

Cranefield, S., Purvis, M., and Nowostawski, M., “Is it an Ontology or an Abstract Syntax? Modelling Objects, Knowledge and Agent Messages”, Proceedings of the Workshop on Applications of Ontologies and Problem-Solving Methods at the 14th European Conference on Artificial Intelligence (ECAI'00), Berlin, Germany (2000) 16.1–16.4.

Huang, Z., Sun, C., Purvis, M. and Cranefield, S. “View-based Consistency for Distributed Shared Memory”, in Proceedings of the Joint Meeting of the 4th World Multiconference on Systemics, Cybernetics and Informatics (SCI'2000) and the 6th International Conference on Information Systems Analysis and Synthesis (ISAS'2000), Volume VIII, Lee, J. K., Juric, M., Bruzzone, A., Klovshy, D. and Fujita, M. (eds.) International Institute of Informatics and Cybernetics (2000) 1–6

Bush, G., Nowostawski, M., Cranefield, S. and Purvis, P. Platforms for Agent-Oriented Software Engineering, in Proceedings of the 7th Asia Pacific Software Engineering Conference (APSEC 2000), Dong J. S., He, J. and Purvis, M. (eds.), Los Alamitos, CA, USA, IEEE Computer Society Press (2000) 480–488

Cranefield, S. and Purvis, M., “Integrating Environmental Information: Incorporating Metadata in a Distributed Information Systems Architecture”, to appear in Proceedings of the Workshop Symposium on Integration in Environmental Information Systems (ISESS 2000), Environmental Informatics Institute, Gaiberg, Germany (2000).

Nowostawski, M., Purvis, M., and Cranefield, S., “An architectural approach to messaging in multi-agent systems”, Proceedings of the Autonomous Agents 2000 Workshop on Infrastructure for Scalable Multi-agent Systems (W13), Barcelona, Spain (2000) 105-110.

Cranefield, S. and Purvis, M., “Extending Agent Messaging to Enable OO Information Exchange”, Cybernetics and Systems 2000, (Proceedings of the 15th European Meeting on Cybernetics and Systems Research), R.Trappl (ed.), Vienna, Austrian Society for Cybernetic Studies, Vienna, Austria (2000) 573-578.

Purvis, M., Cranefield. S., Bush, G., Carter, D., McKinlay, B., Nowostawski, M., and Ward, R., “The NZDIS Project: an Agent-based Distributed Information Systems Architecture”, Proceedings of the Hawai`i International Conference on System Sciences (HICSS-33), R. H. Sprague, Jr. (ed.), (CD ROM) IEEE Computer Society Press, Los Alamitos, CA (2000).

Huang, Z., Cranefield, S., Chee, V. K. M., Purvis, M., “A Java Networking API for ATM Networks”, in Proceedings of the 31th International Conference and Exhibition on Technology of Object-Oriented Languages and Systems (TOOLS Asia’99), Nanjing, China, (1999) 306-315.

Cranefield, S. and Purvis, M., “UML as an ontology modelling language” Proceedings of the Workshop on Intelligent Information Integration, 16th International Joint Conference on Artificial Intelligence (IJCAI-99), (1999) 46-53.

Cranefield, S.J.S., Moreale, E., McKinlay, B. and Purvis, M. K., “Automating the Interoperation of Information Processing Tools”, Proceedings of the 32nd Hawaii International Conference on System Sciences (HICSS-32). Maui, Hawaii, IEEE (CD-ROM) (1999) 10 pages.

Cranefield, S.J.S., McKinlay, B., Moreale, E. and Purvis, M.K. “Automating Information Processing Tasks: An Agent-based Architecture”, TZI Report 9/98: Proceedings of the Workshop on Intelligent Agents in Information and Process Management, 22. Deutsche Jahrestagung für Künstliche Intelligenz (22nd German Conference on Artificial Intelligence, KI-98). Bremen, Center for Computing Technologies (TZI), University of Bremen (1998).

Purvis, M., Cranefield, S., and Ward, R., “Distributed Software Systems: From Objects to Agents”, Software Engineering: Education & Practice, Proceedings of the 1998 International Conference, IEEE Computer Society Press, Los Alamitos, CA (1998) 158-165.

Yu, Byung Hyun, Design and Implementation of a Native Java Interface for ATM Network Applications, M. Sc. Thesis, University of Otago (2000).

Nowostawski, M., Bush, G., Purvis, M. K., and Cranefield, S., ""Platforms for Agent-oriented Software"" Information Science Discussion Paper Series, Number 2000/13, ISSN1172-6024 (2000).

Cranefield, S., Purvis, M., and Nowostawski, M., “Is it an Ontology or an Abstract Syntax? Modelling Objects, Knowledge and Agent Messages”, Information Science Discussion Paper Series, Number 2000/08, ISSN1172-6024 (2000).

Cranefield, S. and Purvis, M., “Extending Agent Messaging to Enable OO Information Exchange”, Information Science Discussion Paper Series, Number 2000/07, ISSN1172-6024 (2000).

Cranefield, S. and Purvis, M., “Integrating Environmental Information: Incorporating Metadata in a Distributed Information Systems Architecture”, Information Science Discussion Paper Series, Number 2000/02, ISSN1172-6024 (2000).

Purvis, M., Cranefield, S., Nowostawski, M., Bush, G., Carter, D., McKinlay, B., Ward, R., “The NZDIS Project: an Agent-based Distributed Information Systems Architecture”, Information Science Discussion Paper Series, Number 99/17, ISSN1172-6024 (1999).

Purvis, M., Cranefield. S., and Nowostawski, M., “A Distributed Architecture for Environmental Information Systems”, Information Science Discussion Paper Series, Number 99/06, ISSN 1172-6024 (1999).

Cranefield, S. and Purvis, M., “UML as an Ontology Modelling Language”, Information Science Discussion Paper Series, Number 99/01, ISSN 1172-6024 (1999).

Cranefield, S., McKinlay, B., Moreale, E., Purvis, M., “Automating Information Processing Tasks: an Agent-based Architecture”, Information Science Discussion Paper Series, Number 98/7, ISSN 1172-6024 (1998).

Brendon Cahoon, Kathryn S. McKinley and Zhihong Lu. “Evaluating the performance of distributed architectures for information retrieval using a variety of workloads”. ACM Trans. Inf. Syst. 18, 1 (Jan. 2000), Pages 1 – 43.

S.-H Gary Chan and Fouad Tobagi. “Distributed servers architecture for networked video services”. IEEE/ACM Trans. Networking 9, 2 (Apr. 2001), Pages 125 – 136.

Conrad T. K. Chang and Bruce R. Schatz. “Performance and implications of semantic indexing in a distributed environment”. Proceedings of the eighth international conference on Information knowledge management, 1999, Pages 391 – 398.

Waiman Cheung and Cheng Hsu. “The model-assisted global query system for multiple databases in distributed enterprises”. ACM Trans. Inf. Syst. 14, 4 (Oct. 1996), Pages 421 – 470.

Miyi Chung, Ruth Wilson, Kevin Shaw and Maria A. Cobb. “Distributing Mapping Objects with the Geospatial Information Database”. Proceedings of the International Symposium on Distributed Objects and Applications, published by the IEEE, 1998.

Paul Dourish. “Using metalevel techniques in a flexible toolkit for CSCW applications”. ACM Trans. Comput.-Hum. Interact. 5, 2 (Jun. 1998), Pages 109 – 155.

Jeri Edwards. “3-Tier Client/Server at Work”. Pub. John Wiley, 1997.

Jacob Harris and Vivek Sarkar. “Lightweight object-oriented shared variables for distributed applications on the Internet”. Proceedings of the ACM conference on Object-oriented programming, systems, languages, and applications, 1998, Pages 296 – 309.

Nigel Hinds, Chinya V. Ravishankar. “Managing Metadata for Distributed Information Servers”. Proceedings of the 31st Hawaii International Conference on System Sciences (HICSS'98), published by the IEEE, 1998.

Nicholas Kassem. “Designing Enterprise Applications with the Java 2 Platform, Enterprise Edition”. Pub. Addison-Wesley, 2000.

Oscar Luiz Monteiro de Farias and Luiz Má C. P. M. de Farias. “Distributed information system on an Internet/intranet environment (DISI2E)”.Proceedings of the eighth ACM symposium on Advances in geographic information systems, 2000, Pages 197 – 198.

Julio C. Navas and Michael Wynblatt. “The network is the database: data management for highly distributed systems”. To be published in: ACM SIGMOD international conference on Management of Data on Management of data, 2001.

Okada, R.; Eun-Seok Lee; Shiratori, N. “Agent-based approach for information gathering on highly distributed and heterogeneous environment”. Proceedings of the 1996 International Conference on Parallel and Distributed Systems (ICPADS '96), published by the IEEE.

Robert Orfali and Dan Harkey. “Client/Server Programming with JAVA and CORBA”. Pub. John Wiley, 1998.

Robert Orfali, Dan Harkey and Jeri Edwards. “Instant CORBA”. Pub. John Wiley, 1997.

Robert Orfali, Dan Harkey and Jeri Edwards. “The Essential Distributed Objects Survival Guide, 2nd Edition”. Pub. John Wiley, 1996.

Owen de Kretser and Alistair Moffat. “Methodologies for Distributed Information Retrieval”. Proceedings of the The 18th International Conference on Distributed Computing Systems, published by the IEEE, 1998.

Andreas Paepcke, Chen-Chuan K. Chang, Terry Winograd and Héctor García-Molina. “Interoperability for digital libraries worldwide”. Commun. ACM 41, 4 (Apr. 1998), Pages 33 – 42.

Ahmed Saleh and George R. R. Justo. “A configuration-oriented framework for distributed multimedia applications”. Proceedings of the 2000 ACM symposium on Applied computing 2000 (volume 1), 2000, Pages 278 -280.

Wolfgang Theilmann and Kurt Rothermel. “Disseminating Mobile Agents for Distributed Information Filtering”. Proceedings of the First International Symposium on Agent Systems and Applications Third International Symposium on Mobile Agents, published by the IEEE, 1998.

Edgar Weippl. “Building secure knowledge bases: combining Java agents and Dbagents”. To be published in: Proceedings of the fifth international conference on Autonomous agents, 2001.

Jinxi Xu and W. Bruce Croft. “Cluster-based language models for distributed retrieval”. Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 1999, Pages 254 – 261.

Zhili Zhang and William Perrizo. “Distributed query processing using active networks”. Proceedings of the 2000 ACM symposium on Applied computing 2000 (volume 1), 2000, Pages 374 – 380",Distributed information access in New Zealand,,'University of Otago Library',,,core
232134184,2008-03-01T08:00:00,"in the field of human activity recognition has existed for quite sometime, but has gained popularity in recent years for use in many areas of application. In the security industry, suspicious activities could be detected in high-profile areas. In the medical industry, systems could be trained to detect patterns of motion indicating distress or to detect a lack of motion if a person had fallen and was unable to move. However, algorithms with reliable accuracy are difficult to implement in a real-time environment due to computational complexity. This thesis developed a new way of extracting and using data from a human figure in a video frame to determine what type of activity the subject is performing. Following background subtraction, a thinning algorithm operating on the silhouette offered a more robust limb extraction method, while a six-segment representation of the human figure offered more accuracy in deriving limb parameters, or components, such as distance from torso, and angle of displacement from the vertical axis. Neural networks or nearest neighbor classifiers used the limb components to identify a number of activities, such as walking, running, waving and jumping. This entire human activity recognition system was tested with both a MATLAB implementation (non real-time) and a C++ implementation in OpenCV (real-time). The algorithm achieved 96% classification accuracy in video feeds, which is only slightly lower than that of intensive, non real-time systems",Human activity recognition using limb component extraction,https://core.ac.uk/download/232134184.pdf,RIT Scholar Works,,,core
20805994,2008-08-14,"Abstract⎯The multi-agent paradigm for building intelligent systems has gradually been accepted by researchers and practitioners in the research field of artificial intelligence. There are also attempts of adapting agents and agent-based systems for creating industrial applications and providing eservices. In this paper, we present an attempt to use agents for constructing an online after-sale services system. The system is decomposed into four major cooperative agents, and in which each agent concentrates on particular aspects in the system and expresses intelligence by using various techniques. The proposed agent-based framework for the system is presented at both the micro-level and the macro-level according to the Gaia methodology. UML notations are also used to represent some software design models. As the result of this, agents are implemented into a framework for which exploits Case-Based Reasoning (CBR) technique to fulfil real life on-line services ’ diagnoses and tasks. Index Terms⎯Agent-based framework, after-sale services, agent-oriented software engineerin",Towards an Agent-Based Framework for Online After-Sale Services,,,,,core
24662542,2008-04-01,"Abstract. The research in autonomous systems has been influencing the improvement in navigation of mobile vehicles and robots using artificial intelligence techniques. Agriculture and industry are instances of economical segments that may be benefit from the application of those scientific efforts. However, those systems demand control architectures that, most of the time, have a high degree of complexity when having tested by physical implementations. On the other hand, recent works confirm the eficacy of Petri nets for modeling real systems with concurrent activities as well as in planning and controlling of mobile robot tasks. The aim of this paper is to apply Petri nets for modeling and analysing concurrent behaviors present in a simple robotic navigation architecture, which was successfully tested in a mini-robot (Khepera), to apply as reference for implementation in a Autonomous Agricultural Vehicle (VAA). Available software for academic purposes were used for models development and proprieties analysis. The results point that using the formalism proposed for modeling, one is capable of determining control policies, analysing and identifying conflicts in concorrent robotic behaviors with ease in comparison to real implementations",MODELING OF A CONTROL ARCHITECTURE FOR A MINI-ROBOT NAVIGATION USING PETRI NETS,,,,,core
67317742,[[issued]]2010-05-13T07:22:41Z,"[[abstract]]在現實世界裡, 吾人往往必需在資料不足的情況下做成決策, 我們稱之為小資料 學習(small data set learning)。筆者過去的研究裡, 已提出了結合連續資料方法與領域外 擴方式之總合模糊法(mega-fuzzification), 以解決彈性製造系統(flexible manufacturing system, FMS)中小資料學習的問題, 然而, 當總合模糊法學習法應用於其他領域時，往 往遇到參數數量過多以至於計算太困難，超過電腦負荷的情況。本研究擬提出新方法 以簡化過多之參數，但是仍然具有一定水準之學習準確度，同時再將新方法與其他學 習方法做一個比較。

Decisions are often made under limited information in the real world. In our previous study, a method named mega-fuzzification that is using continuous data and domain external expansion methods was proposed to solve the so called small data set learning problem in FMS. However, when the number of input attributes is too large, it is very hard to perform. This paper presents a novel method to solve the problem that knowledge has plenty of attributes in learning. Large number of input attributes not only causes hard computation but also breaks software limits. In addition, the proposed method will compared with other machine learning",於機器學習過程簡化多參數數量之方法研究與比較,,,,,core
79996950,2009,"15 years of industry experience developing large-scale, multi-agent information and control systems for diverse applications including manufacturing, combat pilot decision support and mission management, robotics, and surveillance. In these areas, he developed and applied technologies including distributed, component-based software architectures, software and systems engineering process models, intelligent control, the semantic web, and real-time artificial intelligence. In 1999, Dr. Hawker joined the Computer Science Department at the University of Alabama as an Assistant Professor focusing on software engineering, and in 2004 he moved t",AC 2009-2001: A SOFTWARE PROCESS ENGINEERING COURSE,,,,,core
145021402,2007-01-01T08:00:00Z,"In an upper level undergraduate elective course in vehicle dynamics, the author has developed some off-campus experimental-based assignments that involve the students (in pairs) designing the experiments and providing their own measuring tools and test vehicle to get results that they can compare with their calculated predictions. The students are free to design their own procedure or do some investigation and use an industry-standard approach. The students typically find that their experimental results vary considerably from their predictions.  While this can be due to simple student implementation errors, it is typically a result of more complex issues.  This leads to some deep learning (and a little frustration) for the students as they look into why their results, which they have clearly observed, should differ so much from those predicted by standard machine design and dynamics formulae they have previously used without question.  Students must dig into and understand the assumptions behind the standard formulas and also the assumptions they made in designing and executing their experiments.  (Texts and Internet articles are often misleading on this subject so students also get an appreciation for the nuances of interpreting what someone has written.)  To balance the frustration factor associated with the “reality” of the assignment, there is a fun factor in testing using real vehicles (in various states of conforming to the original manufacturers’ specifications) that pushes the students further in the assignment than they would go with a typical campus lab experiment.  The paper describes two of the experiments providing some sample student approaches with examples of experiment-calculation discrepancies and their likely causes",Using Off-campus Student-designed Experiments to Aid in Student Learning,https://core.ac.uk/download/pdf/145021402.pdf,RIT Scholar Works,,,core
60721767,2011-07-12T00:00:00,"International audienceA major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real world. The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e. cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources - expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex (LPFC and ACC) reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human-robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to ""cheating"" by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real world",Robot cognitive control with a neurophysiologically inspired reinforcement learning model,,'Frontiers Media SA',10.3389/fnbot.2011.00001,,core
71381392,2011-06-22T00:00:00,"The industries have been often seeking to reduce operating expenses, as to increase profits and competitiveness. To achieve this goal, it must take into account, among other factors, the design and implementation of new tools that accurately, efficiently and inexpensively allow access to information relevant to process. Soft sensors have been increasingly applied in
industry. Since it offers flexibility, it can be adapted to make estimations of any measurement, thus a reducing in operating costs without compromising the measurements, and in some cases even improve the quality of generated information. Since they are completely softwarebased, they are not subjected to physical damage as the real sensors, and are better adaptated to harsh environments with hard access. The success of this king of sensors is due to the use
of computational intelligence techniques, which have been widely used in the modeling of several nonlinear complex processes. This work aims to estimate the quality of alumina
fluoride from a Gas Treatment Center (GTC), which is the result of gaseous adsorption on
alumina virgin, using a soft sensor. The model that emulates the behavior of a alumina quality sensor the plant was created using an artificial intelligence technique known as Artificial Neural Network. The motivations of this work are: perform virtual simulations without compromising the GTC and make accurate decisions based not only on the operator's experience, to diagnose potential problems before they can interfere with the quality of alumina fluoride; maintain the aluminum reduction pot control variables within normal limits,
since the production from low quality alumina strongly affects the reaction of breaking the molecule that contains this metal. The benefits this project brings include: increasing the GTC efficiency, producing high quality fluoridated alumina and emitting fewer greenhouse gases into the atmosphere and increasing the pot lifespan.FAPESPA - Fundação Amazônia de Amparo a Estudos e PesquisasCVRD - Companhia Vale do Rio DoceAs indústrias têm buscado constantemente reduzir gastos operacionais, visando o aumento do lucro e da competitividade. Para alcançar essa meta, são necessários, dentre outros fatores, o projeto e a implantação de novas ferramentas que permitam o acesso às informações relevantes do processo de forma precisa, eficiente e barata. Os sensores virtuais têm sido aplicados cada vez mais nas indústrias. Por ser flexível, ele pode ser adaptado a qualquer tipo de medição, promovendo uma redução de custos operacionais sem comprometer, e em alguns
casos até melhorar, a qualidade da informação gerada. Como estão totalmente baseados em software, não estão sujeitos a danos físicos como os sensores reais, além de permitirem uma melhor adaptação a ambientes hostis e de difícil acesso. A razão do sucesso destes tipos de sensores é a utilização de técnicas de inteligência computacional, as quais têm sido usadas na modelagem de vários processos não lineares altamente complexos. Este trabalho tem como objetivo estimar a qualidade da alumina fluoretada proveniente de uma Planta de Tratamento de Gases (PTG), a qual é resultado da adsorção de gases poluentes em alumina virgem, via sensor virtual. O modelo que emula o comportamento de um sensor de qualidade de alumina foi criado através da técnica de inteligência computacional conhecida como Rede Neural Artificial. As motivações deste trabalho consistem em: realizar simulações virtuais, sem comprometer o funcionamento da PTG; tomar decisões mais precisas e não baseada somente
na experiência do operador; diagnosticar potenciais problemas, antes que esses interfiram na qualidade da alumina fluoretada; manter o funcionamento do forno de redução de alumínio dentro da normalidade, pois a produção de alumina de baixa qualidade afeta a reação de quebra da molécula que contém este metal. Os benefícios que este projeto trará consistem em: aumentar a eficiência da PTG, produzindo alumina fluoretada de alta qualidade e emitindo menos gases poluentes na atmosfera, além de aumentar o tempo de vida útil do forno de redução",Estimação da porcentagem de flúor em alumina fluoretada proveniente de uma planta de tratamento de gases por meio de um sensor virtual neural,,Programa de Pós-Graduação em Engenharia Elétrica,,,core
21057915,2009-08-24,"Several issues on automatic inspection of textile fabrics are discussed in this paper. To avoid the intense computation for real time inspection, we suggest a parallel pyramid hardware architecture consisting of several channels with CCD cameras in different resolutions working simultaneously. In inspection algorithms, we use a hierarchy of improved BP neural networks which demonstrate a tree structure in the progress of defect detection and classification. Attention is also directed to the system cost and viability under practical industry conditions. Preliminary experiment results show that our suggested hardware and software structures are very promising. 1",Machine Vision Based Inspection of Textile Fabrics*,,,,,core
20778584,2008-08-13,"ARCHON ™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the characteristics which typify this important domain. Secondly, the ARCHON framework is detailed- with a special emphasis being placed upon the implementation architecture. Thirdly, a brief resumee and status report of the main applications is presented. Finally, the lessons learned and the future plans are presented. 1","Mile End Road,",,,,,core
71481625,2009-01-28,"Three dimensional graphics processing requires many complex algebraic and matrix based operations to be performed in real-time. In early stages of graphics processing, such tasks were delegated to a Central Processing Unit (CPU). Over time as more complex graphics rendering was demanded, CPU solutions became inadequate. To meet this demand, custom hardware solutions that take advantage of pipelining and massive parallelism become more preferable to CPU software based solutions. This fact has lead to the many custom hardware solutions that are available today. Since real time graphics processing requires extreme high performance, hardware solutions using Application Specific Integrated Circuits (ASICs) are the standard within the industry. While ASICs are a more than adequate solution for implementing high performance custom hardware, the design, implementation and testing of ASIC based designs are becoming cost prohibitive due to the massive up front verification effort needed as well as the cost of fixing design defects.Field Programmable Gate Arrays (FPGAs) provide an alternative to the ASIC design flow. More importantly, in recent years FPGA technology have begun to improve in performance to the point where ASIC and FPGA performance has become comparable. In addition, FPGAs address many of the issues of the ASIC design flow. The ability to reconfigure FPGAs reduces the upfront verification effort and allows design defects to be fixed easily. This thesis demonstrates that a 3-D graphics processor implementation on and FPGA is feasible by implementing both a two dimensional and three dimensional graphics processor prototype. By using a Xilinx Virtex 5 ML506 FPGA development kit a fully functional wireframe graphics rendering engine is implemented using VHDL and Xilinx's development tools. A VHDL testbench was designed to verify that the graphics engine works functionally. This is followed by synthesizing the design and real hardware and developing test applications to verify functionality and performance of the design. This thesis provides the ground work for push forward the use of FPGA technology in graphics processing applications",Real Time 3-D Graphics Processing Hardware Design using Field-Programmable Gate Arrays.,,,,,core
143924678,2010-01-01T00:00:00,"Service-oriented computing is the new wave emerging from maturing Web services and the adoption of elements from Semantic Web technology. More sophistication, in response to business requirements, does of course not make it easier to use or to control. In particular, business processes demand resilience and real-time adaptation in the face of changing business requirements, incorporation of alternative services and finding suitable substitutes when those needed are unavailable. The European Union-funded Alive project is prototyping ideas, driven by commercial and industrial use cases, that utilize research in organizational modeling, software agents, model-driven engineering, artificial intelligence, the Semantic Web, and Web services to construct tools and demonstrators to address these needs. This article outlines the Alive architecture for service-oriented computing, describes some of the innovative tools we have developed and illustrates it all with a detailed run-through from one of our use cases","Adaptable, Organization-Aware, Service-Oriented Computing",,IEEE Computer Society Press,10.1109/mis.2010.93,,core
25874398,2011-04-01T00:00:00Z,"Control systems and optimization procedures require regular and reliable measurements at the appropriate frequency. At the same time, legal regulations dictate strict product quality specifications and refinery emissions. As a result, a greater number of process variables need to be measured and new expensive process analyzers need to be installed to achieve efficient process control. This involves synergy between plant experts, system analysts and process operators. One of the common problems in industrial plants is the inability of the real time and continuous measurement of key process variables.Absence of key value measurement in a timely manner aggravates control, but it does not mean that it is always an impossible step. As an alternative, the use of soft sensors as a substitute for process analyzers and laboratory testing is suggested. With the soft sensors, the objective is to develop an inferential model to estimate infrequently measured variables and laboratory assays using the frequently measured variables. By development of soft sensors based on measurement of continuous variables (such as flow, temperature, pressure) it is possible to estimate the difficult- -to-measure variables as well as product quality and emissions usually carried by laboratory assays.Software sensors, as part of virtual instrumentation, are focused on assessing the system state variables and quality products by applying the model, thus replacing the physical measurement and laboratory analysis. Multiple linear/nonlinear regression methods and artificial intelligence methods (such as neural network, fuzzy logic and genetic algorithms) are usually applied in the design of soft sensor models for identification of nonlinear processes.Review of published research and industrial application in the field of soft sensors is given with the methods of soft sensor development and nonlinear dynamic model identification. Based on soft sensors, it is possible to estimate product properties in a continuous manner as well as apply the methods of inferential control. By real plant application of the soft sensors, considerable savings could be expected, as well as compliance with strict legal regulations for product quality specifications and emissions",Soft Sensors - Modern Chemical Engineering Tool,,Croatian Society of Chemical Engineers,,"[{'title': None, 'identifiers': ['issn:0022-9830', '0022-9830', '1334-9090', 'issn:1334-9090']}]",core
304297930,2010,"Scopo del lavoro è stato la valutazione delle condizioni di innesco della NN-SCC di un acciaio per tubazioni interrate API 5L X65, adottando provini di flessione in tre punti prelevati in pieno spessore in direzione trasversale rispetto alla tubazione. Sono state effettuate prove di flessione lenta, monotone o monotone fino al raggiungimento di condizioni di deformazione plastica, con successiva variazione ciclica del carico a bassa frequenza. Allo scopo di riprodurre una superficie corrosa con morfologia simile a quella osservata in campo è stata adottata una particolare procedura di precorrosione elettrochimica, sviluppata in precedenti lavori. I risultati ottenuti hanno permesso di osservare l’innesco di numerose microcricche sui provini con superficie precorrosa, mentre tali microcricche sono state osservate sui provini con superficie tal quale in quantità significativamente inferiore. La morfologia delle microcricche innescate dagli attacchi localizzati è risultata molto simile ai casi di NN-SCC osservati in esercizio.Near Neutral Stress Corrosion Cracking (NN-SCC) is a particular form of stress corrosion cracking that takes place on coated buried pipelines It occurs under particular circumstances, in presence of slow plastic straining, in areas of disbondment in which the coating remains intact but a crevice is formed between it and metallic surface Disbonded coating prevents the CP current from reaching deep inside the crevice and allows the accumulation on steel of a dilute bicarbonate solution with CO(2) content from ground moisture, having pH 6-7 Thus, critical conditions are achieved [1]. NN-SCC phenomena are usually reproduced in laboratory experimental study by SSR tensile tests on cylindrical specimens. Cracking is found after the onset of necking, at high plastic strain, i.e. under conditions that are not representative of real loading. Another limitation is related with the corroding surface, as NN-SCC only initiates after several years of service, from localized attacks.  In order to reproduce the nucleation sites that promote NN-SCC on real structures an electrochemical technique for pre-corrosion of has been Improved [2]. This work deals with the initiation and growth of crack for NN-SCC. Initiation conditions were investigated by means of 3 point slow bending tests on full thickness specimens and, for comparison purpose, slow strain rate tests on cylindrical specimens the NN-SCC propagation was investigated through low frequency corrosion-fatigue tests on single notch three point bend beam specimens. Tests were carried out on a API 5L X65 controlled rolled ferritic-pearlitic steel for pipelines (Table I) in NS4 solution (0 483 g/L. NaHCO(3), 0 122 g/L KCl, 0 18 g/L. CaCl(2) and 0 1 g/L MgSO(4)). This solution was proposed by Parkins [4] and represents the average composition of the water found on Canadian pipelines, where NN-SCC phenomena were observed NS4 solution show 8 3 pH that reduces to 7 1 saturation with CO(2)/N(2) gas mixture at 0.05 atm CO(2) partial pressure The electrochemical pre-corrosion procedure involves 10 cycles of cyclic voltammetry from -1.8 to +1.8 V vs SCE (scan rate 20 mV/s) in NS4 solution modified by increasing the CO(2) and bicarbonate content (1 atm and 12.4 g/L, respectively) to obtain neutral pH. Near Neutral Stress Corrosion Cracking (NN-SCC) is a particular form of stress corrosion cracking that takes place on coated buried pipelines It occurs under particular circumstances, in presence of slow plastic straining, in areas of disbondment in which the coating remains intact but a crevice is formed between it and metallic surface Disbonded coating prevents the CP current from reaching deep inside the crevice and allows the accumulation on steel of a dilute bicarbonate solution with CO(2) content from ground moisture, having pH 6-7 Thus, critical conditions are achieved [1]. NN-SCC phenomena are usually reproduced in laboratory experimental study by SSR tensile tests on cylindrical specimens. Cracking is found after the onset of necking, at high plastic strain, i.e. under conditions that are not representative of real loading. Another limitation is related with the corroding surface, as NN-SCC only initiates after several years of service, from localized attacks. In order to reproduce the nucleation sites that promote NN-SCC on real structures an electrochemical technique for pre-corrosion of has been Improved [2]. This work deals with the initiation and growth of crack for NN-SCC. Initiation conditions were investigated by means of 3 point slow bending tests on full thickness specimens and, for comparison purpose, slow strain rate tests on cylindrical specimens the NN-SCC propagation was investigated through low frequency corrosion-fatigue tests on single notch three point bend beam specimens. Tests were carried out on a API 5L X65 controlled rolled ferritic-pearlitic steel for pipelines (Table I) in NS4 solution (0 483 g/L. NaHCO(3), 0 122 g/L KCl, 0 18 g/L. CaCl(2) and 0 1 g/L MgSO(4)). This solution was proposed by Parkins [4] and represents the average composition of the water found on Canadian pipelines, where NN-SCC phenomena were observed NS4 solution show 8 3 pH that reduces to 7 1 saturation with CO(2)/N(2) gas mixture at 0.05 atm CO(2) partial pressure The electrochemical pre-corrosion procedure involves 10 cycles of cyclic voltammetry from -1.8 to +1.8 V vs SCE (scan rate 20 mV/s) in NS4 solution modified by increasing the CO(2) and bicarbonate content (1 atm and 12.4 g/L, respectively) to obtain neutral pH. Both solutions were degassed with nitrogen for at least 24 hours, then were saturated with CO(2)/N(2) gas mixture for at least 2 hours, until a stable desired pH value was achieved Slow strain rate tests were carried out, according to ISO 7539-7, using cylindrical specimens with 3 mm diameter gauge "" length, at 10(-6) S(-1) constant strain rate Loading curves were drawn. After the tests, reduction of area was measured on specimens and fracture surfaces were examined Three-point slow bending tests (SB) were performed with the device shown in Figure 1, under displacement control on full thickness specimens, obtained in transversal direction. The strain field during the bending test has been studied by computer simulation with the software DEFORM-2D (R), developed for the 2D finite element modeling of manufacturing processes. Plane strain was considered [5] An elastoplastic behavior of the material has been considered, defined by tensile tests Details of the numerical simulations are reported elsewhere [5,6] Through computer simulations it was possible to obtain total strain and strain rate of the Most strained fibers during bending A displacement rate of 0.02 mm/min corresponding to a strain rate about 10(-6) s(-1) for the most strained fibers, was adopted Besides monotonic bending tests, Ripple Loading (RL) test were performed They were carried out similarly to SB tests, but bending was stopped after plastic strains was reached at 105% of yield point in the loading curve (maximum load; after the in linear elastic strain Afterwards, loading cycles with 0.9 minimum to maximum load ratio (R=P(mm)/P(max)) were applied at frequency of 10(-2) Hz, for about 14 days After testing, the specimens were observed for detecting nucleation of stress corrosion cracks Fatigue and corrosion-fatigue tests were carried out on Single-Edged-Notched-Three Points Bending (SENB3) specimens at 0 6 R, with sinusoidal cyclic loading. Delta K decreasing tests and constant P test were carried out. The frequency was 18 Hi. in air and 0.2 Hz in NN-SCC solution at free corrosion potential In order to evaluate the contribution of NN-SCC to corrosion fatigue crack growth rate, the Wet and Landes superposition model was considered [3]. It assumes corrosion-fatigue crack growth rate (da/dN)(CF) is sum of the fatigue contribution (da/dN)(F), obtained by Paris law, and stress corrosion contribution (da/dN)(C) Thus, the corrosion contribution can be evaluated by Eq.1 The results show loading curves of SSR tests in NN-SCC solution very similar to those obtained in air (Figure 2) but slightly lower reduction of area was observed due to small brittle crack propagations in necking region (Figure 3). In all cases, high reduction of area was approached. The SB test allows studying the material behavior at high uniform plastic deformations. Figure 4 shows the SB curves in air and in NN-SCC solution There was no change in the curves due crack initiation and propagation However, SEM analysis showed several small cracks on pre-corroded specimens whereas the specimens that have not undergone any pre-corrosion show only rare points of initiation (Figure 5) All cracks were perpendicularly to tensile loading, and occurred on external fibers, from localized attacks, showing a morphology similar to that observed in field, after very long time of exposure Thus, the data confirm the ability of the technique of pre-corrosion to promote suitable surfaces for laboratory studies on NN-SCC Owing to the cracking start from shallow pits, with very small depth, a mayor effect of local variations of solution composition connected with occluded cell can be assumed Inside pit, hydrogen ion concentration significantly inreases by the hydrolysis of Fe(2+) ions Consequently, the formation of atomic hydrogen significantly increases too and NN-SCC may occur. On the contrary, the amount of hydrogen is too small for embrittlement at free corrosion potential, outside occluded cell. The hydrogen embrittlement mechanism in NN-SCC was individuated by Parkins, and confirmed by other works [7,8] However, the mechanism requires continuous plastic deformations In SSR tests and SB tests, plastic deformation exceeding 20% were reached, but cracking were observed at much lower values in RL tests Although no significant changes in the loading curves were found, SEM examinations of the pre-corroded specimens after RL tests showed initiation of a single small crack, within a localized attack (Fig 6) It has been established that crack initiation is related with slow and continuous plastic straining, never being observed under static stress [9,10]. Thus, the cyclical variation of the load during the RL tests represents a promoting factor of Occurrence of the phenomenon In the pipeline, pressure variations may promote both initiation and propagation of micro-cracks, producing local stress concentration and plastic straining at crack tip The significantly lower number of crack embryos on RL specimens with respect to SSR and SB specimens seems to indicate that nucleation of NN-SCC cracks is related to plastic strain, going up with it [11] Figure 7 shows the results of corrosion fatigue crack growth rate tests in NN-SCC solutions compared to fatigue behavior in air. These tests were adopted in order to evaluate crack growth assisted by environment under continuous straining at the crack up The propagation curves show typical behavior of stress corrosion fatigue Crack growth rate increases with respect to fatigue in air at stress intensity factor variation (Delta K) between 1617 MPa root m and 24 MPa root m, as maximum stress intensity factor exceeds 40 MPa root m For K above 25 MPa root m curves obtained in air and in NN-SCC solution (at low frequency) approach together, because mechanical effect becomes prevalent on corrosion contribution The effect is evident by analyzing the data according to the Wet and Landes model (Figure 8): the contribution of corrosion to CF crack growth rate increases from Delta K threshold, reaching a plateau typical for stress corrosion phenomena. At high Delta K values, the increase of in contribution hides the corrosion contribution NN-SCC crack growth rate can be evaluated by Eq 2 by assuming that crack propagation for corrosion can only occur during increasing phase of loading cycle, with tensile stress at crack tip Plateau crack growth rate for stress corrosion cracking can be estimated at 3.2-10(5) mm/s Significant crack propagation occurs due to synergic action of corrosion and fatigue, differently from RL. tests It should be emphasized that the ratio between maximum load and in load for CF. tests is lower than RI, tests Moreover, stress concentration at crack tip is much higher with respect to the pit bottom. Finally, in the CF tests very high plastic straining occurs at crack tip at every cycles On the base of these results it can be point out that crack initiation seems to be mainly affected by the presence of localized attacks and the amount of plastic deformation achieved during continuous straining, while load variation amplitude and plastic strain rate are more important for the propagation from crack embryos",Studio della NN-SCC di acciai per tubazioni interrate tramite prove di flessione in tre punti,,AIM,,,core
24531601,2008-02-07,"Abstract — This paper addresses a practical intelligent multiagent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3] and defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], we are proceeding to build a system prototype and simulate it in real time to validate its logical behavior in normal and abnormal process situations. We also conducted a thorough system performance analysis to detect any computational bottlenecks. Although the preliminary system prototype design has limitations, simulation results have demonstrated an effective system logical behavior and performance. I",Prototype Design of A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities,,,10.1109/acc.2008.4587179,,core
24715544,2008-04-02,"Q-Learning is a Reinforcement Learning method for solving sequential decision problems, where the utility of actions depends on a sequence of decisions and there exists uncertainty about the dynamics of the environment the agent is situated on. This general framework has allowed that Q-Learning and other Reinforcement Learning methods to be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others. Despite its interesting properties, Q-learning is a very slow method that requires a long period of training for learning an acceptable policy. In order to solve or at least reduce this problem, we propose a parallel implementation model of Q-learning using a tabular representation and via a communication scheme based on cache. This model is applied to a particular problem and the results obtained with different processor configurations are reported. A brief discussion about the properties and current limitations of our approach is finally presented",A parallel implementation of Q-Learning based on communication with cache,,,,,core
54587494,2010,"The purpose of the work described in this paper is to construct and implement prediction models for optimizing container handling in particular at Cagliari’s Terminal Container.
Prediction models are based on heuristic algorithms such as neural networks and classification and regression trees and evolutionary algorithms such as Genetic Algorithms (GA) and Ant Colony Optimization (ACO). These models form part of a Web Oriented Decision Support System, for real time external data acquisition (GPS information, weather information, etc.), providing operators with the information processed in real time.
The most commonly used parameter for assessing terminal performance is productivity, namely the number of containers handled in the unit of time considered.
This parameter can be associated with the terminal as a whole, or with the ship, the stevedores, each vehicle used, the single operator and related to different time intervals (year, month, week, day, hour and shift). Usually the hourly average is considered for monitoring operations and identifying shortcomings.
The rate at which operations are performed can significantly reduce turn round time and thus minimize the loss of productivity associated with the ship’s time in port.
Because of the complexity of analyzing decision-making processes two sublevels are defined, that differ for type of decision and time horizon:
- The first level, generally organized around a weekly time horizon (from 10 to 1 days prior to ship’s arrival in port), for scheduling operations and activities in the different areas such as, ship, quay, yard, for making decisions that satisfy the different requirements;- The second level, aimed at specific resource allocation (personnel and equipment) on the basis of the decisions made at the first planning level in order to maximise productivity and minimize costs over a time horizon of roughly 24h.
Both levels of planning are characterized by temporal fragmentation and uncertain information. The information is received at undefined times and is continually updated, resulting in uncertain content. The strong dependence of the planning process on information flow, means it is necessarily dynamic and makes it difficult to effectively optimize and integrate decisions over sufficiently broad time horizons.
The aim of the study is to construct a model for predicting containership arrivals using heuristic-based evolutionary algorithms. The so-called ―Inspect Inspired Algorithms‖ are proving effective tools for solving industrial optimization issues.
In this study the different models proposed are implemented in a ―Decision Support System‖, while data are analysed from a temporal aspect adopting a ―learning from data‖ approach. Indeed the observation of real data (actual arrival time of ships and handling in port) form the knowledge base which relies on learning from the past. All discrepancies observed between prediction models and reality, along with other factors governing that condition prediction errors, create a historical base on which models are automatically recalibrated.
This approach has the dual purpose of analysing the causes (shortcomings) of prediction errors while refining models for future prediction; an analysis of the causes and effects of recalibrating the models.
The proposed DSS can also be used for simulation purposes. In fact the algorithms will be implemented for studying the effects of external variables taken individually or interacting with one another, thus providing a useful planning tool",Development of prediction models for container traffic,,,,,core
155519714,2009,"Recently, automatic 3D caricature generation has attracted much attention from both the research community and the game industry. Machine learning has been proven effective in the automatic generation of caricatures. However, the lack of 3D caricature samples makes it challenging to train a good model. This paper addresses this problem by two steps. First, the training set is enlarged by reconstructing 3D caricatures. We reconstruct 3D caricatures based on some 2D caricature samples with a Principal Component Analysis (PCA)-based method. Secondly, between the 2D real faces and the enlarged 3D caricatures, a regressive model is learnt by the semi-supervised manifold regularization (MR) method. We then predict 3D caricatures for 2D real faces with the learnt model. The experiments show that our novel approach synthesizes the 3D caricature more effectively than traditional methods. Moreover, our system has been applied successfully in a massive multi-user educational game to provide human-like avatars.Computer Science, Software EngineeringSCI(E)EI5ARTICLE82104-21162",Semi-Supervised Learning in Reconstructed Manifold Space for 3D   Caricature Generation,,computer graphics forum,10.1111/j.1467-8659.2009.01418.x,"[{'title': None, 'identifiers': ['issn:0167-7055', '0167-7055']}]",core
235570187,2003-04-01T00:00:00,"An archive of the Milo Canopener.The University of Lethbridge Library received permission from the Archives at Milo Library to digitize and display this content.Milo Can Opener
Box 12, Milo, AB T0L1L0
Canada Post Agmt. # 40607518
April, 2003
I 1
J^a^GO UO 0\)iAd Rates
Subscription Rates
Business Directory
$ 5.00
Milo $ 16.00
Quarter Page
6.00
(- pickup,delivery or mailed)
Half Page
8.00
Mailed (outside Milo) 24.00
Full Pages
15.00
Single Copies 2.00
Classifieds
2.00
Please note that those who paid for a Milo subscription by mail ($24) on our previous rates
have payed $8 too much. Your subscription
The following items are free of charge
renewal date will be extended by 6 months.
Notices Please sign them, no letters will be
Announcements printed if not signed.
( Wedding, Anniversary, Births, Showers, etc.) Requests to remain anonymous
Cards of Thanks will not be honoured.
News items Articles
Please send items to the following volunteer staff
Layout Editors - Barb Godkin - 599
-2213, 485-8389
Iris Gough - ..........................
599-2377
Production - Colleen Deitz
599-2306
Betty Armstrong
Zola Webber
Subscriptions -Georgina Ully -........................
..... 599 - 2424
Notices - Julie Nelson -.................................
.... 599-2175
Charlotte Nelson -..........................
599-2253
Cartoons & “Kids Say” - Marina Vannatta -...
..... 381 - 6389
Milo Can Opener
Box 12, Milo, Alberta, TOL 1L0
f Fax# 599 - 2457
? (fax shares line with phone so you will get the answering machine sometimes. You can also fax to Milo Municipal Library at 599-3850) Email: igodkin@telusplanet.net
Items may be left at Jamie’s Foods in the Can Opener box at back of store
or at Milo Municipal Library.
Please note the new fax number!
Please Note: The deadline for articles that need typing, etc. to be submitted is the Monday before the last Friday of each month. If your article is ready for press, we can accept it until Wednesday or Thursday.GOODS & SERVICES
Donald W. Kinney
Manager
Box 150
Milo, Alberta TOL 1LO
i
Tel: (403) 599-4101 Fax: (403) 599-2409
Scotiabank
BUSINESS HOURS:
Mon - Thurs 10:00 - 12:00 1:00 - 3:00 Friday 9:30 -12:00 1:00- 5:30
GRANT, KRYSTALOWICH & BENNETT
CERTIFIED GENERAL ACCOUNTANTS
FULL ACCOUNTING SERVICES AND CONSULTING
P.O. Box 239 Vulcan, Alberta TOL 2B0
Phone: 485-2996 485-2681
■
\^luVenture
Travel
Val Umscheid
travel consultant/tour guide
|J§g&
Box 88
Milo, AB mil T0L1L0 ^^1
e-mail: valuventure@telusplanet.net www.valuventure.com
tel: 1(403)599-2406 fax: 1(403)599-2247 toll free: 1(877)599-2499
*;■ •: ! Tift.
MILO
SEED CLEANING ASSOCIATION LTD.
599-2150
Cleaner Seed is Sown Cleaner Crops are Grown
Ed Posein - Manager
Box 7 Milo, AB T0L1L0
Doug Marks
PRESIDENT
Office: (403) 599-0003 Fax: (403) 599-3990 Mobile: (403) 362-1764
Marks
Oilfield Services Inc.
Trucking, Gravel
Oilfield Maintenance and Construction Pipelining
Pressure Washing and Steaming
(£sso)
VULCAN VILLAGE GAS BAR
P.O. BOX 425 VULCAN. ALTA. TOL 200 PHONE: 485-6000
FOOD TO CO
Garry &
Bernardine Nelson 485-2519
CORNER STORE & GARAGE
OIL - GAS - DIESEL - REPAIRS - WELDING A.M.A. TOWING
MERV & FRANCES GOLDTHORPE 485 - 6671We would like to thank our advertisers for their continued support.
Without them, we would not be able to print this newsletter for the enjoyment of the readers.
Dor, (Go Mo L3s®®imlb®
(403)485-6005 ■' P.O. Box 87, Vulcan, Alberta, Canada
FAIRBANKS DENTURE CLINIC
125 Centre Street, Vulcan, Alberta TOL 2B0
485-2368
Scott D. Fairbanks - Denturist
OFFICE HOURS:
Wednesday 9:00 am - 5 00 pm Friday 1.00 pm. - 5:00 p m
_________' ""__________ '_
Lori Vooys, CIM, FCSI Financial Planner
lori_vooys©scotiamcleoc).com
Suite 1800, Scotia Centre 700 - 2nd Street SW Calgary, AB T2P 2W1
Tel: (403) 298-7823 Fax: (403) 298-4054 Toll Free: 1-800-372-9274 Cell: (403) 815-6002
% ScotiaMcLeod
ScotiaMcLeod is a division of Scotia Capital Inc, a member of the Scotiabank Group.
V.->
Xr. B. X Drump
OPTOMETRIST
BOX 972
VULCAN, ALBERTA TGL280
telephones
485-2177
485-2886
Jamie’s H Foods
Carol and Jams Robertson Box 38 Milo, AB.TOLILO
Ph. 403-599-3922 Fax 599-3835
^LMARy KA>4
Donna Bennett Deiti
independent Beauty Consultant
P.O.. Box 37,
Milo, Alta. TOL 110 (403) 599-2140
A. P. C. S.
AARDVARK PEST CONTROL SERVICES®
JERRY GAUTREAU
P.C.T. Diploma, AIB Certified & ASI Certified
SUITE 213, 204 - 1440 52nd STREET N.E. CALGARY, ALBERTA T2A 4T8
Tel: (403) 273-MICE (6423) Fax: (403) 204-2125
^ ' -
Sc ©lean Sc pimple
INTERNATIONAL RECORDING ARTISTS
Cell: (403) 512-9066 Fax: (403) 599-2398
Keepln' the Country In music
LAH-MAR PROMO Ph: (403) 381-6389 Fax: (403) 381-6341& <5 £ Gsiecdiue ^beilc^t
for a&bMasitf*
599-2466
Milo, Alberta
Open - Monday, Wednesday, Friday, Saturday
RENO BEXTE
Weed Co ntrof (Centre
P & H GRAIN LTD. AGENT FOR ALTA HAIL INS.
TELEPHONE:
(403) 534-3461 ANYTIME FAX: (103) 534-2182
MOSSLE3GH. AB TOL IPO
■■ ■ / • ■■ .
MILO CAFE
CHINESE A'WESTERN TAKE OUT ORDERS
599-3832
Closed Monday
Monday-Sunday 8:30 am - 800 pm
Beer&. Wine with meals
_.......................
Next Canopener Deadline April 28""Organized for Savings Not fo r P rofit”
t
COOP
ARROWWOOD CO-OPERATIVE ASSOCIATION LIMITED
P.O. BOX 120 ARROWWOOD, Alberta TOL 0B0
(403) 534-3803 Store (403) 534-3804 Tire & Lube Center Fax (403) 534-3330
Your Suppliers of:
Petroleum Products - Fuel & Lubricants Tires - On and Off Road - New/Used/Repair Services Lube Center - Most vehicles - including 1 ton trucks Hardware /Lumber / Plumbing / Electrical Filters - Complete line of oil / air / fuel Belts / Bearings / Hydraulic Hoses Paint - interior/exterior - mixed to your color specifications Batteries - automotive/flashlight / watch / etc.
Automotive - lights /fuses /accessories Housewares / Sporting Goods Feed and Animal Health Supplies
Hours: Mon-Fri 8am-12 noon 1pm-5:30pm Saturday 8am-12 noon
DR. JAMES L. POTVIN
B.Sc (Hons), D.D.S.
General Dentistry
Patient Services
Nitrous gas for anxious patients Electronic freezing (no needle) Televisions & Walkmans Highest standard of sterilization for your protection Financing and payment plans . available 0A.C
Quality Dental Services
- Comprehensive preventative exams including periodontal and oral cancer screening
• Gentle hygiene care
• Toothcolored & silver fillings
• Cosmetic Bonding and Veneers to cover chips, cracks and stains
• Tooth whitening to brighten your smile
• Crown and Bridge
• Complete and Partial Dentures
Alt members of our Cavity Free Club are entered into a monMy draw for a S25 Gif Certificate Redeemable at Wolfe's Hardware Toy and Sporting Goods Department
Snake Valley Drop-In News
GENERAL MEETING Friday, April 4 at 2:30 p.m.
Everyone Welcome
Games
Bridge - every Tuesday at 1:30 p.m.
Crib - Monday, April 7 at 7:30 p.m. Monday, April 21 at 7:30 p.m.
NEW PATIENTS ARE ALWAYS WELCOME
J H, 3rd Avenue North, Vulcan
Health Nurse
The public health nurse will be at the drop-in Wednesday, April 9 from 1:00 to 2:00 p.m.VILLAGE OF MILO MINUTES
The regular meeting of the Village of Milo was held on Monday, February 17, 2003 at 7:00 p.m. at the Village Office.
Present were Mayor Vooys, Councillor Whaley, Councillor Phillips, Municipal Administrator Dorothy Way, John and Bea Kuzma, and Bob Phair.
The minutes of the meeting held on January 20, 2003 were read. Councillor Phillips moved the adoption of the minutes. Councillor Whaley seconded. CARRIED.
Mayor Vooys attended a FCSS meeting. The FCSS will be running a courtesy vehicle in this area. Councillor Whaley attended a VBDC meeting and they will be hosting an economic development training course. The VBDC will be joining up with Tourism. Mayor Vooys thanked the Council for the flowers.
Town man Victor Crowe reported that the tractor is leaking power steering fluid. The tractor is to be repaired before we start cutting the grass in the spring. It was suggested that a barrel be put up at the water treatment plant for the water to drain into after the trucks fill up. The water could also be drained into the sewer pipe on the south side of the water treatment plant. The culvert at the end of Center Street needs to be flushed out. Mayor Vooys will talk to Doug Marks and see if we can have an oilfield hydro truck flush out this culvert. The other end of this culvert will have to be cleaned out before this can be done.
Mr. Bob Phair attended the meeting to gather information on the water reservoirs so the company he works for KENECO can put together a proposal for this project. Mr. Bob Phair and a construction personnel will take a look at the reservoirs. The Municipal Administrator to look into funding from Alta. Transportation & Utilities as well as Infrastructure funding. A letter to be sent to Alpine Engineering Ltd. stating that we will not require his services.
A letter has been sent to JRD Contracting confirming that he has the contract for doing the sidewalk repairs. The Village has received the funding for this project.
Mr. & Mrs. John Kuzma attended the meeting in regards to the sale of the lots in BLOCK C PLAN 1403JK. The 2 trailers that are on this lot will be moved. There is no gas line on the property. The trailers that were there previously used propane.
MOVED by Councillor Phillips that the Village apply for the STEP grant this year. CARRIED.
We received 2 quotes one from Trinus Technologies Inc. and one from the Management Information Group on Municipal software. The Municipal Administrator to talk to Delanoy and Ziel and see if they have any information on what programs we should be using.
AXIA was here to take measurements of where the wire and the box will be installed for the Supemet. Mayor Vooys and Councillor Phillips want to be contacted when they return.
Letters were received from the Oldman River Intermunicipal Service Agency and the Vulcan County on the proposed subdivision for a petroleum card lock facility. Letter to be sent to the Vulcan County and the Oldman River Intermunicipal Service Agency stating that the Village of Milo has no objection to this proposed subdivision and petroleum card lock facility. MOVED by Councillor Phillips that the Village of Milo approve the petroleum card lock facility. Mayor Vooys seconded. CARRIED. Councillor Whaley is opposed to this proposal. Councillor Phillips to contact the Arrowwood Co-op and get a copy of the letter from Alta. Transportation and Utilities for Councillor Whaley.
MOVED by Councillor Whaley that the Village of Milo approve the proposed resolution for membership in the Oldman River Regional Services Commission. The proposed resolution is as follows: THAT the Village of Milo join the Oldman River Regional services Commission. CARRIED.
No one has come forward to organize the Communities in Bloom for the Village.
The next Council meeting of the Village of Milo was set for Monday, March 17, 2003 at 7:00p.m. at the Village Office.
The meeting adjourned at 8:55 p.m.DUSTING
M A house becomes a home when you can write “I love you” on the furniture!”
I can’t tell you how many countless hours that I have spent CLEANING! I used to spend at least 8 hours every weekend making sure things were just perfect—“in case someone came over.” Then I realized one day that no one came over; they were all out living life
and having fun!
Now when people visit, I find no need to explain the ‘condition’ of my home. They are more interested in hearing about the things I’ve been doing while I was away living life and having fun! If you haven’t figured this out yet, Please heed this advice!!!
LIFE IS SHORT! ENJOY IT! Dust if you must, but wouldn’t it be better to paint a picture or write a letter, bake a cake or plant a seed,—ponder the difference between
want and need?
Dust if you must, but there’s not much time, with rivers to swim and mountains to climb, music to hear and books to read, —friends to cherish and life to lead.
Dust if you must, but the world’s out there, with the sun in your eyes, the wind in your hair, the flutter of snow, a shower of rain,—this day will not come around again!
Dust if you must, but bear in mind, old age will come, and it’s not kind.
And when you go—and go you must—you, yourself, will make more dust!
4
“It’s not what you gather, but what you scatter, that tells what kind of life you have
lived!!”
• Aerator covers • Screens • Covers • Tarps • Tents
• Security pouches for radios
Bug sweeps
• Boat covers
• Patio swings
• Sprayer covers
• Tire covers
• Winter fronts
• Cargo nets
• Outfitter's tents
• Fertilizer & chemical spreader covers
N-l Upholstery & Tarp Manufacturing — 120 Main Street, Neville
&
Irene
Champion, flB P0 Box 206 TOL 0R0
Phone: 1-888-227-0170
Please phone before you corr>e if you're coming any distance to ensure we are here to ouoid disappointment.
• Portable generator covers
• Hopper covers
• Auger extension spouts
• Air conditioner covers
• Tonneau covers
• Windshield covers
• Bug screens
• Asphalt tarps 600°F rating
• Pack covers
• Truck tarps : flat & roll
• Recreational tents-
NAME
ADDRESS
Milo Minor Soccer Program 2003 Registration Form
______BIRTHD ATE: (d/m/y)__/__/__/ AGE(as at deadline)_
PHONE___________________________________SEX.
HEALTHCARE NUMBER
DOCTOR,
Please indicate any illness and physical limitations
Please indicate any illness or injury occurred over the last year,
PARENT/GUARDIAN(please print)__________________________
AGE CATEGORIES: (please check appropriate box)
(age categories may be combined if there are not enough registrations)
□
U-6 Mini-Soccer
4 to 5 years
□
U-8 Mini-Soccer
6 to 7 years
□
. U-10 Mini-Soccer
8to 9 years
□
U-12 Mini-Soccer
10 to 11 years
Important Information
All players must supply their own certified shin guards, soccer socks, and water bottles When: Practice to begin as weather permits(note will be sent home)
Parents will be responsible for ensuring their child has a ride for games.
Cost: $20/ participant (covers insurance and equipment costs)
Please make check payable to Milo Community Soccer Return form/payment by: April 10,2003
Shane Cranston Milo Community School
I the undersigned accept responsibility for the above registered player during activities associated with Milo Minor Soccer Program during the 2003 season in such that Milo Soccer Board and/or any of its agents cannot be held liable for any accidents which occur incidental thereto, and the player is accepted on this condition.
I would be interested in coaching.___________________________
Signature of Parent/Guardian:________________________________________Frank Mclnenly Auctions Ltd.
Vulcan, AB
. „ . - • , ..jss-**,.*,— .**»$*.*•; •' -
Serving The Agriculture Industry
Since 1967
(403)485-2440
Frank Mclnenly Stacey Mclnenly Les McIntyre
Foothills Ljvestock Auction
(403) 549-2120
Regular sales every Friday Special Calf Sales Bred Sales as announced
For up to date marketing call:
Frank Mclnenly (403) 485-2440 cell: (403) 485-8123
Marvin Fowler (403) 646 -2334 cell: (403) 625-6070
F M Trailer World
Located at Foothills Livestock Auction
Stavely. AB
SoulZern A/lerta's Exclusive JVorlerl Dealer
N0RBERT DEX TRAILTECH
Stock, Horse, Flatdecks SNew & Used
1-877-205-1999
Call StaceyGOALS:
* To provide affordable transportation where needed.
* To assist the Senior shut-in population and persons with special needs to be able to access health care, shopping and socialization.
* To make the transportation available every second week in each small community.
PARTNERS:
* Headwaters Health Authority
* Family and Community Support Services
* Vulcan Lions Club
* Friends of Little Bow
NOTE:
People who can not access the service without assistance would need to bring a companion along.
Pick up from residences will be made in each small town. Efforts will be made to accommodate stops in Vulcan
Headwafers Health Authority Regional Health Authority #3
Three Month Pilot Project
Start Date:
April 02, 2003
Pick up. Carmangay 9:00 am
Pickup Champion 9:30am
Pick up Milo 9:00 am
Arrive
Vulcan Community Health Centre
10:00 am
Downtown Vulcan - Mainstreet
10:30 am
Pick up VCHC 11:00 am
Arrive downtown 11:30 am
Pick up at designated point at 1:30 pm
Leave Vulcan for home 2:00 pm
April 09, 2003
Pick up Arrowwood 9:00 am
Pick up Mossleigh 9:30 am
Pick up Lomond 9:00 am
Amve
Vulcan Community Health Centre
10:00 am
Downtown Vulcan - Mainstreet
'l0:30am
Pick up VCHC 11:00 am
Arrive downtown T1:30 am
Pick up at designated point at 1:30 pm
Leave Vulcan for home 2:00 pm
Suggested Donation:
$6.00 per round trip
BUS SERVICE COMING TO THE COMMUNITIES OF THE VULCAN COUNTY
HANDI-GO BUS VULCAN COUNTY
ANSWERING A NEED IN OUR COMMUNITIES
Phone 485-2285 For information or bookingsif
caf
*'0**+' Are You A Poet and Don’t Know It?
4 Your talents could pay off
Enter the Milo Library Poetry Contest And you could win a “Chapters” gift certificate
3 Categories:
Grade 3 and Under Grades 4 to 6 Grades 7 and up
Your poem should have something to do with the history of Milo and/or the Milo Library Any type of poem is acceptable Submit to the Library by Thursday May 29th.
isrfbjfcrrH & comjpany
BARRISTERS and SOLJCnXJRS
SERVICING ALL YOUR LEGAL NEEDS
Dr. Robert J. (Bob) Langrldge will be In attendance at the Village Office in Milo the first Friday of each month from 1:00 p.m. to 3:00 p.m. Appointments may be made by calling 485-2070
Brian J. Murray and Robert J. (Bob) Langrldge servicing our Vulcan office 104 Centre Street Vulcan, Alberta Phone: (403) 485-2070
Areas of Law: Real Estate, Personal Injury, Divorce and Family Law, Wills and Estates. Dependent Adutts, Employment Law. Criminal Law, Business and Corporate Law. Mediation, Uhgatlon and Tax Law.
LETHBRIDGE OFFICE
#600, 220 - 4m Street South Phone: 403) 278-7781 Fax. (403) 320-8958 Toll Free: 1-800-552-8022
SOUTHERN ALBERTA'S REGIONAL LAW FIRMMilo Municipal Library
NEWS
a member of the Chinook Arch Regional Library System www.chinookarch.ab.ca
NASA arfti the “Association of Library Services to Children” have partnered to offer online and discovery-based activities focusing on space science and technology. http://www.ala.orq/alsc/spaceplace/clubspace.ht
ml
Search for current affairs radio and t.v.clips. http://archives.cbc.ca
Our next Library Board Meeting will be held on Thursday May 1st 2003.
THANK YOU
A HUGE THANK YOU goes out to David Healy for donating an incredible oak bookcase that he built. The “Friends of the Library” will be selling raffle tickets on it as a fund raiser. Check next months’ Can Opener for further details.
MARCH BESTSELLERS
“2nd Chance” by James Patterson “Everything’s Eventual” by Stephen King “Portrait in Death” by J.D. Robb “Body of Lies” by Iris Johansen “Midnight Voices” by John Saul “City of Bones” by Michael Connelly “Hunting Season” by Nevada Barr “The English Assassin” by Daniel Silva “Gone For Good” by Harlan Coben “Little Altars Everywhere” by Rebecca Wells
New Books
“The Academy Awards A Complete History of Oscar”
“Red Mafia” by Robert Friedman “Rush Hour Recipes” by Jean Pare “He Sees You When You’re Sleeping” by Mary Higgins Clark
“Take a Thief’ by Mercedes Lackey “Fortress Draconis” by Michael Stackpole “The Diamond Hunters” by Wilbur Smith “Shout at the Devil” by Wilbur Smith “Star by Star” by Troy Jenning
“Fit Not Fat at 40 Plus”
New Videos
“Cats and Dogs”
Junior Books
“Pokemon Jr. The Wobuffet Village” “Mathamazing” by Raymond Blum “Under The Stars” by Ben Baglio “Trouble With Girls” by Ted Stanton “Stinky” by Ted Stanton “Breakout at the Bug Lab” by Ruth Horowitz “Franklin’s Music Lessons” “Franklin and the Magic Show”
New CDs
“Scooby Doo Showdown in Ghost Town” “Kid Pix 3”
“Ocean Voyager”
“JumpStart Explorers”
“Improving Classroom Behavior” “Compton’s Encyclopedia 2000”
LIBRARY HOURS
Tuesdays 9:30 am -12:30 pm
.............1:30 pm -5:00 pm
Thursdays.........9:30 am - 12:30 pm
1:30 pm - 5:00 pm 6:30 pm - 8:00 pm Phone and Fax: 599-3850 email messages to libmil@chinookarch.ab.ca
FAMILY HAIRSTYLING
X 599-2491 X
MILO
HOURS TUES ■ FRI 9:00-5:00 SAT 10:00-2:00
WED. Mens walk in 9:00-12:00
NOTICE: Trends will be closed from March 11 to the 29th, and also on April 10th and 11th.
We apologize for any inconvenience.
Waxing, Eyelash & Eyebrow TintingSHARE the NEWS
MILO CORRESPONDENT for NEWS only
L. STUMPF 599-3748
NEW PUBLISHING DATE: THURSDAYS Deadline remains the same: Friday at 4:00 p.m.
CALL WANDA - 485-2036
COUNTY CALENDAR
Coming Events for non-profit groups Call Economic Development - 485-2992
CLASSIFIED AD RATES
$7.42 for 20 words + ,10p each additional word 2nd week half price (Minimum $4.45)
Phone. 485-2036 • Fax: 485-6938 Web site: www.vulcanadvocate.com
Customer Service
CHECK OUR WEBSITE!!
See the Classifieds, News and Photos on-line! www.vulcanadvocate.com
rnwr
the (rAP
and SHARE the NEWS
Bernice Finlay
main§vulHnadvocale.com
SUBSCRIPTION RATE
$25.00 per year (within county)For Those Who Grieve the Loss of a Loved One
It is hard to remain in the world and not feel a part of it. To watch others rush about like nothing has changed \$hen everything has.
Grief makes you feel alone.
Yet you do have a place in the world And others who care for you.
There is still beauty and meaning in life And you will find it again,
For now, it is enough to rest, to mourn,
And wrap yourself in your memories.
The world can wait.
A MOTHER’S WISH,
Oh, give me patience when the little hands Tug at me with their ceaseless demands.
Oh, give me gentle lips and smiling eyes And keep my mouth from hasty, sharp replies. Oh, let me not in weariness, confusion or noise Obscure life’s vision from its fleeting joys.
And when in years to come my house is still No bitter memories its rooms may fill.
Cl
-b_
a,
|h
5
P
p
\
1In
Jbu
.JL-
._U
?
T
X
v
X
\K
ID-
□
r
s
V
11
V
t]
£-
n 1 j
IX-
S-
o
p
UV
0
X
r
A_
nr
c
b-
p
a
P
In
31
JL
\ L
0
_y_
X-
b
m
a
^ r
Oh
hQ-
o
■P
P
<°
n
_s_
V 1
p
b_
_V_j
P
t
y
-* i
n
r
n
SI
Ul
a
JZj
b_
cv
q
^r
r
X
P
V |
V-
b
b
u
i.
_C_
i
Q
Z
c -
A
AJ
X.
\
b
lb
g
o_
_o__
c
\
T-
4
10
q
P
b.
a
b
$
-b
Q_
\
",Milo Community Volunteers,"Milo Canopener (April 1, 2003)",,,,core
84895075,2003-01-01T00:00:00,"The past thirty years have seen a rapid growth in the popularity and use of Genetic Algorithms for searching for optimal or near-optimal solutions to optimisation problems. One of the reasons for their immense success is the fact that the principles governing the algorithm are simple enough to be appreciated and understood. The major differences between one Genetic Algorithm and another lie within the schemes used to represent chromosomes, the semantics of the genetic operators, and the measures used to evaluate their fitness. Yet, these very differences make Genetic Algorithms so complex to design and implement when opposed with most real-world optimisation problems. The truth is that the people faced with these types of optimisation problems are not necessarily computer sci- entists or machine learning experts. Indeed, these types of problems constantly appear in various non-computing disciplines ranging from biology to manufacturing and economics. In this report, we present a simple, yet powerful, high-level technique that can be used to describe the structure of chromosomes and how their fitness can be evaluated. The method is abstract enough to insulate the practitioner from all the implementation, design, and coding details usually associated with a Genetic Algorithm. Nonetheless, a wide array of optimisation problems ranging from the classical travelling salesman problem and the n-Queens problem to time-table scheduling and dynamic programs can be described.peer-reviewe",University of Malta. Faculty of ICT,Generic chromosome representation and evaluation for genetic algorithms,https://core.ac.uk/download/84895075.pdf,,,core
54822692,1997-01-01T00:00:00,"In this paper a real-time quality control system for steel industry is presented. The system implements the surface defect classification of steel strips in flat rolled mills in real-time. To achieve reliable classification accuracy the system implements a MLP based hierarchical neural network. A dedicated hardware implementation has been designed and manufactured to meet the realtime constraints of the application. An ASIC neural chip directly implements the neural network and it is integrated on a custom high speed co-processor board, compatible with many commercial carrier board. The entire system has been tested with data coming from the plant",place:Heidelberg Germany,A hardware implementation of hierarchical neural networks for real-time quality control systems in industrial applications,,10.1007/bfb0020319,,core
9713914,2006-01-01T00:00:00,"Purpose – To provide a good insight into solving a multi-response optimization problem using neuro-fuzzy model and Taguchi method of experimental design.



Design/methodology/approach – Over the last few years in many manufacturing organizations, multiple response optimization problems were resolved using the past experience and engineering judgment, which leads to increase in uncertainty during the decision-making process. In this paper, a four-step procedure is proposed to resolve the parameter design problem involving multiple responses. This approach employs the advantage of both artificial intelligence tool (neuro-fuzzy model) and Taguchi method of experimental design to tackle problems involving multiple responses optimization.



Findings – The proposed methodology is validated by revisiting a case study to optimize the three responses for a double-sided surface mount technology of an electronic assembly. Multiple signal-to-noise ratios are mapped into a single performance statistic through neuro-fuzzy based model, to identify the optimal level settings for each parameter. Analysis of variance is finally performed to identify parameters significant to the process.



Research limitations/implications – The proposed model will be validated in future by conducting a real life case study, where multiple responses need to be optimized simultaneously.



Practical implications – It is believed that the proposed procedure in this study can resolve a complex parameter design problem with multiple responses. It can be applied to those areas where there are large data sets and a number of responses are to be optimized simultaneously. In addition, the proposed procedure is relatively simple and can be implemented easily by using ready-made neural and statistical software like Neuro Work II professional and Minitab.



Originality/value – This study adds to the literature of multi-optimization problem, where a combination of the neuro-fuzzy model and Taguchi method is utilized hand-in-hand",'Emerald',Multiple response optimization using Taguchi methodology and neuro-fuzzy based model,,10.1108/17410380610688232,,core
21082524,1997,"crucial for achieving the timely and costeffective execution of industrial production processes. In recent years, interest has increased in the use of artificial intelligence technologies for production planning and scheduling. 1,2 However, scheduling research typically has been theoretical, has had a narrow focus, and has not covered adaptation to unforeseen events (see the “Scheduling problem ” and “Previous scheduling research” sidebars). Our objective has been to use computerbased scheduling systems to enhance the problem-solving capabilities of human domain experts. During our research, we have developed a generic framework for building practical scheduling systems. This framework fosters the reuse of algorithms and the integration of knowledge-based technology into the organizational environment. It also supports dynamic adaptation. We successfully applied our framework in the implementation of three scheduling systems—that is, they all share the same system architecture and use similar problem-solving methodologies. The first two systems deal with serious real-life problems in the manufacturing industry: the rarely investigated continuous-flow scheduling problem and the widely known job-shop problem. The third system shows how concepts and techniques developed for industry can be transferred successfully to a medical domain. The framework Our generic framework is based on two design principles",,Knowledge-Based Scheduling Systems in Industry and,,,,core
22287329,1997,"This paper presents the architecture of a software dedicated to real-time manufacturing systems control. It is built on three main subsystems: a scheduler which implements a huge library of algorithms, an universal simulator which tests the solutions on an accurate model of the manufacturing system and a complete interface which helps the user to manage the system and to choose the best solution, according to the current objectives. The multi-model approach used here combines the advantages of several methods coming from operational research, artificial intelligence, human-computer interface and software engineering to build a flexible, portable and user-friendly tool",,Simulation Based Smart Scheduler for Manufacturing Systems,,,,core
146910538,2001,"Design research must be concerned with providing industry with significant competitive advantages in product and process design by developing advanced design methods and computer-based design tools, techniques, systems ad applications that support the creation of reliable, high quality, cost-effective, innovation and competitive products. The Design Technology Research Centre (DTRC) in the School of Design at the Hong Kong Polytechnic University focuses on the development of computer enhanced design processes and product-orientated and user-centred design tools and systems. Evolutionary computation, generative and knowledge based systems, Artificial Intelligence, integrated and interactive system techniques, virtual reality and computer supported collaborative work are employed for the implementation of these processes, tools and systems. In this paper, the author presents an overview of the research undertaken by the DTRC and discusses the issues related to the development of the design workstation of the future",International Academic Publishers / Beijing World Publishing Corporation,Design Workstation of the Future,,,,core
147921699,2006-11-24T14:05:59,"Over the last decades, calibration techniques have been widely used in robotics since they represent a cost-effective solution for improving the accuracy of robots and machine-tools. They only involve software modification without the necessity of revising the robot design or tightening the manufacturing tolerances. The goal of this thesis is to propose a procedure that guides the engineer through the calibration of a given multi-DOF flexure parallel robot within sub-µm accuracy. Two robots having 3 and 6 degrees of freedom have been considered as a case-study throughout the work. As in any calibration procedure, the work has been conducted on three different fronts: measurement, data processing and validation. The originality of this thesis in respect to published material lies in these three points. Measurements were carried out in a chamber inside which the measuring environment was protected against mechanical and thermal perturbations. In particular, the temperature variations experienced by the different parts of the measuring loop during a typical measurement session were stabilized within less than ± 0.1 °C. Proposed procedures allow the collection of reliable sets of data on the two robots. Delicate aspects of practical implementation are discussed. In particular, the problem of collecting a complete set of 6D data within accuracies in the nanometre range, for which there is still a lack of standard equipment, is solved using a procedure comprising several steps and making use of existing instrumentation. Suggestions for future investigations are given, regarding either long-term research problems or short-term industrial implementation issues. Data processing was performed using two different techniques in order to reach absolute accuracies after calibration better than ± 100 nm for translations and ± 3 arcsec for rotations (± 0.3 arcsec inside a more restricted range of ± 0.11°). The first method is called the ""model-based approach"" and requires the use of a known analytical relationship between the motor and operational coordinates of the robot. This relationship involves a certain number of parameters that can be related to the geometry of the robot (physical models) or simply mathematical coefficients of an approximating mathematical function (behavioural models). In the case of high-precision multi-DOF flexure parallel robots, we show that polynomial-based behavioural models are preferable to physical models in terms of accuracy for data processing tasks. In the second method, called the ""model-free approach"", the user does not need to model explicitly the main error sources (or their effect) affecting the robot accuracy. A model-free approach has been implemented using Artificial Neural Networks. We show that, using a heuristic search based on a decision-tree, the architecture of a network with satisfactory prediction capability can be found systematically. In particular, this algorithm can find a network able to predict the direct correspondence between the motor and operational coordinates (within the desired accuracy) without the help of the Inverse Geometric Model of the robot, i.e. even if the nominal geometry of the robot being calibrated remains unknown. This result contradicts conclusions reported by previous researchers. It is claimed that any robot (not necessarily a high-precision flexure parallel mechanism) can be calibrated by means of a ""neural approach"" in which the architecture of an appropriate network is determined with the help of our algorithm. Two examples (other than the robots measured in this thesis) are given to illustrate this universality. In the last part of this work, we provide a feasibility study on the use of indentation, a technique traditionally used for material testing, as a validation procedure to assess the accuracy of the calibrated degrees of freedom. The industrial interest of this technique lies in the fact that the robot is asked to execute similar motions to those involved in a real micro-machining operation","Lausanne, EPFL",Calibration of high-precision flexure parallel robots,https://core.ac.uk/download/147921699.pdf,10.5075/epfl-thesis-3712,,core
21439003,1998,"The hype in agent research will not last forever. If the  agent research community does not succeed in bringing its  results into real-world application within the next two or  three years it will share the fate of some other AI sections  which are today thought to be just academic. Multi-agent  systems applied to manufacturing are candidates for  bringing the first breakthrough. To achieve that,  methodologies for the design of agent systems for  manufacturing by software developers are needed. This  paper reports on the design process chosen in the  MASCADA project, whose goal is not only to develop an  algorithmic solution to manufacturing problems but also  to show a transfer strategy for these results",,Designing Agents for Manufacturing Control,,,,core
24553638,2004,"Geographic Information Systems (GIS) and Computer Aided Facility Management-Systems (CAFM) are currently undergoing the transition to storing and processing real 3D geospatial data. Applications for this type of data are, among others, location based services, navigation systems and the planning of large-scale construction projects. For presentation purposes and especially when working in the field, powerful visualisation systems are needed that are also capable of running on mobile devices like notebooks, personal digital assistants (PDA) or even cell phones. In such application areas, the free movement of the viewer’s position and the interaction with the data are of great importance. Real-time visualisation of 3D geospatial data is already well established and also commercially successful in the entertainment industry, namely in the market of 3D video games. The development of software in this field is very cost-intensive, so that the packages are often used for several game products and are therefore universally applicable to a certain extend. These so-called game engines include not only visualisation functionality, but also offer physics, sound, network, artificial intelligence and graphical user interfaces to handle user in- and output. As certain portions or sometimes even the whole engine are released as open source software, these engines can be extended to build more serious applications at very little costs. The paper shows how these game engines can be used to create interactive 3D applications that present texture-mapped geospatial data. The integration of 3D data into such systems is discussed. Functionality like thematic queries can be implemented by extending the internal data structures and by modification of the game’s accompanying dynamic link libraries. 1",,Visualisation Using Game Engines,,,,core
23264637,2000,"Holonic manufacturing aims to design standardized, modular manufacturing systems made of interchangeable parts, to enable flexibility and self-organizing capabilities for the production systems. Recent advances in Distributed Artificial Intelligence and Networking Technologies have proven that the theoretical Multi-Agent Systems (MAS) concepts are very suitable for the real life implementation of holonic concepts. Building on our recent results in the design and implementation of holonic reconfiguring architectures, this paper introduces a novel approach to the self-organization of distributed systems. First, by using fuzzy sets and measures theoretical concepts, we construct a mathematical foundation for modeling MAS. Then, by minimizing the vagueness facet of uncertainty in the information dealt with by the MAS, appropriate holonic structures emerge for each particular application. This approach opens new possibilities for the design of any distributed system that needs self-organiza..",,Fuzzy Modeling of Multi-Agent Systems Behavior. Vagueness Minimization.,,,,core
23268946,1995,"One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ..",,Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions,,,,core
24448742,1996,"ARCHON ™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest ever project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Two of these applications, electricity transportation management and particle accelerator control, have been run successfully on-line in the organisation for which they were developed (respectively, Iberdrola an electricity utility in the north of Spain and CERN the European Centre for high energy physics research near Geneva). This paper recounts the problems, insights and experiences gained whilst deploying ARCHON technology in these real-world industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the key design forces which shape work in this important domain. Secondly, the ARCHON framework is described- with a special emphasis being placed upon the implementation architecture. Thirdly, detailed descriptions of the Iberdrola and CERN applications are given- the motive for a DAI approach is outlined, the multiple agent systems which were built are described, and the benefits which accrued are stated. Finally, the lessons distilled from this work are discussed so that the engineers of future DAI systems may profit from our experiences. 1",,Using ARCHON to develop real-world dai applications for electricity transportation management and particle accelerator control,,,,core
4677755,2005-08-25T00:00:00,"Classification, the development of rules for the allocation of observations to one or more groups, is a fundamental problem in machine learning and has been applied to many problems in medicine and business.  We consider aspects of a classification model developed by Gallagher, Lee, and Patterson that is based on a result by Anderson.  The model seeks to maximize the probability of correct G-group classification, subject to limits on misclassification probabilities.  The mixed-integer programming formulation of the model is an empirical method for estimating the parameters of an optimal classification rule, which are identified as coefficients of linear functions by Anderson.  

The model is shown to be a consistent method for estimating the parameters of the optimal solution to the problem of maximizing the probability of correct classification subject to limits on inter-group misclassification probabilities.  A polynomial time algorithm is described for two-group instances.  The method is NP-complete for a general number of groups, and an approximation is formulated as a mixed-integer program (MIP).  The MIP is difficult to solve due to the formulation of constraints wherein certain variables are equal to the maximum of a set of linear functions.  These constraints are conducive to an ill-conditioned coefficient matrix.  Methods for generating edges of the conflict graph and conflict hypergraphs are discussed.  The conflict graph is employed for finding cuts in a branch-and-bound framework.  This technique and others lead to improvement in solution time over industry-standard software on instances generated by real-world data.  The classification accuracy of the model in relation to standard classification methods on real-world and simulated data is also noted.Ph.D.Committee Chair: Lee, Eva; Committee Member: Johnson, Ellis; Committee Member: Nemhauser, George; Committee Member: Prausnitz, Mark; Committee Member: Vidakovic, Bran",Georgia Institute of Technology,Solving a mixed-integer programming formulation of a classification model with misclassification limits,https://core.ac.uk/download/4677755.pdf,,,core
24650809,2003,"Grid computing provides key infrastructure for distributed problem solving in dynamic virtual organizations. It has been adopted by many scientific projects, and industrial interest is rising rapidly. However, Grids are still the domain of a few highly trained programmers with expertise in networking, high-performance computing, and operating systems. This paper describes our initial work in capturing knowledge and heuristics about how to select application components and computing resources, and using that knowledge to generate automatically executable job workflows for the Grid. Our system is implemented and integrated with a Grid environment where it has generated dozens of workflows with hundreds of jobs in real time. The paper also discusses the prospects of using AI to improve current Grid infrastructure",,Transparent Grid Computing: A Knowledge-Based Approach,,,,core
293434590,2006-01-01T00:00:00,"Este trabalho teve por objetivo utilizar redes neurais artificiais para explicar fenômenos que ocorrem em matadouros- frigoríficos avícolas. A estatística descritiva e a diferença entre as variáveis foram calculadas através do programa computacional SPSS for Windows 10.0 e para a construção das redes neurais artificiais foi utilizado o programa Neuroshell Predictor desenvolvido pela Ward Systems Group. Foram utilizados dados de 2004 a 2006 de dois matadouros- frigoríficos de aves e ocasionalmente um matadouro- frigorífico de suínos. Nos arquivos oferecidos pelas empresas, haviam dados de gerenciamento de matadouros-frigoríficos, como por exemplo, planilhas de condenação da inspeção, dados sobre chiller, teste de absorção das carcaças, dados sobre peso de carcaças, entre outros. Para a construção dos modelos foram escolhidas as “entradas”, para o cálculo do modelo preditivo, e a variável de “saída” a ser predita. Foram gerados oito (8) modelos com diferentes bancos de dados.  Todos esses modelos apresentaram redes neurais artificiais bem ajustadas, com valores altos para Correlação e Coeficiente de Determinação Múltipla (R²) e valores baixos para o Erro Médio e o Quadrado médio do Erro (QME). Não houve diferenças significativas entre os valores reais e os valores preditos em todas as validações dos oito (8) modelos. As redes neurais artificiais, com o software utilizado, foram capazes de explicar os fenômenos que envolvem o gerenciamento de matadouros-frigoríficos de aves, e ocasionalmente de suínos. A técnica utilizada oferece critérios objetivos, cientificamente desenvolvida, que será de grande valia no auxílio do técnico responsável pela tomada de decisões. A técnica permite também realizar simulações e mensurar a contribuição de diferentes variáveis que influenciam no fenômeno. É importante ressaltar que a utilização das redes neurais artificiais é uma ferramenta de auxílio à tomada de decisões, e não um programa que substitua o conhecimento científico e técnico.This work aimed to use artificial neural networks to explain the occurred phenomena in the poultry slaughterhouse. The descriptive statistics and the difference among the variable averages from the initial data were calculated with SPSS software, and the software used to build the artificial neural networks was Neruoshell Predictor, developed by Ward Systems Group. Data from three poultry and one swine slaughterhouses, gathered from 2004 until 2006, were used in this study. In these data there were information about carcasses condemnation, chillers, absorption tests, carcasses weights, medications and others. To build the neural networks,the chosen variables were identified as “input” and “output”. The “input” variables were selected for the predictive model calculation and the “output” variable for the one to be predicted. It was made 8 models with different databases. In all these models, the generated artifical neural networks were well adjusted always presenting a high Multiple Deteminant Coefficient (R²) and correlation and the lowest Mean Squared Error (QME) and mean error.  Also the differences between the real value and the predicted value in the 8 models studied were not statistically significant.This study concludes that artifical neural network, with the used software, were capable to explain the phenomena involved in the poultry slaughterhouse and that the modeling can also be extended to swine slaughterhouse. This powerfull technique offers objective criteria, scientifically generated, which can be used to assist in the decisions analysis process for this industry. It also allows to make simulations and to measure the contribution of each variable in the phenomena studied. It’s important to point out that artificial neural networks are intruments to assist the technician in decision making. Thus, it is not a program able to replace the cientific and technical knowledge",,Uso de redes neurais artificiais no gerenciamento de matadouros-frigoríficos de aves e suínos no sul do Brasil,,,,core
51969767,1999-01-01T00:00:00,"International audienceThis paper describes the real-time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling. MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach",'Institute of Electrical and Electronics Engineers (IEEE)',Real-time DSP implementation for MRF-based video motion detection,,,,core
205397125,1999-01-31,"AbstractMany organizations today are facing the problem of software migration: porting existing code to new architectures and operating systems. In many cases, such legacy code is written in a mainframe-specific assembly language and needs to be translated to a high-level language in order to be run on different architectures. Our research addresses this problem in a large-scale, real-life case study. We built an automatic tool, called Bogart, that translates IBM 370 assembly language programs to C. Bogart is based on Artificial Intelligence tools and techniques such as the Plan Calculus, translation by abstraction and re-implementation, program transformations, constraint propagation, and pattern recognition.Bogart was tested on real legacy code of a large commercial application: a database system and application generator, the main product of Sapiens International, Ltd. Bogart is compared with the literal brute-force translator initially developed by Sapiens, and is found to be superior on all counts, including portability of the resulting code, the amount of manual preparation required, and code size and speed. The results are shown for several small examples as well as a typical module consisting of several thousand lines of code from the Sapiens application. Bogart also seems to be more comprehensive than other reengineering systems reported in the literature. Bogart's analysis technology has recently been applied with significant commercial success to the analysis and remediation of Year 2000 bugs.This study demonstrates that certain AI techniques can be carefully combined to create industrial-strength applications that solve acute problems of Software Engineering. The fact that the research was carried out in industry on a real test case also revealed some of the problems of this approach. One example is the higher development cost of the AI approach, and the further effort that will be needed in order to extend it. (On the other hand, the literal translator has reached the end of its road, and cannot be enhanced at all.) Another problem we discovered is the difficulty of debugging the code produced by Bogart. The literal translator preserved the structure of the original program, whereas Bogart abstracted the code in various ways. As a result, the original assembly-language programmers found it harder to debug Bogart's code. This reaffirms the need for an explanation facility in intelligent applications",Published by Elsevier B.V.,Portability by automatic translation: A large-scale case study ,,10.1016/S0004-3702(98)00101-5,,core
55606136,2004-01-01T00:00:00,"A novel meso reactor based on oscillatory flow technology (Harvey et al., 2001) has been
recently presented in Harvey et al. (2003) as a new technology for reaction engineering and
particle suspension applications. Due to the demonstrated enhanced performances for fluid micro
mixing and suspension of catalyst beads and to the small volume of the reactor, this novel
miniature reactor is suitable for applications at specialist chemical manufacture and high
throughput screening. Furthermore, a high control of environment conditions (e.g. mixing
intensity, temperature) coupled with an online monitoring turns this reactor suitable for smallscale
applications to the bioengineering field, such as for fast parallel bioprocessing tasks.
This work concerns with the fluid dynamics characterisation of a novel miniature reactor.
Experimental results using state-of-art fibre-optic technology is used in order to demonstrate that
an accurate control of the residence time distribution (RTD) of liquid and solid phases can be
achieved within this reactor as well as enhanced (oxygen) mass transfer rates. Furthermore,
numerical simulations using Fluent ® software will be presented where simulated RTDs agrees
with the experimental results.
The meso reactor unit consists of 4.4 mm internal diameter and 35 cm long jacketed glass tubes,
with a unit volume of 4.5 ml and provided with smooth periodic constrictions (SPCs), with an
average baffle spacing of 13 mm. The internal diameter at the constricted zone (baffle internal
diameter) is 1.6 mm, leading to a reduction of the baffle free are of 87 %. This unit is able to
support batch or continuous operations mode, simply by configuring the tubes in parallel or in
series, according to the intended application. Mixing is achieved by oscillating the fluid at the
bottom or the top of the reactor by means of a piston pump, using oscillation amplitudes and
frequencies ranging from 0 to 4 mm centre-to-peak and 0 to 25 Hz, respectively.
Experimental studies using the Particle Image Velocimetry (PIV) technique (Harvey et al., 2003)
showed that different fluid mechanics are originated at different oscillation conditions
(oscillation amplitudes and frequencies). A plug flow or a stirred tank behaviour can be obtained
just by controlling the oscillation conditions. At low oscillatory Reynolds numbers (Reo), e.g. 10
to 100, the formation of axisymmetric eddies detached from the constrictions is coupled with low
axial velocities and makes it possible to continuously operate the reactor in a plug flow mode.
Increasing the Reo to values higher than 100, the eddy symmetry is broken and a complete
mixing state is achieved inside the meso reactor. Low oscillation amplitudes must be used if
axial dispersion is intended to be minimized, namely at plug flow setup.
Through an overall oscillation cycle, changes of the location of the main flow stream from near
the wall to the centre of each cavity and vice-versa was observed and is expected to lead to high
mass and heat transfer rates (Perry, 2002). Due to the observed high radial velocities, narrow
residence times distributions are expected to be obtained (Perry, 2002). Also high axial
circulation rates were also observed at high Reos (above 100) and it was proved to lead to an
enhanced performance on catalyst beads suspension. The relation of this fluid mechanics with
the real performance of this novel meso reactor will be demonstrated.
Tracer injection technique is applied to perform RTD studies inside a single SPC tube of the
meso reactor. Spectroscopy UV/VIS technique is used to measure the concentration of a
coloured tracer at the inlet and outlet (at continuous mode) or at the bottom and the top of the
tube (at batch mode). A fibre optic apparatus is employed in order to obtain highly accurate
online measurements of the UV/VIS absorbance. Mixing times are calculated for experiments at
batch mode. Different flow rates are used to determine the effect of the flow rate over the RTD at
continuous operation and axial dispersion is presented by the Bodenstein number, Bo.
Determination of KL.a values is achieved by online measurement of the oxygen concentration
using a special fibre optic probe. The working tip of the probe was dip-coated with a ruthenium
complex immobilised in a sol-gel matrix. This complex is excited to fluorescence by a blue led
(470 nm outpuk peak) and the level of the fluorescence is inversely related to the concentration
of the oxygen through the Stern-Volmer equation (Wang et al., 1999), which is measured by the
fibre-optic apparatus. Retention of solid phases (e.g. catalyst beads and yeast cells) inside the
meso reactor will also be tested.
Further studies using the Computation Fluid Dynamics (CFD) technique will be presented where
accurate prediction of the distribution of residence times is achieved. The use of the distributionfunctions
permits to classify the flow behaviour inside this novel meso reactor patterns and to
calculate mixing efficiencies and axial dispersion coefficients (expressed by the Bo number) at
different oscillation conditions.
A simple 2-D axisymmetric laminar model showed good agreement with flow patterns
visualisations using PIV for Reo below 100 but a 3-D model with a very fine mesh was required
to simulate breakage of axisymmetry. Consequently, 3-D models based on laminar and Large
Eddy Simulations (LES) will be used to maximize the matching of RTD at higher oscillation
conditions. Main intended application of CFDs to this novel meso reactor is the design of a meso
reactor unit, which could operate at the best oscillation conditions and flow rate for cell cultures
and biocatalyst applications",,Residence times and mixing of a novel continuous oscillatory flow meso reactor,https://core.ac.uk/download/55606136.pdf,,,core
199611538,1996-01-01T00:00:00,"Earthmoving is a common activity at mines, construction sites, hazardous waste cleanup locations, and road works. Expensive and sophisticated machines such as front-end-loaders (FEL), backhoe loaders, LHD loaders and front shovels are used for these excavation tasks. Autonomous excavation control for these machines has gained considerable attention in order to remove human operators from hazardous environments, improve productivity and utilization, reduce machine abuse, as well as decrease machine operating costs. However, automatic control of excavation tasks for many sites that require digging in rock cannot be implemented using existing factory-based automation techniques. For example, control of bucket motions by simply partitioning the terrain into a set of volumes where each equals the bucket capacity often does not work. Planning in this way is possible only when digging in the materials such as loose soils where bucket motion resistance through the media can be predicted. Resistance predictions are impossible and/or infeasible to generate for excavation in the environments which consists mainly of irregular rigid objects such as rock piles with oversized particles, since no means exists to predetermine subsurface bucket/material interactions that are required to preplan the bucket trajectory. As a result, bucket actions must be determined through on-line decision making based on sensory feedback of the current excavation status in the unpredictable, unstructured and dynamic rock excavation environment. This research proposes a control method for autonomous rock excavation. The control architecture is designed following the behavior-based control concept. That is, the rock excavation control problem is solved by decomposition of the complicated task into a variety of simple elements that can be implemented by excavation behaviors. However, this control approach presents a new structure and operational paradigm that is developed based on, but different from the traditional behavior control method. Here, the behaviors are chosen using fuzzy excavation situation assessment with guidance of excavation task planning which embodies excavation heuristics and human strategies. Task plans are formulated using finite state machines which integrate neural networks for decision making. This organizational structure has the capability to include more excavation goals and to adapt to different environments via learning. Excavation behaviors are performed by primitive and machine executable actions or action sequences structured using finite state machines and simple action arbitration rules. The actions of human FEL operators were observed and analyzed to extract basic bucket actions and define rules of arbitration for different actions or action sequences under particular excavation environments. Fuzzy logic is applied to implement each excavation action where fuzzy rules represent the human experience and heuristics that are intrinsically linguistic, and bucket excavation motions are evaluated based on insufficient and inaccurate input sensory data. A variety of experiments were performed to test the ability of the proposed control algorithm. The laboratory-based experimental autonomous excavation system consists of a robotic arm, an excavation testbed, a force/torque sensor mounted between the robot arm wrist and the excavation bucket, and a control computer. Various rock piles to simulate realistic excavation environments and conditions were generated in the testbed. With these experiments, the control algorithm has demonstrated the ability to execute real-time automated loading cycles effectively and efficiently in complex excavation environments and under difficult digging conditions, through the use of the flexible excavation behaviors",The University of Arizona.,Intelligent control of autonomous rock excavation: Theory and experimentation,,,,core
20700348,2002,"This paper presents the design, development and implementation of a Dynamic Fuzzy Neural Networks (D-FNNs) Controller suitable for real-time industrial applications. The unique feature of the D-FNNs controller is that it has dynamic self-organising structure, fast learning speed, good generalisation and flexibility in learning. The approach of rapid prototyping is employed to implement the D-FNNs controller with a view of controlling a Selectively Compliance Assembly Robot Arm (SCARA) in real time. Simulink, iterative software for simulating dynamic systems, is used for modelling, simulation and analysis of the dynamic system. The D-FNNs controller was implemented through Real-Time Workshop (RTW). RTW generates C-codes from the Simulink block diagrams and in turn, the generated codes (object codes) are downloaded to the dSPACE DS1102 floating-point processor, together with the supporting files, for execution. The performance of the D-FNNs controller was found to be superior and it matches favourably with the simulation results",,Real-time implementation of a dynamic fuzzy neural networks controller for a SCARA,,10.1016/s0141-9331(02)00069-8,,core
82215202,1995-06-30,"AbstractOne reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparative experiments in the real-world domain of electricity transportation management. Finally, the success of the approach of building a system with an explicit and grounded representation of cooperative problem solving is used to outline a proposal for the next generation of multi-agent systems",Published by Elsevier B.V.,Controlling cooperative problem solving in industrial multi-agent systems using joint intentions ,https://core.ac.uk/download/pdf/82215202.pdf,10.1016/0004-3702(94)00020-2,,core
143105982,2002-01-01,"Q-Learning is a Reinforcement Learning method for solving sequential decision problems, where the utility of actions depends on a sequence of decisions and there exists uncertainty about the dynamics of the environment the agent is situated on. This general framework has allowed that Q-Learning and other Reinforcement Learning methods to be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others. Despite its interesting properties, Q-learning is a very slow method that requires a long period of training for learning an acceptable policy. In order to solve or at least reduce this problem, we propose a parallel implementation model of Q-learning using a tabular representation and via a communication scheme based on cache. This model is applied to a particular problem and the results obtained with different processor configurations are reported. A brief discussion about the properties and current limitations of our approach is finally presented.Facultad de Informátic",,A parallel implementation of Q-learning based on communication with cache,,,,core
323942903,2006-06-30T00:00:00,"针对工业现场对高可靠性低压配电系统的迫切需求,开发出基于现场总线与组态技术的工业配电监控系统.该系统主要采用现场总线技术(FCS)、MCGS组态技术,借助现场AI系列仪表,同时结合单片机、PLC控制技术研发而成.自行开发了应用软件在触摸屏上实时控制工业现场设备、测控AI仪表以读取设备运行参数,从而对用电设备进行监控、报警与保护,实现工业现场的分散化和智能化管理,提高了工业控制的自动化水平.According to the requirement of high reliability in the industrial low tension field,an industrial electricity allocating and monitoring system is developed.This system which based on FCS technology and MCGS configuration technology,is a real-time touch-screen monitor system,and it has an on-the-spot AI instrument and combined with the control technology of PLC and SCM.Using the MCGS software,It can protect and monitor the electrical device through real-time controlling and monitoring every electrical device and AI instrument,showing varied equipment parameters on touch-sreen.Thus to achieve the intelligent management in industrial production lines.It can not only improve the automation level in industrial production control,but also accomplish separate and intelligent management in industrial control field.福建省科技厅科技三项费用资助项目(2003H087",,A Design of the Industrial Electricity Allocation Monitor System Based on Fieldbus Technology and Configuration Technology,,,"[{'title': None, 'identifiers': ['issn:1007-7405', '1007-7405']}]",core
196676964,1997,"Proceeding of: International Work-Conference on Artificial and Natural Neural Networks, IWANN'97 Lanzarote, Canary Islands, Spain, June 4–6, 1997Power plant management relies on monitoring many signals that represent the technical parameters of the real plant. The use of neural networks (NN) is a novel approach that can help to produce decisions when integrated in a more general system. In this paper we introduce a NN module using an ART-MAP to discriminate different situations from the plant in order to prevent future malfunctions. A special process to generate of a complete training set has been designed. This process is developed in order to deal with the absence of data in abnormal plant situations. This module belongs to a more general system for predictive maintenance that has been implemented and incorporated in an hydroelectric plant.
This project has been supported by CDTI (belonging to Spanish Industry Ministery) and EEC (European Economic Comunity), reference number: PASO PC067",Springer,Unsupervised neural network for forecasting alarms in hydroelectric power plant,,10.1007/BFb0032590,,core
216357522,2006-01-01T00:00:00,"Este trabalho teve por objetivo utilizar redes neurais artificiais para explicar fenômenos que ocorrem em matadouros- frigoríficos avícolas. A estatística descritiva e a diferença entre as variáveis foram calculadas através do programa computacional SPSS for Windows 10.0 e para a construção das redes neurais artificiais foi utilizado o programa Neuroshell Predictor desenvolvido pela Ward Systems Group. Foram utilizados dados de 2004 a 2006 de dois matadouros- frigoríficos de aves e ocasionalmente um matadouro- frigorífico de suínos. Nos arquivos oferecidos pelas empresas, haviam dados de gerenciamento de matadouros-frigoríficos, como por exemplo, planilhas de condenação da inspeção, dados sobre chiller, teste de absorção das carcaças, dados sobre peso de carcaças, entre outros. Para a construção dos modelos foram escolhidas as “entradas”, para o cálculo do modelo preditivo, e a variável de “saída” a ser predita. Foram gerados oito (8) modelos com diferentes bancos de dados.  Todos esses modelos apresentaram redes neurais artificiais bem ajustadas, com valores altos para Correlação e Coeficiente de Determinação Múltipla (R²) e valores baixos para o Erro Médio e o Quadrado médio do Erro (QME). Não houve diferenças significativas entre os valores reais e os valores preditos em todas as validações dos oito (8) modelos. As redes neurais artificiais, com o software utilizado, foram capazes de explicar os fenômenos que envolvem o gerenciamento de matadouros-frigoríficos de aves, e ocasionalmente de suínos. A técnica utilizada oferece critérios objetivos, cientificamente desenvolvida, que será de grande valia no auxílio do técnico responsável pela tomada de decisões. A técnica permite também realizar simulações e mensurar a contribuição de diferentes variáveis que influenciam no fenômeno. É importante ressaltar que a utilização das redes neurais artificiais é uma ferramenta de auxílio à tomada de decisões, e não um programa que substitua o conhecimento científico e técnico.This work aimed to use artificial neural networks to explain the occurred phenomena in the poultry slaughterhouse. The descriptive statistics and the difference among the variable averages from the initial data were calculated with SPSS software, and the software used to build the artificial neural networks was Neruoshell Predictor, developed by Ward Systems Group. Data from three poultry and one swine slaughterhouses, gathered from 2004 until 2006, were used in this study. In these data there were information about carcasses condemnation, chillers, absorption tests, carcasses weights, medications and others. To build the neural networks,the chosen variables were identified as “input” and “output”. The “input” variables were selected for the predictive model calculation and the “output” variable for the one to be predicted. It was made 8 models with different databases. In all these models, the generated artifical neural networks were well adjusted always presenting a high Multiple Deteminant Coefficient (R²) and correlation and the lowest Mean Squared Error (QME) and mean error.  Also the differences between the real value and the predicted value in the 8 models studied were not statistically significant.This study concludes that artifical neural network, with the used software, were capable to explain the phenomena involved in the poultry slaughterhouse and that the modeling can also be extended to swine slaughterhouse. This powerfull technique offers objective criteria, scientifically generated, which can be used to assist in the decisions analysis process for this industry. It also allows to make simulations and to measure the contribution of each variable in the phenomena studied. It’s important to point out that artificial neural networks are intruments to assist the technician in decision making. Thus, it is not a program able to replace the cientific and technical knowledge",,Uso de redes neurais artificiais no gerenciamento de matadouros-frigoríficos de aves e suínos no sul do Brasil,,,,core
216551166,2004-01-01T00:00:00,"Humanos virtuais são modelos computacionais de pessoas. Se necessário, podem apresentar uma aparência bastante realista, baseada em princípios fisiológicos e biomecânicos. Além disso, são capazes de comportar-se de forma autônoma e inteligente em ambientes dinâmicos, podendo apresentar até mesmo individualidade e personalidade. Humanos virtuais podem ser utilizados como atores sintéticos. Tais atores têm sido usados em uma série de aplicações com a finalidade de simular a presença de atores reais. A indústria de jogos por computador requer personagens que sejam capazes de reagir apropriadamente a eventos e circunstâncias inesperadas, e até mesmo de alterar o progresso do jogo com seus cursos de ação autônomos. Um modo natural para desenvolver tais personagens prevê o uso de técnicas de inteligência artificial, em particular aquelas relacionadas às áreas de agentes autônomos e sistemas multiagentes. Neste trabalho, propõese o uso do modelo BDI (Belief-Desire-Intention) para modelar agentes cognitivos, com a finalidade de implementar personagens animados. O modelo BDI é uma abordagem bastante conhecida e bem sucedida para o desenvolvimento de agentes autônomos em sistemas multiagentes. Trata-se de uma arquitetura poderosa para sistemas dinâmicos e complexos, nos quais agentes podem precisar agir sob informação incompleta e incorreta sobre o seu ambiente e os outros habitantes. Esta dissertação reúne um modelo articulado para animação de personagens, o qual requer a especificação de movimento em cada junta individualmente, e um interpretador para AgentSpeak(L), uma linguagem de programação orientada a agentes que implementa a arquitetura BDI. Foi desenvolvida uma interface que permite que o sistema de raciocínio de um agente, baseado em BDI, seja usado para dirigir o comportamento de um personagem em um sistema de animação.  O uso de AgentSpeak(L) é uma abordagem promissora para a especificação em alto nível de animações complexas por computador. O modelo conceitual e sua implementação são apresentados em capítulos distintos. Esta separação visa simplificar a compreensão do modelo proposto, permitindo primeiro analisá-lo em um nível mais alto de abstração, para então verificar detalhes de programação. Este trabalho apresenta também duas animações 3D, usadas para ilustrar a abordagem proposta. A principal animação apresentada envolve um agente situado em um ambiente dinâmico; o agente continuamente percebe o ambiente e raciocina para determinar como agir sobre ele, baseado em seu estado mental BDI. A outra aplicação é bastante simples, mas útil para mostrar algumas questões que são relevantes para obter-se mais eficiência em programas AgentSpeak(L).Virtual humans are computational models of people. If necessary, they can portray a very realistic appearance, based on biomechanical and physiological principles. Besides, they are able to behave in an autonomous and intelligent way in dynamic environments, and even to exhibit individuality and personality. Virtual humans can be used as synthetic actors. Such kind of actors have been used in several applications, such as games, in order to simulate the presence of real actors. The computer-game industry requires characters that are able to react appropriately to unexpected events and circumstances, and even to change the game progress with their autonomous courses of actions. A natural way for developing such characters is by the use of artificial intelligence techniques, in particular those related to the areas of autonomous agents and multi-agent systems. In this work, the use of the Belief-Desire-Intention (BDI) model for cognitive agents in order to implement animated characters is proposed. The BDI model is a well-known and successful approach for the development of autonomous agents in multiagent systems. It is a very powerful architecture for dynamic and complex systems where agents may need to act under incomplete and incorrect information on other agents and their environment. This work brings together an articulated model for character animation, which requires the specification of motion on each joint individually, and an interpreter for AgentSpeak(L), an agent-oriented programming language that implements the BDI architecture. I have developed an interface that allows the BDI-based agent reasoning system to be used for guiding the behaviour of a character in an animation system. The use of AgentSpeak(L) is a promising approach for the high-level specification of complex computer animations. The conceptual model and its implementation are presented in distinct chapters.  This separation aims at simplifying the comprehension of the proposed model, allowing its analysis first at a higher abstraction level, and after that to check programming details. This work also presents two 3-D animations used to illustrate the proposed approach. The main animation presented involves an agent that is situated in a dynamic environment; the agent continuously perceives the environment and reasons on how to act upon it based on its BDI mental state. The other application is quite simple, but useful to show some issues that are relevant for obtaining better performance from AgentSpeak(L) programs",,Uma arquitetura para animar agentes autônomos em ambientes virtuais usando o modelo BDI,,,,core
197928930,2005-04-04T00:00:00,"International audienceFlight simulators have been part of aviation history since its beginning. With the development of modern aeronautics industry, flight simulators have gained an important place and the industry devoted to their manufacture has become significant. In the case of transportation aircraft, accurate mathematical models based on extensive experimental data have been developed by their manufacturers to optimize their aerodynamic and propulsive characteristics and to design efficient flight control systems. However, in the case of small general aviation aircraft this kind of knowledge is not commonly available and the design of accurate flight simulators can result in a tedious try and modify process until the simulator presents a qualitative behaviour close to the one of the real aircraft. This communication proposes through the use of neural networks a method to perform a direct estimation of the aerodynamic forces acting on aircraft. Artificial neural networks appear to be an appropriate numerical technique to achieve the mapping of these continuous relationships and detailed aerodynamics and thrust models should become no more mandatory to produce accurate flight simulation software",'Institute of Electrical and Electronics Engineers (IEEE)',A neural approach for fast simulation of flight mechanics,,10.1109/ANSS.2005.8,,core
101853796,2004,"Abstract: Changes in the natural environment affect our quality of life. Thus, government, industry, and the public call for integrated environmental management systems capable of supplying all parties with validated, accurate and timely information. The ‘near real-time ’ constraint reveals two critical problems in delivering such tasks: the low quality or absence of data, and the changing conditions over a long period. These problems are common in environmental monitoring networks and although harmless for off-line studies, they may be serious for near real-time systems. In this work, we discuss the problem space of near real-time reporting Environmental Management Systems and present a methodology for applying agent technology this area. The proposed methodology applies powerful tools from the IT sector, such as software agents and machine learning, and identifies the potential use for solving real-world problems. An experimental agent-based prototype developed for monitoring and assessing air-quality in near real time is presented. A community of software agents is assigned to monitor and validate measurements coming from several sensors, to assess air-quality, and, finally, to deliver air quality indicators and alarms to appropriate recipients, when needed, over the web. The architecture of the developed system is presented and the deployment of a real-world test case is demonstrated",,Applying agent technology in environmental management systems under real-time constraints,,,,core
108585329,1999,"Abstract—This paper describes the real-time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling. MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementa-tions toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach. Index Terms—Digital signal processor (DSP), Markov random field (MRF), motion detection, real-time implementation. I",,Real-time DSP implementation for MRF-based video motion detection,,,,core
286563964,2003-01-01T00:00:00,"This paper shows an inferential sensor that has been developed to be used in the olive oil industry. This sensor has
been designed to measure two variables that appear in the
elaboration of olive oil in a mill which are very difficult to
be measured on line by a physical sensor. The knowledge
of these variables on line is crucial for the optimal operation of the process, since they provide the state of the
plant, allowing the development of a control strategy that
can improve the quality and yield of the product. This
sensor measures variables that in other case should come
form laboratory analysis with large processing delays or
from very expensive and difficult to use on line analysers.
The sensor has been devised based upon artificial Neural
Networks (NN) and has been implemented as a routine
running on a Programmable Logic Controller (PLC) and
successfully tested on a real plant.Ministerio de Ciencia y Tecnología DPI2001-2380-C02-0",,Inferential sensor for the olive oil industry,https://core.ac.uk/download/286563964.pdf,,,core
11783306,2002-01-01T00:00:00,"Fault detection and diagnosis are very much needed in many industrial applications. One of the most popular scheme is the model-based fault diagnostic. Recently, artificial intelligence techniques have been found to be suitable for fault detection and diagnosis and a variety of techniques have been proposed in this area. However, reported applications or real time implementation of the schemes are still very few. In this paper, we use a fault detection and diagnostic scheme based on the model-based approach using parameter estimation and Fuzzy inferencing and experimented it on a d.c. motor servo trainer. The model of the plant is obtained using recursive least squares parameter estimation technique and fuzzy inferencing is used for the interpretation of the fault. Several faults have been identified on the system. The faults are then simulated on the motor and experiments are carried out to diagnose the types of faults. The experiments have shown the proposed technique is viable for real-time application",,Application of a model-based fault detection and diagnosis using parameter estimation and fuzzy inference to a DC-servomotor,,,,core
24507214,2004,"Geographic Information Systems (GIS) and Computer Aided Facility Management-Systems (CAFM) are currently undergoing the transition to storing and processing real 3D geospatial data. Applications for this type of data are, among others, location based services, navigation systems and the planning of large-scale construction projects. For presentation purposes and especially when working in the field, powerful visualisation systems are needed that are also capable of running on mobile devices like notebooks, personal digital assistants (PDA) or even cell phones. In such application areas, the free movement of the viewer’s position and the interaction with the data are of great importance. Real-time visualisation of 3D geospatial data is already well established and also commercially successful in the entertainment industry, namely in the market of 3D video games. The development of software in this field is very cost-intensive, so that the packages are often used for several game products and are therefore universally applicable to a certain extend. These so-called game engines include not only visualisation functionality, but also offer physics, sound, network, artificial intelligence and graphical user interfaces to handle user in- and output. As certain portions or sometimes even the whole engine are released as open source software, these engines can be extended to build more serious applications at very little costs. The paper shows how these game engines can be used to create interactive 3D applications that present texture-mapped geospatial data. The integration of 3D data into such systems is discussed. Functionality like thematic queries can be implemented by extending the internal data structures and by modification of the game’s accompanying dynamic link libraries. 1",,Visualisation Using Game Engines,,,,core
240133312,2004-01-01T00:00:00,"The main contribution of this Dissertation is the development of new learning and convergence methodologies for Fuzzy Cognitive Maps that are proposed for the improvement and adaptation of their behaviour, as well as for the increase of their performance, electing them in effective dynamic systems of modelling. The new improved Fuzzy Cognitive Maps, via the learning and adaptation of their weights, have been used in medicine for diagnosis and decision-making, as well as to alleviate the problem of the potential uncontrollable convergence to undesired states in models of industrial process control systems, with very satisfactory results. In this Dissertation are presented, validated and implemented two new learning algorithms without supervision for Fuzzy Cognitive Maps, the algorithms Active Hebbian Learning (AHL) and Nonlinear Hebbian Learning (NHL), based on the classic unsupervised Hebb-type learning algorithm of neural networks, as well as a new approach of learning for Fuzzy Cognitive Maps based on the evolutionary algorithms and more specifically on the algorithm of Particles Swarm Optimization and on the Differential Evolution algorithm. The proposed algorithms AHL and NHL support new learning methodologies for FCMs that improve their operation, efficiency and reliability, and that provide in the experts of each problem that develop the FCM, the learning of parameters for the regulation (fine-tuning) of cause-effect relationships (weights) between the concepts. These types of learning that are accompanied with the right knowledge of each problem-system, contribute in the increase of performance of FCMs and extend their use. Additionally to the unsupervised learning algorithms of Hebb-type for the FCMs, are developed and proposed new learning techniques of FCMs based on the evolutionary algorithms. More specifically, it is proposed a new learning methodology for the application of evolutionary algorithm of Particle Swarm Optimisation in the adaptation of FCMs and more concretely in the determination of the optimal regions of weight values of FCMs. With this method it is taken into consideration the experts’ knowledge for the modelling with the form of restrictions in the concepts that interest us their values, and are defined as output concepts, and for weights are received the arithmetic values of the fuzzy regions that have agreed all the experts. Thus considering restrictions in all weights and in the output concepts and determining a suitable objective function for each problem, result appropriate weight matrices that can lead the system to desirable regions of operation and simultaneously satisfy specific conditions of problem. The first two proposed methods of unsupervised learning that have been suggested for the FCMs are used and applied with success in two complicated problems in medicine, in the problem of decision-making in the radiotherapy process and in the problem of tumor characterization for urinary bladder in real clinical cases. Also all the proposed algorithms are applied in models of industrial systems that concern the control of processes with very satisfactory results. These algorithms, as it results from their application in concrete problems, improve the model of FCMs, they contribute in more intelligent systems and they extend their possibility of application in real and complex problems. The main contribution of the present Dissertation is to develop new learning and convergence methodologies for Fuzzy Cognitive Maps proposing two new unsupervised learning algorithms, the algorithm Active Hebbian Learning and the algorithm Nonlinear Hebbian Learning for the adaptation of weights of the interconnections between the concepts of Fuzzy Cognitive Maps, as well as Evolutionary Algorithms optimizing concrete objective functions for each examined problem. New improved Fuzzy Cognitive Maps via the algorithms of weight adaptation have been used for the development of an Integrated Two-level hierarchical System for the support of decision-making in the radiotherapy, for the development of a new diagnostic tool for tumour characterization of urinary bladder, as well as for the solution of industrial process control problems.Αντικείµενο της διατριβής είναι η ανάπτυξη νέων µεθοδολογιών εκµάθησης και σύγκλισης των Ασαφών Γνωστικών ∆ικτύων που προτείνονται για τη βελτίωση και προσαρµογή της συµπεριφοράς τους, καθώς και για την αύξηση της απόδοσής τους, αναδεικνύοντάς τα σε αποτελεσµατικά δυναµικά συστήµατα µοντελοποίησης. Τα νέα βελτιωµένα Ασαφή Γνωστικά ∆ίκτυα, µέσω της εκµάθησης και προσαρµογής των βαρών τους, έχουν χρησιµοποιηθεί στην ιατρική σε θέµατα διάγνωσης και υποστήριξης στη λήψη απόφασης, καθώς και σε µοντέλα βιοµηχανικών συστηµάτων που αφορούν τον έλεγχο διαδικασιών, µε πολύ ικανοποιητικά αποτελέσµατα. Στη διατριβή αυτή παρουσιάζονται, αξιολογούνται και εφαρµόζονται δύο νέοι αλγόριθµοι εκµάθησης χωρίς επίβλεψη των Ασαφών Γνωστικών ∆ικτύων, οι αλγόριθµοι Active Hebbian Learning (AHL) και Nonlinear Hebbian Learning (NHL), βασισµένοι στον κλασσικό αλγόριθµό εκµάθησης χωρίς επίβλεψη τύπου Hebb των νευρωνικών δικτύων, καθώς και µια νέα προσέγγιση εκµάθησης των Ασαφών Γνωστικών ∆ικτύων βασισµένη στους εξελικτικούς αλγορίθµους και πιο συγκεκριµένα στον αλγόριθµο Βελτιστοποίησης µε Σµήνος Σωµατιδίων και στον ∆ιαφοροεξελικτικό αλγόριθµο. Οι προτεινόµενοι αλγόριθµοι AHL και NHL στηρίζουν νέες µεθοδολογίες εκµάθησης για τα ΑΓ∆ που βελτιώνουν τη λειτουργία, και την αξιοπιστία τους, και που παρέχουν στους εµπειρογνώµονες του εκάστοτε προβλήµατος που αναπτύσσουν το ΑΓ∆, την εκµάθηση των παραµέτρων για τη ρύθµιση των αιτιατών διασυνδέσεων µεταξύ των κόµβων. Αυτοί οι τύποι εκµάθησης που συνοδεύονται από την σωστή γνώση του εκάστοτε προβλήµατος-συστήµατος, συµβάλλουν στην αύξηση της απόδοσης των ΑΓ∆ και διευρύνουν τη χρήση τους. Επιπρόσθετα µε τους αλγορίθµους εκµάθησης χωρίς επίβλεψη τύπου Hebb για τα ΑΓ∆, αναπτύσσονται και προτείνονται νέες τεχνικές εκµάθησης των ΑΓ∆ βασισµένες στους εξελικτικούς αλγορίθµους. Πιο συγκεκριµένα, προτείνεται µια νέα µεθοδολογία για την εφαρµογή του εξελικτικού αλγορίθµου Βελτιστοποίησης µε Σµήνος Σωµατιδίων στην εκµάθηση των Ασαφών Γνωστικών ∆ικτύων και πιο συγκεκριµένα στον καθορισµό των βέλτιστων περιοχών τιµών των βαρών των Ασαφών Γνωστικών ∆ικτύων. Με τη µεθοδο αυτή λαµβάνεται υπόψη η γνώση των εµπειρογνωµόνων για τον σχεδιασµό του µοντέλου µε τη µορφή περιορισµών στους κόµβους που µας ενδιαφέρουν οι τιµές των καταστάσεών τους, που έχουν οριστοί ως κόµβοι έξοδοι του συστήµατος, και για τα βάρη λαµβάνονται υπόψη οι περιοχές των ασαφών συνόλων που έχουν συµφωνήσει όλοι οι εµπειρογνώµονες. Έτσι θέτoντας περιορισµούς σε όλα τα βάρη και στους κόµβους εξόδου και καθορίζοντας µια κατάλληλη αντικειµενική συνάρτηση για το εκάστοτε πρόβληµα, προκύπτουν κατάλληλοι πίνακες βαρών (appropriate weight matrices) που µπορούν να οδηγήσουν το σύστηµα σε επιθυµητές περιοχές λειτουργίας και ταυτόχρονα να ικανοποιούν τις ειδικές συνθήκες- περιορισµούς του προβλήµατος. Οι δύο νέες µέθοδοι εκµάθησης χωρίς επίβλεψη που έχουν προταθεί για τα ΑΓ∆ χρησιµοποιούνται και εφαρµόζονται µε επιτυχία σε δυο πολύπλοκα προβλήµατα από το χώρο της ιατρικής, στο πρόβληµα λήψης απόφασης στην ακτινοθεραπεία και στο πρόβληµα κατηγοριοποίησης των καρκινικών όγκων της ουροδόχου κύστης σε πραγµατικές κλινικές περιπτώσεις. Επίσης όλοι οι προτεινόµενοι αλγόριθµοι εφαρµόζονται σε µοντέλα βιοµηχανικών συστηµάτων που αφορούν τον έλεγχο διαδικασιών µε πολύ ικανοποιητικά αποτελέσµατα. Οι αλγόριθµοι αυτοί, όπως προκύπτει από την εφαρµογή τους σε συγκεκριµένα προβλήµατα, βελτιώνουν το µοντέλο του ΑΓ∆, συµβάλλουν σε ευφυέστερα συστήµατα και διευρύνουν τη δυνατότητα εφαρµογής τους σε πραγµατικά και πολύπλοκα προβλήµατα. Η κύρια συνεισφορά αυτής της διατριβής είναι η ανάπτυξη νέων µεθοδολογιών εκµάθησης και σύγκλισης των Ασαφών Γνωστικών ∆ικτύων προτείνοντας δυο νέους αλγορίθµους µη επιβλεπόµενης µάθησης τύπου Hebb, τον αλγόριθµο Active Hebbian Learning και τον αλγόριθµο Nonlinear Hebbian Learning για την προσαρµογή των βαρών των διασυνδέσεων µεταξύ των κόµβων των Ασαφών Γνωστικών ∆ικτύων, καθώς και εξελικτικούς αλγορίθµους βελτιστοποιώντας συγκεκριµένες αντικειµενικές συναρτήσεις για κάθε εξεταζόµενο πρόβληµα. Τα νέα βελτιωµένα Ασαφή Γνωστικά ∆ίκτυα µέσω των αλγορίθµων προσαρµογής των βαρών τους έχουν χρησιµοποιηθεί για την ανάπτυξη ενός ∆ιεπίπεδου Ιεραρχικού Συστήµατος για την υποστήριξη λήψης απόφασης στην ακτινοθεραπεία, για την ανάπτυξη ενός διαγνωστικού εργαλείου για την κατηγοριοποίηση του βαθµού κακοήθειας των καρκινικών όγκων της ουροδόχου κύστης, καθώς και για την επίλυση βιοµηχανικών προβληµάτων για τον έλεγχο διαδικασιών",Πανεπιστήμιο Πατρών,New learning techniques to train fuzzy cognitive maps and applications in medicine and industry,,,,core
323902917,2004-08-01T00:00:00,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy. © 2003 Elsevier Ltd. All rights reserved",'Elsevier BV',Modelling SO2 concentration at a point with statistical approaches,,10.1016/j.envsoft.2003.10.003,,core
53791904,1997-01-01T00:00:00,"This volume contains selected papers from WIRN VIETRI-97, the 9th Italian Workshop on Neural Nets, held Vietri sul Mare, Salerno, Italy, from 22-24 May 1997. The papers cover a variety of topics related to neural networks, including pattern recognition, signal processing, theoretical models, applications in science and industry, virtual reality, fuzzy systems, and software algorithms. By providing the reader with a comprehensive overview of recent research work in this area, the volume makes an invaluable contribution to the Perspectives in Neural Computing Series. Neural Nets - WIRN VIETRI-97 will provide invaluable reading material for anyone who needs to keep up to date with the latest developments in neural networks and related areas. It will be of particular interest to academic and industrial researchers, and postgraduate and graduate students","Springer-Verlag New York, LLC","Neural Nets, Wirn Vietri-'97: Proceedings of the 9th Italian Workshop On Neural Nets",,,,core
62717057,2004-03-08T00:00:00,"This article presents a comparison of artificial neural networks andneuro-fuzzy systems appliedfor modelling andcontrolling a
real system. The main objective is to model and control the temperature inside of a kiln for the ceramic industry. The details of all
system components are described. The steps taken to arrive at the direct and inverse models using the two architectures: adaptive
neuro fuzzy inference system and feedforward neural networks are described and compared. Finally, real-time control results using internal model control strategy are resented.
Using available Matlab software for both algorithms, the objective is to show the implementation steps for modelling and controlling a real system. Finally, the performances of the two solutions were comparedthrough different parameters for a specific real didactic cas",'Elsevier BV',Artificial neural networks and neuro-fuzzy systems for modelling and controlling real systems: a comparative study,,10.1016/j.engappai.2004.03.001,"[{'title': 'Engineering Applications of Artificial Intelligence', 'identifiers': ['issn:0952-1976', '0952-1976']}]",core
298894910,2003-07-27,"The enzyme penicillin G acylase (E.C.3.5.1.11) is used in the production of 6-aminopenicillanic acid (6-APA) and 7-aminocephalosporinic acid (7-ACA), which are key intermediates for the production of &#946;-lactam antibiotics. Its industrial importance was one of the motivations for this thesis. On-line measurement and monitoring of cultivations of Bacillus megaterium ATCC 14945 in a agitated and aerated bioreactor allowed the acquisition of variables such as mol fraction of CO2 and O2 in the exhaust
gases, pH, dissolved oxygen. Batch and fed-batch runs, using either enzymatically hydrolyzed casein or a pool of free amino acids provided information concerning the extend of the cultivation, preservation of the microorganism and addition/exclusion of nutrients. Usual carbon
sources as glucose, fructose and glycerol increased the cellular mass, but did not improve the productivity of PGA. The use of amino acids resulted in a 2.5-fold increase of productivity. Adding phenyl acetic acid at the beginning of the experiments did not inhibit cell growth.
Three non-structured models of microbial growth were put forth, assuming one, two and three limiting substrates. However, these models were too simple for our purposes. Another approach was then followed, using neural networks (NN) as
softsensors. Before implementing the inference algorithm, the blank noise of the instrumentation had to be reduced. A non-conventional filter was developed, combining
a recursive NN, a moving average and a second recursive NN. The smoothed variables were the input for a second NN, for pattern recognition, which classified the run in one of the main growth phases: lag, exponential and stationary. The main purpose of this NN was to identify the exponential phase, which would be the domain of the next NN, a multilayer perceptron (MLP) for inference of the cellular mass. Several NN, with
different topologies, were tested for this purpose. Finally, the product concentration (PGA activity in the medium) was estimated through a hybrid approach, using the growth rate inferred by the MLP NN, coupled to the cell-product yield, obtained from the fitting of the non-structured models. Another important information for this last algorithm was the knowledge that production of PGA was growth-associated, but with a 2hr-delay. The algorithm for inference was robust and accurate.Universidade Federal de Minas GeraisA grande importância da enzima penicilina G acilase (E.C.3.5.1.11), usada para a produção dos ácidos 6-aminopenicilânico (6-APA) e 7-aminocefalosporânico (7-ACA), compostos-chave na produção industrial de antibióticos &#946;-lactâmicos, foi a principal motivação deste trabalho. Através de realização de experimentos de produção
de PGA por Bacillus megaterium ATCC 14945 em reator convencional, utilizando sistema de aquisição de dados, foi possível monitorar em tempo real variáveis como, por exemplo, fração molar de oxigênio e dióxido de carbono nos gases de saída, pH, oxigênio dissolvido. Foram realizados cultivos em batelada e batelada alimentada tendo como
substratos limitantes tanto caseína hidrolisada enzimaticamente como aminoácidos livres. Obtiveram-se informações relacionadas ao tempo de cultivo, à estocagem do microrganismo e à adição e exclusão de nutrientes. Fontes usuais de carbono, como glicose, lactose e glicerol, quando utilizadas, promoviam o crescimento da massa celular sem, no entanto, aumentar a produção de PGA. O uso de aminoácidos como principal
substrato elevou em 2,5 vezes a produtividade. Observou-se, ainda, que a adição de ácido fenilacético (AFA) desde o início do cultivo não inibiu o crescimento do microrganismo.
As informações experimentais permitiram a proposição e validação de três modelos cinéticos não-estruturados deste processo, com um ou mais substrato(s) limitante(s). Como essas propostas se mostraram demasiadamente simplificadas, optouse
pelo uso de redes neurais como sensores baseados em  software , já que não se chegou a um modelo fenomenológico satisfatório. Para implementação do algoritmo de inferência baseado em rede neural,
mostrou-se necessário filtrar os ruídos das medidas provenientes do sistema de aquisição de forma a minimizar erros aleatórios da instrumentação. Propõe-se para isso filtro que utiliza redes recorrentes, combinadas a média móvel. As variáveis filtradas foram utilizadas numa segunda rede de identificação de padrões, que dividia o cultivo nas três principais fases do crescimento microbiano. O principal objetivo desta foi
identificar a fase de crescimento exponencial, que seria a enfocada pelo algoritmo de inferência. Para essa inferência, com base nas variáveis medidas em tempo real, treinaram-se redes com várias topologias e entradas, de modo a escolher as variáveis que possibilitassem o aprendizado dos aspectos essenciais da dinâmica do processo.
Por fim, a concentração de produto (atividade de PGA no meio de cultura) foi estimada através de enfoque híbrido, usando a velocidade de crescimento inferida por rede  feedforward , acoplada ao fator de rendimento célula-produto estimado no ajuste dos modelos não-estruturados. Outra informação importante utilizada neste último
algoritmo foi o fato da produção ser associada ao crescimento, mas com atraso de 2h. Novamente, os resultados quantitativos do algoritmo de inferência do produto foram muito satisfatórios",Programa de Pós-graduação em Engenharia Química,Inferência de variáveis do processo de produção de penicilina G acilase por Bacillus megaterium ATCC-14945.,,,,core
20851133,1997,"this article, we describe and develop methodologies for mod- eling and transferring human control strategy (HCS). This research has potential application in a variety of areas such as the Intelligent Vehicle Highway System (IVHS), human-machine interfacing, real-time training, space telerobotics, and agile manufacturing. We specifically address the following issues: (1) how to efficiently model human control strategy through learning cascade neural networks, (2) how to select state inputs in order to generate reliable models, (3) how to validate the computed models through an independent, Hidden Markov Model-based procedure, and (4) how to effectively transfer human control strategy. We have implemented this approach experimentally in the real-time control of a human driving simulator, and are working to transfer these methodologies for the control of an autonomous vehicle and a mobile robot. In providing a framework for abstracting computational models of human skill, we expect to facilitate analysis of human control, the development of humanlike intelligent machines, improved human-robot coordination, and the transfer of skill from one  human to anothe",,"Human Control Strategy: Abstraction, Verification, and Replication",,10.1109/37.621469,,core
21981134,1995,"Although Artificial Neural Networks (ANNs) have been successfully  applied to a wide variety of complex problems, they rarely exhibit 100%  accuracy on independent Tests. If they are to be employed effectively in  real-world industrial applications, particularly in the safety-critical industries,  their reliability must be assured. This paper reports research  on the use of the software engineering concept of reliability through redundancy  (diversity) to develop reliable systems by combining diverse  ANNs. Four different types of diversity are identified and discussed",,How to Improve the Reliability of Artificial Neural Networks,,,,core
24587631,2000,"Neural networks have been applied within manufacturing domains, in particular electronics industries, to address the inherent complexity, the large number of interacting process features and the lack of robust analytical models of real industrial processes. The ability of neural systems to provide nonlinear mappings between process features and desired outputs has been the major driving force behind implementations. One of the major issues limiting the widespread industrial uptake of neural systems is the lack of detailed understanding of their design, implementation and operation. In many cases, network topologies and training parameters are systematically varied until satisfactory convergence is achieved. There is little discussion of the rationale behind the adopted training methods. A review of research into the functions that can be readily represented by neural networks are presented in this paper. The application focus is the control and monitoring of a discrete manufacturing process that is part of the manufacturing cycle of mixed technology surface mount printed circuit boards. Detailed knowledge of the process operation and functionality that can be represented by simple network topologies have been combined to develop a structured, partially interconnected neural network that provides optimised convergence performance. A comparison of the designed solution with standard approaches to neural network implementation is given. It has been demonstrated that if there is sufficient confidence in the operation of the process, input feature interaction within the network can be constrained to produce a robust control and monitoring system",,DESIGN ISSUES ASSOCIATED WITH NEURAL NETWORK SYSTEMS APPLIED WITHIN THE ELECTRONICS MANUFACTURING DOMAIN,,10.1142/s0960313100000046,,core
102809567,2005,"Abstract—In this paper, we present an automatic system and algorithms for the classification of marble slabs into different groups in real time in production line, according to slabs quality. The application of the system is aimed at the marble industry, in order to automate and improve the manual classification process of marble slabs carried out at present. The system consists of a mechatronic prototype, which houses all the required physical components for the acquisition of marble slabs images in suitable light conditions, and computational algorithms, which are used to analyze the color texture of the marble surfaces and classify them into their corresponding group. In order to evaluate the color representation influence on the image analysis, four color spaces have been tested: RGB, XYZ, YIQ, and K-L. After the texture analysis performed with the sum and difference histograms algo-rithm, a feature extraction process has been implemented with principal component analysis. Finally, a multilayer perceptron neural network trained with the backpropagation algorithm with adaptive learning rate is used to classify the marble slabs in three categories, according to their quality. The results (successful clas-sification rate of 98.9%) show very high performance compared with the traditional (manual) system. Index Terms—Artificial neural networks, marble surfaces, pat-tern classification, principal component analysis, sum and differ-ence histograms, texture analysis. I",,Automatic system for quality-based classification of marble textures,,,,core
216383565,2005-08-17T07:00:00,"In the Robotics industry, it is a frequent requirement that robots operate in real-time. The usual approach to this issue involves creating robots driven entirely by direct environmental input rather than complicated planning and decision-making AI. This approach means that the current state of the robot in relation to its environment exclusively determines the actions of the robot. In the simplest terms, this approach creates a Finite State Machine (FSM). Clearly, a standard FSM is completely pre-deterministic upon its creation. This is a drawback which immediately disallows the robot to cope with dynamic environments in an autonomous manner. This research suggests a solution to this problem, while still maintaining real-time performance of the FSM structure, through the development of a Self-Adjusting FSM (SA-FSM). A SA-FSM is a FSM with an additional module which adds, removes, and adjusts specific states of its FSM structure. By adjusting its FSM the SA-FSM will have the basis for autonomous attributes. It will be capable of coping with drastic changes in its environment by making necessary fundamental adjustments to its behavior. Through this mechanism, the process of learning can be implemented. In this regard, only the inherent learning/inference algorithms the SA-FSM employs to adjust its FSM determine the complexity of the behavior produced by a SA-FSM based robot",Digital Commons @ Trinity,Self-Adjusting Finite State Machines: an approach to Real-Time Autonomous Behavior in Robots,https://core.ac.uk/download/216383565.pdf,,,core
235571144,1995-08-23T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.rMAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
30 cents
WEDNESDAY August 23,1995
SOUTHCHIEF ALL STARS
3RD IN CANADA!!!
The Southchief All Stars Baseball Team came
away from the Canadian Little League
Championships in Perth, Ontario tied for 3rd
place. Ontario, represented by High Park of
Toronto captured the championship with a 5-1
victory over Glace Bay, Nova Scotia, in the
final. Southchief beat the B.C. & Quebec
champions but lost to Ontario & Glace Bay in
their round-robin games.
In addition to their tournament games the
boys were treated to an Ottawa Lynx
Professional game, a tour of Ottawa, a players
banquet featuring Ron Piche of the Montreal
Expos & numerous other activities. Players
from Magrath included: Adam Mehew,
Chris Shaw, Matt Harris, Booker Alston, Dylan
Alston, Brooks Blackmer & Jimmy Balderson.
Coaches are Ross Blackmer & Wes Balderson.
Many Thanks to those in Magrath who
supported the team financially and otherwise.
3>c ft A A A A
! ¡ATTENTION!!
Junior High School Boys
Football starts Monday, August 28th, at 6:00
pm. Be at the football field at 5:30 to register.
Equipment will be handed out that night. Our
1st game is Sept. 9th. See you there.
A * Jc * & A
Being broke isn't all that bad: at least it gives
you something to think about while watching
television.
SOUTHERN ALBERTA'S TOP TALENT
converge at the BROADWAY THEATRE
this summer.
1940's Radio Hour August 25th: relive the
hayday of live radio! Take an intimate look at
the workings of a broadcast station & a
glimpse of the performers who risked it all in
song & drama.
Moments from Stage & Screen
Aug. 26th: catch a ride on a musical tour of the
hits of Broadway. T.V. & the Big Screen.
Enjoy the songs that have made musical
history. This live production features Jennifer
Ing, Audra Foggin, Olivia & Sterling McClung
performing.
Tickets available at the door or
by calling 752-4717.
A A A A A A
‘A
ANNUAL SENIORS CARNIVAL
Please mark your calendar for
Sept. 22, Friday, to be held in the Arena.
Kids Games, Fish Pond, Face Painting,
White Elephant Table, Craft Tables,
Bake Table, Pop Corn Balls, Door Prize,
Hot Dog Stand & Hot Scones, will be for sale
also. Time will be announced later - Mark
your calendar now
Friday Sept. 22.
LIBRARY NEWS:
Are you aware that you can obtain any book
you would like that is available in over 20
different libraries?; Although each week we
receive new books for our own library, perhaps
we have not ordered a particular book that you
are interested in. Come in & find the book you
want on the computer & we can have it here
for you in a week.
SUMMER READING PROGRAM
This week we will be winding up our Summer
Reading Program with a PARTY to be held at
the Arena Thur. Aug. 24th from 1-2 pm.
Please bring your purple log books! There will
be lots of FUN, PRIZES & TREATS. So come
& bring a friend if you like! We will also be
making draws for the ""WORLDS"" balloons.
See you there !!!
HIGH SCHOOLERS:
The summer reading program will end Aug. 25.
If you haven't tried for a prize yet, you can
still add to our on-going story; draw a bat cave
or make a pop bottle villain suitable for a Bat
Man Story(there are no entries in that'as yet)
We have a Bat Man T-- shirt, some gift
certificates, a movie poster & several other
prizes. It's not quite too late! Hurry in!!
A A A A A A
BINGO - BINGO - BINGO - BINGO
All members of the community are welcome to
play Bingo at the Seniors' Centre every second
Thursday. Next Bingo Aug. 31st, 1995. )
BETTY STARKE 758-3339
COUNTRY LANE SALON
Family Hair Cuts
Back to School Specials
Tanning Packages
A*A AÀ A
The Sign Of An
Exceptional Sales
Associate.
YOU CAN COUNTON?
(403) 752-3449 OR 327-5152 BUS.
(4Ö3) 752-3878 FAX, 758-3551 RES
JACK ELLIOTT. BSc., MA.
S«!*t Associate
COLDWELL BANKER
HANCOCK LAND
REAL ESTATE
52, BROADWAY N.
RAYMOND, AB TOK 2S0
cm
Expect the besL*
C 1994 Cold »X11 Bantrr Corpuntx >n
Each Ofiiec I* an IrxJejxrrklenclr arvJ OpcrjtcJ Mrmhwr Bajt« CokiII Bunker
Aftlorn Cxr-jJa
For all your roofing
& siding estimates!
KEN JENSEN
758-3669
MAGRATH,ALBERTA
ENTERPRISES
MEAT SPECIALS August 21 - 26
Sirloin Steak $2.88/lb $6.35/kg
Top Round Roast $2.88/lb $6.35/kg
Pork Sideribs $1.88/lb $4.14/kg
Chicken Legs (back attached) .78/lb $1.72/kg
Chicken Breast (back attached) $2.28/lb $5.03/kg
Kent Bacon 500g $1.98 each
Schneider’s Juicy Jumbo Smokies 450g $3.28 each
Maple Leaf Deli Style Meats 125g $1.98 each
RAYMOND AG SOCIETY FALL FAIR!!!
FAIR; Sat. Aug. 26th, 9:00-5:00
Horse & Cattle Shows, Petting Zoo,
Fair Entries & Winners
Entries should be at Ag Building Friday by
1:00 as judging is at 2:00, For fair books &
info call Mary Heggie at 752-3661.
Diane Cant will be exhibiting & selling crafts in
THE STORE in Magrath. These will be
displayed beside the Greeting Cards in THE
STORE. You are invited to drop in & browse
and also take a look at the craft ideas.
Brian & Rebeccah Kindt(Larsen) are the proud
parents of Emily-Elizabeth, Briannah, born Sat.
Aug. 19th weighing 7 lbs - 6 ozs. Pround
grand parents are Merrill & Judy Larsen. &
George & Rita Kindt of Raymond.
To give away: 3 black & tan coon hound-i-puppies.
Call 758-6434 or 758-3588.
NO-COST CLOTHING DAY: ""THANK YOU""
to all of our community members who
supported our No-Cost Clothing Day. Because
of your volunteer work/donated clothing/getting
clothing, our committee felt that this service
project was a success. Just for your
information, the leftover clothing was donated
to the Lethbridge Food Bank. Start planning
now to participate again next August in the
4th Annual Co-Cost Clothing Day. Thanks
again! From Doris Gruninger, on behalf of the
No-Cost Clothing Day Committee.
******
The immediate families of Arnold and Shirley
Godionton had the pleasure of meeting recently
for a backyard BBQ. Family pictures were
taken, also a 5 generation family photo.
Warren & Stacey Adamson & family from
Ottawa were in attendance. It was wonderful
to all be together again.
******
Successful candidates who performed in the
1995 Summer Session of the Royal
Conservatory of Music Piano examinations:
Tiffany Hatch (gr 5); Jessica Heath (gr 9)
Jillienne Heninger (gr 8) Jodi Johnson (gr 4)
Joan Leishman (gr 3) Karren Leishman (gr 8)
Lisa Leishman (gr 4) Michelle Perry( gr 5Traci
Perry (gr 8)
Congratulations on your achievements!!
PRODUCE SPECIALS August 21 - 26
B.C. Freestone Peaches 20 lb $12.98 each
Strawberries $1.79/lb $3.94/kg
Granny Smith Apples .89/lb $1.96/kg
Tomatoes .59/lb $1.30/kg
Pickling Cucumbers .79/lb $1.74/kg
Lettuce .69 each
Green Peppers .69/lb $1.52/kg
SENIORS NEWS:
Our Aug. Potluck ""Corn Bust"" is waiting for
the corn to ripen. Watch for news of the day it
will be held.
******
SENIORS TRAVEL NEWS:
Our Outing to Waterton to the supper &
program at Olsen's Riverside Campground is
this Sat. Aug. 26th. For those who have signed
up we will leave the Senior's Centre at 4:30 pm
If interested contact one of the travel
committee:
Baker's 758-3207, Doreen Alston 758-3281,
Hazel Rasmussen 758-3545 or
Margaret Leishman at 758-3241
SENIORS GAMES NITE -
Alternate thur. night to the Bingo. Our next
games nite will be Thurs. August 24th. at the
Seniors Centre. ******
NEEDED: ride to U of L in early Sept, will
share expenses. Must be there by 8 am.
Call 758-3712.
******
r « r c Ü!»VlMAT£t
24 HOUR SERVICE (329-0965)
rn.,«« i Stc-«.:- Srr.l S.
Need a Piano Teacher?
I will be teaching again this fall,
starting Sept. 18th
I have grade 9 Royal Conservatory in Piano.
For more information and cost
Please call me at 758-6757
Teejay Lybbert
Clogging Lessons
will be held for anyone 4 yrs. & over starting
the week of Sept. 11th, 1995. To register or for
more information, please call either
Jennifer Sabey @ 758-3327 or
Kourtney Alston @ 758-3627
by August 31, 1995. A.30
Needed 2 guitar players!!
I want to start a band just for jamming & fun.
Leanne Glavin at 758-3295
CHARLES KING would like to contact
individuals who have know his grandfather.
NICK KING. He is interested in stories or
photographs. He can be contacted at 737-3092
after 7 p.m.
• No-drip way to paint the ceiling
Here’s an easy way to keep paint from dripping
down your arms when you’re painting the ceiling:
just put on long rubber gloves and cuff them—
any drips will collect in
the cuff.
! ! ! S ALE ! ! !
MANUFACTURERS' CLEARANCE !
*Hardwood oval & round picture frames - hundreds of frames from small to very large. Finished &
unfinished. Discontinued & imperfect frames at very low prices.
*Straight wood picture frame liner, cloth covered, large quantities in many shapes & colors. BELOW
COST!
*Wood stains & lacquers in various colors in unused can. UNBELIEVABLE LOW PRICED!
*Ribbon - all widths & colors in Satin, Grosgrain, Feather Edge & Patterned. LOW CLEARANCE
PRICES!.
*Children’s Hair Accessories, Hats & Caps, Straw Hats, Sandals, Balloons & Balloon Launchers.
WHERE?? InLine Ovals Plant 277 N. 1st St. W. Magrath.(at the rear of The Blue Goose Petocan)
WHEN?? Friday, August 25, 6 pm to 9 pm
Saturday August 26, 9 am to 3 pm.
HOW?? Cash, Check, Visa & Mastercard.
MAGRATH&D STRICT AG SOCETY
FARMERS MARKET & HORSE SHOW
Saturday, September 9
Magrath Fairgrounds
Farmer's Market - 10 til 12 noon
Bring produce for sale and table
Cash prizes for the longest, largest, fatest,
and most unusual in each category.
Children's games and small animal pet
show - 10 til 12 noon
Concession 11 til 2 pm
Mutton busting and steer riding
Horse show - 1 til 3 pm
Halter showmanship & weanling colt classes
Western Riding, barrel racing, pole bending,
and other fun games.
Wagon Rides tool!
Info - 758-3588 or 752-4111
h. RICK’S
PORTABLE
WELDING
OWNER RICK BERES
WE WELD EVERYTHING FROM
CHAIRS TO TRACTORS
PLEASE CALL ME FOR ALL YOUR
WELDING NEEDS
PRESSURE QUALIFIED
75 6427
CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758-6377
Less than 30 words—$1.07
Small ad (2.5""X3.5"")-$5.35
Dalmation Puppies-lst litter, ready Aug.l9th, 1st shots, excellent temperament, great with kids. Call 758-6894.
******
For Sale: DILL, call Warren Harris 758-3792.
******
For Sale: industrial serger. Union Special, 3-4 spool, in good condition. Asking $1350, phone 626-3697 after
5
pm. A.23
******
Custom Swathing - Call C.W. Swathing at (403)328-9788 Early morning. S.6.
******
Computer for Sale: Macintosh llsi with 5 mg ram & 80 mgHD, extended keyboard, color monitor, CD­Rom drive, 14.4 fax/modem with Internet Starter Kit & software including MacWrite Pro., Print Shop Deluxe, Kid Pix & Companion, Prince of Persia, King's Quest
6
more $999 - Phone Bonny West - 758-3072 #2-external 230 MG hard drive - $250.
#3-Apple Laserwriter 11NT Postcript Laser Printer $550. Call Bonny West-758-3072.
******
For Sale: Boler trailer complete w/kitchen ware- 1400 studio quality exercise bike,
1/4 page---------------------$7.49
1/3 page---------------------$8.56
1/2 page-------------------$10.70
like new, was $1,000, asking $250.
Stepping machine $100.
Pawnee Island double hammock $60, woman's bike $15, Troy built rotortiller was $400, jused once asking $250. Call 758-6260. S.13
******
Wanted: Harvest help­swathing, combining, truck driving. Call Bob 758-6709.
******
Garage Sale
Sat. Aug. 26th 9-12 noon 143N-lSt.E (Kirk Godionton) ******
Hay for Sale Small Square Bales,
It has been put up with no rain. On a year like this you better get good hay early before its all gone.
Rod Foggin 758-6832 A. 30
DON'T THROW THAT KNIFE WAY
II!IGET IT SHARPENED!!!! LOCAL SHARPENING PHONE 758-3207
A. 30
******
Interested in teaching Private Craft Lessons from ages 7 & up.
For more information call
Diane Cant at 758-3257.
Full Page—Copy Ready—$25.00 Full Page—We do---------$37.45
Flyer insertion (your paper)
For Sale: Jim's Carpet & Upholstery Cleaning equipment mounted in a Ford Van. Call 758-6596.
******
For Sale: 1983 Plymouth Carevelle, 4 cylinder, front wheel drive, low mileage, a.c. am-fm cassette, good tires, call 758-6316. A.23
******
For Sale: 1982 Dodge SuperCab w/topper, fully equipped $2300. Call 758-6260. A.30
******
For SAle: 1984 Toyota Van LE, 7 passenger, fully loaded-A-1 condition. Call 758-3705. A.23
******
For Sale: Massey Ferguson 75' pull type combine with a 351 MelRose pick-up (10') 758-6060(day) 758-6725 (eve) ******
For Sale: 1979 Pinto, new motor & trans, good fixer for youth. $200. Call 758-3701.
******
Greyhound Bus converted to a camperized motorhome. 471 Detroit Diezel with stove, fridge, table, seats 10 people, sleeps 10, has water system & sewage tank. Please call 758-6060 (day) 758-6725(eve) *****KINDERGARTEN
There will be a staggered entrance for kindergarten this year. The schedule is as follows:
Sept: 5
A.M.
Sept. 6 P.M.
Sept. 7 A.M.
Connor Barnett Jade Blumei Ashylla Chipman Bethany Dahl John Gruninger Jeanette Gruninger Natalie Wolsey . Nathan Thomson
Sarah Balderson Bonnie Balderson Jeremy Charlesworth Britney Bennett Robert Clifton Steven Scott Zachary Brandham Brandon Fortner Russell Bennett Karissa Bingley
Courtnee Karren Layne Nordquist Kyle Stringam Jimmy Stringam Katie Tollestrup Michelle Baines James Grisack Samyra Alston
Sept. 6
A.M.
Sept. 7 P.M.
Jenna Johnson Kristian Johnson Chase Leavitt Tyanne Tidmarsh Bailey Jensen Bryanna Miller Corinne Jensen Gregg Karren
Wade Bullock
Cody Folkes
Crystal Godsalve Trinda Sheridan Logan Mendenhall Celestina Alston Bobby Xerxz Gemmell Jordaii Zaugg Natäsha Smith
Kindergarten fees are $163.00. These fees are due as your child enters the program unless other arrangements have been made. Thanks, Jackie Barnett 758-3066.
The Magrath Golf Course is holding the Ladies Scramble Golf Tournament on Saturday September 2, 1995, shotgun start,
3 ladies per team. Please call the Pro Shop @ 758-3054
to enter by August 31, 1995.
sfr;******
To give away
2 male kittens
758-3588
!¡Moving Away!!
giving away various items
table, chairs, head board, check out our front lawn & see what's there,!!
Alvina Dunn's place
To Give Away
Kittens, 4 male, 2 female, long-haired white with grey markings - 6 weeks old. Phone 758-3097
Reuses:Sandwich Box, the flat box that holds 4 sticks of margarine side by side makes a crushproff sandwich box.
Bait Cups: when you go fishing, store your live bait in styrofoam cups so it will stay cool and lively.THE CREATIVE OUTLET
Woodworking and Crafts
Jeanine Passey 75A-6530
Monthly Woodcraft Classes
VVV
We are gearing up for another great year of woodcrafts. It will
RUN THE SAME AS LAST YEAR WITH NEW PROJECTS MONTHLY. THERE WILL BE
LOTS OF SEASONAL, COUNTRY AND HOME DECORATING PROJECTS AVAILABLE IN
«SGI*Nt* AND XNTERMEDCATE LEVELS.
|?c jiskAÎwn : Ay 3 b b V
► KITS ARE PRE-ORDERED A MONTH IN ADVANCE
► KITS ARE PRE-PAID WHEN ORDERED
► ALL SUPPLIES NEEDED TO COMPLETE PROJECTS
ARE PROVIDED AT THE WORKSHOP
► ANY NUMBER OF KITS MAY BE ORDERED
► YOU MAY NEED TO REGISTER FOR MORE THAN
ONE WORKSHOP IF COMPLETING SEVERAL
PROJECTS
► A SHOP FEE OF $5.00 IS CHARGED FOR EACH
WORKSHOP REGARDLESS OF THE NUMBER OF
PROJECTS ORDERED
*- ALL KITS CAN BE ORDERED TO BE FINISHED ON
YOUR OWN AND NO SHOP FEE WOULD BE CHARGED
► workshops; run September through April
Bring a frxew and come see what we
or by Appointment
19 LeLhbrldge
Jeanine Passey
!i 758-6530
ARE DOING!
iise^Vbrand..
fit name-#
icftsènW:
saver: Rar. -he air co*dLorer at the
selling wthWtan oo.high '
! compressor-syc/ng at a Minimum,
ose the ¡fettina thàrhaç.ar corririg.out
Nutz & Boltz, America's Automotive Consumer
Newsletter, is available by calling (410)-584-7574.
0
■ Wear golf shoes to mow the lawn. You get a good grip—especially
on hills—while aerating the soil.
t’L If the ketchup (or any other thick liquid) is stuck at the bottom of a
bottle, grab the bottom, make sure you've got an all clear, and swing
your arm in a big circle a few times. Centrifugal force will
bring it to the top.
Si To clean lettuce for a crowd, wash the lettuce leaves,
then drop them into a clean pillowcase. Knot the top
of the pillowcase, toss it in the washing machine and turn
the dial to spin dry
■I,If. smoke in the kitchen sets off your smoke alarm,
get out your plant mister and spray underneath the
alarm. It lowers the temperature and stops the alarm.
W When you fertilize your lawn, mix the fertilizer with
a little flour so you can see exactly which areas
you've already covered.
Cover the bottoms of hanging plants with shower
caps when you water them. Or hang an umbrel­la
underneath. No more drips!
Moms who work
part time are
happier than
moms who work
full time or moms
, who stay home
| with the kids.
For Sale: 1984 Pontiac 6000.
In excellent condition. Kept
well!! Asking $2300. Phone
758-6678 & ask for Dexter or
Jaime. If no answer, please
leave message. A. 23
******
""Custom Haying""Cutting &
round baling
On Cash or Crop Share
Basis
Rick Strate
758-6749
******
SERVICES:
JEANIE’S HAIR FASHION
Professional Haircare
at pleasing prices
Open hours 9 am - 5pm
Mon thru Sat
For appointment call
758-3379
ALL TREE SERVICES
Certified Arborist providing
the following services:
Tree & Hedge Trimming,
Removals, Hauling &
Cartage of unwanted debris
Call for a Free Estimate.
Cam Bruce at 758-3729
******
Wanted: Trees for firewood,
if you are planning to
remove trees from your
property, call me. I want
them for Firewood. Call Cam
Bruce at 758-3729
******
W aterW orks
Window Washing is now
starting. Please call 758-3061
for free estimates & excellent
window
washing service. Ask for
Tom or leave a message.
Aug. 23
******
REAL ESTATE
For Sale by Owner
222W 1 Ave N
1200 sq.ft, bungalow on large
lot. Features include full
basement, 3 bed up, 3 bed
down, 3 bath, large kitchen -
eating area, living room up,
large family/playroom with
fireplace down. Built in
vacuum, built in dishwasher,
fully fenced in backyard with
garden spot, no maintenance
exterior, new shingles, the
extras go on!. For more info
call 758-6344
******
QUALITY BUILT FAMILY
HOME!!
1 yr. old very comfortable
family home,
1330 sq.ft, mainfloor,
completely finished basement,
3 baths, 4 bed. attached
double car garage. Reduced
to $125,000. Assumable
mortgage, low d/p
Call Merrill Larsen Century
21 Chinook Realty.
758-3300 or 380-2100
For Rent: 2 bedroom house
near school & hospital.
Includes stove & fridge. Rent
$350/mos. $300 d.d. Available
Sept.3, abstainers please. Call
Craig Thomson at 758-3312
after 8 pm. A.30.
******
House for Sale - Lethbridge
1280 sq.ft., 3 yr. old town
home. 3 bedrooms, 1 1/2
baths, 2 stories + undev.
bsmnt, mainfloor laundry,
off-street parking, fenced &
landscaped, close to shopping
& bus stop. Call 327-6989.
A.23
******
House for Sale in Spring
Coulee: 1250 sq.ft.,
3 bedrooms, 2 bath,
full basement, well
maintained Call
758-6658 or 758-6721. S.6
* * * * * *
Selling below appraised value
4 bedroom - 2 bathroom
big sundeck
Pretty as a Picture
$89,900
in Magrath
Bungalow Fixer-Upper
approx 20 yrs. old
4 bedrooms, 2 baths,
developed basement, carport.
Needs carpet & paint
$69,900 in Raymond.
Call
Jack Elliott
Coldwell Banker
Hancock Real Estate
753-3449 or 758-3551
One qf the quickest ways to meet new people is to
pick up the wrong ball on a golf course. — The Lioa
MORE GROCERY SPECIALS August 21 - 26
Western Family Coffee Whitener
500g
$2.28 each
Nestea Iced Tea
680 g
$3.48 each
Nabisco Shreddies
675 g
$3.48 each
Sugar
10 kg
$6.98 each
Post Honeycombs
400 g
$3.98 each
Kraft Marshmallows
400 g
.98 each
Kraft Salad Dressing
250 ml
$1.58 each
Heinz White Vinegar
4 liter
$2.98 each
Western Family BBQ Sauce
455 ml
$1.68 each
Value Priced Dog Food
4 kg
$3.48 each
Friskies Cat Food
156 g
2 for .88
Kleenex Ultra Soft Bath Tissue
12 pk
$4.48 each
Western Family Paper Towels
2 pk
.98 each
Western Family Bleach
3.6 liter
$1.68 each
Western Family Soft Margarine
907 g
$1.78 each
Western Family Cheese Spread
500 g
$3.78 each
Black Diamond Cheese
750 g
$6.98 each
Western Family French Fries
1 kg
$1.48 each
McCain Super Pockets or Fizza Pockets
400 g
$2.98 each
Swanson Meat Pies
200 g
.98 each
Minute Maid Orange Juice
355 ml
$1.18 each
Crest Toothpaste
75 ml
.98 each
Bernardin Standard Mason Jars 1 liter $8.98 ea
Kerr Widemouth Pint Jars 473 ml $8.98 ea Kerr Re§ular Half Pint Jars 270 $7‘98 ea
Kerr Widemouth Half Pint Jars 270 ml $8.98 ea Bernardmg Wide Mouth Salmon Jars 250 ml $7.98 ea
Kerr Jelly Jar 270 ml $8.98 ea Rerr Jar______________________355 ml $9.98 ea
Bernardin Standard Mason Jars 500 ml $7.98 ea Rerr Wide Mouth Quart Jars_______ 946 ml $9.98 eaDo you need a reliable babysitter, well balanced meals provided. Large outside play area. Will babysit in my home. All ages welcomed. Shift work can be accommodated.
Phone Thomasina Hatch
at 758-6569. A.23
Piano Theory and Singing Lessons
for all ages.
Register now for Fall Semester by calling
Erica Thomson at 758-6324.
Qualification L.R.S.M.
Over 25 years teaching experience.
S.6
£
CARRIAGE HOUSE THEATRE -Presents- ""THE NET"" «August 25 - 31
Showing at 9:15
’ FREE WILLY”
Showing at 7 pm.
GARDEN CITY REALTY 758-06060 MAGRATH
Will laminate paper goods in my home. Reasonable rates, 27"" wide machine. Posters, charts, games, decorations, learning resources, visual aides, recipes, art, any paper you want preserved.
Phone Sharon 758-6423
S. 6
DRAMA!! SPEECH!! DRAMA!!
I will be teaching Speech & Drama Lessons for the School year 95/96.
Private or group lessons available, questions??? or to register phone Missy Gibb at 758-6879.
S.6
i Magrath Hospital Auxiliary
e Please Note
| First meeting of the Fall Season will be held *7 Thurs. Sept. 7-2 pm at the hospital.
t Important meeting all members are asked to t try & attend.
y We would be happy to have any lady or man of
< the community to come join us. It's great to be a volunteer.
*Scenic Acreage 143 acres of rolling farm land, 2 mi.W & 1 mi.S of Magrath.Borders #5 Highway- irrigation ri",J. A. Ririe,"Magrath Store News (August 23, 1995)",,,,core
154468456,2005-06-01T07:00:00,"Online process monitoring and feedback control are two widely researched aspects that can impact the performance of a myriad of process applications. Semiconductor manufacturing is one such application that due to the ever increasing demands placed on its quality and speed holds tremendous potentials for further research and development in the areas of monitoring and control. One of the key areas of semiconductor manufacturing that has received significant attention among researchers and practitioners in recent years is the online sensor based monitoring and feedback control of its nanoscale wafer fabrication process. Monitoring and feedback control strategies of nanomanufacturing processes often require a combination of monitoring using nonstationary and multiscale signals, and a robust feedback control using complex process models. It is also essential for the monitoring and feedback control strategies to possess stringent properties such as high speed of execution, low
cost of operation, ease of implementation, high accuracy, and capability for online implementation. Due to the above requirement, a need is being felt to develop state-of-the-art sensor data processing algorithms that can perform far superior to those that are currently available both in the literature and commercially in the form of softwares.The contributions of this dissertation are three fold. It first focuses on the development of an efficient online scheme for process monitoring. The scheme combines the potentials of wavelet based multiresolution analysis and sequential probability ratio test to develop a very sensitive strategy to detect changes in nonstationary signals. Secondly, the dissertation presents a novel online feedback control scheme. The control problem is cast in the framework of probabilistic dynamic decision making, and the control scheme is built on the mathematical foundations of wavelet based multiresolution analysis, dynamic programming, and machine learning.
Analysis of convergence of the control scheme is also presented. Finally, the monitoring and the control schemes are tested on a nanoscale manufacturing process (chemical mechanical planarization, CMP) used in silicon wafer fabrication. The results obtained from experimental data clearly indicate that the approaches developed outperform the existing approaches. The novelty of the research in this dissertation stems from the fact that they further the science of sensor based process monitoring and control by uniting sophisticated concepts from signal processing, statistics, stochastic processes, and artificial intelligence, and yet remain versatile to many real world process applications",Scholar Commons,Process monitoring and feedback control using multiresolution analysis and machine learning,https://core.ac.uk/download/154468456.pdf,,,core
29399608,1997-01-01T00:00:00,"Proceeding of: International Work-Conference on Artificial and Natural Neural Networks, IWANN'97 Lanzarote, Canary Islands, Spain, June 4–6, 1997Power plant management relies on monitoring many signals that represent the technical parameters of the real plant. The use of neural networks (NN) is a novel approach that can help to produce decisions when integrated in a more general system. In this paper we introduce a NN module using an ART-MAP to discriminate different situations from the plant in order to prevent future malfunctions. A special process to generate of a complete training set has been designed. This process is developed in order to deal with the absence of data in abnormal plant situations. This module belongs to a more general system for predictive maintenance that has been implemented and incorporated in an hydroelectric plant.
This project has been supported by CDTI (belonging to Spanish Industry Ministery) and EEC (European Economic Comunity), reference number: PASO PC067",'Springer Science and Business Media LLC',Unsupervised neural network for forecasting alarms in hydroelectric power plant,,10.1007/BFb0032590,,core
143936412,1999-01-01T00:00:00,"With increasing competitive pressures, manuf
acturing systems in the automotive industry 
are being driven more and more aggressively. 
The pressures imposed on the processes and lack 
of system \u27slack\u27 have led to increased use of
 Tool Condition Monitoring 
systems. In parallel, 
there has been wide-ranging research in academ
ia. However, a closer examination shows that 
there has been very little migration of this re
search into industrial pr
actice. Furthermore, the 
success of industrially deployed monitoring systems 
has been poor. It has been suggested that a 
significant factor behind both these phenomenon ha
s been the \u27difficult\u27 environment in which 
such systems must operate; an en
vironment where they are subject 
to many stochastic influences, 
ranging from ambient conditions, to user
 input, to workpiece consistency. 
Neural networks have found increasing favour 
in manufacturing systems research because 
of their ability to perform robustly in noisy envi
ronments. Almost all the applications of this 
technology in tool condition monitoring have been in
 the detection/prediction of tool wear. From 
an academic standpoint, it may be speculated that 
the lack of focus on breakage and missing tool 
detection has been due to the relatively trivial nature of detecting such anomalies in the 
laboratory environment. However, detection in
 the production environment is compromised by a 
wide range of factors, which can give rise to fa
lse alarms when such strategies are transported 
from laboratory conditions. 
In this paper, data from a real manufacturi
ng process is used to demonstrate the potential 
application of neural networks to the task of 
anomaly detection in the production environment",,"Proc. 2nd Int. Conf. On Int. Manuf. Sys.,",,,,core
24349702,1995,"Most real-time scheduling has focused on a relatively small set of independent tasks directly invoked periodically or via interrupts. In many real-time applications such as flexible manufacturing, this system model is too simplistic. In flexible manufacturing two entirely different sets of resources must be scheduled and &quot;connected.&quot; At the highest level, there are raw materials, robot arms, platform space, etc., and at the lowest level there are computational resources. Upon ordering products, high level resources must be scheduled, and the associated computational resources to achieve the manufacturing of those products must also be scheduled. This gives rise to the need for high level Real-Time Artificial Intelligence (RTAI) planners, low level schedulers that can handle large numbers of precedence- and resource-constrained tasks, and a suitable interface between the two schedulers. This paper presents the initial stages of the design and implementation of a flexible manufacturing t..",,Multi-Level Scheduling for Flexible Manufacturing,,,,core
103002371,2004,"Abstract—Microsystems packaging is fundamentally depen-dent on the manufacture of microelectronic, photonic, radio frequency (RF), and MEMS devices. The system-on-package (SOP) approach has been identified as a key strategy for inte-grating these strategic packaging technologies. Because of rising costs, the challenge before SOP manufacturers is to offset capital investment with greater automation and technological innovation in the fabrication process. To reduce manufacturing cost, several important subtasks have emerged, including increasing fabrica-tion yield, reducing product cycle time, maintaining consistent levels of product quality and performance, and improving the reliability of processing equipment. Because of the large number of steps involved, maintaining product quality in an SOP man-ufacturing facility requires the control of hundreds of process variables. The interdependent issues of high yield, high quality, and low cycle time are addressed by the ongoing development of several critical capabilities in state-of-the-art computer-inte-grated manufacturing (CIM) systems: in situ process monitoring, process/equipment modeling, real-time process control, and equipment diagnosis. Recently, the use of computational intel-ligence in various manufacturing applications has increased, and the SOP manufacturing arena is no exception to this trend. Artificial neural networks, genetic algorithms (GAs), and other techniques have emerged as powerful tools for assisting CIM systems in performing various process monitoring, modeling, and control functions. This paper reviews current research in these areas, as well as the potential for deployment of these capabilities in state-of-the-art SOP manufacturing facilities. Index Terms—Artificial intelligence, process modeling, process control, system-on-package (SOP) manufacturing",,Intelligent SOP manufacturing,,10.1109/tadvp.2004.828824,,core
1498756,1996-01-01T00:00:00,"Distributed artificial intelligence (DAI) systems, in which multiple agents communicate and co-operate with one another to achieve their individual and collective goals, are a promising enabling technology for constructing large, real world industrial control applications. To facilitate the development of such systems a number of generic DAI frameworks have been devised. These frameworks typically aid the development process by providing a language, a set of structures, and/or some tools with which the necessary infrastructure and support mechanisms for interacting agents can be instantiated. The paper reports on one such framework, called ARCHON, which has been used to build DAI systems in the following industrial control domains: electricity distribution management, electricity transportation management, cement factory control, particle accelerator control and flexible assembly robotic cells. A distinguishing and novel feature of the ARCHON framework is that it extends the level of support offered to the system builder - it provides generic and reusable knowledge about the process of cooperation, in addition to the more standard development facilities. This generic knowledge is embedded in a domain-independent co-ordination module and it is the rationale, design, implementation and evaluation of this module which forms the major contribution of the paper",,Designing a Re-Usable Coordination Module for Cooperative Industrial Control Applications,,10.1049/ip-cta:19960186,,core
293523791,2004-01-01T00:00:00,"Humanos virtuais são modelos computacionais de pessoas. Se necessário, podem apresentar uma aparência bastante realista, baseada em princípios fisiológicos e biomecânicos. Além disso, são capazes de comportar-se de forma autônoma e inteligente em ambientes dinâmicos, podendo apresentar até mesmo individualidade e personalidade. Humanos virtuais podem ser utilizados como atores sintéticos. Tais atores têm sido usados em uma série de aplicações com a finalidade de simular a presença de atores reais. A indústria de jogos por computador requer personagens que sejam capazes de reagir apropriadamente a eventos e circunstâncias inesperadas, e até mesmo de alterar o progresso do jogo com seus cursos de ação autônomos. Um modo natural para desenvolver tais personagens prevê o uso de técnicas de inteligência artificial, em particular aquelas relacionadas às áreas de agentes autônomos e sistemas multiagentes. Neste trabalho, propõese o uso do modelo BDI (Belief-Desire-Intention) para modelar agentes cognitivos, com a finalidade de implementar personagens animados. O modelo BDI é uma abordagem bastante conhecida e bem sucedida para o desenvolvimento de agentes autônomos em sistemas multiagentes. Trata-se de uma arquitetura poderosa para sistemas dinâmicos e complexos, nos quais agentes podem precisar agir sob informação incompleta e incorreta sobre o seu ambiente e os outros habitantes. Esta dissertação reúne um modelo articulado para animação de personagens, o qual requer a especificação de movimento em cada junta individualmente, e um interpretador para AgentSpeak(L), uma linguagem de programação orientada a agentes que implementa a arquitetura BDI. Foi desenvolvida uma interface que permite que o sistema de raciocínio de um agente, baseado em BDI, seja usado para dirigir o comportamento de um personagem em um sistema de animação.  O uso de AgentSpeak(L) é uma abordagem promissora para a especificação em alto nível de animações complexas por computador. O modelo conceitual e sua implementação são apresentados em capítulos distintos. Esta separação visa simplificar a compreensão do modelo proposto, permitindo primeiro analisá-lo em um nível mais alto de abstração, para então verificar detalhes de programação. Este trabalho apresenta também duas animações 3D, usadas para ilustrar a abordagem proposta. A principal animação apresentada envolve um agente situado em um ambiente dinâmico; o agente continuamente percebe o ambiente e raciocina para determinar como agir sobre ele, baseado em seu estado mental BDI. A outra aplicação é bastante simples, mas útil para mostrar algumas questões que são relevantes para obter-se mais eficiência em programas AgentSpeak(L).Virtual humans are computational models of people. If necessary, they can portray a very realistic appearance, based on biomechanical and physiological principles. Besides, they are able to behave in an autonomous and intelligent way in dynamic environments, and even to exhibit individuality and personality. Virtual humans can be used as synthetic actors. Such kind of actors have been used in several applications, such as games, in order to simulate the presence of real actors. The computer-game industry requires characters that are able to react appropriately to unexpected events and circumstances, and even to change the game progress with their autonomous courses of actions. A natural way for developing such characters is by the use of artificial intelligence techniques, in particular those related to the areas of autonomous agents and multi-agent systems. In this work, the use of the Belief-Desire-Intention (BDI) model for cognitive agents in order to implement animated characters is proposed. The BDI model is a well-known and successful approach for the development of autonomous agents in multiagent systems. It is a very powerful architecture for dynamic and complex systems where agents may need to act under incomplete and incorrect information on other agents and their environment. This work brings together an articulated model for character animation, which requires the specification of motion on each joint individually, and an interpreter for AgentSpeak(L), an agent-oriented programming language that implements the BDI architecture. I have developed an interface that allows the BDI-based agent reasoning system to be used for guiding the behaviour of a character in an animation system. The use of AgentSpeak(L) is a promising approach for the high-level specification of complex computer animations. The conceptual model and its implementation are presented in distinct chapters.  This separation aims at simplifying the comprehension of the proposed model, allowing its analysis first at a higher abstraction level, and after that to check programming details. This work also presents two 3-D animations used to illustrate the proposed approach. The main animation presented involves an agent that is situated in a dynamic environment; the agent continuously perceives the environment and reasons on how to act upon it based on its BDI mental state. The other application is quite simple, but useful to show some issues that are relevant for obtaining better performance from AgentSpeak(L) programs",,Uma arquitetura para animar agentes autônomos em ambientes virtuais usando o modelo BDI,https://core.ac.uk/download/293523791.pdf,,,core
57092051,1995,"During the last ten years neural networks made their way from research toys to well accepted industrial tools. Actual application however requires real-time capable implementations of neural algorithms. The wide range of applications and performance requirements as well as the considerable number of algorithms motivated numerous efforts of neural network hardware implementation that in their numbers and diversity are hard to overlook even for researchers working in the field. Introducing, this paper will give an outline of relevant neural network implementation categories, criteria for performance assessment, and of principle design issues, e. g., accuracy of computation. Ensuing, a survey of state-of-the-art implementations will be given, focusing on recent designs that contribute intriguing and promising features or architectures, and systems that made all the way from concept to actual scientific or commercial application. Concluding, a perspective of anticipated future developments will be given",,Survey and current status of neural network hardware,,,,core
217066628,2001-08-01T07:00:00,"The thesis focuses on the design and implementation of a control scheme for a photolithography process. The process requires a tight control to maintain a desired gate critical dimension (CD). The control approach, which is currently in use in most semiconductor facilities is based on operator experience and does not provide satisfactory control on the CD variation. Implementation of an automatic feedback control system in the industry has been difficult because the CD cannot be measured in real-time but only after the process has been completed. In this thesis, a neural network (NN) is used to predict CD based on the input parameters using historical data that are collected at a manufacturing facility. Using neural networks an inverse model of the process is designed and cascaded with the process model to form the feed forward controller. A feedback CD controller that provides a tighter control in the CD variation is obtained by connecting a fuzzy controller in the feedback loop",DigitalCommons@UMaine,Intelligent Control of Critical Dimensions in the Semiconductor Industry,,,,core
197996926,1997-01-01T00:00:00,"The thesis is about computer architectures specially tuned to an application area. This means that the work spans the area from implementation technology via processor and computer system organization to the applications themselves. The work reported here is in the area of embedded high performance computing, near the area of application specific hardware. The thread throughout the thesis is how to design computers to suit a specific application area, while maintaining as much computing performance, programmability, scalability and flexibility as possible. The idea is that the multiple SIMD computing model can be a flexible and reasonably scalable concept for the high end applications considered. To test this hypothesis the approach taken is to use application examples, algorithm analysis and implementation experiments to derive suitable computing modules. These modules are then evaluated according to scalability, generality, efficiency, and implementation aspects. The application areas are artificial neural network computing and signal processing in phased array radar. For the artificial neural network computing a multiple SIMD architecture is suggested and artificial neural network algorithms are mapped onto a typical such module. Implementation aspects are discussed and the design of a prototype is shown. Then the use of artificial neural networks in an industrial real-time application is presented. The artificial neural networks are used to extract information from noisy and non-linear signals in combustion engines. It is shown that the neural networks are feasible, and close to optimal, in this application. In the area of signal processing for phased array radar, two application examples are analyzed and architectures suitable for the these are derived. An intermodule communication for implementation on a fiber-optic network is evaluated in a radar application. Then implementation issues for the processing modules are considered and discussed. This is done in the light of instruction statistics gathered from the application examples. Finally, the results are combined and the VEGA architecture is described and motivated. In the thesis it is shown that the modular, multiple SIMD model can be efficiently used in both signal processing for phased array radar and artificial neural network computing. Furthermore, a conclusion drawn is that the linear array SIMD module with broadcast and ring communication is enough for many popular neural network models. It is also concluded that the moderately parallel MIMD machine with moderately parallel SIMD computing modules is a feasible architecture for signal processing in phased array radar",,SIMD Architectures for Radar Signal Processing and Artificial Neural Networks,,,,core
101988193,2002,"The Problem: Reinforcement-learning (RL) [1] techniques have elegant theoretical foundations and have proven useful in learning a variety of off-line tasks, such as playing Backgammon. However, they are still much too inefficient to be of use in the majority of robot-learning domains. In this project, we seek to make reinforcement learning effective for real robots. We are particularly interested in domains with continuous sensory inputs and continuous actions, and require that learning take place online from a relatively small amount of experience. Motivation: In order to deploy robots in a wide variety of applications, from household to military, we must find a way to “program ” them efficiently. Direct programming by humans is a tedious process, requiring a large amount of trial-and-error debugging on the part of the human. In addition, such hand-built programs are only suited for a single domain, and must be re-engineered for new houses or military situations. Thus, behavior learning must play a major role in the wide deployment of robots. Previous Work: There has been work on RL for real robots that takes good advantage of particular properties of the application domain. Schaal and Atkeson [3] built a juggling robot that assumed continuous deterministic world dynamics. Moore developed a system that learned to control a factory packaging machine that took advantage of very slow variation in the process [1]. Approach: Our first step will be to use nearest neighbor (and locally weighted regression) [2] as a function approx",,Making Reinforcement Learning Work on Real Robots,,,,core
1498744,1995-01-01T00:00:00,"One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparative experiments in the real-world domain of electricity transport management. Finally, the success of the approach of building a system with an explicit and grounded representation of cooperative problem solving is used to outline a proposal for the next generation of multi-agent systems",,Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions,,,,core
1640743,1995-01-01T00:00:00,"The Teaching Company Scheme (TCS) I am participating in involves investigating the potential for using neural networks in the development of complex software products. Recently neural networks have enjoyed a resurgence in popularity in academia and industry (cf. DTI scheme), and it has become apparent that there are a number of ways in which the technology can be applied to real-world problems. Neural networks have a statistical basis, and they can be viewed as making these powerful statistical techniques available and accessible to non-staticians",University of Hertfordshire,Applications of neural networks in telecommunications,https://core.ac.uk/download/1640743.pdf,,,core
78773058,2005-12-15T00:00:00,"Research topic A problem of fundamental interest is semantic video understanding, particularly in terms of human location and identity. There are several reasons why one is interested in this research topic. First of all, a semantic video description can provide the basis for a video indexing system, which can be used to process video signals so as to make possible search requests on a natural language. A second reason why face detection and recognition in video is important is that they are potentially capable of letting us learn a great deal about the entire video without having to have semantically understand it on a very deep level for majority of applications (i.e. humans are the key objects of interest in semantic video analysis that capture significantly higher degree of importance then any other objects in a general case). This property is especially important when the cost of getting semantic meaning from video is very high. Finally, the most important reason why face detection and recognition in video are important is that they often work extremely well in practice, and enable us to realize accurate and reliable practical systems, in a very efficient manner. In this work we concern ourselves strictly with face detection and recognition problem being a detached part of more general semantic video understanding problem. Thus we didn’t explore possibilities of better video comprehension by integration of multiple types of semantic descriptors together. We have focused our attention on the solution that is accurate and reliable enough to work more effective than a human expert in video analysis applications. Particularly we are interesting in all types of video, where human individuals can be found. Problems and Objective Neither the theory of face detection and recognition in video nor its applications to pattern recognition is new. Many research papers were published staring from the 1970th. However, widespread application of the theory of face detection and recognition to semantic video understanding has occurred only within the past decade. There are several reasons why this has been the case. First, the growth of the telecommunication industry reaches the barriers of human-wise media information management and the need of automatic video understanding and management systems became critical for further development. The second reason was that the original applications of the face detection and recognition, mainly in security domain, didn’t provide sufficient performance for most semantic video analysis demands. As a result, the fundamentals of the face detection and recognition theory, has provided a sufficient level of details for a number of research labs to begin to work on further development in the direction discussed above. Therefore the objective of this work can be summarized as follows: to propose face detection and recognition in video solution that is enough fast, accurate and reliable to be implemented in the semantic video understanding system that is capable of replacing human expert in a variety of multimedia indexing applications. Meanwhile we assume that the research results that were raised during this work are complete enough to be adapted or modified as a part of other image processing, pattern recognition and video indexing and analysis systems. Our Approach A video signal can be represented by a series of layers, each containing one semantic object. The only difference between semantically uncovered signal and a standard video is that all layers are mixed in the last case. Given the above scenario, the problem of interest is how do we decompose the video signal into layers to isolate semantic objects in it. Because of unique nature, human face was used as a token to solve the semantic video understanding issue (and particularly understanding of human individuals). The problems one faces are mainly reflected by variability of human appearance (expression, orientation, scale, etc.) and photometric conditions within the video (lighting conditions, video quality, duration and others). Unlike many problems in image processing and pattern recognition for which an exact solution can be given, there are several possible ways of solving face detection and recognition problem associated with the given semantic video understanding issue. The difficulty lies with the optimal choice of a general architecture and basic principles of such a system. For some of these implementation issues we can prescribe exact analytical solutions; for other issue, mainly concerning the architecture fundamentals, we can only provide some experience gained from working with the problem of interest from the scientific society. Considering the polar concepts of a human-wise data comprehension and a computer-wise, which is fast but very simple in nature we have to benefit from both approaches to meet real-world demands. According to the hypothesis presented above, our approach consists of choosing computationally-wise core with simple instructions, which are adapted by human knowledge in order to empower the entire solution, thus solve the issue. Contributions In order to illustrate all contributions made in this work we first present our general concept of face detection and recognition as a part of the semantic video understanding problem, then we separately discuss face annotating, detection, matching and recognition problems and our particular solutions to them. Face detection and recognition as a part of the semantic video understanding problem: in this part of the thesis we point out that each particular classification/analysis solution for any semantic object has theoretical limits on its learning capacity. However each of these solutions can be trained to accurately and reliably solve a partial part of the entire issue. Therefore we conclude that the face detection and recognition must be decomposed into a series of connected sub-problems that could be efficiently solved. Thus, this architecture relies on human knowledge on the subject of interest in order to decompose and connect separate solutions. The same time it is grounded on fast, accurate and reliable image processing and pattern recognition methods for resolving of particular sub-issues. Face annotating: this section presents a method for semi-automatic ground truth segmentation for benchmarking of face detection and recognition in video. We aim to illustrate the solution to the issue where an image processing and pattern recognition expert is able to segment and annotate facial patterns in video sequences at the rate of 7500 frames per hour. Evaluation criteria are discussed within different aspects of manual face segmentation. We extend these ideas to the semi-automatic face segmentation methodology, where all facial patterns are categorized into 4 classes in order to increase flexibility of evaluation results analysis. We present a strict guide how to speedup manual segmentation process up to 30 times and illustrate it with the sample test video sequences that consists of more than 90000 frames, 800 individuals and 50000 facial images. Experimental evaluation of the face detection using the ground truth data, that was semi-automatically segmented, demonstrates effectiveness of current approach both for learning and test stages. Face detection: given the form of face detection and recognition in semantic video understanding, there are basic problems of interest that have been solved in this thesis. The first problem is the integration problem, namely given a set of methods for face detection, how do we combine their results together in the most efficient way. Our solution utilizes five independent face localizers in order to select potential regions of interest; next these regions are classified using data-mining, which produces an optimal detection strategy. Following this strategy we search potential facial regions by a neural network classifier in order to localize facial position with minimum computational expenses, preserving high accuracy and reliability. The second problem is the one in which we attempt to construct face/non face classifier with high accuracy (>0.7) and extremely high reliability (10E-8<). The presented solution is based on series of cascades of three different kinds of classifier. The first set of cascade is the modified version of the state-of-the art classifier that uses integral facial features. It is followed by a set of neural network cascades on integral facial features. The last type of classifiers is intensity-based neural networks. Although this solution is computationally more expensive comparing to the reference works it provides higher reliability that is the most significant factor for the issue of interest. Finally we consider the problem of adapting face detection to current video stream conditions, so that it provides better results comparing to a fixed solution. The application of interest is media streaming on mobile phones. For selected cases, for example videoconferences, we were able to achieve frame rate face detection on a standard smartphone. Face Matching: the issue of face detection and recognition problem has leads to many schools of though. Our idea is to let some face with known parameters correspond roughly to a probe face that is found by a face detector and must be recognized by a face recognizer. Thus the recognition process will benefit an advantage of a priory available probe characteristics such as lighting conditions, orientation, expression, etc. Furthermore we restrict each detected facial image to have a corresponding best match; this implies that the models used for recognition will work best when they are properly adjusted. The face matching requires a large gallery of training facial images to correctly approximate practically infinite variability of probe images. The concept of a compact facial model, a facial image that is encoded and can be decoded by a 3d face modeling engine when needed, and fast search methods were used in this work in order to solve the issue. The very first important issue that rises face matching is the accuracy of a probe image. This entirely depends on the face detector. The way in which the face matching was solved refers to a 60 bit encoded compact facial model and ability to correctly match probe facial image with varying in-plane/out-plane orientations, head mesh transforms, emotions, texture details, lighting conditions, scale and translations. Face Recognition: this process places constraints on face detection and matching parts of the entire solution so that the probe facial image located by the detector has to be described by the compact facial model in the face matching unit. Genetic algorithms are used for matching on the learning stage because of higher accuracy comparing to the fast search methods used in the previous case (the drawback is significantly higher computational complexity). The probe facial image is reconstructed into a normalized texture by 3d face modeling engine that is installed with compact facial model parameters. Then the facial feature matching finds the most suitable templates for eyes, eyebrows, nose and mouse by using fast matching methods. Such a facial image can again be represented by a set of facial feature templates and their locations. This information is used for preliminary classification, where only top-N individuals are selected for further recognition. Finally a set of neural network cascades based on the encoded facial description (that is independently trained for each individual) is used for choosing the appropriate response. This again serves to make the recognition task more accurate and reliable, thus leads to higher performance of the system. The results of this work were applied into the multimedia indexing system CINDI in a framework of RNRT Cyrano project on personal media distribution and management. Fast search methods were used in biomedical images indexing project B-705. A series of face detection and tracking methods was integrated into a commercial video assets management system",HAL CCSD,"Face detection, matching and recognition for semantic video understanding",,,,core
82184962,1999-01-31,"AbstractMany organizations today are facing the problem of software migration: porting existing code to new architectures and operating systems. In many cases, such legacy code is written in a mainframe-specific assembly language and needs to be translated to a high-level language in order to be run on different architectures. Our research addresses this problem in a large-scale, real-life case study. We built an automatic tool, called Bogart, that translates IBM 370 assembly language programs to C. Bogart is based on Artificial Intelligence tools and techniques such as the Plan Calculus, translation by abstraction and re-implementation, program transformations, constraint propagation, and pattern recognition.Bogart was tested on real legacy code of a large commercial application: a database system and application generator, the main product of Sapiens International, Ltd. Bogart is compared with the literal brute-force translator initially developed by Sapiens, and is found to be superior on all counts, including portability of the resulting code, the amount of manual preparation required, and code size and speed. The results are shown for several small examples as well as a typical module consisting of several thousand lines of code from the Sapiens application. Bogart also seems to be more comprehensive than other reengineering systems reported in the literature. Bogart's analysis technology has recently been applied with significant commercial success to the analysis and remediation of Year 2000 bugs.This study demonstrates that certain AI techniques can be carefully combined to create industrial-strength applications that solve acute problems of Software Engineering. The fact that the research was carried out in industry on a real test case also revealed some of the problems of this approach. One example is the higher development cost of the AI approach, and the further effort that will be needed in order to extend it. (On the other hand, the literal translator has reached the end of its road, and cannot be enhanced at all.) Another problem we discovered is the difficulty of debugging the code produced by Bogart. The literal translator preserved the structure of the original program, whereas Bogart abstracted the code in various ways. As a result, the original assembly-language programmers found it harder to debug Bogart's code. This reaffirms the need for an explanation facility in intelligent applications",Published by Elsevier B.V.,Portability by automatic translation: A large-scale case study ,https://core.ac.uk/download/pdf/82184962.pdf,10.1016/S0004-3702(98)00101-5,,core
300426791,2000-12-01T00:00:00,"This paper presents the use of a micro-controller-based integrated process supervision (IPS) system as a real-time platform for investigative work in structuring expert control. Two different control approaches, based on classical and artificial intelligence techniques, were integrated within IPS and serve as practical examples of the structured approach to expert control. The IPS is a

refinement of the expert control architecture. It allows the integration of several control techniques in a single generic framework. Specifically, the paper presents the extensive experimental results derived from a micro-controller-based implementation of IPS on the real-time control of a typical industrial heat-exchanger process. The classical approach, based on auto-tuning techniques, was

implemented under the IPS framework. Three auto-tuning techniques, namely Ziegler–Nichols tuning, amplitude tuning and phase tuning were incorporated. In addition, neural-network-based control techniques using the modified cerebellar model articulation controller (MCMAC) were also seamlessly incorporated within the IPS scheme. The real-time experimental results using the IPS architecture significantly demonstrated the effectiveness of IPS in handling varying operating conditions. Furthermore, the inclusion of both AI and classical control techniques within a common supervisory framework adequately shows the generality of the architecture",Elsevier Science Ltd.,Real-time integrated process supervision,,,,core
71319460,2004-12-01T00:00:00,"Nuclear weapons and their storage facilities may benefit from in-situ structural health monitoring systems. Appending health-monitoring functionality to conventional materials and structures has been only marginally successful. The purpose of this project was to evaluate feasibility of a new smart material that includes self-sensing health monitoring functions similar to that of a nervous system of a living organism. Reviews of current efforts in the fields of heath-monitoring, nanotechnology, micro-electromechanical systems (MEMS), and wireless sensor networks were conducted. Limitations of the current nanotechnology methods were identified and new approaches were proposed to accelerate the development of self-sensing materials. Wireless networks of MEMS sensors have been researched as possible prototypes of self-sensing materials. Sensor networks were also examined as enabling technologies for dense data collection techniques to be used for validation of numerical methods and material parameter identification. Each grain of the envisioned material contains sensors that are connected in a dendritic manner similar to networks of neurons in a nervous system. Each sensor/neuron can communicate with the neighboring grains. Both the state of the sensor (on/off) and the quality of communication signal (speed/amplitude) should indicate not only a presence of a structural defect but the nature of the defect as well. For example, a failed sensor may represent a through-grain crack, while a lost or degraded communication link may represent an inter-granular crack. A technology to create such material does not exist. While recent progress in the fields of MEMS and nanotechnology allows to envision these new smart materials, it is unrealistic to expect creation of self-sensing materials in the near future. The current state of MEMS, nanotechnology, communication, sensor networks, and data processing technologies indicates that it will take more than ten years for the technologies to mature enough to make self-sensing materials a reality. Nevertheless, recent advances in the field of nanotechnology demonstrate that nanotubes, nanorods, and nanoparticles of carbon, boron and other materials have remarkable mechanical and electrical properties. This would provide. for a plethora of potential applications including self-sensing materials. Record strength-to-weight ratios, ballistic conductivity, and sensing capabilities (i.e., piezo- resistance and piezoelectricity) have been reported for carbon nanotubes. The first transistors, sensors, and actuators have been made from the carbon nanotubes and other nanomaterials. However, nanomaterials are notoriously difficult to manipulate into useful geometries. Nano-manufacturing processes often produce bundles or random networks of nanostructured materials. Samples of the material are then manipulated with advanced microscopy tools to measure properties or to create a single device. This is a laborious and time consuming process. An often overlooked property of the manufactured nanotube bundles is their similarity to the dendritic structure of neural networks with a great quantity of interconnects that may serve as initiation sites for artificial neurons in a self-sensing material nervous system. To accelerate the development of self-sensing materials, future research should concentrate on naturally occurring dendritic nano-structures. While self-sensing materials with subgrain size sensors (scale of micrometers) remain in the realm of basic research, meso-scale (millimeters to centimeters) sensors and their networks are in the state of mature research and have begun to find their way into commercial applications. Macro-scale (centimeters to decimeters) sensors and their networks are commercially available from various sources. The majority of applications that employ sensor networks are driven by the needs of the Department of Defense. Widespread adaptation of sensor networks has been limited by, on one hand, the sensor's high cost of design, development, and deployment, and on the other hand, a lack of reliable long-term power sources. Solutions to both of these drawbacks require significant investments driven by real-life applications. Possible applications for sensor networks at Sandia National Laboratories include dense data collection techniques for validation of numerical methods and material parameter identification. For example, an array of distributed wireless macro-scale sensors can record the structural response of soils and reinforced concrete during explosive loading. Another example is an array of surface mounted micro-sensors that can record the modal response of nuclear weapon components. The collected data would be used to validate existing numerical codes and to identify new physical mechanisms to improve Sandia's computational models",'Office of Scientific and Technical Information  (OSTI)',New smart materials to address issues of structural health monitoring.,https://core.ac.uk/download/71319460.pdf,10.2172/920836,,core
250577317,2005-04-04T00:00:00,"International audienceFlight simulators have been part of aviation history since its beginning. With the development of modern aeronautics industry, flight simulators have gained an important place and the industry devoted to their manufacture has become significant. In the case of transportation aircraft, accurate mathematical models based on extensive experimental data have been developed by their manufacturers to optimize their aerodynamic and propulsive characteristics and to design efficient flight control systems. However, in the case of small general aviation aircraft this kind of knowledge is not commonly available and the design of accurate flight simulators can result in a tedious try and modify process until the simulator presents a qualitative behaviour close to the one of the real aircraft. This communication proposes through the use of neural networks a method to perform a direct estimation of the aerodynamic forces acting on aircraft. Artificial neural networks appear to be an appropriate numerical technique to achieve the mapping of these continuous relationships and detailed aerodynamics and thrust models should become no more mandatory to produce accurate flight simulation software",'Institute of Electrical and Electronics Engineers (IEEE)',A neural approach for fast simulation of flight mechanics,,10.1109/ANSS.2005.8,,core
475189999,2005-01-01T08:00:00,"Online process monitoring and feedback control are two widely researched aspects that can impact the performance of a myriad of process applications. Semiconductor manufacturing is one such application that due to the ever increasing demands placed on its quality and speed holds tremendous potentials for further research and development in the areas of monitoring and control. One of the key areas of semiconductor manufacturing that has received significant attention among researchers and practitioners in recent years is the online sensor based monitoring and feedback control of its nanoscale wafer fabrication process.
Monitoring and feedback control strategies of nanomanufacturing processes often require a combination of monitoring using nonstationary and multiscale signals, and a robust feedback control using complex process models. It is also essential for the monitoring and feedback control strategies to possess stringent properties such as high speed of execution, lowcost of operation, ease of implementation, high accuracy, and capability for online implementation. Due to the above requirement, a need is being felt to develop state-of-the-art sensor data processing algorithms that can perform far superior to those that are currently available both in the literature and commercially in the form of softwares.
The contributions of this dissertation are three fold. It first focuses on the development of an efficient online scheme for process monitoring. The scheme combines the potentials of wavelet based multiresolution analysis and sequential probability ratio test to develop a very sensitive strategy to detect changes in nonstationary signals. Secondly, the dissertation presents a novel online feedback control scheme. The control problem is cast in the framework of probabilistic dynamic decision making, and the control scheme is built on the mathematical foundations of wavelet based multiresolution analysis, dynamic programming, and machine learning.Analysis of convergence of the control scheme is also presented. Finally, the monitoring and the control schemes are tested on a nanoscale manufacturing process (chemical mechanical planarization, CMP) used in silicon wafer fabrication. The results obtained from experimental data clearly indicate that the approaches developed outperform the existing approaches. The novelty of the research in this dissertation stems from the fact that they further the science of sensor based process monitoring and control by uniting sophisticated concepts from signal processing, statistics, stochastic processes, and artificial intelligence, and yet remain versatile to many real world process applications",Scholar Commons,Process monitoring and feedback control using multiresolution analysis and machine learning,,,,core
334251233,2005-01-01T00:00:00,"WOS: 000230723800010In this paper, Petri nets and neural networks are used together in the development of an intelligent logic controller for an experimental manufacturing plant to provide the flexibility and intelligence required from this type of dynamic systems. In the experimental setup, among deformed and good parts to be processed, there are four different part types to be recognised and selected. To distinguish the correct part types, a convolutional neural net le-net5 based on-line image recognition system is established. Then, the necessary information to be used within the logic control system is produced by this on-line image recognition system. Using the information about the correct part types and Automation Petri nets, a logic control system is designed. To convert the resulting Automation Petri net model of the controller into the related ladder logic diagram (LLD), the token passing logic (TPL) method is used. Finally, the implementation of the control logic as an LDD for the real time control of the manufacturing system is accomplished by using a commercial programmable logic controller (PLC)",'Springer Science and Business Media LLC',Neurovision-based logic control of an experimental manufacturing plant using neural net le-net5 and automation Petri nets,,10.1007/s10845-005-1662-5,"[{'title': 'Journal of Intelligent Manufacturing', 'identifiers': ['issn:0956-5515', '1572-8145', 'issn:1572-8145', '0956-5515']}]",core
250208003,2005-01-01T00:00:00,"WOS: 000230723800010In this paper, Petri nets and neural networks are used together in the development of an intelligent logic controller for an experimental manufacturing plant to provide the flexibility and intelligence required from this type of dynamic systems. In the experimental setup, among deformed and good parts to be processed, there are four different part types to be recognised and selected. To distinguish the correct part types, a convolutional neural net le-net5 based on-line image recognition system is established. Then, the necessary information to be used within the logic control system is produced by this on-line image recognition system. Using the information about the correct part types and Automation Petri nets, a logic control system is designed. To convert the resulting Automation Petri net model of the controller into the related ladder logic diagram (LLD), the token passing logic (TPL) method is used. Finally, the implementation of the control logic as an LDD for the real time control of the manufacturing system is accomplished by using a commercial programmable logic controller (PLC)",'Springer Science and Business Media LLC',Neurovision-based logic control of an experimental manufacturing plant using neural net le-net5 and automation Petri nets,,10.1007/s10845-005-1662-5,"[{'title': 'Journal of Intelligent Manufacturing', 'identifiers': ['issn:0956-5515', '1572-8145', 'issn:1572-8145', '0956-5515']}]",core
232675738,1997-01-01T08:00:00,"A formal theory for the development of a generic model of an autonomous sensor is proposed and implemented. An autonomous sensor is defined as an intelligent sensor that has machine learning capabilities. It not only interprets the acquired data in accordance with an embedded expert system knowledge base, but is also capable of using this data to modify and enhance this knowledge base. Hence, the system is capable of learning and thereby improving its performance over time. The main objective of the model is to combine the capabilities of the physical sensor and an expert operator monitoring the sensor in real-time. The system has been successfully tested using various simulated data sets as well as a real thermistor that has been developed as an autonomous sensor. This work has significant impact on modem production systems since sensors form an integral part of all closed loop control systems, and modem manufacturing processes rely heavily on sensor based control systems. The long range aim of this work is to develop highly autonomous production systems that have self diagnostic, maintenance, self correction, and learning capabilities embedded at the local and global levels. This work builds upon work on a formalized theory for autonomous sensing called Dynamic Across Time Autonomous-Sensing, Interpretation, Model learning and Maintenance Theory (DATA-SEIALAMT) that has been supported by the NSF and the SME Education Foundation",'Springer Science and Business Media LLC',A Novel Method to Create Intelligent Sensors with Learning Capabilities to Improve Modern Production Systems,,10.1007/978-0-387-35291-6_39,,core
58782647,1997,"Proceeding of: International Work-Conference on Artificial and Natural Neural Networks, IWANN'97 Lanzarote, Canary Islands, Spain, June 4–6, 1997Power plant management relies on monitoring many signals that represent the technical parameters of the real plant. The use of neural networks (NN) is a novel approach that can help to produce decisions when integrated in a more general system. In this paper we introduce a NN module using an ART-MAP to discriminate different situations from the plant in order to prevent future malfunctions. A special process to generate of a complete training set has been designed. This process is developed in order to deal with the absence of data in abnormal plant situations. This module belongs to a more general system for predictive maintenance that has been implemented and incorporated in an hydroelectric plant.
This project has been supported by CDTI (belonging to Spanish Industry Ministery) and EEC (European Economic Comunity), reference number: PASO PC067",Springer,Unsupervised neural network for forecasting alarms in hydroelectric power plant,,10.1007/BFb0032590,,core
51081158,2000-01-01T08:00:00,"Failure Modes and Effects Analysis (FMEA) is a Six Sigma tool for identifying, analyzing and prioritizing failures and solutions. FMEA is widely used in many different industries worldwide. The FMEA model is neither easy to learn nor easy to use. Effective FMEA automation has been an elusive goal for some time. The scope of meaningful FMEA automation has been limited to specific proprietary or academic application domains where substantial time and effort have been invested. Commercially available FMEA software packages do little more than reduce clerical effort. There has been no published research on the usability of FMEA. Most of the recent FMEA research has applied various artificial intelligence technologies. The vast majority of FMEA research has been directed toward manufactured products rather than information systems. The author has examined the use of FMEA for software development.
The author\u27s goal for this dissertation was the creation of a usable failure analysis matrix (FAM) model for prioritizing solutions to failures in information systems. The two-dimensional F AM worksheets are smaller than the linear FMEA worksheets, requiring less scrolling. The F AM is an alternative method to help identify the most important potential failures or failures and help prioritize alternative solutions using approximate expected costs. This new tool, implemented in Excel, provides integrated data entry and reporting. The F AM may be used without having detailed information.
The author validated four usability attributes for the F AM: usefulness, ease of use, ease of learning, and satisfaction. The validation process employed expert reviews, usability testing, and a usability questionnaire. The final mean value of 5.30 for usability questionnaire items compares favorably with a neutral value of 4. The F AM underwent expert reviews with reviewers from three different domains of expertise: usability, FMEA, and Six Sigma. Usability test subjects were 20 GE Industrial Systems information systems professionals. The test subjects applied the F AM to real information systems failures.
The author has improved professional practice by applying usability engineering to a problem analysis tool for software development. Expert reviews and usability testing both proved to be applicable to the design of the F AM. The expert reviews resulted in the inclusion of several important attributes of the FAM. The usability testing resulted in shorter task completion times and less severe usability problems",NSUWorks,The Failure Analysis Matrix: A Usable Model for Ranking Solutions to Failures in Information Systems,,,,core
23703128,1995,". In this paper we take a look at real-time systems from an implementation-oriented perspective. We are interested in the formal description of genuinely distributed systems whose correct functional behaviour depends on real-time constraints. The question of how to combine real-time with distributed processing in a clean and satisfactory way is the object of our investigation. The approach we wish to advance is based on PMC, an asynchronous process algebra with multiple clocks. The keywords here are `asynchrony&apos; as the essential feature of distributed computation and the notion of a `clock&apos; as an elementary real-time mechanism. We base the discussion on an actual industrial product: The Bruel &amp; Kjaer 2145 Vehicle Signal Analyzer, an instrument for measuring and analyzing noise generated by cars and other machines with rotating objects. We present an extension of  PMC by ML-style value passing and demonstrate its use on a simplified version of the Bruel &amp; Kjaer Signal Analyzer. 1 Introd..",Springer-Verlag,Describing a Signal Analyzer in the Process Algebra PMC - A Case Study,,,,core
147898898,2005-03-16T13:28:13,"There is a large number of possible applications in the field of mobile robotics: Mail delivery robots, domestic or industrial vacuum cleaners, surveillance robots, demining robots and many others could be very interesting products. Despite this potential market and the actual technology, only few simple systems are commercially available. This proves that there are several important and problematic issues in this field, mainly at the intelligence level. As a reaction to the failure of the classical artificial intelligence applied to the field of mobile robotics, several new approaches have been proposed. Artificial neural networks are one of these, and genetic algorithms, supported by the Artificial Life trend, are also getting more and more consideration. These two techniques have already been applied to mobile robotics, but mainly in simulation, and without a final test on a real mobile robot. The use of physical robots for this research seems to be still problematic due to the lack of efficient tools. Several neural structures for the control of mobile robots have been analysed in this work. All experiences have been carried out on physical robots. To reach this goal, an important effort has been made in order to design new efficient robotic tools. Together with Edo Franzi, André Guignard and Yves Cheneval, we have developed and built hardware and software tools that make an efficient research work possible. Along with several analysis software tools, the mobile robot Khepera has been a result of this development. Using this equipment, six experiences have been carried out, covering a large spectrum of the possible ways neural networks can be used for the control of mobile robots. These experiments have nevertheless been restricted to simple behaviours and small neural networks. The first two experiments show, with a very simple and manually adjusted behaviour, the important role of the interaction of the robot with its environment. The first experiment is based on a collective behaviour, the second on a collaborative one. The adaptation of the robot to the environment is introduced in the third experiment, in which a learning technique is applied. The result is a robot able to learn how to use visual stimuli to avoid particular obstacles. Despite its interesting results, this approach has turned out to be very limited, due to the rigid structure needed. The last three experiments demonstrate the possibilities of the use of genetic algorithms, which proved to be a very flexible adaptation mechanism. The first of these three experiments tests the feasibility of this approach. The second one takes advantage of the characteristics of genetic algorithms to achieve more complex behaviours. Finally, genetic algorithms and learning techniques are associated in the last experiment, showing a high adaptive structure. An important effort has been made to show both advantages and disadvantages of each technique, in order to provide the necessary elements for the continuation of this research activity","Lausanne, EPFL",Conception de structures neuronales pour le contrôle de robots mobiles autonomes,https://core.ac.uk/download/147898898.pdf,10.5075/epfl-thesis-1598,,core
212082183,2005-04-01,"Induction motors are largely used in several industry sectors. The selection of an induction motor has still been inaccurate because in most of the cases the load behavior in its shaft is completely unknown. The proposal of this article is to use artificial neural networks for torque estimation with the purpose of best selecting the induction motors rather than conventional methods, which use classical identification techniques and mechanical load modeling. Since proposed approach estimates the torque behavior from the transient to the steady state, one of its main contributions is the potential to also be implemented in control schemes for real-time applications. Simulation results are also presented to validate the proposed approach",Taylor & Francis Inc,Neural network based estimation of torque in induction motors for real-time applications,,10.1080/15325000590479910,"[{'title': None, 'identifiers': ['1532-5008', 'issn:1532-5008']}]",core
10880482,2001-01-01T00:00:00,"Design research must be concerned with providing industry with significant competitive advantages in product and process design by developing advanced design methods and computer-based design tools, techniques, systems ad applications that support the creation of reliable, high quality, cost-effective, innovation and competitive products. The Design Technology Research Centre (DTRC) in the School of Design at the Hong Kong Polytechnic University focuses on the development of computer enhanced design processes and product-orientated and user-centred design tools and systems. Evolutionary computation, generative and knowledge based systems, Artificial Intelligence, integrated and interactive system techniques, virtual reality and computer supported collaborative work are employed for the implementation of these processes, tools and systems. In this paper, the author presents an overview of the research undertaken by the DTRC and discusses the issues related to the development of the design workstation of the future",,Design Workstation of the Future,,,,core
210554045,2005-01-01T08:00:00,"Techniques of realizing CMOS continuous-time analog recurrent neural networks are studied in this paper. We discuss the application of an analogue recurrent neural network to learn and track the dynamics of an industrial robot. The observations made from this study suggest that RNNs (similar to those in Fig. 1) can be applied to the control of real systems that manifest complex properties - specifically, high-dimensionality, non-linearity and requiring continuous action. Examples of these real systems include aircraft control, satellite stabilization, and robot manipulator control. We conclude that robust controllers of partially observable (non-Markov). systems require real-time electronic systems that can be designed as single-chip Integrated Circuits (CMOS IC). This paper explored such techniques and identified suitable circuits. The synaptic weights are modeled as variable gain cells that can be implemented with a few MOS transistors. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. On-line versions of the synaptic update can be formulated using simple CMOS circuits. The simulated network contains interconnected recurrent neurons ·with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI","Edith Cowan University, Research Online, Perth, Western Australia",Analog Recurrent networks for signal tracking and control applications,,,,core
216377388,1997-01-01T00:00:00,"Este trabalho apresenta um estudo sobre a classificação de imagens digitais através da textura com o auxílio de redes neurais. São utilizadas técnicas e conceitos de duas áreas da Informática: O Processamento de Imagens Digitais e a Inteligência Artificial. São apresentados os principais tópicos de processamento de imagens, as principais aplicações em tarefas industriais, reconhecimento de padrões e manipulação de imagens, os tipos de imagem e os formatos de armazenamento. São destacados os atributos da imagem a textura e sua quantificação através da matriz de concorrência dos níveis de cinza. Também apresenta-se alguns sistemas computacionais disponíveis para processamento de imagens. Na área de Inteligência Artificial, o enfoque é para técnicas computacionais inteligentes, mais especificamente as Redes Neurais. É feita uma breve apresentação da área, incluindo seu histórico e suas principais aplicações. As redes neurais são classificadas quanto ao tipo de treinamento, à regra de aprendizado, à topologia da rede e quanto ao tipo de interconexão dos neurônios.  O modelo BPN (BackPropagation Network) é visto com maior detalhe, visto ser utilizado na implementação do sistema IMASEG (Sistema para Classificação de Imagens) que faz parte desse trabalho. O BPN é descrito quanto ao seu funcionamento, a forma de aprendizado e as respectivas equações utilizadas. O sistema IMASEG foi desenvolvido com o objetivo de implementar as técnicas propostas para a classificação de imagens utilizando textura e redes neurais. Seu funcionamento e algoritmos utilizados são detalhados e ao final, apresenta-se os resultados obtidos com a respectiva análise. A classificação de imagens é uma das principais etapas no processamento de imagens digitais. Dado um conjunto de classes e um padrão apresentado como entrada para o sistema, o problema consiste em decidir a que classe o padrão pertence. Deve haver a alternativa de rejeição do padrão. Podemos extrair da imagem atributos espectrais, espaciais e de contexto. Por serem mais facilmente quantificáveis, a maioria dos sistemas tradicionais utiliza apenas atributos espectrais para caracterizar uma imagem. Essa abordagem é muito utilizada em imagens multiespectrais.  Entretanto, utilizando apenas atributos espectrais, não se obtém uma informação completa sobre a imagem, pois não são levados em consideração as relações espaciais entre seus pixels, bem como a forma de objetos. A textura, atributo espacial, é ainda pouco utilizada, visto que tem origem na sensação visual causada pelas variações tonais existentes em uma determinada região da imagem, tornando difícil sua quantificação. Neste trabalho, é feito um estudo sobre a utilização dos atributos espaciais da imagem no seu processamento. É feita uma análise do comportamento de cinco deles: média, desvio-padrão, uniformidade, entropia e contraste, todos extraídos de janelas pertencentes à uma classe. A uniformidade, entropia e contraste provém da matriz de concorrência dos níveis de cinza. Através do cálculo do valor desses atributos em diversas imagens, constata-se que existem algumas importantes relações entre eles. A partir da análise dos diferentes modelos de redes neurais e das diversas formas de quantificar a textura de uma imagem, é proposto um sistema computacional com o objetivo de classificar imagens. Esse sistema faz o processamento das imagens através de uma janela móvel.  O usuário deve escolher o tamanho para a janela: 3x3, 5x5 ou 7x7 pixels. Essa escolha irá depender do tipo e da granularidade da textura que a imagem contém. Em seguida, utilizando a janela, deve selecionar amostras representativas de cada textura (classe) presente na imagem que se deseja classificar. O sistema então, encarrega-se de treinar a rede neural utilizando as amostras selecionadas pelo usuário. Para realizar o treinamento, é necessário encontrar uma forma de mapear os dados da realidade para a rede neural. Essa tarefa nem sempre é trivial. Nesse sistema, são propostas duas abordagens para realizar essa tarefa. Na primeira, o mapeamento é feito através do cálculo das feições da média, desvio-padrão e uniformidade, sendo esse último obtido da matriz de concorrência. Essas feições, após um escalonamento para a mesma faixa de valores, serão os parâmetros de entrada para a rede neural. Na segunda abordagem, o mapeamento é direto, ou seja, o valor de cada pixel, após o escalonamento, corresponde a uma entrada da rede neural. Após a etapa de treinamento, a imagem é processada por inteiro, fazendo-se uma varredura com a janela, gerando como saída uma imagem temática na qual cada tema representa uma das texturas existentes na imagem original.  Para testar o sistema IMASEG, foram geradas várias imagens sintéticas com 256 níveis de cinza. Deste total, foram selecionadas 6 imagens para serem apresentadas nesse trabalho. Elas são representativas das diversas situações que podem ocorrer em relação aos valores da média, desvio-padrão e uniformidade. Cada imagem original é processada pelas duas abordagens, gerando duas imagens de saída. É feita uma análise quantitativa e qualitativa dos resultados obtidos, apontando-se as prováveis causas de sucessos e problemas encontrados. Conclui-se que a classificação por textura atinge o objetivo proposto e é muito útil no processamento de imagens, levando-se em consideração os bons resultados obtidos.This paper is a study about the classification of digital images through texture with the aid of neural networks. The techniques and concepts from the field of Computer Science employed are: Digital Images Processing and Artificial Intelligence. The focus in Image Processing is on its main application in industrial tasks. pattern recognition and image manipulation, the types of images and the storing formats. The specific aspects analyzed are image attributes, texture and its quantification through the Coocurrence Matrix. Several available computing systems for image classification are presented. In Artificial Intelligence, the attention is concentrated on intelligent computational systems, more specifically on the neural networks which are briefly introduced. The subject's historical data and its main application are also addressed. The neural networks are classified according to the type of training, the learning rules, the network topology and the interconnection of neurones. The BPN model (Back Propagation Network) is examined more closely since it is employed in the implementation of the IMASEG system (classifying images system) which is part of this study.  The BPN system is described in according to its functioning capacities, the learning method and the respective equations utilized. The IMASEG system was developed with the specific aim of implementing the techniques of image classification. Throughout the paper, the system's operation and related algorithms are presented to the reader, as well as the results obtained and the analysis performed provided in the end of the paper The image classification is one of the principal steps for the processing of digital images. It consists to decide of which class the pattern belong. It can refuse the pattern. We can extract spectral, spatial and contextual image's attributes. Because they are easily quantified, a major part of the traditional systems of image processing employ only the spectral attributes to work the images and are, therefore, extensively used in the processing of multispectral images. However, the exploration of ima ges through spectral attributes is not enough to provide a complete recognition of the image since information such as spatial relations among its pixels as well as the form of objects are not taken into consideration.  The use of image processing with spatial attributes is also considered in this paper. Texture is still not a commonly employed attribute. This is due to the fact that its based on visual sensation which is produced by the existing tonal variations of a specific image region, making its quantification a difficult task to perform. A behavior analysis of the spatial attributes under consideration in this paper are the following: mean, standard deviation, uniformity, entropy and contrast. These five attributes were all taken from windows belonging to a single class. Uniformity, entropy and contrast are issued from the gray level coocurrence matrix. Via a calculation of the value of these attributes is observed that there is an important relationship among them. This paper proposes a system of image classification based on the analysis of different models of neural networks and also through the analysis of the diverse ways of quantifying the texture of an image. This system performs the image processing through a shifting window. Then, the user must choose the window's size from among the following dimensions: 3x3, 5x5 or 7x7 pixels.  The choice will vary depending on the type and on the image's texture granularity. The selection of meaningful samples of each texture (class) present in the image one wishes to classify is the next step in the process. The system, then, is in charge of training the neural networks by applying the user's selected samples. In order to perform the training, it is necessary to first establish a way of mapping the data reality to the neural network, oftentimes a difficult task. In this system two approaches are proposed for the execution of this task. In the first, the mapping is done through the calculation of the mean, standard deviation and uniformity features. The last item is obtained from the coocurrence matrix. After these features have been scaled to the same value band, they will become the input to the neural networks. In the second approach, it is expected that the neural network will be able to extract textures attributes without executing an explicit calculation exercise.  After the training phase, the image is completely processed through a window scanning generatin g a thematic image as the output onto which each theme will represent one of the texture's original image. In order to verify the adequacy of the IMASEG system, several synthetical graylevel images were created. Of these, 7 images were chosen as objects for this analysis, representing the various possible situations that might occur in relation to the average, standard deviation and uniformity. Each original image is processed in according with these two chosen approaches, thus generating two images as outputs, as well as a quantitative and qualitative analysis of the obtained results, pointing to the probable successes and failures generated. The final conclusion is that the classification through texture partially attains the proposed objectives and can be very useful in the processing of images, serving as an aid in the traditional classification process",,Classification of di gital images through texture with the aid of neural networks,,,,core
53791900,1997-01-01T00:00:00,"This volume contains selected papers from WIRN VIETRI-96, the 8th
Italian Workshop on Neural Nets, held Vietri sul Mare, Salerno,
Italy, from 23-25 May 1996. The papers cover a variety of topics
related to neural networks, including pattern recognition, signal
processing, theoretical models, applications in science and industry,
virtual reality, fuzzy systems, and software algorithms. By providing
the reader with a comprehensive overview of recent research work in this area, the volume makes an invaluable contribution to the
Perspectives in Neural Computing Series. Neural Nets WIRN VIETRI-96 will provide invaluable reading material for anyone who needs to keep up to date with the latest developments in neural networks and related areas. It will be of particular interest to academic and industrial researchers, and postgraduate and graduate students",Springer-Verlag Telos,Neural Nets Wirn Vietri-96: Proceedings of the 8th Italian Workshop on Neural Nets,,,,core
23793716,1996,"ARCHON ™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest ever project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Two of these applications, electricity transportation management and particle accelerator control, have been run successfully on-line in the organisation for which they were developed (respectively, Iberdrola an electricity utility in the north of Spain and CERN the European Centre for high energy physics research near Geneva). This paper recounts the problems, insights and experiences gained whilst deploying ARCHON technology in these real-world industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the key design forces which shape work in this important domain. Secondly, the ARCHON framework is described- with a special emphasis being placed upon the implementation architecture. Thirdly, detailed descriptions of the Iberdrola and CERN applications are given- the motive for a DAI approach is outlined, the multiple agent systems which were built are described, and the benefits which accrued are stated. Finally, the lessons distilled from this work are discussed so that the engineers of future DAI systems may profit from our experiences. 1",,Using ARCHON to develop real-world DAI applications for electricity transportation management and particle accelerator control,,,,core
293444193,1997-01-01T00:00:00,"Este trabalho apresenta um estudo sobre a classificação de imagens digitais através da textura com o auxílio de redes neurais. São utilizadas técnicas e conceitos de duas áreas da Informática: O Processamento de Imagens Digitais e a Inteligência Artificial. São apresentados os principais tópicos de processamento de imagens, as principais aplicações em tarefas industriais, reconhecimento de padrões e manipulação de imagens, os tipos de imagem e os formatos de armazenamento. São destacados os atributos da imagem a textura e sua quantificação através da matriz de concorrência dos níveis de cinza. Também apresenta-se alguns sistemas computacionais disponíveis para processamento de imagens. Na área de Inteligência Artificial, o enfoque é para técnicas computacionais inteligentes, mais especificamente as Redes Neurais. É feita uma breve apresentação da área, incluindo seu histórico e suas principais aplicações. As redes neurais são classificadas quanto ao tipo de treinamento, à regra de aprendizado, à topologia da rede e quanto ao tipo de interconexão dos neurônios.  O modelo BPN (BackPropagation Network) é visto com maior detalhe, visto ser utilizado na implementação do sistema IMASEG (Sistema para Classificação de Imagens) que faz parte desse trabalho. O BPN é descrito quanto ao seu funcionamento, a forma de aprendizado e as respectivas equações utilizadas. O sistema IMASEG foi desenvolvido com o objetivo de implementar as técnicas propostas para a classificação de imagens utilizando textura e redes neurais. Seu funcionamento e algoritmos utilizados são detalhados e ao final, apresenta-se os resultados obtidos com a respectiva análise. A classificação de imagens é uma das principais etapas no processamento de imagens digitais. Dado um conjunto de classes e um padrão apresentado como entrada para o sistema, o problema consiste em decidir a que classe o padrão pertence. Deve haver a alternativa de rejeição do padrão. Podemos extrair da imagem atributos espectrais, espaciais e de contexto. Por serem mais facilmente quantificáveis, a maioria dos sistemas tradicionais utiliza apenas atributos espectrais para caracterizar uma imagem. Essa abordagem é muito utilizada em imagens multiespectrais.  Entretanto, utilizando apenas atributos espectrais, não se obtém uma informação completa sobre a imagem, pois não são levados em consideração as relações espaciais entre seus pixels, bem como a forma de objetos. A textura, atributo espacial, é ainda pouco utilizada, visto que tem origem na sensação visual causada pelas variações tonais existentes em uma determinada região da imagem, tornando difícil sua quantificação. Neste trabalho, é feito um estudo sobre a utilização dos atributos espaciais da imagem no seu processamento. É feita uma análise do comportamento de cinco deles: média, desvio-padrão, uniformidade, entropia e contraste, todos extraídos de janelas pertencentes à uma classe. A uniformidade, entropia e contraste provém da matriz de concorrência dos níveis de cinza. Através do cálculo do valor desses atributos em diversas imagens, constata-se que existem algumas importantes relações entre eles. A partir da análise dos diferentes modelos de redes neurais e das diversas formas de quantificar a textura de uma imagem, é proposto um sistema computacional com o objetivo de classificar imagens. Esse sistema faz o processamento das imagens através de uma janela móvel.  O usuário deve escolher o tamanho para a janela: 3x3, 5x5 ou 7x7 pixels. Essa escolha irá depender do tipo e da granularidade da textura que a imagem contém. Em seguida, utilizando a janela, deve selecionar amostras representativas de cada textura (classe) presente na imagem que se deseja classificar. O sistema então, encarrega-se de treinar a rede neural utilizando as amostras selecionadas pelo usuário. Para realizar o treinamento, é necessário encontrar uma forma de mapear os dados da realidade para a rede neural. Essa tarefa nem sempre é trivial. Nesse sistema, são propostas duas abordagens para realizar essa tarefa. Na primeira, o mapeamento é feito através do cálculo das feições da média, desvio-padrão e uniformidade, sendo esse último obtido da matriz de concorrência. Essas feições, após um escalonamento para a mesma faixa de valores, serão os parâmetros de entrada para a rede neural. Na segunda abordagem, o mapeamento é direto, ou seja, o valor de cada pixel, após o escalonamento, corresponde a uma entrada da rede neural. Após a etapa de treinamento, a imagem é processada por inteiro, fazendo-se uma varredura com a janela, gerando como saída uma imagem temática na qual cada tema representa uma das texturas existentes na imagem original.  Para testar o sistema IMASEG, foram geradas várias imagens sintéticas com 256 níveis de cinza. Deste total, foram selecionadas 6 imagens para serem apresentadas nesse trabalho. Elas são representativas das diversas situações que podem ocorrer em relação aos valores da média, desvio-padrão e uniformidade. Cada imagem original é processada pelas duas abordagens, gerando duas imagens de saída. É feita uma análise quantitativa e qualitativa dos resultados obtidos, apontando-se as prováveis causas de sucessos e problemas encontrados. Conclui-se que a classificação por textura atinge o objetivo proposto e é muito útil no processamento de imagens, levando-se em consideração os bons resultados obtidos.This paper is a study about the classification of digital images through texture with the aid of neural networks. The techniques and concepts from the field of Computer Science employed are: Digital Images Processing and Artificial Intelligence. The focus in Image Processing is on its main application in industrial tasks. pattern recognition and image manipulation, the types of images and the storing formats. The specific aspects analyzed are image attributes, texture and its quantification through the Coocurrence Matrix. Several available computing systems for image classification are presented. In Artificial Intelligence, the attention is concentrated on intelligent computational systems, more specifically on the neural networks which are briefly introduced. The subject's historical data and its main application are also addressed. The neural networks are classified according to the type of training, the learning rules, the network topology and the interconnection of neurones. The BPN model (Back Propagation Network) is examined more closely since it is employed in the implementation of the IMASEG system (classifying images system) which is part of this study.  The BPN system is described in according to its functioning capacities, the learning method and the respective equations utilized. The IMASEG system was developed with the specific aim of implementing the techniques of image classification. Throughout the paper, the system's operation and related algorithms are presented to the reader, as well as the results obtained and the analysis performed provided in the end of the paper The image classification is one of the principal steps for the processing of digital images. It consists to decide of which class the pattern belong. It can refuse the pattern. We can extract spectral, spatial and contextual image's attributes. Because they are easily quantified, a major part of the traditional systems of image processing employ only the spectral attributes to work the images and are, therefore, extensively used in the processing of multispectral images. However, the exploration of ima ges through spectral attributes is not enough to provide a complete recognition of the image since information such as spatial relations among its pixels as well as the form of objects are not taken into consideration.  The use of image processing with spatial attributes is also considered in this paper. Texture is still not a commonly employed attribute. This is due to the fact that its based on visual sensation which is produced by the existing tonal variations of a specific image region, making its quantification a difficult task to perform. A behavior analysis of the spatial attributes under consideration in this paper are the following: mean, standard deviation, uniformity, entropy and contrast. These five attributes were all taken from windows belonging to a single class. Uniformity, entropy and contrast are issued from the gray level coocurrence matrix. Via a calculation of the value of these attributes is observed that there is an important relationship among them. This paper proposes a system of image classification based on the analysis of different models of neural networks and also through the analysis of the diverse ways of quantifying the texture of an image. This system performs the image processing through a shifting window. Then, the user must choose the window's size from among the following dimensions: 3x3, 5x5 or 7x7 pixels.  The choice will vary depending on the type and on the image's texture granularity. The selection of meaningful samples of each texture (class) present in the image one wishes to classify is the next step in the process. The system, then, is in charge of training the neural networks by applying the user's selected samples. In order to perform the training, it is necessary to first establish a way of mapping the data reality to the neural network, oftentimes a difficult task. In this system two approaches are proposed for the execution of this task. In the first, the mapping is done through the calculation of the mean, standard deviation and uniformity features. The last item is obtained from the coocurrence matrix. After these features have been scaled to the same value band, they will become the input to the neural networks. In the second approach, it is expected that the neural network will be able to extract textures attributes without executing an explicit calculation exercise.  After the training phase, the image is completely processed through a window scanning generatin g a thematic image as the output onto which each theme will represent one of the texture's original image. In order to verify the adequacy of the IMASEG system, several synthetical graylevel images were created. Of these, 7 images were chosen as objects for this analysis, representing the various possible situations that might occur in relation to the average, standard deviation and uniformity. Each original image is processed in according with these two chosen approaches, thus generating two images as outputs, as well as a quantitative and qualitative analysis of the obtained results, pointing to the probable successes and failures generated. The final conclusion is that the classification through texture partially attains the proposed objectives and can be very useful in the processing of images, serving as an aid in the traditional classification process",,Classification of di gital images through texture with the aid of neural networks,,,,core
51097781,1997-01-01T08:00:00,"The recent technological changes in computer and industrial control systems have been steadily extending the capabilities to handle a broad range of complex systems. The emergence and development of computer technology and intelligent systems during the past few decades have created a highly promising direction in the field of artificial intelligence. It is increasingly difficult to describe any real system as the level of complexity continues to increase. A combination of systems and techniques are necessary to solve many complex problems. This new direction involves the use of fuzzy logic and artificial neural network theory to enhance the ability of intelligent systems that can learn from experience and to adapt to changes in an environment of uncertainty and imprecision.
The Intelligent Automotive Collision Warning System was developed as a rule based system by integrating a fuzzy logic controller with artificial neural network software. The Intelligent Automotive Collision Warning system constantly monitors the speed of the vehicle and the distance of any object in front of the vehicle using an ultrasonic ranging module to warn the operator to maintain a safe operating distance by using fuzzy logic theory and artificial neural network software.
Descriptive statistics was used for collecting and organizing the data. Inferential statistics was used to prove the hypotheses based on the results of the collected data. NeuFuz4 software was used to train the neural network and to optimize the fuzzy rule base. The fuzzy logic technology provided a means of converting a linguistic control strategy to operate the warning system. The input/output relationship was defined by fuzzy membership functions which enabled the numerical inputs to be expressed as fuzzy variables using linguistic terms. A new fuzzy logic operator was also developed to optimize the fuzzy input/output relationship",NSUWorks,Intelligent Collision Warning System Based on Fuzzy Logic and Neural Network Technologies,,,,core
376547930,1999-01-01T08:00:00,"Since the widespread use of computers in business and industry, a lot of research has been done on the design of computer systems to support the decision making task. Decision support systems support decision makers in solving unstructured decision problems by providing tools to help understand and analyze decision problems to help make better decisions. Artiﬁcial intelligence is concerned with creating computer systems that perform tasks that would require intelligence if performed by humans. Much research has focused on using artiﬁcial intelligence to develop decision support systems to provide intelligent decision support.
Knowledge discovery from databases, centers around data mining algorithms to discover novel and potentially useful information contained in the large volumes of data that is ubiquitous in contemporary business organizations. Data mining deals with large volumes of data and tries to develop multiple views that the decision maker can use to study this multi-dimensional data. On-line analytical processing (OLAP) provides a mechanism that supports multiple views of multi-dimensional data to facilitate efficient analysis. These two techniques together can provide a powerful mechanism for the analysis of large quantities of data to aid the task of making decisions.
This research develops a model for the real time process control of a large manufacturing process using an integrated approach of data mining and on-line analytical processing. Data mining is used to develop models of the process based on the large volumes of the process data. The purpose is to provide prediction and explanatory capability based on the models of the data and to allow for efﬁcient generation of multiple views of the data so as to support analysis on multiple levels. Artiﬁcial neural networks provide a mechanism for predicting the behavior of nonlinear systems, while decision trees provide a mechanism for the explanation of states of systems given a set of inputs and outputs. OLAP is used to generate multidimensional views of the data and support analysis based on models developed by data mining. The architecture and implementation of the model for real-time process control based on the integration of data mining and OLAP is presented in detail. The model is validated by comparing results obtained from the integrated system, OLAP-only and expert opinion. The system is validated using actual process data and the results of this veriﬁcation are presented. A discussion of the results of the validation of the integrated system and some limitations of this research with discussion on possible future research directions is provided",VCU Scholars Compass,A model to integrate Data Mining and On-line Analytical Processing: with application to Real Time Process Control,https://core.ac.uk/download/376547930.pdf,,,core
1503431,2000-01-01T00:00:00,"Support Vector Machines (SVM's) and other kernel based methods have grown in popularity in recent years. Although they have many benefits, such as the ability to deal with a large number of parameters, one drawback of these successful techniques is their lack of the ability to provide rigorous confidence measures for the predictions they make. This thesis is devoted to the efficient implementation and experimental testing of transductive algorithms developed at the computer science department, Royal Holloway. The algorithms are tested against several benchmark data sets, and methods for comparing quantitative confidence values are described and evaluated. These techniques and other machine-learning methods are also applied to the industrial application of fault diagnosis and automated repair. An extensive case study of applying these machine learning techniques to a real-world problem is carried out. Many problems such as data collection and representation -- which are common to most real-world applications of machine learning techniques, but sometimes over-sighted in literature -- are highlighted and discussed",,Efficient Implementation and Experimental Testing of Transductive Algorithms for Predicting with Confidence,,,,core
4964288,2002-01-01T08:00:00,"The sector of industrial facility construction has been experiencing unsuccessful project implementations for a long time. Both the industry and the academic world have realized the significant impacts of engineering activities on the success of project implementation. Improved engineering design performance leads to better project outcomes. However, industrial construction projects are complex processes involving large number of input and output. Therefore, first of all comes the need to understand how well the engineering activities are performed. Researchers and industry experts have been making efforts in measuring engineering performance. Better understanding of engineering performance lays the foundation for stepping forward to seek ways to improving engineering performance. Former studies on engineering performance improvement have focused on the promotion of certain techniques or products, or looked at specific engineering processes or areas. Few tried to make contribution to the whole facility development process. There is a lack of a systematic and analytical approach that improves engineering performance based on the understanding of the cause-effect relationships between engineering input and performance output from the perspective of the whole facility development process. This research proposes a neurogenetic system, which integrates genetic algorithms with artificial neural networks, for modeling engineering performance measurement and improvement in industrial construction projects. The system starts with a neural network model for establishing the cause-effect relationship between engineering input factors and engineering performance output measures. Because of its robust and efficient searching ability in complicated situations, genetic algorithms are employed to search for better engineering performance; the fitness function for the genetic search is the neural network model that predicts engineering performance. To make suggestions for possible engineering performance improvement, the research introduces the self-comparison evaluation that evaluates a project\u27s engineering performance by comparing its actual engineering performance with its possible better engineering performance generated by the genetic search. Using real project data, the research developed and tested the proposed system. The testing produced significant results that demonstrated the plausibility of the GA-ANN integration in seeking the potential engineering performance and illustrated how the self-comparison concept could provide unique, project-specific, and objective engineering performance evaluation",'Purdue University (bepress)',Engineering performance improvement based on the integration of genetic algorithms and artificial neural networks,,,,core
44249545,2005-01-01T00:00:00,"Neural networks represent a powerful data processing technique that has reached maturity and broad application. When clearly understood and appropriately used, they are a mandatory component in the toolbox of any engineer who wants make the best use of the available data, in order to build models, make predictions, mine data, recognize shapes or signals, etc. Ranging from theoretical foundations to real-life applications, this book is intended to provide engineers and researchers with clear methodologies for taking advantage of neural networks in industrial, financial or banking applications, many instances of which are presented in the book. For the benefit of readers wishing to gain deeper knowledge of the topics, the book features appendices that provide theoretical details for greater insight, and algorithmic details for efficient programming and implementation. The chapters have been written by experts ands seemlessly edited to present a coherent and comprehensive, yet not redundant, practically-oriented introduction",'Springer Science and Business Media LLC',Neural Networks: Methodology and Applications,,10.1007/3-540-28847-3,,core
101991114,2004,"Geographic Information Systems (GIS) and Computer Aided Facility Management-Systems (CAFM) are currently undergoing the transition to storing and processing real 3D geospatial data. Applications for this type of data are, among others, location based services, navigation systems and the planning of large-scale construction projects. For presentation purposes and especially when working in the field, powerful visualisation systems are needed that are also capable of running on mobile devices like notebooks, personal digital assistants (PDA) or even cell phones. In such application areas, the free movement of the viewer’s position and the interaction with the data are of great importance. Real-time visualisation of 3D geospatial data is already well established and also commercially successful in the entertainment industry, namely in the market of 3D video games. The development of software in this field is very cost-intensive, so that the packages are often used for several game products and are therefore universally applicable to a certain extend. These so-called game engines include not only visualisation functionality, but also offer physics, sound, network, artificial intelligence and graphical user interfaces to handle user in- and output. As certain portions or sometimes even the whole engine are released as open source software, these engines can be extended to build more serious applications at very little costs. The paper shows how these game engines can be used to create interactive 3D applications that present texture-mapped geospatial data. The integration of 3D data into such systems is discussed. Functionality like thematic queries can be implemented by extending the internal data structures and by modification of the game’s accompanying dynamic link libraries",,Visualisation Using Game Engines. The,,,,core
2776809,2004-01-01T00:00:00,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy",'Elsevier BV',Modelling SO2 concentration at a point with statistical approaches,,10.1016/j.envsoft.2003.10.003,,core
41534236,2005-01-01T08:00:00,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI","Edith Cowan University, Research Online, Perth, Western Australia",An analogue recurrent neural networks for trajectory learning and other industrial applications,https://core.ac.uk/download/41534236.pdf,,,core
161115193,2006-01-01T00:00:00,"The multi-agent paradigm for building intelligent systems has gradually been accepted by researchers and practitioners in the research field of artificial intelligence. There are also attempts of adapting agents and agent-based systems for creating industrial applications and providing e-services. In this paper, we present an attempt to use agents for constructing an online after-sale services system. The system is decomposed into four major cooperative agents, and in which each agent concentrates on particular aspects in the system and expresses intelligence by using various techniques. The proposed agent-based framework for the system is presented at both the micro-level and the macro-level according to the Gaia methodology. UML notations are also used to represent some software design models. As the result of this, agents are implemented into a framework for which exploits Case-Based Reasoning (CBR) technique to fulfil real life on-line services' diagnoses and tasks",'Institute of Electrical and Electronics Engineers (IEEE)',Towards an agent-based framework for online after-sales services,https://core.ac.uk/download/161115193.pdf,10.1109/IS.2006.348456,,core
235573990,2000-08-16T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH NEWS
Published Weekly Since 1932 by
The Magrath Trading Company
Wednesday August 16, 2000
35 cents
The Other Side Of Emergency Services!
The parameds in our town have a lot more to do than wait for the
hospitals to call to take someone in to Lethbridge, although that is
equally as important. Izm talking about the 12 active volunteers who
work with the Ambulance services.
I spoke with Ken Hoibac who is the Ambulance Services Coordinator
and he informs me that tiie 12 volunteers consist of 7 EMT's, 3 EMR's
and 2 drivers which was established in 1986. An EMT, or Emergency
Medical Technician, must take a 1 year training course which quali­fies
them to start IV's, administer medication and use the defibrilla­tor
along with other skills. The EMR or Emergency Medical Re­sponder,
received 5 months of training to be able to assist the emer­gency
response team. All are skilled under the College of Paramedics.
On the average the Response Team receives about 200 calls a year.
""We^have probably the largest response area covering 2000 kilome­ters"",
says Ken. When you call the emergency line it goes to the Hos­pital
which can also be sent to a cell phone directly to the staff. Ken
also hopes to be able to take advantage of the 911 services as other
communities have and incorporate our line to theirs.
An average volunteer has 3500 hours a year which roughly works
out to 1 and a half full time jobs. Many work during the day and then
take on the volunteer shift at night. ""There was an increasing need
for Rescue equipment as well as services so we started fund raising
to be able to buy equipment such as the ""JAWS"" and equipment to
help them on their water, ice, rope and motor vehicle rescues."" Since
'92 they have raised $130,000 towards new equipment including am­bulance
and a boat for water rescues. They will qualify for Wild Rose
funding and the next big fund raiser will be in April of next year.
They hope to raise enough for additional equipment. ""We have the
best equipment and the best staff to be able to maintain rescues and
services. Most have stayed since the beginning. Over half. There is a
waiting list for volunteering.""
Our Rescue services staff is so skilled that they are asked to go
train other Fire Departments and Rescue Service Teams. So if you
every have the need of Rescue or Emergency Services rest assured
that you are in the very best hands in Alberta.
*Just my opinion, Melanie Stone*
Special points of in­terest:
•Youth For Action
•Sports updates
•Trading Company Gro­cery
Specials
•Home Hardware Sale
Inside this issue:
SPORTS 2
Community interests 3&
4
Real Estate &
Classifieds
5-6
More Classifieds &
Free Market
7
Trading Company
Meats and Produce
Specials
8&
9
Trading Company
Grocery Specials
10
II
Home Hardware
Specials
12
J
ALBERTA WINS SILVER!
The Alberta Provincial Under 17 Basketball Team won the silver medal at the Canadian Champi­onships held in Langley, B.C. last week. The Alberta squad with Magraths Jimmy Balderson won 5 straight games before losing to Ontario 73-64 in the Championship Final. Jimmy Balderson contributed 17 points in the final game and was named to the tournaments first ALL STAR Team.
MAGRATH CONTINUING CARE will be holding a silent auction Au­gust 22-29. All proceeds go to tiie residents council. We would like to invite everyone to come and place a bid on numerous items. Call the Rec Therapy Department 758-3371
EARLY BIRTHPAY CELEBRATION!
Alice ""Sparky""Stevenson Family & friends are invited to honor her by coming to the Magrath Seniors Centre Wed Aug 30, 2000 from 7-9 pm. No gifts
NATURE RESERVE VANDALIZED!
Over the August long weekend a person or persons went int o the Ma­grath Nature Reserve and tore down a steel goose-nesting platform that was standing in water. This structure was located west of the spill gate towards the golf course. This platform has consistently been used by geese to raise a gaggle of goslings over the years and has provided a unique wildlife viewing experience for people using the area. This par­ticular goose nesting platform require three men to put up, so whomever tore it down had to work at it for sometime.
It is always disturbing to see an area vandalised when particularly vol­unteers created a ""Jewwl"" of a Natural Area on the edge of a town. The Fel Balderson Nature Reserve was developed and is maintained by the Magrath Rod & Gun Club named after a well-known local conservationist. Fel Balderson in conjunctionwith the Magrath Rod & Gun Club wanted to provide a Natural area that could be shared by both the wildlife and peo­ple of Magrath and area. The vandalism not only destroyed a nesting site for geese but it also is and offence against the Criminal code and violates provision of the Wildlife Act and Migratory Birds Convention Act Anyone having knowledge of whoo destroyed this nesting platform is asked to contact The Natural Resources Office in Cardston 653-5158•
WHAT DID YOU DO THIS SUMMER?
•
PageS
Have you ever heard of YOUTH FOR ACTION? Well a few youth from around town did at a Town Council Meeting in January of this year and it sparked some interest. The pur­pose of this Youth group was to raise awareness about the break down of the Family. Shani Hatch filled me in. After hearing about it in the Town Council Mtg in January she became interested but when she attended a Youth Fireside in Lethbridge it really got her going. Beside being advocates against Child Pornography she, Dave Mendenhall, Con­nor Low & Kyle Clifton along with 13 others from Southern Alberta attended the Youth Conference in L.A. Leaving July 20th in a 1 5 person van for a 22 hour ride she tells me was “So much fun.” Arriving in L.A. they met representatives from all over the world. “There was Canada, United States, Hungary, Sierra Leone and Kingdom of Swaziland. There were a lot more who were supposed to attend but their VISAs’ were denied. One group from Nigeria were in a car accident while on their way to show their VISAs’.” There were 5 areas of Working Committees to volunteer on: Special Issues, Violence Crime & Drug Abuse, Poverty, Education, Social & Health Issues. “Everything we wanted to put in the Plan, everyone seemed to agree with. The youth just worked so smoothly together.” It makes you feel good that the youth are taking such an active role to help Preservation of the Family. There seems to be a lot to be proud of in our Community.
We would like to thank many people and businesses that so gen­erously helped us in any way that they could to make our trip to the Young General Assembly a success. We will forever be grate­ful to you, for your support and prayers. Thanks once again Shani Hatch, David Mendenhall, Connor Low, Kyle Clifton
THE RAYMOND AND DISTRICT
Magrath has various spaces for rent ideal for small busi"" nesses ph 758-3876
DAHL FRUIT *Arriving weekly*
TO ORDER CALL BY SUNDAY EVE EVERY WEEK FOR ORDERS ARRIVING AUGUST 9,16,23 & 30 KATHY 758-3039 OR LYNDSY 758-6357.
Tomatoes 26 lb/ $18 * Nectarines 261b/$20 Walla Walla Sweet Onions 50lb/$18
Free Stone Peaches 26lb/$20 Melons 26lb/$18 or purchase one individually
Melons 26lb/$l8 or one individually
If no answer please leave a message.
AGRICULTURAL SOCIETY WILL BE
HOLDING THEIR 13TH ANNUAL
FALL FAIR AND HORSE SHOW, FRI AUG 25 & SAT AUG 26 AT THE AG SOCIETY COMPLEX IN RAYMOND ENTRY FEE. FRI 9-1 BENCH DISPLAY ENTRIES ACCEPTED, GRAINS, BAKING, PRESERVES, THREAD, FABRIC, GENERAL & FINE ARTS, ETC JUDGING 2PM
SAT $2 PANCAKE BREAKFAST, HORSE SHOW, PET SHOW, BENCH SHOW VIEWING & ENTERTAINENT. INFO? DONNA 752-4957 MARY 752-5661MAGRATHNEWS
COMMUNITY INTERESTS
FRED RIEL IS 95 ON
AUGUST 26, 2000.
THERE IS AN OPEN
HOUSE FROM 3-5 PM
AT THE SENIORS
CENTRE. EVERYONE
WELCOME. NO GIFTS
PLEASE.
SHIRLEY & DORAN FERRY
CELEBRATE THEIR
25TH ANNIVERSARY
TODAY AUG16
CONGRATULATIONS
WE LOVE YOU!
TRACI, MICHELLE, JAMES
& SHAWN
Tired of the same recipes?
Why not try the cook book
that Cheryl Rasmussen
Webster compiled of
“Family & Friends” recipes.
If you would like one for
yourself or to give one to a
friend you can place an or­der
with Sadie in the Hard­ware
Department. Price is
$6.00 or you can phone
758-3545.
Please come help
Merrill and Lise
Thomson celebrate
their 30th wedding
anniversary at an
Open House Sept 5
7“9pm an the Thom­son
farm. Ho gifts
I
JENSEN ENTERPRISES
KENJENSEN
758-3669
Roofing
Carpentry
Siding
Additions
New Homo Constzucbon
1 Finishing Carpentry
} J J 20 yean looting eipe* vret
T3 yean wtfi foe wedwrt
The library has a lot of excit­ing
new books available in
ADULT rtOH FICTION,
ADULT FICTION, JUVENILE,
CHILDREN'S and VIDEOS
stop by and check one out
today.
Magrath, Alberta
SENIORS MEWS CENTRE
The corn is ready! The
Seniors will hold their
corn feed Wed Aug 23 at
the Seniors Centre 5pm
the hostesses are Maxine
Kraft and Jean Butlin
Bookings for the Senior’s
Centre call Jack or Jean
Butlin 758-3030
Rental Fee is $75
We still have some bus
seats for Olsens’ on Au­gust
26th which leaves at
5pm sharp. $23.00 Please
pay Norma Owens at
150-1 Ave N or Hazel 37 -
2 St W as soon as possible
or call:
Le Vaun 758-6672
Norma 758-3560
Hazel 758-3545
BINGO* BINGO* BINGO
NEXT BINGO IS TOMOR­ROW
AUGUST I7r 2000 AT
THE SENIORS CENTRE.
BOORS OREN AT 6:30PM
BINGO AT 7PM EVERY­ONE
WELCOME!
® • ^k
James W. Smith will be return-
PageS
FAMILY FUN NIGHT
ing home from Montreal, Can­ada mission soon. He will be speaking in the Stake Centre during Magrath 4th ward sac­rament at 9 a.m. August 20
The Magrath United Church will not be holding services during the month of Aug as Rev. Dave will be on holidays. Rev. Leroy Angle of Milk River will be available for emergencies at 647-3983. They will reconvene Sunday Sept 3 at 9:30a.m.
DEEP SOUTH CLOGGERS are now in progress for registering the fall sessions September 11- November 30. Cost is $50/ session (competition track $75) Late fee of $5 applies after registration date of August 18 For more information call: Jennifer Sabey 758'3327 Kristine Alston 758'3627
South Country Jamboree Society will be holding their regular jam on Sun Aug 27, 702 Wing at the
Airport. Meeting at 12:30 pm, jam at 2pm. Members $2 and non-members $4
Welling is having a Family Fun Night Aug 26 Fun starts at 4pm with kids & adult races. There will be Face painting and activities for little kids as well. Followed by a BBQ $3/ person includes Pop, Chips, Hamburger & Com. Call Karla to register for the races and more information at 329-6402
Magrath Public Library
Store Netfs
August 9, 2000
Short -term ‘Ÿouth Employment Opportunity at Community Internet Site: Magrath Public Library Duties and Responsibilities:
Assist businesses and individuals -With internet usage
Promote internet usage
Prepare reports for submission to goVemement QUALIFICATIONS
Must be behVeen 15-30 years of age
Ability to teach internet skills to community members
Good communication skdlsfxVriting, speaking, listening)
Strong forking knowledge of internet, email, computer hardware & software
DURATION
12 -Weeks
Deadline AUGUST 18,2000
Submit applications to C. Cunningham, Magrath Public Library, P.O. Box 295, Magrath, AB. T0KU0 or drop off at Magrath Public Library, bufuiries contact 758-6498
Funding provided by Industry Canada
L
ÏCLASSIFIEDS AUGUST 2- 9, 2000
FREE MARKET
*Left in my car in error after the Hough Anniversary party — 2 black plastic holders call 758-3156
*Strayed to our house a small Siamese Kitten call 758-3156
*We loaned our new 'SAWZALL' to someone in town fit we sure could use it Please ph 758-3079 Sheldon Gurney
‘LOST Black fit Red checked blanket and a blue Baseball cap with a 'Z' on the front call Bums or Karen Alston 758-6894
‘Needed 1 crock used for making soap or pickles etc. 20 litre minimum call Charley Sheridan 328­2460
BUY fit SELL
*4 sale Acklands“ Ag 300. 300 Amp welder with 3 kv built in generator. In excellent running order Call Rick 758-6427
*4 salel green CCM Mountain Bike 18 speed, good condition (adult) $60.00 obo also - 1 blue CCM Mountain Bike 18 speed, good condition $65.00 also - Go cart, new paint Job, row bar. Good condition. $250.00 Perry""s 758""3638
*4 sale 1 gold 30' Enterprize gas kitchen stove. In excellent condition Ph 758-3545
*4 sale DB bed in excellent condition $75 OBO phone Rhonda 758-6761
*4 sale Trombone make an offer Bums or Karen Alston 758-6894
*4 sale Magic Cooking Bag/Box $30 phone Pat Harrison 758-3714
GARAGE SALE
******
*Yard Sale Fri Aug 18 4“8 #10 3 st SE the Ehlerts new hemp fit leather jewellery. Antique canisters db bed in the bag, T.V. microwave, tripod, blue jeans, mens dress shirts, luggage, misc.
*Seear Family Garage Sale Sat Aug 19 9""2 139 S 2 st W (next to Jim fit Dayna Blumel) kitchen odds 8? ends 7.5 horsepower long shaft boat motor, hid a bed, microwave fit other household goods.
‘Sat Aug 19 south of Treasury Branch baby swing, stroller, playpen, fartrel, wool, glassware, crafts, records, lots more weather permitting
BUSINESS
******
HELP WANTED
‘YINGS RESTAURANT is opening soon Part time waitresses are needed Apply in person to Ying at the Restaurant Phone for appointment 758-3198
*
Magrath Credit Union requires a p/t member service rep. Related exp preferred. Please contact Kathryn Blackmer 758“3477. 8/2
‘Help Wanted light housekeeping fit laundry 758­3832
FOR HIRE
******
*
DAHLS' HANDIMAN SERVICES, YARD MAINTENANCE, ROTOTILLING, LAWN MOWING, HAULING TRASH 758­6264
*Bennett's lawn mowing business, riding mower, push mower fit trimmer. 758-6222 Jonathan, Lauren fit Russell l.r
*Vanderleek Roofing Services are reliable, quality service, fully certified, insured, ticketed fit a 10 year business. 758-6324 Mike or 328-7744
*A Vanderleek roof is a no leak roof!"" *Music Lessons 1 have opening for piano fit singing lessons Contact Erica Thomson @ 758“ 6324 l/msg if not home
‘Will tutor grade 1-12 Math or Chemistry call Martine Rollingson 758-3648
‘if your child needs help in Math or Reading the Kumon Method might help. For more info call Rusty or Martine Rollingson 758-3648| CLASSIFIEDS COHTinUED . , .
Jeanne's Hair Fashion 136 S 1 St West 4
doors South of the Trading Company 758""
3379 Open Tues thru Fri
Professional Hair care at pleasing prices
TSEED YOUR FEED BARLEY MOVED?
Will also haul hay or straw. Also can haul
your quota for you. Sabey Trucking 758""
3119 or 308-1944
CANADIAN SECURITY SYSTEMS
We sell, install service alarms, safes,
camera systems, dead bolts & key locks
Call Ross Moore 758""3945 free estimate.
Will do General Bobcat Loader Work Post
holes, Back fill. Garage Pads, Driveway
Phone 758""3278
For all your cleaning needs, from hospital
clean to a touch up, carpet to ceiling &
everything in between. Ho job too big or
too small Call Wayne's Carpet & Upholstry
Cleaning 758-6414
VANDERLEEK
~ ROOFING SERVICES ~
• RELIABLE • QUALITY • SERVICE
“A VANDERLEEK ROOF
IS A NO LEAK ROOF“
_______ 328-7744
Ask for Mike
REAL ESTATE
*4 sale older 3 bdrm 1236 sq ft bungalow
(Ragans) Ho bsmt 20x26 ft shop/garage
& root cellar/garden area 758""3427
*MOVINGI MUST SEELIE
12'x68' mobile home, recently renovated, 2
bdrm, 1 bath/laundiy, new fridge & lino,
newer carpets, good appliances, nice sized
deck. MAKE US AH OFFER Leavitt's 758­6855
*House for sale east Magratfi, 3 bdrms plus
loft. Currently half renovated, needs
handyman. Rent to Own. Ho deposit 4
renovator, immed possession. $29,900
(604) 824-6787
*2 bedroom house for rent 758-3198
*4 rent 2 bdrm, furnished, abstainers, no
pets call Ty Alston 758-3322
USED VEHICLES
******
*4 sale 90 Chrysler Dynasty 4 door/dark
blue. Cracked engine block. Parked in
Magrath. $350 or offers (604) 824""6787
7/19
*4 sale 96 Ford F350 crewcab, 4x4,
212,000 km, blue on white. $18,000 will
deliver to Magrath July 29 (604) 824­6787
7/19
*4 sale truck camper sleeps 4 has furnace,
water, table. Ice box $1000 OBO 758­3576
*4 sale '86 Hundai Excel very good cond
must see 8? drive standard ph 758-3517
e & • & • & 4
There are other events happening inland around Magrath. Check out our
Monthly Calendar featuring the current weeks for places to go and do fun
things. If you have something you would like to see in the community cal­endar
please call me at 758.-6377 or 758-6362 Melanie Stone
AUGUST MAGRATH NEWS
In Our Community...
Sunday Monday Tuesday________Wednesday Thursday Friday________ Saturday
13
‘Dahl fruit
order
14
‘Mobile
Pent u re
Clinic
15 16
‘Dahl fruit arrives
‘Shirley &
Doran Perry
Anniversary
‘Submit Reading
Logbooks to
Library
17
‘BINGO
Seniors
Centre
6:50
p.m.
16
‘Deadline
for Magra­th
Library
Job
‘Ehlert
Yard Sale
4-Ô
19
‘Seear Family
Garage Sale 9-2
’South of Tre­asury
Branch {
Garage Sale
20
‘Dahl fruit
order
‘James W
Smith Rep­orts
9am
Stake
Centre
21 22 23
‘Dahl fruit
arrives
’Seniors
Corn Fest
5pm
24 25 26
*13 Annual Fall Fair
&Horse Show
‘Fred Riel 95
Birthday
‘Seniors Trip to
Olsens 5pm
‘Welling Family Fun
Night 4pm
27
‘Dahl fruit
order
South Co­untry
Ja­mboree
Society
jam
2Ô 29 30
‘Alice ""Sparky""
Stevenson
Birthday
‘Dahl fruit arrives
‘Merrill &Lise Th­omson
Anniversary
31 1
SEPT
2
EM5ER
COMPLETE ASPHALT MAINTENANCE
• Seal Coating « Crack Sealing
• Asphalt Repair $ Line Painting
Rob Gough Bus: 758-6001 Box 810
Rob Jones Magrath, Ab.
TOUR SEAL OF APPROVAL’ T0K1J0
‘Call for FREE estimates on driveway
Domestic & Commercial
Water Hauling
3000 Gal & 4000 Gal capacity
3 l’uinping Services Available
' . Cistern Cleaning
. 328-M«°
Steele Sheridan
*No Deliveries Sundays and Holidays
MAGRATH TRADING COMPANY 1
GROCERY SPECIALS
K FROM OUR FAMILY TO YOURS . .
Kellogg’s Cereals
Sei Var
$2.98
Folgers Coffee
Sei Var
2 for $5.00
Post Cereals
Sei Var
$3.95
Western Family Sliced Bread
567g
10 for $5.50
Sun-Rype Juice, Blends, Hi-5 or Cocktails
3x250ml
$1.15
S. R. Juice, Blends, Cocktails, Cots or Nectars
1 litre
$1.55
Christie Cookies
350g
$2.98
McCain Punch
3x200ml
.95
Christie Ritz Crackers
25Og
$2.25
Hershey Family Size Chocolate Bars
Sei Var
$1.25
Pringles Potato Chips
Sei Var
2 for $3.95
McCormick Imagine Cookies
300g
2 for $5.00
Setty Crocker Fruit Snacks or Punkaroos
Sei Var
$2.28
Mott’s Clamato Juice
945 ml
$2.25
Armstrong Cheddar Cheese
750 g
$6.95
Pairyland Multipack Yogurt
5x125 g
$2.98
Armstrong Cheese Slices
45 slices
$6.98
Green Giant Vegetables
1 kg
$3.45
McCain Rising Crust Pizza
760-535g
$6.95
Sunny Pelight Chilled Beverage
Sei Var
2 for $4.00
Armstrong Aged Cheddar
750 g
$7.95
Kellogg’s Eggo Waffles
625 g
$3.48 |
Tampico Punch Multipack
5x375 ml
$3.95 |
McCain Triple Chill or Peep ‘N Pelicious Cake
510-530g
$3.25
Good Humor Novelties
6-10 pack
$3.95
Breyer’s Frozen Yogurt
1 litre
$3.95MORE GROCERY SPECIALS ...
Western Family Macaroni & Cheese
225 g
2 for $1,00
Lipton Noodle or Rice Sidekicks
Sel Var
3 for $3.99
Hellmann’s Mayonnaise
1 litre
$5.96
Kraft Pourable Salad Dressings
475ml
2 for $4.96
Value Priced Ramen Noodles
S5 g
5 for $1.00
Royale Bathroom Tissue
Reg24 Db12
$7.46
Ultra or Origional Tide Detergent
Sei Var
$9.46
Ultra Ivory Liquid for Dishes
626 ml
$2.96
Pedigree Dog Food
630 g
$1.56
Kibbles ‘N Bits ‘N Bits ‘N Bits
6 kg
$11.96
Whiskas Dry Cat Food
1.6-2 kg
$5.96
Whiskas Premium Cat Food
170 g
.56
HEY, THAT PICKLES! ITS CANNING SEASON
Bernard in Mason Jars
12/500 ml
$6.96
Sifto Coarse Pickling Salt
2 kg
$1.96
Glad Freezer Bags
Lrg or Med
$1.26
Ziplock Containers
Sei Sizes
$3.46
Heinz White or Pickling Vinegar *WOW*
4 litre
$2.46
Pickling Cucumbers *WOW*
$1.74/kg
.76BUTCHER'S BUTS OF THE WEEK.'
Lean Ground Beef
$4.147kg
$1.66/lb
Eye of Round Oven Roast
$S.77/kg
$3.96/lb
Outside Round Oven Roast
$5.O3/kg
$2.26/lb
Western Family Party Sticks
500 g
$2.96
Western Family Cooked Ham
175 g
$1.95 each
Olympic Sliced Meats
575 g
$2.45 each
Olympic Wieners
450 g
$2.26
Olympic Sliced bacon
500 g
$4.26
Olympic Bologna
500 g
$2.46
MARKET FRESH PRODUCE.'
Fresh Peaches
$2.16/kg
.95/lb
Fresh Strawberries
11b basket
$1.96
Outspan Navel Oranges
$1.72/kg
.75/lb
Medium Tomatoes
$1.50/kg
.65/lb
Fresh Bulk Beets
$1.06/kg
.45/lb
SUBSCRIPTION ORDER FORM MAGRATH NEWS
Please check one of the options Magrath Trading Company so we can better serve you.
and return to the
New Subscription
Subscription renewal
Would like home delivery
Verify Street Address
The cost for I years subscription is $15.00 which can be applied to your account if preferrable
Subscriber information
Name:_________________________
Street Address:_____-__________
P.O. Box:________ Postal Code:
Co m m u n ity :____________________
Phone:HOME HARDWARE
“Home of the Handyman”
BOATING SAFETY KITS
Meets Government specs.
Storage box, line & float, flashlight,
whistle, bailing scoop & fire
extinguisher.
$49.99
WHIRLPOOL AIR CONDITIONER
7000 BTU, multi speed, window
air conditioner, Last Years Price
$469.00
APPLIANCES
Check out prices on MOFFAT, G.E.,
INGLIS, WHIRLPOOL & MAYTAG
RCA TV’S
! Come see our selection of 9
inch to 36 inch competitively
priced
COLLAPSABLE STORAGE
CONTAINERS
$5.97
FANS
We still have a few for the hot
weather.
GAS MOWERS
We have a variety to choose from
Come on in and check out an end
of the season special
i
DANBY 9CU FT MICROWAVE
900 watts
Reg $139.99
SALE $119.99
LORETTA SALT
.79 cents each or 2 for $1.00
PROPANE BARBECUES
There are several to choose
from.
PLASTIC LAWN ORNAMENTS
25% Off
We have several to choose from.
MACTAC
Has been discontinued except
for the clear. We have a lot in
stock",J. A. Ririe,"Magrath Store News (August 16, 2000)",,,,core
24296461,1996,". In this paper we present an example of the application of a technique, which we call robot shaping, to designing and building learning autonomous robots. Our autonomous robot (called HAMSTER  1  ) is a multi-sensor mobile robot that performs the task of collecting &quot;food&quot; and bringing it to its &quot;nest&quot;. Its control architecture is based on the behavioral paradigm. The behavioral modules are implemented as classifier systems and are learned by a reinforcement learning technique exploiting the Bucket Brigade and an extended version of the Genetic Algorithm. The chief features of HAMSTER are that it combines innate (i.e., prewired) and learned behaviors, and that training was carried out in a simulated environment and then transferred to the real robot. Published in Proceedings of ISRAM&apos;96, Sixth International Symposium on Robotics and Manufacturing, M.Jamshidi et al. (Eds.), May 28--30, 1996, Montpellier, France. ^  Currently Ph.D. student at the University of Padua, Italy.  1  HAMSTER :..",,Robot Shaping: The Hamster Experiment,,,,core
155566679,2006,"The multi-agent paradigm for building intelligent systems has gradually been accepted by researchers and practitioners in the research field of artificial intelligence. There are also attempts of adapting agents and agent-based systems for creating industrial applications and providing e-services. In this paper, we present an attempt to use agents for constructing an online after-sale services system. The system is decomposed into four major cooperative agents, and in which each agent concentrates on particular aspects in the system and expresses intelligence by using various techniques. The proposed agent-based framework for the system is presented at both the micro-level and the macro-level according to the Gaia methodology. UML notations are also used to represent some software design models. As the result of this, agents are implemented into a framework for which exploits Case-Based Reasoning (CBR) technique to fulfil real life on-line services&apos; diagnoses and tasks.Computer Science, Artificial IntelligenceEICPCI-S(ISTP)",,Towards an agent-based framework for online after-sale services,,10.1109/IS.2006.348456,,core
29139966,2006-01-01T00:00:00,"Today, combinatorial optimization is one of the youngest and most active areas of discrete mathematics. It is a branch of optimization in applied mathematics and computer science, related to operational research, algorithm theory and computational complexity theory. It sits at the intersection of several fields, including artificial intelligence, mathematics and software engineering. Its increasing interest arises for the fact that a large number of scientific and industrial problems can be formulated as abstract combinatorial optimization problems, through graphs and/or (integer) linear programs. Some of these problems have polynomial-time (“efficient”) algorithms, while most of them are NP-hard, i.e. it is not proved that they can be solved in polynomial-time. Mainly, it means that it is not possible to guarantee that an exact solution to the problem can be found and one has to settle for an approximate solution with known performance guarantees. Indeed, the goal of approximate methods is to find “quickly” (reasonable run-times), with “high” probability, provable “good” solutions (low error from the real optimal solution). In the last 20 years, a new kind of algorithm commonly called metaheuristics have emerged in this class, which basically try to combine heuristics in high level frameworks aimed at efficiently and effectively exploring the search space. This report briefly outlines the components, concepts, advantages and disadvantages of different metaheuristic approaches from a conceptual point of view, in order to analyze their similarities and differences. The two very significant forces of intensification and diversification, that mainly determine the behavior of a metaheuristic, will be pointed out. The report concludes by exploring the importance of hybridization and integration methods",Brunel University,Combinatorial optimization and metaheuristics,https://core.ac.uk/download/29139966.pdf,,,core
162408305,2007-01-01T00:00:00,"This paper focuses on the results of different consumer surveys conducted between 2004 and 2006 with regard to consumers ' perceptions and reactions concerning AI in Vietnam, (mainly in Hanoi). The main results observed are as follows: - A high proportion of consumers consider AI to be a food-related risk. However, over time, there has been a slight shift from a fear of consuming poultry to a fear of preparing it (slaughtering it). - AI has had a profound effect on poultry consumption, even outside peak crisis times, more in terms of the quantity consumed (approximately a third less in 2006) than in terms of the number of consumers (6% less). - Blood and internal organs are considered particularly risky, while eggs are viewed as being safer. Poultry from industrial farms is considered to be more risky than poultry from small farms. - Purchasing practices have also been affected by AI: in Hanoi, consumers declare that they prefer to buy poultry directly from producers that they know, or from supermarkets in the case of the wealthiest consumers. A high proportion still buy live poultry from market traders, but more consumers now ask sellers to slaughter it for them. With a view to lessening market shocks in the wake of the crisis while maintaining the priority of consumer safety, a number of measures should nevertheless be implemented: Risk communication should not over-emphasize AI as a food-related risk. - Reliable safe distribution channels should be promoted (with reliable quality signs and controls) in order to encourage safe production and poultry consumption. Otherwise, a market recovery will only benefit supermarkets and large-scale farmers capable of supplying supermarkets. - As numerous live birds are still slaughtered in urban market places, facilities should be provided for safe slaughter. At the same time, more attention should be paid to the provision of a real ""cold chain"" with a view to promoting the sale of slaughtered poultry. (Résumé d'auteur",'Food and Agriculture Organization of the United Nations (FAO)',Consumer perceptions and reactions concerning AI (Avian Influenza),http://agritrop.cirad.fr/539844/1/document_539844.pdf,,,core
326405776,2007-03-08T00:00:00,"International audienceThis paper focuses on the results of different consumer surveys conducted between 2004 and 2006 with regard to consumers’ perceptions and reactions concerning AI in Vietnam, (mainly in Hanoi). The main results observed are as follows: A high proportion of consumers consider AI to be a food-related risk. However, over time,there has been a slight shift from a fear of consuming poultry to a fear of preparing it (slaughtering it). AI has had a profound effect on poultry consumption, even outside peak crisis times, more in terms of the quantity consumed (approximately a third less in 2006) than in terms of the number of consumers (6% less). Blood and internal organs are considered particularly risky, while eggs are viewed as being safer. Poultry from industrial farms is considered to be more risky than poultry from small farms. Purchasing practices have also been affected by AI: in Hanoi, consumers declare that they prefer to buy poultry directly from producers that they know, or from supermarkets in the case of the wealthiest consumers. A high proportion still buy live poultry from market traders, but more consumers now ask sellers to slaughter it for them. With a view to lessening market shocks in the wake of the crisis while maintaining the priority of consumer safety, a number of measures should nevertheless be implemented: Risk communication should not over-emphasize AI as a food-related risk. Reliable safe distribution channels should be promoted (with reliable quality signs and controls) in order to encourage safe production and poultry consumption. Otherwise, a market recovery will only benefit supermarkets and large-scale farmers capable of supplying supermarkets. As numerous live birds are still slaughtered in urban market places, facilities should be provided for safe slaughter. At the same time, more attention should be paid to the provision of a real “cold chain” with a view to promoting the sale of slaughtered poultry",HAL CCSD,La perception et les réactions du consommateur à propos de la grippe aviaire,,,,core
21319153,2006,"This paper describes an effective medical claim fraud/abuse detection system based on data mining used by a Chilean private health insurance company. Fraud and abuse in medical claims have become a major concern within health insurance companies in Chile the last years due to the increasing losses in revenues. Processing medical claims is an exhausting manual task carried out by a few medical experts who have the responsibility of approving, modifying or rejecting the subsidies requested within a limited period from their reception. The proposed detection system uses one committee of multilayer perceptron neural networks (MLP) for each one of the entities involved in the fraud/abuse problem: medical claims, affiliates, medical professionals and employers. Results of the fraud detection system show a detection rate of approximately 75 fraudulent and abusive cases per month, making the detection 6.6 months earlier than without the system. The application of data mining to a real industrial problem through the implementation of an automatic fraud detection system changed the original non-standard medical claims checking process to a standardized process helping to fight against new, unusual and known fraudulent/abusive behaviors",,A Medical Claim Fraud/Abuse Detection System based on Data Mining: A Case Study in Chile,,,,core
21780476,2001,"In most cases in intelligent manufacturing applications the communication functions depend on the capabilities of the intelligent tool (e.g. expert system). Three different types of working mode and different logical levels of the communication of an intelligent cell-controller in a CIM environment is shown in the paper. These levels are implemented - of course - within the same protocol. After this concept the paper explains the connection between these logical levels and the communication protocols and ontologies. Finally a simulation tool introduced in the second part of the paper was developed to examine the effects of the different communication messages and to analyse how the different type of messages can be measured in a real Flexible Manufacturing System (FMS) environment.  Key words: distributed artificial intelligence, knowledge communication, flexible  manufacturing system  1",,Logical Communication Levels in an Intelligent Flexible Manufacturing System,,,,core
23613990,1999,"Abstract — This paper describes the real-time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling. MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach. Index Terms—Digital signal processor (DSP), Markov random field (MRF), motion detection, real-time implementation. I",,Real-Time DSP Implementation for MRF-Based Video Motion Detection,,,,core
38301465,2005-01-01T00:00:00,"Περιέχει το πλήρες κείμενοThe paper introduces a concept for utilisation of ICT in teaching of subjects from field of mechatronics, namely mechatronics of production systems that in our case are represented by industrial production lines processing continuous strip materials. These production lines occur in various areas of industry, e.g. paper making machines (paper strip), rolling mills (steel strip), and production lines in chemical industry (plastic material), etc. Such systems are rather complex and their explanation using multimedia is extremely suitable and visible. What is to be explained in the modules: principle of setting up mathematical models of the mechanical subsystems, electrical equipment (electrical machines, electrical drives, and power electronics converters) of production machines, and finally, control systems with implemented control algorithms (up to the artificial intelligence algorithms). In detail there are presented two modules – one form field of setting up mathematical models of mechanical subsystems and the second one from field of production lines. The topics of the modules deal with generalisation of basic principles of the real existing production and finishing lines whose study usually requires practical trainings in industry",'University of Zilina',Computer Supported Education For Industrial Mechatronics Systems,https://core.ac.uk/download/38301465.pdf,,,core
10543849,"October 12, 2006","Electroactive polymers (EAP) are human made actuators that are the closest to mimic biological muscles. Technology was advanced to the level that biologically inspired robots are taking increasing roles in the world around us and making science fiction ideas a closer engineering reality. Artificial technologies (AI, AM, and others) are increasingly becoming practical tools for making biologically inspired devices and instruments with enormous potential for space applications. Polymer materials are used to produce figures that resemble human and animals. These materials are widely employed by the movie industry for making acting figures and by the orthopedic industry to construct cyborg components. There are still many challenges ahead that are critical to making such possibilities practical. The annual armwrestling contest is providing an exciting measure of how well advances in EAP are implemented to address the field challenges. There is a need to document natures inventions in an engineering form to possibly inspire new capabilities",,Biomimetics as a Model for Inspiring Human Innovation,,,,core
235572039,1996-01-17T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
30 cents
Wednesday January 17,1996
MAGRATH SPORTS NEWS:
The Westwinds Region Jr. High basketball
league saw the Magrath girls defeated
Raymond by 28 - 27, while the boys won 58-25,
Ben Wood scored 12 pts & Jimmy Balderson
had 11 pts. The Gr. 8 girls won against
Raymond with a big 63-26 win. The Gr. 8 boys
won 43-41.
The Magrath JV Pandas played in the
Foremost Girls tournament, although they
didn't win, Kalli Johnson had 10 pts. in the
second game.
In Hockey News: The MagRay Black
Devils were in Great Falls last weekend at a
Bantam-A Tourney. The Devils lost 7-1, but
goalie Doug Eakitt in goal, faced 45 shots.
Matt Stanford scored the lone MagRay goal.
The Devils lost their second game.
The Devils were in Picture Butte on Sunday
Jan. 14th, where they won a 6-3 victory.
Goal-getters were: Andy Court-2,
John Wift-1, Matt Stanford-1, Mike Lucas-1,
Leonard Hunter-1, while Brigham Leishman
had 2 assists.
A Aü; A* A
SCHOOL NEWS
Next week is the annual Jr. & Hi Ski
Trips. From January 23 - 25th some of our
students will be heading out to do some skiing.
The first time for some and a great return for
others. Diploma Exams will also be starting for
Social 30, English 30 & 33, Physics 30 and
Math 30 & 33. On Jan. 24th the Gr. 8 Host
Glenwood while the Gr. 7 áre at Glenwood.
On the 25th Gr. 9 host Raymond.
MAGRATH CHE ELEADERS'
FUNDRAISING DINNER THEATRE
PRESENTS
THE MAGRATH SCHOOL PLAYERS
IN
’’TAKE FIVE’’
FEBRUARY 5, 1996
DOORS OPEN AT 6:00 P.M.
DINNER BEGINS AT 6:30 P.M.
$10.00 PER COUPLE
Tickets available at the School or from any
Cheeleader.
* * Ä Ä Ä Jc
Piano Light
Hospital Piano.
Please phone
758-3030.
LIBRARY NEWS:
New Books at the Library
""Magic: the Final Fantasy Collection"" by Isaac Asimov, ""The Road Ahead"" by Bill Gates(includes a CD), ""The Faithful Gardener' by Clarissa Pinkola, ""Getting Canada on Line: understanding the information highway"" by David Johnson, ""William Shakespeare: the Extraordinary Life"" by Andrew Gurr, ""Tomorrow's Promise"" by Sandra Brown, ""Gypsy"" by Carole Mortimer, ""Aftershocks: by Catherine Coulter.
Books for the Youth:
""Soup Ahoy"" by Robert Newton Peck, ""The Berenstein Bears Scouts in Giant Bat Cave"", ""Shoot for the Hoop"" by Matt Christopher, ""The Deadly Fire"" an R.L. Stine Fear Street book, ""Run with Horses"" by Brian Burks. If you have any overdue or lost books or any unpaid fines, please come and take care of these before we go on computer, which will be very soon now. Thank You.
We would like to Thank all those who have generously donated books & videos to the library in the last year. We have just received 2 complete new series, The First American Saga"" & ""The First North American Series"" from Fawntella Trainer. Others who have donated are: Rose & Stephanie Groenhof, Debbie Leishman, Travis Killian, Leonard Bondy, Carol Taylor, Shirley Miller, Sylvadell Johnson, Pinky Coleman, Sharon Mustard, Chris Alston, Marilyn Bullock, The Davies, Carolyn Mehew, Wayne Toly, Koraley Wallace, Beth Wyght, Cody Thomson, Deborah Hoekstra, Laurel Bennett, Gayion Dudley, Sally Wilde, George Smith, Robert Mostowy, Janet Voth, Ben Rasmussen, Vicki Davis, Kathy Beck, Ilona Schneyder, Mrs. Woody Anderson & Mr. James Salmon. If we have missed someone, we are sorry, but we do appreciate your support.
******
BINGO - BINGO - BINGO - BINGO All members of the community are welcome to play Bingo at the Seniors' Centre every second Thursday. Next Bingo Jan. 18th, 1996
******
FOCUS ON CHILDREN
The Redesign of Services for Children in Alberta Meeting to be held Jan. 18 at 7 pm in the Magrath School Library to look closely at the needs of children and families in our community.
The working groups are open to a wide range of community members - including families, children, youth, volunteers, elders, board members, professionals who work with children, agencies that provide services to children, cultural, spiritual and religious organizations, sports groups and others.
Each local working group will develop a set of goals and prepare a preliminary plan which identifies services that would benefit the local population.
Your input is needed. For more information contact.
L. Balderson 758-6380.
******
Magrath & Area Birthday Calendar sold by the Spirit of Alberta Band have arrived at the School. If you have paid for yours & have not received it come to the Band Room & pick it up. We have additional calendars on sale for $5.00. Come support the Band & pick up a 1996 calendar.
******
THE FRIENDS OF THE LIBRARY have received donations from: Joan Karren in memory of Leitha Leishman, Warren & Desmonia Harris & Wendell & Alice Davies in memory of Emerson Blumell. Julia Dean Bondy in memory of Emerson Blumell & Wallace Wilde. Ray & Marie Stevenson in memory of Eva Jensen. Devere & Lucille Dudley & Tam & Kay Terakita in memory of Kenneth E. Neilson.
******
For your Carpet & Upholstery steam cleaning & general cleaning needs. Call Wayne Bourelle 758-6414ct J116! ?,Wn rctvc,in^ trailer is off to a £Ood
bxi pu ts Btin Sb hThere “ iTs su ""Sha0 Uwldt °°fr cSohn™fulsdi ™
Ji ug« , wLhichh h\ ave bU eenp rliansstiecd vwititahm ceolrds we.agt emr i&lk
have caps removed, coloured plastic container
With lids removed & rinsed with hot water all
=SK
glass, chinaware or CERAMICS
ceX°bXXLo“b&otheSe
boxes, kleenex boxes, c^tLcCpa“™
J?"" S x?Uld be Clean With t0P ,i(J slightly
attached. Newspaper, inserts, magazine glossv
magazines, coupons & small catalogue“. ’ ""
u° 1 K 3 ,LSt °f products NOT accepted
JtlCF?fl^«0 ^D B°XES’ MELK 0R
J LICE CARTONS, CARBON PAPER
TELEPHONE books, books with CT IT
OR WIRE BINDINGS, PAINT CANS
aTroCs°SS’B“ )
Please help this program succeed by sortins I
your materials and placing them in the correct I
I
ir
carriage house
theatre
-Presents-
""SABRINA""
showing at 8 p.m.
starting Fri Jan 19th
""JUMANJI""
showing at 8 p.m.
;
IÌÌ : i
******
TO MAGRATH & DISTRICT RESIDENTS
AA The Heart & Stroke Foundation
„e °nce again asked the Magrath /
V HospitaI Auxiliary to handle the '
canvas in the town & district.
February B Heart & Stroke Month. Watch tor
helL pT5“-'0 On yOU' Evw-' helps. Please give.
Thanks Alma Whitt. President.
******
Call Jack Toda y!
(403) 752-3449 OR 327-5152 BUS
(403) 752-3878 FAX, 758-3551 RES
•JACK ELLIOTT, BSc., MA.
Sales Associale
• Easier gripping
If you—or an older relative-have a hard time
?lidPP'n?hPenS' PenC',S or eye-makeup wands try
shdmg the pencil into a soft foam hair curler'
ube. It gives a fatter gripping surface that reallv
makes a comfortable difference! 1 V
COLDWELL BANKER
HANCOCK LAND
REAL estate
52 8A0ADWAY N
RAYMONO. A8 TOK 2S0
3^,
Ì’»
----- ---
i
Attention One-Act Play Enthusiasts
Regional One-Act Drama Festival 1996
The Regional One-Act Drama Festival this year will be held at the Sterndale Bennett Theatre,
Yates Memorial Centre, Lethbridge, on March 11 & 12, 1996. You are encouraged to select a
scripted one act play, no less than 10 min. & no longer than 1 hour. One play will be chosen to
represent our zone in the Provincial Adult One-Act Drama Festival. Deadline for entries is Feb. 9, 96
For information, registration forms, and regulations contact the Allied Arts Council of Lethbridge
at 327-2813 or for additional assistance you can contact the Regional representative Neil Boyden @
329-2463(work) or 329-0242(home). The entry fee is $25.00 per play entered. This festival is
sponsored by the Alberta Drama Festival Association,The Allied Arts Council of Lethbridge, & the
Playgoers of Lethbridge.
I ! 1996 New Year Resolution ! !
Getting Started - Pre-Authorized Savings Plans
(An Easy Way To Keep Your Resolution)
If you're like most people, every year about this time you start thinking of New Year's resolutions.
If you are like most people, by the time the end of Jan. rolls around, your resolutions are as much a
part of history as the old year.
If one of your resolutions for 1996 is to start a savings plan to improve your financial health, here
are some tips to help you keep your resolve. First, the sooner you start the better. Consider the case
of the 23 yr old & the 31 yr old who each decide to start saving $100 per month. The 23 yr old keeps
doing it for 9 yrs. & then stops, having saved $10,800 which is invested until age 65. The 31 yr old
keeps putting aside $100 a month until age 65, saving a total of $42,000. Despite the perseverance,
the 31 yr old never does catch up with the 23 yr old. Assuming both earned an average annual return
on their savings of 8%, at age 65 the 31 yr old's $42,000 has turned into $229,400, the 23 yr old's
$10,800 has turned into $236,800. Obviously the sooner you start the better.
Whatever age you start at however, the other key is regularity. A pre-authorized savings plan can
be set up to put aside a fixed amount at regular intervals. This can be quarterly, monthly, weekly or
whatever interval fits your situation. By making your saving regular its easier to keep that resolution.
Another secret is to get your money invested as soon as possible and keep it invested. A good
approach is to use your pre-authorized purchase plan to make regular contributions to a family of
mutual funds. You can do this for as little as $25 per Fund which allows you to allocate your savings
into several different types of investments to match your own personal financial objectives.
So if you want to keep your financial resolution this year: start now, save regularly, stay invested
and allocate your savings. Contributed by Toronto-Dominion Bank Investment Hotline.
For all your roofing
& siding estimates!
KEN JENSEN
758-3669
MAGRATH, ALBERTA
E L E C TRO LUX
!! Call for help !!
IDELIA - 380-7864 cell
LESLIE - 327-4481 office
1 free carpet shampoo
SALES AND SERVICE.
Mon-Tues-Wed each week.
ALBERTA
HUNTER EDUCATION
INSTRUCTOR'S ASSOCIATION
WHITE-TAILED DEER SEMINAR
TABER, ALBERTA
FEBRUARY 24, 1996
A white-tailed deer hunting seminar has been scheduled for February 24, 1996 at the Taber
ommunity Centre, 4712 - 50th St. Taber. A fee of $30 is being charged for this information seminar
“ X “ a “. 30 attCliOn- 11 you wish t0 a“end senli“ar oW, the cost is $20.
rT r“C,V y Feb- 161 1996' We take ,he firet 500 mirations for the seminar
and the first 400 registrations for the banquet.
for further information contact Seminar Coordinator - Martin Robillard at 297-6423 or the
Lethbridge District Fish and Wildlife office at 381-5266. The evening program & auction should raise
money that will be put back into Conservation Education programs across the Province.
NAME:__ __
ADDRESS
CITY/TOWN
REGISTRATION
_____________PHONE:_________ _____
_________________ _ POSTAL:_____________
I wish to attend.
I wish to attend'
I wish to attend.
I wish to order:
_______ Seminar & Banquet.........$30.00
_______ Seminar only............. . .$ 20.00
_______Banquet only ............... $ 20.00
_______ Lunch .............. $ 7 00
Youth:.... $ 15.00
Youth:.. $10 00
Youth:.... $ 10 00
Youth..... $ 7 00
* YOUTH - 15 years of age or under ( must be accompanied by an adult)
Please send registration with cheque or money order payable to
ALBERTA HUNTER EDUCATION INSTRUCTOR’S ASSOCIATION:
TO
LETHBRIDGE DISTRICT FISH AND WILDLIFE
530 - 8TH STREET SOUTH
YPM PLACE
LETHBRIDGE, ALBERTA
TIJ 2 J8
The Daughters of Utah Pioneers
are reminded of their January
meeting on Monday January 22
at 1:30 at the 3rd & 4th
Ward Church. RSH.
HELPING.
THE MEDICINE
GG DOWN
How can you get your
kids to take yucky-tast-ing
medicines? Phar­macist
Andy Ullrich
recommends:
® Chilling liquid medi­cine
in your refrigerator
(ask your phar macist
if this is okay) to take
the edge off a bitter or
unpleasant taste.
SS Mixing a teaspoon of
com syrup with liquid
medicine to sweeten a
sour taste.
• Offering something
sweet and strong, like
grape juice, right after
the medicine to coun­teract
its aftertaste.
Lolliop Trick
If your child is
complaining of a sore
throat but won't let
you look inside her
mouth, try using a
lollipop as a tongue
depressor. She'll
think it's a treat, and
you'll have a
cooperative patient.
Don't forget to hand
the lollipop over as a
reward.
CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758-6377
SERVICES
Less than 30 words—$1.07
Small ad (2.5""X 3.5”)-$5.35
1/4 page---------------- $7.49
1/3 page---------------------$8.56
1/2 page-------------------$10.70
Full Page—Copy Ready—$25.00
Full Page---We do----------$37.45
Flyer insertion (your paper)
For Sale: 10 yr. old built-in Maytag Dishwasher $125.
10 yr old Whirlpool Trash Compactor $125. Call 758-6006. J.17.
A A A A A A
For Sale: 1 pair Vokul skies, 2 pairs ski poles, 1 ski bag, 1 pair Ditrani ski pants, 1 set Cara ski goggles-$150 o.b.o. Call 758-6228. J. 17.
A A A A A A
Need a babysitter!!
For quality care in my home, call Deanne @ 758-6826
A A A A A A A
For Sale: 4 tires P215 R7515 $300 with rims $400. Fits Ford, 4 mos old. 758-6237.
A A A A A A
For Sale: 1979 Van, swivel bucket, fold down bed & new rear heater. 103,000 org. kms. $1900 o.b.o. Call 758-3830. J.17.
A A A A A A
Found 1 pair of binoculars on Fri. Jan. 12 in the Stake Centre parking lot.
Phone 758-6406.
A A A A A A
Do you need someone to houseclean or tend your kids? If so contact
LaDean Thomson 758-3209 after 6 p.m. J.24.
A A A A A A
Wanted to use: male pygmy goat. Call 758-3410. J.24
Jean Ys Hair Fashion
Professional Haircare at pleasing prices Open hours 9 am - 5 pm Tues, thru Sat.
For appointment call 758-3379.
A*AAAA
CAKES BY RITA
Specialty cakes for Birthdays Anniversaries etc.
I also do Wedding Cakes Specializing in
Rolled Fondant icing Call 758-6315
F.7
A A A A A A
RICK'S PORTABLE WELDING owner Rick Beres we weld everything from chairs to tractors. Please call me for all your Welding needs.
""B"" Pressure qualified 758-6427
A A A A A A
Of course TV turns up the volume on commer­cials — how else could you hear them in the kitchen?
— Sam Ewing
COUNTRY LANE SALON 758-3339 AAAAAAAA
Your Family Hair Care Centre & Tanning Salon
CUTS * PERMS * COLORS Seniors Specials at all times. Open Monday thru Saturday 7 a.m. to 9 p.m. By Appointment Only.
A A A A A A
ALL TREE SERVICES Certified Arborist providing the following services: Tree & Hedge Trimming, Removals, Hauling & Cartage of unwanted debris. For a Free Estimate. Call Cam Bruce at 758-3729.
A A A A A A
WANTED
If you are removing the following type of trees - Ash, Elm, Evergreen, Birch or Apple & are planning to take it to the dump, call me & I will take away the larger pieces for firewood. Call
Cam Bruce @ 758-3729 A A A A A A
For AH Your Multi-Pro or Fuller Brush Products.
Please call the McClung’s @ 758-3781.
A A A A A A
A A A A A AFrom the Television Series
on Channel 12 Whole Food
Cooking with Sheila Clair, J
now have a cookbook
showing how cooking
techniques & safer
seasonings can make whole
healthy foods taste
wonderful. Introductory
price $14.95. Call 758-3284.
J. 10.
******
For Rent: 3 bedroom Mobile
Home with addition - with
4th bedroom or Family
Room. Newly redecorated
Call Evenings 758-3829.
******
For Rent - 2 bedroom house,
$375+ $300 damage and
cleanup deposit. Abstainers
please. Call 758-3714.
******
For Rent: beautiful new 2
bedroom apartment available
Phone Sandi at 758-3781.
******
For Rent: 2 bedroom house
$400 a month plus utilities.
Abstainers only. Phone
S. Anderson 758-3081.
******
House for rent: 3 bedroom,
fenced yard, excellent
location. $400 + utilities.
$300 damage deposit.
Abstainers only please.
Available Feb. 1st. Call
Lisa 758-3977.
Treating Carbon
Monoxide Poisoning
When a heating appliance
isn't vented properly, carbon
monoxide gas - odorless,
colorless and toxic-can build
up. Oxygen is the antidote:
some patients may need
treatment in a hyperbaric
chamber, a cylinder into
which doctors pump a
megadose of pressurized
pure oxygen.
The best strategy though is to
be proactive, installing
carbon monoxide detectors-especially
outside sleeping
areas-can salve lives.
IW WE HOW
1 Towucwramo
i Ik Orr/ZÌ'-. tW. «-
Top dog
“I want a dog that I can be
proud of,” said the rich lady to
the store owner. “Does the
one over here have a good
pedigree?”
‘Til say,” replied the owner,
“if he could talk, he wouldn’t
speak to either of us.”
—- Suzan L. Wiener
*4 3/4 acres in grass for
sale, zoned agricultural.
Asking $17,900.
*Nicely refinished 3
bedroom home & garage
for sale, rural location..
Asking $26,900.
*Need resale homes in
Magrath for purchasers.
Please call Jack.
Jack Elliott 758-3551
Coldwell Banker.
Most of us can keep a secret: It's the people we ■
it to who can't. — -Sam Ewing
******
DUN
AMSTtt Drain Master
Drain Cleaning Service
24 Hour Emergency Service
- Sewer & Drain Cleaning - Line Thawing
- Vcauum Truck - Line Jetting
- Hot & Cold Pressure Washing
Fax: 328-6896 Leroy Schalk
Res: 327-7270 Ph: 328-8588
• AOWtCULTU«»*' •C©M‘*C«»S-Ak * INDUSTRIAL * MAIKTCmm« * £»*>*«*• C» • 2£ HOURS*
****
Box 1942 TIJ 4K5
Lethbridge, Alberta
Res/Fax: (403) 758-6339
: $29.00 per hour
ELAD
ElectncLt
DALE GREAVES
24 HOUR SERVICE (329-0965)
•TeiIephone Installation s * Alar m & Security Systems• Computer Systems & Gables*
1
Morning aerobics will begin again.
Fri Jan 1, mon & fri 10 - 11 a.m.
MORE GROCERY SPECIALS JANUARY 15 - 20
Western Family Evaporated Milk
385 ml
.98 each
Western Family Coffee
300g
$2.98 each
Western Family Hot Chocolate
500g
$1.98 each
Kelloggs Corn Flakes
400g
$1.98 each
Western Family Granola Bars
$1.88 each
Western Family Salad Dressing
500 ml
$1.98 each
Western Family Ketchup
1 liter
$1.48 each
Ragu Pasta Sauce
750 ml
$2.28 each
Dr. Ballard Dog Food
380g
.88 each
Kleenex Ultrasoft Bathroom Tissue
24 pk
$8.98 each
Western Family Jumbo Paper Towels
2 pk
$1.98 each
Ultra Cheer
6 liter
$7.98 each
Western Family Bleach
3.6 liter
$1.68 each
Western Family Margarine
1.36 kg
$2.88 each
Kraft Cheese Whiz
500g
$3.98 each
Western Family Hash Browns
1 kg
.98 each
Western Family Ice Cream
2 liter
$2.48 each
Minute Maid Orange Juice
355 ml
$1.18 each
Good Humor Ice Cream
4 liter
$3.85 each
Danny & Lori Degenstein are pleased to announce the safe arrival of their son ""Danny Layne"" born Jan. 9, 1996, weighing 6 lbs. 3 ozs. A special little brother for ""Tyler"". Proud grandparents are Betty Degenstein of Lethbridge and
Reid & Audrey Gruninger of Magrath. Great-grandmothers Olive Gruninger & Florence Robinson.
• Another use for lemons
Before tossing out that lemon you've just squeezed, rub the pulpy side against the bottoms of copper pots—it'll take the tarnish away in an instant!
Alberta Treasury Branches, Magrath is pleased to announce we have extended our ""Prime Rate Personal Loan Sale""* to end on Jan. 31/96.
Call or come in to discuss your personal loan needs with our friendly and capable staff. *Some restrictions apply.MEAT SPECIALS JANUARY 15 - 20
Standing Rib Roast $3.98/lb $8.77/kg
Sirloin Steak $2.98/lb $6.57/kg
Porkloin Chops (rib & tenderloin) $2.28/lb $5.03/kg
Cut-Up Frying Chicken $1.78/lb $3.92/kg
Beef Sausage $1.78/lb $3.92/kg
Swift Bacon 500g $2.88 each
Bulk Wieners .98/Ib $2.16/kg
Schneiders Wieners 450g $2.88 each
Seasoned Turkey Breast $4.99/lb $10.99/kg
Friends in Magrath & district will be
saddened to hear that Martha Henderson
Purdey passed away Sunday Jan. 7, 1996 at her
home in Fort Collins,Colorado at the age of 64.
She was born & raised in Magrath. Her
parents were Oswald C. Henderson & Frieda
Rosa Senn. She was predeceased by her
husband Dr. Edward Purdey and one brother
Waldo. She leaves to mourn her children
Susan (Ronald) Lovold, Timothy and Chuck
Purdey. She also leaves 3 sisters:
Elizabeth (Betty) Gillette, of Logan, Utah;
Lillian(George)Dudley & Della (Allen)Holladay
both of Magrath, also 3 brothers,
Lester(Loa)Henderson, Hyrum (Shirley)
Henderson, Ollen(Ethel) Henderson.
?|c * ****
Mrs. Maria Zundel passed away Jan.11,
1996, at the age of 88 yrs. She was
predeceased by her 1st husband George
Widmer in 1933 & her 2nd husband Reinhold
Zundel in 1958.
She is survived by her son, Ernst(Erna)
Widmer of Magrath. The funeral was held on
Mon Jan.15, 1996 in St. John's Lutheran
Church. Interment in the Magrath Cemetery.
Adulf Education Computer Course
""The Basies""
Beginning January 25, <996
This computer course will provide
a “gentle” introduction to basic computer operations
& a variety of software application for everyday use.
This course is designed for those with little
or no computer experience.
(An advanced course will be offered if interest warrants.)
Disks will be provided. Hands-on experience will be given
using the following application types:
SS” word processing
database
KT spreadsheets
home finance
KT graphics software for greeting cards
& invitations
KS” multimedia reference tools,
(such as CD-ROM encyclopedias)
KS” telecommunications & the Internet
Time: Thursdays, beginning January 25, 7:00-10:00 pm
(Note: The weekday could be adjusted if necessary}
Place: Magrath School Computer Room (new lab)
Course Length: 8 weeks
Instructor: Bonny West
To Register phone:
Sheila Sillito 758-3652
Bonny West 758-3072
PRODUCE SPECIALS JANUARY 15 - 20
Red Seedless Grapes $2.68/lb $5.90/kg
Kiwi Fruit 6 for .98
Red Grapefruit 5 lb $1.88 each
Money's Mushrooms 225g $1.38 each
Green Cabbage 3 lb for .98 .72/kg
Tomatoes .58/lb $1.28/kg
Large Eggs $1.58 each
Navel Oranges .69/lb $1.52/kg
Long English Cucumbers $2.79 each
SENIORS NEWS:
Our regular Wed. supper will be tonight
Jan.l7th @ 5 p.m. at the Seniors Centre.
Then on Fri Jan. 26 we'll have our Annual
Meeting & Potluck dinner at 5 p.m. Come
hear reports from the Committees & vote for a
New Executive. All Seniors are welcome.
SENIORS AEROBICS
Aerobics classes are held each Mon &
Thurs. at 9 a.m. at the Seniors Centre. Fee is
$10.00 each for the 3 month session. Give
considera",J. A. Ririe,"Magrath Store News (January 17, 1996)",,,,core
215485051,1999-01-01T08:00:00,"Since the widespread use of computers in business and industry, a lot of research has been done on the design of computer systems to support the decision making task. Decision support systems support decision makers in solving unstructured decision problems by providing tools to help understand and analyze decision problems to help make better decisions. Artiﬁcial intelligence is concerned with creating computer systems that perform tasks that would require intelligence if performed by humans. Much research has focused on using artiﬁcial intelligence to develop decision support systems to provide intelligent decision support.
Knowledge discovery from databases, centers around data mining algorithms to discover novel and potentially useful information contained in the large volumes of data that is ubiquitous in contemporary business organizations. Data mining deals with large volumes of data and tries to develop multiple views that the decision maker can use to study this multi-dimensional data. On-line analytical processing (OLAP) provides a mechanism that supports multiple views of multi-dimensional data to facilitate efficient analysis. These two techniques together can provide a powerful mechanism for the analysis of large quantities of data to aid the task of making decisions.
This research develops a model for the real time process control of a large manufacturing process using an integrated approach of data mining and on-line analytical processing. Data mining is used to develop models of the process based on the large volumes of the process data. The purpose is to provide prediction and explanatory capability based on the models of the data and to allow for efﬁcient generation of multiple views of the data so as to support analysis on multiple levels. Artiﬁcial neural networks provide a mechanism for predicting the behavior of nonlinear systems, while decision trees provide a mechanism for the explanation of states of systems given a set of inputs and outputs. OLAP is used to generate multidimensional views of the data and support analysis based on models developed by data mining. The architecture and implementation of the model for real-time process control based on the integration of data mining and OLAP is presented in detail. The model is validated by comparing results obtained from the integrated system, OLAP-only and expert opinion. The system is validated using actual process data and the results of this veriﬁcation are presented. A discussion of the results of the validation of the integrated system and some limitations of this research with discussion on possible future research directions is provided",VCU Scholars Compass,A model to integrate Data Mining and On-line Analytical Processing: with application to Real Time Process Control,https://core.ac.uk/download/215485051.pdf,,,core
23372536,1996,"ARCHON ^TM (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe&apos;s largest ever project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Two of these applications, electricity transportation management and particle accelerator control, have been run successfully on-line in the organisation for which they were developed (respectively, Iberdrola an electricity utility in the north of Spain and CERN the European Centre for high energy physics research near Geneva). This paper recounts the problems, insights and experiences gained whilst deploying ARCHON technology in these real-world industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the key design forces which shape work in this important domain. Secondly, the..",,Using ARCHON to develop real-world DAI applications for electricity transportation management and particle accelerator control,,,,core
24337211,1991,"The Constraint Satisfaction Problem (CSP) has been a useful model for various industrial and engineering applications. These include image processing and pattern recognition, VLSI engineering, robotics manipulation, and computer hardware design automation. In this paper, we give a novel AI architecture for discrete relaxation that effectively prunes a backtracking search tree in CSP. This algorithm has been implemented on a finegrained, massively parallel hardware computer architecture. For practical application problems, many orders of magnitude of efficiency improvement can be reached on this hardware architecture. This enables real-time processing of a large class of practical problems. Keywords: Artificial Intelligence, backtrack search, constraint satisfaction problem (CSP), Discrete Relaxation Algorithm (DRA), AI architectures. 1 Introduction  Constraint-based search problems have three components: variables, values, and constraints. The goal is to find an assignment of values to..",A Parallel Architecture for Constraint Satisfaction,,,,,core
33894700,1989-01-01T00:00:00,"Computer-integrated manufacturing (CIM) is still an objective for the future rather than a present reality.  Mechanical integration was introduced over 60 years ago by Ford to increase output and cut unit costs, but at the cost of making high volume production extremely inflexible.  The answer is apparently ""computer integration"" with multi-purpose machines linked together by digital communications networks and adaptively controlled by computers. 



In practice this means replacing flexible human workers with high quality sensory interpretative abilities as decision-makers by ""smart sensors"" with artificial intelligence.  However, adaptive controls are severely constrained by the capabilities of existing sensors and interpretative computer software, especially the latter.  Most existing sensors are narrow-band, producing at most a few bits of data per second for control purposes.  This provides enough information for a small class of machine control decisions, but is insufficient for part recognition, part orientation, or quality inspection. 



CIM also means that workpieces (components and work-in-progress) will have to communicate with machines, as machines will have to communicate with each other.  Thus, the true economic significance of recent breakthroughs in machine vision/taction is that they will finally unlock the door to CIM, or ""5th generation"" automation.  It is argued that the economics of machine vision/taction should not be assessed in the narrow context of specific tasks in direct competition with human workers but as the hitherto missing link that will permit all the elements of the factory of the future to communicate efficiently with each other so as to function as an organism rather than as a set of independent cells",The Role of Machine Sensing in CIM,https://core.ac.uk/download/33894700.pdf,"RR-89-013. Reprinted from Robotics and Computer-Integrated Manufacturing, 5(1):53-71  [1989]",10.1016/0736-5845(89)90030-6,,core
29194283,,"The financial industry is witnessing major changes. The financial enterprise is undergoing major business process renewal accompanied with the introduction of new technologies including electronic commerce; the financial market is shifting from an old to a new trading model that introduces major structural changes to the market and new roles for market participants; investment offers access to ever larger repositories of financial information and a wider choice of financial instruments to fulfill rising needs and expectations. In all these developments, there is a central role for human intelligence that can potentially influence the pattern of change and direct appropriate decisions in adapting to change. There is also a vital need for computer-based technology to support this human activity. 



The relation between human and computer activities in classical models for computer-based support is characterised by rigidity and framed patterns of interaction. The emphasis in such models is on automation, not only in respect of routine trading operations, but even of the role of market participants. An alternative culture is emerging through the use of advanced technologies incorporating databases, spreadsheets, virtual reality, multi-media and AI. There is an urgent need for a framework in which to unify the classical culture, in which mathematical financial modelling has a central place, with the emerging culture, where there is greater emphasis upon human interaction and experiential aspects of computer use. This thesis addresses the problem of developing software that takes into account the human factor, the integration of the social and technical aspects, human insight, the experiential and situated aspects, different viewpoints of analysis, a holistic rather than an abstract view of the domain of study, cognitive rather than operational activities, and group social interaction. The ultimate aspiration for this work is to transform the computer as it is used in finance from an advanced calculator to an 'instrument of mind'. 



Meeting the challenges of software support for finance is not only a matter of deployment, but also of software system development (SSD): this motivates our focus on the potential applications and prospects for an Empirical Modelling (EM) approach to SSD in finance. EM technology is a suite of principles, techniques, notations, and tools. EM is a form of situated modelling that involves the construction of artefacts that stand in a special relationship to the modeller's understanding and the situation. The modelling activity is rooted in observation and experiment, and exploits the key concepts of observables, dependencies and agency. The thesis extends the major findings of Sun (1999), in respect of the essential character of SSD, and its contextual and social aspects, by considering its particular application to the finance domain. 



The principles and qualities of EM as an approach to SSD are first introduced and illustrated with reference to a review of relevant existing models. The suitability of EM as a framework for SSD in finance is then discussed with reference to case studies drawn from the finance domain (the financial enterprise, the financial market, and investment). In particular, EM contributes: principles for software integration and virtual collaboration in the financial enterprise; a novel modelling approach adapting to the new trading model in the financial market; computer-based support for distributed financial engineering; and principles for a closer integration of the software system development and financial research development activities. This contribution is framed in a Situated Integration Model, a Human Information Behaviour Model, an Open Financial Market Model, a framework for distributed financial engineering, and a situated account of the financial research development cycle",An empirical modelling approach to software system development in finance : applications and prospects,https://core.ac.uk/download/29194283.pdf,,,,core
236584827,1994-01-01T08:00:00Z,"The search for a viable alternative clean energy source is a subject of extensive contemporary research. Among the different sources identified to date, hydrogen seems to be the most promising. One of the important aspects that needs to be addressed before it is put into active use, is its safety. Hydrogen is the lightest gas, and explodes violently posing a potential explosion hazard when it may leak into the enclosed chamber of an automobile, or an aeroplane or in domestic settings. Such chambers can be made safe if vented properly.The experiments to test hydrogen leaks in a 3-D enclosure were conducted in the indoor setting of Miami laboratory of National Oceanic and Atmospheric Administration to find how hydrogen would behave under real world conditions. The laboratory had light, electric and other fixtures, and was representative of any industrial and domestic settings. The scaled experimental enclosure kept in the laboratory had an inlet at the center of the bottom and was vented by providing the outlet at the top. Two leakage rates of hydrogen of 977 ml/min and 1942 ml/min were tested to find any burnable mixture in the chamber.A theoretical analysis was done to determine hydrogen concentration levels for the experimental geometry. Flows arising out of mass transport in an enclosure can be predicted by solving mass, momentum and advection-diffusion equations. These transient strongly coupled (because of buoyancy term) nonlinear partial differential equations were solved by finite difference methods. FLUENT, a computational fluid dynamics software was used to solve these equations and determine the concentration levels of hydrogen in air.Theoretical study was also conducted to analyze unsafe scenarios that might occur in real life environs, such as kitchen enclosures, due to hydrogen leaks. Besides hydrogen, concentration levels of other commonly used gaseous fuels viz., methane and propane in air were determined and compared regarding their nature and extent of the burnable cloud produced",Dispersion of hydrogen in air in vented three-dimensional enclosures,,Scholarly Repository,,,core
341985689,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds  Sarra Alqahtani, Rose Gamble  Adv. Sci. Technol. Eng. Syst. J. 2(3), 449-459 (2017); View Description Image Segmentation Using Fuzzy Inference System on YCbCr Color Model  Alvaro Anzueto-Rios, Jose Antonio Moreno-Cadenas, Felipe Gómez-Castañeda, Sergio Garduza-Gonzalez  Adv. Sci. Technol. Eng. Syst. J. 2(3), 460-468 (2017); View Description Segmented and Detailed Visualization of Anatomical Structures based on Augmented Reality for Health Education and Knowledge Discovery  Isabel Cristina Siqueira da Silva, Gerson Klein, Denise Munchen Brandão  Adv. Sci. Technol. Eng. Syst. J. 2(3), 469-478 (2017); View Description Intrusion detection in cloud computing based attack patterns and risk assessment  Ben Charhi Youssef, Mannane Nada, Bendriss Elmehdi, Regragui Boubker  Adv. Sci. Technol. Eng. Syst. J. 2(3), 479-484 (2017); View Description Optimal Sizing and Control Strategy of renewable hybrid systems PV-Diesel Generator-Battery: application to the case of Djanet city of Algeria  Adel Yahiaoui, Khelifa Benmansour, Mohamed Tadjine  Adv. Sci. Technol. Eng. Syst. J. 2(3), 485-491 (2017); View Description RFID Antenna Near-field Characterization Using a New 3D Magnetic Field Probe  Kassem Jomaa, Fabien Ndagijimana, Hussam Ayad, Majida Fadlallah, Jalal Jomaah  Adv. Sci. Technol. Eng. Syst. J. 2(3), 492-497 (2017); View Description Design, Fabrication and Testing of a Dual-Range XY Micro-Motion Stage Driven by Voice Coil Actuators  Xavier Herpe, Matthew Dunnigan, Xianwen Kong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 498-504 (2017); View Description Self-Organizing Map based Feature Learning in Bio-Signal Processing  Marwa Farouk Ibrahim Ibrahim, Adel Ali Al-Jumaily  Adv. Sci. Technol. Eng. Syst. J. 2(3), 505-512 (2017); View Description A delay-dependent distributed SMC for stabilization of a networked robotic system exposed to external disturbances",,'ASTES Journal',10.25046/aj020366,,core
475661028,,"The very small body of existing literature on content marketing consistently echoes the refrain that more research must be undertaken on the topic. My research, published since 2013, comprises the largest body of work on digital content marketing and content strategy. I have conducted and published more marketing-oriented research on digital content than anyone else in the field. Current literature examines content in narrow tranches, such as by platform or industry vertical. My research considers content as a practice, discipline, and strategic approach to marketing unto itself. This work provides an overview of digital marketing’s evolution and illustrates how it has diverged from initial expectations and offline corollaries: from advertising-centric marketing to new models in which advertising and media buying is either absent from the equation or constitutes a smaller proportion of strategy, which has shifted to owned and earned content models. My exploratory studies are collectively a holistic examination of content strategy and content marketing: creative and form factors, content’s intersection with other marketing forms, the evolution of technologies from publishing tools to more sophisticated applications of artificial intelligence (AI), Beacons, and sensors; the institutional and organisational challenges inherent in shifting organisations to become publishers and producers; and new forms of metrics and analytics. My research encompasses the following five areas: 1. People: How executives, employees, and consumers ideate, create, publish, amplify, and consume content. 2. Organisational Ecosystem: The roles played by brands, agencies, and technology vendors. 3. Technology: I have mapped and created a framework for content technology, assessing how content technology relates to and integrates with other marketing technologies as well as with enterprise software. 4. Strategy and Process: How content is conceived, created, disseminated, and functions within the paid, owned, and earned ecosystem as well as within the real- and near real-time enterprise demands. 5. Measurement: I developed a framework of ‘metrics that matter’ for content, relating content performance to enterprise strategy rather than the ‘soft metrics’ of likes and shares. Marketing that entertains, educates, informs, or provides utility meets consumer needs. This increases with the evolution of technology. Content (and marketing) now bleeds off screens and begins to permeate the physical world as everyday objects become ‘smart’ and imbued iii with content. Content is learning how to create itself, primarily via AI. My research scrutinises the implications of this evolution and content’s future state for consumers, brands, and businesses. Much of this work consists of firsts: the first research-based definitions of content marketing and content strategy; the first detailed study of content technologies and how they interrelate with other marketing and enterprise tools; the first research on native advertising and realtime marketing; the first research on where content resides within the enterprise (organisationally, globally, and interdepartmentally); and the first research on meaningful content measurement. While these areas are constantly evolving, my work provides the baselines and benchmarks to inform future research in the field","Content Marketing & Content Strategy: Identification of Research Trends, Best Practices, and Directions for Future Research",,,,,core
162667077,,"Evolutionary Computation (EC) has been an active research area for over 60 years, yet its commercial/home uptake has not been as prolific as we might have expected. By way of comparison, technologies such as 3D printing, which was introduced about 35 years ago, has seen much wider uptake, to the extent that it is now available to home users and is routinely used in manufacturing. Other technologies, such as immersive reality and artificial intelligence have also seen commercial uptake and acceptance by the general public. In this paper we provide a brief history of EC, recognizing the significant contributions that have been made by its pioneers. We focus on two methodologies (Genetic Programming and Hyper-heuristics), which have been proposed as being suitable for automated software development, and question why they are not used more widely by those outside of the academic community. We suggest that different research strands need to be brought together into one framework before wider uptake is possible. We hope that this position paper will serve as a catalyst for automated software development that is used on a daily basis by both companies and home users",Is Evolutionary Computation evolving fast enough?,https://core.ac.uk/download/162667077.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/MCI.2018.2807019,,core
42623694,,"Due to current trends in adopting flexible manufacturing philosophies, there has been a growing interest in applying Artificial Intelligence (AI) techniques to implement these manufacturing strategies. This is because conventional computational methods alone are not sufficient to meet these requirements for more flexibility. This research examines the possibility of applying AI techniques to process planning and also addresses the various problems when implementing such techniques.



In this project AI planning techniques were reviewed and some of these techniques were adopted and later extended to develop an assembly planner to illustrate the feasibility of applying AI techniques to process planning. The focus was on assembly process planning because little work in this area has been reported. Logical decisions like the sequencing of tasks which is a part of the process planning function can be viewed as an AI planning problem.



The prototype Automatic Assembly Planner (AAP) was implemented using Edinburgh Prolog on a SUN workstation. Even though expected assembly sequences were obtained, the major problem facing this approach and perhaps AI applications in general is that of extracting relevant design data for the process planning function as illustrated by the planner. It is also believed that if process planning can be regarded as making logical decisions with the knowledge of company specific data then perhaps AAP has also provided some possible answers as to how human process planners perform their tasks. The same kind of reasoning for deciding the sequence of operations could also be employed for planning different products based on a different set of company data.



AAP has illustrated the potentialities of applying AI techniques to process planning. The complexity of assembly can be tackled by breaking assemblies into sub-goals. The Modal Truth Criterion (MTC) was applied and tested in a real situation. A system for representing the logic of assembly was devised. A redundant goals elimination feature was also added in addition to the MTC in the AAP. Even though the ideal is a generative planner, in practice variant planners are still valid and perhaps closer to manual assembly process planning",Artificial intelligence techniques for assembly process planning,https://core.ac.uk/download/42623694.pdf,,,,core
327015908,,"Recent advancement in predictive machine learning has led to its application in various use cases in manufacturing. Most research focused on maximising predictive accuracy without addressing the uncertainty associated with it. While accuracy is important, focusing primarily on it poses an overfitting danger, exposing manufacturers to risk, ultimately hindering the adoption of these techniques. In this paper, we determine the sources of uncertainty in machine learning and establish the success criteria of a machine learning system to function well under uncertainty in a cyber-physical manufacturing system (CPMS) scenario. Then, we propose a multi-agent system architecture which leverages probabilistic machine learning as a means of achieving such criteria. We propose possible scenarios for which our architecture is useful and discuss future work. Experimentally, we implement Bayesian Neural Networks for multi-tasks classification on a public dataset for the real-time condition monitoring of a hydraulic system and demonstrate the usefulness of the system by evaluating the probability of a prediction being accurate given its uncertainty. We deploy these models using our proposed agent-based framework and integrate web visualisation to demonstrate its real-time feasibility",Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System,,,,,core
56101873,1992-01-01T00:00:00,"Production automation has been the focus of the research to improve product quality and to increase productivity. Implementation of computer-based untended machining has attracted great attention in the manufacturing community. In this paper, a framework for sensor-based intelligent decision-making systems to perform on-line monitoring is proposed. Such a monitoring system interprets the detected signals from the sensors, extracts the relevant information, and decide on the appropriate control action. Emphasis is given to applying neural networks to perform information processing, and to recognize the process abnormalities in a machining operation. A prototype monitoring system is implemented. For signal detection, an instrumented force transducer is designed and used in a real time turning operation. A neural network monitor based on a feedforward back- propagation algorithm is developed. The monitor is trained by the detected cutting force signal and measured surface finish. The superior learning and noise suppression abilities of the developed monitor enable high success rates for monitoring the cutting force and the quality of surface finish under the machining of advanced ceramic materials",A Neural Network Approach to On-line Monitoring of a Turning Process,,,,,core
417686134,,"Nowadays, many small or medium size manufacturing companies are struggling to identify the right solution to tackle the problems of long production cycle time, poor quality and expensive cost in their manufacturing processes. In order to facilitate overcoming these struggles for small or medium companies, this thesis presents a research that intends to develop an effective knowledge management system, which includes a flexible sensor based cost model for calculation of unit manufacturing cost of a product, for small or medium size manufacturing companies with the potential of taking the advantages of Industry 4.0 which is an approach of generation of smart facilities with smart cyber-physical systems. (Thames & Schaefer, 2016) In this research, it is identified that the facilitation of suitable sensors to observe and monitor manufacturing processes could assess the efficiency of the manufacturing processes in real time and help the manufacturing process cost calculation to be more accurate. Besides, the cost model is converted from the traditional cost model to a dynamic one through adding flexibility to the model via two ways: Transferring the data of the cost parameters from the sensor on a machine to the knowledge support system directly via the internet and measuring the efficiency of a manufacturing process by calculating the utilization rate of the machine through utilizing the power values of the machine that comes from the sensor. In order to achieve flexibility in the cost model, three case studies were done. In each case study, manufacturing processes are observed via a power meter that can be easily assembled on the machines of interests. Subsequently, the efficiency measurement of the manufacturing process is provided through classifying the power values of the machine via an artificial intelligence method and the value of the utilization rate of the machine is obtained via utilizing these classified power values in Matlab Software. Lastly, the efficiency measurement of the manufacturing process and calculation of the unit manufacturing cost of a product are gathered into a knowledge management system that is created by utilising Microsoft Access Software. In this project, it is aimed to create a knowledge management system that has some benefits to small or medium companies. These benefits are presented below: • The knowledge management system has a capability to collect data via the internet and this capability provides a foundation for establishing or improving Industry 4.0. approach in a facility. • Another key beneficial feature of the database is the calculation of the manufacturing cost taking account of the real utilization rate of the machine and the data that comes 1 from the sensors on the machine. Therefore, the cost model becomes dynamic which means that the user can reflect the changes in a manufacturing process to the cost model and also it can monitor the efficiency of the manufacturing process via the value of the utilization rate of the machine. In this project, how the real utilisation rate is monitored and integrated into the general cost model is presented together with the application cases of two industry companies and Liverpool John Moores University laboratory",A MANUFACTURING COST ESTIMATION BY UTILIZING A NOVEL SENSOR BASED COST MODEL IN A KNOWLEDGE MANAGEMENT SYSTEM,https://core.ac.uk/download/417686134.pdf,,10.24377/LJMU.t.00014841,,core
200996908,,"Predicting the future is no longer about the mystical reading of natural and celestial phenomena. Today it is all about data.

The Real Prediction Machine (RPM) is a domestic product that uses big and small data, in combination with machine learning and predictive modelling to make predictions about specific future events.



Contemporary use of digital networked technology, such as personal computers and smart phones, is effectively feeding a live global human behaviour laboratory with data scientists experimenting on an (often) unknowing pool of billions. The futures that emerge from this research are as yet mostly unknown, but there are hints – as this data accumulates it can be analysed, mined and used in algorithms; patterns or trends invisible to the human observer can be identified; and seemingly random events become predictable. At this time prediction algorithms are predominantly being exploited by big industries such as banking, insurance and commerce, or examined in massive research projects such as the EU funded FuturICT project. They are, however, making surreptitious steps into our lives through tailored internet browsing and predictive shopping with occasional Kafkaesque consequences. 

RPMs exploits the potential of this technology motivated not by the interests of industry and research but by the more emotive and personal needs/desires of people – this has the purpose of communicating the transformative potential of big data in domestic life, and asking if the future possibilities described by the project are desirable.



The concept

When things fail they rarely do so instantaneously but through a gradual process of deterioration. Based on this observation, predictive analytics, through the deployment of sensors in pertinent places and contexts, can determine the when things begin to fail. Such techniques are increasingly used in the mechanical and structural world - to predict for example when a vehicle or bridge might be in need of pre-emptive maintenance.



The RPMs use similar techniques but in the context of human everyday life to predict anything from health related events such as a heart attack to more emotive forecasts such as a domestic argument.



Once the event has been chosen the necessary and available data streams, from local sensors to RSS feeds, determined they are fed into the prediction algorithm - the output of which controls the visual display on the prediction machine. This informs the viewer if the chosen event is approaching, receding or impending. 



The Real Prediction Machines was commissioned by the Crafts Council for the exhibition Crafting Narrative. This explored how contemporary designers and makers use objects as mediums to tell stories. 



Project developed in collaboration with Subramanian Ramamoorthy and Alan Murray. Engineering by Nick Williamson",Real Prediction Machines,https://core.ac.uk/download/200996908.pdf,Self Published,,,core
77027,,"Most practitioners recognise the important part accurate estimates of development effort play in the successful management of major software projects. However, it is widely recognised that current estimation techniques are often very inaccurate, while studies (Heemstra 1992; 

Lederer and Prasad 1993) have shown that effort estimation research is not being effectively transferred from the research domain into practical application. Traditionally, research has been almost exclusively focused on the advancement of algorithmic models (e.g. COCOMO (Boehm 1981) and SLIM (Putnam 1978)), where effort is commonly expressed as a function of system size. However, in recent years there has been a discernible movement away from algorithmic models with non-algorithmic systems (often encompassing machine learning facets) being actively researched. This is potentially a very exciting and important time in this 

field, with new approaches regularly being proposed. One such technique, estimation by analogy, is the focus of this thesis. The principle behind estimation by analogy is that past experience can often provide insights and solutions to present problems. Software projects are characterised in terms of collectable features (such as the number of screens or the size of the functional requirements) and stored in a historical case base as they are completed. Once a case base of sufficient size has been cultivated, new projects can be estimated by finding similar historical projects and re-using the recorded effort. To make estimation by analogy feasible it became necessary to construct a software tool, dubbed ANGEL, which allowed the collection of historical project data and the generation of 

estimates for new software projects. A substantial empirical validation of the approach was made encompassing approximately 250 real historical software projects across eight industrial data sets, using stepwise regression as a benchmark. Significance tests on the results accepted 

the hypothesis (at the 1% confidence level) that estimation by analogy is a superior prediction system to stepwise regression in terms of accuracy. A study was also made of the sensitivity of the analogy approach. By growing project data sets in a pseudo time-series fashion it was possible to answer pertinent questions about the approach, such as, what are the effects of outlying projects and what is the minimum data set size? The main conclusions of this work are that estimation by analogy is a viable estimation 

technique that would seem to offer some advantages over algorithmic approaches including, improved accuracy, easier use of categorical features and an ability to operate even where no statistical relationships can be found",An Empirical investigation into software effort estimation by analogy,https://core.ac.uk/download/77027.pdf,,,,core
236529994,,",/
/,r""""M
t:-/~, /-
'I, f tZ~....< (;
/~;-'G.I
£eL c. N'I~ ""'Tk N~ (}p»~ ;""a..7/U..u-'-ti"" '7P- 3(;.3 ~/'!f'V~2<?r:-.i/O;- / - r 79S
his view, is the denial of explanation itself.
Dennett attempts to persuade his read-ers
that the problem of consciousness is
just a childishly tenacious attachment to
Descartes"" version of consciousness. We
must abandon, he says, this ""Cartesian
Theatre"": the notion- tbat the ""1"":of the ego
responds to an observer of events which
are ""present to consciousness"".
Dennett's starting-point is ""parallel dis-tributed
processing"". the idea that the
brain is not so much like a single-sequence
digital computer but, rather. a vast array of
processing units simultaneously at work
which relate to each other through myriad
connections. It is this new technology of
computing, a system which does not re-quire
a single controlling processer, he
argues, that ""blazes the first remotely
plausible trails of unification in the huge
terra incognita lying between the mind
sciences and the brain sciences"". Con-sciousness
for him is an illusion which
emerges from the sheer complexity of
parallel processing units.
At the heart of his ""explanation"" is a
metaphor taken from the art of narrative
---:'the concept of ""multiple drafts"", as in
the many drafts of a text that will eventual-ly
take on the ""semblance"" of a completed
version with a compelling internal integri-ty.
The self, then, the knowing I,"" exists
only in tenns of multiple drafts of stories
that we spin about ourselves on the ""virtual
machine"" of the brain.
TIIE TABLET 25 June-l994
I '-tJ fV~
.c.
From soul to software
n j . • John Cornwell-f/.
'""1~' ..d~.""'-1 tr '"" '~;.. '/ ,-..-,.
The advance of neuroscience has brougbt new.aa.pts to explain consciousness
- or to explain it away, according to tbe critics. 'I1Iisweek end next, the director
or the Science and Human Dimension projod at Jesus CoUegeCambridge
examinesIbe work of two or Ibe leading e~ f1l eliminativemateria6sm.
No human phenomenon has so resisted
scientific explanation as the idea of the
self, traditionally known as the soul;
human higher-order self-consciousness,
that unique inner observation point from
which we view the world.
Consciousness, wrote the Princeton
psychologist, Julian Jaynes, somewhat
lushly, is ""a secret theatre of speechless
monologue end prevenient counsel, an
invisible mansion of all moods, musings
and mysteries, an infinite resort of
i:lisappointments and discoveries"".
The notion of consciousness seems to
stand at the frontier of mind and matter,
subject and object, soul and body, deter-minism
and free will, life and death. Not
only is it the great, final mystery of human
experience; there is something within
many of us that wishes to preserve that
mystery at all costs. Some unconscious
instinct wants to resist the temptation
to explain it, or explain it away; as if a
final explanation, or elucidation, would
threaten the very core of one's being.
Julian Jaynes carved a niche for himself
in the history of psychology with his book
The Origin of Consciousness in the Break-down
of the Bicameral Mind, published in
1976. He came down firmly on the side of
the reductionists, arguing that conscious-ness
is an illusion evolved 3,(0) years ago
as a result of hallucinated internal dia-logues
in times of stress. Curiously, he
argued in conclusion that contemporary
scepticism about consciousness would pro-voke
a fragmentation of authority, values,
the destruction of the cohesion of com-munities
and societies. From this distance
his prophecy seems prescient.
By 1976 deconstruction of the self was
already fashionable under the influence of.
the French philosopher Jacques Derrida
and was soon to spread by a process of
adaption and imitation throughout depart-ments
of literary studies in Europe and
North America. In his novel Nice Work,
published in 1988, David Lodge satirised a
fashionable version familiar in depart-ments
of English literature throughout
Britain in the Eighties:
There is no such thing as the ""self"" on which
capitalism and the classic novel are founded
- that is to say. a finite, unique soul or
essence that constitutes a person's identity;
there is only a subject position in an infinite
web of discourses - the discourses of power,
sex, family, science. religion, poetry, etc.
David Lodge was perhaps unaware that
the object of his satire would soon flourish
in the realms of popularised philosophy of
mind ia the 1990s. Two recent books,
Daniel C. Dennett's Consciousness
ExplaiJooll'(published in 1991). and Francis
~~~!~~~i:,~~n~~';;~~~S~~d~:
May, exemplify a trend in popular exposi-tion
. towards a self-confident denial of
traditional ideas of personhood. Dennett,
a philosopber. and Crick, a bioJogist, have
published new, and related, hypotheses
about 1he self. accessible to. wide
readership!. and representing a formidable
challenge to Christianaccounts of human
identity. :.'
Dennett"" an American in his late fifties,
a former pupil of Gilbert Ryle, and now a
De1Ulett had a way of
delivering left hooks into the
air, as if there were spooky
Cartesians out there to be
thumped into submission.
professor at Tufts University in Boston,
came to Cambridge in the autumn of 1991
as part of a whistle-stop tour to publicise
his book ..Lady Mitchell Hall was fined to
capacity. For an hour he assailed them in a
rapid, combative voice. He had a way of
delivering left hooks into the air, as if there
were spooky Cartesians out there, still
believing in mind-body dualism, to be
thumped into submission.
Dennett is no mere ""promissory mater-ialist""
(Sir Jobn Eccles's phrase for those
cognitive scientists who believe that the
mind-body problem could be solved at
some point in the future). He believes that
we have fOlmd the explanation of mind-body
relationship, and he is convinced that
his solution 8 on the side of human dignity.
He portrays his deconstructionist view of
the self DOt only as a moment of new
intellectual maturity, but as an affirmation
rather than a denial of personhood. Con-sciousness
Explained is an important mile-stone
of contemporary philosophy, pre-cisely
because it denies a version of the
self, of conscious experience, that is clearly
respected and held dear by so many who
ponder these matters. Yet Dennett - the
eliminative materialist par excellence r:
manages to make his opponents, who
stress the mystery, the imponderable
nature of oonsciousness, sound pessimistic,
nihilistic, reductionist, negative. Denial, in
We are almost constantly engaged in pre-senting
ourselves to others, and to ourselves,
and hence representing ourselves - in Jan-
:~,guage and gesture, external and internal.
The most obvious difference in our environ-
. ..' ment that would explain this difference in
our behaviour is the behaviour itself. Our
human environment contains not just food
and shelter, enemies to fight or flee, and
conspecifics with whom to mate, but words,
words, words ... protective strings of narra-ti,,~.
Perhaps the most remarkable aspect of
Dennett's theory is his admission that his
materialist explanation involves the sub-stitution
of one set of metaphors for,
another. This comes in the startling two .
sentences with which he concludes his
book:
I haven't replaced metaphorical theory, the
Cartesian Theatre. with a non-metaphorical
(""literal, scientific"") theory. All I have.done,
really, is to replace one family of metaphors
and images with another, trading in the
Theatre, the Witness. the Central Meaner.
the Figment, for Software, Virtual
Machines. Multiple Drafts, a Pandemonium
ot Homunculi!
This Jlublication
Is available
in microform
from University_
Microfilms
International.
796
TO RUSSIA WITH LO·YE .' ;'~'£""_-' r-,.,/:.,_t:.... t,.
We can't Just talk about unity:' we have'to:llvelt.'
Many of the 6,0.0.0.Russian Orthodox 'priests are. d'esp~~tely
poor, earning £20. a month or less. They have to work to feed
their families, instead of looking after their faithful. ,....-
In co-operanon with the Orthodox bishops Aid to the Church
In Need is offering support to all the Orthodox priests in
Russia - and to all the Catholic priests as well. .
This year, we want to give £70.0. to every priest in Russia.
Please help us in this ecumenical initiative -:- responding with
love to the Pope's call for unity with our Orthodox sister-churches.
Al\ Aid to the Church in Need
•• 124 carshalton Road, Sutton, Surrey SM1 4RL
Tel: 081-642 8888 Reglst_ Charily No.285582
Name .•.•.•.•.•...•.........•.•.•....•.•.•.•..•.•.....•...•.•.•.•..•.......•.......•........•.•.•.. ,..•.•.•..
Address .•.•.•.•..•.•...•..•..•...•.•.•.•.•........•.•.•.•.•.•.•.•.•.•. :•.•..•.•.•.•.•.•.•.•.. :.•.•.•.•.•.•..
I enclose 0 £150 £500 £1000 £2500£7000
o Other £ .
o for Russian Orthodox priests
o for Calholic priests in Russia and Eastern Europe
I enclose a cheque OR
Please debit my VISN ACCESs/CAF CharityCard account no.
00000000 00000000
Expiry date--.1_ Tab25/6
THE TABLET 25 June 1994
.imet Daniel Dennett at the launch party
in London for his book. With reason, I
could congratulate him on. his success.
Consciousness Explained had been greeted
with ecstatic reviews on both sides of the
Atlantic, and declared one of the New
York Times ten best books of 1991. And
yet not all his populist press reviewers were
so adulatory. so uncritical. Sitting in the
comer of the library of the Travellers""
Oub, the venue for the launch party. was
the lugubrious figure ui Professor Stuart
Sutherland •. director of the centre for
research on perception and cognition at
Sussex University. perhaps pondering the
article he had written on Dennett's book
for the Boston Glooe, the principal daily
paper in Dennett's home town.
Sutherland's overall verdict in the review
calls in question the curious hubris of
eliminative materialism in going well
beyond what can be inferred from the logic
of its position. His criticisms a~e an exam-ple
of clear thinking - on a topic that
professional philosophers all too often tend
to obfuscate among themselves:
Dennett believes that the brain is made up of
a great many small unintelligent specialist
centres, whose combined activity is responsi-
... ble for intelligent behaviour and constitutes
consciousness: there is no controlling centre
where all the information is put together and
which is the seat of consciousness. Even if he
were tight -and he may be wrong since only
, a very small number of ideas can be enter-tained
in consciousness at anyone time -
.this would.not prove that consciousness and
brain processes were identical. Moreover, an
army, a. factory or an ant-hill are also
, organised in separate units working for the
most part independently, yet we are under
no temptation to ascribe to them some form
of collective consciousness.
Philosophers are of course the supreme
intellectuals and Dennett believes that if a
-computer could think intelligently it would
be conscious. But would we really consider
something conscious that evinced no sign of
emotion? A computer. could never bave.,
'genuine human emotions for the simple
:.reason tbat it does not have the same
biological constraints. How could an entity
.. ,that does not reproduce sexually feel real
«sexual jealousy?
And what of Dennett's assurance that
his brand of eliminative materialism offers
no insult to human dignity? In the final
chapter of Consciousness Explained he
attempts to make good his promise to
restore a maturer human dignity. He refers
there to ""that delicate part of our belief
environment concerned, with the disposi-tionof
our bodies after death"".
Few of us, he says, would approve of the
idea of disposing of our dead in plastic bags
as trash, not because we believe ""that
corpses can actually suffer some indignity"".
but because that corpse is the ""body of
dear old Jones, a Centre of Narrative
Gravity that owes its reality as much to our
collaborative efforts of mutual heterophe-nomenologicaJ
interpretation as to the
body that is now lifeless"".
The term ""heterophencmenological"" is
interesting in this context. It is used by
THETABLET 25 June 1994
anthropologists in investigating unfamiliar
cultures. The heterophenomenological in-vestigator
neither challenges nor accepts as
entirely true the assertions of his subjects,
but rather maintains a constructive and
sympathetic neutrality in the hopes of
compiling a definitive description of their
world.
Adopting this perspective, Dennett goes
on:
Treating a corpse ""badly"" may not directly
harm any dying person, and certainly doesn't
harm the corpse, but if it became common
practice and this became widely known (as it
would), this wou1d significantly· change the
belief environment that surrounds dying.
People would imagine tbe events that were
due 10 follow their demise differently from
the way they now imagine them, and in ways
that would be particularly depressing. Maybe
not for any good reason, but so what? If
people are going to be depressed, that in
itself is a good reason for not adopting a
policy.
It is interesting that Dennett should
choose a dead body rather than a living, or
a dying, body, for his thought experiment.
For presumably he would invoke the same
rationale of respect with the living as with
the dead. Why should we not treat a living
body as a piece of garbage? Because,
Dennett would say, tbis might change the
""belief environment that surrounds the
living""; and that would be wrong because
they might be depressed.
So what if two groups of people - say,
Bosnians and Serbs - find the ""belief
environment"" of their neighbours objec-tionable
to the point where their geog-raphical
co-existence becomes ""depress-ing""?
Would this justify mutual attempts at
ethnic cleansing? .
I do not wish to suggest that Dr Dennett
would condone any such tbing; but it is
surely crucial that we test an account of
human respect to see if it answers -every
case.
Our reluctance to depress people, De-nnett
continues, does not mean that we
should not open the Pandora's Box wbich
may lead to the exposure of their myths:
myths, in bis view, including such nonsense
as individual personhood, and certainly the
traditional idea of the soul. ""Those who
are worried"", he says, ""about the costs
threatened by this unasked-for enlighten-ment
should take a hard look at the costs of
the current myths. Do we really tbink what
we are currently confronted with is worth
protecting with some creative obscurant-ism?""
Currently confronted with what? And
now, at last, he conjures up a few scenarios
more to the point than the fate of corpses:
Do we think, for instance, that vast resources
should be seraside to preserve the imaginary
prospects of a renewed mental life for deeply
comatose people, while there are no re-sources
to spare to enhance the desperate,
but far from imaginary, expectations of the
poor? Myths about the sanctity of life, or of
consciousness, cut both ways. They may be
useful in erecting barriers (against euthana-sia,
against capital punishment, against abor-tion,
against eating meat) to impress the
unimaginative, but at the price of offensive
hypocrisy or ridiculous self-deception among
the more enlightened ..
The moral relativism inf~rred by placing
meat-eating in the same sentence as abor-tion
speaks for itself; bllt what can he
possibly mean?
He is saying that the ""sanctity of life"" is a
myth which the enlightened should patro-
'ruse with a heterophenomenoiogical in-dulgence
until such time as it conflicts with
mature principles as understood by the
imaginative; at which point it will be just
too bad if the unimaginitive are depressed
by an offence against their ""belief environ-ment"",
So what, in_the ~alamuYSis',is Dennett
The contemporary crisis over
human identity lfnddignity is
every hit as grave and far-reaching
as one could have
supposed it to he.
putting forward as the basis of human
dignity? How- does, he propose that we
should transcend all these 'degenerate
myths? What is a human being? His answer
is as follows: '
The campaign that used to be waged against
materialism has already succumbed .to
797
embarrassment, and the campaign against
""strong AI"" [the theory that human beings
and robots are equivalent), while equally
well-intentioned, can offer only the most
threadbare alternative models of the mind.
Surely it would be better to try to foster an
appreciation for the non-absolutist, non-intrinsic,
non-dichotomised grounds for mor-
'at concern that can co-exist with our increas-ing
knowledge of the inner workings of that
most amazing machine, the brain.
So this is bis challenge. If all the argu-ments
that can be brought against the
proposition of an equivalence between
minds and macbines are threadbare, our
account of what a human being is should
proceed from that proposition.
It is interesting that Dennett should see
the issue of consciousness not SO much as
an enquiry or a debate but as a ""cam-paign"",
to be won or lost. Interesting too
that he should deem materialism's success
a consequence of the ""threadbare"" altema-tives
put forward by the ""opposition"". The
fact that his campaign had been assisted by
the widely adulatory reception of Con-sciousness
Explained in newspapers and
periodicals not especially committed to
discussing issues in moral philosophy, sure-ly
demonstrates that the contemporary
crisis over human identity and dignity is
every bit as grave and far- reaching as one
could have supposed it to be.
1. Penguin edition. (1993) £8.99.
2. Simon & Schuster, £16.99.
••• In a second article nut week, John Cornwell
appraises Francis Crick's new book. ' .
WITHAM WELD
solicitors
For legal advice and assistance:
• Churches • Charities & Trusts
• Schools & ColIeges • Employers and Employees
• Companies • Private Individuals
• Clubs and Licensees • Hospitals & Nursing Homes
. • Local Councils and Societies
Can we help you? We are happy to let our reputation
speak for itself. For over 200 years we have been closely
associated not only with the Catholic Church but with
every kind of client and, today, our aim remains the same
- to provide the best quality legal service founded on
expertise and experience: this is ensured by a programme
of continuous improvement and innovation which aims at
providing the highest quality professional service whilst
adhering to traditional values.
For advice and assistance on legal matters or to request a
copy of our brochure please contact:
Mrs Alexa Beale or Patrick Herschan
70 ST GEORGE'S SQUARE, LONDON SWl V 3RD
TEL: 071-8218211 FAX: 071·630 648",Item 0004,,Saint Louis University Libraries Special Collections,,,core
343121984,,"Condition monitoring (CM) deliveries significant benefits to the industry by minimising breakdown losses and enhancing the safety and high-performance operation of machinery. However, the use of data acquisition systems with multiple sensors and high sampling rates leads to massive data and causes considerably high cost for purchasing and deploying hardware for data transmission, storage and processing. Hence, data compression is crucial and important to reduce the data size and speed up the calculation for the development of intelligent machine CM systems. Although data compression has received high attention in many fields, few researchers have focused on their research in the field of machine CM. Therefore, this PhD research concentrates on investigating novel and high-performance data compression algorithms according to the characteristics of one-dimensional (1D) and two-dimensional (2D) signals to solve the bottleneck of the massive data transmission, and hence improve the performance of remote and real-time machine CM systems.





The research is carried out according to a compound experimental and analytic route based on a wireless senor network. To demonstrate the effectiveness of data compression based techniques for CM, the prototype of an intelligent wireless sensing system is developed using cost-effective micro-electromechanical systems (MEMS) accelerometers and the Bluetooth low energy (BLE) communication module. Moreover, various waveform parameters with low cost computing in time and frequency domains are investigated and identified that RMS is the most effective parameter to give good indication for the leakage in a piping system, showing that data compression via statistics is effective and thus indicates that the performance of data compression for CM highly depends on applications.





Subsequently, high-performance but high-complexity methods are proposed base on dimension reduction, sparse representation, feature extraction and advanced compressive sensing (CS) for fault diagnosis of rotating machinery with 1D or 2D signals, which have the potentials to be implemented on MEMS modules in a wireless sensor network (WSN) in future. Firstly, a compression scheme based on dimension reduction is proposed to extract the periodic characteristics of the 1D vibration signal. Recurrence plot (RP) of vibration phase space trajectory and its quantification indicators, as well as principal component analysis (PCA), are combined to realize feature extraction, compression and fault classification for a tapered roller bearing system.





Secondly, a two-step compression method is performed on 1D vibration signals based on frequency shift, adaptive sparse representation and CS is explored to overcome the problem of the large quantity of data storage for ball bearing fault diagnosis. Simultaneously, this compression method has the capability to reconstruct envelope signals with noise elimination.Then, for 2D thermal images captured from a two-stage reciprocating compressor, the dense scale-invariant feature transform (SIFT) features indicating edge information are extracted and represented as a sparse matrix by sparse coding. The compressed features are used for the classification of six different types of faults with the support vector machine (SVM).





Finally, the advanced CS technique is exploited on pre-processing the 2D thermal images of gearboxes to realise intelligent fault classification with high accuracy of more than 99.81% by a typical deep learning algorithm, namely convolutional neural network (CNN). The CNN calculation speed is dramatically accelerated with compressed images. All these proposed approaches are evaluated by simulations and experiments, which verifies that they can reliably detect the fault types or classify different fault types with very high accuracy. Besides, the proposed data compression based intelligent CM approaches provide theoretical bases for maintenance-free CM systems because data compression can save the transmission bandwidth and power consumption for remote and real-time machine CM systems",Investigation of Data Compression Methods for Intelligent Machine Condition Monitoring,,,,,core
1496955,1994-01-01T00:00:00,"The issue of Command and Control (C2) is generally associated with the management infrastructure of large scale systems for warfare, public utilities and public transportation, and is concerned with ensuring that the distributed human elements of command and control can be fully integrated into a coherent, total system. Intelligent Autonomous Systems (IASs) are a class of complex systems that perform tasks autonomously in uncertain, dynamic environments, the management of which can be viewed from the perspective of embedded command and control systems. This thesis establishes a vision for the modular construction of intelligent autonomous embedded C2 systems, which defines a complex integration problem characterised by distributed intelligence, world knowledge and control, concurrent processing on heterogeneous platforms, and real-time performance requirements. It concludes that by adopting an appropriate systems infrastructure model, based on Object Technology, it is possible to view the construction of embedded C2 systems as the integration of a temporally assembled collection of reusable components. To support this metaphor it is necessary to construct a common reference model, or standards framework, for the representation and specification of modular C2 systems. This framework must support the coherent long term development and evolution in system capability, ensuring that systems are extensible, robust and perform correctly. In this research, which draws together the themes of other published research in object oriented systems and robotics, classical AI models for intelligent systems architectures are used to specify the overall system structure, with open systems technologies supporting the interoperation of elements within the architecture. All elements of this system are modelled in terms of objects, with well defined, implementation independent interfaces. This approach enables the system to be specified in terms of an object model, and the development process to be framed in terms of object technology, defining a new approach to IAS development. The implementation of an On-board Command and Control System for an Autonomous Underwater Vehicle is used to validate these concepts. The further application of emergent industrial standards in distributed object oriented systems means that this kind of component-based integration is scaleable, providing a near-term solution to generic command and control problems, including Computer Integrated Manufacturing and large scale autonomous systems, where individual autonomous systems, such as robots, form elements of a complete, total intelligent system, for application to areas such as fully automated factories and cooperating intelligent autonomous vehicles for construction sites",Embedded command and control infrastructures for Intelligent Autonomous Systems,,'University of Southampton',,,core
25159518,1993-03-23T00:00:00,"Distributed Artificial Intelligence (DAI) systems in which multiple problem solving agents cooperate to achieve a common objective is a rapidly emerging and promising technology. However, as yet, there have been relatively few reported cases of such systems being employed to tackle real-world problems in realistic domains. One of the reasons for this is that DAI researchers have given virtually no consideration to the process of incorporating pre-existing systems into a community of cooperating agents. Yet reuse is a primary consideration for any organisation with a large software base. To redress the balance, this paper reports on an experiment undertaken at the CERN laboratories, in which two pre-existing and standalone expert systems for diagnosing faults in a particle accelerator were transformed into a community of cooperating agents. The experiences and insights gained during this process provide a valuable first step towards satisfying the needs of potential users of DAI technology - identifying the types of changes required for cooperative problem solving, quantifying the effort involved in transforming standalone systems to ones suitable for cooperation and highlighting the benefits of a cooperating system approach in a realistic industrial application",Transforming stand-alone expert system into a community of cooperating agents,,,,,core
287348946,,"US;.L""'~'-'""
.-.....; ...... ""0&.1
UNIVERSIlY COUEGE
GRADU ATE
CATA L oG
ACADEMIC CALENDAR
The Graduate School conducts many courses and programs each term, scheduling them to meet at times and places convenient to students. Because of the variety the Graduate School offers, however, conflicts may develop that affect the dates. Therefore the follOWing dates are approximate, Actual times, dates, and locations may be found in the Schedule Of Classes, which is published for each fall, spring, and summer term.
ASchedule ofClasses for Statewide Programs is available by writing to the follOWing:
Office of Publications University of Maryland University College University Boulevard at Adelphi Road College Park, MD 20742-1672
The Schedule Of Classes may also be requested by telephoning 985-7800.
SPRING 1990
Schedule Of Classes available November Mail-in registration deadline December 16 Telephone registration January 3-5,
8-12 Off-campus registration January 4-24
(varies by site) Walk-in registration January 16-18 Classes begin January 22 Late registration January 22-23 Classes end Mav 10 Commencement May 19
SUMMER 1990
Schedule Of Classes available Mail-in registration deadline Telephone registration
Off-campus registration
Walk-in registration Classes begin Late registration Classes end April May 9 May 14-18, 21-25 May21-25 (varies by site) May 31 June 4 june 4-5 July 26
UNIVERSITY POLICY STATEMENTS
The provisions of this publication are not to be regarded as an irrevocable contract between the student and the University of Maryland University College. From time to time, changes are made in the general regulations and in the academic reqUirements. There are established procedures for making changes, procedures that protect the institution's integrity and the individual student'S interests and welfare. A curriculum or graduation requirement, when altered, is not made retroactive unless the alteration is to the student'S advantage and can be accommodated within the span of years normally required for graduation.
Accreditation
'The University of Maryland University College is accredited by the Commission on Higher Education of the Middle States Association of Colleges and Schools.
Nondiscrimination
The University of Maryland University College welcomes applications from prospective students and employees without regard to race, age, sex, physical or mental handicap, religion, national origin, or political affiliation,
University of Maryland University College Graduate School University Boulevard at Adelphi Road College Park, MD 20742-1614 Telephone: (301) 985-7155 or 1-800-888-UMUC, toll-free
FALL 1990
Schedule ofClasses available July Registration deadlines See Schedule Of Classes
Classes begin September 4
Classes end December 23
1
TABLE
INTRODUCTION TO THE
2
GRADUATE SCHOOL
Student Profile
2
M5. IN TECHNOLOGY MANAGEMENT
Overview ........................
General Curricula .................
Tracks. .............. .. ... ... ........ ..
Biotechnology Management. . . . . . . . . . .. Manufacturing Systems Management. . . . .. Technology Innovation and
Entrepreneurship .................. ..
Technology Systems Management . . . . . . .. Requirements for Admission Locations
MS. IN TELECOMMUNICA­TIONS MANAGEMENT
Overview ................. .... ..
Curriculum ..................... ..
17
1~ 17 18 18 18
19 19 19 19
20
20 20
OF
COr'\ITENTS
MASTER OF GENERAL ADMINISTRATION
Overview General Curricula Tracks
Commercial Real Estate Financial Management General Management Health-Care Administration . Hotel and Restaurant Management Human Resources Administration Management Information Systems Marketing for Managers .
3
.3 .3 .4 .4 .4 .4 5 5 5 5 5
State and Local Government
66 7
Requirements for Admission 21
Locations ... ........................ 21
.
Requirements for Admission .
Locations. ........ . .
EXECUTIVE MASTER OF 8 GENERAL ADMINISTRATION
Overview. . Curriculum . . . . . . . . . . . . . . Requirements for Admission Location
. .
. .
8 8 9 9
SENIOR EXECUTIVE MASTER OF GENERAL
10
ADMINISTRATlON
Overview. .... ..................... .. 10 Curriculum ..... ...................... .. 10 Requirements for Admission 11 Location. ........ .. 11
M5. IN COMPUTER 12 SYSTEMS MANAGEMENT
Overview.............. ....... .. 12 Curriculum .................... .. 12 Requirements for Admission 13 Locations .................... 13
MS. IN ENGINEERING 14 MANAGEMENT
Overview. ............ ............. .. 14 Curriculum ........................... .. 14 Requirements for Admission 15 Locations ........... .......... 15
MASTER OF INTERNATIONAL 16 MANAGEMENT
Overview. ........ ......... 16 Curriculum .................... 16 Requirements for Admission 17 Locations ...... ............. .. 17
RESEARCH AND 22 COMMUNICATION
Center for the Study of Future Management . 22 ColloquiumSeries .......... .. 22 Newsletter ............................ .. 22 Graduate Advisory Panel 23 CEO in Residence .... 23
COURSE DESCRIPTIONS 24
FACULTY 36
ACADEMIC REGULATIONS 42
Academic Load . 42 Advising. ............... . 42 Attendance . 42 Credit. ........ . 42 Grading System ........................ .. 43 Management Project 44 Program Completion Requirements 44 Scholarship 44 Time Limit for Degrees . . . . . . c.l4 Cos~ 44 Application c!4 TUition. ............... . 44 State-Residency Status 44 Financial Aid 45 Financial Aid Opportunities c.l5 Veterans' Benefits. .................. .. c.l5 Admission Information 45 Instructions and Regulations 45 Foreign and Foreign-Educated Applicants 46 Social Security Number. . --le6 Program Codes ........................ 46
APPLICATlON FOR 47 ADMISSION
CAMPUS MAP Inside Back Cover 2
INTRODUCTION
TOTHE
GRADUATE
SCHOOL
UNIVERSITY OF MARYLAND UNIVERSITY COLLEGE GRADUATE SCHOOL
The University of Marylalld University College Graduate School has sustained remarkable growth since its creation in 1978. Today, nearly 3,400 students are actively pursuing their graduate degrees. University College extends the resources of the University of Maryland System to adult, part-time students throughout the State of Maryland and the District of Columbia, and in 20 other countries on four continents. Widely recognized for excellence in continuing education, for more than 40 years University College has proVided a full range of educational opportunities to adult students at
times and locations that accommodate their career and family commitments.
The University College Graduate School has a decade of experience in educating and training managers from business, industry, and government.
Its graduate degree programs were developed specifically for the fully employed professional person to combine management concepts,
theories, and approaches with their practical
applications in various specialized areas and
technical disciplines. Each of the programs addresses the challenges managers face in today's globally competitive economic and
political environments.
Faculty, with extensive professional management experience in their areas of expertise, teach what they practice. The Graduate School selects these individuals for their current involvement in the topics covered in class, their teaching ability, and their advanced education.
Courses generally meet once a week in the
evening, at many locations throughout
Maryland, and in Washington, D.C.
Student Profile
The average age of the student body is about 3';. Forty-four percent of the students are female: about 60 percent are employed in the private sector of the economy.
These students are generally motivated to pursue graduate study for two reasons. Some are already employed as managers and want to develop their management skills further by acquiring knowledge of management research in a systematic way. Others are currently employed in a specific technical capacity and want to direct their careers onto a management track. In either case, students of Universitv College share the perspective that they have in some sense exhausted the equity in their undergraduate degrees and now need to reinvest in themselves and their futures bv pursuing a graduate degree. '
The University of Maryland University College Graduate School offers eight graduate degree programs:
•
Master of General Administration with optional tracks in
•
Commercial Real Estate
•
Financial Management
•
General Management
•
Ilealth-Care Administration
•
Hotel and Restaurant Management
•
Human Resources Administration
•
Management Information Systems
•
Marketing for Managers
•
State and Local Government
•
Executive Master of General Administration
•
Senior Executive Master of General Administration
•
Master of Science in Computer Systems Management
•
Master of Science in Engineering Management
•
Master of International Management
•
Master of Science in Technology Management with optional tracks in
•
Biotechnology Management
•
Manufacturing Management
•
Technology Innovation and
Entrepreneurship
•
Technology Systems Management
•
Master of Science in Telecommunications Management MASTER OF GENERAL ADMINISTRATION
OVERVIEW
As individuals assume increasing responsibility within an organization, the basis for success generally shifts from technological expertise to knowledge and skills in managing human resources. Furthermore. as the interval between a technological or scientific discovery and its implementation decreases, successful administrators find themselves more and more compelled to understand the implications of technological advancements for their organizations. as well as their ethical and moral
responsibilities to society at large.
The mastcr of general administration focuses on theories and skills of human resources management that are applicable in both public­and private-sector organizations. Important topics covered in the required courses include: methods and conduct of organizational assessments, the organization/environment relationship. long-range planning. organizational communication, budgeting and resources allocation. leadership, and organizational decision-making. Throughout the curriculum, major cmphasis is on the effects of rapid technological change on organizations and administrative processes.
Classes are generally held one evening a week for three hours on a semester basis, and on weekends. Teaching methods include lectures. discussion, students' presentations, case studies. and structured experiential activities.
Structure
The master of general administration can be pursued in either of two forms. depending on the student's goal: a degree program, leading to a master's degree. or a professional develop­ment program. leading to a certificate.
Degree Program
The degree program consists of three segments:
• Required Core Courses Seven (5-credit) courses consider methodologies relating to the study of organizations, forecasting models and long-range planning, organizational communication. and the responsibilities of the manager in a technological society. Trends in society. business, and organizational structure are examined. Students investigate the nature of organizations. budgeting and resources allocation, and leadership. The capstone course of the program explores issues in organizational decision-making.
•
Track Courses Four (3-credit) courses provide an opportunity for students to concentrate on skills and knowledge specific to their areas of administrative interest. The general management track includes two required courses plus any of the approved graduate electives in other tracks of University College.
•
Management Project This ( 3-credit) segment provides practical experience in selected management topics under the direction of a member of the University College faculty and an on-site supervisor. The management project is usually conducted at the student's place of employment; or the student may locate another appropriate site in some organization.
In each segment of the degree program, theory and concepts are presented to provide an opportunity for the student to develop and evaluate administrative skills. In each course, faculty members combine theoretical concepts with the practical application of usable skills.
Certificate Program
The certificate program was designed for students who are not interested in the degree program but who desire a sequence of graduate courses leading to a certificate in general administration. The program is composed of 21 semester hours of coursework, which are satisfied bv the core courses listed below under curricula..
GENERAL CURRICULA
Degree Program
The curriculum for students seeking the degree of master of general administration (M.G.A.) is as specified below. All courses except ADMN 650 may be taken in any order. It is recom­mended that ADMN 638 be taken the semester before ADMN 650. The numerical sequence does not necessarily refer to the order in which courses should be taken. Unless noted otherwise, all courses carry 3 credits.
• Core Courses (21 semester hours)
ADMN 601 The Manager in a Technological Societv ADMN 603 Planning and Forecasting for
Managers ADMN 625 Organizational Communication ADMN 630 Budgeting and Resource
Allocation for Managers ADMN 635 Techniques of Leadership ADMN 638 Organizational Assessment for
Managers
ADMN 650 Organizational Decision-Making (Prerequisites: ADMN 601, 603, 625.630.635,638.) This is the
M.G.A. capstone course that integrates all of the core courses.
MASTER OF
GENERAL
ADMINISTRATION
4
MASTER OF
GENERAL
ADrvllNISTRATION
• Track Courses (12 semester hours)
These courses are taken from the tracks in commercial real estate, financial management, general management, health-care administration, hotel and restaurant management, human resources administration, management information systems, marketing for managers, and state and local government. Courses specific to each track are listed following the track overview.
• Management Project (ADMN 690)
Students may begin the management project after completing 27 semester hours in the
M.G.A. program, including ADMN 650. As part of the exit requirement, students must demonstrate their ability to integrate material from the program in an oral examination.
Certificate Program
ADMN 601 The Manager in a Technological Society ADMN 603 Planning and Forecasting for Managers
ADMN 62'5 Organizational Communication
ADMN 630 Budgeting and Resource Allocation for Managers ADMN 63'5 Techniques of Leadership ADMN 638 Organizational Assessment for
Managers
ADMN 650 Organizational Decision-Making (Prerequisites: ADMN 601, 603, 625, 630, 63'5, 638.)
TRACKS FOR THE MASTER OF
GENERALADMW~TRAnON
The M.G.A. tracks give students an opportunity to concentrate their courses in a cohesive block. The track each student chose is specified on transcripts at graduation.
Commercial Real Estate (12 semester hours)
The courses in the commercial real estate track survey the financial, legal, and tax issues created by the constantly changing commercial (nonresidential) real estate industry. The track is intended for people seeking managerial expertise in the field of commercial real estate.
ADMN 681 Comprehensive Property
Management ADMN 682 Investment Analysis and Strategies ADMN 683 Legal Issues of Commercial Real
Estate Management ADMN 684 Commercial Real Estate Development
NOTE: These courses are not intended to license individuals as real estate brokers in the State of Maryland.
Financial Managemt~nt (12 semester hours)
The track in financial management is intended for people seeking to exercise managerial responsibilities over the financial functions of their organizations, or for those who plan to be general managers to strengthen their knowledge of and skills in the financial aspects of their organizations.
ADMN 631 Financial Information for Managerial Dlecisions ADMN 632 Financial Markets and Cash Management ADMN 633 Long-Term Financing of Organizations
Plus one of the following:
ADMN 637 Legal Aspects of Administration
or
ADMN 639 International Financial Management
General Management (12 semester hours)
The track in general management allows students to choose any two courses in any of the tracks for which they have the prerequisites; students must also take the two required courses listed below. Students may take any two other graduate courses at University College, or they may transfer courses taken at other universities according to the rules stated under Transfer Credit. The M.G.A./GM is the least structured track, allOWing the broadest possible choice of courses. Students who do not wish to specialize in any particular track should list the general management option on theJir applications.
ADMN 637 Legal Aspects of Administration ADMN 640 Information Systems for Managers Health-Care Administration ( 12 semester hours)
The health-care track prepares students to assume administrative and managerial positions in health-care institutions. Although the program is broadly based on hospital administration, the content is diverse enough to provide vocational competence in a wide range of health-care organizations. Previous experience in health-care-delivery systems is not a requirement for entry, but it can reasonably be expected that many health-care personnel will choose this track to acquire managerial skills.
ADMN 670 The Health-Care System ADMN 672 Financial Management for Health-Care Organizations ADMN 673 Legal Aspects of Health-Care Administration ADMN 674 Health-Care Institutional Organization and Management
Hotel and Restaurant Management (12 semester hours)
The hotel and restaurant management track provides a concentration in the theory of and practical approaches to the planning, marketing, and operations functions of the hospitality industry. These courses are an appropriate introduction to those relatively new to the industry, as well as a comprehensive overview of the components of management for those already in the field.
ADMN 67'; Hotel Operations ADMN 676 Food, Beverage, and Hotel Management ADMN 677 Marketing, Planning, and
Operations in the Hospitality
Industrv
ADMN 678 Property Management
Human Resources Administration
(12 semester hours)
The track in human resources administration provides a concentration of theory and practice as used by human-resources and personnel­department executives and specialists. The courses can help round out the capabilities of human-resources personnel who have specialized in particular aspects of the field, and they provide applied instruction for individuals considering a change to a career in the administration of human resources.
ADMN 661 Employee Relations
ADMN 662 Personnel Issues and Practices
ADMN 663 Job Analysis, Assessment, and
Compensation
ADMN 664 Organization Development and
Change
Management Information Systems (12 semester hours)
The management information systems track provides four courses on the ways a manager can use computer-based information systems to increase productivity and effectiveness. This track is structured to accommodate the needs of students who have little or no experience with computers as well as those with advanced computer skills.
ADMN 640 Information Systems for Managers·
ADMN 641 Construction of Computer-Based Information Systems (Prerequisite: ADMN 640)
ADMN 642 Managerial Aspects of Information Systems (Prerequisite: ADMN 640, 64l)
ADMN 643 Systems Analysis (Prerequisite: ADMN 640, 641 ) ADMN 644 Artificial Intelligence and Expert Systems
Marketing for Managers (12 semester hours)
The marketing track focuses on the theories and skills that managers can use to increase the demand for their organizations' products, services, or programs. This track applies to managers who must market products internally in their organizations, to those who market external products, and to marketers of free services as well as those of services with specific charges. Thus, this program applies to students in both the profit and nonprofit sectors as well as managers in a highly charged marketing culture.
ADMN 685 Strategic Market Planning ADMN 686 Marketing Management ADMN 687 Market Segmentation and
Penetration
ADMN 688 Marketing lntelligence and Research Systems (Prerequisite: ADMN 638)
·Students who have sufficient computer background, as determined by the Director Of Information and Telecommunications Studies, may take ADMN 644 in lieu ofADMN 640.
MASTER OF
GENERAL
ADMINISTRATION
6
MASTER OF
GENERAL
ADrvllNISTRATION
State and Local Government
( 12 semester hours)
The state and local government track addresses the needs of individuals who are responsible for or assist in the complex decisions facing state and local government. These decisions concern education. services (police, fire/disaster), housing, unemployment, fiscal solvency, and urban renewal and planning. Problems in these areas are intensifying; and more of the federal government's power is being relegated to the state and local level. The need is increasing, therefore. for trained planners and administrators who can use scarce resources to create a safe. high-quality environment and. at the same time. provide an attractive location tor appropriate economic development.
ADMN 646 State and Local Government Management Issues ADMN 647 Negotiating State and Local
Government Policy Issues ADMN 648 Financial Analysis ADMN 649 Urban Development
REQUIREMENTS FOR ADMISSION
A student may be admitted in one of four classifications: degree, degree-special. professional development, or special. Another classification. decision pending. allows students to be admitted at registration for one semester only while completing the formal admission process. The admission requirements for each classification are as follows.
Degree Program
•
A baccalaureate from a regionally accredited university or college.
•
An overall undergraduate grade-point average (GPA) of at least 2.:; on a 4.0 system.
•
An average of .3.0 in the undergraduate m",1989 - 1990 UMUC Stateside Graduate - Catalog,,University of Maryland University College,,,core
236530027,,"~t,7f1UnC --:)-
14;'Q -7· 14 C:WP50\DOC\CSCCSTLO.UIS
Centre for the study of Communication and Culture, st. Louis, MO
AN INFORMATION OR A COMMUNICATION AGE?
INFORMATION COMMUNICATION:: DIGITIZATION/ HERMENEUTICS
, ---c
""'fJ'.,..-r.2. "":)•.;.:n --u
d-:tl/,-'b
""~g~ ,~ Hermeneutics has become a buzzword today. Little has been
J"" d. made of the fact that the apotheosis of hermeneutics is a pheno-
~'menon that has coincided with the technological developments
which have made this the age of digitization and, concomitantly,
""the information age"" or ""the communications age."" As an academ-ic
subject, hermeneutics has emerged from a recondite philologi-cal
and philosophical background, involving deep historical study
initially entered into for the interpretation of ancient texts.
This seems a far fry from the technological world which has pro-duced
the digital computer and which manages the incalculable
""bits"" of information now floating about the globe. Hermeneutics
appears to have emerged from a different level of existence than f that oJftechnology. It would appear closer to the world of com-munic~
tion, which, unlike information, implies personal interac-tion
between living and conscious beings. On the other hand,
communication between human beings can often involve massive
technology, as in the use of computers and all they entail, from
desk-top publishing to electronic mail. The developments in com-munication
which mark our age as ""the communications age"" seem-ingly
as much as it is ""the information age"" have been imple-mented
precisely by technological inventions from the telegraph
through the telephone, radio, television, and the electronic
world of the computer and related devices. However much it
implies interaction between human beings, communications every
day is more and more conspicuously involved with technology.
Hermeneutics, on the other hand, appears to have emerged from a
different level of existence than that of technology. It is
concerned with the sense of language as language is employed by
human beings.
However, if we look at the historical development of com-munications
over the centuries, it is possible to discern a close
connection between the hermeneutics explosion of out age and the
computer and other information and communication devices.
walter J. Ong, SJ
Hermeneutics=interpretation.
Any utterance can demand interpretation: total verbal explicit-ness
is impossible,
Interpretation: Latin-based word = hermeneutics, Greek-based.
Hermeneutics came to the fore in ""higher criticism"" of the Bible,
then of other texts. Because of what a text is: a thing.
Textual stability of print made a text even more a thingJ~.
An abstract floating something unattached to a surface (Ivan
Illich on Hugh of st. Victor).
Oral utterance not a technology at all: it does not use tools and
leaves no product, etc. (from other typescript). Speech
InuV\,H/.} """" t-<;fik:vA, t., IQ t· Cl ,-. //'AI'I~ <;;:""~'
,ivn1().. 'f I J'-- I/I ~ ltr 7 "" ;, I
2
exists only when it is going out of existence: it is an
event, not a thing.
Writing is 'a technology. Even more, so is print <hermeneutics
deals ultimately with print-stabilized text. Even more, so
is the computer. More and more technology is interposed
between interlocutors. This interposition, calculated in
some sense to reduce distance, only superficially reduces
it; fundamentally, humanly, increases distance.
SOFTWARE AND HARDWARE OF HERMENEUTICS
DIGITIZATION AND HERMENEUTICS
THE ABSTRACTION OF THE TEXT
Walter J. Ong, SJ
Through much of the twentieth century, the greater and
greater attention to texts, running from the New criticism in the
English-language world through structuralism and deconstruction,
has obscured awareness that texts are never independent as simply
texts but in every instance are always part of something else
that is not textual. They connect with the lifeworld of their
writers and readers and with the technologies indispensable to
their manufacture (clay tablets or papyrus or parchment or paper
or wood or other writing surfaces, huge printing machines or, ~ ~ ~a now, computers, and much else)/ Texts connect with their write~-
~s' extratextual history and with their readers and the lifeworlds
in which readers incarnate a given text, and so on. There are no
texts just ""out there"" isolated from all else. Indeed, more than
most phenomena, texts connect with other things and, indeed,
eventually with everything. It is impossible to examine an
isolated texts. There are no such things./ All texts have a
h i.s t.o ry ; tt..vrf - '~Aa.""""7 __ ¥'_ ""1.14'./t1.<'!2 -h /.1.v /d a-t,."". , ?)J. 7~)·f7,tjl)(,t'/ ./
The present-day concept of a text can be commonly a concept
of something rather free-floatin~~an assemblage of visibly coded
words attached to no particular surface. Ivan Illich's recent Du
lisible au visible: sur l'art de lire du Hugues de Saint-victoire
has shown some of the ways in which the present-day concept of
the free-floating text, attached to no material object, put in
its appearance in a late manuscript culture. The text of the
Declaration of Independence, for example, is seldom thought of as
a series of hand-inscriptions or printed impressions on a given
surface--white paper, plain or rippled finish, with letters of a
given size and color. Rather, it is thought of as a free-float-ing
assemblage of letters and punctuation which lodges no place
in particular but is re~~h~pe appropriated imaginatively or in
external reality by anyone'-ttrtn~nk about it or incarnate it, to
put it onto a given surface.
Earlier, as in the lifetime of Hugh of st. victor (1096-
1141), the text was commonly thought of and felt differently. It
was this or that piece of parchment or other writing surface,
with all the particularity that the individual scribe had im-
3
parted to the marks on the parchment's surface. The text was
typically felt to be a particularized object, which could not be
fully abstracted because no two copies were quite the same in the
distribution and positioning of the letters and other signs and,
in the case particularly of longer texts, did/not dontain quite
exactly the same words in the same order--that is,jdid not en-tirely
match word-for-word, since perfect cO{Ying of a lengthy
text--say, of several hundreds of pages--was impossible in a
manuscript culture. The concept of a free-f oating text had put
in its appearance well ahead of the development of letterpress
printing, which with the proofreading it made possible, made the
concept of a free-floating text more accessible to the imagina-tion,
since any number of copies of a carefully proofread printed
book could present to any reader exactly the same configuration
of letters and other associated visible marks.
REFERENCES
Illich, Ivan. Du lisible au visible: sur l'art de lire de Hugues
de saint-victor. Paris: Editions du Cerf, 1991.
/?;vvU ML; ""- UM'W1'
c",Item 0002,,Saint Louis University Libraries Special Collections,,,core
19159999,1990-06-17T07:00:00,"In this paper we describe a hybrid architecture that integrates artificial neural networks and knowledge-based expert systems to generate solutions for the real time scheduling of flexible manufacturing systems. The artificial neural networks perform pattern recognition and, due to their inherent characteristics, support the implementation of automated knowledge acquisition and refinement schemes through a feedback mechanism. The artificial neural network structures enable the system to recognize patterns in the tasks to be solved in order to select the best scheduling rule according to different demands. The knowledge-based expert systems are the higher order elements which drive the inference strategy and interpret the constraints and restrictions imposed by the upper levels of the flexible manufacturing system control hierarchy. The level of self-organization achieved provides a system with a higher probability of success than traditional approaches",Synergy of Artificial Neural Networks and Knowledge-Based Expert Systems for Intelligent FMS Scheduling,https://core.ac.uk/download/19159999.pdf,DigitalCommons@CalPoly,,,core
101517205,1994,"ARCHON is an ongoing ESPRIT II project (P-2256) which is approximately half way through its five year duration. It is concerned with defining and applying techniques from the area of Distributed Artificial Intelligence to the development of real-size industrial applications. Such techniques enable multiple problem solvers (e.g. expert systems, databases and conventional numerical software systems) to communicate and cooperate with each other to improve both their individual problem solving behavior and the behavior of the community as a whole. This paper outlines the niche of ARCHON in the Distributed AI world and provides an overview of the philosophy and architecture of our approach the essence of which is to be both general (applicable to the domain of industrial process control) and powerful enough to handle real-world problems. After more than a decade of successful exploitation there are now over 100,00",Nationwide summary of U.S,,,,,core
78885573,,"Prior to the development of a production standard control system for ML Aviation's plan-symmetric remotely piloted helicopter system, SPRITE, optimum solutions to technical requirements had yet to be found for some aspects of the work. This thesis describes an industrial project where solutions to real problems have been provided within strict timescale constraints. Use has been made of published material wherever appropriate, new solutions have been contributed where none existed previously. A lack of clearly defined user requirements from potential Remotely Piloted Air Vehicle (RPAV) system users is identified, A simulation package is defined to enable the RPAV designer to progress with air vehicle and control system design, development and evaluation studies and to assist the user to investigate his applications. The theoretical basis of this simulation package is developed including Co-axial Contra-rotating Twin Rotor (CCTR), six degrees of freedom motion, fuselage aerodynamics and sensor and control system models. A compatible system of equations is derived for modelling a miniature plan-symmetric helicopter. Rigorous searches revealed a lack of CCTR models, based on closed form expressions to obviate integration along the rotor blade, for stabilisation and navigation studies through simulation. An economic CCTR simulation model is developed and validated by comparison with published work and practical tests. Confusion in published work between attitude and Euler angles is clarified. The implementation of package is discussed. dynamic adjustment of assessment. the theory into a high integrity software Use is made of a novel technique basing the integration time step size on error Simulation output for control system stability verification, cross coupling of motion between control channels and air vehicle response to demands and horizontal wind gusts studies are presented. Contra-Rotating Twin Rotor Flight Control System Remotely Piloted Plan-Symmetric Helicopter Simulation Six Degrees of Freedom Motion ( i i",The dynamics and control of a remotely piloted plan-symmetric helicopter,https://core.ac.uk/download/78885573.pdf,,,,core
291401898,,"The book covers a variety of topics in Information and Communications Technology (ICT) and their impact on innovation and business. The authors discuss various innovations, business and industrial motivations, and impact on humans and the interplay between those factors in terms of finance, demand, and competition. Topics discussed include the convergence of Machine to Machine (M2M), Internet of Things (IoT), Social, and Big Data. They also discuss AI and its integration into technologies from machine learning, predictive analytics, security software, to intelligent agents, and many more. Contributions come from academics and professionals around the world.

Covers the most recent practices in ICT related topics pertaining to technological growth, innovation, and business; Presents a survey on the most recent technological areas revolutionizing how humans communicate and interact; Features four sections: IoT, Wireless Ad Hoc & Sensor Networks, Fog Computing, and Big Data Analytics.(Chapter) The recent advancements in robotic systems set new challenges for robotic simulation software, particularly for planning. It requires the realistic behavior of the robots and the objects in the simulation environment by incorporating their dynamics. Furthermore, it requires the capability of reasoning about the action effects. To cope with these challenges, this study proposes an open-source simulation tool for knowledge-oriented physics-based motion planning by extending The Kautham Project, a C++ based open-source simulation tool for motion planning. The proposed simulation tool provides a flexible way to incorporate the physics, knowledge and reasoning in planning process. Moreover, it provides ROS-based interface to handle the manipulation actions (such as push/pull) and an easy way to communicate with the real robotsPeer Reviewe",A tool for knowledge-oriented physics-based motion planning and simulation,,,,,core
6760402,,"Like many others, Textile-apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems. However, forecasters have to deal with some singular constraints of the textile-apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust. After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.Sales forecasts Textile-apparel supply chain Clothing industry Sourcing simulation",Sales forecasts in clothing industry: The key success factor of the supply chain management,,,,,core
6333305,,"Recent developments in computing and technology, along with the availability of large amounts of raw data, have contributed to the creation of many effective techniques and algorithms in the fields of pattern recognition and machine learning. The main objectives for developing these algorithms include identifying patterns within the available data or making predictions, or both. Great success has been achieved with many classification techniques in real-life applications. With regard to binary data classification in particular, analysis of data containing rare events or disproportionate class distributions poses a great challenge to industry and to the machine learning community. This study examines rare events (REs) with binary dependent variables containing many more non-events (zeros) than events (ones). These variables are difficult to predict and to explain as has been evidenced in the literature. This research combines rare events corrections to Logistic Regression (LR) with truncated Newton methods and applies these techniques to Kernel Logistic Regression (KLR). The resulting model, Rare Event Weighted Kernel Logistic Regression (RE-WKLR), is a combination of weighting, regularization, approximate numerical methods, kernelization, bias correction, and efficient implementation, all of which are critical to enabling RE-WKLR to be an effective and powerful method for predicting rare events. Comparing RE-WKLR to SVM and TR-KLR, using non-linearly separable, small and large binary rare event datasets, we find that RE-WKLR is as fast as TR-KLR and much faster than SVM. In addition, according to the statistical significance test, RE-WKLR is more accurate than both SVM and TR-KLR.Classification Endogenous sampling Logistic regression Kernel methods Truncated Newton",Robust weighted kernel logistic regression in imbalanced and rare events data,,,,,core
22726287,1994,"This paper deals with a real-time implementation of a neural net controller and its application to an induction machine. The controller used within this framework is the Modelbased Predictive Control structure (MPC) with a neural network model of the process dynamics.  The hardware platform, a multi-transputer system, is described as are the implemented algorithms. Some results show the benefits of the neural network controller compared to the field oriented control structure which is the most common structure used to control induction machines.  1. Introduction  Daimler-Benz is engaged in a research project for  the subsidiary company AEG Daimler-Benz Industry.   The project concerns neural networks for nonlinear  control of industrial processes such as steel  manufacturing [Sba93]. In this paper a real-time implementation  of a Model Predictive Controller  (MPC) using Radial Basis Function neural networks  (RBF) will be described.  The conventional method for controlling an induction..",Real-Time Application of Neural Model Predictive Control for an Induction Servo Drive,,,10.1109/cca.1994.381407,,core
161339573,,"With significantly increasing concerns about greenhouse effects and sustainable economy, the marine industry presents great potential for reducing its environmental impact. Recent developments in power electronics and hybridisation technologies create new opportunities for innovative marine power plants which utilize both traditional diesel generators and energy storage like batteries and/or supercapacitors as the power sources. However, power management of such complex systems in order to achieve the best efficiency becomes one of the major challenges. 



Acknowledging this importance, this research aims to develop an optimal control strategy (OCS) for hybrid marine power plants. First, architecture of the researched marine power plant is briefly discussed and a simple plant model is presented. The generator can be used to charge the batteries when the ship works with low power demands. Conversely, this battery energy can be used as an additional power source to drive the propulsion or assist the generators when necessary. In addition, energy losses through braking can be recuperated and stored in the battery for later use. Second, the OCS is developed based on equivalent fuel consumption minimisation (EFCM) approach to manage efficiently the power flow between the power sources. This helps the generators to work at the optimal operating conditions, conserving fuel and lowering emissions. In principle, the EFCM is based on the simple concept that discharging the battery at present is equivalent to a fuel burn in the future and vice-versa and, is suitable for real-time implementation. However, instantaneously regulating the power sources’ demands could affect the system stability as well as the lifetime of the components. To overcome this drawback and to achieve smooth energy management, the OCS is designed with a number of penalty factors by considering carefully the system states, such as generators’ fuel consumption and dynamics (stop/start and cranking behaviour), battery state of charge and power demands. Moreover, adaptive energy conversion factors are designed using artificial intelligence and integrated in the OCS design to improve the management performance. The system therefore is capable of operating in the highest fuel economy zone and without sacrificing the overall performance. Furthermore, a real-time simulation platform has been developed for the future investigation of the control logic. The effectiveness of the proposed OCS is then verified through numerical simulations with a number of test cases",Optimal control and real-time simulation of hybrid marine power plants,https://core.ac.uk/download/161339573.pdf,,,,core
467107248,,"The need for in-process measurement has surpassed the processing capability of traditional computer hardware. As Industry 4.0 changes the way modern manufacturing occurs, researchers and industry are turning to hardware acceleration to increase the performance of their signal processing to allow real-time process and quality control.



This thesis reviewed Industry  4.0  and  the challenges  that have  arisen from transitioning towards  a connected smart factory. It has investigated the different hardware acceleration techniques available and  the bespoke  nature  of  software that industry  and researchers are being forced towards in  the pursuit of greater performance. In addition, the application of hardware acceleration within surface and  dimensional instrument  signal  processing was researched and  to  what  extent it  is  benefitting researchers.  The collection  of algorithms  that  the  field are  using were examined finding  significant commonality across  multiple  instrument  types,  with  work  being  repeated  many  times  over  by different people.



The  first  use  of  PDDL  to  optimise heterogenous signal  processing within surface  and  dimensional measurements is  proposed. Optical  Signal  Processing  Workspace  (OSPW) is presented  as  a self-optimising software  package using GPGPU acceleration using  Compute  Unified  Device  Architecture (CUDA)for  Nvidia  GPUs. OSPW was designed  from  scratch  to  be  easy  to  use  with very little-to-no programming  experience  needed,  unlike  other  popular  systems  such  LabVIEW  and  MATLAB.  It provides  an intuitive  and easy to navigate User Interface (UI) that allows a user to select the signal processing algorithms required, display system outputs, control actuation devices, and modify capture device properties.



OSPW automatically profiles the execution time of the signal processing algorithms selected by the user and creates and  executes a  fully  optimised  version  using  an AI planning  language,  Planning Description  Domain  Language   (PDDL), by selecting  the  optimum  architecture  for  each  signal processing function. 



OSPW was then evaluated against two  case  studies,  Dispersed  Reference  Interferometry  (DRI)  and Line-Scanning  Dispersed  Interferometry  (LSDI).  These  case  studies demonstrated that  OSPW can achieve  at  least21x greater  performance than  an  identical  MATLAB  implementation with a  further 13% improvement found using the PDDL’s heterogenous solution.



This novel approach to providing a configurable signal processing library that is self-optimising using AI planning will provide considerable performance gains to researchers and industrial engineers. With some  additional  development work it  will  save both academia and industry time  and money which can be reinvested to further advance surface and dimensional instrumentation research",Applying Artificial Intelligence Planning to Optimise Heterogeneous Signal Processing for Surface and Dimensional Measurement Systems,https://core.ac.uk/download/467107248.pdf,,,,core
58829204,1994-07,"Many techniques have been developed for abstracting, or ""learning,"" rules and relationships from diverse data sets, in the hope that machines can help in the often tedious and error-prone process of acquiring knowledge from empirical data. While these techniques are plausible, theoretically well-founded, and perform well on more or less artificial test data sets, they stand or fall on their ability to make sense of real-world data. This paper describes a project that is applying a range of learning strategies to problems in primary industry, in particular agriculture and horticulture. We briefly survey some of the more readily applicable techniques that are emerging from the machine learning research community, describe a software workbench that allows users to experiment with a variety of techniques on real-world data sets, and detail the problems encountered and solutions developed in a case study of dairy herd management in which culling rules were inferred from a medium-sized database of herd information",Applying machine learning to agricultural data,,,,"[{'title': None, 'identifiers': ['issn:1170-487X', '1170-487x']}]",core
29195186,1994-07-01T00:00:00,"Many techniques have been developed for abstracting, or ""learning,"" rules and relationships from diverse data sets, in the hope that machines can help in the often tedious and error-prone process of acquiring knowledge from empirical data. While these techniques are plausible, theoretically well-founded, and perform well on more or less artificial test data sets, they stand or fall on their ability to make sense of real-world data. This paper describes a project that is applying a range of learning strategies to problems in primary industry, in particular agriculture and horticulture. We briefly survey some of the more readily applicable techniques that are emerging from the machine learning research community, describe a software workbench that allows users to experiment with a variety of techniques on real-world data sets, and detail the problems encountered and solutions developed in a case study of dairy herd management in which culling rules were inferred from a medium-sized database of herd information",Applying machine learning to agricultural data,https://core.ac.uk/download/29195186.pdf,,,"[{'title': None, 'identifiers': ['issn:1170-487X', '1170-487x']}]",core
33575090,,"Game-based learning is a growing field that provides education with a new perspective of teaching through games. Game based learning is still considered an emerging field due to problems that have been identified in its real applications in official education in classes. The research conducted shows a very attractive market ahead for game based learning around the world. However the businesses success in this domain lie in providing value proposition that addresses the real barriers faced today.

Prime Ace is a company that has been conceptualized to be starting up via an equal partnership among the four students from the University of Nottingham, Computer Science and Entrepreneurship course who are doing this dissertation. Prime Ace is a digital educational game solutions provider that is ambitious to be a leader of its focused market in providing education to children through games not only at home but also in class. Its products and services will span to the teachers and parents to help them through their mission of educating children in its innovative way. Based on the theory studied and practical opportunity, Prime Ace aims to target the Middle Eastern game based learning market. It has taken a focused marketing approach in targeting the attractive demographic group of kindergarten levels up to primary grade level two (prek-2). The company has drawn a detailed 5-year operational and financial plan. Based upon them, the company is seeking a seed investment of $150,000 (USD) at this stage in order to move on to its next stage of moving to Dubai.

Prime Ace has so far conducted its first project and digitized and implemented a serious board game which targets its own market. The game aims to teach students how to manipulate DNA in a completely safe and low-cost environment rather than necessarily performing these tasks in actual labs to learn. Prime Ace have had a fair portion of experience in working with a real client, actually developing game and understanding specific learning objectives. The team have gained valuable experience on how to work together with a real client to solve project related problems. The problems faced include software development problems such as unfamiliarity with the development software as well as project management problems such as unclear and changing requirements. The team has learnt to work on solving these problems by following industry practices on software development methodology, using appropriate tools and processes, linking theoretical research of Human-Computer Interaction and artificial intelligence. After multi-level and stage-by-stage testing which involves actual users, the product has shown potential its effectiveness for learning. The project’s dissertation requirements were exceeded, producing more than a demo. The project’s client expectations were also exceeded and he was able to use the product for his purpose of pitching for investments. Last but not least, the project’s team goals were met as it has proven them as an effective team that is hopefully worthy to invest in",Planning start-up: digital educational game solutions provider,https://core.ac.uk/download/33575090.pdf,,,,core
78884185,,"This thesis introduces and develops a novel real-time predictive maintenance system to estimate the machine system parameters using the motion current signature. Recently, motion current signature analysis has been addressed as an alternative to the use of sensors for monitoring internal faults of a motor. A maintenance system based upon the analysis of motion current signature avoids the need for the implementation and maintenance of expensive motion sensing technology. By developing nonlinear dynamical analysis for motion current signature, the research described in this thesis implements a novel real-time predictive maintenance system for current and future manufacturing machine systems. A crucial concept underpinning this project is that the motion current signature contains infor­mation relating to the machine system parameters and that this information can be extracted using nonlinear mapping techniques, such as neural networks. Towards this end, a proof of con­cept procedure is performed, which substantiates this concept. A simulation model, TuneLearn, is developed to simulate the large amount of training data required by the neural network ap­proach. Statistical validation and verification of the model is performed to ascertain confidence in the simulated motion current signature. Validation experiment concludes that, although, the simulation model generates a good macro-dynamical mapping of the motion current signature, it fails to accurately map the micro-dynamical structure due to the lack of knowledge regarding performance of higher order and nonlinear factors, such as backlash and compliance. Failure of the simulation model to determine the micro-dynamical structure suggests the pres­ence of nonlinearity in the motion current signature. This motivated us to perform surrogate data testing for nonlinearity in the motion current signature. Results confirm the presence of nonlinearity in the motion current signature, thereby, motivating the use of nonlinear tech­niques for further analysis. Outcomes of the experiment show that nonlinear noise reduction combined with the linear reverse algorithm offers precise machine system parameter estimation using the motion current signature for the implementation of the real-time predictive maintenance system. Finally, a linear reverse algorithm, BJEST, is developed and applied to the motion current signature to estimate the machine system parameters",An advanced real-time predictive maintenance framework for large scale machine systems,https://core.ac.uk/download/78884185.pdf,,,,core
201687981,,"Abstract  Introduction: The interest in Expert systems has increased in the medical area. Some of them are employed even for diagnosis. With the variability of transcatheter prostheses, the most appropriate choice can be complex. This scenario reveals an enabling environment for the use of an Expert system. The goal of the study was to develop an Expert system based on artificial intelligence for supporting the transcatheter aortic prosthesis selection.  Methods: The system was developed on Expert SINTA. The rules were created according to anatomical parameters indicated by the manufacturing company. Annular aortic diameter, aortic area, aortic perimeter, ascending aorta diameter and Valsalva sinus diameter were considered. After performing system accuracy tests, it was applied in a retrospective cohort of 22 patients with submitted to the CoreValve prosthesis implantation. Then, the system indications were compared to the real heart team decisions.  Results: For 10 (45.4%) of the 22 patients there was no concordance between the Expert system and the heart team. In all cases with discordance, the software was right in the indication. Then, the patients were stratified in two groups (same indication vs. divergent indication). The baseline characteristics did not show any significant difference. Mortality, stroke, acute myocardial infarction, atrial fibrillation, atrioventricular block, aortic regurgitation and prosthesis leak did not present differences. Therefore, the maximum aortic gradient in the post-procedure period was higher in the Divergent Indication group (23.9 mmHg vs. 11.9 mmHg, P=0.03), and the mean aortic gradient showed a similar trend.  Conclusion: The utilization of the Expert system was accurate, showing good potential in the support of medical decision. Patients with divergent indication presented high post-procedure aortic gradients and, even without clinical repercussion, these parameters, when elevated, can lead to early prosthesis dysfunction and the necessity of reoperation",Development and Application of a System Based on Artificial Intelligence for Transcatheter Aortic Prosthesis Selection,,'Sociedade Brasileira de Cirurgia Cardiovascular',10.21470/1678-9741-2018-0072,"[{'title': 'Brazilian Journal of Cardiovascular Surgery', 'identifiers': ['issn:1678-9741', '1678-9741']}]",core
291395061,,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.Peer Reviewe",Incorporating negative information to process discovery of complex systems,,,,,core
286293899,,"PDFTech ReportDOT/FRA/ORD-13/09Wireless communication systemsPositive train controlRadio equipmentArtificial intelligenceDecision makingData communicationsTrain operationsRailroad safetyUnited StatesUnited States. Federal Railroad Administration. Office of Research and DevelopmentAmanna, AshwinVirginia Polytechnic Institute and State UniversityNTL-RAIL TRANSPORTATION-Rail SafetyNTL-SAFETY AND SECURITY-Rail SafetyNTL-OPERATIONS AND TRAFFIC CONTROLS-Traffic Control DevicesUS Transportation CollectionRobust and interoperable wireless communications are vital to Positive Train Control (PTC). The railway industry has started adopting software-defined radios (SDRs) for packet-data transmission. SDR systems realize previously fixed components as reconfigurable software. This project developed a railway cognitive radio (Rail-CR) which implements Artificial Intelligence (AI) decisionmaking in concert with an SDR to adapt to changing wireless conditions and learn from past experience. Objectives of the project included developing a concept of operations for wireless link adaptation based on use-case scenarios for packet radio systems, designing and implementing a decisionmaking architecture on an SDR, designing strategies for radio environment observations, defining operational objectives and performance metrics, and designing and exercising a test plan to demonstrate performance under varying conditions. The decisionmaking architecture of the Rail-CR begins with observations of the wireless environment and performance metrics. The architecture enables adaptation to new situations and the capability to learn from past decisions. The Rail-CR was tested under a variety of interference conditions designed to simulate real-world experiences. Results show that a radio operating with no-cognition was unable to mitigate interference conditions causing either significantly high errors or a loss of connectivity. The Cognitive Engine (CE) successfully overcame the interference by changing configurable parameters","Railway cognitive radio to enhance safety, security, and performance of positive train control.",,,,,core
229267628,1990-01-01T08:00:00,"Flexible manufacturing system (FMS) scheduling is a complex problem in nature that leads to a high level of uncertainty due to limited feasible solutions in an extensive search space. Heuristics involving dispatching rules have been widely utilized to obtain good solutions. This strategy has been recently enhanced by FMS scheduling researchers using knowledge-based expert systems as means of resolving scheduling problems. Unfortunately, the knowledge-based expert systems (KBESs) developed are limited in real-time performance due to cracks in their encoded knowledge or lack of adequate plans to address the changing environment.
A framework is developed displaying the capabilities of automatic learning and self-improvement, providing the necessary adaptive scheme to respond to the dynamic nature of flexible manufacturing systems. This proposed framework uses a hybrid architecture that integrates artificial neural networks and knowledge-based expert systems to generate solutions for the real time scheduling of flexible manufacturing systems. In this framework, the artificial neural networks perform pattern recognition and, due to their inherent characteristics, support the implementation of automated knowledge acquisition and refinement strategies through a feedback mechanism. They enable the system to recognize patterns in the tasks to be solved in order to select the best scheduling rule according to different criteria. The knowledge-based expert systems, on the other hand, drive the inference strategy and interpret the constraints and restrictions imposed by the upper levels of the control hierarchy of the flexible manufacturing system. The level of self-organization thus achieved provides a system architecture with a higher probability of success than traditional approaches --Abstract, page iii",A hybrid artificial neural networks and knowledge-based expert systems approach to flexible manufacturing system scheduling,,Scholars\u27 Mine,,,core
132199214,,"The interest with which academia, industry and societies anticipate a full scale launch of autonomous and connected vehicles, which is one of the most critical components underpinning the transformation of a modern city to a truly ‘smart’ one, is higher than ever before. This is because these vehicles have, in theory at least, the potential to completely transform urban development as known today, with a revolution in ground transport, regulations permitting, that could dramatically change the landscape of cities and have an enormous economic, social, spatial, and mobility impact. Artificial intelligence with its deep learning functions that model high-level data concepts through the use of architectures of multiple non-linear transformations is ultimately employed as a tool which empowers the car to make better decisions than a human driver ever could; but at the same time needs to be a user-centred technology that ‘understands’ and ‘satisfies’ the human user and the markets. Although recent studies showed that a priori acceptability of fully automated cars could be likely for many drivers today, the universal embracement of such a monumental mobility paradigm transition is still a complex proposition. This is because, despite a number of potential beneficial outcomes that make driverless vehicles an inescapable future reality, the implementation of vehicle automation, will not be straightforward, predictable or unproblematic; there is a wide spectrum of social dilemmas and complicated human factors issues that may arise from such an ‘untested’ and ‘powerful’ innovation.  The present work develops an understanding of its key opportunities and challenges via the analysis of a series of semi-structured in-depth interviews with academic experts in transport and vehicle automation studies. The interviews were designed to test some of the myths referring to autonomous and connected vehicles with an emphasis on exploring some of the potentially darker or ambiguous sides of this smart mobility paradigm. Therefore the present study’s main output is the formation of a ‘road map’ or ‘theory-driven taxonomy’ of the possible key impacts (positive and negative) of car automation","Examining the Myths Around Autonomous Cars: 

What Does the Future Hold?",,,,,core
357572668,1990-01-01T00:00:00,"A framework for intelligent sensors in unmanned machining is proposed. In the absence of human operators, the process monitoring function has to be performed with sensors and associated decision-making systems which are able to interpret incoming sensor information and decide on the appropriate control action. In this paper, neural networks are used to integrate information from multiple sensors (acoustic emission and force) in order to recognize the occurrence of tool wear in a turning operation. The superior learning and noise suppression abilities of these networks enable high success rates for recognizing tool wear under a range of machining conditions. The parallel computation ability of these networks offers the potential for constructing intelligent sensor systems that are able to learn, perform sensor fusion, recognize process abnormalities, and initiate control actions in real-time manufacturing environments. Introduction Successful automation of machining operations relies, to a great extent, on the ability to recognize process abnormalities and initiate corrective action. In the absence of human operators, this function has to be performed with sensors and associated decision-making systems which are able to interpret incoming sensor information and decide on the appropriate control action. According to  Intelligent sensor systems are expected to replace the knowledge, experience, and sensory and pattern recognition abilities of human operators. Successful implementations of these tasks depend on two factors: first, the quality of information generated by the monitoring sensors and second, the techniques used to process this information in order to make decisions. The first factor relates to the type and number of sensors used, and the signal/noise ratio of the information generated by these sensors. The second factor concerns the learning and decision-making procedures used to analyze this information in the context of the process state. Sensing strategies for unmanned machining should aim at integrating both these factors, thereby allowing for a sensor system design which possesses the ability to successfully mimic the sensory abilities and pattern recognition skills of human operators. Metal cutting operations constitute a large percentage of current manufacturing activity (Barash, 1980). As a result, there is a strong thrust in research directed at automating the process. An important component of this research is aimed at developing reliable sensor technology for detecting factors such as chip form, tool condition, workpiece roughness, machine vibrations and bearing failure. Tool wear monitoring, which is the focus of the current work, is an area of active research, primarily because the condition of the tool exerts a strong influence on the surface finish and dimensional integrity of the workpiece and vibration levels of the machine tool. The development of reliable tool wear monitoring systems is also expected to reduce tool material costs and machine down times associated with tool change operations. Additionally, the availability of such systems is vital for implementing optimal strategies (such as adaptive control with optimization) in unmanned machining operations. Several sensing strategies for tool wear detection have been proposed and evaluated in a number of review articles  In this paper, we present a technique for intelligent tool condition monitoring which employs information from multiple sensors. This information is integrated via a neural network, a parallel computing architecture which can learn to recognize patterns of sensor information and associate them with decisions on the tool wear state. Initial efforts by  Intelligent Sensor Systems for Tool Wear Monitoring A human operator can detect whether a tool is fresh or worn by observing the machining operation and associating patterns of sensory cues with a decision on the tool state. The sensory information used to make this decision is usually of various types: visual (observation of chip color, presence of smoke, deteriorating surface finish of the workpiece), audio (sound generated by rubbing action of tool flank on workpiece), and olfactory (smell of smoke generated due to machining with a worn tool). Associating the sensory cues with tool wear depends to a great extent on the knowledge and experience of the operator. In many cases, information from a single sensor, say audio, may not be sufficient and visual information may also be necessary. The fact that human operators are very successful at the process monitoring task suggests that one possible method for designing computer-based monitoring systems is to model their learning and decision-making abilities after those of a-htiman operator. The philosophy pursued in this paper is that an &quot;intelligent sensor system&quot; should be able to emulate as closely as possible, the learning) pattern recognition and sensor fusion abilities of human operators. Human pattern recognition is a highly developed and poorly understood characteristic, and the task of simulating it on a computer is a formidable one. The factors involved in human pattern recognition and how they may be mapped in order to develop computer-based pattern recognition capability is shown in  Use of Multiple Sensors In the current work, it was decided to use AE and cutting force information in order to develop an intelligent tool condition monitoring system. The primary and secondary shear zones are important sources of AE when cutting with a fresh tool.  The root mean square (RMS) level of the AE signal (V RMS ) measures the total power level of the signal and has been found to be sensitive to the degree of flank wear in a turning operation. Experiments conducted by Lan (1983) for machining of SAE 4340 steel with carbide tools indicate that V RMS increases with machining time due to increased flank wear. However, in cases where the crater wear is significant, V RMS tends to decrease or remains constant. Since the presence of flank wear is expected to increase V RMS , Lan concluded that the effect of crater wear is to cause a drop in V RMS . The fact that V RMS remains constant with increased tool wear due to opposing effects of flank and crater wear makes it difficult to 220/Vol. 112, AUGUST 1990   Transactions of the ASME  design an AE-based tool wear monitoring system which uses only information on the RMS level of the signal. Emel and Kannatey-Asibu (1988) present experimental data which shows that the power spectrum is sensitive to tool wear and process conditions. Results for machining of AISI 1060 with carbide inserts  The performance of an AE-based tool wear monitoring system can be enhanced by complementing the AE information with information from other sensors mounted on the machine tool (for example, force or power sensors). The magnitude of the cutting force is sensitive to the occurrence of tool wear in a turning operation (Andrews and Tlusty, 1983). According to  The AE and cutting force information relate to different effects of tool wear. Acoustic emission is sensitive to the microscopic activities (and the resulting stress waves) related to plastic deformation and friction in the cutting zone. The cutting force spectrum is sensitive to the vibrations induced in the tool and workpiece due to the effects of flank wear. The advantage of using AE and cutting force sensors is that they provide information relating to microscopic (stress waves) and macroscopic (vibrations) effects of tool wear. This helps provide better signal features to the pattern classifier, allowing a greater reliability in making decisions on the state of tool wear. Background on Neural Networks The human brain consists of a large number of interconnected neurons, each possessing very simple computational abilities. However, the interactions (through a dense system of connections of &quot;synapses&quot;) between the neurons allows for parallel processing of information, which greatly enhances the speed of computation and causes a large amount of knowledge to be brought to bear in processing this information  In this paper, a special class of neural networks called feedforward networks is used for tool wear monitoring in a machining operation. A software implementation of this network on a serial computer is used as the learning and decisionmaking component. The structure of this type of a network is shown i","Sensor integration using neural networks for intelligent tool condition monitoring, Trans.",https://core.ac.uk/download/357572668.pdf,,,,core
1498853,1991-01-01T00:00:00,"ARCHON is an ongoing ESPRIT II project (P-2256) which is approximately half way through its five year duration. It is concerned with defining and applying techniques from the area of Distributed Artificial Intelligence to the development of real-size industrial applications. Such techniques enable multiple problem solvers (e.g. expert systems, databases and conventional numerical software systems) to communicate and cooperate with each other to improve both their individual problem solving behavior and the behavior of the community as a whole. This paper outlines the niche of ARCHON in the Distributed AI world and provides an overview of the philosophy and architecture of our approach the essence of which is to be both general (applicable to the domain of industrial process control) and powerful enough to handle real-world problems",Cooperation in Industrial Systems,,,,,core
464716303,,,Implementation of efficient real-time industrial wireless interference identification algorithms with fuzzified neural networks,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/EUSIPCO.2016.7760546,,core
478092818,,"Over the past few years we have seen an increasing number of legal proceedings related to inappropriately implemented technology. At the same time career paths have diverged from the foundation of statistics out to Data Scientist, Machine Learning and AI. All of these new branches being fundamentally branches of statistics and mathematics. This has meant that formal training has struggled to keep up with what is required in the plethora of new roles. Mathematics as a taught subject is still based in decades old teaching specifications and has not been updated centrally as a curriculum to include new technologies, coding or ethics. This subject area is firmly split between ICT and Mathematics in secondary school, continuing on to be split between Computer Science and Mathematics at University. As we move forwards with technology we see these once seperate fields becoming increasingly intertwined. We propose that existing provision for concepts such as ethics and societal responsibility in analysis currently exist but have not been incorporated into the mainstream curriculum of School or University. This is partially due to the split between fields in an educational setting but also the speed with which education is able to keep up with Industry and its requirements. Principles and frameworks of socially responsible modelling beginning at school level means that ethics and real-life modelling are introduced much earlier than normal. Integrating these concepts with philosophical principles of society and politics ensures a suitable background for future modellers and users of technology to draw on. Modelling is currently undertaken in technical sciences at University but the Subject Benchmark Statements are not current (Subject Benchmark Statements describe the nature of study and the academic standards expected of graduates in specific subject areas. They show what graduates might reasonably be expected to know, do and understand at the end of their studies). Even in 2019 the UK did not yet have Benchmark Statements that discuss the learning to be done in Higher Education around AI and advanced Machine Learning. Where there is provision for AI and Data Science within degree courses ethics is not generally highlighted as a key concept. As such there can be a lack of focus on the teaching of modelling or ethics as specific skills. The skills required to use a basic statistical model, for example would not be sufficient to start from scratch and build an ethical model reflecting real-world scenarios with which to inform policy or organisational decision making. This is a skill in itself and includes such aspects as awareness of data quality, ethics, user implementation problems, context and an understanding of the environment that the model is being created in. As the field of analytics has progressed so quickly in the last decade modules such as those covering AI have simply been bolted on to Maths or Computing degrees rather than being fully detailed as key areas of study or driving new, more integrated courses of study. This paper posits that gaps at primary, secondary and tertiary educational levels need to be addressed. Implementing and integrating key concepts from school level is essential so that areas such as assumptions, caveats, quality assurance and answering the right questions with constructive challenge become a cultural fixture. This not only helps developers of technology but users of rapidly developing technology also. In addition, leadership and soft skills as part of this education will ensure that a cultural shift can take place and promote continuous improvement in analysis within organisations. The addition of key concepts throughout the educational system and the updating of potentially outdated curriculums is key to ensuring a functional society. A society where every citizen is a user of tech and those that develop it can be ethical and socially responsible in its development",Towards Pedagogy supporting Ethics in Analysis,,,,,core
23920862,1992,"In this paper, a peg-in-hole insertion task is used as an example to illustrate the utility of direct associative reinforcement learning methods for learning control under real-world conditions of uncertainty and noise. An associative reinforcement learning system has to learn appropriate actions in various situations through search guided by evaluative performance feedback. We used such a learning system, implemented as a connectionist network, to learn active compliant control for peg-in-hole insertion. Our results indicate that direct reinforcement learning can be used to learn a reactive control strategy that works well even in the presence of a high degree of noise and uncertainty.  1 Introduction  The peg-in-hole insertion task has been widely used by roboticists for testing various approaches to robot control. Peg-in-hole insertion is also studied as a canonical robot assembly operation by researchers in industrial robotics. Both two-dimensional [15] and three-dimensional [8] pe..",Learning Reactive Admittance Control,,IEEE,,,core
24279249,1992,"Seven di#erent Radial Basis Functions have been applied in a Feedforward Neural Network and tested on five di#erent real or simulated multivariate modelling problems. A short theory of Radial Basis Functions is presented as well as the particular implementation of the Radial Basis Function Network (RBFN). The real world data modelling problems are; identifying the dynamic actuator characteristics of a hydraulic industrial robot, modelling carbon consumption in a metallurgic industrial process and estimation of the water content in fish food products based on NIRspectroscopy. In addition the RBFNs have been applied for modelling data generated from a simulated chemical reactor and to identify a 10-dimensional test function. Key-words : Artificial Neural Networks, Radial Basis Functions, Nonlinear data modelling, Applications. This paper has been published in Proceedings of Neuro-Nimes&apos;92,  Neural Networks and their Applications, EC2, France, 1992, pp.623-633.  1 A short theory of Radia..",Radial Basis Function Networks and Nonlinear Data Modelling.,,,,,core
161952878,,"Master of ScienceDepartment of Computer ScienceWilliam H. HsuWe present gestural music instruments and interfaces that aid musicians and audio engineers to express themselves efficiently. While we have mastered building a wide variety of physical instruments, the quest for virtual instruments and sound synthesis is on the rise. Virtual instruments are essentially software that enable musicians to interact with a sound module in the computer. Since the invention of MIDI (Musical Instrument Digital Interface), devices and interfaces to interact with sound modules like keyboards, drum machines, joysticks, mixing and mastering systems have been flooding the music industry.
Research in the past decade gone one step further in interacting through simple musical gestures to create, shape and arrange music in real time. Machine learning is a powerful tool that can be smartly used to teach simple gestures to the interface. The ability to teach innovative gestures and shape the way a sound module behaves unleashes the untapped creativity of an artist. Timed music and multimedia programs such as Max/MSP/Jitter along with machine learning techniques open gateways to embodied musical experiences without physical touch. This master's report presents my research, observations and how this interdisciplinary field of research could be used to study wider neuroscience problems like embodied music cognition and human-computer interactions",Gestural musical interfaces using real time machine learning,,,,,core
429951768,,"Vital to human survival and the protection of the environment, is the wastewater treatment industry. The practices employed work very effectively to protect human populations from pathogens and to safely treat water for discharge to the environment; securing future water supplies and protecting the rivers, lakes and seas. However, the drawback of this process is the high energy use, making the process expensive and a contributor to climate change. In the UK, approximately 3% of all energy is used by the water industry. The high energy costs also pose a problem for the developing world, as many nations cannot afford to treat their wastewater streams, resulting in unsafe discharge to the environment of raw untreated effluents. 



The pollutants (organic and inorganic compounds) are a potential source of value and the industry is failing to recover the majority of this value. Some is recovered via the use of anaerobic digestors for sludge treatment and biogas production, but the majority of the recoverable energy is lost. A potential solution for this problem is the Microbial Electrolysis Cell (MEC), a subtype of a bio-electrochemical system. 



The MEC is a system comprised of an anode, cathode and optionally; a membrane that separates the two electrodes. The MEC can sustain a biofilm on its anode which is electroactive and is able treat wastewater by facilitating the oxidation of the organic compounds, producing H2 – a renewable and potentially sustainable energy source when produced by this method. The system requires the addition of a small voltage as the reaction is not spontaneous. However, this technology is not ready to solve the challenges the wastewater treatment industry faces, primarily due to its high capital costs but also its low energy efficiency recovery. 



To address the capital costs, a recycled carbon fibre material (used for components in the automotive industry) was tested and shown to have electrocatalytic properties within an electrochemical cell, comparable to a commercially available graphite battery felt. The recycled materials were then used as anodes in 100 mL MECs using real wastewater, demonstrating potentially superior performance to graphite at a significantly reduced cost. This was confirmed at a larger scale (10 L) at a wastewater treatment plant, where hydrogen gas production and wastewater treatment performance were significantly superior but with a 96% reduction in the anode cost relative to the graphite felt used. A detailed cost benefit analysis using multiple TotEx scenarios confirmed the potential cost savings attributed to the use of the recycled carbon fibre anode, where an equally scaled MEC has the potential to be cost-competitive or less expensive than an activated sludge pool during a 20- or 50-year period. A placement abroad with a water technology consultancy did highlight that there are other technologies that are far more developed and are closer to commercial availability (i.e. sludge destruction via pyrolysis). The MEC offers something different and potentially, better, but larger scales are required to prove the technology. 



The use of the recycled carbon fibre as the anode now makes larger-scale deployment of MECs far more likely. The significantly reduced capital cost but lack of performance compromise, mean that academia and industry alike can seriously consider the construction and testing of larger and more ambitiously scaled MECs. The material is a lower environmental impact (relative to virgin graphite and carbon), meaning that the life cycle impact of an MEC using the recycled carbon would be more less and more likely to have a positive impact, assuming performance optimisation. This could increase knowledge around the problems associated with upscaling and therefore, dramatically increase the likelihood of the technology becoming a commercially available product for water industries",Microbial electrolysis cells for wastewater treatment using inexpensive and sustainable recycled carbon fibre anodes : innovation report,https://core.ac.uk/download/429951768.pdf,,,,core
23161088,1993,"this paper. MaTourA is currently under development by the University of Athens and Expert Systems International in the context of the ESPRIT 6708 APPLAUSE Project (Application &amp; Assessment of Parallel Programming Using Logic). MaTourA has been designed and is currently being implemented in the ElipSys language [2, 9, 4], a parallel constraint logic programming system developed at the European Computer-Industry Research Centre (ECRC). APPLAUSE is a three year project that began  in May 1992. The main aim of the project is to assess the advantages (and disadvantages) of the ElipSys language in various application areas, namely planning &amp; scheduling, decision support and multi-agent systems. ElipSys is a pure parallel logic programming system which supports, apart from parallelism, other additional features, such as constraint satisfaction over finite domains, that may be exploited to design and implement applications useful in a real-world environment. The multi-agent systems approach adopted in the case of MaTourA is a major research area of Distributed Artificial Intelligence (DAI) [3], in which agents of various types and capabilities cooperate in problem solving. MaTourA comprises a set of autonomous agents reflecting the procedures involved in a tourist advisory environment. In order to support the development of MaTourA, ElipSys was extended appropriately with a threelayered socket based communication scheme capable to satisfy the interaction requirements of the MaTourA agents. MaTourA is the evolution of another application, the single-agent PErsonalized Tourist INformation Advisor (PETINA) [12, 13] developed by the University of Athens in the ElipSys language. PETINA is a prototype with rather shallow knowledge and simplistic form of reasoning. Thus, it was strai..",Matoura: Multi-Agent Tourist Advisor,,,,,core
151393698,,"Drawbeads control the flow of material into the die cavity during pressing operations. The tribological and forming properties of aluminium necessitate specific frictional and drawbead geometry requirements that are different from those established over many years for steels. Academic research on this topic is limited, requiring industry to rely on trial and error methods to determine the coefficient of friction and drawbead geometry.



This research project focused on developing an innovative, scientific and holistic methodology to determine the optimum drawbead geometry and an appropriate coefficient of friction value to be used in forming feasibility simulations for aluminium panels. Special attention was given to the ease with which this research could be implemented in an industrial environment. Hence, extensive experiments to gather material properties such as plane strain and pure shear tests, complex material models, or optimisation models based on artificial neural networks (ANN), and non-linear friction models were avoided.



Three approaches identified in the literature for designing drawbeads, namely, experimental, analytical and numerical modelling were investigated to test the underlying assumptions, strengths and limits of each. For example, analytical models assumed symmetric material flow passing over the drawbeads, which in reality does not occur. Based on these findings a systematic, hybrid approach has been developed which uses a combination of physical drawbead tests and numerical modelling, to determine the coefficient of friction which is then used to obtain the drawbead restraining force. Using a novel criterion, different drawbead geometry conditions have been ranked to aid selection of an optimised drawbead geometry.



The optimised drawbead geometry obtained from the hybrid approach was validated by stamping of rectangular pans. The rectangular pan, when stamped using the optimised geometry obtained from the hybrid approach, did not show defects such as severe thinning and wrinkles. The numerical stamping model with geometric drawbead predicted the punch force with a 4.5% error, thinning with a 5% error and draw-in with an 8% error.



An innovative hybrid approach has been proposed which is capable of accurately predicting the coefficient of friction, the drawbead restraining force and the drawbead geometry. The same coefficient of friction and the drawbead geometry when used in the forming simulation accurately predicted the punch force, thinning and draw-in. As a direct application of innovation, Jaguar Land Rover can use the novel criteria for selecting the drawbead geometry to use effectively the drawbead geometry generation feature in the commercial sheet metal forming software package during forming feasibility simulations. The hybrid approach can potentially save 34% of the die tryout time and provide average cost savings of £34,400 per die set per tryout attempt",Numerical modelling of drawbeads for forming of aluminium alloys,https://core.ac.uk/download/151393698.pdf,,,,core
8595117,,"A major challenge in modern robotics is to liberate robots from controlled industrial settings, and allow them to interact with humans and changing environments in the real-world. The current research attempts to determine if a neurophysiologically motivated model of cortical function in the primate can help to address this challenge. Primates are endowed with cognitive systems that allow them to maximize the feedback from their environment by learning the values of actions in diverse situations and by adjusting their behavioral parameters (i.e., cognitive control) to accommodate unexpected events. In such contexts uncertainty can arise from at least two distinct sources – expected uncertainty resulting from noise during sensory-motor interaction in a known context, and unexpected uncertainty resulting from the changing probabilistic structure of the environment. However, it is not clear how neurophysiological mechanisms of reinforcement learning and cognitive control integrate in the brain to produce efficient behavior. Based on primate neuroanatomy and neurophysiology, we propose a novel computational model for the interaction between lateral prefrontal and anterior cingulate cortex reconciling previous models dedicated to these two functions. We deployed the model in two robots and demonstrate that, based on adaptive regulation of a meta-parameter β that controls the exploration rate, the model can robustly deal with the two kinds of uncertainties in the real-world. In addition the model could reproduce monkey behavioral performance and neurophysiological data in two problem-solving tasks. A last experiment extends this to human–robot interaction with the iCub humanoid, and novel sources of uncertainty corresponding to “cheating” by the human. The combined results provide concrete evidence for the ability of neurophysiologically inspired cognitive systems to control advanced robots in the real-world",Robot Cognitive Control with a Neurophysiologically Inspired Reinforcement Learning Model,https://core.ac.uk/download/pdf/8595117.pdf,Frontiers Research Foundation,,,core
24470382,1994,"ARCHON is an ongoing ESPRIT II project (P-2256) which is approximately half way through its five year duration. It is concerned with defining and applying techniques from the area of Distributed Artificial Intelligence to the development of real-size industrial applications. Such techniques enable multiple problem solvers (e.g. expert systems, databases and conventional numerical software systems) to communicate and cooperate with each other to improve both their individual problem solving behavior and the behavior of the community as a whole. This paper outlines the niche of ARCHON in the Distributed AI world and provides an overview of the philosophy and architecture of our approach the essence of which is to be both general (applicable to the domain of industrial process control) and powerful enough to handle real-world problems. 1 1",Nationwide summary of U.S,,,,,core
17301421,,"Computerised notational analysis is now widely used in the sports industry. Most, if not all, professional sports teams or individuals use some form of video analysis through a computer and software packages. However there is little research into the efficiency of match analysis systems, furthermore, the areas of human-computer interaction and artificial intelligence in match analysis systems have also been neglected.

    Human-computer interaction is the study of the relationship, which exists between human users and the computer systems they use in the performance of their various tasks (Faulkner 1998). Whilst there are many definitions of artificial intelligence, it is first important to understand intelligence. There are very many definitions of intelligence; Negnevitsky (2001) defined intelligence as, ‘The ability to learn and understand, to solve problems and make decisions. The definition offered by Lapham and Bartlett (1995) was, ‘The faculty of understanding’; ‘the action or process of understanding’. Negnevitsky considered artificial intelligence to be a science that aims, ‘To make machines do things that would require intelligence if done by humans’. Lapham and Bartlett expressed the concept as, ‘The capacity of a machine to simulate or surpass intelligent human behaviour’.

    This paper therefore investigates and discusses the efficiency and ergonomics of selected squash match analysis systems. The three systems analysed were Focus X2 manual system using a mouse, Focus X2 Voice Interactive system and the SWEAT (Murray and Hughes, 2001) system using keyboard data entry. The three systems were analysed in real time match analysis and lapsed time analysis using winner and error analysis. 4 matches were analysised in lapsed time and real time for each analysis system. Whilst the analysis procedure was being completed, video recording of the data entry were recorded. The study investigated data inputs per minute and analysis time, in lapsed time analysis. Whereas in real time analysis the paper examined the analysis times, errors made, error corrections, error correction times and total analysis time.  A training study was conducted prior to the efficiency analysis to enable the researcher to gain sufficient learning of systems and prevent bias. A percentage difference calculation stated by Hughes et al. (2004) was used to perform an intra-operator reliability investigation in real time analysis and lapsed time analysis, overall highest errors being 3.9%, which were deemed satisfactory (Howells, 2006). 

   From the results, it found that the Focus X2 manual system was the most efficient in both lapsed time and real time analysis. The efficiency profiles of the analysis highlight the strengths and weakness of the human computer interaction of the analysis systems. The findings of the study also concluded that from the three-match analysis systems there were significant ergonomic characteristics that affected the operator in the case of repetitive injuries. Furthermore, discussions into the development of technology to enhance the efficiency and ergonomic characteristics of match analysis software and computers. It was found from the study that more research into human computer interaction is required and also to research the use of artificial intelligence and the application to performance analysis",The efficiency and ergonomics of different data entry systems in real-time and lapsed-time computer notation systems,,,,,core
200997231,,"Icosahedron is an artificially intelligent crystal ball that predicts the future of prediction.



Silicon Valley nurtures and promotes certain thinkers of the future, from Ayn Rand to Stewart Brand, Ray Kurzweil to Michio Kaku. The visions of such writers have provided technologists and entrepreneurs alike with highly effective discursive framings for predicting and influencing the future, which has resulted in the acceleration and fortification of neoliberal techno-utopianism.



Technical predictions of the future—not only philosophical foresight—have become a major preoccupation of the global tech industry, as evidenced by rapid developments in machine learning, risk assessment software, consumer analytics, and predictive policing. In turn, Silicon Valley companies frequently integrate fantasy and magic into their predictive tools. Consider Palantir Technologies, co-founded by Peter Thiel: a data analytics company named after a crystal ball in The Lord of the Rings.



While tech elite toy with the world’s future, Icosahedron plays with their worldview of the future. Icosahedron is modeled after the twenty-sided die inside a Magic 8-Ball, a popular American fortune-telling toy designed to offer ten affirmative responses, five non-committal, and five negative. Similarly structured, Icosahedron is trained via machine learning techniques on twenty writings influential to Silicon Valley’s approach to predicting the future, including critics like Yuval Noah Harari and works of fiction such as Lord of the Flies. The outcome is an artificial intelligence created by accentuating the often overlooked condition that all predictive technologies are bound to material constraints and limitations.



Icosahedron exists between high fantasy and social reality: the computer is a crystal ball, and on the other side of its transparent glass, an immortal elf resides, predicting the fictions and futures of Silicon Valley futurism. Skip the TED Talk: to find out what’s shaping the future, ask Icosahedron",Icosahedron,,Walker Art Center and Thoma Art Foundation,,,core
1498740,1993-01-01T00:00:00,"Distributed Artificial Intelligence (DAI) systems in which multiple problem solving agents cooperate to achieve a common objective is a rapidly emerging and promising technology. However, as yet, there have been relatively few reported cases of such systems being employed to tackle real-world problems in realistic domains. One of the reasons for this is that DAI researchers have given virtually no consideration to the process of incorporating pre-existing systems into a community of cooperating agents. Yet reuse is a primary consideration for any organisation with a large software base. To redress the balance, this paper reports on an experiment undertaken at the CERN laboratories in which two pre-existing and standalone expert systems for diagnosing faults in a particle accelerator were transformed into a community of cooperating agents. The experiences and insights gained during this process provide a valuable first step towards satisfying the needs of potential users of DAI technology - identifying the types of changes required for cooperative problem solving, quantifying the effort involved in transforming standalone systems to ones suitable for cooperation and highlighting the benefits of a cooperating systems approach in a realistic industrial application",Transforming Stand-alone Expert Systems into a Community of Cooperating Agents,,,10.1016/0952-1976(93)90016-Q,,core
235570211,1994-03-01T00:00:00,"An archive of the Milo Canopener.The University of Lethbridge Library received permission from the Archives at Milo Library to digitize and display this content.Alvin WinchAd ladles j
Business BirectoryJS/mon 1
Quarter page...........
.$0.00 I
Half I’age......................
.$11.00 1
Full I'aqe.................
,$15.00 1
Classifieds..............
..$2.00 1
Sotiees...................
..$2.00 E
Thankyous..............
Baby & Wedding
Annouincemcuts.....n/c
Sews items, reports
.....ai/c
Subscription Rates
\
If you rociovo your mail in Atilo.........................$12.00
Out of Alilo nron.......$20.00
II.S.A
Out of country.......$30.00
Single copy .................... $2.00
Letters to the Can Opener are Welcome!
PLEASE SIGN THEM, no letter will be printed without
a signature.
The Can Opener is printed on the LAST MONDAY of each month, unless otherwise stated in the calendar on the back page.
PLEASE HAVE ALL ITEMS IN NO LATER THAN THE WEDNESDAY
BEFORE PRINTING.
You may mail them to: MILO CAN OPENER
BOX 12
MILO, ALBERTA TOL 1LO or
leave them at Jamies AG Foods store
or
Fax them to us at 599-383 5
or
Give them to one of the following people:
Sheila Winch, Levona Dixon, June Beckner, Marina Vannatta, Carol Henry, Zola ''Webber, Sue I vers, Ellen Watt or Betty Lahd§ftaii$ht (ffimi Jftmpk □nncE BHnn
LARKY VANNATT A 6-41-2,390
JK.capx.rLg- the aoxjCn.try In.
rn.xj.sic
Scotxafcaittk
The Bank Of Oava ScatTa tAdt)d(J6n Dan Hlnney
Bxxsincss IIoars: Mon.-TlLxx.rs- 10:00-12:00 1:00-3:00 Friday 9:30-12:00 1:00-5:30
'SQUTHERR CRERTianS
Ralph & D«rl«ng Gixut
Local made handcrafts
Bxxsr. 599-389S \”*T\\-v Home 599-5892 \ Box 99 Milo, AB TOL ILO
Lora-Lee
* Manicure nPedicure
X Kail tips t wraps
* Full body waxing i S Q'jb i brow tinting
POOUStronghtoolnq Agriculture
—Mvl (xw/ng it every day
FOR fiUL YOUR W MARKETING fit© AGRO PRODUCT NEEDS
Please caS anytime
M3o 599-3866 Queenstown 593-2151
OQWSCYR . 599-3745 TRENT JENSEN 599-2172 WAYNE HALM 599-2162
YOUR PATRONAGE !S APPRECIATED
PHILLIPS FERTILIZER RRO CHEftllCHL
Cxxs-tom Blending . Soil Sampling . Custom Sprending . Sprecedes Rentals- . Urea . Avadex Elephant Brand Dry &. Bxxltc
Lomond 792-3757 Milo • 599-3791
KILO CAFE
CHIRESE & UUESTERn
taice OUT ORDERS RIT. 599-3832
TUES - SAT 8-8 SUN 9-8 MON 8-7
Beer & wine with meals
Queenstown Seed Cleaning Association Ltd.
599-21SQ
When cleaned seed Is sown
Clcuuiec crop* aee jrowo ED POSEEN - manager
L.H. Phillips & Sans Ltd.
Box 39
Milo, Alberta TOL ILO TeL (403)599-3766
DR-' G.M. LISCOMBE
Cttlro praetor
(403)485-6005
P.O. Box 87 ;
Vulcan, Alberta. Canada
Poatlao-Bulck Chev-G3VTC
Che'irOltk Trucks
GM
PHARMASAVE
BUB'S SERVICE (1960) Ltd
- Bassano, Alberts.
Ph: 641-3828
PU0tE«03}4aj-2a 12 rWRMftSflME302
MrrcuejL’s pharmacy
201 CfXTTCISTRQET - PA.00X240
tULOYCOLOORTA TOL 200CON'fTOUED S wportK wraSur T?Z Wp'TTTT FOR Tlmm
PRINT THIS PAPER TOR THeInJOYMEN^P OUR READE^ T°
CP‘
’rain
erfeci ion
CATERING /SERVICE MmtCotecn B*txh RRII IOo.MkxU TOC ICO (*03) S»Z20<
Or. John IVL Helgers
^5' Qentist
P.O. Box 398 114 Third Avenue North Vulcan, Alberta TOL t.LO (403)485-0008
nelson Taxidermy
J3Locb: JRug*
Fish -Liiesize Mowib
-Btg Game He-axis: ’
Bef'n.ccrd.irx.e. Wetjrorx. (403)534-3764
J3ojc 2,24
Xrrowwoorf, AB TOL 1LO
(<03) 599-3922 BUS. (<03) 599-3X1 RES. 003) 599-3335 FAX
S/
•r r_j
» - \ >- i
fooos
JAMIE’S A. G. FOOOS
JAJAES L CARO). ROBERTSON
Proprietors
P.O. BOX 38 WlLO. ALBERTA TOL 1L0
THE
PSON__
fxwCCr OuwiCOmANF Um«YO
Friendly Efficient SERVICE For an your Farming needs
-Cod MIKE HINGtJEV anytime Our. 599-3787 «cc. 599-39 46
17 tiv fAarcK. Sav^
P^v ’xcVs
Mm Gbcfo
HAIR DESIGN Milo - 599-3940
OPGN
Tuesday to Friday
9.00 am-5:30 pm
SaOrdry
9:00 am - too pm Wednesday
ftOOam-12:00pm (dfopinsoafy) j owet-oremvoa rewreAntJ
Meadowlark Sewing floSert and Oartcne PWSos
Cox 32. Kkj. £4>ert a TOEIEO
«03«^-373l Fax <4031539-3791 V 14
/ZjJB*//-o F7A/4
EDr33SCE3i3TirrriTTr^
Ux: Monument PcofctVKXvil* SinceVillage of Milo
The Special meeting of the Village of Milo was held on Tuesday, January 11, 1994 at 4:00 p.m. in the Village Office.
Present were Mayor Umscheid, Councillors Monner and Hingley, Lynda Cyr, Bill Brown and Terry Mullinger on behalf of Mullinger Engineering.
^r' Terry Mullinger met with Council and staff and presented a complete engineering package and design spec, for the proposed new water; treatraenty.plant. The total »■ cost of the package is $ 307,110.00. Terry feels this can be reduced with a possible estimate of $ 250,000.00. The government will pay for 75% of the cost, and the village would be responsible for the remaining 25% or about $ 70,000.00. One thing that is not included in the quote will be some soil tests which will be about $ 1,000.00. This will be a demand pump system not a pressure system. Council wondered about the cost of running the system compared to the present one — Terry 8et back to them about this. Council discussed the current problems they are faced with now and feel there will not always be grant money to cover a project such as this. Councillor Monner made a motion that the Village of Milo hire Terry MuHi-nger Engineering with Terry Mullinger on a project management basis as per the proposal trying to cut costs where possible. Seconded by Councillor Hingley. CARRIED.
letter arid phone call is to be sent to Terry informing him of Council's approval.
A new account will be set up at the bank for this project.
Councillor Monner reported that he had attended a solid waste meeting in Vulcan with several engineers attending.
Councillor Monner made a motion to cancel cheque #0891 dated November 16, 1993 payable to Sebo Pump & Irrigation for $ 3,997.52. There have been some problems wxth this pump and Council are still negotiating. A cheque for $ 1,000.00 was sent last week and $ 1,500.00 is to be sent now. Councillor MOnner will be talking to Sebo.
A reply is to be sent to the County in regards to the cuts to the board of education. The Village of Milo is not in favour of this proposal.
A budget meeting will be held in the Village office on January 24, 1994 at 5:00 p.m. Lynda Cyr is to get the budget sheets to Council prior to this meeting.
By-Laws No. 299, 300 and 301 were presented to Council for reading. Councillor Monner moved second reading. CARRIED.
Mayor Umscheid moved the By-Laws be given a third and final reading.at this meeting CARRIED UNANIMOUS.
Councillor Hingley moved third reading of By-Laws NO. 299, 300 & 301.
Tarragon Oil and Gas is proposing to construct a pipeline in the Queenstown area and will be crossing the Village's Sewer Right-of-Way Plan 871 1524. Mayor Umscheid- made a motion that the Village sign a Standard Crossing Agreement with Tarragon in regards to this. CARRIED.
The meeting adjourned at 9:30 p.m.County Councillor (Grant Lafiti)
We now have the M.U.S.T. Project from our Government. This stands for Management of Underground Storage Tanks. God help anyone who has a leaky underground tank. Not many farmers would have one of these types of tanks but we nave not been over looked. In the not too distant future tanks over bUO gallons will have to have a burm around them. A nice thing to have in the middle of your yard, weeds and all. You can thank some envir­onmentalist for this expensive idea.
It is interesting to note that the Alberta Tax Reform Commission did not have any agriculture type people among the seven commission members. Some of their recommendations, if implemented, will certainly be detri­mental to agriculture. I would think one of the most viable overall in­dustries in Alberta should■;have received one seat on the commission opposed to, say the Mayor of Peace River. The machinery and equipment tax (mostly paid by oil companies on well head and pumping equipment plants) was certainly mentioned as being unfair. It should be of no surprise that there was an oil industry person on the commission.
I was unable to attend council meeting on Friday the 18th. I would have liked to question the school board chairman on the proposed amalgamation of two counties. I have heard the board had talked to Foothills already and had meetings proposed with at least one other adjoining board. Per- sonaly I think Foothills would be a poor choice. I think Newell or:.Wil- low Creek would be a more likely choice, mostly because they don't have the large number of small acreage people. These transplanted cityjpeopJLe don't think like we do. If amalgamation takes place it will be interes­ting to see if there will be any staff cut at the office level. My com­ments at our municipal meeting were not too well received by our admin­istrator and also some councilors. However, if we have to cut services where do we start. I have always maintained the elected councilors should be making the real decisions that effect this county.
These comments are my opinion and do not represent the County Policy.
Grant Lahd';
""T* ”TeA\ i^ou- i , \j om. Ko-i/e.
A oJowA e-PvxA Jt, ts Dr\""CVveRiver Wranglers Light Horse 4 —H Club
by Tammy Bushel 1
The River Wranglers hopes everyone had a safe and merry holiday season. On December 5, 1993 we had our
Christmas Party at the Milo Community School. We started off with a little volleyball which turned out to be a lot of fun. Next came the gift exchange with lots of wonderful presents. Hot dogs were served for lunch and they were delicious!
We rescheduled our January 9 meeting to January 12 and it was still held at Brooke Nelson’s household. The first thing we did was role call which was our horse's name. The 4-H Teen Dance, coming up on January 21, was the first thing on the agenda. All the 4-H members then signed up for either working at the door or concession.
The next thing on the agenda was our 4-H Public Speaking. We set the date for February 13 at 1:00 pm at the Arrowwood Hall.
Jeremy, the president, then read an invitation from the Indus 4-H club. It was an invitation to a curling bonspiel and potluck supper.
The Graham's volunteered to have the next meeting at their house on
February 9 at 7:00.
Thankyou to the Nelson's for the delicious snacks and juice that they provided after the meeting.
Any new members are more than
welcome and you can get further information by calling Susan
Williams at 485-6922 during the day or 534-2355 in the evening. ****************
Murray McCartney
B.A. LL.B
Barrister Solicitor Notary Public
115 - 2nd Avenue Vulcan, Alberta Phone 485-2039 485-2953 (res)
OFFICE HOURS: Monday - Friday 8:30 am -12 noon 1 pm - 4:30 pm
Evenings and Weekends
By appointment_______________
Happy 40
March 11
Love from your familyDOC
By J. Tom Bateman
-continued from last month
Doc became my primary concern. I was concerned over the state of my gluteus maximus from constant violent contact with sheep horns. It became a war where Doc was the powerful aggressor and I was the terrified victim.
My mother counselled me to quit acting afraid because animals can sense fear. I assured her I wasn't acting and that my fear was genuine and based on real exper­iences together with my natural cowardice.
I hated pain but even more I disliked the humiliation of being beaten by a 2 1/2 year old.
Doc always attacked from the rear. He would lie in wait behind the manure spreader or the fresno and only after I had passed by and was engrossed in innocent childhood fantasies would he get me. He could run on his tippy toes and his acceleration was phenomenal, ending with a great leap at the moment of impact. My head and feet would remain in place while the rest of me was propelled forward. The breath would leave my lungs and I would roll and tumble out of control for several meters. By the time I could get re-oriented and my vision focused Doc would be walking backward mumbling, threatening and gloating, getting ready for a supplementary strike. I developed a unique way of starting the 100 yard dash from a horizontal position as a result of these encounters. Doc provided me with considerable motivation for my track career although my teachers never did get used to my unorthodox starting style.
In 1949 I had been sentenced to 10 months of hard labor including a kind of torture.
I was forced to stay in a classroom for hours at a time. It was called grade 5 by the adults in our community. Being in school however provided me with time to carefully figure out ways to out-maneuver Doc. I fully intended to kill him if I could but he was too large and I was sure my Dad would notice he was gone, especially when there were no lambs next spring. My teacher misinterpreted my planning sessions as
daydreaming and was sometimes highly critical of me even though I was deadly serious in the search for ways to survive Doc.
One of my jobs on our farm was to feed the sheep. My Dad carefully instructed me to feed those blankety blank sheep plenty of blankety blank grain and emphasized the importance of diligence and prompt­ness. At the time, I was about 4 feet 8 inches tall. Carrying 2 five-gallon pails full of oats I headed for the trough where I intended to spread the oats out evenly, thereby affording all the sheep equal opportunity to eat leisurely and contentedly. It worked the first time Dut the sheep learned the buckets cont­ained oats, and they attacked me from all directions. The oats spilled and I was hog tied to the extent that I couldn't even fall down. A few sheep got lots of oats, most got none and I had a serious problem to work out during school hours.
I decided on a diversionary tactic that drew the sheep into an enclosure.
The gate was quickly closed and they were trapped. The bleating and baaing created a near riot that nowadays would bring the SPCA constables from all directions. It worked like a charm until the fourth day.
I did my diversion, locked up the sheep and filled the pails with oats. Half way to the trough I was violently propelled 24 feet through the air. One bucket bail was torn loose and I lost interest in the other. After tumbling and somer­saulting out of control I skidded to a stop facing Doc. ""Baa."" he pontificated, as he blew snot in the air.
The ideas and thoughts that passed through my 11 year old mind were creative, original and usually expected from someone much older. Doc had hid behind the fresno after I passed his blind. The oats were evenly spread out albeit not in the trough so I sprinted in a wobbly way to the near­est fence and cleared it easily. I let the sheep out of their trap and they stampeded for the oats. This was getting serious. Something had to be done.
- continued next page -DOC
- continued from previous page -
I couldn't go on suffering the intense levels of anxiety Doc was inflicting.
The solution had nothing to do with me.My Dad decided to feed the sheep more grain and Doc deployed the same strategy exactly on Dad as he had on me. Dad apparently didn't fly as far as I did and I doubt he landed as gracefully. He did however, decide on Doc's punishment while he was air­borne and how the punishment would be carried out while he was still tumbling.
We soon had another male sheep with a much more docile attitude. Doc was a part of an interlude in my life that left an impression that I could still live without.
My Dad decided I was old enough to learn to shear the wool off the sheep and he took me to a sheep shearing clinic. The first sheep I sheared I worked on for 2 hours and had to let her up for a coffee break before I separated her from her fleece. I kept at it and soon could shear a sheep in 10 minutes or so. I was shearing our sheep when the new male sheep - the replacement for Doc - had his turn. Somehow I forgot that male sheep were anatomically different from female sheep and my Dad had to get another male sheep.
My Dad seemed to be convinced that the accident was deliberate and was done only to aggravate him. Maybe there was a sub­conscious connection in my mind between Doc and all male sheep.
L4.J.X J,XXJ
.JLXXXJLXXXXX
tAAAAAAAAAA
CORRECTION
It was reported in the January Can Opener that the air fare to Russia was $4000.00 if purchased in Canada. This is much higher than it really is. As of February 23 on the Apex plan the return fare is $1574.00 if purchased in Canada. If bought in Moscow it is about $1200.00 - still a substantial difference
Congratulations!
Te
Dauid + dkat-Utte
ea
dt\ ""tWs. to l r llv. o“A
Ionise Plairleoe flruaivd a,
cAA VlospTt*<il
k.ao,fW -
^#W0p - LoRhte. +-
Plarlene-
+» Ckrt -aTf ne. 7
©
p a

The Milo Grain Marketing Club has held 4 meetings. We have looked at various grain marketing newsletters,grain marketing stragetories, crop and price projections, as well as marketing alternatives and the use of options and futures.
If you are interested, drop in st any of the following meetings.
Meeting time is 7:30 p.m. on the first and third Tuesdays.
March 1 - Cord and Kelly Nelson
March 15 - Gordon Vooys April 5 - Cathy Nelson April 19 - Ken Nelson May 3 - Gary McMorris May 17 - Keith Godkin June 7 - Michael Monner June 21 - Keith Deite
’******************************************CABARET
Saturday, March 5 L Milo Community Hall
$10 /person includes late lunch
3L
Live mysic b^:
Karen K*
doors open 9:00 p.m., music till 2:00 sponsored by Milo Curling Club
J VOLUNTEER J V APPRECIATION NIGHT*
V
V
V
V
V
V ¥
V
V
V ¥
THURSDAY, APRIL 21
liiiiiiiiiiiiiiiiiiuiiiinniiiiiiiiiuiiiniiiii
GOOD WORK
d
7:00 PH
V
V
V
V
V
V
k
HILO
QHHUNITY HALL * THERE WILL BE ENTERTAINMENT ? BRING THE WHOLE FAHILY
* EVERYONE WELCOHE !
COUNTRY
CARPET
CARE
Specializing in:
Carpets, Upholstery, and Drapery RVs, Autos,. Windows and Exterior Home Cleaning
WE CLEAN DRAPERY RIGHT IN YOUR HOME!
Ross and Shirley Zuehlke Box 566, Vulcan, Alberta TOE 2B0
Phone 485-6359MILO PARENT TEACHERS GROUP February 15,1994.
Joanne called the meeting to order.
The GST reimbursement will be used to cover some of the costs of unloading the recycling trailer. Ross Nelson generously donated the use of his truck to pull the trailer to Lethbridge. Barb said she’d drive it Grant Lahd offered to pull the trailer down to unload as well.
Colleen has offered to supervise the students at noon while they use the library's new CD Rom.
There will be a community assembly at the school for everyone on every second Monday at 8:45. This is to open the doors of the school to all and will highlight school happenings and give special student recognition.
Thankyou to the Milo Lions Club who generously donated $750 to our group for our student awards program.
The School Board has accepted the School calendar we agreed on at the last meeting.
Grade 4 students agreed to send their kids to Camp Chief Hector. Thankyou to the Ag Society who offered to pay 1/2. The parents will pay the other 1/2.
We decided to have a yearbook this year. Fridays off will give a great opportunity to work on this. The students will as for sponsors for this.
It was decided to raise the price of hot lunch to $3 from $2.
If you have a step that the kids could borrow for step aerobics in gym class, they would greatly appreciate it
The students bonspeil will tentatively be March 22 - 25.
Colleen Deitz attended the School Board Meeting and gave us a report on it The Assistant Deputy Minister of Education was there talking about regionalization.
Mike Monner and Colleen gave the board a presentation similar to the one given at our public meeting a month ago. The board didn’t dispute their figures. Mike also gave a suggestion as to how they could use the county surplus.
Marlys adjourned the meeting.
V
YOUTH
If anyone is interested in being a host family for the Lions Youth Exchange Program for the summer of 1994, please call Bill Sharp at 485-6468
^_______________________________________&
LIONS
EXCHANGE PROGRAM
%lowers V*
1 'v flower Shop SRwue ’VJ
Look for us in our
New Location
(former Milo Promotions Building)
THANKYOUTO ALL WHO SUPPORTED US IN THE PAST TEN YEARS. YOUR PATRONAGE WAS MUCH APPRECIATED. WE LOOK FORWARD TO SERVING YOU IN OUR NEW PREMISES WITH OUR FLCWERSHOP&GIFTWARE
it* to* to* kftit* kt* hr* to* ift to* if* to
THE TEAROOM WOX REMAIN IN THE SAME LOCATION UNDER NEW v MANAGEMENT
Vulcan, AB T0L2B0 Dor othy Healy 485-2746
S3—.rtMilo Community School Scoop
The students from grades five through to nine have once again completed a successful ski trip to Fernie. The conditions were ideal. Warm with no snow or RAIN on Thursday. Friday was again warm with snow all day. Powder was plentiful!!!!
The Milo students conducted themselves really well and the motel management remarked how they really do enjoy having the Milo Community School coming to their facility.
Thanks to John Davidson who volunteered his time and bus driving which went along way to making the trip a successful one.
Thanks also to the following adults: Kathy Vooys, Blahne Sukut, Virginia Beckner, Neil Godkin, Barry Monner, Betty Godkin, Barb Godkin, Mike Hingley, Carol Robertson, Sharleen Bushell, Loretta Doore, Colleen Bartsch, Karen Forestell and Rocky Wilson.
The junior high students are now preparing for their outdoor survival trip to Caroline. The trip is scheduled for March 16th to the 18th. This proves to be a very educational outing. The concepts covered in class and in the outing prove to be useful for the rest of their lives. I have talked to past students and adults who have taken part in the outing and they have said how useful the concepts have proven to them afterwards. The skills and concepts covered have to do with - solo survival, hypothermia, first aid, setting snares, wildlife identification, orienteering (compass work), gun safety, food & menu planning and developing stronger group skills. I’m hoping for good weather!!
The students have been fortunate to have members of the community share with them their expertise in new areas of physical skills. Blahne Sukut provided us with two weeks of karate classes. During the two weeks, he introduced the students to an","Milo Canopener (March 1, 1994)",,Milo Community Volunteers,,,core
468698940,,,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/AIKE.2018.00042,,core
341985688,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds",,'ASTES Journal',10.25046/aj020358,,core
334578949,,"Beginning and experienced programmers will use this comprehensive guide to persistent memory programming. You will understand how persistent memory brings together several new software/hardware requirements, and offers great promise for better performance and faster application startup times—a huge leap forward in byte-addressable capacity compared with current DRAM offerings. This revolutionary new technology gives applications significant performance and capacity improvements over existing technologies. It requires a new way of thinking and developing, which makes this highly disruptive to the IT/computing industry. The full spectrum of industry sectors that will benefit from this technology include, but are not limited to, in-memory and traditional databases, AI, analytics, HPC, virtualization, and big data. Programming Persistent Memory describes the technology and why it is exciting the industry. It covers the operating system and hardware requirements as well as how to create development environments using emulated or real persistent memory hardware. The book explains fundamental concepts; provides an introduction to persistent memory programming APIs for C, C++, JavaScript, and other languages; discusses RMDA with persistent memory; reviews security features; and presents many examples. Source code and examples that you can run on your own systems are included. What You’ll Learn Understand what persistent memory is, what it does, and the value it brings to the industry Become familiar with the operating system and hardware requirements to use persistent memory Know the fundamentals of persistent memory programming: why it is different from current programming methods, and what developers need to keep in mind when programming for persistence Look at persistent memory application development by example using the Persistent Memory Development Kit (PMDK) Design and optimize data structures for persistent memory Study how real-world applications are modified to leverage persistent memory Utilize the tools available for persistent memory programming, application performance profiling, and debugging Who This Book Is For C, C++, Java, and Python developers, but will also be useful to software, cloud, and hardware architects across a broad spectrum of sectors, including cloud service providers, independent software vendors, high performance compute, artificial intelligence, data analytics, big data, etc",Programming Persistent Memory,https://core.ac.uk/download/334578949.pdf,,,,core
23270923,1994,"ARCHON  ^TM  (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe&apos;s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applic..",The Archon System And Its Applications,,,,,core
42619901,,"In recent years there has been a growth in the automotive industry, coupled with a growth in the amount of electronic components and systems in a modern vehicle. The higher amount of electronics has led to an increased amount of Electronic Control Units (ECU) in a vehicle which require advanced simulation based testing procedures throughout their development process. One such method is Hardware in the Loop (HIL) simulation in which a real ECU is connected to simulation models of its environment via a real-time simulator. This project is concerned with developing a plant model of a windscreen wiper system for use in the development of Jaguar Land Rover’s (JLR) body electronics ECU.



The system is divided into four parts which are modelled separately: Wiper motor, linkages, arm and blades, and the windscreen environment. The wiper motor and mechanical elements models are derived and implemented using the physical modelling tools SimScape and SimMechanics. A dynamic friction model describing the interaction between the wiper blades and the windscreen is developed, based on results presented in the literature. A simple aerodynamic model describing the forces on the wiper blades is also established.



The parameters of the models are derived using three sequential optimisation methods: Transfer function parameter identification, Genetic Algorithms (GA) and a nonlinear least squares local optimiser. A transfer function relating the motor current to the voltage was derived for step one, and a bespoke GA has been developed for step two. The parameters were successfully identified. Following this, Artificial Neural Networks (ANN) were used to convert the physical models into real-time capable models suitable for HIL simulation. Finally, adaptive control systems are designed in order to maintain the motor at a constant velocity.



The models are presented in a Simulink library and graphical user interface modelling tool for ease of use","Modelling, real-time simulation and control of automotive windscreen wiper systems for electronic control unit development",https://core.ac.uk/download/42619901.pdf,,,,core
1359683,,"Reverse engineering (RE) is a process to create computer aided design (CAD) models from the scanned data of a existing part acquired using 3D position scanners. This paper proposes a novel methodology for extracting geometric features directly from a set of 3D scanned points. It uses the concepts of feature-based technology and artificial neural networks (ANNs). The use of ANNs has enabled the development of a flexible feature-based RE application that can be trained to deal with various features. The following four main tasks were investigated and implemented:
 
1. Point data reduction module.
 
2. Edge detection module.
 
3. ANN-based feature recogniser.
 
4. Feature extraction modules.
 
The approach was validated with a variety of real industrial components. The test results show that the developed feature-based RE application proved to be suitable for reconstructing prismatic features such as blocks, pockets, steps, slots, holes, and bosses, which are very common in mechanical engineering products. An example is presented to validate this approach",Geometric feature recognition for reverse engineering using neural networks,,SPRINGER-VERLAG LONDON LTD,,,core
235571268,1993-07-14T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
30 cents Wednesday July 14, 1993
The Chinnok Health Unit would like to
congratulate the following winners in the
*93 FITNESS CHALLENGE*.
First Place Winners:
Awful/Lawful R.C.M.P,
Donna Warnock
Reyko Nishiyama
Lori McRoberts
Greg Squire
Ron McGeachy
Darrin Turnbull
Mac Nishiyama
Bob Muskovich
Greg Perrett
Gavin Warnock
Second Place Winners:
""Chinooks"" Health Unit
Juliann Sommerfeldt
Dalan Sommerfeldt
Marie Sabey
Donna Kearl
Lydia Brunner
Melodie Johnston
Mary Sallenback
Jean Bolokoski
Charlotte Webster
Janet Squire
Third Place Winners:
""The Germinators""
Dahl Clinic
Bev Biddlecome
Sharon Jarvas
Cathy Gruninger
DeNai Harker
Dennis Dahl
Mark Dahl
Ken Dahl
Brenda Stringam
Lance Harker
Marilyn Dahl
BEST TEAM NAME; ""Kalorie Killers""
S.A.S.H. Team Members: Drena Harker,
Bonnie Soltys, Ted Clifton, Sybil George, Grace
Lonnerts, Karin Sabey, Phil Sillito, Lorna Harker,
Zola Wolff, and Karyn Healy.
Prizes were awarded to all winning participants.
We would like to acknowledge the other teams
entered and compliment them on their good work:
Diamond Willow Diamonds - Magrath Lodge
Fat Fighters - S.A.S.H.
Mighty Jocks - S.A.S.H.
David & The Goliaths - Town of Magrath
Bulging Money Bags - Bank of Montreal
Grandmas & Grandpa + 3 - TOPS Club
Tanco Woodchucks - Tanco
*******
The children of Carol Beswick and Ed
Ririe wish to announce the marriage of their
parents. The unpredictable couple were married
in the Magrath United Church, June 29, by Rev.
Doug Cowan (B.B.). Sharing the occasion were
JZd's brother Rod and Carol's daughter Anne. A
musical solo was provided by grandson Evan.
The couple will reside in Australia for five
months, spend Christmas in Florida, and return to
Magrath in January when Ms. Beswick will
complete the school year.
*******
DO YOU LIKE TO LIP SYNC?
DO YOU LIKE FUN TIMES?
DO YOU LIKE GREAT PRIZES?
If you answered yes to any of these questions then
you need to enter the First Annual Magrath
Celebrations Lip Sync Contest! This exciting
and entertaining event will be held in conjunction
with the town dance on Friday, July 23rd starting
at 9:00 p.m. in the Tom Karren Gym. This
contest is for all ages to participate. For more
information call Brad or Shannon Sabey or you
can register at the dance.
COMING EVENTS
You are all invited to
come and say hello to
Courtney & Ilena, the new
baby, and Steele & Laree
Brewerton. Hope to see you
July 18th from 4 - 6 p.m. in
the Senior Citizens Centre.
*******
WEDDING
ANNOUNCEMENT:
Bonnie Bolokoski to Rhett
Mandin Saturday, July 17th.
A calling reception will be
held in their honor at the
Magrath Seniors Centre from
7:00 - 9:00 that evening.
Everyone welcome.
*******
NOTICE: The Open House
for Dave and Lisa Sannes has
been cancelled due to illness.
Call 752-4594.
*******
SENIORS NEWS
Our dinner for this month will
be held Wednesday, July 14th
(TODAY) at 6:00 p.m. at the
Seniors Centre. Come and
enjoy the good food and
pleasant company.
On Saturday, August
14th we are planning for a day
trip that will include a visit to
the Old Man River Dam, lunch
at noon in Fincher Creek (not
included in the price of the
trip) and on to Bellevue for a
tour of the coal mine there.
Then off to Waterton for
supper and entertainment at
Olsen's. The price is $30 each.
If you are interested
phone at once as spaces are
going fast. Call Bakers at
758-3207, Margaret Leishman
at 758-3241, and Doreen
DRY GOODS SPECIALS f
c
J
ALL FABRIC
Including Tricot, Brushed Nylon,
Nvlon Sheer, broadcloth, flannelette, etc. 30% OFF
ALL LACE
20% OFF -**•>. F-------- <—n • ~ W r-M W
j—_jj—
ALL BABY QUILT PANELS
50% OFF
FABRIC PENS & PAINT
50% OFF
Alston at 758-3281, or Hazel
Rasmussen at 758-3545.
REMINDER: Spirit of
Alberta Mini Guard Camp will
be held July 19-24. There is a
late registration fee and
registration forms are available
from Mr. Chatwin at 758-3765
or Elizabeth Strong at 758-
3166, or send to Box 222.
COED RECREATIONAL
SOFTBALL SUMMER
LEAGUE - 18 years and
older. To begin July 7th and
will continue throughout the
summer on Thursday nights.
Phone and register with Laurel
Bennett (758-6222).
FRIENDS OF THE
MAGRATH PUBLIC
LIBRARY FOUNDATION
QUILT RAFFLE
Come and see this beautiful
quilt on display
in the front
window of the
Magrath
Trading
Company.
Draw will be
made July 24th.
TICKETS: $1.00 available at
the Library or call:
Ann Pilling
Ann Fazikos
Marie Stevenson
Bernice Sillito
758-3088
758-6425
758-3540
758-3618
Hazel Dudley 758-3213
t LADIES WEAR SPECIALS J
/•K.Z.
&&&ONE RACK OF CO-ORDINATES
skirts, slacks, blouses
1/2 PRICE
/come in and check out our great
Si SELECTION OF ""MAGRATH"" T-SHIRTS.
(ft
ALL NIGHTWEAR, DUSTERS, ROBES, ■
SLIPS, CAMISOLES, & BRUNCHCOATS [
1/2 PRICE
ONE RACK OF CHILDRENS WEAR
1/2 PRICE
~ SELECTED EARRINGS
3 PAIRS FOR $1.00
ALL JEWELLERY
1/2 PRICE
HYPERMEDIA COMPUTER CAMP
dairy specials
1 Alpha Sour Cream 500 ml $1.78 each
1-------------------------- - —
j 1886 Ice Cream 1 litre $2.68 each
j Beatrice Diet Yogurt 175 g 2 for $1.09
1 Beatrice Cottage Cheese 500 g $1.49 each
Western Family Cheese Slices 500 g $3.38 each
Western Family Margarine 1.36 kg $1.98 each
Western Family Soft Margarine 454 g .78 each
will be held from Tuesday, August 3th to
Once again the four Magrath Wards Relief
Societies, the Magrath United Church, Catholic
Church, and Lutheran Church ladies are
combining their efforts this Friday, July 23rd
and Saturday, July 24th in a ’’GIANT
COMMUNITY BAKE SALE"" to be held in the
Magrath Ice Arena. Please have your baking at
the Arena by 2:00 p.m. on Friday, July 23rd for
pricing. ""Thanks"". Selling will begin Friday; ;
July 23rd at 4:00 - 7:00 p.m. Selling Saturday,
July 24th will be
from noon until
baking is all gone.
Please bring your
baking wrapped and
ready to sell. All
proceeds will go to
the ""LIBRARY AND BUILDING
FUND"" Best sellers are meat pies, butterhorns,
bread, fruit pies, tarts, donuts, iced or un-iced
cakes, popcorn balls, Rice Krispie squares, and
various rhubarb treats. Very slow sellers are fruit
loaves, squares, and muffins, or any kind of
cupcakes and cookies. Bake ahead and freeze if
you won't have time later. VOLUNTEERS ARE
NEEDED. Save your bags for bake sale counter,
PLEASE SUPPORT THIS VERY
WORTHWHILE COMMUNITY PROJECT. For
more information please contact Irene Ririe 758-
3456, Lorraine Balderson 758-6380, or Alice
Stevenson 758-3190
Saturday, August 7th at the Magrath
H School Computer Room. There will
. be two sessions offered: Grades 4-5
from 9:00 a.m. - 12 noon and Grades
? 6 - 8: from 1:00 4:00 p.m. The camp will be
limited to 15 students per session. The camp will
focus on enhancing students computer skills in the
area of multimedia. Students will work with a
variety of software and hardware. The Instructor
is Bonny West and the cost is $50.00. Please
phone Mrs. West at 758-3072 for more
information and registration forms.
*******
THANK -YOU: We wish to express our
appreciation to all those who helped in any way to
make our wedding such a special day. A very
special thank you to all of our friends and family
who helped with the decorations and family
dinner.
Joanne & Kevin Tidball
James & Charlotte Anderson
Dennis & Donna Tidball
*******
MAGRATH GOLF CLUB RESTAURANT
SPECIALS July 19 - 23""
Monday - Baked Virginia Ham
Tuesday - Roast Breast of Chicken
Wednesday - Swiss Steak
Thursday - Grilled Pork Cutlet
Friday - Roast Beef
SOUTHCHIEF BASEBALL
The Southchief baseball
All-Star teams began exhibition
games and tournaments this
past week.
The Seniors (14-15)
participated in a tournament in
Kalispell and placed fourth
winning two
and losing
two. Darren
Baker was the
winning
pitcher in one
of the games.
The
Juniors (13)
have played in
two
tournaments
and a few
exhibition
games.
Although they have lost some
games, they are improving
with each game.
The Majors (11-12)
participated in a tournament in
Lethbridge last weekend and
were 1-2. They lost their
first game to Lethbridge
Norcrest, their second game 3
- 1 to Regina, and won their
3rd game by defeating
Edmonton 12-2. Casey
Christensen was the winning
pitcher in the 3rd game while
Darren Balderson had a double
and single to lead the
Southchief hitters.
The Triple A's (9-10)
A Team won their first district
playoff game and then went
undefeated in a tournament in
Fort Macleod. Winning
pitchers were Brooks
Blackmer, Ben Wood, and
Jimmy Balderson. The Triple
A - B Team won one game
and lost two in the same
tournament.
District Playoffs
continue this week with the
following home (Magrath)
games for the teams.
SENIORS: Wednesday, July
14th at 6:30 p.m.
MAJORS: Tuesday, July 13th
at 6:30 p.m. & Wednesday,
July 14th at 6:30 p.m.
TRIPLE A: Tuesday, July
20th at 6:30 p.m. & Saturday,
July 24th at 12:00 noon - final
tournament.
£ 1
WHi «s» wi
<r»
’*JAr '
Oate: July 26 - July 30
Place: Tom Karren Gym
Cost: $60.80 Per Rthlete
Eligibility:
Camp *0*- Grades 4-6 (Co-ed)
Camp ”8""- Grades 1-9 (Cs-etij
Time: Camp ’fl’- 9:00am-12:00pm
Camp ""B’- t :80pm-4:08pm
Instructor Philip Tollestrup
Note: -This camp will operate on a first come first serve basis.
- Rthletes must proulde their own sneakers and gym clothes.
- Each camp will include fundamental drills on Shooting, Defence,
-Individual moves. Simple Team Offences, and Rebounding.
-There will also be scrimmages each day and a tournament on
the final day of each camp.
-Camp T-Shirts will be given to each participant.
EOEE OSTO DOT DDDN (F(TOM
Name:__________
Grade:___________
Please Check one:
.Phone:_____________________________
Camp “R'-Upper Elementary $68.00
Camp “B“-Junior High $68.00
Send this form with tuition fees to: Philip Tollestrup
Boh 398
Magrath Rlberta
TBK 1J8
758-6716
OH: Bring the form with tuition to Mr. Tollestrup at schoo
MAGRATH RECREATION
NEWS
Registration for the Third
Session of Swimming Lessons
will be held July 17th at 9:00
a.m. Lessons begin the
following Monday, July 19-30.
* # sje
Special thanks to the
Magrath Ambulance Service
and Dr. Dennis Dahl for the
quick response and medical
attention they gave my dad last
Friday. He's doing well now.
Thanks ever so much.
Kirk, Judy, & family
CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758-6377
Less than 30 words—$1.07
Small ad (2.5""X3.5"")-$5.35
1/4 page---------------------$7.49 ! Full Page—Copy Ready—$25.00
1/3 page---------------------$8.56 j Full Page—We do----------$37.45
1/2 page--------------------$10.70 jj Flyer insertion (your paper)$21.40
FOUND:
FOUND: Bicycle helmet at the school playground.
Identify to claim. Phone 758- 6840.
GARAGE SALES: MULTI-FAMILY GARAGE SALE: Friday, July 16th from 9:00 a.m. - 2:00 p.m. at the PanTree Foods Location
*******
If you are interested in having a Garage Sale, or just have items you want us to sell for you, call Lynne at 758-3107 or 758-6212 for more information.
*******
MOVING SALE: To be held July 31st. Variety of items - big and small. All must sell! Don't miss it! Watch for location and times in next weeks paper. Yoshihara's.
LOST:
LOST: Pink and White ""Lil"" Princess bike. If you have / seen it, please call Jacie
Chipman at 758-3675. Thank you.
MERCHANDISE:
FOR SALE: Used bathtub, sink, and toilet. Phone 758- 3724.
4: * * ** *
TO GIVE AWAY: Free hamster, food, and shavings. Call 758-3615.
FOR SALE: Various waterbed mattresses & liners, also one fiberglass shell for full size truck - $100 o.b.o. Phone D. Hatch 758-6652.
* * *****
FOR SALE: 20 poles 17' long, 8 - 9"" bottoms. $15.00 each. Call Cam Jordan at 758- 6532. 144 N 2nd Ave.
FOR SALE: 1. One Culligan water softener complete.
2.
One iron remover - complete.
3.
One set of wooden stock racks to fit L.W.B. Narrow Box Chev.
4.
One Bell & Howell slide projector with 38 cubes.
5.
500 gallon stock watering trough.
For any of these items, call 758-6271.
*******
FOR SALE: Large log doghouse. Can be seen at 81 E. 2nd AveN. For more information ' call 329-4938.
*******
FOR SALE: 2.5 - 20 lb Fire Extinguishers through the fire department to raise money for equipment. Firefighters will be going door to door or phone 758-3167 or 758-6804.
*******
FOR SALE: Nintendo - $75.00 or best offer. Games for sale - individually marked. Phone 758-3615.
FOR SALE: New Singer Knitting Machine with ribber attachment, complete with instruction books and custom built table. Asking $1100 cash. Phone 758-3545.
3k 3k He* 3k 3k 9k
FOR SALE: Fresh raspberries. Call Marie at 758-3527.
*******
FOR SALE: Old garage door. Give offers. Phone 758-3605.
*******
SEASONAL CRAFTS FOR SALE - by Lisa Jensen at Sue's On First.
*******
FOR SALE: 35 used golf balls including 7 Titleist, 11 Topflight - $16.00. Call Ryan at 758-6894.
*******
FOR SALE: 2 childrens ""Big Wheels"" - $8 each. 758-6378.
REAL ESTATE:
FOR SALE: 1288 sq. ft. bungalow in Magrath. Priced to sell at 39,900. Call Sharon Bacon at 382-7696 or Royal Lepage at 327-2111. MLS.
SERVICES:
Have mowers will travel. Call Russel or Robert at 758-3107.
*******
CAKES BY RITA - Will do cakes for almost any occasion. Advance notice greatly appreciated. To order, please phone 758-6315.SERVICES:
ATTENTION ALL MTM CUSTOMERS:
Due to the Magrath Celebrations we will have
pick up on Friday, July 23 instead of the 24th.
TED GREGSON STUDIOS -
Weddings, family groups, general
photography. Reminder to 3rd & 4th Ward
members - the building dedication memorial
ward photos are available at $10 for an 8x10.
Call 758-3110.
*******
NOTICE: Joy Christensen is doing hair on
Wednesday and Friday at Sue’s On First, 128
N. 1st St. W. Call for an appointment 758-
6851 or 758-6711 on Wednesday or Friday.
*******
CUSTOM HAYING - Cutting and round
baling. Phone Rick Strate at 758-6749.
*******
FOR ALL YOUR
LASER PRINTER NEEDS
! labels 1 flyers / resumes
I type setting for books I
Call Bonny 758-6309
VEHICLES;
FOR SALE: 1986 Honda Spree Scooter.
Red in color, 2000 km. You get 50 km to a
tank of gas. Please phone 758-3209.
*******
FOR SALE: 1980 Chrysler LeBaron. Also a
1972 Chev Van- $1000 obo. Phone 758-6652.
WANTED:
WANTED: Two single or double beds. If
you have any, please give us a call. Lise or
LaDean Thompson 758-3209.
*******
WANTED: A pitchfork. Do you have one
just hanging around your place just doing
nothing? Please give me a call if you want
to part with it.
NOTICE: The Swimming Pool is holding
""AQUAFIT"" nightly from 9:00 - 10:00 p.m.
Anyone interested is welcome.
COALHURST MINERS DAYS CRAFT SALE:
August 21, 1993 from 10:00 a.m. - 4:00 p.m. at
the Coaihurst Community Center. Tables sold at
$25 each. To book tables call Ruth at 381-4528.
The Monument Professionals Since 1924
I am now your Magrath area Remco Counsellor.
If I can help you with your memorial monument needs
please give me a call at 758-6386.
An important part of planning your estate
is selecting a cemetery monument. Like
a will, life insurance or arranging for a
professional funeral home director, a
decision on a memorial to your life
finalizes the important things for those
left behind - with love.
HEATHER THOMSON
MEMORIAL COUNSELLOR
758-6386 RgMCO
REMCO MEMORIALS LTD
». ....
I WILLIAM RON ALDA
I 1■■ 91•' --1 —■ v1 9.3 ■6■■■ ■ 1917—. • 19. 3■ 6
CARL Vtxi UNOIRty CAVt
RfllRhiD te.rXR LXtXLSS iOVf
WWW MEAT SPEOAI.S
I ........ 1 1 ---- T
i Outside Round Roast
$2.29 lb
$5.05 kg
Top Round Steak
$3.29 each
$7.25 kg
Seasoned Chicken Legs
$1.49 lb
$3.28 kg
Seasoned Chicken Breast
$2.69 lb
$5.93 kg
Devon Sliced Cooked Ham
175 g
$1.29 each
-------—-
Country Cottage Bacon
500 g
$1.39 each
Maple Leaf Deli Style Meats
125 g
$2.29 each
Schneiders Wieners
450 g
$1.89 each
Fletchers Deli Sticks
500 g
$2.69 each
LIBRARY NEWS
All children in grades 1-6 who are registered in the Summer Reading Program: Drop in at the library this Thursday and each Thursday all summer between 1:00 - 2:00 p.m for the special ""Green Team Club"" activities. Each week we will be recycling different items. This week bring your empty pop bottles and cans to the club. The public is welcome to donate their empty cans and bottles too. Please drop them off at the library during the Town or Library hours. Thanks for your support.
All Junior High students who register for hte Summer Reading Program will be eligible for a daily draw for a free Slurpee from the Blue Goose Gas Bar and a weekly draw for a free video from The Store. We thank these businesses for their support.
New books at the library: ""The Firm"" by John Grisham, ""Legacy of Secrets"" by Elizabeth Adler, ""Cauldron"" by Larry Bond, ""Virgins in Paradise"" by Barbara Wook, ""Missing Joseph"" by Elizabeth George, and ""A Woman's Choice"" by Marianne Williamson.
For the youth: ""Maxine's Tree"", ""The Legend of Indian Paintbrush"", The Ladybird Green Book"", The Cry of the Crow"", ""Seal Child"", and King Emmett the Second"". These books were purchased by the Chinook System with a grant provided by the Alberta Federation
for the Arts to promote the Summer Reading Theme ""Book to the Future"".
*******
The Public Library is again sponsoring an exciting SUMMER READING PROGRAM - ""Book To The Future"" for all children who have completed Grades 1-6. Come to the library every day for fun, games, activities, and prizes, too. Registration has started and will continue all summer until August 26th. Even if you are out of town a lot, you can get your name in the draw when you come in. Drop in for fun and surprises each week before going to Public Swim. Attention all Junior High readers - we are having a separate contest for you this year with prizes and fun. Summer Reading Programs prevent the loss of reading and vocabulary skills that commonly occurs among children over the summer vacation. They are also fun and challenging activities which students will enjoy. We hope that parents will encourage their children to visit the library.
ATTENTION JR. HIGH KIDS:
Plan to create an ""Alien"" for display and judging during our summer reading program (androids, mutants, and robots also welcome). Make them any size up to foot high, and use any items and material you want. Bring them to us any time during regular library hours. Enter our weekly prize draw and watch for other projects. All entrants must have a library card.Natural and Herbal Cosmetics
r ■■ ■- ■■■>■■■ ■ . .. ■ ■■ 1 ■—■■ ■■■■■■ ------------------------;
»PRODUCE SPECIALS JULY 12 - 17»
Bing Cherries .99 lb , $2.18 kg
Nectarines $1.59 lb $3.51
Navel Oranges .79 lb $1.74 kg
Green Leaf Lettuce .69 each
New White Potatoes 5 lb bag $1.79 each
Celery 3 lbs ! .99 .73 kg
nqevr Money continues to pour in for
our new Library/Museum. We
have received donations from:
* James & Shelby Anderson
* Pat Chipman
* Herb & Dianna McKelvey
* Dan & Alma Davies
* Dale Davies
* Norma & Gaelynd Pilling
* George & Joan Taylor
* William Wocknitz
* Vivian Avery
* Lorelle Gurney
* Roger & Sybil Ferguson in
honor of Jill & Blaine Harker
Donations have been
made to the Library Building
Fund in memory of Mae
Dudley by Cecelia Foote,
Marilyn & Blaine Neilson,
Lucille & DeVere Dudley, and
Marjorie & Eldon Coleman.
Anonymous donations have
been made in memory of
Lydia Anderson. The amount
from donations is now $1,940.
*******
MAGRATH STAKE FAMILY
HISTORY CENTRE
Hours:
Tues. & Thurs. 1 - 9:00 p.m.
Wed. 9:00 a.m. - 9:00 p.m.
Sunday 3 - 6:00 p.m.
Director: Betty Clifton 758-
6772
WHY IS NOEVIR UNIQUE?
□□□□
□
□
□□
□
□□□□
□
Natural to the skin
Ingredients are in full compliance with FDA Regulations
Manufacture their own products (no middle man)
Natural Bases (They come closest to the moisturizers our bodies
produce naturally.)
• Stearic Acid and HS Lipids from Soy Beans of China
• Squalene from Shark Liver Oil of Australia
• Beeswax from Honeycombs of Africa
• Jojoba Oil from Jojoba Plants of Arizona
9-24 herbs and herbal extracts from the Black Forest of West
Germany and France
No mineral oil (Most skincare companies use mineral oil as the main
base ingredient — not found in human sebum and
not natural to our skin.)
No artificial colors or fragrances
No artificial preservatives
No chemical binders or fillers
No drying alcohol
No animal testing
No human placenta
003 Line - made without animal ingredients or testing and packaged
in recyclable containers
100% money back guarantee within 30 days
A PHENOMENON IN SKINCARE!
HEATHER THOMSON
758-6386
0 “ 1993 MAGRATH CELEBRATIONS
M s ’ 94 YEARS YOUNG! $
.^WHAPPINESS IS HOMESPUN"" °
V"" U/ » * . , 0
I IX THURSDAY. JULY 22 -
¿6:00 p.m
xk»« « -, y
19-
» 4
Softball Tournament &
o Contact: Bill Alston a > L?
jU” ° & 1 V' a FRIDAY, JULY 23 n
......Bake Sale at the Arena ’
Contact: Lorraine Balderson
......Softball Tournament p
......Community Dance - Tom
Karren Gym - Pyramid o /
Entertainment a Ar I
Contact: Tyler Mandin F*
Cost: $2 /person or $10 /family \
or a donation to the Library Fund.&
° /
SATURDAY. JULY 24 V* /
........... Pancake Breakfast - Lions I
7 ! Club. Contact: John Bourne °
a.m...’..........PARADE!
fe!
a.m
□ \
7:00 a.m..
o D
11:00 m fl T
Meet in front of the School at 10:00
for Line-up and Judging.
Childrens entries meet at
Grusendorfs corner. a
n
o PARADE MARSHALL - DON JOHNSON 1
a a
12 noon Luncheon at Ice Arena
p. o Magrath 4th Scouts v *
1:00 p.m................Children's Races at the Track
a o Field. Contact: Dave Clark
1:00 p.m............... Free Swim at the Pool |
2:00 p.m...............Softball Tournament 0 <
3:00 p.m...............Lions Club Bottle Race at 1
JL, * Canal by Allan Owens, p /
a Contact: Norm Robinson ( p
FEtf. f ■£$>- a P x.
OaFTER THE SOFTBALL TOURNAMENT
Program and free Beef on a Bun at the School
Auditorium - Co-ordinators: Shirley Perry, 0
Marilyn Henderson, and Marie Stevenson.
p a e
PARADE ROUTE: Meet aCthe school, go
NORTH to the Hospital, WEST to Highway 62, °
j ° o
r then SOUTH to the Trading Company W
corn er, then EAST to the LDS Church, I
loop back t","Magrath Store News (July 14, 1993)",,J. A. Ririe,,,core
232677543,1995-09-01T07:00:00,"A formal theory for the development of a generic model of an autonomous sensor is proposed and implemented. An autonomous sensor is defined as an intelligent sensor that has machine learning capabilities. It not only interprets the acquired data in accordance with an embedded expert system knowledge base, but is also capable of using this data to modify and enhance this knowledge base. Hence, the system is capable of learning and thereby improving its performance over time. The main objective of the model is to combine the capabilities of the physical sensor and an expert operator monitoring the sensor in real-time. The system has been successfully tested using various simulated data sets as well as a real thermistor that has been developed as an autonomous sensor. This work has significant impact on modem production systems since sensors form an integral part of all closed loop control systems, and modem manufacturing processes rely heavily on sensor based control systems. The long range aim of this work is to develop highly autonomous production systems that have self diagnostic, maintenance, self correction, and learning capabilities embedded at the local and global levels. This work builds upon work on a formalized theory for autonomous sensing called Dynamic Across Time Autonomous-Sensing, Interpretation, Model learning and Maintenance Theory (DATA-SEIALAMT) that has been supported by the NSF and the SME Education Foundation","Dynamic Across Time Autonomous — Sensing, Interpretation, Model Learning and Maintenance Theory (DATA -SIMLAMT)",,'Elsevier BV',10.1016/0957-4158(95)E0003-M,,core
42781573,"May 11, 1994","The research mission is the development of computer assisted diagnostic (CAD) methods for improved diagnosis of medical images including digital x-ray sensors and tomographic imaging modalities. The CAD algorithms include advanced methods for adaptive nonlinear filters for image noise suppression, hybrid wavelet methods for feature segmentation and enhancement, and high convergence neural networks for feature detection and VLSI implementation of neural networks for real time analysis. Other missions include (1) implementation of CAD methods on hospital based picture archiving computer systems (PACS) and information networks for central and remote diagnosis and (2) collaboration with defense and medical industry, NASA, and federal laboratories in the area of dual use technology conversion from defense or aerospace to medicine",Neural networks: Application to medical imaging,https://core.ac.uk/download/pdf/42781573.pdf,,,,core
56101958,1992-01-01T00:00:00,"The need to improve quality and decrease scrap rate while increasing the production rate is motivating industry to consider untended machining as viable alternative. On-line monitoring of a machining process is the key component to success for an untended machining operation. In this chapter, a framework for sensorbased intelligent decision-making systems to perform on- line monitoring is proposed. Such a monitoring system interprets the detected signals from the sensors, extracts the relevant information, and decides on the appropriate control action. Emphasis is laid on applying neural networks to perform information processing, and to recognize the process abnormalities in a machining operation. A prototype monitoring system is implemented to demonstrate the working mechanism. For successful implementation of the developed intelligent monitor, an instrumented force transducer is designed for signal detection and is used in a real time turning operation. A neural network monitor based on feedforward back-propagation algorithm is developed and tested under the machining of advanced ceramic materials and steel. The monitor is trained by the detected cutting force signal, the measured surface finish, and the observed tool wear. The superior learning and noise suppression abilities of the developed monitor enable high success rates for monitoring in machining process",Neural Network Applications in On-line Monitoring of Turning Processes,,,,,core
77012861,1991-11-25T00:00:00,"ARCHON is an ongoing ESPRIT II project (P-2256) which is approximately half way through its five year duration. It is concerned with defining and applying techniques from the area of Distributed Artificial Intelligence to the development of real-size industrial applications. Such techniques enable multiple problem solvers (e.g. expert systems, databases and conventional numerical software systems) to communicate and cooperate with each other to improve both their individual problem solving behavior and the behavior of the community as a whole. This paper outlines the niche of ARCHON in the Distributed AI world and provides an overview of the philosophy and architecture of our approach the essence of which is to be both general (applicable to the domain of industrial process control) and powerful enough to handle real-world problems",Cooperation in Industrial Systems,,,,,core
477902989,,"The automotive industry aims to deploy commercial level-5 fully autonomous self-driving vehicles (FA-SDVs) in a diverse range of benefit-driven concepts on city roads in the years to come. In all future visions of operating networks of FA-SDVs, humans are expected to intervene with some kind of remote supervisory role. Recent advances in cyber-physical systems (CPS) within the concept of Internet of Everything (IoE) using tactile internet (TI) teleport us to teleoperate remote objects within the cyber-world. Human-on-the-loop (HOTL) haptic teleoperation with an extension of human control and sensing capability by coupling with artificial sensors and actuators with an increased sense of real-time driving in the remote vehicle can help overcome the challenging tasks when the new driver - artificial intelligence (AI) agent - encounters an unorthodox situation that can't be addressed by the autonomous capabilities. This paper analyses HOTL real-time haptic delay-sensitive teleoperation with FA-SDVs, in the aspects of human-vehicle teamwork by establishing two similar remote parallel worlds --- real-world vehicle time-varying environment and cyber-world emulation of this environment, i.e., digital twins (DTs) --- in which a human telesupervisor (HTS), as a biological agent, can be immersed within a reasonable timescale with no cybersickness enabling omnipresence and a bidirectional flow of energy and information. The experiments conducted as a proof of concept of HOTL haptic teleoperation shows promising results and the potential of benefiting from the proposed framework",Conceptualisation of human-on-the-loop haptic teleoperation with fully autonomous self-driving vehicles in the urban environment,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
42827675,"Apr 1, 1989","An overview is given of a design optimization project that is in progress at the GE Research and Development Center for the past few years. The objective of this project is to develop a methodology and a software system for design automation and optimization of structural/mechanical components and systems. The effort focuses on research and development issues and also on optimization applications that can be related to real-life industrial design problems. The overall technical approach is based on integration of numerical optimization techniques, finite element methods, CAE and software engineering, and artificial intelligence/expert systems (AI/ES) concepts. The role of each of these engineering technologies in the development of a unified design methodology is illustrated. A software system DESIGN-OPT has been developed for both size and shape optimization of structural components subjected to static as well as dynamic loadings. By integrating this software with an automatic mesh generator, a geometric modeler and an attribute specification computer code, a software module SHAPE-OPT has been developed for shape optimization. Details of these software packages together with their applications to some 2- and 3-dimensional design problems are described",Integrated design optimization research and development in an industrial environment,https://core.ac.uk/download/pdf/42827675.pdf,,,,core
42814707,"Sep 25, 1991","In fault diagnosis, control and real-time monitoring, both timing and accuracy are critical for operators or machines to reach proper solutions or appropriate actions. Expert systems are becoming more popular in the manufacturing community for dealing with such problems. In recent years, neural networks have revived and their applications have spread to many areas of science and engineering. A method of using neural networks to implement rule-based expert systems for time-critical applications is discussed here. This method can convert a given rule-based system into a neural network with fixed weights and thresholds. The rules governing the translation are presented along with some examples. We also present the results of automated machine implementation of such networks from the given rule-base. This significantly simplifies the translation process to neural network expert systems from conventional rule-based systems. Results comparing the performance of the proposed approach based on neural networks vs. the classical approach are given. The possibility of very large scale integration (VLSI) realization of such neural network expert systems is also discussed",Automated implementation of rule-based expert systems with neural networks for time-critical applications,https://core.ac.uk/download/pdf/42814707.pdf,,,,core
341985691,,,"Volume 2, Issue 3, Special issue on Recent Advances in Engineering Systems (Published Papers) Articles Transmit / Received Beamforming for Frequency Diverse Array with Symmetrical frequency offsets  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 1-6 (2017); View Description Detailed Analysis of Amplitude and Slope Diffraction Coefficients for knife-edge structure in S-UTD-CH Model  Eray Arik, Mehmet Baris Tabakcioglu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 7-11 (2017); View Description Applications of Case Based Organizational Memory Supported by the PAbMM Architecture  Martín, María de los Ángeles, Diván, Mario José  Adv. Sci. Technol. Eng. Syst. J. 2(3), 12-23 (2017); View Description Low Probability of Interception Beampattern Using Frequency Diverse Array Antenna  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 24-29 (2017); View Description Zero Trust Cloud Networks using Transport Access Control and High Availability Optical Bypass Switching  Casimer DeCusatis, Piradon Liengtiraphan, Anthony Sager  Adv. Sci. Technol. Eng. Syst. J. 2(3), 30-35 (2017); View Description A Derived Metrics as a Measurement to Support Efficient Requirements Analysis and Release Management  Indranil Nath  Adv. Sci. Technol. Eng. Syst. J. 2(3), 36-40 (2017); View Description Feedback device of temperature sensation for a myoelectric prosthetic hand  Yuki Ueda, Chiharu Ishii  Adv. Sci. Technol. Eng. Syst. J. 2(3), 41-40 (2017); View Description Deep venous thrombus characterization: ultrasonography, elastography and scattering operator  Thibaud Berthomier, Ali Mansour, Luc Bressollette, Frédéric Le Roy, Dominique Mottier  Adv. Sci. Technol. Eng. Syst. J. 2(3), 48-59 (2017); View Description Improving customs’ border control by creating a reference database of cargo inspection X-ray images  Selina Kolokytha, Alexander Flisch, Thomas Lüthi, Mathieu Plamondon, Adrian Schwaninger, Wicher Vasser, Diana Hardmeier, Marius Costin, Caroline Vienne, Frank Sukowski, Ulf Hassler, Irène Dorion, Najib Gadi, Serge Maitrejean, Abraham Marciano, Andrea Canonica, Eric Rochat, Ger Koomen, Micha Slegt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 60-66 (2017); View Description Aviation Navigation with Use of Polarimetric Technologies  Arsen Klochan, Ali Al-Ammouri, Viktor Romanenko, Vladimir Tronko  Adv. Sci. Technol. Eng. Syst. J. 2(3), 67-72 (2017); View Description Optimization of Multi-standard Transmitter Architecture Using Single-Double Conversion Technique Used for Rescue Operations  Riadh Essaadali, Said Aliouane, Chokri Jebali and Ammar Kouki  Adv. Sci. Technol. Eng. Syst. J. 2(3), 73-81 (2017); View Description Singular Integral Equations in Electromagnetic Waves Reflection Modeling  A. S. Ilinskiy, T. N. Galishnikova  Adv. Sci. Technol. Eng. Syst. J. 2(3), 82-87 (2017); View Description Methodology for Management of Information Security in Industrial Control Systems: A Proof of Concept aligned with Enterprise Objectives.  Fabian Bustamante, Walter Fuertes, Paul Diaz, Theofilos Toulqueridis  Adv. Sci. Technol. Eng. Syst. J. 2(3), 88-99 (2017); View Description Dependence-Based Segmentation Approach for Detecting Morpheme Boundaries  Ahmed Khorsi, Abeer Alsheddi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 100-110 (2017); View Description Paper Improving Rule Based Stemmers to Solve Some Special Cases of Arabic Language  Soufiane Farrah, Hanane El Manssouri, Ziyati Elhoussaine, Mohamed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 111-115 (2017); View Description Medical imbalanced data classification  Sara Belarouci, Mohammed Amine Chikh  Adv. Sci. Technol. Eng. Syst. J. 2(3), 116-124 (2017); View Description ADOxx Modelling Method Conceptualization Environment  Nesat Efendioglu, Robert Woitsch, Wilfrid Utz, Damiano Falcioni  Adv. Sci. Technol. Eng. Syst. J. 2(3), 125-136 (2017); View Description GPSR+Predict: An Enhancement for GPSR to Make Smart Routing Decision by Anticipating Movement of Vehicles in VANETs  Zineb Squalli Houssaini, Imane Zaimi, Mohammed Oumsis, Saïd El Alaoui Ouatik  Adv. Sci. Technol. Eng. Syst. J. 2(3), 137-146 (2017); View Description Optimal Synthesis of Universal Space Vector Digital Algorithm for Matrix Converters  Adrian Popovici, Mircea Băbăiţă, Petru Papazian  Adv. Sci. Technol. Eng. Syst. J. 2(3), 147-152 (2017); View Description Control design for axial flux permanent magnet synchronous motor which operates above the nominal speed  Xuan Minh Tran, Nhu Hien Nguyen, Quoc Tuan Duong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 153-159 (2017); View Description A synchronizing second order sliding mode control applied to decentralized time delayed multi−agent robotic systems: Stability Proof  Marwa Fathallah, Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 160-170 (2017); View Description Fault Diagnosis and Tolerant Control Using Observer Banks Applied to Continuous Stirred Tank Reactor  Martin F. Pico, Eduardo J. Adam  Adv. Sci. Technol. Eng. Syst. J. 2(3), 171-181 (2017); View Description Development and Validation of a Heat Pump System Model Using Artificial Neural Network  Nabil Nassif, Jordan Gooden  Adv. Sci. Technol. Eng. Syst. J. 2(3), 182-185 (2017); View Description Assessment of the usefulness and appeal of stigma-stop by psychology students: a serious game designed to reduce the stigma of mental illness  Adolfo J. Cangas, Noelia Navarro, Juan J. Ojeda, Diego Cangas, Jose A. Piedra, José Gallego  Adv. Sci. Technol. Eng. Syst. J. 2(3), 186-190 (2017); View Description Kinect-Based Moving Human Tracking System with Obstacle Avoidance  Abdel Mehsen Ahmad, Zouhair Bazzal, Hiba Al Youssef  Adv. Sci. Technol. Eng. Syst. J. 2(3), 191-197 (2017); View Description A security approach based on honeypots: Protecting Online Social network from malicious profiles  Fatna Elmendili, Nisrine Maqran, Younes El Bouzekri El Idrissi, Habiba Chaoui  Adv. Sci. Technol. Eng. Syst. J. 2(3), 198-204 (2017); View Description Pulse Generator for Ultrasonic Piezoelectric Transducer Arrays Based on a Programmable System-on-Chip (PSoC)  Pedro Acevedo, Martín Fuentes, Joel Durán, Mónica Vázquez, Carlos Díaz  Adv. Sci. Technol. Eng. Syst. J. 2(3), 205-209 (2017); View Description Enabling Toy Vehicles Interaction With Visible Light Communication (VLC)  M. A. Ilyas, M. B. Othman, S. M. Shah, Mas Fawzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 210-216 (2017); View Description Analysis of Fractional-Order 2xn RLC Networks by Transmission Matrices  Mahmut Ün, Manolya Ün  Adv. Sci. Technol. Eng. Syst. J. 2(3), 217-220 (2017); View Description Fire extinguishing system in large underground garages  Ivan Antonov, Rositsa Velichkova, Svetlin Antonov, Kamen Grozdanov, Milka Uzunova, Ikram El Abbassi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 221-226 (2017); View Description Directional Antenna Modulation Technique using A Two-Element Frequency Diverse Array  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 227-232 (2017); View Description Classifying region of interests from mammograms with breast cancer into BIRADS using Artificial Neural Networks  Estefanía D. Avalos-Rivera, Alberto de J. Pastrana-Palma  Adv. Sci. Technol. Eng. Syst. J. 2(3), 233-240 (2017); View Description Magnetically Levitated and Guided Systems  Florian Puci, Miroslav Husak  Adv. Sci. Technol. Eng. Syst. J. 2(3), 241-244 (2017); View Description Energy-Efficient Mobile Sensing in Distributed Multi-Agent Sensor Networks  Minh T. Nguyen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 245-253 (2017); View Description Validity and efficiency of conformal anomaly detection on big distributed data  Ilia Nouretdinov  Adv. Sci. Technol. Eng. Syst. J. 2(3), 254-267 (2017); View Description S-Parameters Optimization in both Segmented and Unsegmented Insulated TSV upto 40GHz Frequency  Juma Mary Atieno, Xuliang Zhang, HE Song Bai  Adv. Sci. Technol. Eng. Syst. J. 2(3), 268-276 (2017); View Description Synthesis of Important Design Criteria for Future Vehicle Electric System  Lisa Braun, Eric Sax  Adv. Sci. Technol. Eng. Syst. J. 2(3), 277-283 (2017); View Description Gestural Interaction for Virtual Reality Environments through Data Gloves  G. Rodriguez, N. Jofre, Y. Alvarado, J. Fernández, R. Guerrero  Adv. Sci. Technol. Eng. Syst. J. 2(3), 284-290 (2017); View Description Solving the Capacitated Network Design Problem in Two Steps  Meriem Khelifi, Mohand Yazid Saidi, Saadi Boudjit  Adv. Sci. Technol. Eng. Syst. J. 2(3), 291-301 (2017); View Description A Computationally Intelligent Approach to the Detection of Wormhole Attacks in Wireless Sensor Networks  Mohammad Nurul Afsar Shaon, Ken Ferens  Adv. Sci. Technol. Eng. Syst. J. 2(3), 302-320 (2017); View Description Real Time Advanced Clustering System  Giuseppe Spampinato, Arcangelo Ranieri Bruna, Salvatore Curti, Viviana D’Alto  Adv. Sci. Technol. Eng. Syst. J. 2(3), 321-326 (2017); View Description Indoor Mobile Robot Navigation in Unknown Environment Using Fuzzy Logic Based Behaviors  Khalid Al-Mutib, Foudil Abdessemed  Adv. Sci. Technol. Eng. Syst. J. 2(3), 327-337 (2017); View Description Validity of Mind Monitoring System as a Mental Health Indicator using Voice  Naoki Hagiwara, Yasuhiro Omiya, Shuji Shinohara, Mitsuteru Nakamura, Masakazu Higuchi, Shunji Mitsuyoshi, Hideo Yasunaga, Shinichi Tokuno  Adv. Sci. Technol. Eng. Syst. J. 2(3), 338-344 (2017); View Description The Model of Adaptive Learning Objects for virtual environments instanced by the competencies  Carlos Guevara, Jose Aguilar, Alexandra González-Eras  Adv. Sci. Technol. Eng. Syst. J. 2(3), 345-355 (2017); View Description An Overview of Traceability: Towards a general multi-domain model  Kamal Souali, Othmane Rahmaoui, Mohammed Ouzzif  Adv. Sci. Technol. Eng. Syst. J. 2(3), 356-361 (2017); View Description L-Band SiGe HBT Active Differential Equalizers with Variable, Positive or Negative Gain Slopes Using Dual-Resonant RLC Circuits  Yasushi Itoh, Hiroaki Takagi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 362-368 (2017); View Description Moving Towards Reliability-Centred Management of Energy, Power and Transportation Assets  Kang Seng Seow, Loc K. Nguyen, Kelvin Tan, Kees-Jan Van Oeveren  Adv. Sci. Technol. Eng. Syst. J. 2(3), 369-375 (2017); View Description Secure Path Selection under Random Fading  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 376-383 (2017); View Description Security in SWIPT with Power Splitting Eavesdropper  Furqan Jameel, Faisal, M Asif Ali Haider, Amir Aziz Butt  Adv. Sci. Technol. Eng. Syst. J. 2(3), 384-388 (2017); View Description Performance Analysis of Phased Array and Frequency Diverse Array Radar Ambiguity Functions  Shaddrack Yaw Nusenu  Adv. Sci. Technol. Eng. Syst. J. 2(3), 389-394 (2017); View Description Adaptive Discrete-time Fuzzy Sliding Mode Control For a Class of Chaotic Systems  Hanene Medhaffar, Moez Feki, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 395-400 (2017); View Description Fault Tolerant Inverter Topology for the Sustainable Drive of an Electrical Helicopter  Igor Bolvashenkov, Jörg Kammermann, Taha Lahlou, Hans-Georg Herzog  Adv. Sci. Technol. Eng. Syst. J. 2(3), 401-411 (2017); View Description Computational Intelligence Methods for Identifying Voltage Sag in Smart Grid  Turgay Yalcin, Muammer Ozdemir  Adv. Sci. Technol. Eng. Syst. J. 2(3), 412-419 (2017); View Description A Highly-Secured Arithmetic Hiding cum Look-Up Table (AHLUT) based S-Box for AES-128 Implementation  Ali Akbar Pammu, Kwen-Siong Chong, Bah-Hwee Gwee  Adv. Sci. Technol. Eng. Syst. J. 2(3), 420-426 (2017); View Description Service Productivity and Complexity in Medical Rescue Services  Markus Harlacher, Andreas Petz, Philipp Przybysz, Olivia Chaillié, Susanne Mütze-Niewöhner  Adv. Sci. Technol. Eng. Syst. J. 2(3), 427-434 (2017); View Description Principal Component Analysis Application on Flavonoids Characterization  Che Hafizah Che Noh, Nor Fadhillah Mohamed Azmin, Azura Amid  Adv. Sci. Technol. Eng. Syst. J. 2(3), 435-440 (2017); View Description A Reconfigurable Metal-Plasma Yagi-Yuda Antenna for Microwave Applications  Giulia Mansutti, Davide Melazzi, Antonio-Daniele Capobianco  Adv. Sci. Technol. Eng. Syst. J. 2(3), 441-448 (2017); View Description Verifying the Detection Results of Impersonation Attacks in Service Clouds  Sarra Alqahtani, Rose Gamble  Adv. Sci. Technol. Eng. Syst. J. 2(3), 449-459 (2017); View Description Image Segmentation Using Fuzzy Inference System on YCbCr Color Model  Alvaro Anzueto-Rios, Jose Antonio Moreno-Cadenas, Felipe Gómez-Castañeda, Sergio Garduza-Gonzalez  Adv. Sci. Technol. Eng. Syst. J. 2(3), 460-468 (2017); View Description Segmented and Detailed Visualization of Anatomical Structures based on Augmented Reality for Health Education and Knowledge Discovery  Isabel Cristina Siqueira da Silva, Gerson Klein, Denise Munchen Brandão  Adv. Sci. Technol. Eng. Syst. J. 2(3), 469-478 (2017); View Description Intrusion detection in cloud computing based attack patterns and risk assessment  Ben Charhi Youssef, Mannane Nada, Bendriss Elmehdi, Regragui Boubker  Adv. Sci. Technol. Eng. Syst. J. 2(3), 479-484 (2017); View Description Optimal Sizing and Control Strategy of renewable hybrid systems PV-Diesel Generator-Battery: application to the case of Djanet city of Algeria  Adel Yahiaoui, Khelifa Benmansour, Mohamed Tadjine  Adv. Sci. Technol. Eng. Syst. J. 2(3), 485-491 (2017); View Description RFID Antenna Near-field Characterization Using a New 3D Magnetic Field Probe  Kassem Jomaa, Fabien Ndagijimana, Hussam Ayad, Majida Fadlallah, Jalal Jomaah  Adv. Sci. Technol. Eng. Syst. J. 2(3), 492-497 (2017); View Description Design, Fabrication and Testing of a Dual-Range XY Micro-Motion Stage Driven by Voice Coil Actuators  Xavier Herpe, Matthew Dunnigan, Xianwen Kong  Adv. Sci. Technol. Eng. Syst. J. 2(3), 498-504 (2017); View Description Self-Organizing Map based Feature Learning in Bio-Signal Processing  Marwa Farouk Ibrahim Ibrahim, Adel Ali Al-Jumaily  Adv. Sci. Technol. Eng. Syst. J. 2(3), 505-512 (2017); View Description A delay-dependent distributed SMC for stabilization of a networked robotic system exposed to external disturbances  Fatma Abdelhedi, Nabil Derbel  Adv. Sci. Technol. Eng. Syst. J. 2(3), 513-519 (2017); View Description Modelization of cognition, activity and motivation as indicators for Interactive Learning Environment  Asmaa Darouich, Faddoul Khoukhi, Khadija Douzi  Adv. Sci. Technol. Eng. Syst. J. 2(3), 520-531 (2017); View Description Homemade array of surface coils implementation for small animal magnetic resonance imaging  Fernando Yepes-Calderon, Olivier Beuf  Adv. Sci. Technol. Eng. Syst. J. 2(3), 532-539 (2017); View Description An Encryption Key for Secure Authentication: The Dynamic Solution  Zubayr Khalid, Pritam Paul, Khabbab Zakaria, Himadri Nath Saha  Adv. Sci. Technol. Eng. Syst. J. 2(3), 540-544 (2017); View Description Multi-Domain Virtual Network Embedding with Coordinated Link Mapping  Shuopeng Li, Mohand Yazid Saidi, Ken Chen  Adv. Sci. Technol. Eng. Syst. J. 2(3), 545-552 (2017); View Description Semantic-less Breach Detection of Polymorphic Malware in Federated Cloud",,'ASTES Journal',10.25046/aj020371,,core
229089491,1994-01-01T08:00:00,"As research fields in AI accelerate and a high number of experts are demanded by industry, Expert Systems play an important role in meeting the technological sophistication required in today’s competitive world. Industries are demanding the assistance of human experts to solve complicated problems. However, there is a shortage of experts due to this demand. Expert Systems are rapidly becoming one of the major approaches to solve engineering and manufacturing problems. They have been implemented for several practical applications in many decision making problems. Expert Systems are helping major companies to diagnose processes in real time, schedule operations, maintain machinery and design services and production facilities.
The area of robotics is a fertile one for application of Expert Systems. Robots are an integral part of today’s manufacturing environment. New tasks are being defined for robots in order to meet the challenges of Flexible Manufacturing Systems. Robots are entering every fact of manufacturing. Along with this grown there is an increasing variety of robots to choose from. One of the major problems facing the potential robot user will be his/her choice of an optimum robot for a particular task. Various parameters should be considered and the user should choose an industrial robot whose characteristics satisfy the requirements of the intended task.
This work will present a viable solution to the problem of selecting an optimum robot by building a Knowledge-Based Expert System using the LEVEL5 shell. LEVEL5 is an Expert System software created by Information Builders Inc., which runs on the IBM Personal Computer, XT and AT. The system asks the user several questions regarding the usage and requirements of the desired robot. It uses the knowledge base and the rules to determine an optimum robot --Abstract, page iii",Application of knowledge-based expert systems for selection of industrial robots,,Scholars\u27 Mine,,,core
37330224,1995-01-01T00:00:00,"The rapid development of computer vision, computational resources, artificial intelligence and the integration of these technologies are creating new possibilities in the design and implementation of commercial machine vision systems. In minerals engineering numerous opportunities for the application of these systems exist, such as the characterization of flotation froth structures which is discussed in this paper by way of example. A general model for the development of feasible, real-time machine vision systems is proposed, which is based on an analogy with biological visual perception made possible by a connectionist approach and the ability of neural networks to solve ill-posed problems. It is shown that both supervised and unsupervised neural nets can be used in different ways to analyze froth structures of industrial flotation cells. Unsupervised (self-organizing) neural nets can monitor process behaviour on a continuous rather than on a discrete basis, which makes the early detection of erratic process control possible. Since some losses in information are incurred with the use of self-organizing systems, intelligent monitoring and control systems would in practice probably be comprised of both types of neural nets. © 1994.Articl",The monitoring of froth surfaces on industrial flotation plants using connectionist image processing techniques,,,10.1016/0892-6875(94)00099-x,,core
1540536,,"There is a trend in manufacturing towards fully automated production facilities in which all operations are integrated by computer based information systems. The current generation of industrial inspection systems lack the necessary flexibility to operate in these environments. AI based Image Understanding systems have the necessary level of generality, achieved through the use of domain specific object models. These models are used to guide early visual processing, and must be supplied to the system.



Current theories in cognitive psychology call for a reevaluation of the role of such 'auxiliary' knowledge in early visual processing. Recent work suggests that very general cognitive processes may build up a hierarchical representation of the world. The emphasis is currently on such generic cognitive processes rather than on the use of world knowledge.



A novel approach to image processing, in which emphasis is placed on generic low and intermediate level techniques, is proposed in this thesis. This approach, termed the descriptor approach, delays the use of domain specific models until a full description of the image has been produced. 



A prototype industrial inspection system has been implemented, based on the descriptor approach: the Hierarchical Scene Description (HISD) system. General image features are extracted from images of populated PCBs, and subsequently transformed into a database of prolog facts by an interface subsystem. Finally the intermediate level vision subsystem uses rules to reason about these features, building up a semantic net based description of the scene.



HISD successfully builds up hierarchical descriptions of real industrial PCB images in terms of geometric shapes, their coordinates, and spatial relationships between shapes. The results are displayed graphically and are achieved without the use of any object models, thus avoiding the problems of inflexibility and lack of generality associated with more complex model based systems",An intermediate level industrial vision system,,Kingston University,,,core
6285086,,"This year the 4th WSEAS International Conference on COMPUTER ENGINEERING and APPLICATIONS (CEA '10) was held at Harvard University, Cambridge, USA, January 27-29, 2010. The conference remains faithful to its original idea of providing a platform to discuss network architecture, network design software, mobile networks and mobile services, digital broadcasting, e-commerce, optical networks, hacking, trojian horses, viruses, worms, spam, information security, standards of information security: aes, ipsec, high-tech crime prevention, real-time operating systems, hardware engineering, supercomputing, artificial intelligence, microprocessors, microcomputers, antennas and radars, lightwave technology, numerical methods for electromagnetics, aerospace systems, atm networks, military communications, cyber-science and cyber-space, mathematical logic and computers, image, video and internet technologies, web-based education, law aspects related to informatics etc. with participants from all over the world, both from academia and from industry.cyber-science and cyber-space; mathematical logic and computers; image; video and internet technologies; web-based education",Recent Advances in Computer Engineering and Applications,,,,,core
207901081,1995-06-30,"AbstractOne reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparative experiments in the real-world domain of electricity transportation management. Finally, the success of the approach of building a system with an explicit and grounded representation of cooperative problem solving is used to outline a proposal for the next generation of multi-agent systems",Controlling cooperative problem solving in industrial multi-agent systems using joint intentions ,,Published by Elsevier B.V.,10.1016/0004-3702(94)00020-2,,core
22384258,1995,"This paper first overviews a real-world AI system, developed over the past several years, that addresses the knowledge barriers that impede software reuse. The paper then describes our version of the `industry as laboratory&apos; research methodology followed in this project. It should be noted that we have not attempted to directly address `programming-in-the-large&apos;, but rather have concentrated on developing a completely automated solution to a significant real-world software engineering problem facing individual programmers. The effectiveness of our solution depends upon exploiting the power of domain-oriented specification languages. In that context, `scaling-up&apos; means facilitating the use of our system across domains. The last section of this paper addresses our research toward that goal",Automating Software Reuse,,,,,core
237735590,,"This thesis is split into three parts. 



Part I: An Automatic Self-optimising Continuous-flow Reactor for Electrosynthesis



Assembly-line and/or continuous, steady-state strategies are widely spreading in the manufacturing industry. Electrosynthesis in an automatic continuous flow manner is drawing more and more attention to both pharmaceutical industry and research. This part describes the develop and use of an automated self-optimising continuous-flow electrochemistry reactor system. The main focus of this work is to develop an bi-language (MATLAB and LabVIEW), server-client structure automation software framework to conduct automation, control and monitoring of flow electrochemistry processes, which enables quick system setup, reconfiguration and high flexibility. Stable noisy optimisation by branch and fit (SNOBFIT) and simplex algorithm (modified simplex and super-modified simplex method) were developed and tested on simulators, which were then applied to the synthesis action of methoxylation of N-formylpyrrolidine and electro-oxidation of 3-bromobenzyl alcohol. Searching of the optimum operation condition of a reaction in an automatic manner is a major step forward to establish convenient and straightforward use of organic electrosynthesis in routine laboratory synthesis or industrial applications.



Part II: Applying FTIR Imaging to Address Challenges in Plastic Recycling



Plastic pollution is ubiquitous throughout the earth, and reusing/recycling of plastic has the potential to reduce the global abundance and weight of waste plastics. The main focus of this work is investigating plastic sample using Fourier transform infrared spectroscopy (FTIR), single point and imaging, for recycling purpose. An quantitative calibration of talcum concentration in talcum reinforced virgin polypropylene sample with IR peak ratio/integration was conducted, and the application of the result to analyse the talcum disperse in polypropylene matrix was reported. Micron scale FTIR imaging was conducted on the film sample. Pseudo-colour image visualising the distribution of talcum in the polypropylene matrix indicated a highly uneven distribution, a result of the reprocessing method. 



FTIR imaging was applied to investigate the composite structures of 'real-world' composites sample for recycled industry plastics, including: virgin polypropylene with short milled recycled carbon fibre, virgin polypropylene, maleic anhydride grafted polypropylene with carbon fibre, acrylonitrile butadiene styrene with calcium carbonate and virgin polypropylene with poly(ethylene terephthalate) on the micron scale. Imaging technique in FTIR spectroscopy not only provide micron level spatial information of the composition but also direct solution of improving the inter-facial interactions between compositions, thus improving the physical/chemical performance of the plastic products. Those pilot studies provide insights into applying FTIR imaging for plastic sample analysis.



Part III: FTIR spectroscopy for Breast Cancer Prognosis



Breast cancer is a major cause of deaths for females worldwide. Cancer prognosis provides a patient's likely outcome based on their current standing, which can help to decide the treatment for the patient. The current golden standard prognosis index, Nottingham Prognosis Index, is a time-consuming, un-objective process to which limited confidence can be assigned because of inherent operator variability. Applying Fourier transform infrared spectroscopy (FTIR) imaging to breast cancer tissue offers a non-destructive, label free tool for cellular and extracellular breast cancer tissue studying. In this work, we evaluate the prognostic ability of FTIR spectroscopy for identifying different grades of breast cancer. Different combinations of data pre-processing, feature extraction and unsupervised learning methodologies are explored. Spectrum quality control methods are applied to correct or minimise spectral problems, including high noise level, baseline offset and outlier. A multi-stage data analysis algorithm developed can provide statistical control over the breast cancer classification process and produce a precise cancer prognosis",Analysis of plastic recycling and breast cancer prognosis using vibrational spectroscopy,https://core.ac.uk/download/237735590.pdf,,,,core
1498818,1994-01-01T00:00:00,"ARCHON™ (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe’s largest project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Some examples of the applications to which it has been successfully applied include: electricity distribution and supply, electricity transmission and distribution, control of a cement kiln complex, control of a particle accelerator, and control of a robotics application. The type of cooperating community that it supports has a decentralised control regime and individual problem solving agents which are large grain, loosely coupled, and semi-autonomous. This paper will tackle a broad range of issues related to the application of ARCHON technology to industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the characteristics which typify this important domain. Secondly, the ARCHON framework is detailed - with a special emphasis being placed upon the implementation architecture. Thirdly, a brief resumee and status report of the main applications is presented. Finally, the lessons learned and the future plans are presented",The ARCHON System and its Applications,,,,,core
78898986,,"This study presents a computational fluid dynamic (CFD) study of Dimethyl Ether (DME) gas adsorptive separation and steam reforming (DME-SR) in a large scale Circulating Fluidized Bed (CFB) reactor. The CFD model is based on Eulerian-Eulerian dispersed flow and solved using commercial software (ANSYS FLUENT). Hydrogen is currently receiving increasing interest as an alternative source of clean energy and has high potential applications, including the transportation sector and power generation. Computational fluid dynamic (CFD) modelling has attracted considerable recognition in the engineering sector consequently leading to using it as a tool for process design and optimisation in many industrial processes. In most cases, these processes are difficult or expensive to conduct in lab scale experiments. The CFD provides a cost effective methodology to gain detailed information up to the microscopic level. The main objectives in this project are to: (i) develop a predictive model using ANSYS FLUENT (CFD) commercial code to simulate the flow hydrodynamics, mass transfer, reactions and heat transfer in a large scale dual fluidized bed system for combined gas separation and steam reforming processes (ii) implement a suitable adsorption models in the CFD code, through a user defined function, to predict selective separation of a gas from a mixture (iii) develop a model for dimethyl ether steam reforming (DME-SR) to predict hydrogen production (iv) carry out detailed parametric analysis in order to establish ideal operating conditions for future industrial application. The project has originated from a real industrial case problem in collaboration with the industrial partner Dow Corning (UK) and jointly funded by the Engineering and Physical Research Council (UK) and Dow Corning. The research examined gas separation by adsorption in a bubbling bed, as part of a dual fluidized bed system. The adsorption process was simulated based on the kinetics derived from the experimental data produced as part of a separate PhD project completed under the same fund. The kinetic model was incorporated in FLUENT CFD tool as a pseudo-first order rate equation; some of the parameters for the pseudo-first order kinetics were obtained using MATLAB. The modelling of the DME adsorption in the designed bubbling bed was performed for the first time in this project and highlights the novelty in the investigations. The simulation results were analysed to provide understanding of the flow hydrodynamic, reactor design and optimum operating condition for efficient separation. Bubbling bed validation by estimation of bed expansion and the solid and gas distribution from simulation agreed well with trends seen in the literatures. Parametric analysis on the adsorption process demonstrated that increasing fluidizing velocity reduced adsorption of DME. This is as a result of reduction in the gas residence time which appears to have much effect compared to the solid residence time. The removal efficiency of DME from the bed was found to be more than 88%. Simulation of the DME-SR in FLUENT CFD was conducted using selected kinetics from literature and implemented in the model using an in-house developed user defined function. The validation of the kinetics was achieved by simulating a case to replicate an experimental study of a laboratory scale bubbling bed by Vicente et al [1]. Good agreement was achieved for the validation of the models, which was then applied in the DME-SR in the large scale riser section of the dual fluidized bed system. This is the first study to use the selected DME-SR kinetics in a circulating fluidized bed (CFB) system and for the geometry size proposed for the project. As a result, the simulation produced the first detailed data on the spatial variation and final gas product in such an industrial scale fluidized bed system. The simulation results provided insight in the flow hydrodynamic, reactor design and optimum operating condition. The solid and gas distribution in the CFB was observed to show good agreement with literatures. The parametric analysis showed that the increase in temperature and steam to DME molar ratio increased the production of hydrogen due to the increased DME conversions, whereas the increase in the space velocity has been found to have an adverse effect. Increasing temperature between 200 oC to 350 oC increased DME conversion from 47% to 99% while hydrogen yield increased substantially from 11% to 100%. The CO2 selectivity decreased from 100% to 91% due to the water gas shift reaction favouring CO at higher temperatures. The higher conversions observed as the temperature increased was reflected on the quantity of unreacted DME and methanol concentrations in the product gas, where both decreased to very low values of 0.27 mol% and 0.46 mol% respectively at 350 °C. Increasing the steam to DME molar ratio from 4 to 7.68 increased the DME conversion from 69% to 87%, while the hydrogen yield increased from 40% to 59%. The CO2 selectivity decreased from 100% to 97%. The decrease in the space velocity from 37104 ml/g/h to 15394 ml/g/h increased the DME conversion from 87% to 100% while increasing the hydrogen yield from 59% to 87%. The parametric analysis suggests an operating condition for maximum hydrogen yield is in the region of 300 oC temperatures and Steam/DME molar ratio of 5. The analysis of the industrial sponsor’s case for the given flow and composition of the gas to be treated suggests that 88% of DME can be adsorbed from the bubbling and consequently producing 224.4t/y of hydrogen in the riser section of the dual fluidized bed system. The process also produces 1458.4t/y of CO2 and 127.9t/y of CO as part of the product gas. The developed models and parametric analysis carried out in this study provided essential guideline for future design of DME-SR at industrial level and in particular this work has been of tremendous importance for the industrial collaborator in order to draw conclusions and plan for future potential implementation of the process at an industrial scale",Computationsl modelling of dimethyl ether separation and steam reforming in fluidized bed reactors,https://core.ac.uk/download/78898986.pdf,,,,core
1540278,,"The principal objective of this thesis is to develop improved motion estimation and segmentation techniques that meet the image-processing requirements of the post¬production industry. Starting with a rigorous taxonomy of existing image segmentation techniques, we proceed by focusing on motion estimation by means of optical flow calculation. A parametric motion model based method to estimate optical flow fields on three consecutive frames is developed and tested on a number of colour real sequences. Initial estimates are robustly refined in an iterative scheme and are enhanced by colour probability distribution information to enable foreground/background segmentation in a maximum a posteriori pixel classification scheme. Experiments, . show the significant contribution of the colour part towards a well-segmented image.Additionally, a very accurate variational optical flow computation method based on brightness constancy, gradient constancy and spatiotemporal smoothness constraints is modified and implemented so that it can robustly estimate global motion over three consecutive frames. Motion is enhanced by colour evidence in a similar manner and the method adopts the same probabilistic labelling procedure. After a comparison of the two methods on the same colour sequences, a third neural network based method is implemented, which initially estimates motion by employing two twin-layer optical flow calculating Gellular Neural Networks and proceeds in a similar manner, (incorporating colour information and probabilistic ally classifying pixels), leading to similar or improved quality results with the added advantage of significantly accelerated performance. Moreover, another CNN is employed with the task of offering spatial and temporal pixel compatibility constraint support, further improving the quality of the segmented images. Weights are used to control the respective contributing terms enabling optimization of the segmentation results for each sequence individually. Finally, as a case study of CNN implementation in hardware (FPGA), the use of Handel-G, a C-like, high-level, parallel, hardware description language, is exploited to allow for rapid translation of our algorithms to efficient hardware",Motion estimation and segmentation of colour image sequences,,Kingston University,,,core
