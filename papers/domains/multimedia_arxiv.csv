id,updated,published,title,summary,database
http://arxiv.org/abs/2001.05703v1,2020-01-16T09:13:31Z,2020-01-16T09:13:31Z,"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data","Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.",arxiv
http://arxiv.org/abs/1810.07829v1,2018-10-17T23:06:06Z,2018-10-17T23:06:06Z,"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality","The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it.",arxiv
http://arxiv.org/abs/2012.05410v1,2020-12-10T02:08:47Z,2020-12-10T02:08:47Z,Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.",arxiv
http://arxiv.org/abs/2106.15021v1,2021-06-21T11:23:12Z,2021-06-21T11:23:12Z,"How to Reach Real-Time AI on Consumer Devices? Solutions for
  Programmable and Custom Architectures","The unprecedented performance of deep neural networks (DNNs) has led to large
strides in various Artificial Intelligence (AI) inference tasks, such as object
and speech recognition. Nevertheless, deploying such AI models across commodity
devices faces significant challenges: large computational cost, multiple
performance objectives, hardware heterogeneity and a common need for high
accuracy, together pose critical problems to the deployment of DNNs across the
various embedded and mobile devices in the wild. As such, we have yet to
witness the mainstream usage of state-of-the-art deep learning algorithms
across consumer devices. In this paper, we provide preliminary answers to this
potentially game-changing question by presenting an array of design techniques
for efficient AI systems. We start by examining the major roadblocks when
targeting both programmable processors and custom accelerators. Then, we
present diverse methods for achieving real-time performance following a
cross-stack approach. These span model-, system- and hardware-level techniques,
and their combination. Our findings provide illustrative examples of AI systems
that do not overburden mobile hardware, while also indicating how they can
improve inference accuracy. Moreover, we showcase how custom ASIC- and
FPGA-based accelerators can be an enabling factor for next-generation AI
applications, such as multi-DNN systems. Collectively, these results highlight
the critical need for further exploration as to how the various cross-stack
solutions can be best combined in order to bring the latest advances in deep
learning close to users, in a robust and efficient manner.",arxiv
http://arxiv.org/abs/2102.09360v1,2021-02-18T14:12:24Z,2021-02-18T14:12:24Z,"All-optical spiking neurosynaptic networks with self-learning
  capabilities","Software-implementation, via neural networks, of brain-inspired computing
approaches underlie many important modern-day computational tasks, from image
processing to speech recognition, artificial intelligence and deep learning
applications. Yet, differing from real neural tissue, traditional computing
architectures physically separate the core computing functions of memory and
processing, making fast, efficient and low-energy brain-like computing
difficult to achieve. To overcome such limitations, an attractive and
alternative goal is to design direct hardware mimics of brain neurons and
synapses which, when connected in appropriate networks (or neuromorphic
systems), process information in a way more fundamentally analogous to that of
real brains. Here we present an all-optical approach to achieving such a goal.
Specifically, we demonstrate an all-optical spiking neuron device and connect
it, via an integrated photonics network, to photonic synapses to deliver a
small-scale all-optical neurosynaptic system capable of supervised and
unsupervised learning. Moreover, we exploit wavelength division multiplexing
techniques to implement a scalable circuit architecture for photonic neural
networks, successfully demonstrating pattern recognition directly in the
optical domain using a photonic system comprising 140 elements. Such optical
implementations of neurosynaptic networks promise access to the high speed and
bandwidth inherent to optical systems, which would be very attractive for the
direct processing of telecommunication and visual data in the optical domain.",arxiv
http://arxiv.org/abs/2012.01913v1,2020-12-03T13:51:05Z,2020-12-03T13:51:05Z,Transfer Learning as an Enabler of the Intelligent Digital Twin,"Digital Twins have been described as beneficial in many areas, such as
virtual commissioning, fault prediction or reconfiguration planning. Equipping
Digital Twins with artificial intelligence functionalities can greatly expand
those beneficial applications or open up altogether new areas of application,
among them cross-phase industrial transfer learning. In the context of machine
learning, transfer learning represents a set of approaches that enhance
learning new tasks based upon previously acquired knowledge. Here, knowledge is
transferred from one lifecycle phase to another in order to reduce the amount
of data or time needed to train a machine learning algorithm. Looking at common
challenges in developing and deploying industrial machinery with deep learning
functionalities, embracing this concept would offer several advantages: Using
an intelligent Digital Twin, learning algorithms can be designed, configured
and tested in the design phase before the physical system exists and real data
can be collected. Once real data becomes available, the algorithms must merely
be fine-tuned, significantly speeding up commissioning and reducing the
probability of costly modifications. Furthermore, using the Digital Twin's
simulation capabilities virtually injecting rare faults in order to train an
algorithm's response or using reinforcement learning, e.g. to teach a robot,
become practically feasible. This article presents several cross-phase
industrial transfer learning use cases utilizing intelligent Digital Twins. A
real cyber physical production system consisting of an automated welding
machine and an automated guided vehicle equipped with a robot arm is used to
illustrate the respective benefits.",arxiv
http://arxiv.org/abs/1905.03418v2,2019-06-07T15:31:48Z,2019-05-09T02:39:37Z,"Deep Learning Acceleration Techniques for Real Time Mobile Vision
  Applications","Deep Learning (DL) has become a crucial technology for Artificial
Intelligence (AI). It is a powerful technique to automatically extract
high-level features from complex data which can be exploited for applications
such as computer vision, natural language processing, cybersecurity,
communications, and so on. For the particular case of computer vision, several
algorithms like object detection in real time videos have been proposed and
they work well on Desktop GPUs and distributed computing platforms. However
these algorithms are still heavy for mobile and embedded visual applications.
The rapid spreading of smart portable devices and the emerging 5G network are
introducing new smart multimedia applications in mobile environments. As a
consequence, the possibility of implementing deep neural networks to mobile
environments has attracted a lot of researchers. This paper presents emerging
deep learning acceleration techniques that can enable the delivery of real time
visual recognition into the hands of end users, anytime and anywhere.",arxiv
http://arxiv.org/abs/1808.02134v1,2018-08-06T22:13:33Z,2018-08-06T22:13:33Z,"Kerman: A Hybrid Lightweight Tracking Algorithm to Enable Smart
  Surveillance as an Edge Service","Edge computing pushes the cloud computing boundaries beyond uncertain network
resource by leveraging computational processes close to the source and target
of data. Time-sensitive and data-intensive video surveillance applications
benefit from on-site or near-site data mining. In recent years, many smart
video surveillance approaches are proposed for object detection and tracking by
using Artificial Intelligence (AI) and Machine Learning (ML) algorithms.
However, it is still hard to migrate those computing and data-intensive tasks
from Cloud to Edge due to the high computational requirement. In this paper, we
envision to achieve intelligent surveillance as an edge service by proposing a
hybrid lightweight tracking algorithm named Kerman (Kernelized Kalman filter).
Kerman is a decision tree based hybrid Kernelized Correlation Filter (KCF)
algorithm proposed for human object tracking, which is coupled with a
lightweight Convolutional Neural Network (L-CNN) for high performance. The
proposed Kerman algorithm has been implemented on a couple of single board
computers (SBC) as edge devices and validated using real-world surveillance
video streams. The experimental results are promising that the Kerman algorithm
is able to track the object of interest with a decent accuracy at a resource
consumption affordable by edge devices.",arxiv
http://arxiv.org/abs/2009.10679v1,2020-09-22T16:55:44Z,2020-09-22T16:55:44Z,"An embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.",arxiv
http://arxiv.org/abs/2007.00641v1,2020-07-01T17:46:08Z,2020-07-01T17:46:08Z,Democratizing the Edge: A Pervasive Edge Computing Framework,"The needs of emerging applications, such as augmented and virtual reality,
federated machine learning, and autonomous driving, have motivated edge
computing--the push of computation capabilities to the edge. Various edge
computing architectures have emerged, including multi-access edge computing and
edge-cloud, all with the premise of reducing communication latency and
augmenting privacy. However, these architectures rely on static and
pre-deployed infrastructure, falling short in harnessing the abundant resources
at the network's edge. In this paper, we discuss the design of Pervasive Edge
Computing (PEC)--a democratized edge computing framework, which enables
end-user devices (e.g., smartphones, IoT devices, and vehicles) to dynamically
participate in a large-scale computing ecosystem. Our vision of the
democratized edge involves the real-time composition of services using
available edge resources like data, software, and compute-hardware from
multiple stakeholders. We discuss how the novel Named-Data Networking
architecture can facilitate service deployment, discovery, invocation, and
migration. We also discuss the economic models critical to the adoption of PEC
and the outstanding challenges for its full realization.",arxiv
http://arxiv.org/abs/2005.01557v1,2020-05-04T15:16:30Z,2020-05-04T15:16:30Z,"Off-the-shelf deep learning is not enough: parsimony, Bayes and
  causality","Deep neural networks (""deep learning"") have emerged as a technology of choice
to tackle problems in natural language processing, computer vision, speech
recognition and gameplay, and in just a few years has led to superhuman level
performance and ushered in a new wave of ""AI."" Buoyed by these successes,
researchers in the physical sciences have made steady progress in incorporating
deep learning into their respective domains. However, such adoption brings
substantial challenges that need to be recognized and confronted. Here, we
discuss both opportunities and roadblocks to implementation of deep learning
within materials science, focusing on the relationship between correlative
nature of machine learning and causal hypothesis driven nature of physical
sciences. We argue that deep learning and AI are now well positioned to
revolutionize fields where causal links are known, as is the case for
applications in theory. When confounding factors are frozen or change only
weakly, this leaves open the pathway for effective deep learning solutions in
experimental domains. Similarly, these methods offer a pathway towards
understanding the physics of real-world systems, either via deriving reduced
representations, deducing algorithmic complexity, or recovering generative
physical models. However, extending deep learning and ""AI"" for models with
unclear causal relationship can produce misleading and potentially incorrect
results. Here, we argue the broad adoption of Bayesian methods incorporating
prior knowledge, development of DL solutions with incorporated physical
constraints, and ultimately adoption of causal models, offers a path forward
for fundamental and applied research. Most notably, while these advances can
change the way science is carried out in ways we cannot imagine, machine
learning is not going to substitute science any time soon.",arxiv
http://arxiv.org/abs/1906.05925v1,2019-06-13T20:53:33Z,2019-06-13T20:53:33Z,Deep Learning Development Environment in Virtual Reality,"Virtual reality (VR) offers immersive visualization and intuitive
interaction. We leverage VR to enable any biomedical professional to deploy a
deep learning (DL) model for image classification. While DL models can be
powerful tools for data analysis, they are also challenging to understand and
develop. To make deep learning more accessible and intuitive, we have built a
virtual reality-based DL development environment. Within our environment, the
user can move tangible objects to construct a neural network only using their
hands. Our software automatically translates these configurations into a
trainable model and then reports its resulting accuracy on a test dataset in
real-time. Furthermore, we have enriched the virtual objects with
visualizations of the model's components such that users can achieve insight
about the DL models that they are developing. With this approach, we bridge the
gap between professionals in different fields of expertise while offering a
novel perspective for model analysis and data interaction. We further suggest
that techniques of development and visualization in deep learning can benefit
by integrating virtual reality.",arxiv
http://arxiv.org/abs/2103.13997v1,2021-03-25T17:34:59Z,2021-03-25T17:34:59Z,Real-time low-resource phoneme recognition on edge devices,"While speech recognition has seen a surge in interest and research over the
last decade, most machine learning models for speech recognition either require
large training datasets or lots of storage and memory. Combined with the
prominence of English as the number one language in which audio data is
available, this means most other languages currently lack good speech
recognition models.
  The method presented in this paper shows how to create and train models for
speech recognition in any language which are not only highly accurate, but also
require very little storage, memory and training data when compared with
traditional models. This allows training models to recognize any language and
deploying them on edge devices such as mobile phones or car displays for fast
real-time speech recognition.",arxiv
http://arxiv.org/abs/1909.11145v2,2019-10-01T09:02:01Z,2019-09-24T19:29:30Z,"Brain-Inspired Hardware for Artificial Intelligence: Accelerated
  Learning in a Physical-Model Spiking Neural Network","Future developments in artificial intelligence will profit from the existence
of novel, non-traditional substrates for brain-inspired computing. Neuromorphic
computers aim to provide such a substrate that reproduces the brain's
capabilities in terms of adaptive, low-power information processing. We present
results from a prototype chip of the BrainScaleS-2 mixed-signal neuromorphic
system that adopts a physical-model approach with a 1000-fold acceleration of
spiking neural network dynamics relative to biological real time. Using the
embedded plasticity processor, we both simulate the Pong arcade video game and
implement a local plasticity rule that enables reinforcement learning, allowing
the on-chip neural network to learn to play the game. The experiment
demonstrates key aspects of the employed approach, such as accelerated and
flexible learning, high energy efficiency and resilience to noise.",arxiv
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv
http://arxiv.org/abs/2005.08076v1,2020-05-16T19:42:16Z,2020-05-16T19:42:16Z,"A Deep Learning based Wearable Healthcare IoT Device for AI-enabled
  Hearing Assistance Automation","With the recent booming of artificial intelligence (AI), particularly deep
learning techniques, digital healthcare is one of the prevalent areas that
could gain benefits from AI-enabled functionality. This research presents a
novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266
platform capable of assisting those who suffer from impairment of hearing or
deafness to communicate with others in conversations. In the proposed solution,
a server application is created that leverages Google's online speech
recognition service to convert the received conversations into texts, then
deployed to a micro-display attached to the glasses to display the conversation
contents to deaf people, to enable and assist conversation as normal with the
general population. Furthermore, in order to raise alert of traffic or
dangerous scenarios, an 'urban-emergency' classifier is developed using a deep
learning model, Inception-v4, with transfer learning to detect/recognize
alerting/alarming sounds, such as a horn sound or a fire alarm, with texts
generated to alert the prospective user. The training of Inception-v4 was
carried out on a consumer desktop PC and then implemented into the AI based IoT
application. The empirical results indicate that the developed prototype system
achieves an accuracy rate of 92% for sound recognition and classification with
real-time performance.",arxiv
http://arxiv.org/abs/2006.15350v1,2020-06-27T12:13:22Z,2020-06-27T12:13:22Z,"MiniNet: An extremely lightweight convolutional neural network for
  real-time unsupervised monocular depth estimation","Predicting depth from a single image is an attractive research topic since it
provides one more dimension of information to enable machines to better
perceive the world. Recently, deep learning has emerged as an effective
approach to monocular depth estimation. As obtaining labeled data is costly,
there is a recent trend to move from supervised learning to unsupervised
learning to obtain monocular depth. However, most unsupervised learning methods
capable of achieving high depth prediction accuracy will require a deep network
architecture which will be too heavy and complex to run on embedded devices
with limited storage and memory spaces. To address this issue, we propose a new
powerful network with a recurrent module to achieve the capability of a deep
network while at the same time maintaining an extremely lightweight size for
real-time high performance unsupervised monocular depth prediction from video
sequences. Besides, a novel efficient upsample block is proposed to fuse the
features from the associated encoder layer and recover the spatial size of
features with the small number of model parameters. We validate the
effectiveness of our approach via extensive experiments on the KITTI dataset.
Our new model can run at a speed of about 110 frames per second (fps) on a
single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it
achieves higher depth accuracy with nearly 33 times fewer model parameters than
state-of-the-art models. To the best of our knowledge, this work is the first
extremely lightweight neural network trained on monocular video sequences for
real-time unsupervised monocular depth estimation, which opens up the
possibility of implementing deep learning-based real-time unsupervised
monocular depth prediction on low-cost embedded devices.",arxiv
http://arxiv.org/abs/1905.04127v1,2019-05-10T12:43:52Z,2019-05-10T12:43:52Z,"Design of Artificial Intelligence Agents for Games using Deep
  Reinforcement Learning","In order perform a large variety of tasks and to achieve human-level
performance in complex real-world environments, Artificial Intelligence (AI)
Agents must be able to learn from their past experiences and gain both
knowledge and an accurate representation of their environment from raw sensory
inputs. Traditionally, AI agents have suffered from difficulties in using only
sensory inputs to obtain a good representation of their environment and then
mapping this representation to an efficient control policy. Deep reinforcement
learning algorithms have provided a solution to this issue. In this study, the
performance of different conventional and novel deep reinforcement learning
algorithms was analysed. The proposed method utilises two types of algorithms,
one trained with a variant of Q-learning (DQN) and another trained with SARSA
learning (DSN) to assess the feasibility of using direct feedback alignment, a
novel biologically plausible method for back-propagating the error. These novel
agents, alongside two similar agents trained with the conventional
backpropagation algorithm, were tested by using the OpenAI Gym toolkit on
several classic control theory problems and Atari 2600 video games. The results
of this investigation open the way into new, biologically-inspired deep
reinforcement learning algorithms, and their implementation on neuromorphic
hardware.",arxiv
http://arxiv.org/abs/1605.02097v2,2016-09-20T19:12:49Z,2016-05-06T20:46:34Z,"ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement
  Learning","The recent advances in deep neural networks have led to effective
vision-based reinforcement learning methods that have been employed to obtain
human-level controllers in Atari 2600 games from pixel data. Atari 2600 games,
however, do not resemble real-world tasks since they involve non-realistic 2D
environments and the third-person perspective. Here, we propose a novel
test-bed platform for reinforcement learning research from raw visual
information which employs the first-person perspective in a semi-realistic 3D
world. The software, called ViZDoom, is based on the classical first-person
shooter video game, Doom. It allows developing bots that play the game using
the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a
convenient mechanism of user scenarios. In the experimental part, we test the
environment by trying to learn bots for two scenarios: a basic move-and-shoot
task and a more complex maze-navigation problem. Using convolutional deep
neural networks with Q-learning and experience replay, for both scenarios, we
were able to train competent bots, which exhibit human-like behaviors. The
results confirm the utility of ViZDoom as an AI research platform and imply
that visual reinforcement learning in 3D realistic first-person perspective
environments is feasible.",arxiv
http://arxiv.org/abs/2110.09625v1,2021-10-18T21:21:23Z,2021-10-18T21:21:23Z,Personalized Speech Enhancement: New Models and Comprehensive Evaluation,"Personalized speech enhancement (PSE) models utilize additional cues, such as
speaker embeddings like d-vectors, to remove background noise and interfering
speech in real-time and thus improve the speech quality of online video
conferencing systems for various acoustic scenarios. In this work, we propose
two neural networks for PSE that achieve superior performance to the previously
proposed VoiceFilter. In addition, we create test sets that capture a variety
of scenarios that users can encounter during video conferencing. Furthermore,
we propose a new metric to measure the target speaker over-suppression (TSOS)
problem, which was not sufficiently investigated before despite its critical
importance in deployment. Besides, we propose multi-task training with a speech
recognition back-end. Our results show that the proposed models can yield
better speech recognition accuracy, speech intelligibility, and perceptual
quality than the baseline models, and the multi-task training can alleviate the
TSOS issue in addition to improving the speech recognition accuracy.",arxiv
http://arxiv.org/abs/2108.01704v1,2021-08-03T18:58:39Z,2021-08-03T18:58:39Z,"Bifocal Neural ASR: Exploiting Keyword Spotting for Inference
  Optimization","We present Bifocal RNN-T, a new variant of the Recurrent Neural Network
Transducer (RNN-T) architecture designed for improved inference time latency on
speech recognition tasks. The architecture enables a dynamic pivot for its
runtime compute pathway, namely taking advantage of keyword spotting to select
which component of the network to execute for a given audio frame. To
accomplish this, we leverage a recurrent cell we call the Bifocal LSTM
(BFLSTM), which we detail in the paper. The architecture is compatible with
other optimization strategies such as quantization, sparsification, and
applying time-reduction layers, making it especially applicable for deployed,
real-time speech recognition settings. We present the architecture and report
comparative experimental results on voice-assistant speech recognition tasks.
Specifically, we show our proposed Bifocal RNN-T can improve inference cost by
29.1% with matching word error rates and only a minor increase in memory size.",arxiv
http://arxiv.org/abs/2105.13331v2,2021-09-23T16:04:49Z,2021-05-27T17:39:06Z,Quantization and Deployment of Deep Neural Networks on Microcontrollers,"Embedding Artificial Intelligence onto low-power devices is a challenging
task that has been partly overcome with recent advances in machine learning and
hardware design. Presently, deep neural networks can be deployed on embedded
targets to perform different tasks such as speech recognition,object detection
or Human Activity Recognition. However, there is still room for optimization of
deep neural networks onto embedded devices. These optimizations mainly address
power consumption,memory and real-time constraints, but also an easier
deployment at the edge. Moreover, there is still a need for a better
understanding of what can be achieved for different use cases. This work
focuses on quantization and deployment of deep neural networks onto low-power
32-bit microcontrollers. The quantization methods, relevant in the context of
an embedded execution onto a microcontroller, are first outlined. Then, a new
framework for end-to-end deep neural networks training, quantization and
deployment is presented. This framework, called MicroAI, is designed as an
alternative to existing inference engines (TensorFlow Lite for Microcontrollers
and STM32CubeAI). Our framework can indeed be easily adjusted and/or extended
for specific use cases. Execution using single precision 32-bit floating-point
as well as fixed-point on 8- and 16-bit integers are supported. The proposed
quantization method is evaluated with three different datasets (UCI-HAR, Spoken
MNIST and GTSRB). Finally, a comparison study between MicroAI and both existing
embedded inference engines is provided in terms of memory and power efficiency.
On-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq
Apollo3 and STM32L452RE).",arxiv
http://arxiv.org/abs/1912.06321v2,2020-08-17T03:26:55Z,2019-12-13T04:29:38Z,"Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World
  Performance?","Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",arxiv
http://arxiv.org/abs/1901.04985v1,2019-01-12T06:00:05Z,2019-01-12T06:00:05Z,"NNStreamer: Stream Processing Paradigm for Neural Networks, Toward
  Efficient Development and Execution of On-Device AI Applications","We propose nnstreamer, a software system that handles neural networks as
filters of stream pipelines, applying the stream processing paradigm to neural
network applications. A new trend with the wide-spread of deep neural network
applications is on-device AI; i.e., processing neural networks directly on
mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy
issues, data transmission costs, and operational costs signifies the need for
on-device AI especially when a huge number of devices with real-time data
processing are deployed. Nnstreamer efficiently handles neural networks with
complex data stream pipelines on devices, improving the overall performance
significantly with minimal efforts. Besides, nnstreamer simplifies the neural
network pipeline implementations and allows reusing off-shelf multimedia stream
filters directly; thus it reduces the developmental costs significantly.
Nnstreamer is already being deployed with a product releasing soon and is open
source software applicable to a wide range of hardware architectures and
software platforms.",arxiv
http://arxiv.org/abs/2002.06637v1,2020-02-16T18:18:19Z,2020-02-16T18:18:19Z,Real-time binaural speech separation with preserved spatial cues,"Deep learning speech separation algorithms have achieved great success in
improving the quality and intelligibility of separated speech from mixed audio.
Most previous methods focused on generating a single-channel output for each of
the target speakers, hence discarding the spatial cues needed for the
localization of sound sources in space. However, preserving the spatial
information is important in many applications that aim to accurately render the
acoustic scene such as in hearing aids and augmented reality (AR). Here, we
propose a speech separation algorithm that preserves the interaural cues of
separated sound sources and can be implemented with low latency and high
fidelity, therefore enabling a real-time modification of the acoustic scene.
Based on the time-domain audio separation network (TasNet), a single-channel
time-domain speech separation system that can be implemented in real-time, we
propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that
takes binaural mixed audio as input and simultaneously separates target
speakers in both channels. Experimental results show that the proposed
end-to-end MIMO system is able to significantly improve the separation
performance and keep the perceived location of the modified sources intact in
various acoustic scenes.",arxiv
http://arxiv.org/abs/2105.11216v1,2021-05-24T11:51:46Z,2021-05-24T11:51:46Z,"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad
  Virtual Y Conocimiento Experto para generación de entornos de aprendizaje
  Hombre-Máquina","This work presents the results of project CONECT4, which addresses the
research and development of new non-intrusive communication methods for the
generation of a human-machine learning ecosystem oriented to predictive
maintenance in the automotive industry. Through the use of innovative
technologies such as Augmented Reality, Virtual Reality, Digital Twin and
expert knowledge, CONECT4 implements methodologies that allow improving the
efficiency of training techniques and knowledge management in industrial
companies. The research has been supported by the development of content and
systems with a low level of technological maturity that address solutions for
the industrial sector applied in training and assistance to the operator. The
results have been analyzed in companies in the automotive sector, however, they
are exportable to any other type of industrial sector. -- --
  En esta publicaci\'on se presentan los resultados del proyecto CONECT4, que
aborda la investigaci\'on y desarrollo de nuevos m\'etodos de comunicaci\'on no
intrusivos para la generaci\'on de un ecosistema de aprendizaje
hombre-m\'aquina orientado al mantenimiento predictivo en la industria de
automoci\'on. A trav\'es del uso de tecnolog\'ias innovadoras como la Realidad
Aumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,
CONECT4 implementa metodolog\'ias que permiten mejorar la eficiencia de las
t\'ecnicas de formaci\'on y gesti\'on de conocimiento en las empresas
industriales. La investigaci\'on se ha apoyado en el desarrollo de contenidos y
sistemas con un nivel de madurez tecnol\'ogico bajo que abordan soluciones para
el sector industrial aplicadas en la formaci\'on y asistencia al operario. Los
resultados han sido analizados en empresas del sector de automoci\'on, no
obstante, son exportables a cualquier otro tipo de sector industrial.",arxiv
http://arxiv.org/abs/1804.09997v1,2018-04-26T11:37:03Z,2018-04-26T11:37:03Z,PANDA: Facilitating Usable AI Development,"Recent advances in artificial intelligence (AI) and machine learning have
created a general perception that AI could be used to solve complex problems,
and in some situations over-hyped as a tool that can be so easily used.
Unfortunately, the barrier to realization of mass adoption of AI on various
business domains is too high because most domain experts have no background in
AI. Developing AI applications involves multiple phases, namely data
preparation, application modeling, and product deployment. The effort of AI
research has been spent mostly on new AI models (in the model training stage)
to improve the performance of benchmark tasks such as image recognition. Many
other factors such as usability, efficiency and security of AI have not been
well addressed, and therefore form a barrier to democratizing AI. Further, for
many real world applications such as healthcare and autonomous driving,
learning via huge amounts of possibility exploration is not feasible since
humans are involved. In many complex applications such as healthcare, subject
matter experts (e.g. Clinicians) are the ones who appreciate the importance of
features that affect health, and their knowledge together with existing
knowledge bases are critical to the end results. In this paper, we take a new
perspective on developing AI solutions, and present a solution for making AI
usable. We hope that this resolution will enable all subject matter experts
(eg. Clinicians) to exploit AI like data scientists.",arxiv
http://arxiv.org/abs/2012.10342v3,2021-09-14T20:26:13Z,2020-12-18T16:40:32Z,Exploring and Interrogating Astrophysical Data in Virtual Reality,"Scientists across all disciplines increasingly rely on machine learning
algorithms to analyse and sort datasets of ever increasing volume and
complexity. Although trends and outliers are easily extracted, careful and
close inspection will still be necessary to explore and disentangle detailed
behavior, as well as identify systematics and false positives. We must
therefore incorporate new technologies to facilitate scientific analysis and
exploration. Astrophysical data is inherently multi-parameter, with the
spatial-kinematic dimensions at the core of observations and simulations. The
arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as
well as the availability of versatile development tools for video games, has
enabled scientists to deploy such technology to effectively interrogate and
interact with complex data. In this paper we present development and results
from custom-built interactive VR tools, called the iDaVIE suite, that are
informed and driven by research on galaxy evolution, cosmic large-scale
structure, galaxy-galaxy interactions, and gas/kinematics of nearby galaxies in
survey and targeted observations. In the new era of Big Data ushered in by
major facilities such as the SKA and LSST that render past analysis and
refinement methods highly constrained, we believe that a paradigm shift to new
software, technology and methods that exploit the power of visual perception,
will play an increasingly important role in bridging the gap between
statistical metrics and new discovery. We have released a beta version of the
iDaVIE software system that is free and open to the community.",arxiv
http://arxiv.org/abs/2005.13601v1,2020-05-27T19:19:57Z,2020-05-27T19:19:57Z,"The Adversarial Resilience Learning Architecture for AI-based Modelling,
  Exploration, and Operation of Complex Cyber-Physical Systems","Modern algorithms in the domain of Deep Reinforcement Learning (DRL)
demonstrated remarkable successes; most widely known are those in game-based
scenarios, from ATARI video games to Go and the StarCraft~\textsc{II} real-time
strategy game. However, applications in the domain of modern Cyber-Physical
Systems (CPS) that take advantage a vast variety of DRL algorithms are few. We
assume that the benefits would be considerable: Modern CPS have become
increasingly complex and evolved beyond traditional methods of modelling and
analysis. At the same time, these CPS are confronted with an increasing amount
of stochastic inputs, from volatile energy sources in power grids to broad user
participation stemming from markets. Approaches of system modelling that use
techniques from the domain of Artificial Intelligence (AI) do not focus on
analysis and operation. In this paper, we describe the concept of Adversarial
Resilience Learning (ARL) that formulates a new approach to complex environment
checking and resilient operation: It defines two agent classes, attacker and
defender agents. The quintessence of ARL lies in both agents exploring the
system and training each other without any domain knowledge. Here, we introduce
the ARL software architecture that allows to use a wide range of model-free as
well as model-based DRL-based algorithms, and document results of concrete
experiment runs on a complex power grid.",arxiv
http://arxiv.org/abs/2006.02230v2,2020-11-17T15:43:42Z,2020-06-02T06:44:09Z,"PolyDL: Polyhedral Optimizations for Creation of High Performance DL
  primitives","Deep Neural Networks (DNNs) have revolutionized many aspects of our lives.
The use of DNNs is becoming ubiquitous including in softwares for image
recognition, speech recognition, speech synthesis, language translation, to
name a few. he training of DNN architectures however is computationally
expensive. Once the model is created, its use in the intended application - the
inference task, is computationally heavy too and the inference needs to be fast
for real time use. For obtaining high performance today, the code of Deep
Learning (DL) primitives optimized for specific architectures by expert
programmers exposed via libraries is the norm. However, given the constant
emergence of new DNN architectures, creating hand optimized code is expensive,
slow and is not scalable.
  To address this performance-productivity challenge, in this paper we present
compiler algorithms to automatically generate high performance implementations
of DL primitives that closely match the performance of hand optimized
libraries. We develop novel data reuse analysis algorithms using the polyhedral
model to derive efficient execution schedules automatically. In addition,
because most DL primitives use some variant of matrix multiplication at their
core, we develop a flexible framework where it is possible to plug in library
implementations of the same in lieu of a subset of the loops. We show that such
a hybrid compiler plus a minimal library-use approach results in
state-of-the-art performance. We develop compiler algorithms to also perform
operator fusions that reduce data movement through the memory hierarchy of the
computer system.",arxiv
http://arxiv.org/abs/1812.00825v2,2018-12-04T05:36:36Z,2018-11-21T21:02:50Z,"Microscope 2.0: An Augmented Reality Microscope with Real-time
  Artificial Intelligence Integration","The brightfield microscope is instrumental in the visual examination of both
biological and physical samples at sub-millimeter scales. One key clinical
application has been in cancer histopathology, where the microscopic assessment
of the tissue samples is used for the diagnosis and staging of cancer and thus
guides clinical therapy. However, the interpretation of these samples is
inherently subjective, resulting in significant diagnostic variability.
Moreover, in many regions of the world, access to pathologists is severely
limited due to lack of trained personnel. In this regard, Artificial
Intelligence (AI) based tools promise to improve the access and quality of
healthcare. However, despite significant advances in AI research, integration
of these tools into real-world cancer diagnosis workflows remains challenging
because of the costs of image digitization and difficulties in deploying AI
solutions. Here we propose a cost-effective solution to the integration of AI:
the Augmented Reality Microscope (ARM). The ARM overlays AI-based information
onto the current view of the sample through the optical pathway in real-time,
enabling seamless integration of AI into the regular microscopy workflow. We
demonstrate the utility of ARM in the detection of lymph node metastases in
breast cancer and the identification of prostate cancer with a latency that
supports real-time workflows. We anticipate that ARM will remove barriers
towards the use of AI in microscopic analysis and thus improve the accuracy and
efficiency of cancer diagnosis. This approach is applicable to other microscopy
tasks and AI algorithms in the life sciences and beyond.",arxiv
http://arxiv.org/abs/2002.08242v1,2020-02-11T08:23:14Z,2020-02-11T08:23:14Z,AI Online Filters to Real World Image Recognition,"Deep artificial neural networks, trained with labeled data sets are widely
used in numerous vision and robotics applications today. In terms of AI, these
are called reflex models, referring to the fact that they do not self-evolve or
actively adapt to environmental changes. As demand for intelligent robot
control expands to many high level tasks, reinforcement learning and state
based models play an increasingly important role. Herein, in computer vision
and robotics domain, we study a novel approach to add reinforcement controls
onto the image recognition reflex models to attain better overall performance,
specifically to a wider environment range beyond what is expected of the task
reflex models. Follow a common infrastructure with environment sensing and AI
based modeling of self-adaptive agents, we implement multiple types of AI
control agents. To the end, we provide comparative results of these agents with
baseline, and an insightful analysis of their benefit to improve overall image
recognition performance in real world.",arxiv
http://arxiv.org/abs/1808.06352v1,2018-08-20T09:06:21Z,2018-08-20T09:06:21Z,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",arxiv
http://arxiv.org/abs/2107.05989v1,2021-07-13T11:17:00Z,2021-07-13T11:17:00Z,"Emotion Recognition for Healthcare Surveillance Systems Using Neural
  Networks: A Survey","Recognizing the patient's emotions using deep learning techniques has
attracted significant attention recently due to technological advancements.
Automatically identifying the emotions can help build smart healthcare centers
that can detect depression and stress among the patients in order to start the
medication early. Using advanced technology to identify emotions is one of the
most exciting topics as it defines the relationships between humans and
machines. Machines learned how to predict emotions by adopting various methods.
In this survey, we present recent research in the field of using neural
networks to recognize emotions. We focus on studying emotions' recognition from
speech, facial expressions, and audio-visual input and show the different
techniques of deploying these algorithms in the real world. These three emotion
recognition techniques can be used as a surveillance system in healthcare
centers to monitor patients. We conclude the survey with a presentation of the
challenges and the related future work to provide an insight into the
applications of using emotion recognition.",arxiv
http://arxiv.org/abs/1810.11359v4,2020-10-09T07:51:26Z,2018-10-26T15:05:04Z,"gpuRIR: A Python Library for Room Impulse Response Simulation with GPU
  Acceleration","The Image Source Method (ISM) is one of the most employed techniques to
calculate acoustic Room Impulse Responses (RIRs), however, its computational
complexity grows fast with the reverberation time of the room and its
computation time can be prohibitive for some applications where a huge number
of RIRs are needed. In this paper, we present a new implementation that
dramatically improves the computation speed of the ISM by using Graphic
Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and
the computation of the images inside each RIR. Additional speedups were
achieved by exploiting the mixed precision capabilities of the newer GPUs and
by using lookup tables. We provide a Python library under GNU license that can
be easily used without any knowledge about GPU programming and we show that it
is about 100 times faster than other state of the art CPU libraries. It may
become a powerful tool for many applications that need to perform a large
number of acoustic simulations, such as training machine learning systems for
audio signal processing, or for real-time room acoustics simulations for
immersive multimedia systems, such as augmented or virtual reality.",arxiv
http://arxiv.org/abs/2007.08501v1,2020-07-16T17:53:02Z,2020-07-16T17:53:02Z,Accelerating 3D Deep Learning with PyTorch3D,"Deep learning has significantly improved 2D image recognition. Extending into
3D may advance many new applications including autonomous vehicles, virtual and
augmented reality, authoring 3D content, and even improving 2D recognition.
However despite growing interest, 3D deep learning remains relatively
underexplored. We believe that some of this disparity is due to the engineering
challenges involved in 3D deep learning, such as efficiently processing
heterogeneous data and reframing graphics operations to be differentiable. We
address these challenges by introducing PyTorch3D, a library of modular,
efficient, and differentiable operators for 3D deep learning. It includes a
fast, modular differentiable renderer for meshes and point clouds, enabling
analysis-by-synthesis approaches. Compared with other differentiable renderers,
PyTorch3D is more modular and efficient, allowing users to more easily extend
it while also gracefully scaling to large meshes and images. We compare the
PyTorch3D operators and renderer with other implementations and demonstrate
significant speed and memory improvements. We also use PyTorch3D to improve the
state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D
images on ShapeNet. PyTorch3D is open-source and we hope it will help
accelerate research in 3D deep learning.",arxiv
http://arxiv.org/abs/1908.01853v1,2019-08-02T01:13:50Z,2019-08-02T01:13:50Z,DELTA: A DEep learning based Language Technology plAtform,"In this paper we present DELTA, a deep learning based language technology
platform. DELTA is an end-to-end platform designed to solve industry level
natural language and speech processing problems. It integrates most popular
neural network models for training as well as comprehensive deployment tools
for production. DELTA aims to provide easy and fast experiences for using,
deploying, and developing natural language processing and speech models for
both academia and industry use cases. We demonstrate the reliable performance
with DELTA on several natural language processing and speech tasks, including
text classification, named entity recognition, natural language inference,
speech recognition, speaker verification, etc. DELTA has been used for
developing several state-of-the-art algorithms for publications and delivering
real production to serve millions of users.",arxiv
http://arxiv.org/abs/2104.09164v1,2021-04-19T09:41:32Z,2021-04-19T09:41:32Z,"HEAR: Human Action Recognition via Neural Networks on Homomorphically
  Encrypted Data","Remote monitoring to support ""aging in place"" is an active area of research.
Advanced computer vision technology based on deep learning can provide near
real-time home monitoring to detect falling and symptoms related to seizure,
and stroke. Affordable webcams, together with cloud computing services (to run
machine learning algorithms), can potentially bring significant social and
health benefits. However, it has not been deployed in practice because of
privacy and security concerns. People may feel uncomfortable sending their
videos of daily activities (with potentially sensitive private information) to
a computing service provider (e.g., on a commercial cloud). In this paper, we
propose a novel strategy to resolve this dilemma by applying fully homomorphic
encryption (FHE) to an alternative representation of human actions (i.e.,
skeleton joints), which guarantees information confidentiality while retaining
high-performance action detection at a low cost. We design an FHE-friendly
neural network for action recognition and present a secure neural network
evaluation strategy to achieve near real-time action detection. Our framework
for private inference achieves an 87.99% recognition accuracy (86.21%
sensitivity and 99.14% specificity in detecting falls) with a latency of 3.1
seconds on real-world datasets. Our evaluation shows that our elaborated and
fine-tuned method reduces the inference latency by 23.81%~74.67% over a
straightforward implementation.",arxiv
http://arxiv.org/abs/2010.01217v1,2020-10-02T22:28:02Z,2020-10-02T22:28:02Z,Artificial Intelligence Enabled Traffic Monitoring System,"Manual traffic surveillance can be a daunting task as Traffic Management
Centers operate a myriad of cameras installed over a network. Injecting some
level of automation could help lighten the workload of human operators
performing manual surveillance and facilitate making proactive decisions which
would reduce the impact of incidents and recurring congestion on roadways. This
article presents a novel approach to automatically monitor real time traffic
footage using deep convolutional neural networks and a stand-alone graphical
user interface. The authors describe the results of research received in the
process of developing models that serve as an integrated framework for an
artificial intelligence enabled traffic monitoring system. The proposed system
deploys several state-of-the-art deep learning algorithms to automate different
traffic monitoring needs. Taking advantage of a large database of annotated
video surveillance data, deep learning-based models are trained to detect
queues, track stationary vehicles, and tabulate vehicle counts. A pixel-level
segmentation approach is applied to detect traffic queues and predict severity.
Real-time object detection algorithms coupled with different tracking systems
are deployed to automatically detect stranded vehicles as well as perform
vehicular counts. At each stages of development, interesting experimental
results are presented to demonstrate the effectiveness of the proposed system.
Overall, the results demonstrate that the proposed framework performs
satisfactorily under varied conditions without being immensely impacted by
environmental hazards such as blurry camera views, low illumination, rain, or
snow.",arxiv
http://arxiv.org/abs/2109.00984v1,2021-09-02T14:36:55Z,2021-09-02T14:36:55Z,CrypTen: Secure Multi-Party Computation Meets Machine Learning,"Secure multi-party computation (MPC) allows parties to perform computations
on data while keeping that data private. This capability has great potential
for machine-learning applications: it facilitates training of machine-learning
models on private data sets owned by different parties, evaluation of one
party's private model using another party's private data, etc. Although a range
of studies implement machine-learning models via secure MPC, such
implementations are not yet mainstream. Adoption of secure MPC is hampered by
the absence of flexible software frameworks that ""speak the language"" of
machine-learning researchers and engineers. To foster adoption of secure MPC in
machine learning, we present CrypTen: a software framework that exposes popular
secure MPC primitives via abstractions that are common in modern
machine-learning frameworks, such as tensor computations, automatic
differentiation, and modular neural networks. This paper describes the design
of CrypTen and measure its performance on state-of-the-art models for text
classification, speech recognition, and image classification. Our benchmarks
show that CrypTen's GPU support and high-performance communication between (an
arbitrary number of) parties allows it to perform efficient private evaluation
of modern machine-learning models under a semi-honest threat model. For
example, two parties using CrypTen can securely predict phonemes in speech
recordings using Wav2Letter faster than real-time. We hope that CrypTen will
spur adoption of secure MPC in the machine-learning community.",arxiv
http://arxiv.org/abs/2009.05533v1,2020-09-11T17:09:35Z,2020-09-11T17:09:35Z,Deep Learning Interference Cancellation in Wireless Networks,"With the crowding of the electromagnetic spectrum and the shrinking cell size
in wireless networks, crosstalk between base stations and users is a major
problem. Although hand-crafted functional blocks and coding schemes are proven
effective to guarantee reliable data transfer, currently deep learning-based
approaches have drawn increasing attention in the communication system
modeling. In this paper, we propose a Neural Network (NN) based signal
processing technique that works with traditional DSP algorithms to overcome the
interference problem in realtime. This technique doesn't require any feedback
protocol between the receiver and transmitter which makes it very suitable for
low-latency and high data-rate applications such as autonomy and augmented
reality. While there has been recent work on the use of Reinforcement Learning
(RL) in the control layer to manage and control the interference, our approach
is novel in the sense that it introduces a neural network for signal processing
at baseband data rate and in the physical layer. We demonstrate this ""Deep
Interference Cancellation"" technique using a convolutional LSTM autoencoder.
When applied to QAM-OFDM modulated data, the network produces significant
improvement in the symbol error rate (SER). We further discuss the hardware
implementation including latency, power consumption, memory requirements, and
chip area.",arxiv
http://arxiv.org/abs/2110.04697v1,2021-10-10T03:51:39Z,2021-10-10T03:51:39Z,"An Augmented Reality Platform for Introducing Reinforcement Learning to
  K-12 Students with Robots","Interactive reinforcement learning, where humans actively assist during an
agent's learning process, has the promise to alleviate the sample complexity
challenges of practical algorithms. However, the inner workings and state of
the robot are typically hidden from the teacher when humans provide feedback.
To create a common ground between the human and the learning robot, in this
paper, we propose an Augmented Reality (AR) system that reveals the hidden
state of the learning to the human users. This paper describes our system's
design and implementation and concludes with a discussion on two directions for
future work which we are pursuing: 1) use of our system in AI education
activities at the K-12 level; and 2) development of a framework for an AR-based
human-in-the-loop reinforcement learning, where the human teacher can see
sensory and cognitive representations of the robot overlaid in the real world.",arxiv
http://arxiv.org/abs/2007.01137v2,2020-11-24T11:30:08Z,2020-06-30T12:41:35Z,Testing match-3 video games with Deep Reinforcement Learning,"Testing a video game is a critical step for the production process and
requires a great effort in terms of time and resources spent. Some software
houses are trying to use the artificial intelligence to reduce the need of
human resources using systems able to replace a human agent. We study the
possibility to use the Deep Reinforcement Learning to automate the testing
process in match-3 video games and suggest to approach the problem in the
framework of a Dueling Deep Q-Network paradigm. We test this kind of network on
the Jelly Juice game, a match-3 video game developed by the redBit Games. The
network extracts the essential information from the game environment and infers
the next move. We compare the results with the random player performance,
finding that the network shows a highest success rate. The results are in most
cases similar with those obtained by real users, and the network also succeeds
in learning over time the different features that distinguish the game levels
and adapts its strategy to the increasing difficulties.",arxiv
http://arxiv.org/abs/1811.02320v1,2018-11-06T12:32:27Z,2018-11-06T12:32:27Z,Hierarchical Neural Network Architecture In Keyword Spotting,"Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it
is essential to ensure a high recall rate. However, its real-time property
requires low computation complexity. This contradiction inspires people to find
a suitable model which is small enough to perform well in multi environments.
To deal with this contradiction, we implement the Hierarchical Neural
Network(HNN), which is proved to be effective in many speech recognition
problems. HNN outperforms traditional DNN and CNN even though its model size
and computation complexity are slightly less. Also, its simple topology
structure makes easy to deploy on any device.",arxiv
http://arxiv.org/abs/2103.10804v1,2021-03-19T13:48:25Z,2021-03-19T13:48:25Z,"Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and
  VR Interfaces","Self-adaptation approaches usually rely on closed-loop controllers that avoid
human intervention from adaptation. While such fully automated approaches have
proven successful in many application domains, there are situations where human
involvement in the adaptation process is beneficial or even necessary. For such
""human-in-the-loop"" adaptive systems, two major challenges, namely transparency
and controllability, have to be addressed to include the human in the
self-adaptation loop. Transparency means that relevant context information
about the adaptive systems and its context is represented based on a digital
twin enabling the human an immersive and realistic view. Concerning
controllability, the decision-making and adaptation operations should be
managed in a natural and interactive way. As existing human-in-the-loop
adaptation approaches do not fully cover these aspects, we investigate
alternative human-in-the-loop strategies by using a combination of digital
twins and virtual reality (VR) interfaces. Based on the concept of the digital
twin, we represent a self-adaptive system and its respective context in a
virtual environment. With the help of a VR interface, we support an immersive
and realistic human involvement in the self-adaptation loop by mirroring the
physical entities of the real world to the VR interface. For integrating the
human in the decision-making and adaptation process, we have implemented and
analyzed two different human-in-the-loop strategies in VR: a procedural control
where the human can control the decision making-process and adaptations through
VR interactions (human-controlled) and a declarative control where the human
specifies the goal state and the configuration is delegated to an AI planner
(mixed-initiative). We illustrate and evaluate our approach based on an
autonomic robot system that is accessible and controlled through a VR
interface.",arxiv
http://arxiv.org/abs/2104.01384v2,2021-08-08T22:28:56Z,2021-04-03T12:16:19Z,"ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit
  of Kaldi","This paper describes the ExKaldi-RT online automatic speech recognition (ASR)
toolkit that is implemented based on the Kaldi ASR toolkit and Python language.
ExKaldi-RT provides tools for building online recognition pipelines. While
similar tools are available built on Kaldi, a key feature of ExKaldi-RT that it
works on Python, which has an easy-to-use interface that allows online ASR
system developers to develop original research, such as by applying neural
network-based signal processing and by decoding model trained with deep
learning frameworks. We performed benchmark experiments on the minimum
LibriSpeech corpus, and it showed that ExKaldi-RT could achieve competitive ASR
performance in real-time recognition.",arxiv
http://arxiv.org/abs/2009.06875v2,2021-02-02T18:57:46Z,2020-09-15T05:50:13Z,Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,"Gaze tracking is an essential component of next generation displays for
virtual reality and augmented reality applications. Traditional camera-based
gaze trackers used in next generation displays are known to be lacking in one
or multiple of the following metrics: power consumption, cost, computational
complexity, estimation accuracy, latency, and form-factor. We propose the use
of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to
traditional camera-based gaze tracking approaches while taking all of these
metrics into consideration. We begin by developing a rendering-based simulation
framework for understanding the relationship between light sources and a
virtual model eyeball. Findings from this framework are used for the placement
of LEDs and photodiodes. Our first prototype uses a neural network to obtain an
average error rate of 2.67{\deg} at 400Hz while demanding only 16mW. By
simplifying the implementation to using only LEDs, duplexed as light
transceivers, and more minimal machine learning model, namely a light-weight
supervised Gaussian process regression algorithm, we show that our second
prototype is capable of an average error rate of 1.57{\deg} at 250 Hz using 800
mW.",arxiv
http://arxiv.org/abs/1909.06493v1,2019-09-14T00:35:21Z,2019-09-14T00:35:21Z,Flight Controller Synthesis Via Deep Reinforcement Learning,"Traditional control methods are inadequate in many deployment settings
involving control of Cyber-Physical Systems (CPS). In such settings, CPS
controllers must operate and respond to unpredictable interactions, conditions,
or failure modes. Dealing with such unpredictability requires the use of
executive and cognitive control functions that allow for planning and
reasoning. Motivated by the sport of drone racing, this dissertation addresses
these concerns for state-of-the-art flight control by investigating the use of
deep neural networks to bring essential elements of higher-level cognition for
constructing low level flight controllers.
  This thesis reports on the development and release of an open source, full
solution stack for building neuro-flight controllers. This stack consists of
the methodology for constructing a multicopter digital twin for synthesize the
flight controller unique to a specific aircraft, a tuning framework for
implementing training environments (GymFC), and a firmware for the world's
first neural network supported flight controller (Neuroflight). GymFC's novel
approach fuses together the digital twinning paradigm for flight control
training to provide seamless transfer to hardware. Additionally, this thesis
examines alternative reward system functions as well as changes to the software
environment to bridge the gap between the simulation and real world deployment
environments.
  Work summarized in this thesis demonstrates that reinforcement learning is
able to be leveraged for training neural network controllers capable, not only
of maintaining stable flight, but also precision aerobatic maneuvers in real
world settings. As such, this work provides a foundation for developing the
next generation of flight control systems.",arxiv
http://arxiv.org/abs/2103.03928v1,2021-03-05T20:22:52Z,2021-03-05T20:22:52Z,Accelerator Real-time Edge AI for Distributed Systems (READS) Proposal,"Our objective will be to integrate ML into Fermilab accelerator operations
and furthermore provide an accessible framework which can also be used by a
broad range of other accelerator systems with dynamic tuning needs. We will
develop of real-time accelerator control using embedded ML on-chip hardware and
fast communication between distributed systems in this proposal. We will
demonstrate this technology for the Mu2e experiment by increasing the overall
duty factor and uptime of the experiment through two synergistic projects.
First, we will use deep reinforcement learning techniques to improve the
performance of the regulation loop through guided optimization to provide
stable proton beams extracted from the Delivery Ring to the Mu2e experiment.
This requires the development of a digital twin of the system to model the
accelerator and develop real-time ML algorithms. Second, we will use
de-blending techniques to disentangle and classify overlapping beam losses in
the Main Injector and Recycler Ring to reduce overall beam downtime in each
machine. This ML model will be deployed within a semi-autonomous operational
mode. Both applications require processing at the millisecond scale and will
share similar ML-in-hardware techniques and beam instrumentation readout
technology. A collaboration between Fermilab and Northwestern University will
pull together the talents and resources of accelerator physicists, beam
instrumentation engineers, embedded system architects, FPGA board design
experts, and ML experts to solve complex real-time accelerator controls
challenges which will enhance the physics program. More broadly, the framework
developed for Accelerator Real-time Edge AI Distributed Systems (READS) can be
applied to future projects as the accelerator complex is upgraded for the
PIP-II and DUNE era.",arxiv
http://arxiv.org/abs/1604.06195v1,2016-04-21T06:55:42Z,2016-04-21T06:55:42Z,Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",arxiv
http://arxiv.org/abs/1510.03727v1,2015-10-13T15:06:03Z,2015-10-13T15:06:03Z,SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes,"We present an open-source, real-time implementation of SemanticPaint, a
system for geometric reconstruction, object-class segmentation and learning of
3D scenes. Using our system, a user can walk into a room wearing a depth camera
and a virtual reality headset, and both densely reconstruct the 3D scene and
interactively segment the environment into object classes such as 'chair',
'floor' and 'table'. The user interacts physically with the real-world scene,
touching objects and using voice commands to assign them appropriate labels.
These user-generated labels are leveraged by an online random forest-based
machine learning algorithm, which is used to predict labels for previously
unseen parts of the scene. The entire pipeline runs in real time, and the user
stays 'in the loop' throughout the process, receiving immediate feedback about
the progress of the labelling and interacting with the scene as necessary to
refine the predicted segmentation.",arxiv
http://arxiv.org/abs/2001.06202v1,2020-01-17T09:02:36Z,2020-01-17T09:02:36Z,"FedVision: An Online Visual Object Detection Platform Powered by
  Federated Learning","Visual object detection is a computer vision-based artificial intelligence
(AI) technique which has many practical applications (e.g., fire hazard
monitoring). However, due to privacy concerns and the high cost of transmitting
video data, it is highly challenging to build object detection models on
centrally stored large training datasets following the current approach.
Federated learning (FL) is a promising approach to resolve this challenge.
Nevertheless, there currently lacks an easy to use tool to enable computer
vision application developers who are not experts in federated learning to
conveniently leverage this technology and apply it in their systems. In this
paper, we report FedVision - a machine learning engineering platform to support
the development of federated learning powered computer vision applications. The
platform has been deployed through a collaboration between WeBank and Extreme
Vision to help customers develop computer vision-based safety monitoring
solutions in smart city applications. Over four months of usage, it has
achieved significant efficiency improvement and cost reduction while removing
the need to transmit sensitive data for three major corporate customers. To the
best of our knowledge, this is the first real application of FL in computer
vision-based tasks.",arxiv
http://arxiv.org/abs/2011.06144v1,2020-11-12T01:06:17Z,2020-11-12T01:06:17Z,I-POST: Intelligent Point of Sale and Transaction System,"We propose a novel solution for the cashier problem. Current cashier
system/Point of Sale (POS) terminals can be inefficient, cumbersome and
time-consuming for the users. There is a need for a solution dependent on
modern technology and ubiquitous computing resources. We present I-POST
(Intelligent Point of Sale and Transaction) as a software system that uses
smart devices, mobile phone and state of the art machine learning algorithms to
process the user transactions in automated and real time manner. I-POST is an
automated checkout system that allows the user to walk in a store, collect his
items and exit the store. There is no need to stand and wait in a queue. The
system uses object detection and facial recognition algorithm to process the
authentication of the client and the state of the object. At point of exit, the
classifier sends the data to the backend server which execute the payments. The
system uses Convolution Neural Network (CNN) for the image recognition and
processing. CNN is a supervised learning model that has found major application
in pattern recognition problem. The current implementation uses two classifiers
that work intrinsically to authenticate the user and track the items. The model
accuracy for object recognition is 97%, the loss is 9.3%. We expect that such
systems can bring efficiency to the market and has the potential for broad and
diverse applications.",arxiv
http://arxiv.org/abs/1807.00560v3,2018-07-09T01:35:50Z,2018-07-02T09:34:34Z,Weight-importance sparse training in keyword spotting,"Large size models are implemented in recently ASR system to deal with complex
speech recognition problems. The num- ber of parameters in these models makes
them hard to deploy, especially on some resource-short devices such as car
tablet. Besides this, at most of time, ASR system is used to deal with
real-time problem such as keyword spotting (KWS). It is contradictory to the
fact that large model requires long com- putation time. To deal with this
problem, we apply some sparse algo- rithms to reduces number of parameters in
some widely used models, Deep Neural Network (DNN) KWS, which requires real
short computation time. We can prune more than 90 % even 95% of parameters in
the model with tiny effect decline. And the sparse model performs better than
baseline models which has same order number of parameters. Besides this, sparse
algorithm can lead us to find rational model size au- tomatically for certain
problem without concerning choosing an original model size.",arxiv
http://arxiv.org/abs/2009.09906v1,2020-09-21T14:26:12Z,2020-09-21T14:26:12Z,End-to-End Speaker-Dependent Voice Activity Detection,"Voice activity detection (VAD) is an essential pre-processing step for tasks
such as automatic speech recognition (ASR) and speaker recognition. A basic
goal is to remove silent segments within an audio, while a more general VAD
system could remove all the irrelevant segments such as noise and even unwanted
speech from non-target speakers. We define the task, which only detects the
speech from the target speaker, as speaker-dependent voice activity detection
(SDVAD). This task is quite common in real applications and usually implemented
by performing speaker verification (SV) on audio segments extracted from VAD.
In this paper, we propose an end-to-end neural network based approach to
address this problem, which explicitly takes the speaker identity into the
modeling process. Moreover, inference can be performed in an online fashion,
which leads to low system latency. Experiments are carried out on a
conversational telephone dataset generated from the Switchboard corpus. Results
show that our proposed online approach achieves significantly better
performance than the usual VAD/SV system in terms of both frame accuracy and
F-score. We also used our previously proposed segment-level metric for a more
comprehensive analysis.",arxiv
http://arxiv.org/abs/1805.10604v2,2018-06-27T14:04:07Z,2018-05-27T11:01:30Z,"Deployment of Customized Deep Learning based Video Analytics On
  Surveillance Cameras","This paper demonstrates the effectiveness of our customized deep learning
based video analytics system in various applications focused on security,
safety, customer analytics and process compliance. We describe our video
analytics system comprising of Search, Summarize, Statistics and real-time
alerting, and outline its building blocks. These building blocks include object
detection, tracking, face detection and recognition, human and face
sub-attribute analytics. In each case, we demonstrate how custom models trained
using data from the deployment scenarios provide considerably superior
accuracies than off-the-shelf models. Towards this end, we describe our data
processing and model training pipeline, which can train and fine-tune models
from videos with a quick turnaround time. Finally, since most of these models
are deployed on-site, it is important to have resource constrained models which
do not require GPUs. We demonstrate how we custom train resource constrained
models and deploy them on embedded devices without significant loss in
accuracy. To our knowledge, this is the first work which provides a
comprehensive evaluation of different deep learning models on various
real-world customer deployment scenarios of surveillance video analytics. By
sharing our implementation details and the experiences learned from deploying
customized deep learning models for various customers, we hope that customized
deep learning based video analytics is widely incorporated in commercial
products around the world.",arxiv
http://arxiv.org/abs/1904.07633v1,2019-04-16T13:02:01Z,2019-04-16T13:02:01Z,"HARK Side of Deep Learning -- From Grad Student Descent to Automated
  Machine Learning","Recent advancements in machine learning research, i.e., deep learning,
introduced methods that excel conventional algorithms as well as humans in
several complex tasks, ranging from detection of objects in images and speech
recognition to playing difficult strategic games. However, the current
methodology of machine learning research and consequently, implementations of
the real-world applications of such algorithms, seems to have a recurring
HARKing (Hypothesizing After the Results are Known) issue. In this work, we
elaborate on the algorithmic, economic and social reasons and consequences of
this phenomenon. We present examples from current common practices of
conducting machine learning research (e.g. avoidance of reporting negative
results) and failure of generalization ability of the proposed algorithms and
datasets in actual real-life usage. Furthermore, a potential future trajectory
of machine learning research and development from the perspective of
accountable, unbiased, ethical and privacy-aware algorithmic decision making is
discussed. We would like to emphasize that with this discussion we neither
claim to provide an exhaustive argumentation nor blame any specific institution
or individual on the raised issues. This is simply a discussion put forth by
us, insiders of the machine learning field, reflecting on us.",arxiv
http://arxiv.org/abs/2110.06428v1,2021-10-13T01:24:32Z,2021-10-13T01:24:32Z,All-neural beamformer for continuous speech separation,"Continuous speech separation (CSS) aims to separate overlapping voices from a
continuous influx of conversational audio containing an unknown number of
utterances spoken by an unknown number of speakers. A common application
scenario is transcribing a meeting conversation recorded by a microphone array.
Prior studies explored various deep learning models for time-frequency mask
estimation, followed by a minimum variance distortionless response (MVDR)
filter to improve the automatic speech recognition (ASR) accuracy. The
performance of these methods is fundamentally upper-bounded by MVDR's spatial
selectivity. Recently, the all deep learning MVDR (ADL-MVDR) model was proposed
for neural beamforming and demonstrated superior performance in a target speech
extraction task using pre-segmented input. In this paper, we further adapt
ADL-MVDR to the CSS task with several enhancements to enable end-to-end neural
beamforming. The proposed system achieves significant word error rate reduction
over a baseline spectral masking system on the LibriCSS dataset. Moreover, the
proposed neural beamformer is shown to be comparable to a state-of-the-art
MVDR-based system in real meeting transcription tasks, including AMI, while
showing potentials to further simplify the runtime implementation and reduce
the system latency with frame-wise processing.",arxiv
http://arxiv.org/abs/1905.07082v6,2021-06-26T12:14:13Z,2019-05-17T01:35:26Z,"The Audio Auditor: User-Level Membership Inference in Internet of Things
  Voice Services","With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy.",arxiv
http://arxiv.org/abs/1903.05757v1,2019-03-13T23:31:21Z,2019-03-13T23:31:21Z,"VRKitchen: an Interactive 3D Virtual Environment for Task-oriented
  Learning","One of the main challenges of advancing task-oriented learning such as visual
task planning and reinforcement learning is the lack of realistic and
standardized environments for training and testing AI agents. Previously,
researchers often relied on ad-hoc lab environments. There have been recent
advances in virtual systems built with 3D physics engines and photo-realistic
rendering for indoor and outdoor environments, but the embodied agents in those
systems can only conduct simple interactions with the world (e.g., walking
around, moving objects, etc.). Most of the existing systems also do not allow
human participation in their simulated environments. In this work, we design
and implement a virtual reality (VR) system, VRKitchen, with integrated
functions which i) enable embodied agents powered by modern AI methods (e.g.,
planning, reinforcement learning, etc.) to perform complex tasks involving a
wide range of fine-grained object manipulations in a realistic environment, and
ii) allow human teachers to perform demonstrations to train agents (i.e.,
learning from demonstration). We also provide standardized evaluation
benchmarks and data collection tools to facilitate a broad use in research on
task-oriented learning and beyond.",arxiv
http://arxiv.org/abs/2103.16938v1,2021-03-31T09:43:38Z,2021-03-31T09:43:38Z,"Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein
  GANs","Real-time estimation of actual environment depth is an essential module for
various autonomous system tasks such as localization, obstacle detection and
pose estimation. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks yielded successful
approaches for realistic depth synthesis out of a simple RGB modality. While
most of these models rest on paired depth data or availability of video
sequences and stereo images, there is a lack of methods facing single-image
depth synthesis in an unsupervised manner. Therefore, in this study, latest
advancements in the field of generative neural networks are leveraged to fully
unsupervised single-image depth synthesis. To be more exact, two
cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are
implemented and simultaneously optimized using the Wasserstein-1 distance. To
ensure plausibility of the proposed method, we apply the models to a self
acquised industrial data set as well as to the renown NYU Depth v2 data set,
which allows comparison with existing approaches. The observed success in this
study suggests high potential for unpaired single-image depth estimation in
real world applications.",arxiv
http://arxiv.org/abs/1911.07845v3,2020-09-15T01:07:47Z,2019-11-18T02:28:04Z,Neural Random Subspace,"The random subspace method, known as the pillar of random forests, is good at
making precise and robust predictions. However, there is not a straightforward
way yet to combine it with deep learning. In this paper, we therefore propose
Neural Random Subspace (NRS), a novel deep learning based random subspace
method. In contrast to previous forest methods, NRS enjoys the benefits of
end-to-end, data-driven representation learning, as well as pervasive support
from deep learning software and hardware platforms, hence achieving faster
inference speed and higher accuracy. Furthermore, as a non-linear component to
be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear
feature representations in CNNs more efficiently than previous higher-order
pooling methods, producing good results with negligible increase in parameters,
floating point operations (FLOPs) and real running time. Compared with random
subspaces, random forests and gradient boosting decision trees (GBDTs), NRS
achieves superior performance on 35 machine learning datasets. Moreover, on
both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN
architectures achieves consistent improvements with minor extra cost. Code is
available at https://github.com/CupidJay/NRS_pytorch.",arxiv
http://arxiv.org/abs/1609.06612v1,2016-09-21T15:59:59Z,2016-09-21T15:59:59Z,Multimedia Communication Quality Assessment Testbeds,"We make an intensive use of multimedia frameworks in our research on modeling
the perceived quality estimation in streaming services and real-time
communications. In our preliminary work, we have used the VLC VOD software to
generate reference audiovisual files with various degree of coding and network
degradations. We have successfully built machine learning based models on the
subjective quality dataset we have generated using these files. However,
imperfections in the dataset introduced by the multimedia framework we have
used prevented us from achieving the full potential of these models.
  In order to develop better models, we have re-created our end-to-end
multimedia pipeline using the GStreamer framework for audio and video
streaming. A GStreamer based pipeline proved to be significantly more robust to
network degradations than the VLC VOD framework and allowed us to stream a
video flow at a loss rate up to 5\% packet very easily. GStreamer has also
enabled us to collect the relevant RTCP statistics that proved to be more
accurate than network-deduced information. This dataset is free to the public.
The accuracy of the statistics eventually helped us to generate better
performing perceived quality estimation models.
  In this paper, we present the implementation of these VLC and GStreamer-based
multimedia communication quality assessment testbeds with the references to
their publicly available code bases.",arxiv
http://arxiv.org/abs/2101.00793v2,2021-01-05T07:28:38Z,2021-01-04T06:16:52Z,"A Framework for Fast Scalable BNN Inference using Googlenet and Transfer
  Learning","Efficient and accurate object detection in video and image analysis is one of
the major beneficiaries of the advancement in computer vision systems with the
help of deep learning. With the aid of deep learning, more powerful tools
evolved, which are capable to learn high-level and deeper features and thus can
overcome the existing problems in traditional architectures of object detection
algorithms. The work in this thesis aims to achieve high accuracy in object
detection with good real-time performance.
  In the area of computer vision, a lot of research is going into the area of
detection and processing of visual information, by improving the existing
algorithms. The binarized neural network has shown high performance in various
vision tasks such as image classification, object detection, and semantic
segmentation. The Modified National Institute of Standards and Technology
database (MNIST), Canadian Institute for Advanced Research (CIFAR), and Street
View House Numbers (SVHN) datasets are used which is implemented using a
pre-trained convolutional neural network (CNN) that is 22 layers deep.
Supervised learning is used in the work, which classifies the particular
dataset with the proper structure of the model. In still images, to improve
accuracy, Googlenet is used. The final layer of the Googlenet is replaced with
the transfer learning to improve the accuracy of the Googlenet. At the same
time, the accuracy in moving images can be maintained by transfer learning
techniques. Hardware is the main backbone for any model to obtain faster
results with a large number of datasets. Here, Nvidia Jetson Nano is used which
is a graphics processing unit (GPU), that can handle a large number of
computations in the process of object detection. Results show that the accuracy
of objects detected by the transfer learning method is more when compared to
the existing methods.",arxiv
http://arxiv.org/abs/2109.09828v1,2021-09-20T20:17:40Z,2021-09-20T20:17:40Z,iRNN: Integer-only Recurrent Neural Network,"Recurrent neural networks (RNN) are used in many real-world text and speech
applications. They include complex modules such as recurrence,
exponential-based activation, gate interaction, unfoldable normalization,
bi-directional dependence, and attention. The interaction between these
elements prevents running them on integer-only operations without a significant
performance drop. Deploying RNNs that include layer normalization and attention
on integer-only arithmetic is still an open problem. We present a
quantization-aware training method for obtaining a highly accurate integer-only
recurrent neural network (iRNN). Our approach supports layer normalization,
attention, and an adaptive piecewise linear approximation of activations, to
serve a wide range of RNNs on various applications. The proposed method is
proven to work on RNN-based language models and automatic speech recognition.
Our iRNN maintains similar performance as its full-precision counterpart, their
deployment on smartphones improves the runtime performance by $2\times$, and
reduces the model size by $4\times$.",arxiv
http://arxiv.org/abs/1612.05571v1,2016-12-16T17:57:15Z,2016-12-16T17:57:15Z,Delta Networks for Optimized Recurrent Network Computation,"Many neural networks exhibit stability in their activation patterns over time
in response to inputs from sensors operating under real-world conditions. By
capitalizing on this property of natural signals, we propose a Recurrent Neural
Network (RNN) architecture called a delta network in which each neuron
transmits its value only when the change in its activation exceeds a threshold.
The execution of RNNs as delta networks is attractive because their states must
be stored and fetched at every timestep, unlike in convolutional neural
networks (CNNs). We show that a naive run-time delta network implementation
offers modest improvements on the number of memory accesses and computes, but
optimized training techniques confer higher accuracy at higher speedup. With
these optimizations, we demonstrate a 9X reduction in cost with negligible loss
of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on
the large Wall Street Journal speech recognition benchmark even existing
networks can be greatly accelerated as delta networks, and a 5.7x improvement
with negligible loss of accuracy can be obtained through training. Finally, on
an end-to-end CNN trained for steering angle prediction in a driving dataset,
the RNN cost can be reduced by a substantial 100X.",arxiv
http://arxiv.org/abs/2010.03625v1,2020-10-07T19:54:01Z,2020-10-07T19:54:01Z,Online Safety Assurance for Deep Reinforcement Learning,"Recently, deep learning has been successfully applied to a variety of
networking problems. A fundamental challenge is that when the operational
environment for a learning-augmented system differs from its training
environment, such systems often make badly informed decisions, leading to bad
performance. We argue that safely deploying learning-driven systems requires
being able to determine, in real time, whether system behavior is coherent, for
the purpose of defaulting to a reasonable heuristic when this is not so. We
term this the online safety assurance problem (OSAP). We present three
approaches to quantifying decision uncertainty that differ in terms of the
signal used to infer uncertainty. We illustrate the usefulness of online safety
assurance in the context of the proposed deep reinforcement learning (RL)
approach to video streaming. While deep RL for video streaming bests other
approaches when the operational and training environments match, it is
dominated by simple heuristics when the two differ. Our preliminary findings
suggest that transitioning to a default policy when decision uncertainty is
detected is key to enjoying the performance benefits afforded by leveraging ML
without compromising on safety.",arxiv
http://arxiv.org/abs/1911.04469v1,2019-11-09T19:59:17Z,2019-11-09T19:59:17Z,"A Proposed Artificial intelligence Model for Real-Time Human Action
  Localization and Tracking","In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems.",arxiv
http://arxiv.org/abs/2109.02915v1,2021-09-07T08:04:02Z,2021-09-07T08:04:02Z,"Few-shot Learning in Emotion Recognition of Spontaneous Speech Using a
  Siamese Neural Network with Adaptive Sample Pair Formation","Speech-based machine learning (ML) has been heralded as a promising solution
for tracking prosodic and spectrotemporal patterns in real-life that are
indicative of emotional changes, providing a valuable window into one's
cognitive and mental state. Yet, the scarcity of labelled data in ambulatory
studies prevents the reliable training of ML models, which usually rely on
""data-hungry"" distribution-based learning. Leveraging the abundance of labelled
speech data from acted emotions, this paper proposes a few-shot learning
approach for automatically recognizing emotion in spontaneous speech from a
small number of labelled samples. Few-shot learning is implemented via a metric
learning approach through a siamese neural network, which models the relative
distance between samples rather than relying on learning absolute patterns of
the corresponding distributions of each emotion. Results indicate the
feasibility of the proposed metric learning in recognizing emotions from
spontaneous speech in four datasets, even with a small amount of labelled
samples. They further demonstrate superior performance of the proposed metric
learning compared to commonly used adaptation methods, including network
fine-tuning and adversarial learning. Findings from this work provide a
foundation for the ambulatory tracking of human emotion in spontaneous speech
contributing to the real-life assessment of mental health degradation.",arxiv
http://arxiv.org/abs/2105.02613v1,2021-05-06T12:40:28Z,2021-05-06T12:40:28Z,"Challenges and Obstacles Towards Deploying Deep Learning Models on
  Mobile Devices","From computer vision and speech recognition to forecasting trajectories in
autonomous vehicles, deep learning approaches are at the forefront of so many
domains. Deep learning models are developed using plethora of high-level,
generic frameworks and libraries. Running those models on the mobile devices
require hardware-aware optimizations and in most cases converting the models to
other formats or using a third-party framework. In reality, most of the
developed models need to undergo a process of conversion, adaptation, and, in
some cases, full retraining to match the requirements and features of the
framework that is deploying the model on the target platform. Variety of
hardware platforms with heterogeneous computing elements, from wearable devices
to high-performance GPU clusters are used to run deep learning models. In this
paper, we present the existing challenges, obstacles, and practical solutions
towards deploying deep learning models on mobile devices.",arxiv
http://arxiv.org/abs/1808.00286v1,2018-08-01T12:08:02Z,2018-08-01T12:08:02Z,Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs,"Deep Learning (DL) applications are gaining momentum in the realm of
Artificial Intelligence, particularly after GPUs have demonstrated remarkable
skills for accelerating their challenging computational requirements. Within
this context, Convolutional Neural Network (CNN) models constitute a
representative example of success on a wide set of complex applications,
particularly on datasets where the target can be represented through a
hierarchy of local features of increasing semantic complexity. In most of the
real scenarios, the roadmap to improve results relies on CNN settings involving
brute force computation, and researchers have lately proven Nvidia GPUs to be
one of the best hardware counterparts for acceleration. Our work complements
those findings with an energy study on critical parameters for the deployment
of CNNs on flagship image and video applications: object recognition and people
identification by gait, respectively. We evaluate energy consumption on four
different networks based on the two most popular ones (ResNet/AlexNet): ResNet
(167 layers), a 2D CNN (15 layers), a CaffeNet (25 layers) and a ResNetIm (94
layers) using batch sizes of 64, 128 and 256, and then correlate those with
speed-up and accuracy to determine optimal settings. Experimental results on a
multi-GPU server endowed with twin Maxwell and twin Pascal Titan X GPUs
demonstrate that energy correlates with performance and that Pascal may have up
to 40% gains versus Maxwell. Larger batch sizes extend performance gains and
energy savings, but we have to keep an eye on accuracy, which sometimes shows a
preference for small batches. We expect this work to provide a preliminary
guidance for a wide set of CNN and DL applications in modern HPC times, where
the GFLOPS/w ratio constitutes the primary goal.",arxiv
http://arxiv.org/abs/2001.09346v2,2020-03-04T19:22:37Z,2020-01-25T18:43:47Z,"CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records","Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.",arxiv
http://arxiv.org/abs/2102.04932v1,2021-02-09T16:41:31Z,2021-02-09T16:41:31Z,Sparsification via Compressed Sensing for Automatic Speech Recognition,"In order to achieve high accuracy for machine learning (ML) applications, it
is essential to employ models with a large number of parameters. Certain
applications, such as Automatic Speech Recognition (ASR), however, require
real-time interactions with users, hence compelling the model to have as low
latency as possible. Deploying large scale ML applications thus necessitates
model quantization and compression, especially when running ML models on
resource constrained devices. For example, by forcing some of the model weight
values into zero, it is possible to apply zero-weight compression, which
reduces both the model size and model reading time from the memory. In the
literature, such methods are referred to as sparse pruning. The fundamental
questions are when and which weights should be forced to zero, i.e. be pruned.
In this work, we propose a compressed sensing based pruning (CSP) approach to
effectively address those questions. By reformulating sparse pruning as a
sparsity inducing and compression-error reduction dual problem, we introduce
the classic compressed sensing process into the ML model training process.
Using ASR task as an example, we show that CSP consistently outperforms
existing approaches in the literature.",arxiv
http://arxiv.org/abs/1607.06854v3,2016-09-30T17:41:41Z,2016-07-22T22:13:04Z,"Unsupervised Learning from Continuous Video in a Scalable Predictive
  Recurrent Network","Understanding visual reality involves acquiring common-sense knowledge about
countless regularities in the visual world, e.g., how illumination alters the
appearance of objects in a scene, and how motion changes their apparent spatial
relationship. These regularities are hard to label for training supervised
machine learning algorithms; consequently, algorithms need to learn these
regularities from the real world in an unsupervised way. We present a novel
network meta-architecture that can learn world dynamics from raw, continuous
video. The components of this network can be implemented using any algorithm
that possesses three key capabilities: prediction of a signal over time,
reduction of signal dimensionality (compression), and the ability to use
supplementary contextual information to inform the prediction. The presented
architecture is highly-parallelized and scalable, and is implemented using
localized connectivity, processing, and learning. We demonstrate an
implementation of this architecture where the components are built from
multi-layer perceptrons. We apply the implementation to create a system capable
of stable and robust visual tracking of objects as seen by a moving camera.
Results show performance on par with or exceeding state-of-the-art tracking
algorithms. The tracker can be trained in either fully supervised or
unsupervised-then-briefly-supervised regimes. Success of the briefly-supervised
regime suggests that the unsupervised portion of the model extracts useful
information about visual reality. The results suggest a new class of AI
algorithms that uniquely combine prediction and scalability in a way that makes
them suitable for learning from and --- and eventually acting within --- the
real world.",arxiv
http://arxiv.org/abs/2107.01001v2,2021-07-08T13:19:43Z,2021-06-03T08:35:10Z,"Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets
  Deep Reinforcement Learning","This paper investigates the problem of providing ultra-reliable and
energy-efficient virtual reality (VR) experiences for wireless mobile users. To
ensure reliable ultra-high-definition (UHD) video frame delivery to mobile
users and enhance their immersive visual experiences, a coordinated multipoint
(CoMP) transmission technique and millimeter wave (mmWave) communications are
exploited. Owing to user movement and time-varying wireless channels, the
wireless VR experience enhancement problem is formulated as a
sequence-dependent and mixed-integer problem with a goal of maximizing users'
feeling of presence (FoP) in the virtual world, subject to power consumption
constraints on access points (APs) and users' head-mounted displays (HMDs). The
problem, however, is hard to be directly solved due to the lack of users'
accurate tracking information and the sequence-dependent and mixed-integer
characteristics. To overcome this challenge, we develop a parallel echo state
network (ESN) learning method to predict users' tracking information by
training fresh and historical tracking samples separately collected by APs.
With the learnt results, we propose a deep reinforcement learning (DRL) based
optimization algorithm to solve the formulated problem. In this algorithm, we
implement deep neural networks (DNNs) as a scalable solution to produce integer
decision variables and solving a continuous power control problem to criticize
the integer decision variables. Finally, the performance of the proposed
algorithm is compared with various benchmark algorithms, and the impact of
different design parameters is also discussed. Simulation results demonstrate
that the proposed algorithm is more 4.14% energy-efficient than the benchmark
algorithms.",arxiv
http://arxiv.org/abs/1807.08775v1,2018-07-23T18:19:57Z,2018-07-23T18:19:57Z,CNN-based Facial Affect Analysis on Mobile Devices,"This paper focuses on the design, deployment and evaluation of Convolutional
Neural Network (CNN) architectures for facial affect analysis on mobile
devices. Unlike traditional CNN approaches, models deployed to mobile devices
must minimise storage requirements while retaining high performance. We
therefore propose three variants of established CNN architectures and
comparatively evaluate them on a large, in-the-wild benchmark dataset of facial
images. Our results show that the proposed architectures retain similar
performance to the dataset baseline while minimising storage requirements:
achieving 58% accuracy for eight-class emotion classification and average RMSE
of 0.39 for valence/arousal prediction. To demonstrate the feasibility of
deploying these models for real-world applications, we implement a music
recommendation interface based on predicted user affect. Although the CNN
models were not trained in the context of music recommendation, our case study
shows that: (i) the trained models achieve similar prediction performance to
the benchmark dataset, and (ii) users tend to positively rate the song
recommendations provided by the interface. Average runtime of the deployed
models on an iPhone 6S equates to ~45 fps, suggesting that the proposed
architectures are also well suited for real-time deployment on video streams.",arxiv
http://arxiv.org/abs/2007.03578v2,2020-07-08T22:53:16Z,2020-07-07T15:55:50Z,"A Vision-based Social Distancing and Critical Density Detection System
  for COVID-19","Social distancing has been proven as an effective measure against the spread
of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are
not used to tracking the required 6-feet (2-meters) distance between themselves
and their surroundings. An active surveillance system capable of detecting
distances between individuals and warning them can slow down the spread of the
deadly disease. Furthermore, measuring social density in a region of interest
(ROI) and modulating inflow can decrease social distancing violation occurrence
chance.
  On the other hand, recording data and labeling individuals who do not follow
the measures will breach individuals' rights in free-societies. Here we propose
an Artificial Intelligence (AI) based real-time social distancing detection and
warning system considering four important ethical factors: (1) the system
should never record/cache data, (2) the warnings should not target the
individuals, (3) no human supervisor should be in the detection/warning loop,
and (4) the code should be open-source and accessible to the public. Against
this backdrop, we propose using a monocular camera and deep learning-based
real-time object detectors to measure social distancing. If a violation is
detected, a non-intrusive audio-visual warning signal is emitted without
targeting the individual who breached the social distancing measure. Also, if
the social density is over a critical value, the system sends a control signal
to modulate inflow into the ROI. We tested the proposed method across
real-world datasets to measure its generality and performance. The proposed
method is ready for deployment, and our code is open-sourced.",arxiv
http://arxiv.org/abs/1707.06265v2,2017-09-22T16:31:05Z,2017-07-19T19:10:44Z,"Unsupervised Domain Adaptation for Robust Speech Recognition via
  Variational Autoencoder-Based Data Augmentation","Domain mismatch between training and testing can lead to significant
degradation in performance in many machine learning scenarios. Unfortunately,
this is not a rare situation for automatic speech recognition deployments in
real-world applications. Research on robust speech recognition can be regarded
as trying to overcome this domain mismatch issue. In this paper, we address the
unsupervised domain adaptation problem for robust speech recognition, where
both source and target domain speech are presented, but word transcripts are
only available for the source domain speech. We present novel
augmentation-based methods that transform speech in a way that does not change
the transcripts. Specifically, we first train a variational autoencoder on both
source and target domain data (without supervision) to learn a latent
representation of speech. We then transform nuisance attributes of speech that
are irrelevant to recognition by modifying the latent representations, in order
to augment labeled training data with additional data whose distribution is
more similar to the target domain. The proposed method is evaluated on the
CHiME-4 dataset and reduces the absolute word error rate (WER) by as much as
35% compared to the non-adapted baseline.",arxiv
http://arxiv.org/abs/2012.05214v2,2020-12-10T12:26:59Z,2020-12-09T18:23:21Z,E3D: Event-Based 3D Shape Reconstruction,"3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.",arxiv
http://arxiv.org/abs/1910.07083v1,2019-09-29T22:04:19Z,2019-09-29T22:04:19Z,"Occurence of A Cyber Security Eco-System: A Nature Oriented Project and
  Evaluation of An Indirect Social Experiment","Because of todays technological developments and the influence of digital
systems into every aspect of our lives, importance of cyber security improves
more and more day-by-day. Projects, educational processes and seminars realized
for this aim create and improve awareness among individuals and provide useful
tools for growing equipped generations. The aim of this study is to focus on a
cyber security eco-system, which was self-occurred within the interactive
educational environment designed under the scope of TUBITAK 4004 Nature
Education and Science Schools Projects (with the name of A Cyber Security
Adventure) with the use of important technologies such as virtual reality,
augmented reality, and artificial intelligence. The eco-system occurred within
the interactive educational process where high school students took place
caused both students and the project team to experience an indirect social
experiment environment. In this sense, it is thought that the findings and
comments presented in the study will give important ideas to everyone involved
in cyber security education, life-long learning processes, and the technology
use in software oriented educational tools.",arxiv
http://arxiv.org/abs/1910.11450v1,2019-10-24T23:00:12Z,2019-10-24T23:00:12Z,An Empirical Study of Efficient ASR Rescoring with Transformers,"Neural language models (LMs) have been proved to significantly outperform
classical n-gram LMs for language modeling due to their superior abilities to
model long-range dependencies in text and handle data sparsity problems. And
recently, well configured deep Transformers have exhibited superior performance
over shallow stack of recurrent neural network layers for language modeling.
However, these state-of-the-art deep Transformer models were mostly engineered
to be deep with high model capacity, which makes it computationally inefficient
and challenging to be deployed into large-scale real-world applications.
Therefore, it is important to develop Transformer LMs that have relatively
small model sizes, while still retaining good performance of those much larger
models. In this paper, we aim to conduct empirical study on training
Transformers with small parameter sizes in the context of ASR rescoring. By
combining techniques including subword units, adaptive softmax, large-scale
model pre-training, and knowledge distillation, we show that we are able to
successfully train small Transformer LMs with significant relative word error
rate reductions (WERR) through n-best rescoring. In particular, our experiments
on a video speech recognition dataset show that we are able to achieve WERRs
ranging from 6.46% to 7.17% while only with 5.5% to 11.9% parameter sizes of
the well-known large GPT model [1], whose WERR with rescoring on the same
dataset is 7.58%.",arxiv
http://arxiv.org/abs/2010.11884v1,2020-10-22T17:20:38Z,2020-10-22T17:20:38Z,"AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder","The ability to interpret social cues comes naturally for most people, but for
those living with Autism Spectrum Disorder (ASD), some experience a deficiency
in this area. This paper presents the development of a multimodal augmented
reality (AR) system which combines the use of computer vision and deep
convolutional neural networks (CNN) in order to assist individuals with the
detection and interpretation of facial expressions in social settings. The
proposed system, which we call AEGIS (Augmented-reality Expression Guided
Interpretation System), is an assistive technology deployable on a variety of
user devices including tablets, smartphones, video conference systems, or
smartglasses, showcasing its extreme flexibility and wide range of use cases,
to allow integration into daily life with ease. Given a streaming video camera
source, each real-world frame is passed into AEGIS, processed for facial
bounding boxes, and then fed into our novel deep convolutional time windowed
neural network (TimeConvNet). We leverage both spatial and temporal information
in order to provide an accurate expression prediction, which is then converted
into its corresponding visualization and drawn on top of the original video
frame. The system runs in real-time, requires minimal set up and is simple to
use. With the use of AEGIS, we can assist individuals living with ASD to learn
to better identify expressions and thus improve their social experiences.",arxiv
http://arxiv.org/abs/1810.00162v2,2018-10-02T20:07:32Z,2018-09-29T06:56:33Z,"NICE: Noise Injection and Clamping Estimation for Neural Network
  Quantization","Convolutional Neural Networks (CNN) are very popular in many fields including
computer vision, speech recognition, natural language processing, to name a
few. Though deep learning leads to groundbreaking performance in these domains,
the networks used are very demanding computationally and are far from real-time
even on a GPU, which is not power efficient and therefore does not suit low
power systems such as mobile devices. To overcome this challenge, some
solutions have been proposed for quantizing the weights and activations of
these networks, which accelerate the runtime significantly. Yet, this
acceleration comes at the cost of a larger error. The \uniqname method proposed
in this work trains quantized neural networks by noise injection and a learned
clamping, which improve the accuracy. This leads to state-of-the-art results on
various regression and classification tasks, e.g., ImageNet classification with
architectures such as ResNet-18/34/50 with low as 3-bit weights and
activations. We implement the proposed solution on an FPGA to demonstrate its
applicability for low power real-time applications. The implementation of the
paper is available at https://github.com/Lancer555/NICE",arxiv
http://arxiv.org/abs/2108.04465v1,2021-08-10T06:20:18Z,2021-08-10T06:20:18Z,"Industrial Digital Twins at the Nexus of NextG Wireless Networks and
  Computational Intelligence: A Survey","By amalgamating recent communication and control technologies, computing and
data analytics techniques, and modular manufacturing, Industry~4.0 promotes
integrating cyber-physical worlds through cyber-physical systems (CPS) and
digital twin (DT) for monitoring, optimization, and prognostics of industrial
processes. A DT is an emerging but conceptually different construct than CPS.
Like CPS, DT relies on communication to create a highly-consistent,
synchronized digital mirror image of the objects or physical processes. DT, in
addition, uses built-in models on this precise image to simulate, analyze,
predict, and optimize their real-time operation using feedback. DT is rapidly
diffusing in the industries with recent advances in the industrial Internet of
things (IIoT), edge and cloud computing, machine learning, artificial
intelligence, and advanced data analytics. However, the existing literature
lacks in identifying and discussing the role and requirements of these
technologies in DT-enabled industries from the communication and computing
perspective. In this article, we first present the functional aspects, appeal,
and innovative use of DT in smart industries. Then, we elaborate on this
perspective by systematically reviewing and reflecting on recent research in
next-generation (NextG) wireless technologies (e.g., 5G and beyond networks),
various tools (e.g., age of information, federated learning, data analytics),
and other promising trends in networked computing (e.g., edge and cloud
computing). Moreover, we discuss the DT deployment strategies at different
industrial communication layers to meet the monitoring and control requirements
of industrial applications. We also outline several key reflections and future
research challenges and directions to facilitate industrial DT's adoption.",arxiv
http://arxiv.org/abs/1710.09860v2,2018-04-12T13:20:15Z,2017-10-26T18:40:45Z,DoShiCo Challenge: Domain Shift in Control Prediction,"Training deep neural network policies end-to-end for real-world applications
so far requires big demonstration datasets in the real world or big sets
consisting of a large variety of realistic and closely related 3D CAD models.
These real or virtual data should, moreover, have very similar characteristics
to the conditions expected at test time. These stringent requirements and the
time consuming data collection processes that they entail, are currently the
most important impediment that keeps deep reinforcement learning from being
deployed in real-world applications. Therefore, in this work we advocate an
alternative approach, where instead of avoiding any domain shift by carefully
selecting the training data, the goal is to learn a policy that can cope with
it. To this end, we propose the DoShiCo challenge: to train a model in very
basic synthetic environments, far from realistic, in a way that it can be
applied in more realistic environments as well as take the control decisions on
real-world data. In particular, we focus on the task of collision avoidance for
drones. We created a set of simulated environments that can be used as
benchmark and implemented a baseline method, exploiting depth prediction as an
auxiliary task to help overcome the domain shift. Even though the policy is
trained in very basic environments, it can learn to fly without collisions in a
very different realistic simulated environment. Of course several benchmarks
for reinforcement learning already exist - but they never include a large
domain shift. On the other hand, several benchmarks in computer vision focus on
the domain shift, but they take the form of a static datasets instead of
simulated environments. In this work we claim that it is crucial to take the
two challenges together in one benchmark.",arxiv
http://arxiv.org/abs/1712.01192v1,2017-12-04T16:54:12Z,2017-12-04T16:54:12Z,"Mixed-precision training of deep neural networks using computational
  memory","Deep neural networks have revolutionized the field of machine learning by
providing unprecedented human-like performance in solving many real-world
problems such as image and speech recognition. Training of large DNNs, however,
is a computationally intensive task, and this necessitates the development of
novel computing architectures targeting this application. A computational
memory unit where resistive memory devices are organized in crossbar arrays can
be used to locally store the synaptic weights in their conductance states. The
expensive multiply accumulate operations can be performed in place using
Kirchhoff's circuit laws in a non-von Neumann manner. However, a key challenge
remains the inability to alter the conductance states of the devices in a
reliable manner during the weight update process. We propose a mixed-precision
architecture that combines a computational memory unit storing the synaptic
weights with a digital processing unit and an additional memory unit
accumulating weight updates in high precision. The new architecture delivers
classification accuracies comparable to those of floating-point implementations
without being constrained by challenges associated with the non-ideal weight
update characteristics of emerging resistive memories. A two layer neural
network in which the computational memory unit is realized using non-linear
stochastic models of phase-change memory devices achieves a test accuracy of
97.40% on the MNIST handwritten digit classification problem.",arxiv
http://arxiv.org/abs/2105.08205v1,2021-05-18T00:01:27Z,2021-05-18T00:01:27Z,Reinforcement Learning for Adaptive Video Compressive Sensing,"We apply reinforcement learning to video compressive sensing to adapt the
compression ratio. Specifically, video snapshot compressive imaging (SCI),
which captures high-speed video using a low-speed camera is considered in this
work, in which multiple (B) video frames can be reconstructed from a snapshot
measurement. One research gap in previous studies is how to adapt B in the
video SCI system for different scenes. In this paper, we fill this gap
utilizing reinforcement learning (RL). An RL model, as well as various
convolutional neural networks for reconstruction, are learned to achieve
adaptive sensing of video SCI systems. Furthermore, the performance of an
object detection network using directly the video SCI measurements without
reconstruction is also used to perform RL-based adaptive video compressive
sensing. Our proposed adaptive SCI method can thus be implemented in low cost
and real time. Our work takes the technology one step further towards real
applications of video SCI.",arxiv
http://arxiv.org/abs/1707.06600v2,2017-09-06T17:32:44Z,2017-07-20T16:35:02Z,"A multi-agent reinforcement learning model of common-pool resource
  appropriation","Humanity faces numerous problems of common-pool resource appropriation. This
class of multi-agent social dilemma includes the problems of ensuring
sustainable use of fresh water, common fisheries, grazing pastures, and
irrigation systems. Abstract models of common-pool resource appropriation based
on non-cooperative game theory predict that self-interested agents will
generally fail to find socially positive equilibria---a phenomenon called the
tragedy of the commons. However, in reality, human societies are sometimes able
to discover and implement stable cooperative solutions. Decades of behavioral
game theory research have sought to uncover aspects of human behavior that make
this possible. Most of that work was based on laboratory experiments where
participants only make a single choice: how much to appropriate. Recognizing
the importance of spatial and temporal resource dynamics, a recent trend has
been toward experiments in more complex real-time video game-like environments.
However, standard methods of non-cooperative game theory can no longer be used
to generate predictions for this case. Here we show that deep reinforcement
learning can be used instead. To that end, we study the emergent behavior of
groups of independently learning agents in a partially observed Markov game
modeling common-pool resource appropriation. Our experiments highlight the
importance of trial-and-error learning in common-pool resource appropriation
and shed light on the relationship between exclusion, sustainability, and
inequality.",arxiv
http://arxiv.org/abs/2103.03898v2,2021-11-17T10:45:24Z,2021-03-05T19:06:15Z,Reducing cybersickness in 360-degree virtual reality,"Despite the technological advancements in Virtual Reality (VR), users are
constantly combating feelings of nausea and disorientation, the so called
cybersickness. Cybersickness symptoms cause severe discomfort and hinder the
immersive VR experience. Here we investigated cybersickness in 360-degree
head-mounted display VR. In traditional 360-degree VR experiences,
translational movement in the real world is not reflected in the virtual world,
and therefore self-motion information is not corroborated by matching visual
and vestibular cues, which may trigger symptoms of cybersickness. We have
evaluated whether a new Artificial Intelligence (AI) software designed to
supplement the 360-degree VR experience with artificial 6-degrees-of-freedom
motion may reduce cybersickness. Explicit (simulator sickness questionnaire and
fast motion sickness rating) and implicit (heart rate) measurements were used
to evaluate cybersickness symptoms during and after 360-degree VR exposure.
Simulator sickness scores showed a significant reduction in feelings of nausea
during the AI supplemented 6-degrees-of-freedom motion VR compared to
traditional 360-degree VR. However, 6-degrees-of-freedom motion VR did not
reduce oculomotor or disorientation measures of sickness. No changes have been
observed in fast motion sickness and heart rate measures. Improving the
congruency between visual and vestibular cues in 360-degree VR, as provided by
the AI supplemented 6-degrees-of-freedom motion system considered, is essential
to provide a more engaging, immersive and safe VR, which is critical for
educational, cultural and entertainment applications.",arxiv
http://arxiv.org/abs/2008.13222v2,2020-12-09T08:19:43Z,2020-08-30T17:29:19Z,Improved Lite Audio-Visual Speech Enhancement,"Numerous studies have investigated the effectiveness of audio-visual
multimodal learning for speech enhancement (AVSE) tasks, seeking a solution
that uses visual data as auxiliary and complementary input to reduce the noise
of noisy speech signals. Recently, we proposed a lite audio-visual speech
enhancement (LAVSE) algorithm. Compared to conventional AVSE systems, LAVSE
requires less online computation and moderately solves the user privacy problem
on facial data. In this study, we extend LAVSE to improve its ability to
address three practical issues often encountered in implementing AVSE systems,
namely, the requirement for additional visual data, audio-visual
asynchronization, and low-quality visual data. The proposed system is termed
improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network
architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin
speech with video dataset. Experimental results confirm that compared to
conventional AVSE systems, iLAVSE can effectively overcome the aforementioned
three practical issues and can improve enhancement performance. The results
also confirm that iLAVSE is suitable for real-world scenarios, where
high-quality audio-visual sensors may not always be available.",arxiv
http://arxiv.org/abs/1805.10190v3,2018-12-06T16:34:25Z,2018-05-25T15:04:17Z,"Snips Voice Platform: an embedded Spoken Language Understanding system
  for private-by-design voice interfaces","This paper presents the machine learning architecture of the Snips Voice
Platform, a software solution to perform Spoken Language Understanding on
microprocessors typical of IoT devices. The embedded inference is fast and
accurate while enforcing privacy by design, as no personal user data is ever
collected. Focusing on Automatic Speech Recognition and Natural Language
Understanding, we detail our approach to training high-performance Machine
Learning models that are small enough to run in real-time on small devices.
Additionally, we describe a data generation procedure that provides sufficient,
high-quality training data without compromising user privacy.",arxiv
http://arxiv.org/abs/1910.09281v2,2019-11-11T14:18:31Z,2019-10-21T12:06:28Z,Dealing with Sparse Rewards in Reinforcement Learning,"Successfully navigating a complex environment to obtain a desired outcome is
a difficult task, that up to recently was believed to be capable only by
humans. This perception has been broken down over time, especially with the
introduction of deep reinforcement learning, which has greatly increased the
difficulty of tasks that can be automated. However, for traditional
reinforcement learning agents this requires an environment to be able to
provide frequent extrinsic rewards, which are not known or accessible for many
real-world environments. This project aims to explore and contrast existing
reinforcement learning solutions that circumnavigate the difficulties of an
environment that provide sparse rewards. Different reinforcement solutions will
be implemented over a several video game environments with varying difficulty
and varying frequency of rewards, as to properly investigate the applicability
of these solutions. This project introduces a novel reinforcement learning
solution by combining aspects of two existing state of the art sparse reward
solutions, curiosity driven exploration and unsupervised auxiliary tasks.",arxiv
http://arxiv.org/abs/2007.13678v1,2020-07-10T20:55:11Z,2020-07-10T20:55:11Z,"Cloud Detection through Wavelet Transforms in Machine Learning and Deep
  Learning","Cloud detection is a specialized application of image recognition and object
detection using remotely sensed data. The task presents a number of challenges,
including analyzing images obtained in visible, infrared and multi-spectral
frequencies, usually without ground truth data for comparison. Moreover,
machine learning and deep learning (MLDL) algorithms applied to this task are
required to be computationally efficient, as they are typically deployed in
low-power devices and called to operate in real-time.
  This paper explains Wavelet Transform (WT) theory, comparing it to more
widely used image and signal processing transforms, and explores the use of WT
as a powerful signal compressor and feature extractor for MLDL classifiers.",arxiv
http://arxiv.org/abs/2006.13378v2,2020-08-02T16:06:12Z,2020-06-23T23:11:30Z,A Benchmarking Framework for Interactive 3D Applications in the Cloud,"With the growing popularity of cloud gaming and cloud virtual reality (VR),
interactive 3D applications have become a major type of workloads for the
cloud. However, despite their growing importance, there is limited public
research on how to design cloud systems to efficiently support these
applications, due to the lack of an open and reliable research infrastructure,
including benchmarks and performance analysis tools. The challenges of
generating human-like inputs under various system/application randomness and
dissecting the performance of complex graphics systems make it very difficult
to design such an infrastructure. In this paper, we present the design of a
novel cloud graphics rendering research infrastructure, Pictor. Pictor employs
AI to mimic human interactions with complex 3D applications. It can also
provide in-depth performance measurements for the complex software and hardware
stack used for cloud 3D graphics rendering. With Pictor, we designed a
benchmark suite with six interactive 3D applications. Performance analyses were
conducted with these benchmarks to characterize 3D applications in the cloud
and reveal new performance bottlenecks. To demonstrate the effectiveness of
Pictor, we also implemented two optimizations to address two performance
bottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering
system, which improved the frame rate by 57.7% on average.",arxiv
http://arxiv.org/abs/2002.09821v1,2020-02-23T03:51:08Z,2020-02-23T03:51:08Z,"A Multi-view CNN-based Acoustic Classification System for Automatic
  Animal Species Identification","Automatic identification of animal species by their vocalization is an
important and challenging task. Although many kinds of audio monitoring system
have been proposed in the literature, they suffer from several disadvantages
such as non-trivial feature selection, accuracy degradation because of
environmental noise or intensive local computation. In this paper, we propose a
deep learning based acoustic classification framework for Wireless Acoustic
Sensor Network (WASN). The proposed framework is based on cloud architecture
which relaxes the computational burden on the wireless sensor node. To improve
the recognition accuracy, we design a multi-view Convolution Neural Network
(CNN) to extract the short-, middle-, and long-term dependencies in parallel.
The evaluation on two real datasets shows that the proposed architecture can
achieve high accuracy and outperforms traditional classification systems
significantly when the environmental noise dominate the audio signal (low SNR).
Moreover, we implement and deploy the proposed system on a testbed and analyse
the system performance in real-world environments. Both simulation and
real-world evaluation demonstrate the accuracy and robustness of the proposed
acoustic classification system in distinguishing species of animals.",arxiv
http://arxiv.org/abs/1610.00552v1,2016-09-30T10:44:32Z,2016-09-30T10:44:32Z,FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks,"In this paper, a neural network based real-time speech recognition (SR)
system is developed using an FPGA for very low-power operation. The implemented
system employs two recurrent neural networks (RNNs); one is a
speech-to-character RNN for acoustic modeling (AM) and the other is for
character-level language modeling (LM). The system also employs a statistical
word-level LM to improve the recognition accuracy. The results of the AM, the
character-level LM, and the word-level LM are combined using a fairly simple
N-best search algorithm instead of the hidden Markov model (HMM) based network.
The RNNs are implemented using massively parallel processing elements (PEs) for
low latency and high throughput. The weights are quantized to 6 bits to store
all of them in the on-chip memory of an FPGA. The proposed algorithm is
implemented on a Xilinx XC7Z045, and the system can operate much faster than
real-time.",arxiv
http://arxiv.org/abs/1812.07221v1,2018-12-18T08:11:00Z,2018-12-18T08:11:00Z,"Continuous Trajectory Planning Based on Learning Optimization in High
  Dimensional Input Space for Serial Manipulators","To continuously generate trajectories for serial manipulators with high
dimensional degrees of freedom (DOF) in the dynamic environment, a real-time
optimal trajectory generation method based on machine learning aiming at high
dimensional inputs is presented in this paper. First, a learning optimization
(LO) framework is established, and implementations with different sub-methods
are discussed. Additionally, multiple criteria are defined to evaluate the
performance of LO models. Furthermore, aiming at high dimensional inputs, a
database generation method based on input space dimension-reducing mapping is
proposed. At last, this method is validated on motion planning for haptic
feedback manipulators (HFM) in virtual reality systems. Results show that the
input space dimension-reducing method can significantly elevate the efficiency
and quality of database generation and consequently improve the performance of
the LO. Moreover, using this LO method, real-time trajectory generation with
high dimensional inputs can be achieved, which lays a foundation for continuous
trajectory planning for high-DOF-robots in complex environments.",arxiv
http://arxiv.org/abs/1710.07368v1,2017-10-19T23:03:33Z,2017-10-19T23:03:33Z,"SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time
  Road-Object Segmentation from 3D LiDAR Point Cloud","In this paper, we address semantic segmentation of road-objects from 3D LiDAR
point clouds. In particular, we wish to detect and categorize instances of
interest, such as cars, pedestrians and cyclists. We formulate this problem as
a point- wise classification problem, and propose an end-to-end pipeline called
SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a
transformed LiDAR point cloud as input and directly outputs a point-wise label
map, which is then refined by a conditional random field (CRF) implemented as a
recurrent layer. Instance-level labels are then obtained by conventional
clustering algorithms. Our CNN model is trained on LiDAR point clouds from the
KITTI dataset, and our point-wise segmentation labels are derived from 3D
bounding boxes from KITTI. To obtain extra training data, we built a LiDAR
simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize
large amounts of realistic training data. Our experiments show that SqueezeSeg
achieves high accuracy with astonishingly fast and stable runtime (8.7 ms per
frame), highly desirable for autonomous driving applications. Furthermore,
additionally training on synthesized data boosts validation accuracy on
real-world data. Our source code and synthesized data will be open-sourced.",arxiv
http://arxiv.org/abs/1908.07517v1,2019-08-20T03:50:57Z,2019-08-20T03:50:57Z,AI for Earth: Rainforest Conservation by Acoustic Surveillance,"Saving rainforests is a key to halting adverse climate changes. In this
paper, we introduce an innovative solution built on acoustic surveillance and
machine learning technologies to help rainforest conservation. In particular,
We propose new convolutional neural network (CNN) models for environmental
sound classification and achieved promising preliminary results on two
datasets, including a public audio dataset and our real rainforest sound
dataset. The proposed audio classification models can be easily extended in an
automated machine learning paradigm and integrated in cloud-based services for
real world deployment.",arxiv
http://arxiv.org/abs/2104.14870v1,2021-04-30T09:53:28Z,2021-04-30T09:53:28Z,"Action in Mind: A Neural Network Approach to Action Recognition and
  Segmentation","Recognizing and categorizing human actions is an important task with
applications in various fields such as human-robot interaction, video analysis,
surveillance, video retrieval, health care system and entertainment industry.
This thesis presents a novel computational approach for human action
recognition through different implementations of multi-layer architectures
based on artificial neural networks. Each system level development is designed
to solve different aspects of the action recognition problem including online
real-time processing, action segmentation and the involvement of objects. The
analysis of the experimental results are illustrated and described in six
articles. The proposed action recognition architecture of this thesis is
composed of several processing layers including a preprocessing layer, an
ordered vector representation layer and three layers of neural networks. It
utilizes self-organizing neural networks such as Kohonen feature maps and
growing grids as the main neural network layers. Thus the architecture presents
a biological plausible approach with certain features such as topographic
organization of the neurons, lateral interactions, semi-supervised learning and
the ability to represent high dimensional input space in lower dimensional
maps. For each level of development the system is trained with the input data
consisting of consecutive 3D body postures and tested with generalized input
data that the system has never met before. The experimental results of
different system level developments show that the system performs well with
quite high accuracy for recognizing human actions.",arxiv
http://arxiv.org/abs/2105.08886v2,2021-10-30T14:21:58Z,2021-05-19T02:11:43Z,"Towards Trusted and Intelligent Cyber-Physical Systems: A
  Security-by-Design Approach","The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a
mechanism that can evaluate the operational behaviour and security without
negatively affecting the operation of live systems. In this regard, Digital
Twins (DTs) are revolutionizing the CPSs. DTs strengthen the security of CPSs
throughout the product lifecycle, while assuming that the DT data is trusted,
providing agility to predict and respond to real-time changes. However,
existing DTs solutions in CPS are constrained with untrustworthy data
dissemination among multiple stakeholders and timely course correction. Such
limitations reinforce the significance of designing trustworthy distributed
solutions with the ability to create actionable insights in real-time. To do
so, we propose a framework that focuses on trusted and intelligent DT by
integrating blockchain and Artificial Intelligence (AI). Following a hybrid
approach, the proposed framework not only acquires process knowledge from the
specifications of the CPS, but also relies on AI to learn security threats
based on sensor data. Furthermore, we integrate blockchain to safeguard product
lifecycle data. We discuss the applicability of the proposed framework for the
automotive industry as a CPS use case. Finally, we identify the open challenges
that impede the implementation of intelligence-driven architectures in CPSs.",arxiv
http://arxiv.org/abs/2105.05092v1,2021-05-11T14:44:12Z,2021-05-11T14:44:12Z,"DeepLight: Robust & Unobtrusive Real-time Screen-Camera Communication
  for Real-World Displays","The paper introduces a novel, holistic approach for robust Screen-Camera
Communication (SCC), where video content on a screen is visually encoded in a
human-imperceptible fashion and decoded by a camera capturing images of such
screen content. We first show that state-of-the-art SCC techniques have two key
limitations for in-the-wild deployment: (a) the decoding accuracy drops rapidly
under even modest screen extraction errors from the captured images, and (b)
they generate perceptible flickers on common refresh rate screens even with
minimal modulation of pixel intensity. To overcome these challenges, we
introduce DeepLight, a system that incorporates machine learning (ML) models in
the decoding pipeline to achieve humanly-imperceptible, moderately high SCC
rates under diverse real-world conditions. Deep-Light's key innovation is the
design of a Deep Neural Network (DNN) based decoder that collectively decodes
all the bits spatially encoded in a display frame, without attempting to
precisely isolate the pixels associated with each encoded bit. In addition,
DeepLight supports imperceptible encoding by selectively modulating the
intensity of only the Blue channel, and provides reasonably accurate screen
extraction (IoU values >= 83%) by using state-of-the-art object detection DNN
pipelines. We show that a fully functional DeepLight system is able to robustly
achieve high decoding accuracy (frame error rate < 0.2) and moderately-high
data goodput (>=0.95Kbps) using a human-held smartphone camera, even over
larger screen-camera distances (approx =2m).",arxiv
http://arxiv.org/abs/2001.09778v2,2020-02-06T14:46:51Z,2020-01-22T15:39:42Z,"Artificial intelligence in medicine and healthcare: a review and
  classification of current and near-future applications and their ethical and
  social Impact","This paper provides an overview of the current and near-future applications
of Artificial Intelligence (AI) in Medicine and Health Care and presents a
classification according to their ethical and societal aspects, potential
benefits and pitfalls, and issues that can be considered controversial and are
not deeply discussed in the literature.
  This work is based on an analysis of the state of the art of research and
technology, including existing software, personal monitoring devices, genetic
tests and editing tools, personalized digital models, online platforms,
augmented reality devices, and surgical and companion robotics. Motivated by
our review, we present and describe the notion of 'extended personalized
medicine', we then review existing applications of AI in medicine and
healthcare and explore the public perception of medical AI systems, and how
they show, simultaneously, extraordinary opportunities and drawbacks that even
question fundamental medical concepts. Many of these topics coincide with
urgent priorities recently defined by the World Health Organization for the
coming decade. In addition, we study the transformations of the roles of
doctors and patients in an age of ubiquitous information, identify the risk of
a division of Medicine into 'fake-based', 'patient-generated', and
'scientifically tailored', and draw the attention of some aspects that need
further thorough analysis and public debate.",arxiv
http://arxiv.org/abs/2105.01777v1,2021-05-04T21:48:18Z,2021-05-04T21:48:18Z,"PathBench: A Benchmarking Platform for Classical and Learned Path
  Planning Algorithms","Path planning is a key component in mobile robotics. A wide range of path
planning algorithms exist, but few attempts have been made to benchmark the
algorithms holistically or unify their interface. Moreover, with the recent
advances in deep neural networks, there is an urgent need to facilitate the
development and benchmarking of such learning-based planning algorithms. This
paper presents PathBench, a platform for developing, visualizing, training,
testing, and benchmarking of existing and future, classical and learned 2D and
3D path planning algorithms, while offering support for Robot Oper-ating System
(ROS). Many existing path planning algorithms are supported; e.g. A*,
wavefront, rapidly-exploring random tree, value iteration networks, gated path
planning networks; and integrating new algorithms is easy and clearly
specified. We demonstrate the benchmarking capability of PathBench by comparing
implemented classical and learned algorithms for metrics, such as path length,
success rate, computational time and path deviation. These evaluations are done
on built-in PathBench maps and external path planning environments from video
games and real world databases. PathBench is open source.",arxiv
http://arxiv.org/abs/1610.07862v2,2016-10-26T02:32:30Z,2016-10-24T02:15:46Z,Intelligence in Artificial Intelligence,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",arxiv
http://arxiv.org/abs/1812.07106v1,2018-12-12T22:22:16Z,2018-12-12T22:22:16Z,"E-RNN: Design Optimization for Efficient Recurrent Neural Networks in
  FPGAs","Recurrent Neural Networks (RNNs) are becoming increasingly important for time
series-related applications which require efficient and real-time
implementations. The two major types are Long Short-Term Memory (LSTM) and
Gated Recurrent Unit (GRU) networks. It is a challenging task to have
real-time, efficient, and accurate hardware RNN implementations because of the
high sensitivity to imprecision accumulation and the requirement of special
activation function implementations.
  A key limitation of the prior works is the lack of a systematic design
optimization framework of RNN model and hardware implementations, especially
when the block size (or compression ratio) should be jointly optimized with RNN
type, layer size, etc. In this paper, we adopt the block-circulant matrix-based
framework, and present the Efficient RNN (E-RNN) framework for FPGA
implementations of the Automatic Speech Recognition (ASR) application. The
overall goal is to improve performance/energy efficiency under accuracy
requirement. We use the alternating direction method of multipliers (ADMM)
technique for more accurate block-circulant training, and present two design
explorations providing guidance on block size and reducing RNN training trials.
Based on the two observations, we decompose E-RNN in two phases: Phase I on
determining RNN model to reduce computation and storage subject to accuracy
requirement, and Phase II on hardware implementations given RNN model,
including processing element design/optimization, quantization, activation
implementation, etc. Experimental results on actual FPGA deployments show that
E-RNN achieves a maximum energy efficiency improvement of 37.4$\times$ compared
with ESE, and more than 2$\times$ compared with C-LSTM, under the same
accuracy.",arxiv
http://arxiv.org/abs/2110.03660v1,2021-08-17T18:01:12Z,2021-08-17T18:01:12Z,"Developing Medical AI : a cloud-native audio-visual data collection
  study","Designing Artificial Intelligence (AI) solutions that can operate in
real-world situations is a highly complex task. Deploying such solutions in the
medical domain is even more challenging. The promise of using AI to improve
patient care and reduce cost has encouraged many companies to undertake such
endeavours. For our team, the goal has been to improve early identification of
deteriorating patients in the hospital. Identifying patient deterioration in
lower acuity wards relies, to a large degree on the attention and intuition of
clinicians, rather than on the presence of physiological monitoring devices. In
these care areas, an automated tool which could continuously observe patients
and notify the clinical staff of suspected deterioration, would be extremely
valuable. In order to develop such an AI-enabled tool, a large collection of
patient images and audio correlated with corresponding vital signs, past
medical history and clinical outcome would be indispensable. To the best of our
knowledge, no such public or for-pay data set currently exists. This lack of
audio-visual data led to the decision to conduct exactly such study. The main
contributions of this paper are, the description of a protocol for audio-visual
data collection study, a cloud-architecture for efficiently processing and
consuming such data, and the design of a specific data collection device.",arxiv
http://arxiv.org/abs/1809.04966v1,2018-09-13T13:53:01Z,2018-09-13T13:53:01Z,"Real-Time Lightweight Chaotic Encryption for 5G IoT Enabled Lip-Reading
  Driven Secure Hearing-Aid","Existing audio-only hearing-aids are known to perform poorly in noisy
situations where overwhelming noise is present. Next-generation audio-visual
(lip-reading driven) hearing-aids stand as a major enabler to realise more
intelligible audio. However, high data rate, low latency, low computational
complexity, and privacy are some of the major bottlenecks to the successful
deployment of such advanced hearing aids. To address these challenges, we
envision an integration of 5G Cloud-Radio Access Network, Internet of Things
(IoT), and strong privacy algorithms to fully benefit from the possibilities
these technologies have to offer. The envisioned 5G IoT enabled secure
audio-visual (AV) hearing-aid transmits the encrypted compressed AV information
and receives encrypted enhanced reconstructed speech in real-time which fully
addresses cybersecurity attacks such as location privacy and eavesdropping. For
security implementation, a real-time lightweight AV encryption is utilized. For
speech enhancement, the received AV information in the cloud is used to filter
noisy audio using both deep learning and analytical acoustic modelling
(filtering based approach). To offload the computational complexity and
real-time optimization issues, the framework runs deep learning and big data
optimization processes in the background on the cloud. Specifically, in this
work, three key contributions are reported: (1) 5G IoT enabled secure
audio-visual hearing-aid framework that aims to achieve a round-trip latency up
to 5ms with 100 Mbps datarate (2) Real-time lightweight audio-visual encryption
(3) Lip-reading driven deep learning approach for speech enhancement in the
cloud. The critical analysis in terms of both speech enhancement and AV
encryption demonstrate the potential of the envisioned technology in acquiring
high-quality speech reconstruction and secure mobile AV hearing aid
communication.",arxiv
http://arxiv.org/abs/1907.06968v1,2019-07-16T12:50:42Z,2019-07-16T12:50:42Z,"A Unified Deep Framework for Joint 3D Pose Estimation and Action
  Recognition from a Single RGB Camera","We present a deep learning-based multitask framework for joint 3D human pose
estimation and action recognition from RGB video sequences. Our approach
proceeds along two stages. In the first, we run a real-time 2D pose detector to
determine the precise pixel location of important keypoints of the body. A
two-stream neural network is then designed and trained to map detected 2D
keypoints into 3D poses. In the second, we deploy the Efficient Neural
Architecture Search (ENAS) algorithm to find an optimal network architecture
that is used for modeling the spatio-temporal evolution of the estimated 3D
poses via an image-based intermediate representation and performing action
recognition. Experiments on Human3.6M, MSR Action3D and SBU Kinect Interaction
datasets verify the effectiveness of the proposed method on the targeted tasks.
Moreover, we show that our method requires a low computational budget for
training and inference.",arxiv
http://arxiv.org/abs/2004.13172v1,2020-04-27T20:58:03Z,2020-04-27T20:58:03Z,Autoencoding Neural Networks as Musical Audio Synthesizers,"A method for musical audio synthesis using autoencoding neural networks is
proposed. The autoencoder is trained to compress and reconstruct magnitude
short-time Fourier transform frames. The autoencoder produces a spectrogram by
activating its smallest hidden layer, and a phase response is calculated using
real-time phase gradient heap integration. Taking an inverse short-time Fourier
transform produces the audio signal. Our algorithm is light-weight when
compared to current state-of-the-art audio-producing machine learning
algorithms. We outline our design process, produce metrics, and detail an
open-source Python implementation of our model.",arxiv
http://arxiv.org/abs/2005.13857v1,2020-05-28T09:15:14Z,2020-05-28T09:15:14Z,"Deep Reinforcement learning for real autonomous mobile robot navigation
  in indoor environments","Deep Reinforcement Learning has been successfully applied in various computer
games [8]. However, it is still rarely used in real-world applications,
especially for the navigation and continuous control of real mobile robots
[13]. Previous approaches lack safety and robustness and/or need a structured
environment. In this paper we present our proof of concept for autonomous
self-learning robot navigation in an unknown environment for a real robot
without a map or planner. The input for the robot is only the fused data from a
2D laser scanner and a RGB-D camera as well as the orientation to the goal. The
map of the environment is unknown. The output actions of an Asynchronous
Advantage Actor-Critic network (GA3C) are the linear and angular velocities for
the robot. The navigator/controller network is pretrained in a high-speed,
parallel, and self-implemented simulation environment to speed up the learning
process and then deployed to the real robot. To avoid overfitting, we train
relatively small networks, and we add random Gaussian noise to the input laser
data. The sensor data fusion with the RGB-D camera allows the robot to navigate
in real environments with real 3D obstacle avoidance and without the need to
fit the environment to the sensory capabilities of the robot. To further
increase the robustness, we train on environments of varying difficulties and
run 32 training instances simultaneously. Video: supplementary File / YouTube,
Code: GitHub",arxiv
http://arxiv.org/abs/1802.02138v3,2018-03-21T16:21:24Z,2018-02-05T19:32:35Z,"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT
  Devices","The prevalence of Internet of things (IoT) devices and abundance of sensor
data has created an increase in real-time data processing such as recognition
of speech, image, and video. While currently such processes are offloaded to
the computationally powerful cloud system, a localized and distributed approach
is desirable because (i) it preserves the privacy of users and (ii) it omits
the dependency on cloud services. However, IoT networks are usually composed of
resource-constrained devices, and a single device is not powerful enough to
process real-time data. To overcome this challenge, we examine data and model
parallelism for such devices in the context of deep neural networks. We propose
Musical Chair to enable efficient, localized, and dynamic real-time recognition
by harvesting the aggregated computational power from the resource-constrained
devices in the same IoT network as input sensors. Musical chair adapts to the
availability of computing devices at runtime and adjusts to the inherit
dynamics of IoT networks. To demonstrate Musical Chair, on a network of
Raspberry PIs (up to 12) each connected to a camera, we implement a
state-of-the-art action recognition model for videos and two recognition models
for images. Compared to the Tegra TX2, an embedded low-power platform with a
six-core CPU and a GPU, our distributed action recognition system achieves not
only similar energy consumption but also twice the performance of the TX2.
Furthermore, in image recognition, Musical Chair achieves similar performance
and saves dynamic energy.",arxiv
http://arxiv.org/abs/2105.01636v1,2021-05-04T17:27:59Z,2021-05-04T17:27:59Z,Learning 3D Granular Flow Simulations,"Recently, the application of machine learning models has gained momentum in
natural sciences and engineering, which is a natural fit due to the abundance
of data in these fields. However, the modeling of physical processes from
simulation data without first principle solutions remains difficult. Here, we
present a Graph Neural Networks approach towards accurate modeling of complex
3D granular flow simulation processes created by the discrete element method
LIGGGHTS and concentrate on simulations of physical systems found in real world
applications like rotating drums and hoppers. We discuss how to implement Graph
Neural Networks that deal with 3D objects, boundary conditions, particle -
particle, and particle - boundary interactions such that an accurate modeling
of relevant physical quantities is made possible. Finally, we compare the
machine learning based trajectories to LIGGGHTS trajectories in terms of
particle flows and mixing entropies.",arxiv
http://arxiv.org/abs/2107.12943v1,2021-07-27T16:59:00Z,2021-07-27T16:59:00Z,"Learning-based Prediction, Rendering and Transmission for Interactive
  Virtual Reality in RIS-Assisted Terahertz Networks","The quality of experience (QoE) requirements of wireless Virtual Reality (VR)
can only be satisfied with high data rate, high reliability, and low VR
interaction latency. This high data rate over short transmission distances may
be achieved via abundant bandwidth in the terahertz (THz) band. However, THz
waves suffer from severe signal attenuation, which may be compensated by the
reconfigurable intelligent surface (RIS) technology with programmable
reflecting elements. Meanwhile, the low VR interaction latency may be achieved
with the mobile edge computing (MEC) network architecture due to its high
computation capability. Motivated by these considerations, in this paper, we
propose a MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by
taking into account the uplink viewpoint prediction and position transmission,
MEC rendering, and downlink transmission. We propose two methods, which are
referred to as centralized online Gated Recurrent Unit (GRU) and distributed
Federated Averaging (FedAvg), to predict the viewpoints of VR users. In the
uplink, an algorithm that integrates online Long-short Term Memory (LSTM) and
Convolutional Neural Networks (CNN) is deployed to predict the locations and
the line-of-sight and non-line-of-sight statuses of the VR users over time. In
the downlink, we further develop a constrained deep reinforcement learning
algorithm to select the optimal phase shifts of the RIS under latency
constraints. Simulation results show that our proposed learning architecture
achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and
about two times improvement in QoE compared to the random phase shift selection
scheme.",arxiv
http://arxiv.org/abs/2004.05740v2,2020-11-03T03:35:35Z,2020-04-13T01:46:29Z,"Deep-Edge: An Efficient Framework for Deep Learning Model Update on
  Heterogeneous Edge","Deep Learning (DL) model-based AI services are increasingly offered in a
variety of predictive analytics services such as computer vision, natural
language processing, speech recognition. However, the quality of the DL models
can degrade over time due to changes in the input data distribution, thereby
requiring periodic model updates. Although cloud data-centers can meet the
computational requirements of the resource-intensive and time-consuming model
update task, transferring data from the edge devices to the cloud incurs a
significant cost in terms of network bandwidth and are prone to data privacy
issues. With the advent of GPU-enabled edge devices, the DL model update can be
performed at the edge in a distributed manner using multiple connected edge
devices. However, efficiently utilizing the edge resources for the model update
is a hard problem due to the heterogeneity among the edge devices and the
resource interference caused by the co-location of the DL model update task
with latency-critical tasks running in the background. To overcome these
challenges, we present Deep-Edge, a load- and interference-aware,
fault-tolerant resource management framework for performing model update at the
edge that uses distributed training. This paper makes the following
contributions. First, it provides a unified framework for monitoring,
profiling, and deploying the DL model update tasks on heterogeneous edge
devices. Second, it presents a scheduler that reduces the total re-training
time by appropriately selecting the edge devices and distributing data among
them such that no latency-critical applications experience deadline violations.
Finally, we present empirical results to validate the efficacy of the framework
using a real-world DL model update case-study based on the Caltech dataset and
an edge AI cluster testbed.",arxiv
http://arxiv.org/abs/1801.09866v1,2018-01-30T06:58:50Z,2018-01-30T06:58:50Z,"Accelerating recurrent neural network language model based online speech
  recognition system","This paper presents methods to accelerate recurrent neural network based
language models (RNNLMs) for online speech recognition systems. Firstly, a
lossy compression of the past hidden layer outputs (history vector) with
caching is introduced in order to reduce the number of LM queries. Next, RNNLM
computations are deployed in a CPU-GPU hybrid manner, which computes each layer
of the model on a more advantageous platform. The added overhead by data
exchanges between CPU and GPU is compensated through a frame-wise batching
strategy. The performance of the proposed methods evaluated on LibriSpeech test
sets indicates that the reduction in history vector precision improves the
average recognition speed by 1.23 times with minimum degradation in accuracy.
On the other hand, the CPU-GPU hybrid parallelization enables RNNLM based
real-time recognition with a four times improvement in speed.",arxiv
http://arxiv.org/abs/1804.00209v2,2019-07-10T14:21:12Z,2018-03-31T20:07:37Z,"Human-in-the-Loop Wireless Communications: Machine Learning and
  Brain-Aware Resource Management","Human-centric applications such as virtual reality and immersive gaming will
be central to the future wireless networks. Common features of such services
include: a) their dependence on the human user's behavior and state, and b)
their need for more network resources compared to conventional cellular
applications. To successfully deploy such applications over wireless and
cellular systems, the network must be made cognizant of not only the
quality-of-service (QoS) needs of the applications, but also of the perceptions
of the human users on this QoS. In this paper, by explicitly modeling the
limitations of the human brain, a concrete measure for the delay perception of
human users in a wireless network is introduced. Then, a novel learning method,
called probability distribution identification, is proposed to find a
probabilistic model for this delay perception based on the brain features of a
human user. The proposed learning method uses both supervised and unsupervised
learning techniques to build a Gaussian mixture model of the human brain
features. Given a model for the delay perception of the human brain, a novel
brain-aware resource management algorithm based on Lyapunov optimization is
proposed for allocating radio resources to human users while minimizing the
transmit power and taking into account the reliability of both machine type
devices and human users. The proposed algorithm is shown to have a low
complexity. Moreover, a closed-form relationship between the reliability
measure and wireless physical layer metrics of the network is derived.
Simulation results using real data from actual human users show that a
brain-aware approach can yield savings of up to 78% in power compared to the
system",arxiv
http://arxiv.org/abs/2010.00432v1,2020-10-01T14:27:28Z,2020-10-01T14:27:28Z,"The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep
  Learning to Radio Frequency Applications","While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces.",arxiv
http://arxiv.org/abs/1706.09453v2,2017-07-04T17:03:20Z,2017-06-28T19:41:25Z,"Toward Computation and Memory Efficient Neural Network Acoustic Models
  with Binary Weights and Activations","Neural network acoustic models have significantly advanced state of the art
speech recognition over the past few years. However, they are usually
computationally expensive due to the large number of matrix-vector
multiplications and nonlinearity operations. Neural network models also require
significant amounts of memory for inference because of the large model size.
For these two reasons, it is challenging to deploy neural network based speech
recognizers on resource-constrained platforms such as embedded devices. This
paper investigates the use of binary weights and activations for computation
and memory efficient neural network acoustic models. Compared to real-valued
weight matrices, binary weights require much fewer bits for storage, thereby
cutting down the memory footprint. Furthermore, with binary weights or
activations, the matrix-vector multiplications are turned into addition and
subtraction operations, which are computationally much faster and more energy
efficient for hardware platforms. In this paper, we study the applications of
binary weights and activations for neural network acoustic modeling, reporting
encouraging results on the WSJ and AMI corpora.",arxiv
http://arxiv.org/abs/2011.14966v1,2020-11-30T16:38:18Z,2020-11-30T16:38:18Z,"Depression Status Estimation by Deep Learning based Hybrid Multi-Modal
  Fusion Model","Preliminary detection of mild depression could immensely help in effective
treatment of the common mental health disorder. Due to the lack of proper
awareness and the ample mix of stigmas and misconceptions present within the
society, mental health status estimation has become a truly difficult task. Due
to the immense variations in character level traits from person to person,
traditional deep learning methods fail to generalize in a real world setting.
In our study we aim to create a human allied AI workflow which could
efficiently adapt to specific users and effectively perform in real world
scenarios. We propose a Hybrid deep learning approach that combines the essence
of one shot learning, classical supervised deep learning methods and human
allied interactions for adaptation. In order to capture maximum information and
make efficient diagnosis video, audio, and text modalities are utilized. Our
Hybrid Fusion model achieved a high accuracy of 96.3% on the Dataset; and
attained an AUC of 0.9682 which proves its robustness in discriminating classes
in complex real-world scenarios making sure that no cases of mild depression
are missed during diagnosis. The proposed method is deployed in a cloud-based
smartphone application for robust testing. With user-specific adaptations and
state of the art methodologies, we present a state-of-the-art model with user
friendly experience.",arxiv
http://arxiv.org/abs/2011.12715v1,2020-11-23T00:34:56Z,2020-11-23T00:34:56Z,"Resonance: Replacing Software Constants with Context-Aware Models in
  Real-time Communication","Large software systems tune hundreds of 'constants' to optimize their runtime
performance. These values are commonly derived through intuition, lab tests, or
A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best
value depends on runtime context. In this paper, we provide an experimental
approach to replace constants with learned contextual functions for Skype - a
widely used real-time communication (RTC) application. We present Resonance, a
system based on contextual bandits (CB). We describe experiences from three
real-world experiments: applying it to the audio, video, and transport
components in Skype. We surface a unique and practical challenge of performing
machine learning (ML) inference in large software systems written using
encapsulation principles. Finally, we open-source FeatureBroker, a library to
reduce the friction in adopting ML models in such development environments",arxiv
http://arxiv.org/abs/2010.12570v3,2021-07-14T12:51:12Z,2020-10-23T17:54:38Z,"Eye Tracking Data Collection Protocol for VR for Remotely Located
  Subjects using Blockchain and Smart Contracts","Eye tracking data collection in the virtual reality context is typically
carried out in laboratory settings, which usually limits the number of
participants or consumes at least several months of research time. In addition,
under laboratory settings, subjects may not behave naturally due to being
recorded in an uncomfortable environment. In this work, we propose a
proof-of-concept eye tracking data collection protocol and its implementation
to collect eye tracking data from remotely located subjects, particularly for
virtual reality using Ethereum blockchain and smart contracts. With the
proposed protocol, data collectors can collect high quality eye tracking data
from a large number of human subjects with heterogeneous socio-demographic
characteristics. The quality and the amount of data can be helpful for various
tasks in data-driven human-computer interaction and artificial intelligence.",arxiv
http://arxiv.org/abs/1904.05734v1,2019-03-18T20:10:13Z,2019-03-18T20:10:13Z,"Practical Hidden Voice Attacks against Speech and Speaker Recognition
  Systems","Voice Processing Systems (VPSes), now widely deployed, have been made
significantly more accurate through the application of recent advances in
machine learning. However, adversarial machine learning has similarly advanced
and has been used to demonstrate that VPSes are vulnerable to the injection of
hidden commands - audio obscured by noise that is correctly recognized by a VPS
but not by human beings. Such attacks, though, are often highly dependent on
white-box knowledge of a specific machine learning model and limited to
specific microphones and speakers, making their use across different acoustic
hardware platforms (and thus their practicality) limited. In this paper, we
break these dependencies and make hidden command attacks more practical through
model-agnostic (blackbox) attacks, which exploit knowledge of the signal
processing algorithms commonly used by VPSes to generate the data fed into
machine learning systems. Specifically, we exploit the fact that multiple
source audio samples have similar feature vectors when transformed by acoustic
feature extraction algorithms (e.g., FFTs). We develop four classes of
perturbations that create unintelligible audio and test them against 12 machine
learning models, including 7 proprietary models (e.g., Google Speech API, Bing
Speech API, IBM Speech API, Azure Speaker API, etc), and demonstrate successful
attacks against all targets. Moreover, we successfully use our maliciously
generated audio samples in multiple hardware configurations, demonstrating
effectiveness across both models and real systems. In so doing, we demonstrate
that domain-specific knowledge of audio signal processing represents a
practical means of generating successful hidden voice command attacks.",arxiv
http://arxiv.org/abs/2107.14569v2,2021-10-23T14:56:56Z,2021-07-30T12:08:16Z,Can You Hear It? Backdoor Attacks via Ultrasonic Triggers,"Deep neural networks represent a powerful approach for many real-world
applications due to their ability to model even complex data relations.
However, such neural networks can also be prohibitively expensive to train,
making it common to either outsource the training process to third parties or
use pretrained neural networks. Unfortunately, such practices make neural
networks vulnerable to various attacks, where one attack is the backdoor
attack. In such an attack, the third party training the model may maliciously
inject hidden behaviors into the model. Then, if a particular input (called
trigger) is fed into a neural network, the network will respond with a wrong
result.
  In this work, we explore backdoor attacks for automatic speech recognition
systems where we inject inaudible triggers. By doing so, we make the backdoor
attack challenging to detect for legitimate users, and thus, potentially more
dangerous. We conduct experiments on two versions of a dataset and three neural
networks and explore the performance of our attack concerning the duration,
position, and type of the trigger. Our results indicate that less than 1% of
poisoned data is sufficient to deploy a backdoor attack and reach a 100% attack
success rate. Since the trigger is inaudible, it makes it without limitations
with respect to the duration of the signal, and we observed that even short,
non-continuous triggers result in highly successful attacks. Finally, we
conducted our attack in actual hardware and saw that a malicious party could
manipulate inference in an Android application by playing the inaudible trigger
over the air.",arxiv
http://arxiv.org/abs/2007.13404v2,2020-10-29T16:23:12Z,2020-07-27T09:50:11Z,"YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications","Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.",arxiv
http://arxiv.org/abs/1911.06636v2,2020-06-16T09:13:58Z,2019-11-15T13:57:35Z,"Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body
  Tasks","We address the longstanding challenge of producing flexible, realistic
humanoid character controllers that can perform diverse whole-body tasks
involving object interactions. This challenge is central to a variety of
fields, from graphics and animation to robotics and motor neuroscience. Our
physics-based environment uses realistic actuation and first-person perception
-- including touch sensors and egocentric vision -- with a view to producing
active-sensing behaviors (e.g. gaze direction), transferability to real robots,
and comparisons to the biology. We develop an integrated neural-network based
approach consisting of a motor primitive module, human demonstrations, and an
instructed reinforcement learning regime with curricula and task variations. We
demonstrate the utility of our approach for several tasks, including
goal-conditioned box carrying and ball catching, and we characterize its
behavioral robustness. The resulting controllers can be deployed in real-time
on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk .",arxiv
http://arxiv.org/abs/1910.12750v1,2019-10-28T15:21:48Z,2019-10-28T15:21:48Z,"Deep-Learning-Based Image Segmentation Integrated with Optical
  Microscopy for Automatically Searching for Two-Dimensional Materials","Deep-learning algorithms enable precise image recognition based on
high-dimensional hierarchical image features. Here, we report the development
and implementation of a deep-learning-based image segmentation algorithm in an
autonomous robotic system to search for two-dimensional (2D) materials. We
trained the neural network based on Mask-RCNN on annotated optical microscope
images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm
is run on a 1024 x 1024 px2 optical microscope images for 200 ms, enabling the
real-time detection of 2D materials. The detection process is robust against
changes in the microscopy conditions, such as illumination and color balance,
which obviates the parameter-tuning process required for conventional
rule-based detection algorithms. Integrating the algorithm with a motorized
optical microscope enables the automated searching and cataloging of 2D
materials. This development will allow researchers to utilize unlimited amounts
of 2D materials simply by exfoliating and running the automated searching
process.",arxiv
http://arxiv.org/abs/1707.04610v2,2018-04-15T17:48:20Z,2017-07-14T19:05:50Z,Cloud-based or On-device: An Empirical Study of Mobile Deep Inference,"Modern mobile applications are benefiting significantly from the advancement
in deep learning, e.g., implementing real-time image recognition and
conversational system. Given a trained deep learning model, applications
usually need to perform a series of matrix operations based on the input data,
in order to infer possible output values. Because of computational complexity
and size constraints, these trained models are often hosted in the cloud. To
utilize these cloud-based models, mobile apps will have to send input data over
the network. While cloud-based deep learning can provide reasonable response
time for mobile apps, it restricts the use case scenarios, e.g. mobile apps
need to have network access. With mobile specific deep learning optimizations,
it is now possible to employ on-device inference. However, because mobile
hardware, such as GPU and memory size, can be very limited when compared to its
desktop counterpart, it is important to understand the feasibility of this new
on-device deep learning inference architecture. In this paper, we empirically
evaluate the inference performance of three Convolutional Neural Networks
(CNNs) using a benchmark Android application we developed. Our measurement and
analysis suggest that on-device inference can cost up to two orders of
magnitude greater response time and energy when compared to cloud-based
inference, and that loading model and computing probability are two performance
bottlenecks for on-device deep inferences.",arxiv
http://arxiv.org/abs/2108.08762v1,2021-08-19T16:06:16Z,2021-08-19T16:06:16Z,"Dynamic Difficulty Adjustment in Virtual Reality Exergames through
  Experience-driven Procedural Content Generation","Virtual Reality (VR) games that feature physical activities have been shown
to increase players' motivation to do physical exercise. However, for such
exercises to have a positive healthcare effect, they have to be repeated
several times a week. To maintain player motivation over longer periods of
time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the
game's challenge according to the player's capabilities. For exercise games,
this is mostly done by tuning specific in-game parameters like the speed of
objects. In this work, we propose to use experience-driven Procedural Content
Generation for DDA in VR exercise games by procedurally generating levels that
match the player's current capabilities. Not only finetuning specific
parameters but creating completely new levels has the potential to decrease
repetition over longer time periods and allows for the simultaneous adaptation
of the cognitive and physical challenge of the exergame. As a proof-of-concept,
we implement an initial prototype in which the player must traverse a maze that
includes several exercise rooms, whereby the generation of the maze is realized
by a neural network. Passing those exercise rooms requires the player to
perform physical activities. To match the player's capabilities, we use Deep
Reinforcement Learning to adjust the structure of the maze and to decide which
exercise rooms to include in the maze. We evaluate our prototype in an
exploratory user study utilizing both biodata and subjective questionnaires.",arxiv
http://arxiv.org/abs/2104.05828v1,2021-04-12T21:30:53Z,2021-04-12T21:30:53Z,"Evidence-based Prescriptive Analytics, CAUSAL Digital Twin and a
  Learning Estimation Algorithm","Evidence-based Prescriptive Analytics (EbPA) is necessary to determine
optimal operational set-points that will improve business productivity. EbPA
results from what-if analysis and counterfactual experimentation on CAUSAL
Digital Twins (CDTs) that quantify cause-effect relationships in the DYNAMICS
of a system of connected assets. We describe the basics of Causality and Causal
Graphs and develop a Learning Causal Digital Twin (LCDT) solution; our
algorithm uses a simple recurrent neural network with some innovative
modifications incorporating Causal Graph simulation. Since LCDT is a learning
digital twin where parameters are learned online in real-time with minimal
pre-configuration, the work of deploying digital twins will be significantly
simplified. A proof-of-principle of LCDT was conducted using real vibration
data from a system of bearings; results of causal factor estimation, what-if
analysis study and counterfactual experiment are very encouraging.",arxiv
http://arxiv.org/abs/1808.04752v2,2018-12-16T08:26:57Z,2018-08-13T14:11:43Z,A Survey on Methods and Theories of Quantized Neural Networks,"Deep neural networks are the state-of-the-art methods for many real-world
tasks, such as computer vision, natural language processing and speech
recognition. For all its popularity, deep neural networks are also criticized
for consuming a lot of memory and draining battery life of devices during
training and inference. This makes it hard to deploy these models on mobile or
embedded devices which have tight resource constraints. Quantization is
recognized as one of the most effective approaches to satisfy the extreme
memory requirements that deep neural network models demand. Instead of adopting
32-bit floating point format to represent weights, quantized representations
store weights using more compact formats such as integers or even binary
numbers. Despite a possible degradation in predictive performance, quantization
provides a potential solution to greatly reduce the model size and the energy
consumption. In this survey, we give a thorough review of different aspects of
quantized neural networks. Current challenges and trends of quantized neural
networks are also discussed.",arxiv
http://arxiv.org/abs/1804.09364v3,2018-12-13T15:42:35Z,2018-04-25T06:20:12Z,Driving Policy Transfer via Modularity and Abstraction,"End-to-end approaches to autonomous driving have high sample complexity and
are difficult to scale to realistic urban driving. Simulation can help
end-to-end driving systems by providing a cheap, safe, and diverse training
environment. Yet training driving policies in simulation brings up the problem
of transferring such policies to the real world. We present an approach to
transferring driving policies from simulation to reality via modularity and
abstraction. Our approach is inspired by classic driving systems and aims to
combine the benefits of modular architectures and end-to-end deep learning
approaches. The key idea is to encapsulate the driving policy such that it is
not directly exposed to raw perceptual input or low-level vehicle dynamics. We
evaluate the presented approach in simulated urban environments and in the real
world. In particular, we transfer a driving policy trained in simulation to a
1/5-scale robotic truck that is deployed in a variety of conditions, with no
finetuning, on two continents. The supplementary video can be viewed at
https://youtu.be/BrMDJqI6H5U",arxiv
http://arxiv.org/abs/1905.05316v1,2019-05-13T23:25:10Z,2019-05-13T23:25:10Z,Wireless Edge Computing with Latency and Reliability Guarantees,"Edge computing is an emerging concept based on distributing computing,
storage, and control services closer to end network nodes. Edge computing lies
at the heart of the fifth generation (5G) wireless systems and beyond. While
current state-of-the-art networks communicate, compute, and process data in a
centralized manner (at the cloud), for latency and compute-centric
applications, both radio access and computational resources must be brought
closer to the edge, harnessing the availability of computing and
storage-enabled small cell base stations in proximity to the end devices.
Furthermore, the network infrastructure must enable a distributed edge
decision-making service that learns to adapt to the network dynamics with
minimal latency and optimize network deployment and operation accordingly. This
article will provide a fresh look to the concept of edge computing by first
discussing the applications that the network edge must provide, with a special
emphasis on the ensuing challenges in enabling ultra-reliable and low-latency
edge computing services for mission-critical applications such as virtual
reality (VR), vehicle-to-everything (V2X), edge artificial intelligence (AI),
and so forth. Furthermore, several case studies where the edge is key are
explored followed by insights and prospect for future work.",arxiv
http://arxiv.org/abs/1711.05734v2,2018-02-20T21:43:55Z,2017-11-15T10:15:44Z,"Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW
  Accelerator for Near-Sensor Recurrent Neural Network Inference","Recurrent neural networks (RNNs) are state-of-the-art in voice
awareness/understanding and speech recognition. On-device computation of RNNs
on low-power mobile and wearable devices would be key to applications such as
zero-latency voice-based human-machine interfaces. Here we present Chipmunk, a
small (<1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC
65 nm technology capable to operate at a measured peak efficiency up to 3.08
Gop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring
in huge memory transfer overhead, multiple Chipmunk engines can cooperate to
form a single systolic array. In this way, the Chipmunk architecture in a 75
tiles configuration can achieve real-time phoneme extraction on a demanding RNN
topology proposed by Graves et al., consuming less than 13 mW of average power.",arxiv
http://arxiv.org/abs/2006.07644v2,2021-05-17T13:59:45Z,2020-06-13T14:12:23Z,"RoadNet-RT: High Throughput CNN Architecture and SoC Design for
  Real-Time Road Segmentation","In recent years, convolutional neural network has gained popularity in many
engineering applications especially for computer vision. In order to achieve
better performance, often more complex structures and advanced operations are
incorporated into the neural networks, which results very long inference time.
For time-critical tasks such as autonomous driving and virtual reality,
real-time processing is fundamental. In order to reach real-time process speed,
a light-weight, high-throughput CNN architecture namely RoadNet-RT is proposed
for road segmentation in this paper. It achieves 90.33% MaxF score on test set
of KITTI road segmentation task and 8 ms per frame when running on GTX 1080
GPU. Comparing to the state-of-the-art network, RoadNet-RT speeds up the
inference time by a factor of 20 at the cost of only 6.2% accuracy loss. For
hardware design optimization, several techniques such as depthwise separable
convolution and non-uniformed kernel size convolution are customized designed
to further reduce the processing time. The proposed CNN architecture has been
successfully implemented on an FPGA ZCU102 MPSoC platform that achieves the
computation capability of 83.05 GOPS. The system throughput reaches 327.9
frames per second with image size 1216x176.",arxiv
http://arxiv.org/abs/1511.03990v1,2015-11-12T17:54:46Z,2015-11-12T17:54:46Z,Automatic Inference of the Quantile Parameter,"Supervised learning is an active research area, with numerous applications in
diverse fields such as data analytics, computer vision, speech and audio
processing, and image understanding. In most cases, the loss functions used in
machine learning assume symmetric noise models, and seek to estimate the
unknown function parameters. However, loss functions such as quantile and
quantile Huber generalize the symmetric $\ell_1$ and Huber losses to the
asymmetric setting, for a fixed quantile parameter. In this paper, we propose
to jointly infer the quantile parameter and the unknown function parameters,
for the asymmetric quantile Huber and quantile losses. We explore various
properties of the quantile Huber loss and implement a convexity certificate
that can be used to check convexity in the quantile parameter. When the loss if
convex with respect to the parameter of the function, we prove that it is
biconvex in both the function and the quantile parameters, and propose an
algorithm to jointly estimate these. Results with synthetic and real data
demonstrate that the proposed approach can automatically recover the quantile
parameter corresponding to the noise and also provide an improved recovery of
function parameters. To illustrate the potential of the framework, we extend
the gradient boosting machines with quantile losses to automatically estimate
the quantile parameter at each iteration.",arxiv
http://arxiv.org/abs/1809.06244v2,2019-01-25T11:15:56Z,2018-09-14T14:29:47Z,"A Virtual Testbed for Critical Incident Investigation with Autonomous
  Remote Aerial Vehicle Surveying, Artificial Intelligence, and Decision
  Support","Autonomous robotics and artificial intelligence techniques can be used to
support human personnel in the event of critical incidents. These incidents can
pose great danger to human life. Some examples of such assistance include:
multi-robot surveying of the scene; collection of sensor data and scene
imagery, real-time risk assessment and analysis; object identification and
anomaly detection; and retrieval of relevant supporting documentation such as
standard operating procedures (SOPs). These incidents, although often rare, can
involve chemical, biological, radiological/nuclear or explosive (CBRNE)
substances and can be of high consequence. Real-world training and deployment
of these systems can be costly and sometimes not feasible. For this reason, we
have developed a realistic 3D model of a CBRNE scenario to act as a testbed for
an initial set of assisting AI tools that we have developed.",arxiv
http://arxiv.org/abs/2107.00359v1,2021-07-01T10:49:55Z,2021-07-01T10:49:55Z,"Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time
  Delays Using Reinforcement Learning","Telerobotic systems must adapt to new environmental conditions and deal with
high uncertainty caused by long-time delays. As one of the best alternatives to
human-level intelligence, Reinforcement Learning (RL) may offer a solution to
cope with these issues. This paper proposes to integrate RL with the Model
Mediated Teleoperation (MMT) concept. The teleoperator interacts with a
simulated virtual environment, which provides instant feedback. Whereas
feedback from the real environment is delayed, feedback from the model is
instantaneous, leading to high transparency. The MMT is realized in combination
with an intelligent system with two layers. The first layer utilizes Dynamic
Movement Primitives (DMP) which accounts for certain changes in the avatar
environment. And, the second layer addresses the problems caused by uncertainty
in the model using RL methods. Augmented reality was also provided to fuse the
avatar device and virtual environment models for the teleoperator. Implemented
on DLR's Exodex Adam hand-arm haptic exoskeleton, the results show RL methods
are able to find different solutions when changes are applied to the object
position after the demonstration. The results also show DMPs to be effective at
adapting to new conditions where there is no uncertainty involved.",arxiv
http://arxiv.org/abs/2110.01863v1,2021-10-05T07:55:19Z,2021-10-05T07:55:19Z,"DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge
  Computing","The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.",arxiv
http://arxiv.org/abs/2104.09056v1,2021-04-19T05:26:11Z,2021-04-19T05:26:11Z,"RingCNN: Exploiting Algebraically-Sparse Ring Tensors for
  Energy-Efficient CNN-Based Computational Imaging","In the era of artificial intelligence, convolutional neural networks (CNNs)
are emerging as a powerful technique for computational imaging. They have shown
superior quality for reconstructing fine textures from badly-distorted images
and have potential to bring next-generation cameras and displays to our daily
life. However, CNNs demand intensive computing power for generating
high-resolution videos and defy conventional sparsity techniques when rendering
dense details. Therefore, finding new possibilities in regular sparsity is
crucial to enable large-scale deployment of CNN-based computational imaging.
  In this paper, we consider a fundamental but yet well-explored approach --
algebraic sparsity -- for energy-efficient CNN acceleration. We propose to
build CNN models based on ring algebra that defines multiplication, addition,
and non-linearity for n-tuples properly. Then the essential sparsity will
immediately follow, e.g. n-times reduction for the number of real-valued
weights. We define and unify several variants of ring algebras into a modeling
framework, RingCNN, and make comparisons in terms of image quality and hardware
complexity. On top of that, we further devise a novel ring algebra which
minimizes complexity with component-wise product and achieves the best quality
using directional ReLU. Finally, we implement an accelerator, eRingCNN, in two
settings, n=2 and 4 (50% and 75% sparsity), with 40 nm technology to support
advanced denoising and super-resolution at up to 4K UHD 30 fps. Layout results
show that they can deliver equivalent 41 TOPS using 3.76 W and 2.22 W,
respectively. Compared to the real-valued counterpart, our ring convolution
engines for n=2 achieve 2.00x energy efficiency and 2.08x area efficiency with
similar or even better image quality. With n=4, the efficiency gains of energy
and area are further increased to 3.84x and 3.77x with 0.11 dB drop of PSNR.",arxiv
http://arxiv.org/abs/2106.12605v1,2021-06-23T18:08:07Z,2021-06-23T18:08:07Z,Deep Fake Detection: Survey of Facial Manipulation Detection Solutions,"Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.",arxiv
http://arxiv.org/abs/2006.01804v2,2020-07-05T19:17:05Z,2020-06-02T17:39:32Z,"Practical sensorless aberration estimation for 3D microscopy with deep
  learning","Estimation of optical aberrations from volumetric intensity images is a key
step in sensorless adaptive optics for 3D microscopy. Recent approaches based
on deep learning promise accurate results at fast processing speeds. However,
collecting ground truth microscopy data for training the network is typically
very difficult or even impossible thereby limiting this approach in practice.
Here, we demonstrate that neural networks trained only on simulated data yield
accurate predictions for real experimental images. We validate our approach on
simulated and experimental datasets acquired with two different microscopy
modalities, and also compare the results to non-learned methods. Additionally,
we study the predictability of individual aberrations with respect to their
data requirements and find that the symmetry of the wavefront plays a crucial
role. Finally, we make our implementation freely available as open source
software in Python.",arxiv
http://arxiv.org/abs/2001.00487v1,2020-01-02T15:22:36Z,2020-01-02T15:22:36Z,"Using CNNs For Users Segmentation In Video See-Through Augmented
  Virtuality","In this paper, we present preliminary results on the use of deep learning
techniques to integrate the users self-body and other participants into a
head-mounted video see-through augmented virtuality scenario. It has been
previously shown that seeing users bodies in such simulations may improve the
feeling of both self and social presence in the virtual environment, as well as
user performance. We propose to use a convolutional neural network for real
time semantic segmentation of users bodies in the stereoscopic RGB video
streams acquired from the perspective of the user. We describe design issues as
well as implementation details of the system and demonstrate the feasibility of
using such neural networks for merging users bodies in an augmented virtuality
simulation.",arxiv
http://arxiv.org/abs/2011.09902v1,2020-11-17T04:11:31Z,2020-11-17T04:11:31Z,"Low-latency Federated Learning and Blockchain for Edge Association in
  Digital Twin empowered 6G Networks","Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.",arxiv
http://arxiv.org/abs/2009.08044v2,2020-12-03T20:51:47Z,2020-09-17T03:38:28Z,Large-Scale Intelligent Microservices,"Deploying Machine Learning (ML) algorithms within databases is a challenge
due to the varied computational footprints of modern ML algorithms and the
myriad of database technologies each with its own restrictive syntax. We
introduce an Apache Spark-based micro-service orchestration framework that
extends database operations to include web service primitives. Our system can
orchestrate web services across hundreds of machines and takes full advantage
of cluster, thread, and asynchronous parallelism. Using this framework, we
provide large scale clients for intelligent services such as speech, vision,
search, anomaly detection, and text analysis. This allows users to integrate
ready-to-use intelligence into any datastore with an Apache Spark connector. To
eliminate the majority of overhead from network communication, we also
introduce a low-latency containerized version of our architecture. Finally, we
demonstrate that the services we investigate are competitive on a variety of
benchmarks, and present two applications of this framework to create
intelligent search engines, and real-time auto race analytics systems.",arxiv
http://arxiv.org/abs/1709.04909v1,2017-09-14T17:54:05Z,2017-09-14T17:54:05Z,Shared Learning : Enhancing Reinforcement in $Q$-Ensembles,"Deep Reinforcement Learning has been able to achieve amazing successes in a
variety of domains from video games to continuous control by trying to maximize
the cumulative reward. However, most of these successes rely on algorithms that
require a large amount of data to train in order to obtain results on par with
human-level performance. This is not feasible if we are to deploy these systems
on real world tasks and hence there has been an increased thrust in exploring
data efficient algorithms. To this end, we propose the Shared Learning
framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving
this, we look into some principles of transfer learning which aim to study the
benefits of information exchange across tasks in reinforcement learning and
adapt transfer to learning our value function estimates in a novel manner. In
this paper, we consider the special case of transfer between the value function
estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further
empirically demonstrate how our proposed framework can help in speeding up the
learning process in $Q$-ensembles with minimum computational overhead on a
suite of Atari 2600 Games.",arxiv
http://arxiv.org/abs/2105.01852v2,2021-10-22T15:54:55Z,2021-05-05T03:28:57Z,Deep Learning for Needle Detection in a Cannulation Simulator,"Cannulation for hemodialysis is the act of inserting a needle into a
surgically created vascular access (e.g., an arteriovenous fistula) for the
purpose of dialysis. The main risk associated with cannulation is infiltration,
the puncture of the wall of the vascular access after entry, which can cause
medical complications. Simulator-based training allows clinicians to gain
cannulation experience without putting patients at risk. In this paper, we
propose to use deep-learning-based techniques for detecting, based on video,
whether the needle tip is in or has infiltrated the simulated fistula. Three
categories of deep neural networks are investigated in this work: modified
pre-trained models based on VGG-16 and ResNet-50, light convolutional neural
networks (light CNNs), and convolutional recurrent neural networks (CRNNs).
CRNNs consist of convolutional layers and a long short-term memory (LSTM)
layer. A data set of cannulation experiments was collected and analyzed. The
results show that both the light CNN and the CRNN achieve better performance
than the pre-trained baseline models. The CRNN was implemented in real time on
commodity hardware for use in the cannulation simulator, and the performance
was verified. Deep-learning video analysis is a viable method for detecting
needle state in a low cost cannulation simulator. Our data sets and code are
released at https://github.com/axin233/DL_for_Needle_Detection_Cannulation",arxiv
http://arxiv.org/abs/2002.08333v1,2020-02-11T15:32:28Z,2020-02-11T15:32:28Z,"Towards Intelligent Pick and Place Assembly of Individualized Products
  Using Reinforcement Learning","Individualized manufacturing is becoming an important approach as a means to
fulfill increasingly diverse and specific consumer requirements and
expectations. While there are various solutions to the implementation of the
manufacturing process, such as additive manufacturing, the subsequent automated
assembly remains a challenging task. As an approach to this problem, we aim to
teach a collaborative robot to successfully perform pick and place tasks by
implementing reinforcement learning. For the assembly of an individualized
product in a constantly changing manufacturing environment, the simulated
geometric and dynamic parameters will be varied. Using reinforcement learning
algorithms capable of meta-learning, the tasks will first be trained in
simulation. They will then be performed in a real-world environment where new
factors are introduced that were not simulated in training to confirm the
robustness of the algorithms. The robot will gain its input data from tactile
sensors, area scan cameras, and 3D cameras used to generate heightmaps of the
environment and the objects. The selection of machine learning algorithms and
hardware components as well as further research questions to realize the
outlined production scenario are the results of the presented work.",arxiv
http://arxiv.org/abs/2102.04871v1,2021-02-09T15:14:27Z,2021-02-09T15:14:27Z,The Factory Must Grow: Automation in Factorio,"Efficient optimization of resources is paramount to success in many problems
faced today. In the field of operational research the efficient scheduling of
employees; packing of vans; routing of vehicles; logistics of airlines and
transport of materials can be the difference between emission reduction or
excess, profits or losses and feasibility or unworkable solutions. The video
game Factorio, by Wube Software, has a myriad of problems which are analogous
to such real-world problems, and is a useful simulator for developing solutions
for these problems. In this paper we define the logistic transport belt problem
and define mathematical integer programming model of it. We developed an
interface to allow optimizers in any programming language to interact with
Factorio, and we provide an initial benchmark of logistic transport belt
problems. We present results for Simulated Annealing, quick Genetic Programming
and Evolutionary Reinforcement Learning, three different meta-heuristic
techniques to optimize this novel problem.",arxiv
http://arxiv.org/abs/2006.09675v1,2020-06-17T06:30:43Z,2020-06-17T06:30:43Z,"A Real-time Action Representation with Temporal Encoding and Deep
  Compression","Deep neural networks have achieved remarkable success for video-based action
recognition. However, most of existing approaches cannot be deployed in
practice due to the high computational cost. To address this challenge, we
propose a new real-time convolutional architecture, called Temporal
Convolutional 3D Network (T-C3D), for action representation. T-C3D learns video
action representations in a hierarchical multi-granularity manner while
obtaining a high process speed. Specifically, we propose a residual 3D
Convolutional Neural Network (CNN) to capture complementary information on the
appearance of a single frame and the motion between consecutive frames. Based
on this CNN, we develop a new temporal encoding method to explore the temporal
dynamics of the whole video. Furthermore, we integrate deep compression
techniques with T-C3D to further accelerate the deployment of models via
reducing the size of the model. By these means, heavy calculations can be
avoided when doing the inference, which enables the method to deal with videos
beyond real-time speed while keeping promising performance. Our method achieves
clear improvements on UCF101 action recognition benchmark against
state-of-the-art real-time methods by 5.4% in terms of accuracy and 2 times
faster in terms of inference speed with a less than 5MB storage model. We
validate our approach by studying its action representation performance on four
different benchmarks over three different tasks. Extensive experiments
demonstrate comparable recognition performance to the state-of-the-art methods.
The source code and the pre-trained models are publicly available at
https://github.com/tc3d.",arxiv
http://arxiv.org/abs/1908.01862v1,2019-08-05T21:10:12Z,2019-08-05T21:10:12Z,Semi-Automatic Labeling for Deep Learning in Robotics,"In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a
semi-automatic method which leverages on moving a 2D camera by means of a
robot, proving precise camera tracking, and an augmented reality pen to define
initial object bounding box, to create large labeled datasets with minimal
human intervention. By removing the burden of generating annotated data from
humans, we make the Deep Learning technique applied to computer vision, that
typically requires very large datasets, truly automated and reliable. With the
ARS pipeline, we created effortlessly two novel datasets, one on
electromechanical components (industrial scenario) and one on fruits
(daily-living scenario), and trained robustly two state-of-the-art object
detectors, based on convolutional neural networks, such as YOLO and SSD. With
respect to the conventional manual annotation of 1000 frames that takes us
slightly more than 10 hours, the proposed approach based on ARS allows
annotating 9 sequences of about 35000 frames in less than one hour, with a gain
factor of about 450. Moreover, both the precision and recall of object
detection is increased by about 15\% with respect to manual labeling. All our
software is available as a ROS package in a public repository alongside the
novel annotated datasets.",arxiv
http://arxiv.org/abs/1603.07341v1,2016-03-23T20:13:11Z,2016-03-23T20:13:11Z,"Acceleration of Deep Neural Network Training with Resistive Cross-Point
  Devices","In recent years, deep neural networks (DNN) have demonstrated significant
business impact in large scale analysis and classification tasks such as speech
recognition, visual object detection, pattern extraction, etc. Training of
large DNNs, however, is universally considered as time consuming and
computationally intensive task that demands datacenter-scale computational
resources recruited for many days. Here we propose a concept of resistive
processing unit (RPU) devices that can potentially accelerate DNN training by
orders of magnitude while using much less power. The proposed RPU device can
store and update the weight values locally thus minimizing data movement during
training and allowing to fully exploit the locality and the parallelism of the
training algorithm. We identify the RPU device and system specifications for
implementation of an accelerator chip for DNN training in a realistic
CMOS-compatible technology. For large DNNs with about 1 billion weights this
massively parallel RPU architecture can achieve acceleration factors of 30,000X
compared to state-of-the-art microprocessors while providing power efficiency
of 84,000 GigaOps/s/W. Problems that currently require days of training on a
datacenter-size cluster with thousands of machines can be addressed within
hours on a single RPU accelerator. A system consisted of a cluster of RPU
accelerators will be able to tackle Big Data problems with trillions of
parameters that is impossible to address today like, for example, natural
speech recognition and translation between all world languages, real-time
analytics on large streams of business and scientific data, integration and
analysis of multimodal sensory data flows from massive number of IoT (Internet
of Things) sensors.",arxiv
http://arxiv.org/abs/2008.06655v1,2020-08-15T05:15:00Z,2020-08-15T05:15:00Z,Object Detection in the Context of Mobile Augmented Reality,"In the past few years, numerous Deep Neural Network (DNN) models and
frameworks have been developed to tackle the problem of real-time object
detection from RGB images. Ordinary object detection approaches process
information from the images only, and they are oblivious to the camera pose
with regard to the environment and the scale of the environment. On the other
hand, mobile Augmented Reality (AR) frameworks can continuously track a
camera's pose within the scene and can estimate the correct scale of the
environment by using Visual-Inertial Odometry (VIO). In this paper, we propose
a novel approach that combines the geometric information from VIO with semantic
information from object detectors to improve the performance of object
detection on mobile devices. Our approach includes three components: (1) an
image orientation correction method, (2) a scale-based filtering approach, and
(3) an online semantic map. Each component takes advantage of the different
characteristics of the VIO-based AR framework. We implemented the AR-enhanced
features using ARCore and the SSD Mobilenet model on Android phones. To
validate our approach, we manually labeled objects in image sequences taken
from 12 room-scale AR sessions. The results show that our approach can improve
on the accuracy of generic object detectors by 12% on our dataset.",arxiv
http://arxiv.org/abs/2011.02833v3,2020-12-04T13:34:23Z,2020-11-02T19:08:49Z,"Digital Twins: State of the Art Theory and Practice, Challenges, and
  Open Research Questions","Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins.",arxiv
http://arxiv.org/abs/2003.07583v1,2020-03-17T08:47:34Z,2020-03-17T08:47:34Z,"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow
  Based QoE","With the merit of containing full panoramic content in one camera, Virtual
Reality (VR) and 360-degree videos have attracted more and more attention in
the field of industrial cloud manufacturing and training. Industrial Internet
of Things (IoT), where many VR terminals needed to be online at the same time,
can hardly guarantee VR's bandwidth requirement. However, by making use of
users' quality of experience (QoE) awareness factors, including the relative
moving speed and depth difference between the viewpoint and other content,
bandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical
Flow Based VR), an interactive method of VR streaming that can make use of VR
users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable
Difference through Optical Flow Estimation (JND-OFE) is explored to quantify
users' awareness of quality distortion in 360-degree videos. Accordingly, a
novel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is
proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling
scheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive BitRate(ABR).
For evaluation, we take two prior VR streaming schemes, Pano and Plato, as
baselines. Vast evaluations show that our system can increase the mean PSNR-OF
score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano
and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that
OFB-VR is a promising prototype for actual interactive industrial VR. A
prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",arxiv
http://arxiv.org/abs/1911.07919v1,2019-11-15T18:44:25Z,2019-11-15T18:44:25Z,ASV: Accelerated Stereo Vision System,"Estimating depth from stereo vision cameras, i.e., ""depth from stereo"", is
critical to emerging intelligent applications deployed in energy- and
performance-constrained devices, such as augmented reality headsets and mobile
autonomous robots. While existing stereo vision systems make trade-offs between
accuracy, performance and energy-efficiency, we describe ASV, an accelerated
stereo vision system that simultaneously improves both performance and
energy-efficiency while achieving high accuracy. The key to ASV is to exploit
unique characteristics inherent to stereo vision, and apply stereo-specific
optimizations, both algorithmically and computationally. We make two
contributions. Firstly, we propose a new stereo algorithm, invariant-based
stereo matching (ISM), that achieves significant speedup while retaining high
accuracy. The algorithm combines classic ""hand-crafted"" stereo algorithms with
recent developments in Deep Neural Networks (DNNs), by leveraging the
correspondence invariant unique to stereo vision systems. Secondly, we observe
that the bottleneck of the ISM algorithm is the DNN inference, and in
particular the deconvolution operations that introduce massive
compute-inefficiencies. We propose a set of software optimizations that
mitigate these inefficiencies. We show that with less than 0.5% hardware area
overhead, these algorithmic and computational optimizations can be effectively
integrated within a conventional DNN accelerator. Overall, ASV achieves 5x
speedup and 85% energy saving with 0.02% accuracy loss compared to today
DNN-based stereo vision systems.",arxiv
http://arxiv.org/abs/2111.08973v2,2021-11-19T04:24:42Z,2021-11-17T08:30:18Z,Generating Unrestricted 3D Adversarial Point Clouds,"Utilizing 3D point cloud data has become an urgent need for the deployment of
artificial intelligence in many areas like facial recognition and self-driving.
However, deep learning for 3D point clouds is still vulnerable to adversarial
attacks, e.g., iterative attacks, point transformation attacks, and generative
attacks. These attacks need to restrict perturbations of adversarial examples
within a strict bound, leading to the unrealistic adversarial 3D point clouds.
In this paper, we propose an Adversarial Graph-Convolutional Generative
Adversarial Network (AdvGCGAN) to generate visually realistic adversarial 3D
point clouds from scratch. Specifically, we use a graph convolutional generator
and a discriminator with an auxiliary classifier to generate realistic point
clouds, which learn the latent distribution from the real 3D data. The
unrestricted adversarial attack loss is incorporated in the special adversarial
training of GAN, which enables the generator to generate the adversarial
examples to spoof the target network. Compared with the existing state-of-art
attack methods, the experiment results demonstrate the effectiveness of our
unrestricted adversarial attack methods with a higher attack success rate and
visual quality. Additionally, the proposed AdvGCGAN can achieve better
performance against defense models and better transferability than existing
attack methods with strong camouflage.",arxiv
http://arxiv.org/abs/2009.05553v1,2020-09-11T17:36:13Z,2020-09-11T17:36:13Z,Deep Analog-to-Digital Converter for Wireless Communication,"With the advent of the 5G wireless networks, achieving tens of gigabits per
second throughputs and low, milliseconds, latency has become a reality. This
level of performance will fuel numerous real-time applications, such as
autonomy and augmented reality, where the computationally heavy tasks can be
performed in the cloud. The increase in the bandwidth along with the use of
dense constellations places a significant burden on the speed and accuracy of
analog-to-digital converters (ADC). A popular approach to create wideband ADCs
is utilizing multiple channels each operating at a lower speed in the
time-interleaved fashion. However, an interleaved ADC comes with its own set of
challenges. The parallel architecture is very sensitive to the inter-channel
mismatch, timing jitter, clock skew between different ADC channels as well as
the nonlinearity within individual channels. Consequently, complex
post-calibration is required using digital signal processing (DSP) after the
ADC. The traditional DSP calibration consumes a significant amount of power and
its design requires knowledge of the source and type of errors which are
becoming increasingly difficult to predict in nanometer CMOS processes. In this
paper, instead of individually targeting each source of error, we utilize a
deep learning algorithm to learn the complete and complex ADC behavior and to
compensate for it in realtime. We demonstrate this ""Deep ADC"" technique on an
8G Sample/s 8-channel time-interleaved ADC with the QAM-OFDM modulated data.
Simulation results for different QAM symbol constellations and OFDM subcarriers
show dramatic improvements of approximately 5 bits in the dynamic range with a
concomitant drastic reduction in symbol error rate. We further discuss the
hardware implementation including latency, power consumption, memory
requirements, and chip area.",arxiv
http://arxiv.org/abs/2104.01036v1,2021-04-02T13:17:11Z,2021-04-02T13:17:11Z,"Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR
  Video Service","Virtual reality (VR) is promising to fundamentally transform a broad spectrum
of industry sectors and the way humans interact with virtual content. However,
despite unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR's full potential. In this paper,
we consider delivering the wireless multi-tile VR video service over a mobile
edge computing (MEC) network. The primary goal is to minimize the system
latency/energy consumption and to arrive at a tradeoff thereof. To this end, we
first cast the time-varying view popularity as a model-free Markov chain to
effectively capture its dynamic characteristics. After jointly assessing the
caching and computing capacities on both the MEC server and the VR playback
device, a hybrid policy is then implemented to coordinate the dynamic caching
replacement and the deterministic offloading, so as to fully utilize the system
resources. The underlying multi-objective problem is reformulated as a
partially observable Markov decision process, and a deep deterministic policy
gradient algorithm is proposed to iteratively learn its solution, where a long
short-term memory neural network is embedded to continuously predict the
dynamics of the unobservable popularity. Simulation results demonstrate the
superiority of the proposed scheme in achieving a trade-off between the energy
efficiency and the latency reduction over the baseline methods.",arxiv
http://arxiv.org/abs/2011.03630v1,2020-11-06T23:17:12Z,2020-11-06T23:17:12Z,"Unmasking Communication Partners: A Low-Cost AI Solution for Digitally
  Removing Head-Mounted Displays in VR-Based Telepresence","Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions.",arxiv
http://arxiv.org/abs/2107.08325v1,2021-07-18T00:00:48Z,2021-07-18T00:00:48Z,"Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement
  Learning","Autonomous car racing is a challenging task in the robotic control area.
Traditional modular methods require accurate mapping, localization and
planning, which makes them computationally inefficient and sensitive to
environmental changes. Recently, deep-learning-based end-to-end systems have
shown promising results for autonomous driving/racing. However, they are
commonly implemented by supervised imitation learning (IL), which suffers from
the distribution mismatch problem, or by reinforcement learning (RL), which
requires a huge amount of risky interaction data. In this work, we present a
general deep imitative reinforcement learning approach (DIRL), which
successfully achieves agile autonomous racing using visual inputs. The driving
knowledge is acquired from both IL and model-based RL, where the agent can
learn from human teachers as well as perform self-improvement by safely
interacting with an offline world model. We validate our algorithm both in a
high-fidelity driving simulation and on a real-world 1/20-scale RC-car with
limited onboard computation. The evaluation results demonstrate that our method
outperforms previous IL and RL methods in terms of sample efficiency and task
performance. Demonstration videos are available at
https://caipeide.github.io/autorace-dirl/",arxiv
http://arxiv.org/abs/2009.05835v3,2021-04-03T15:08:50Z,2020-09-12T17:37:36Z,"How Much Can We Really Trust You? Towards Simple, Interpretable Trust
  Quantification Metrics for Deep Neural Networks","A critical step to building trustworthy deep neural networks is trust
quantification, where we ask the question: How much can we trust a deep neural
network? In this study, we take a step towards simple, interpretable metrics
for trust quantification by introducing a suite of metrics for assessing the
overall trustworthiness of deep neural networks based on their behaviour when
answering a set of questions. We conduct a thought experiment and explore two
key questions about trust in relation to confidence: 1) How much trust do we
have in actors who give wrong answers with great confidence? and 2) How much
trust do we have in actors who give right answers hesitantly? Based on insights
gained, we introduce the concept of question-answer trust to quantify
trustworthiness of an individual answer based on confident behaviour under
correct and incorrect answer scenarios, and the concept of trust density to
characterize the distribution of overall trust for an individual answer
scenario. We further introduce the concept of trust spectrum for representing
overall trust with respect to the spectrum of possible answer scenarios across
correctly and incorrectly answered questions. Finally, we introduce
NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite
of metrics aligns with past social psychology studies that study the
relationship between trust and confidence. Leveraging these metrics, we
quantify the trustworthiness of several well-known deep neural network
architectures for image recognition to get a deeper understanding of where
trust breaks down. The proposed metrics are by no means perfect, but the hope
is to push the conversation towards better metrics to help guide practitioners
and regulators in producing, deploying, and certifying deep learning solutions
that can be trusted to operate in real-world, mission-critical scenarios.",arxiv
http://arxiv.org/abs/1611.02695v1,2016-11-08T09:50:30Z,2016-11-08T09:50:30Z,"Automatic recognition of child speech for robotic applications in noisy
  environments","Automatic speech recognition (ASR) allows a natural and intuitive interface
for robotic educational applications for children. However there are a number
of challenges to overcome to allow such an interface to operate robustly in
realistic settings, including the intrinsic difficulties of recognising child
speech and high levels of background noise often present in classrooms. As part
of the EU EASEL project we have provided several contributions to address these
challenges, implementing our own ASR module for use in robotics applications.
We used the latest deep neural network algorithms which provide a leap in
performance over the traditional GMM approach, and apply data augmentation
methods to improve robustness to noise and speaker variation. We provide a
close integration between the ASR module and the rest of the dialogue system,
allowing the ASR to receive in real-time the language models relevant to the
current section of the dialogue, greatly improving the accuracy. We integrated
our ASR module into an interactive, multimodal system using a small humanoid
robot to help children learn about exercise and energy. The system was
installed at a public museum event as part of a research study where 320
children (aged 3 to 14) interacted with the robot, with our ASR achieving 90%
accuracy for fluent and near-fluent speech.",arxiv
http://arxiv.org/abs/1909.06526v1,2019-09-14T04:02:45Z,2019-09-14T04:02:45Z,FfDL : A Flexible Multi-tenant Deep Learning Platform,"Deep learning (DL) is becoming increasingly popular in several application
domains and has made several new application features involving computer
vision, speech recognition and synthesis, self-driving automobiles, drug
design, etc. feasible and accurate. As a result, large scale on-premise and
cloud-hosted deep learning platforms have become essential infrastructure in
many organizations. These systems accept, schedule, manage and execute DL
training jobs at scale.
  This paper describes the design, implementation and our experiences with
FfDL, a DL platform used at IBM. We describe how our design balances
dependability with scalability, elasticity, flexibility and efficiency. We
examine FfDL qualitatively through a retrospective look at the lessons learned
from building, operating, and supporting FfDL; and quantitatively through a
detailed empirical evaluation of FfDL, including the overheads introduced by
the platform for various deep learning models, the load and performance
observed in a real case study using FfDL within our organization, the frequency
of various faults observed including unanticipated faults, and experiments
demonstrating the benefits of various scheduling policies. FfDL has been
open-sourced.",arxiv
http://arxiv.org/abs/1804.03313v1,2018-04-10T02:33:47Z,2018-04-10T02:33:47Z,Cortex Neural Network: learning with Neural Network groups,"Neural Network has been successfully applied to many real-world problems,
such as image recognition and machine translation. However, for the current
architecture of neural networks, it is hard to perform complex cognitive tasks,
for example, to process the image and audio inputs together. Cortex, as an
important architecture in the brain, is important for animals to perform the
complex cognitive task. We view the architecture of Cortex in the brain as a
missing part in the design of the current artificial neural network. In this
paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is
an upper architecture of neural networks which motivated from cerebral cortex
in the brain to handle different tasks in the same learning system. It is able
to identify different tasks and solve them with different methods. In our
implementation, the Cortex Neural Network is able to process different
cognitive tasks and perform reflection to get a higher accuracy. We provide a
series of experiments to examine the capability of the cortex architecture on
traditional neural networks. Our experiments proved its ability on the Cortex
Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the
same time, which can promisingly reduce the loss by 40%.",arxiv
http://arxiv.org/abs/2103.07220v1,2021-03-12T11:49:51Z,2021-03-12T11:49:51Z,Real-time Timbre Transfer and Sound Synthesis using DDSP,"Neural audio synthesis is an actively researched topic, having yielded a wide
range of techniques that leverages machine learning architectures. Google
Magenta elaborated a novel approach called Differential Digital Signal
Processing (DDSP) that incorporates deep neural networks with preconditioned
digital signal processing techniques, reaching state-of-the-art results
especially in timbre transfer applications. However, most of these techniques,
including the DDSP, are generally not applicable in real-time constraints,
making them ineligible in a musical workflow. In this paper, we present a
real-time implementation of the DDSP library embedded in a virtual synthesizer
as a plug-in that can be used in a Digital Audio Workstation. We focused on
timbre transfer from learned representations of real instruments to arbitrary
sound inputs as well as controlling these models by MIDI. Furthermore, we
developed a GUI for intuitive high-level controls which can be used for
post-processing and manipulating the parameters estimated by the neural
network. We have conducted a user experience test with seven participants
online. The results indicated that our users found the interface appealing,
easy to understand, and worth exploring further. At the same time, we have
identified issues in the timbre transfer quality, in some components we did not
implement, and in installation and distribution of our plugin. The next
iteration of our design will address these issues. Our real-time MATLAB and
JUCE implementations are available at https://github.com/SMC704/juce-ddsp and
https://github.com/SMC704/matlab-ddsp , respectively.",arxiv
http://arxiv.org/abs/2107.13782v1,2021-07-29T07:25:21Z,2021-07-29T07:25:21Z,"Multimodal Co-learning: Challenges, Applications with Datasets, Recent
  Advances and Future Directions","Multimodal deep learning systems which employ multiple modalities like text,
image, audio, video, etc., are showing better performance in comparison with
individual modalities (i.e., unimodal) systems. Multimodal machine learning
involves multiple aspects: representation, translation, alignment, fusion, and
co-learning. In the current state of multimodal machine learning, the
assumptions are that all modalities are present, aligned, and noiseless during
training and testing time. However, in real-world tasks, typically, it is
observed that one or more modalities are missing, noisy, lacking annotated
data, have unreliable labels, and are scarce in training or testing and or
both. This challenge is addressed by a learning paradigm called multimodal
co-learning. The modeling of a (resource-poor) modality is aided by exploiting
knowledge from another (resource-rich) modality using transfer of knowledge
between modalities, including their representations and predictive models.
Co-learning being an emerging area, there are no dedicated reviews explicitly
focusing on all challenges addressed by co-learning. To that end, in this work,
we provide a comprehensive survey on the emerging area of multimodal
co-learning that has not been explored in its entirety yet. We review
implementations that overcome one or more co-learning challenges without
explicitly considering them as co-learning challenges. We present the
comprehensive taxonomy of multimodal co-learning based on the challenges
addressed by co-learning and associated implementations. The various techniques
employed to include the latest ones are reviewed along with some of the
applications and datasets. Our final goal is to discuss challenges and
perspectives along with the important ideas and directions for future work that
we hope to be beneficial for the entire research community focusing on this
exciting domain.",arxiv
http://arxiv.org/abs/2003.13652v1,2020-03-18T01:26:36Z,2020-03-18T01:26:36Z,"Machine Learning enabled Spectrum Sharing in Dense LTE-U/Wi-Fi
  Coexistence Scenarios","The application of Machine Learning (ML) techniques to complex engineering
problems has proved to be an attractive and efficient solution. ML has been
successfully applied to several practical tasks like image recognition,
automating industrial operations, etc. The promise of ML techniques in solving
non-linear problems influenced this work which aims to apply known ML
techniques and develop new ones for wireless spectrum sharing between Wi-Fi and
LTE in the unlicensed spectrum. In this work, we focus on the LTE-Unlicensed
(LTE-U) specification developed by the LTE-U Forum, which uses the duty-cycle
approach for fair coexistence. The specification suggests reducing the duty
cycle at the LTE-U base-station (BS) when the number of co-channel Wi-Fi basic
service sets (BSSs) increases from one to two or more. However, without
decoding the Wi-Fi packets, detecting the number of Wi-Fi BSSs operating on the
channel in real-time is a challenging problem. In this work, we demonstrate a
novel ML-based approach which solves this problem by using energy values
observed during the LTE-U OFF duration. It is relatively straightforward to
observe only the energy values during the LTE-U BS OFF time compared to
decoding the entire Wi-Fi packet, which would require a full Wi-Fi receiver at
the LTE-U base-station. We implement and validate the proposed ML-based
approach by real-time experiments and demonstrate that there exist distinct
patterns between the energy distributions between one and many Wi-Fi AP
transmissions. The proposed ML-based approach results in a higher accuracy
(close to 99\% in all cases) as compared to the existing auto-correlation (AC)
and energy detection (ED) approaches.",arxiv
http://arxiv.org/abs/1201.4339v1,2012-01-20T17:04:18Z,2012-01-20T17:04:18Z,"Recognizing recurrent neural networks (rRNN): Bayesian inference for
  recurrent neural networks","Recurrent neural networks (RNNs) are widely used in computational
neuroscience and machine learning applications. In an RNN, each neuron computes
its output as a nonlinear function of its integrated input. While the
importance of RNNs, especially as models of brain processing, is undisputed, it
is also widely acknowledged that the computations in standard RNN models may be
an over-simplification of what real neuronal networks compute. Here, we suggest
that the RNN approach may be made both neurobiologically more plausible and
computationally more powerful by its fusion with Bayesian inference techniques
for nonlinear dynamical systems. In this scheme, we use an RNN as a generative
model of dynamic input caused by the environment, e.g. of speech or kinematics.
Given this generative RNN model, we derive Bayesian update equations that can
decode its output. Critically, these updates define a 'recognizing RNN' (rRNN),
in which neurons compute and exchange prediction and prediction error messages.
The rRNN has several desirable features that a conventional RNN does not have,
for example, fast decoding of dynamic stimuli and robustness to initial
conditions and noise. Furthermore, it implements a predictive coding scheme for
dynamic inputs. We suggest that the Bayesian inversion of recurrent neural
networks may be useful both as a model of brain function and as a machine
learning tool. We illustrate the use of the rRNN by an application to the
online decoding (i.e. recognition) of human kinematics.",arxiv
http://arxiv.org/abs/1711.07480v1,2017-11-20T17:58:10Z,2017-11-20T17:58:10Z,E-PUR: An Energy-Efficient Processing Unit for Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) are a key technology for emerging
applications such as automatic speech recognition, machine translation or image
description. Long Short Term Memory (LSTM) networks are the most successful RNN
implementation, as they can learn long term dependencies to achieve high
accuracy. Unfortunately, the recurrent nature of LSTM networks significantly
constrains the amount of parallelism and, hence, multicore CPUs and many-core
GPUs exhibit poor efficiency for RNN inference. In this paper, we present
E-PUR, an energy-efficient processing unit tailored to the requirements of LSTM
computation. The main goal of E-PUR is to support large recurrent neural
networks for low-power mobile devices. E-PUR provides an efficient hardware
implementation of LSTM networks that is flexible to support diverse
applications. One of its main novelties is a technique that we call Maximizing
Weight Locality (MWL), which improves the temporal locality of the memory
accesses for fetching the synaptic weights, reducing the memory requirements by
a large extent. Our experimental results show that E-PUR achieves real-time
performance for different LSTM networks, while reducing energy consumption by
orders of magnitude with respect to general-purpose processors and GPUs, and it
requires a very small chip area. Compared to a modern mobile SoC, an NVIDIA
Tegra X1, E-PUR provides an average energy reduction of 92x.",arxiv
http://arxiv.org/abs/1910.05376v2,2019-12-13T22:58:20Z,2019-10-11T18:57:18Z,AffWild Net and Aff-Wild Database,"Emotions recognition is the task of recognizing people's emotions. Usually it
is achieved by analyzing expression of peoples faces. There are two ways for
representing emotions: The categorical approach and the dimensional approach by
using valence and arousal values. Valence shows how negative or positive an
emotion is and arousal shows how much it is activated. Recent deep learning
models, that have to do with emotions recognition, are using the second
approach, valence and arousal. Moreover, a more interesting concept, which is
useful in real life is the ""in the wild"" emotions recognition. ""In the wild""
means that the images analyzed for the recognition task, come from from real
life sources(online videos, online photos, etc.) and not from staged
experiments. So, they introduce unpredictable situations in the images, that
have to be modeled. The purpose of this project is to study the previous work
that was done for the ""in the wild"" emotions recognition concept, design a new
dataset which has as a standard the ""Aff-wild"" database, implement new deep
learning models and evaluate the results. First, already existing databases and
deep learning models are presented. Then, inspired by them a new database is
created which includes 507.208 frames in total from 106 videos, which were
gathered from online sources. Then, the data are tested in a CNN model based on
CNN-M architecture, in order to be sure about their usability. Next, the main
model of this project is implemented. That is a Regression GAN which can
execute unsupervised and supervised learning at the same time. More
specifically, it keeps the main functionality of GANs, which is to produce fake
images that look as good as the real ones, while it can also predict valence
and arousal values for both real and fake images. Finally, the database created
earlier is applied to this model and the results are presented and evaluated.",arxiv
http://arxiv.org/abs/2109.07735v2,2021-11-20T06:25:22Z,2021-09-16T05:59:01Z,"Decentralized Control of Quadrotor Swarms with End-to-end Deep
  Reinforcement Learning","We demonstrate the possibility of learning drone swarm controllers that are
zero-shot transferable to real quadrotors via large-scale multi-agent
end-to-end reinforcement learning. We train policies parameterized by neural
networks that are capable of controlling individual drones in a swarm in a
fully decentralized manner. Our policies, trained in simulated environments
with realistic quadrotor physics, demonstrate advanced flocking behaviors,
perform aggressive maneuvers in tight formations while avoiding collisions with
each other, break and re-establish formations to avoid collisions with moving
obstacles, and efficiently coordinate in pursuit-evasion tasks. We analyze, in
simulation, how different model architectures and parameters of the training
regime influence the final performance of neural swarms. We demonstrate the
successful deployment of the model learned in simulation to highly
resource-constrained physical quadrotors performing station keeping and goal
swapping behaviors. Code and video demonstrations are available on the project
website at https://sites.google.com/view/swarm-rl.",arxiv
http://arxiv.org/abs/2010.14605v3,2021-06-07T10:06:48Z,2020-10-27T20:56:49Z,"Traffic Refinery: Cost-Aware Data Representation for Machine Learning on
  Network Traffic","Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.",arxiv
http://arxiv.org/abs/2006.11305v2,2021-02-09T21:51:41Z,2020-06-18T04:35:22Z,"Generalization of Agent Behavior through Explicit Representation of
  Context","In order to deploy autonomous agents in digital interactive environments,
they must be able to act robustly in unseen situations. The standard machine
learning approach is to include as much variation as possible into training
these agents. The agents can then interpolate within their training, but they
cannot extrapolate much beyond it. This paper proposes a principled approach
where a context module is coevolved with a skill module in the game. The
context module recognizes the temporal variation in the game and modulates the
outputs of the skill module so that the action decisions can be made robustly
even in previously unseen situations. The approach is evaluated in the Flappy
Bird and LunarLander video games, as well as in the CARLA autonomous driving
simulation. The Context+Skill approach leads to significantly more robust
behavior in environments that require extrapolation beyond training. Such a
principled generalization ability is essential in deploying autonomous agents
in real-world tasks, and can serve as a foundation for continual adaptation as
well.",arxiv
http://arxiv.org/abs/2009.07632v1,2020-08-26T08:58:29Z,2020-08-26T08:58:29Z,"Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia
  Research Agenda","Participation on social media platforms has many benefits but also poses
substantial threats. Users often face an unintended loss of privacy, are
bombarded with mis-/disinformation, or are trapped in filter bubbles due to
over-personalized content. These threats are further exacerbated by the rise of
hidden AI-driven algorithms working behind the scenes to shape users' thoughts,
attitudes, and behavior. We investigate how multimedia researchers can help
tackle these problems to level the playing field for social media users. We
perform a comprehensive survey of algorithmic threats on social media and use
it as a lens to set a challenging but important research agenda for effective
and real-time user nudging. We further implement a conceptual prototype and
evaluate it with experts to supplement our research agenda. This paper calls
for solutions that combat the algorithmic threats on social media by utilizing
machine learning and multimedia content analysis techniques but in a
transparent manner and for the benefit of the users.",arxiv
http://arxiv.org/abs/2006.01674v2,2020-06-07T10:04:40Z,2020-06-02T14:48:34Z,"A network paradigm for very high capacity mobile and fixed
  telecommunications ecosystem sustainable evolution","For very high capacity networks (VHC), the main objective is to improve the
quality of the end-user experience. This implies compliance with key
performance indicators (KPIs) required by applications. Key performance
indicators at the application level are throughput, download time, round trip
time, and video delay. They depend on the end-to-end connection between the
server and the end-user device. For VHC networks, Telco operators must provide
the required application quality. Moreover, they must meet the objectives of
economic sustainability. Today, Telco operators rarely achieve the above
objectives, mainly due to the push to increase the bit-rate of access networks
without considering the end-to-end KPIs of the applications. The main
contribution of this paper concerns the definition of a deployment framework to
address performance and cost issues for VHC networks. We show three actions on
which it is necessary to focus. First, limiting bit-rate through video
compression. Second, contain the rate of packet loss through artificial
intelligence algorithms for line stabilization. Third, reduce latency (i.e.,
round-trip time) with edge-cloud computing. The concerted and gradual
application of these measures can allow a Telco to get out of the
ultra-broadband ""trap"" of the access network, as defined in the paper. We
propose to work on end-to-end optimization of the bandwidth utilization ratio.
This leads to a better performance experienced by the end-user. It also allows
a Telco operator to create new business models and obtain new revenue streams
at a sustainable cost. To give a clear example, we describe how to realize
mobile virtual and augmented reality, which is one of the most challenging
future services.",arxiv
http://arxiv.org/abs/1906.04161v1,2019-06-10T17:58:32Z,2019-06-10T17:58:32Z,Self-Supervised Exploration via Disagreement,"Efficient exploration is a long-standing problem in sensorimotor learning.
Major advances have been demonstrated in noise-free, non-stochastic domains
such as video games and simulation. However, most of these formulations either
get stuck in environments with stochastic dynamics or are too inefficient to be
scalable to real robotics setups. In this paper, we propose a formulation for
exploration inspired by the work in active learning literature. Specifically,
we train an ensemble of dynamics models and incentivize the agent to explore
such that the disagreement of those ensembles is maximized. This allows the
agent to learn skills by exploring in a self-supervised manner without any
external reward. Notably, we further leverage the disagreement objective to
optimize the agent's policy in a differentiable manner, without using
reinforcement learning, which results in a sample-efficient exploration. We
demonstrate the efficacy of this formulation across a variety of benchmark
environments including stochastic-Atari, Mujoco and Unity. Finally, we
implement our differentiable exploration on a real robot which learns to
interact with objects completely from scratch. Project videos and code are at
https://pathak22.github.io/exploration-by-disagreement/",arxiv
http://arxiv.org/abs/2010.05502v2,2020-10-13T05:58:43Z,2020-10-12T07:56:03Z,A Lightweight Speaker Recognition System Using Timbre Properties,"Speaker recognition is an active research area that contains notable usage in
biometric security and authentication system. Currently, there exist many
well-performing models in the speaker recognition domain. However, most of the
advanced models implement deep learning that requires GPU support for real-time
speech recognition, and it is not suitable for low-end devices. In this paper,
we propose a lightweight text-independent speaker recognition model based on
random forest classifier. It also introduces new features that are used for
both speaker verification and identification tasks. The proposed model uses
human speech based timbral properties as features that are classified using
random forest. Timbre refers to the very basic properties of sound that allow
listeners to discriminate among them. The prototype uses seven most actively
searched timbre properties, boominess, brightness, depth, hardness, roughness,
sharpness, and warmth as features of our speaker recognition model. The
experiment is carried out on speaker verification and speaker identification
tasks and shows the achievements and drawbacks of the proposed model. In the
speaker identification phase, it achieves a maximum accuracy of 78%. On the
contrary, in the speaker verification phase, the model maintains an accuracy of
80% having an equal error rate (ERR) of 0.24.",arxiv
http://arxiv.org/abs/2106.14739v1,2021-06-28T14:11:48Z,2021-06-28T14:11:48Z,"Real-Time Human Pose Estimation on a Smart Walker using Convolutional
  Neural Networks","Rehabilitation is important to improve quality of life for mobility-impaired
patients. Smart walkers are a commonly used solution that should embed
automatic and objective tools for data-driven human-in-the-loop control and
monitoring. However, present solutions focus on extracting few specific metrics
from dedicated sensors with no unified full-body approach. We investigate a
general, real-time, full-body pose estimation framework based on two RGB+D
camera streams with non-overlapping views mounted on a smart walker equipment
used in rehabilitation. Human keypoint estimation is performed using a
two-stage neural network framework. The 2D-Stage implements a detection module
that locates body keypoints in the 2D image frames. The 3D-Stage implements a
regression module that lifts and relates the detected keypoints in both cameras
to the 3D space relative to the walker. Model predictions are low-pass filtered
to improve temporal consistency. A custom acquisition method was used to obtain
a dataset, with 14 healthy subjects, used for training and evaluating the
proposed framework offline, which was then deployed on the real walker
equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage
and 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms
when deployed on the constrained hardware of the walker. We present a novel
approach to patient monitoring and data-driven human-in-the-loop control in the
context of smart walkers. It is able to extract a complete and compact body
representation in real-time and from inexpensive sensors, serving as a common
base for downstream metrics extraction solutions, and Human-Robot interaction
applications. Despite promising results, more data should be collected on users
with impairments, to assess its performance as a rehabilitation tool in
real-world scenarios.",arxiv
http://arxiv.org/abs/2103.12321v1,2021-03-23T05:33:59Z,2021-03-23T05:33:59Z,Learning 6DoF Grasping Using Reward-Consistent Demonstration,"As the number of the robot's degrees of freedom increases, the implementation
of robot motion becomes more complex and difficult. In this study, we focus on
learning 6DOF-grasping motion and consider dividing the grasping motion into
multiple tasks. We propose to combine imitation and reinforcement learning in
order to facilitate a more efficient learning of the desired motion. In order
to collect demonstration data as teacher data for the imitation learning, we
created a virtual reality (VR) interface that allows humans to operate the
robot intuitively. Moreover, by dividing the motion into simpler tasks, we
simplify the design of reward functions for reinforcement learning and show in
our experiments a reduction in the steps required to learn the grasping motion.",arxiv
http://arxiv.org/abs/1702.01182v1,2017-02-03T21:57:13Z,2017-02-03T21:57:13Z,Uncertainty-Aware Reinforcement Learning for Collision Avoidance,"Reinforcement learning can enable complex, adaptive behavior to be learned
automatically for autonomous robotic platforms. However, practical deployment
of reinforcement learning methods must contend with the fact that the training
process itself can be unsafe for the robot. In this paper, we consider the
specific case of a mobile robot learning to navigate an a priori unknown
environment while avoiding collisions. In order to learn collision avoidance,
the robot must experience collisions at training time. However, high-speed
collisions, even at training time, could damage the robot. A successful
learning method must therefore proceed cautiously, experiencing only low-speed
collisions until it gains confidence. To this end, we present an
uncertainty-aware model-based learning algorithm that estimates the probability
of collision together with a statistical estimate of uncertainty. By
formulating an uncertainty-dependent cost function, we show that the algorithm
naturally chooses to proceed cautiously in unfamiliar environments, and
increases the velocity of the robot in settings where it has high confidence.
Our predictive model is based on bootstrapped neural networks using dropout,
allowing it to process raw sensory inputs from high-bandwidth sensors such as
cameras. Our experimental evaluation demonstrates that our method effectively
minimizes dangerous collisions at training time in an obstacle avoidance task
for a simulated and real-world quadrotor, and a real-world RC car. Videos of
the experiments can be found at https://sites.google.com/site/probcoll.",arxiv
http://arxiv.org/abs/2004.02545v1,2020-04-06T10:39:10Z,2020-04-06T10:39:10Z,"Human action recognition with a large-scale brain-inspired photonic
  computer","The recognition of human actions in video streams is a challenging task in
computer vision, with cardinal applications in e.g. brain-computer interface
and surveillance. Deep learning has shown remarkable results recently, but can
be found hard to use in practice, as its training requires large datasets and
special purpose, energy-consuming hardware. In this work, we propose a scalable
photonic neuro-inspired architecture based on the reservoir computing paradigm,
capable of recognising video-based human actions with state-of-the-art
accuracy. Our experimental optical setup comprises off-the-shelf components,
and implements a large parallel recurrent neural network that is easy to train
and can be scaled up to hundreds of thousands of nodes. This work paves the way
towards simply reconfigurable and energy-efficient photonic information
processing systems for real-time video processing.",arxiv
http://arxiv.org/abs/1912.11350v1,2019-12-22T22:22:55Z,2019-12-22T22:22:55Z,Atmospheric turbulence removal using convolutional neural network,"This paper describes a novel deep learning-based method for mitigating the
effects of atmospheric distortion. We have built an end-to-end supervised
convolutional neural network (CNN) to reconstruct turbulence-corrupted video
sequence. Our framework has been developed on the residual learning concept,
where the spatio-temporal distortions are learnt and predicted. Our experiments
demonstrate that the proposed method can deblur, remove ripple effect and
enhance contrast of the video sequences simultaneously. Our model was trained
and tested with both simulated and real distortions. Experimental results of
the real distortions show that our method outperforms the existing ones by up
to 3.8% in term of the quality of restored images, and it achieves faster speed
than the state-of-the-art methods by up to 23 times with GPU implementation.",arxiv
http://arxiv.org/abs/2109.10460v1,2021-09-21T23:46:37Z,2021-09-21T23:46:37Z,"Graph-based Cluttered Scene Generation and Interactive Exploration using
  Deep Reinforcement Learning","We introduce a novel method to teach a robotic agent to interactively explore
cluttered yet structured scenes, such as kitchen pantries and grocery shelves,
by leveraging the physical plausibility of the scene. We propose a novel
learning framework to train an effective scene exploration policy to discover
hidden objects with minimal interactions. First, we define a novel scene
grammar to represent structured clutter. Then we train a Graph Neural Network
(GNN) based Scene Generation agent using deep reinforcement learning (deep RL),
to manipulate this Scene Grammar to create a diverse set of stable scenes, each
containing multiple hidden objects. Given such cluttered scenes, we then train
a Scene Exploration agent, using deep RL, to uncover hidden objects by
interactively rearranging the scene. We show that our learned agents hide and
discover significantly more objects than the baselines. We present quantitative
results that prove the generalization capabilities of our agents. We also
demonstrate sim-to-real transfer by successfully deploying the learned policy
on a real UR10 robot to explore real-world cluttered scenes. The supplemental
video can be found at https://www.youtube.com/watch?v=T2Jo7wwaXss.",arxiv
http://arxiv.org/abs/2005.01180v2,2020-05-05T11:49:27Z,2020-05-03T20:27:10Z,MAGES 3.0: Tying the knot of medical VR,"In this work, we present MAGES 3.0, a novel Virtual Reality (VR)-based
authoring SDK platform for accelerated surgical training and assessment. The
MAGES Software Development Kit (SDK) allows code-free prototyping of any VR
psychomotor simulation of medical operations by medical professionals, who
urgently need a tool to solve the issue of outdated medical training. Our
platform encapsulates the following novel algorithmic techniques: a)
collaborative networking layer with Geometric Algebra (GA) interpolation engine
b) supervised machine learning analytics module for real-time recommendations
and user profiling c) GA deformable cutting and tearing algorithm d) on-the-go
configurable soft body simulation for deformable surfaces.",arxiv
http://arxiv.org/abs/2103.16511v1,2021-03-30T17:13:29Z,2021-03-30T17:13:29Z,"Flatland Competition 2020: MAPF and MARL for Efficient Train
  Coordination on a Grid World","The Flatland competition aimed at finding novel approaches to solve the
vehicle re-scheduling problem (VRSP). The VRSP is concerned with scheduling
trips in traffic networks and the re-scheduling of vehicles when disruptions
occur, for example the breakdown of a vehicle. While solving the VRSP in
various settings has been an active area in operations research (OR) for
decades, the ever-growing complexity of modern railway networks makes dynamic
real-time scheduling of traffic virtually impossible. Recently, multi-agent
reinforcement learning (MARL) has successfully tackled challenging tasks where
many agents need to be coordinated, such as multiplayer video games. However,
the coordination of hundreds of agents in a real-life setting like a railway
network remains challenging and the Flatland environment used for the
competition models these real-world properties in a simplified manner.
Submissions had to bring as many trains (agents) to their target stations in as
little time as possible. While the best submissions were in the OR category,
participants found many promising MARL approaches. Using both centralized and
decentralized learning based approaches, top submissions used graph
representations of the environment to construct tree-based observations.
Further, different coordination mechanisms were implemented, such as
communication and prioritization between agents. This paper presents the
competition setup, four outstanding solutions to the competition, and a
cross-comparison between them.",arxiv
http://arxiv.org/abs/2012.12104v1,2020-12-09T05:08:41Z,2020-12-09T05:08:41Z,"A Deep Reinforcement Learning Approach for Ramp Metering Based on
  Traffic Video Data","Ramp metering that uses traffic signals to regulate vehicle flows from the
on-ramps has been widely implemented to improve vehicle mobility of the
freeway. Previous studies generally update signal timings in real-time based on
predefined traffic measures collected by point detectors, such as traffic
volumes and occupancies. Comparing with point detectors, traffic cameras-which
have been increasingly deployed on road networks-could cover larger areas and
provide more detailed traffic information. In this work, we propose a deep
reinforcement learning (DRL) method to explore the potential of traffic video
data in improving the efficiency of ramp metering. The proposed method uses
traffic video frames as inputs and learns the optimal control strategies
directly from the high-dimensional visual inputs. A real-world case study
demonstrates that, in comparison with a state-of-the-practice method, the
proposed DRL method results in 1) lower travel times in the mainline, 2)
shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream
of the merging area. The results suggest that the proposed method is able to
extract useful information from the video data for better ramp metering
controls.",arxiv
http://arxiv.org/abs/2008.04572v1,2020-08-11T08:10:58Z,2020-08-11T08:10:58Z,"An Empirical Analysis of Backward Compatibility in Machine Learning
  Systems","In many applications of machine learning (ML), updates are performed with the
goal of enhancing model performance. However, current practices for updating
models rely solely on isolated, aggregate performance analyses, overlooking
important dependencies, expectations, and needs in real-world deployments. We
consider how updates, intended to improve ML models, can introduce new errors
that can significantly affect downstream systems and users. For example,
updates in models used in cloud-based classification services, such as image
recognition, can cause unexpected erroneous behavior in systems that make calls
to the services. Prior work has shown the importance of ""backward
compatibility"" for maintaining human trust. We study challenges with backward
compatibility across different ML architectures and datasets, focusing on
common settings including data shifts with structured noise and ML employed in
inferential pipelines. Our results show that (i) compatibility issues arise
even without data shift due to optimization stochasticity, (ii) training on
large-scale noisy datasets often results in significant decreases in backward
compatibility even when model accuracy increases, and (iii) distributions of
incompatible points align with noise bias, motivating the need for
compatibility aware de-noising and robustness methods.",arxiv
http://arxiv.org/abs/1804.10334v3,2019-02-22T08:15:42Z,2018-04-27T04:07:49Z,"Deep Learning Coordinated Beamforming for Highly-Mobile Millimeter Wave
  Systems","Supporting high mobility in millimeter wave (mmWave) systems enables a wide
range of important applications such as vehicular communications and wireless
virtual/augmented reality. Realizing this in practice, though, requires
overcoming several challenges. First, the use of narrow beams and the
sensitivity of mmWave signals to blockage greatly impact the coverage and
reliability of highly-mobile links. Second, highly-mobile users in dense mmWave
deployments need to frequently hand-off between base stations (BSs), which is
associated with critical control and latency overhead. Further, identifying the
optimal beamforming vectors in large antenna array mmWave systems requires
considerable training overhead, which significantly affects the efficiency of
these mobile systems. In this paper, a novel integrated machine learning and
coordinated beamforming solution is developed to overcome these challenges and
enable highly-mobile mmWave applications. In the proposed solution, a number of
distributed yet coordinating BSs simultaneously serve a mobile user. This user
ideally needs to transmit only one uplink training pilot sequence that will be
jointly received at the coordinating BSs using omni or quasi-omni beam
patterns. These received signals draw a defining signature not only for the
user location, but also for its interaction with the surrounding environment.
The developed solution then leverages a deep learning model that learns how to
use these signatures to predict the beamforming vectors at the BSs. This
renders a comprehensive solution that supports highly-mobile mmWave
applications with reliable coverage, low latency, and negligible training
overhead. Simulation results show that the proposed deep-learning coordinated
beamforming strategy approaches the achievable rate of the genie-aided solution
that knows the optimal beamforming vectors with no training overhead.",arxiv
http://arxiv.org/abs/1905.10906v1,2019-05-26T23:55:35Z,2019-05-26T23:55:35Z,Non-Determinism in Neural Networks for Adversarial Robustness,"Recent breakthroughs in the field of deep learning have led to advancements
in a broad spectrum of tasks in computer vision, audio processing, natural
language processing and other areas. In most instances where these tasks are
deployed in real-world scenarios, the models used in them have been shown to be
susceptible to adversarial attacks, making it imperative for us to address the
challenge of their adversarial robustness. Existing techniques for adversarial
robustness fall into three broad categories: defensive distillation techniques,
adversarial training techniques, and randomized or non-deterministic model
based techniques. In this paper, we propose a novel neural network paradigm
that falls under the category of randomized models for adversarial robustness,
but differs from all existing techniques under this category in that it models
each parameter of the network as a statistical distribution with learnable
parameters. We show experimentally that this framework is highly robust to a
variety of white-box and black-box adversarial attacks, while preserving the
task-specific performance of the traditional neural network model.",arxiv
http://arxiv.org/abs/1901.09963v5,2019-11-20T21:22:50Z,2019-01-28T19:43:27Z,"Defense Methods Against Adversarial Examples for Recurrent Neural
  Networks","Adversarial examples are known to mislead deep learning models to incorrectly
classify them, even in domains where such models achieve state-of-the-art
performance. Until recently, research on both attack and defense methods
focused on image recognition, primarily using convolutional neural networks
(CNNs). In recent years, adversarial example generation methods for recurrent
neural networks (RNNs) have been published, demonstrating that RNN classifiers
are also vulnerable to such attacks. In this paper, we present a novel defense
method, termed sequence squeezing, to make RNN classifiers more robust against
such attacks. Our method differs from previous defense methods which were
designed only for non-sequence based models. We also implement four additional
RNN defense methods inspired by recently published CNN defense methods. We
evaluate our methods against state-of-the-art attacks in the cyber security
domain where real adversaries (malware developers) exist, but our methods can
be applied against other discrete sequence based adversarial attacks, e.g., in
the NLP domain. Using our methods we were able to decrease the effectiveness of
such attack from 99.9% to 15%.",arxiv
http://arxiv.org/abs/1708.05208v1,2017-08-17T11:31:22Z,2017-08-17T11:31:22Z,"Automatic HVAC Control with Real-time Occupancy Recognition and
  Simulation-guided Model Predictive Control in Low-cost Embedded System","Intelligent building automation systems can reduce the energy consumption of
heating, ventilation and air-conditioning (HVAC) units by sensing the comfort
requirements automatically and scheduling the HVAC operations dynamically.
Traditional building automation systems rely on fairly inaccurate occupancy
sensors and basic predictive control using oversimplified building thermal
response models, all of which prevent such systems from reaching their full
potential. Such limitations can now be avoided due to the recent developments
in embedded system technologies, which provide viable low-cost computing
platforms with powerful processors and sizeable memory storage in a small
footprint. As a result, building automation systems can now efficiently execute
highly-sophisticated computational tasks, such as real-time video processing
and accurate thermal-response simulations. With this in mind, we designed and
implemented an occupancy-predictive HVAC control system in a low-cost yet
powerful embedded system (using Raspberry Pi 3) to demonstrate the following
key features for building automation: (1) real-time occupancy recognition using
video-processing and machine-learning techniques, (2) dynamic analysis and
prediction of occupancy patterns, and (3) model predictive control for HVAC
operations guided by real-time building thermal response simulations (using an
on-board EnergyPlus simulator). We deployed and evaluated our system for
providing automatic HVAC control in the large public indoor space of a mosque,
thereby achieving significant energy savings.",arxiv
http://arxiv.org/abs/1906.04591v4,2021-08-13T11:04:40Z,2019-06-10T09:32:41Z,"CNN depth analysis with different channel inputs for Acoustic Scene
  Classification","Acoustic scene classification (ASC) has been approached in the last years
using deep learning techniques such as convolutional neural networks or
recurrent neural networks. Many state-of-the-art solutions are based on image
classification frameworks and, as such, a 2D representation of the audio signal
is considered for training these networks. Finding the most suitable audio
representation is still a research area of interest. In this paper, different
log-Mel representations and combinations are analyzed. Experiments show that
the best results are obtained using the harmonic and percussive components plus
the difference between left and right stereo channels, (L-R). On the other
hand, it is a common strategy to ensemble different models in order to increase
the final accuracy. Even though averaging different model predictions is a
common choice, an exhaustive analysis of different ensemble techniques has not
been presented in ASC problems. In this paper, geometric and arithmetic mean
plus the Ordered Weighted Averaging (OWA) operator are studied as aggregation
operators for the output of the different models of the ensemble. Finally, the
work carried out in this paper is highly oriented towards real-time
implementations. In this context, as the number of applications for audio
classification on edge devices is increasing exponentially, we also analyze
different network depths and efficient solutions for aggregating ensemble
predictions.",arxiv
http://arxiv.org/abs/2110.08307v1,2021-10-15T18:29:46Z,2021-10-15T18:29:46Z,GrowSpace: Learning How to Shape Plants,"Plants are dynamic systems that are integral to our existence and survival.
Plants face environment changes and adapt over time to their surrounding
conditions. We argue that plant responses to an environmental stimulus are a
good example of a real-world problem that can be approached within a
reinforcement learning (RL)framework. With the objective of controlling a plant
by moving the light source, we propose GrowSpace, as a new RL benchmark. The
back-end of the simulator is implemented using the Space Colonisation
Algorithm, a plant growing model based on competition for space. Compared to
video game RL environments, this simulator addresses a real-world problem and
serves as a test bed to visualize plant growth and movement in a faster way
than physical experiments. GrowSpace is composed of a suite of challenges that
tackle several problems such as control, multi-stage learning,fairness and
multi-objective learning. We provide agent baselines alongside case studies to
demonstrate the difficulty of the proposed benchmark.",arxiv
http://arxiv.org/abs/2005.09237v1,2020-05-19T06:25:52Z,2020-05-19T06:25:52Z,"Acoustic Echo Cancellation by Combining Adaptive Digital Filter and
  Recurrent Neural Network","Acoustic Echo Cancellation (AEC) plays a key role in voice interaction. Due
to the explicit mathematical principle and intelligent nature to accommodate
conditions, adaptive filters with different types of implementations are always
used for AEC, giving considerable performance. However, there would be some
kinds of residual echo in the results, including linear residue introduced by
mismatching between estimation and the reality and non-linear residue mostly
caused by non-linear components on the audio devices. The linear residue can be
reduced with elaborate structure and methods, leaving the non-linear residue
intractable for suppression. Though, some non-linear processing methods have
already be raised, they are complicated and inefficient for suppression, and
would bring damage to the speech audio. In this paper, a fusion scheme by
combining adaptive filter and neural network is proposed for AEC. The echo
could be reduced in a large scale by adaptive filtering, resulting in little
residual echo. Though it is much smaller than speech audio, it could also be
perceived by human ear and would make communication annoy. The neural network
is elaborately designed and trained for suppressing such residual echo.
Experiments compared with prevailing methods are conducted, validating the
effectiveness and superiority of the proposed combination scheme.",arxiv
http://arxiv.org/abs/2102.12911v1,2021-02-25T15:02:47Z,2021-02-25T15:02:47Z,"Blocks World Revisited: The Effect of Self-Occlusion on Classification
  by Convolutional Neural Networks","Despite the recent successes in computer vision, there remain new avenues to
explore. In this work, we propose a new dataset to investigate the effect of
self-occlusion on deep neural networks. With TEOS (The Effect of
Self-Occlusion), we propose a 3D blocks world dataset that focuses on the
geometric shape of 3D objects and their omnipresent challenge of
self-occlusion. We designed TEOS to investigate the role of self-occlusion in
the context of object classification. Even though remarkable progress has been
seen in object classification, self-occlusion is a challenge. In the
real-world, self-occlusion of 3D objects still presents significant challenges
for deep learning approaches. However, humans deal with this by deploying
complex strategies, for instance, by changing the viewpoint or manipulating the
scene to gather necessary information. With TEOS, we present a dataset of two
difficulty levels (L1 and L2 ), containing 36 and 12 objects, respectively. We
provide 738 uniformly sampled views of each object, their mask, object and
camera position, orientation, amount of self-occlusion, as well as the CAD
model of each object. We present baseline evaluations with five well-known
classification deep neural networks and show that TEOS poses a significant
challenge for all of them. The dataset, as well as the pre-trained models, are
made publicly available for the scientific community under
https://nvision2.data.eecs.yorku.ca/TEOS.",arxiv
http://arxiv.org/abs/2105.11494v1,2021-05-24T18:40:18Z,2021-05-24T18:40:18Z,3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation,"In this paper, we propose a method for coarse camera pose computation which
is robust to viewing conditions and does not require a detailed model of the
scene. This method meets the growing need of easy deployment of robotics or
augmented reality applications in any environments, especially those for which
no accurate 3D model nor huge amount of ground truth data are available. It
exploits the ability of deep learning techniques to reliably detect objects
regardless of viewing conditions. Previous works have also shown that
abstracting the geometry of a scene of objects by an ellipsoid cloud allows to
compute the camera pose accurately enough for various application needs. Though
promising, these approaches use the ellipses fitted to the detection bounding
boxes as an approximation of the imaged objects. In this paper, we go one step
further and propose a learning-based method which detects improved elliptic
approximations of objects which are coherent with the 3D ellipsoid in terms of
perspective projection. Experiments prove that the accuracy of the computed
pose significantly increases thanks to our method and is more robust to the
variability of the boundaries of the detection boxes. This is achieved with
very little effort in terms of training data acquisition -- a few hundred
calibrated images of which only three need manual object annotation. Code and
models are released at
https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.",arxiv
http://arxiv.org/abs/2006.04001v2,2020-06-09T12:25:49Z,2020-06-07T00:11:42Z,Real-Time Model Calibration with Deep Reinforcement Learning,"The dynamic, real-time, and accurate inference of model parameters from
empirical data is of great importance in many scientific and engineering
disciplines that use computational models (such as a digital twin) for the
analysis and prediction of complex physical processes. However, fast and
accurate inference for processes with large and high dimensional datasets
cannot easily be achieved with state-of-the-art methods under noisy real-world
conditions. The primary reason is that the inference of model parameters with
traditional techniques based on optimisation or sampling often suffers from
computational and statistical challenges, resulting in a trade-off between
accuracy and deployment time. In this paper, we propose a novel framework for
inference of model parameters based on reinforcement learning. The contribution
of the paper is twofold: 1) We reformulate the inference problem as a tracking
problem with the objective of learning a policy that forces the response of the
physics-based model to follow the observations; 2) We propose the constrained
Lyapunov-based actor-critic (CLAC) algorithm to enable the robust and accurate
inference of physics-based model parameters in real time under noisy real-world
conditions. The proposed methodology is demonstrated and evaluated on two
model-based diagnostics test cases utilizing two different physics-based models
of turbofan engines. The performance of the methodology is compared to that of
two alternative approaches: a state update method (unscented Kalman filter) and
a supervised end-to-end mapping with deep neural networks. The experimental
results demonstrate that the proposed methodology outperforms all other tested
methods in terms of speed and robustness, with high inference accuracy.",arxiv
http://arxiv.org/abs/1811.10869v1,2018-11-27T08:28:52Z,2018-11-27T08:28:52Z,"Efficient non-uniform quantizer for quantized neural network targeting
  reconfigurable hardware","Convolutional Neural Networks (CNN) has become more popular choice for
various tasks such as computer vision, speech recognition and natural language
processing. Thanks to their large computational capability and throughput, GPUs
,which are not power efficient and therefore does not suit low power systems
such as mobile devices, are the most common platform for both training and
inferencing tasks. Recent studies has shown that FPGAs can provide a good
alternative to GPUs as a CNN accelerator, due to their re-configurable nature,
low power and small latency. In order for FPGA-based accelerators outperform
GPUs in inference task, both the parameters of the network and the activations
must be quantized. While most works use uniform quantizers for both parameters
and activations, it is not always the optimal one, and a non-uniform quantizer
need to be considered. In this work we introduce a custom hardware-friendly
approach to implement non-uniform quantizers. In addition, we use a single
scale integer representation of both parameters and activations, for both
training and inference. The combined method yields a hardware efficient
non-uniform quantizer, fit for real-time applications. We have tested our
method on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18
and VGG-like architectures, and saw little degradation in accuracy.",arxiv
http://arxiv.org/abs/2011.11557v1,2020-11-23T17:11:50Z,2020-11-23T17:11:50Z,"Planar 3D Transfer Learning for End to End Unimodal MRI Unbalanced Data
  Segmentation","We present a novel approach of 2D to 3D transfer learning based on mapping
pre-trained 2D convolutional neural network weights into planar 3D kernels. The
method is validated by the proposed planar 3D res-u-net network with encoder
transferred from the 2D VGG-16, which is applied for a single-stage unbalanced
3D image data segmentation. In particular, we evaluate the method on the MICCAI
2016 MS lesion segmentation challenge dataset utilizing solely fluid-attenuated
inversion recovery (FLAIR) sequence without brain extraction for training and
inference to simulate real medical praxis. The planar 3D res-u-net network
performed the best both in sensitivity and Dice score amongst end to end
methods processing raw MRI scans and achieved comparable Dice score to a
state-of-the-art unimodal not end to end approach. Complete source code was
released under the open-source license, and this paper complies with the
Machine learning reproducibility checklist. By implementing practical transfer
learning for 3D data representation, we could segment heavily unbalanced data
without selective sampling and achieved more reliable results using less
training data in a single modality. From a medical perspective, the unimodal
approach gives an advantage in real praxis as it does not require
co-registration nor additional scanning time during an examination. Although
modern medical imaging methods capture high-resolution 3D anatomy scans
suitable for computer-aided detection system processing, deployment of
automatic systems for interpretation of radiology imaging is still rather
theoretical in many medical areas. Our work aims to bridge the gap by offering
a solution for partial research questions.",arxiv
http://arxiv.org/abs/1909.06993v2,2020-03-08T13:22:41Z,2019-09-16T05:23:14Z,"Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal
  Representations","Machines are a long way from robustly solving open-world perception-control
tasks, such as first-person view (FPV) aerial navigation. While recent advances
in end-to-end Machine Learning, especially Imitation and Reinforcement Learning
appear promising, they are constrained by the need of large amounts of
difficult-to-collect labeled real-world data. Simulated data, on the other
hand, is easy to generate, but generally does not render safe behaviors in
diverse real-life scenarios. In this work we propose a novel method for
learning robust visuomotor policies for real-world deployment which can be
trained purely with simulated data. We develop rich state representations that
combine supervised and unsupervised environment data. Our approach takes a
cross-modal perspective, where separate modalities correspond to the raw camera
data and the system states relevant to the task, such as the relative pose of
gates to the drone in the case of drone racing. We feed both data modalities
into a novel factored architecture, which learns a joint low-dimensional
embedding via Variational Auto Encoders. This compact representation is then
fed into a control policy, which we trained using imitation learning with
expert trajectories in a simulator. We analyze the rich latent spaces learned
with our proposed representations, and show that the use of our cross-modal
architecture significantly improves control policy performance as compared to
end-to-end learning or purely unsupervised feature extractors. We also present
real-world results for drone navigation through gates in different track
configurations and environmental conditions. Our proposed method, which runs
fully onboard, can successfully generalize the learned representations and
policies across simulation and reality, significantly outperforming baseline
approaches.
  Supplementary video: https://youtu.be/VKc3A5HlUU8",arxiv
http://arxiv.org/abs/1911.06091v1,2019-11-14T13:49:23Z,2019-11-14T13:49:23Z,"EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional
  Neural Network Object Detectors","Visual intelligence at the edge is becoming a growing necessity for low
latency applications and situations where real-time decision is vital. Object
detection, the first step in visual data analytics, has enjoyed significant
improvements in terms of state-of-the-art accuracy due to the emergence of
Convolutional Neural Networks (CNNs) and Deep Learning. However, such complex
paradigms intrude increasing computational demands and hence prevent their
deployment on resource-constrained devices. In this work, we propose a
hierarchical framework that enables to detect objects in high-resolution video
frames, and maintain the accuracy of state-of-the-art CNN-based object
detectors while outperforming existing works in terms of processing speed when
targeting a low-power embedded processor using an intelligent data reduction
mechanism. Moreover, a use-case for pedestrian detection from
Unmanned-Areal-Vehicle (UAV) is presented showing the impact that the proposed
approach has on sensitivity, average processing time and power consumption when
is implemented on different platforms. Using the proposed selection process our
framework manages to reduce the processed data by 100x leading to under 4W
power consumption on different edge devices.",arxiv
http://arxiv.org/abs/1909.07437v4,2020-12-17T02:27:29Z,2019-09-13T17:46:13Z,Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,"Emerging AI-enabled applications such as augmented/virtual reality (AR/VR)
leverage multiple deep neural network (DNN) models for sub-tasks such as object
detection, hand tracking, and so on. Because of the diversity of the sub-tasks,
the layers within and across the DNN models are highly heterogeneous in
operation and shape. Such layer heterogeneity is a challenge for a fixed
dataflow accelerator (FDA) that employs a fixed dataflow on a single
accelerator substrate since each layer prefers different dataflows (computation
order and parallelization) and tile sizes. Reconfigurable DNN accelerators
(RDAs) have been proposed to adapt their dataflows to diverse layers to address
the challenge. However, the dataflow flexibility in RDAs is enabled at the area
and energy costs of expensive hardware structures (switches, controller, etc.)
and per-layer reconfiguration.
  Alternatively, this work proposes a new class of accelerators, heterogeneous
dataflow accelerators (HDAs), which deploys multiple sub-accelerators each
supporting a different dataflow. HDAs enable coarser-grained dataflow
flexibility than RDAs with higher energy efficiency and lower area cost
comparable to FDAs. To exploit such benefits, hardware resource partitioning
across sub-accelerators and layer execution schedule need to be carefully
optimized. Therefore, we also present Herald, which co-optimizes hardware
partitioning and layer execution schedule. Using Herald on a suite of AR/VR and
MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which
demonstrates 65.3% lower latency and 5.0% lower energy than the best FDAs and
22.0% lower energy at the cost of 20.7% higher latency than a state-of-the-art
RDA. The results suggest that HDA is an alternative class of Pareto-optimal
accelerators to RDA with strength in energy, which can be a better choice than
RDAs depending on the use cases.",arxiv
http://arxiv.org/abs/1808.00362v1,2018-08-01T15:13:48Z,2018-08-01T15:13:48Z,Deep Appearance Models for Face Rendering,"We introduce a deep appearance model for rendering the human face. Inspired
by Active Appearance Models, we develop a data-driven rendering pipeline that
learns a joint representation of facial geometry and appearance from a
multiview capture setup. Vertex positions and view-specific textures are
modeled using a deep variational autoencoder that captures complex nonlinear
effects while producing a smooth and compact latent representation.
View-specific texture enables the modeling of view-dependent effects such as
specularity. In addition, it can also correct for imperfect geometry stemming
from biased or low resolution estimates. This is a significant departure from
the traditional graphics pipeline, which requires highly accurate geometry as
well as all elements of the shading model to achieve realism through
physically-inspired light transport. Acquiring such a high level of accuracy is
difficult in practice, especially for complex and intricate parts of the face,
such as eyelashes and the oral cavity. These are handled naturally by our
approach, which does not rely on precise estimates of geometry. Instead, the
shading model accommodates deficiencies in geometry though the flexibility
afforded by the neural network employed. At inference time, we condition the
decoding network on the viewpoint of the camera in order to generate the
appropriate texture for rendering. The resulting system can be implemented
simply using existing rendering engines through dynamic textures with flat
lighting. This representation, together with a novel unsupervised technique for
mapping images to facial states, results in a system that is naturally suited
to real-time interactive settings such as Virtual Reality (VR).",arxiv
http://arxiv.org/abs/2008.12487v1,2020-08-28T05:49:13Z,2020-08-28T05:49:13Z,Classification of Imagined Speech Using Siamese Neural Network,"Imagined speech is spotlighted as a new trend in the brain-machine interface
due to its application as an intuitive communication tool. However, previous
studies have shown low classification performance, therefore its use in
real-life is not feasible. In addition, no suitable method to analyze it has
been found. Recently, deep learning algorithms have been applied to this
paradigm. However, due to the small amount of data, the increase in
classification performance is limited. To tackle these issues, in this study,
we proposed an end-to-end framework using Siamese neural network encoder, which
learns the discriminant features by considering the distance between classes.
The imagined words (e.g., arriba (up), abajo (down), derecha (right), izquierda
(left), adelante (forward), and atr\'as (backward)) were classified using the
raw electroencephalography (EEG) signals. We obtained a 6-class classification
accuracy of 31.40% for imagined speech, which significantly outperformed other
methods. This was possible because the Siamese neural network, which increases
the distance between dissimilar samples while decreasing the distance between
similar samples, was used. In this regard, our method can learn discriminant
features from a small dataset. The proposed framework would help to increase
the classification performance of imagined speech for a small amount of data
and implement an intuitive communication system.",arxiv
http://arxiv.org/abs/1905.03554v1,2019-05-09T11:52:10Z,2019-05-09T11:52:10Z,1D Convolutional Neural Networks and Applications: A Survey,"During the last decade, Convolutional Neural Networks (CNNs) have become the
de facto standard for various Computer Vision and Machine Learning operations.
CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating
convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and
millions of parameters have the ability to learn complex objects and patterns
providing that they can be trained on a massive size visual database with
ground-truth labels. With a proper training, this unique ability makes them the
primary tool for various engineering applications for 2D signals such as images
and video frames. Yet, this may not be a viable option in numerous applications
over 1D signals especially when the training data is scarce or
application-specific. To address this issue, 1D CNNs have recently been
proposed and immediately achieved the state-of-the-art performance levels in
several applications such as personalized biomedical data classification and
early diagnosis, structural health monitoring, anomaly detection and
identification in power electronics and motor-fault detection. Another major
advantage is that a real-time and low-cost hardware implementation is feasible
due to the simple and compact configuration of 1D CNNs that perform only 1D
convolutions (scalar multiplications and additions). This paper presents a
comprehensive review of the general architecture and principals of 1D CNNs
along with their major engineering applications, especially focused on the
recent progress in this field. Their state-of-the-art performance is
highlighted concluding with their unique properties. The benchmark datasets and
the principal 1D CNN software used in those applications are also publically
shared in a dedicated website.",arxiv
http://arxiv.org/abs/1802.08960v2,2019-02-01T17:08:34Z,2018-02-25T06:47:30Z,"Bonnet: An Open-Source Training and Deployment Framework for Semantic
  Segmentation in Robotics using CNNs","The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos.",arxiv
http://arxiv.org/abs/1810.04538v1,2018-10-10T14:04:08Z,2018-10-10T14:04:08Z,"Secure Deep Learning Engineering: A Software Quality Assurance
  Perspective","Over the past decades, deep learning (DL) systems have achieved tremendous
success and gained great popularity in various applications, such as
intelligent machines, image processing, speech processing, and medical
diagnostics. Deep neural networks are the key driving force behind its recent
success, but still seem to be a magic black box lacking interpretability and
understanding. This brings up many open safety and security issues with
enormous and urgent demands on rigorous methodologies and engineering practice
for quality enhancement. A plethora of studies have shown that the
state-of-the-art DL systems suffer from defects and vulnerabilities that can
lead to severe loss and tragedies, especially when applied to real-world
safety-critical applications. In this paper, we perform a large-scale study and
construct a paper repository of 223 relevant works to the quality assurance,
security, and interpretation of deep learning. We, from a software quality
assurance perspective, pinpoint challenges and future opportunities towards
universal secure deep learning engineering. We hope this work and the
accompanied paper repository can pave the path for the software engineering
community towards addressing the pressing industrial demand of secure
intelligent applications.",arxiv
http://arxiv.org/abs/2104.08002v1,2021-04-16T09:54:30Z,2021-04-16T09:54:30Z,Efficient and Generic 1D Dilated Convolution Layer for Deep Learning,"Convolutional neural networks (CNNs) have found many applications in tasks
involving two-dimensional (2D) data, such as image classification and image
processing. Therefore, 2D convolution layers have been heavily optimized on
CPUs and GPUs. However, in many applications - for example genomics and speech
recognition, the data can be one-dimensional (1D). Such applications can
benefit from optimized 1D convolution layers. In this work, we introduce our
efficient implementation of a generic 1D convolution layer covering a wide
range of parameters. It is optimized for x86 CPU architectures, in particular,
for architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions.
We use the LIBXSMM library's batch-reduce General Matrix Multiplication
(BRGEMM) kernel for FP32 and BFloat16 precision. We demonstrate that our
implementation can achieve up to 80% efficiency on Intel Xeon Cascade Lake and
Cooper Lake CPUs. Additionally, we show the generalization capability of our
BRGEMM based approach by achieving high efficiency across a range of
parameters. We consistently achieve higher efficiency than the 1D convolution
layer with Intel oneDNN library backend for varying input tensor widths, filter
widths, number of channels, filters, and dilation parameters. Finally, we
demonstrate the performance of our optimized 1D convolution layer by utilizing
it in the end-to-end neural network training with real genomics datasets and
achieve up to 6.86x speedup over the oneDNN library-based implementation on
Cascade Lake CPUs. We also demonstrate the scaling with 16 sockets of
Cascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs
using a similar power envelop. In the end-to-end training, we get a speedup of
1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on
Cooper Lake with BFloat16 over eight V100 GPUs with FP32.",arxiv
http://arxiv.org/abs/2007.05832v1,2020-07-11T19:02:33Z,2020-07-11T19:02:33Z,Optimizing Prediction Serving on Low-Latency Serverless Dataflow,"Prediction serving systems are designed to provide large volumes of
low-latency inferences machine learning models. These systems mix data
processing and computationally intensive model inference and benefit from
multiple heterogeneous processors and distributed computing resources. In this
paper, we argue that a familiar dataflow API is well-suited to this
latency-sensitive task, and amenable to optimization even with unmodified
black-box ML models. We present the design of Cloudflow, a system that provides
this API and realizes it on an autoscaling serverless backend. Cloudflow
transparently implements performance-critical optimizations including operator
fusion and competitive execution. Our evaluation shows that Cloudflow's
optimizations yield significant performance improvements on synthetic workloads
and that Cloudflow outperforms state-of-the-art prediction serving systems by
as much as 2x on real-world prediction pipelines, meeting latency goals of
demanding applications like real-time video analysis.",arxiv
http://arxiv.org/abs/1702.07825v2,2017-03-07T23:09:23Z,2017-02-25T03:11:04Z,Deep Voice: Real-time Neural Text-to-Speech,"We present Deep Voice, a production-quality text-to-speech system constructed
entirely from deep neural networks. Deep Voice lays the groundwork for truly
end-to-end neural speech synthesis. The system comprises five major building
blocks: a segmentation model for locating phoneme boundaries, a
grapheme-to-phoneme conversion model, a phoneme duration prediction model, a
fundamental frequency prediction model, and an audio synthesis model. For the
segmentation model, we propose a novel way of performing phoneme boundary
detection with deep neural networks using connectionist temporal classification
(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet
that requires fewer parameters and trains faster than the original. By using a
neural network for each component, our system is simpler and more flexible than
traditional text-to-speech systems, where each component requires laborious
feature engineering and extensive domain expertise. Finally, we show that
inference with our system can be performed faster than real time and describe
optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x
speedups over existing implementations.",arxiv
http://arxiv.org/abs/1909.10976v1,2019-09-24T14:58:07Z,2019-09-24T14:58:07Z,"Synthetic dataset generation for object-to-model deep learning in
  industrial applications","The availability of large image data sets has been a crucial factor in the
success of deep learning-based classification and detection methods. While data
sets for everyday objects are widely available, data for specific industrial
use-cases (e.g. identifying packaged products in a warehouse) remains scarce.
In such cases, the data sets have to be created from scratch, placing a crucial
bottleneck on the deployment of deep learning techniques in industrial
applications.
  We present work carried out in collaboration with a leading UK online
supermarket, with the aim of creating a computer vision system capable of
detecting and identifying unique supermarket products in a warehouse setting.
To this end, we demonstrate a framework for using synthetic data to create an
end-to-end deep learning pipeline, beginning with real-world objects and
culminating in a trained model.
  Our method is based on the generation of a synthetic dataset from 3D models
obtained by applying photogrammetry techniques to real-world objects. Using
100k synthetic images generated from 60 real images per class, an InceptionV3
convolutional neural network (CNN) was trained, which achieved classification
accuracy of 95.8% on a separately acquired test set of real supermarket product
images. The image generation process supports automatic pixel annotation. This
eliminates the prohibitively expensive manual annotation typically required for
detection tasks. Based on this readily available data, a one-stage RetinaNet
detector was trained on the synthetic, annotated images to produce a detector
that can accurately localize and classify the specimen products in real-time.",arxiv
http://arxiv.org/abs/2005.07424v1,2020-05-15T09:05:17Z,2020-05-15T09:05:17Z,"Exploring the Capabilities and Limits of 3D Monocular Object Detection
  -- A Study on Simulation and Real World Data","3D object detection based on monocular camera data is a key enabler for
autonomous driving. The task however, is ill-posed due to lack of depth
information in 2D images. Recent deep learning methods show promising results
to recover depth information from single images by learning priors about the
environment. Several competing strategies tackle this problem. In addition to
the network design, the major difference of these competing approaches lies in
using a supervised or self-supervised optimization loss function, which require
different data and ground truth information. In this paper, we evaluate the
performance of a 3D object detection pipeline which is parameterizable with
different depth estimation configurations. We implement a simple distance
calculation approach based on camera intrinsics and 2D bounding box size, a
self-supervised, and a supervised learning approach for depth estimation.
  Ground truth depth information cannot be recorded reliable in real world
scenarios. This shifts our training focus to simulation data. In simulation,
labeling and ground truth generation can be automatized. We evaluate the
detection pipeline on simulator data and a real world sequence from an
autonomous vehicle on a race track. The benefit of simulation training to real
world application is investigated. Advantages and drawbacks of the different
depth estimation strategies are discussed.",arxiv
http://arxiv.org/abs/1711.02666v1,2017-11-07T17:10:28Z,2017-11-07T17:10:28Z,"Tensor-Generative Adversarial Network with Two-dimensional Sparse
  Coding: Application to Real-time Indoor Localization","Localization technology is important for the development of indoor
location-based services (LBS). Global Positioning System (GPS) becomes invalid
in indoor environments due to the non-line-of-sight issue, so it is urgent to
develop a real-time high-accuracy localization approach for smartphones.
However, accurate localization is challenging due to issues such as real-time
response requirements, limited fingerprint samples and mobile device storage.
To address these problems, we propose a novel deep learning architecture:
Tensor-Generative Adversarial Network (TGAN).
  We first introduce a transform-based 3D tensor to model fingerprint samples.
Instead of those passive methods that construct a fingerprint database as a
prior, our model applies artificial neural network with deep learning to train
network classifiers and then gives out estimations. Then we propose a novel
tensor-based super-resolution scheme using the generative adversarial network
(GAN) that adopts sparse coding as the generator network and a residual
learning network as the discriminator. Further, we analyze the performance of
tensor-GAN and implement a trace-based localization experiment, which achieves
better performance. Compared to existing methods for smartphones indoor
positioning, that are energy-consuming and high demands on devices, TGAN can
give out an improved solution in localization accuracy, response time and
implementation complexity.",arxiv
http://arxiv.org/abs/2007.15152v2,2020-09-18T01:32:28Z,2020-07-29T23:41:33Z,"Accelerating Multi-attribute Unsupervised Seismic Facies Analysis With
  RAPIDS","Classification of seismic facies is done by clustering seismic data samples
based on their attributes. Year after year, 3D datasets used by exploration
geophysics increase in size, complexity, and number of attributes, requiring a
continuous rise in the classification performance. In this work, we explore the
use of Graphics Processing Units (GPUs) to perform the classification of
seismic surveys using the well-established Machine Learning (ML) method
k-means. We show that the high-performance distributed implementation of the
k-means algorithm available at the RAPIDS library can be used to classify
facies in large seismic datasets much faster than a classical parallel CPU
implementation (up to 258-fold faster in NVIDIA V100 GPUs), especially for
large seismic blocks. We tested the algorithm with different real seismic
volumes, including Netherlands, Parihaka, and Kahu (from 12GB to 66GB).",arxiv
http://arxiv.org/abs/2003.11100v1,2020-03-24T20:15:12Z,2020-03-24T20:15:12Z,"How deep is your encoder: an analysis of features descriptors for an
  autoencoder-based audio-visual quality metric","The development of audio-visual quality assessment models poses a number of
challenges in order to obtain accurate predictions. One of these challenges is
the modelling of the complex interaction that audio and visual stimuli have and
how this interaction is interpreted by human users. The No-Reference
Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with
this problem from a machine learning perspective. The metric receives two sets
of audio and video features descriptors and produces a low-dimensional set of
features used to predict the audio-visual quality. A basic implementation of
NAViDAd was able to produce accurate predictions tested with a range of
different audio-visual databases. The current work performs an ablation study
on the base architecture of the metric. Several modules are removed or
re-trained using different configurations to have a better understanding of the
metric functionality. The results presented in this study provided important
feedback that allows us to understand the real capacity of the metric's
architecture and eventually develop a much better audio-visual quality metric.",arxiv
http://arxiv.org/abs/1901.05147v1,2019-01-16T06:10:45Z,2019-01-16T06:10:45Z,The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings.",arxiv
http://arxiv.org/abs/1905.03854v2,2020-09-07T15:55:40Z,2019-05-05T01:06:48Z,"Zygarde: Time-Sensitive On-Device Deep Inference and Adaptation on
  Intermittently-Powered Systems","We propose Zygarde -- which is an energy -- and accuracy-aware soft real-time
task scheduling framework for batteryless systems that flexibly execute deep
learning tasks1 that are suitable for running on microcontrollers. The sporadic
nature of harvested energy, resource constraints of the embedded platform, and
the computational demand of deep neural networks (DNNs) pose a unique and
challenging real-time scheduling problem for which no solutions have been
proposed in the literature. We empirically study the problem and model the
energy harvesting pattern as well as the trade-off between the accuracy and
execution of a DNN. We develop an imprecise computing-based scheduling
algorithm that improves the timeliness of DNN tasks on intermittently powered
systems. We evaluate Zygarde using four standard datasets as well as by
deploying it in six real-life applications involving audio and camera sensor
systems. Results show that Zygarde decreases the execution time by up to 26%
and schedules 9%-34% more tasks with up to 21% higher inference accuracy,
compared to traditional schedulers such as the earliest deadline first (EDF).",arxiv
http://arxiv.org/abs/1610.02132v4,2017-12-06T18:28:32Z,2016-10-07T03:44:34Z,QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding,"Parallel implementations of stochastic gradient descent (SGD) have received
significant research attention, thanks to excellent scalability properties of
this algorithm, and to its efficiency in the context of training deep neural
networks. A fundamental barrier for parallelizing large-scale SGD is the fact
that the cost of communicating the gradient updates between nodes can be very
large. Consequently, lossy compression heuristics have been proposed, by which
nodes only communicate quantized gradients. Although effective in practice,
these heuristics do not always provably converge, and it is not clear whether
they are optimal.
  In this paper, we propose Quantized SGD (QSGD), a family of compression
schemes which allow the compression of gradient updates at each node, while
guaranteeing convergence under standard assumptions. QSGD allows the user to
trade off compression and convergence time: it can communicate a sublinear
number of bits per iteration in the model dimension, and can achieve
asymptotically optimal communication cost. We complement our theoretical
results with empirical data, showing that QSGD can significantly reduce
communication cost, while being competitive with standard uncompressed
techniques on a variety of real tasks.
  In particular, experiments show that gradient quantization applied to
training of deep neural networks for image classification and automated speech
recognition can lead to significant reductions in communication cost, and
end-to-end training time. For instance, on 16 GPUs, we are able to train a
ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show
that there exist generic parameter settings under which all known network
architectures preserve or slightly improve their full accuracy when using
quantization.",arxiv
http://arxiv.org/abs/1801.05889v1,2017-12-06T02:49:27Z,2017-12-06T02:49:27Z,"Perceived Audiovisual Quality Modelling based on Decison Trees, Genetic
  Programming and Neural Networks","Our objective is to build machine learning based models that predict
audiovisual quality directly from a set of correlated parameters that are
extracted from a target quality dataset. We have used the bitstream version of
the INRS audiovisual quality dataset that reflects contemporary real-time
configurations for video frame rate, video quantization, noise reduction
parameters and network packet loss rate. We have utilized this dataset to build
bitstream perceived quality estimation models based on the Random Forests,
Bagging, Deep Learning and Genetic Programming methods.
  We have taken an empirical approach and have generated models varying from
very simple to the most complex depending on the number of features used from
the quality dataset. Random Forests and Bagging models have overall generated
the most accurate results in terms of RMSE and Pearson correlation coefficient
values. Deep Learning and Genetic Programming based bitstream models have also
achieved good results but that high performance was observed only with a
limited range of features. We have also obtained the epsilon-insensitive RMSE
values for each model and have computed the significance of the difference
between the correlation coefficients.
  Overall we conclude that computing the bitstream information is worth the
effort it takes to generate and helps to build more accurate models for
real-time communications. However, it is useful only for the deployment of the
right algorithms with the carefully selected subset of the features. The
dataset and tools that have been developed during this research are publicly
available for research and development purposes.",arxiv
http://arxiv.org/abs/1911.01562v1,2019-11-05T01:40:42Z,2019-11-05T01:40:42Z,"DeepRacer: Educational Autonomous Racing Platform for Experimentation
  with Sim2Real Reinforcement Learning","DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ.",arxiv
http://arxiv.org/abs/2107.04284v2,2021-08-14T07:27:49Z,2021-07-09T07:55:21Z,"Universal 3-Dimensional Perturbations for Black-Box Attacks on Video
  Recognition Systems","Widely deployed deep neural network (DNN) models have been proven to be
vulnerable to adversarial perturbations in many applications (e.g., image,
audio and text classifications). To date, there are only a few adversarial
perturbations proposed to deviate the DNN models in video recognition systems
by simply injecting 2D perturbations into video frames. However, such attacks
may overly perturb the videos without learning the spatio-temporal features
(across temporal frames), which are commonly extracted by DNN models for video
recognition. To our best knowledge, we propose the first black-box attack
framework that generates universal 3-dimensional (U3D) perturbations to subvert
a variety of video recognition systems. U3D has many advantages, such as (1) as
the transfer-based attack, U3D can universally attack multiple DNN models for
video recognition without accessing to the target DNN model; (2) the high
transferability of U3D makes such universal black-box attack easy-to-launch,
which can be further enhanced by integrating queries over the target model when
necessary; (3) U3D ensures human-imperceptibility; (4) U3D can bypass the
existing state-of-the-art defense schemes; (5) U3D can be efficiently generated
with a few pre-learned parameters, and then immediately injected to attack
real-time DNN-based video recognition systems. We have conducted extensive
experiments to evaluate U3D on multiple DNN models and three large-scale video
datasets. The experimental results demonstrate its superiority and
practicality.",arxiv
http://arxiv.org/abs/2101.02000v1,2021-01-06T13:15:21Z,2021-01-06T13:15:21Z,Weakly-Supervised Multi-Face 3D Reconstruction,"3D face reconstruction plays a very important role in many real-world
multimedia applications, including digital entertainment, social media,
affection analysis, and person identification. The de-facto pipeline for
estimating the parametric face model from an image requires to firstly detect
the facial regions with landmarks, and then crop each face to feed the deep
learning-based regressor. Comparing to the conventional methods performing
forward inference for each detected instance independently, we suggest an
effective end-to-end framework for multi-face 3D reconstruction, which is able
to predict the model parameters of multiple instances simultaneously using
single network inference. Our proposed approach not only greatly reduces the
computational redundancy in feature extraction but also makes the deployment
procedure much easier using the single network model. More importantly, we
employ the same global camera model for the reconstructed faces in each image,
which makes it possible to recover the relative head positions and orientations
in the 3D scene. We have conducted extensive experiments to evaluate our
proposed approach on the sparse and dense face alignment tasks. The
experimental results indicate that our proposed approach is very promising on
face alignment tasks without fully-supervision and pre-processing like
detection and crop. Our implementation is publicly available at
\url{https://github.com/kalyo-zjl/WM3DR}.",arxiv
http://arxiv.org/abs/1804.09914v1,2018-04-26T07:02:07Z,2018-04-26T07:02:07Z,"iTeleScope: Intelligent Video Telemetry and Classification in Real-Time
  using Software Defined Networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost.",arxiv
http://arxiv.org/abs/1906.01113v2,2019-09-23T21:25:56Z,2019-06-03T22:45:44Z,Learning in situ: a randomized experiment in video streaming,"We describe the results of a randomized controlled trial of video-streaming
algorithms for bitrate selection and network prediction. Over the last eight
months, we have streamed 14.2 years of video to 56,000 users across the
Internet. Sessions are randomized in blinded fashion among algorithms, and
client telemetry is recorded for analysis.
  We found that in this real-world setting, it is difficult for sophisticated
or machine-learned control schemes to outperform a ""simple"" scheme
(buffer-based control), notwithstanding good performance in network emulators
or simulators. We performed a statistical analysis and found that the
variability and heavy-tailed nature of network and algorithm behavior create
hurdles for robust learned algorithms in this area.
  We developed an ABR algorithm that robustly outperforms other schemes in
practice, by combining classical control with a learned network predictor,
trained with supervised learning in situ on data from the real deployment
environment.
  To support further investigation, we are publishing an archive of traces and
results each day, and will open our ongoing study to the community. We welcome
other researchers to use this platform to develop and validate new algorithms
for bitrate selection, network prediction, and congestion control.",arxiv
http://arxiv.org/abs/2103.11052v1,2021-03-19T22:48:03Z,2021-03-19T22:48:03Z,"A first step towards automated species recognition from camera trap
  images of mammals using AI in a European temperate forest","Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices.",arxiv
http://arxiv.org/abs/2107.06397v1,2021-07-13T21:12:34Z,2021-07-13T21:12:34Z,"SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based
  Augmented Reality for Surgical Guidance","We present SurgeonAssist-Net: a lightweight framework making
action-and-workflow-driven virtual assistance, for a set of predefined surgical
tasks, accessible to commercially available optical see-through head-mounted
displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic
surgical workflow, our implementation competes with state-of-the-art approaches
in prediction accuracy for automated task recognition, and yet requires 7.4x
fewer parameters, 10.2x fewer floating point operations per second (FLOPS), is
7.0x faster for inference on a CPU, and is capable of near real-time
performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use
of an efficient convolutional neural network (CNN) backbone to extract
discriminative features from image data, and a low-parameter recurrent neural
network (RNN) architecture to learn long-term temporal dependencies. To
demonstrate the feasibility of our approach for inference on the HoloLens 2 we
created a sample dataset that included video of several surgical tasks recorded
from a user-centric point-of-view. After training, we deployed our model and
cataloged its performance in an online simulated surgical scenario for the
prediction of the current surgical task. The utility of our approach is
explored in the discussion of several relevant clinical use-cases. Our code is
publicly available at https://github.com/doughtmw/surgeon-assist-net.",arxiv
http://arxiv.org/abs/2111.01777v1,2021-11-02T17:53:54Z,2021-11-02T17:53:54Z,"A Framework for Real-World Multi-Robot Systems Running Decentralized
  GNN-Based Policies","Graph Neural Networks (GNNs) are a paradigm-shifting neural architecture to
facilitate the learning of complex multi-agent behaviors. Recent work has
demonstrated remarkable performance in tasks such as flocking, multi-agent path
planning and cooperative coverage. However, the policies derived through
GNN-based learning schemes have not yet been deployed to the real-world on
physical multi-robot systems. In this work, we present the design of a system
that allows for fully decentralized execution of GNN-based policies. We create
a framework based on ROS2 and elaborate its details in this paper. We
demonstrate our framework on a case-study that requires tight coordination
between robots, and present first-of-a-kind results that show successful
real-world deployment of GNN-based policies on a decentralized multi-robot
system relying on Adhoc communication. A video demonstration of this case-study
can be found online. https://www.youtube.com/watch?v=COh-WLn4iO4",arxiv
http://arxiv.org/abs/1904.00923v1,2019-04-01T15:51:12Z,2019-04-01T15:51:12Z,Robustness of 3D Deep Learning in an Adversarial Setting,"Understanding the spatial arrangement and nature of real-world objects is of
paramount importance to many complex engineering tasks, including autonomous
navigation. Deep learning has revolutionized state-of-the-art performance for
tasks in 3D environments; however, relatively little is known about the
robustness of these approaches in an adversarial setting. The lack of
comprehensive analysis makes it difficult to justify deployment of 3D deep
learning models in real-world, safety-critical applications. In this work, we
develop an algorithm for analysis of pointwise robustness of neural networks
that operate on 3D data. We show that current approaches presented for
understanding the resilience of state-of-the-art models vastly overestimate
their robustness. We then use our algorithm to evaluate an array of
state-of-the-art models in order to demonstrate their vulnerability to
occlusion attacks. We show that, in the worst case, these networks can be
reduced to 0% classification accuracy after the occlusion of at most 6.5% of
the occupied input space.",arxiv
http://arxiv.org/abs/1807.07769v2,2018-10-05T18:07:23Z,2018-07-20T10:14:27Z,Physical Adversarial Examples for Object Detectors,"Deep neural networks (DNNs) are vulnerable to adversarial
examples-maliciously crafted inputs that cause DNNs to make incorrect
predictions. Recent work has shown that these attacks generalize to the
physical domain, to create perturbations on physical objects that fool image
classifiers under a variety of real-world conditions. Such attacks pose a risk
to deep learning models used in safety-critical cyber-physical systems. In this
work, we extend physical attacks to more challenging object detection models, a
broader class of deep learning algorithms widely used to detect and label
multiple objects within a scene. Improving upon a previous physical attack on
image classifiers, we create perturbed physical objects that are either ignored
or mislabeled by object detection models. We implement a Disappearance Attack,
in which we cause a Stop sign to ""disappear"" according to the detector-either
by covering thesign with an adversarial Stop sign poster, or by adding
adversarial stickers onto the sign. In a video recorded in a controlled lab
environment, the state-of-the-art YOLOv2 detector failed to recognize these
adversarial Stop signs in over 85% of the video frames. In an outdoor
experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and
63.5% of the video frames respectively. We also use Faster R-CNN, a different
object detection model, to demonstrate the transferability of our adversarial
perturbations. The created poster perturbation is able to fool Faster R-CNN in
85.9% of the video frames in a controlled lab environment, and 40.2% of the
video frames in an outdoor environment. Finally, we present preliminary results
with a new Creation Attack, where in innocuous physical stickers fool a model
into detecting nonexistent objects.",arxiv
http://arxiv.org/abs/1912.10609v1,2019-12-23T03:50:52Z,2019-12-23T03:50:52Z,One-Shot Imitation Filming of Human Motion Videos,"Imitation learning has been applied to mimic the operation of a human
cameraman in several autonomous cinematography systems. To imitate different
filming styles, existing methods train multiple models, where each model
handles a particular style and requires a significant number of training
samples. As a result, existing methods can hardly generalize to unseen styles.
In this paper, we propose a framework, which can imitate a filming style by
""seeing"" only a single demonstration video of the same style, i.e., one-shot
imitation filming. This is done by two key enabling techniques: 1) feature
extraction of the filming style from the demo video, and 2) filming style
transfer from the demo video to the new situation. We implement the approach
with deep neural network and deploy it to a 6 degrees of freedom (DOF) real
drone cinematography system by first predicting the future camera motions, and
then converting them to the drone's control commands via an odometer. Our
experimental results on extensive datasets and showcases exhibit significant
improvements in our approach over conventional baselines and our approach can
successfully mimic the footage with an unseen style.",arxiv
http://arxiv.org/abs/2104.10715v1,2021-04-21T18:28:13Z,2021-04-21T18:28:13Z,Uncertainty-Aware Boosted Ensembling in Multi-Modal Settings,"Reliability of machine learning (ML) systems is crucial in safety-critical
applications such as healthcare, and uncertainty estimation is a widely
researched method to highlight the confidence of ML systems in deployment.
Sequential and parallel ensemble techniques have shown improved performance of
ML systems in multi-modal settings by leveraging the feature sets together. We
propose an uncertainty-aware boosting technique for multi-modal ensembling in
order to focus on the data points with higher associated uncertainty estimates,
rather than the ones with higher loss values. We evaluate this method on
healthcare tasks related to Dementia and Parkinson's disease which involve
real-world multi-modal speech and text data, wherein our method shows an
improved performance. Additional analysis suggests that introducing
uncertainty-awareness into the boosted ensembles decreases the overall entropy
of the system, making it more robust to heteroscedasticity in the data, as well
as better calibrating each of the modalities along with high quality prediction
intervals. We open-source our entire codebase at
https://github.com/usarawgi911/Uncertainty-aware-boosting",arxiv
http://arxiv.org/abs/2101.05885v1,2021-01-14T21:59:04Z,2021-01-14T21:59:04Z,"Cocktail Edge Caching: Ride Dynamic Trends of Content Popularity with
  Ensemble Learning","Edge caching will play a critical role in facilitating the emerging
content-rich applications. However, it faces many new challenges, in
particular, the highly dynamic content popularity and the heterogeneous caching
configurations. In this paper, we propose Cocktail Edge Caching, that tackles
the dynamic popularity and heterogeneity through ensemble learning. Instead of
trying to find a single dominating caching policy for all the caching
scenarios, we employ an ensemble of constituent caching policies and adaptively
select the best-performing policy to control the cache. Towards this goal, we
first show through formal analysis and experiments that different variations of
the LFU and LRU policies have complementary performance in different caching
scenarios. We further develop a novel caching algorithm that enhances LFU/LRU
with deep recurrent neural network (LSTM) based time-series analysis. Finally,
we develop a deep reinforcement learning agent that adaptively combines base
caching policies according to their virtual hit ratios on parallel virtual
caches. Through extensive experiments driven by real content requests from two
large video streaming platforms, we demonstrate that CEC not only consistently
outperforms all single policies, but also improves the robustness of them. CEC
can be well generalized to different caching scenarios with low computation
overheads for deployment.",arxiv
http://arxiv.org/abs/1803.02665v4,2018-09-25T09:00:03Z,2018-03-07T14:16:59Z,"A Neural Network Approach to Missing Marker Reconstruction in Human
  Motion Capture","Optical motion capture systems have become a widely used technology in
various fields, such as augmented reality, robotics, movie production, etc.
Such systems use a large number of cameras to triangulate the position of
optical markers.The marker positions are estimated with high accuracy. However,
especially when tracking articulated bodies, a fraction of the markers in each
timestep is missing from the reconstruction. In this paper, we propose to use a
neural network approach to learn how human motion is temporally and spatially
correlated, and reconstruct missing markers positions through this model. We
experiment with two different models, one LSTM-based and one time-window-based.
Both methods produce state-of-the-art results, while working online, as opposed
to most of the alternative methods, which require the complete sequence to be
known. The implementation is publicly available at
https://github.com/Svito-zar/NN-for-Missing-Marker-Reconstruction .",arxiv
http://arxiv.org/abs/2105.03668v2,2021-09-05T16:41:50Z,2021-05-08T10:42:28Z,"Real-time prediction of probabilistic crack growth with a reduced-order
  digital twin of a helicopter component","To deploy the airframe Digital Twin or to conduct probabilistic evaluations
of the remaining life of a structural component, a (near) real-time crack
growth simulation method is critical. In this paper, a reduced-order simulation
approach is developed to achieve this goal by leveraging two methods. On one
hand, the SGBEM super element - FEM coupling method is combined with parametric
modeling to generate the database of computed Stress Intensity Factors for
cracks with various sizes/shapes in a complex structural component, by which
hundreds of samples are automatically simulated within a day. On the other
hand, machine learning methods are applied to establish the relation between
crack sizes/shapes and crack front SIFs. By combining the reduced-order
computational model with load inputs and fatigue growth laws, a real time
prediction of probabilistic crack growth in complex structures with minimum
computational burden is realized. In an example of a round-robin helicopter
component, even though the fatigue crack growth is simulated cycle by cycle,
the simulation is faster than real-time (as compared to the physical test). The
proposed approach is a key simulation technology towards realizing the Digital
Twin of complex structures, which further requires fusion of model predictions
with flight/inspection/monitoring data.",arxiv
http://arxiv.org/abs/2008.13369v1,2020-08-31T05:12:57Z,2020-08-31T05:12:57Z,"Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection","Automated deception detection systems can enhance health, justice, and
security in society by helping humans detect deceivers in high-stakes
situations across medical and legal domains, among others. This paper presents
a novel analysis of the discriminative power of dimensional representations of
facial affect for automated deception detection, along with interpretable
features from visual, vocal, and verbal modalities. We used a video dataset of
people communicating truthfully or deceptively in real-world, high-stakes
courtroom situations. We leveraged recent advances in automated emotion
recognition in-the-wild by implementing a state-of-the-art deep neural network
trained on the Aff-Wild database to extract continuous representations of
facial valence and facial arousal from speakers. We experimented with unimodal
Support Vector Machines (SVM) and SVM-based multimodal fusion methods to
identify effective features, modalities, and modeling approaches for detecting
deception. Unimodal models trained on facial affect achieved an AUC of 80%, and
facial affect contributed towards the highest-performing multimodal approach
(adaptive boosting) that achieved an AUC of 91% when tested on speakers who
were not part of training sets. This approach achieved a higher AUC than
existing automated machine learning approaches that used interpretable visual,
vocal, and verbal features to detect deception in this dataset, but did not use
facial affect. Across all videos, deceptive and truthful speakers exhibited
significant differences in facial valence and facial arousal, contributing
computational support to existing psychological theories on affect and
deception. The demonstrated importance of facial affect in our models informs
and motivates the future development of automated, affect-aware machine
learning approaches for modeling and detecting deception and other social
behaviors in-the-wild.",arxiv
http://arxiv.org/abs/1711.00541v2,2018-04-18T02:25:29Z,2017-11-01T21:19:22Z,"TasNet: time-domain audio separation network for real-time,
  single-channel speech separation","Robust speech processing in multi-talker environments requires effective
speech separation. Recent deep learning systems have made significant progress
toward solving this problem, yet it remains challenging particularly in
real-time, short latency applications. Most methods attempt to construct a mask
for each source in time-frequency representation of the mixture signal which is
not necessarily an optimal representation for speech separation. In addition,
time-frequency decomposition results in inherent problems such as
phase/magnitude decoupling and long time window which is required to achieve
sufficient frequency resolution. We propose Time-domain Audio Separation
Network (TasNet) to overcome these limitations. We directly model the signal in
the time-domain using an encoder-decoder framework and perform the source
separation on nonnegative encoder outputs. This method removes the frequency
decomposition step and reduces the separation problem to estimation of source
masks on encoder outputs which is then synthesized by the decoder. Our system
outperforms the current state-of-the-art causal and noncausal speech separation
algorithms, reduces the computational cost of speech separation, and
significantly reduces the minimum required latency of the output. This makes
TasNet suitable for applications where low-power, real-time implementation is
desirable such as in hearable and telecommunication devices.",arxiv
http://arxiv.org/abs/0906.5325v1,2009-06-29T17:48:40Z,2009-06-29T17:48:40Z,Online Reinforcement Learning for Dynamic Multimedia Systems,"In our previous work, we proposed a systematic cross-layer framework for
dynamic multimedia systems, which allows each layer to make autonomous and
foresighted decisions that maximize the system's long-term performance, while
meeting the application's real-time delay constraints. The proposed solution
solved the cross-layer optimization offline, under the assumption that the
multimedia system's probabilistic dynamics were known a priori. In practice,
however, these dynamics are unknown a priori and therefore must be learned
online. In this paper, we address this problem by allowing the multimedia
system layers to learn, through repeated interactions with each other, to
autonomously optimize the system's long-term performance at run-time. We
propose two reinforcement learning algorithms for optimizing the system under
different design constraints: the first algorithm solves the cross-layer
optimization in a centralized manner, and the second solves it in a
decentralized manner. We analyze both algorithms in terms of their required
computation, memory, and inter-layer communication overheads. After noting that
the proposed reinforcement learning algorithms learn too slowly, we introduce a
complementary accelerated learning algorithm that exploits partial knowledge
about the system's dynamics in order to dramatically improve the system's
performance. In our experiments, we demonstrate that decentralized learning can
perform as well as centralized learning, while enabling the layers to act
autonomously. Additionally, we show that existing application-independent
reinforcement learning algorithms, and existing myopic learning algorithms
deployed in multimedia systems, perform significantly worse than our proposed
application-aware and foresighted learning methods.",arxiv
http://arxiv.org/abs/1906.01308v1,2019-06-04T10:03:08Z,2019-06-04T10:03:08Z,"Towards better Validity: Dispersion based Clustering for Unsupervised
  Person Re-identification","Person re-identification aims to establish the correct identity
correspondences of a person moving through a non-overlapping multi-camera
installation. Recent advances based on deep learning models for this task
mainly focus on supervised learning scenarios where accurate annotations are
assumed to be available for each setup. Annotating large scale datasets for
person re-identification is demanding and burdensome, which renders the
deployment of such supervised approaches to real-world applications infeasible.
Therefore, it is necessary to train models without explicit supervision in an
autonomous manner. In this paper, we propose an elegant and practical
clustering approach for unsupervised person re-identification based on the
cluster validity consideration. Concretely, we explore a fundamental concept in
statistics, namely \emph{dispersion}, to achieve a robust clustering criterion.
Dispersion reflects the compactness of a cluster when employed at the
intra-cluster level and reveals the separation when measured at the
inter-cluster level. With this insight, we design a novel Dispersion-based
Clustering (DBC) approach which can discover the underlying patterns in data.
This approach considers a wider context of sample-level pairwise relationships
to achieve a robust cluster affinity assessment which handles the complications
may arise due to prevalent imbalanced data distributions. Additionally, our
solution can automatically prioritize standalone data points and prevents
inferior clustering. Our extensive experimental analysis on image and video
re-identification benchmarks demonstrate that our method outperforms the
state-of-the-art unsupervised methods by a significant margin. Code is
available at https://github.com/gddingcs/Dispersion-based-Clustering.git.",arxiv
http://arxiv.org/abs/2004.14619v1,2020-04-30T07:47:14Z,2020-04-30T07:47:14Z,The 4th AI City Challenge,"The AI City Challenge was created to accelerate intelligent video analysis
that helps make cities smarter and safer. Transportation is one of the largest
segments that can benefit from actionable insights derived from data captured
by sensors, where computer vision and deep learning have shown promise in
achieving large-scale practical deployment. The 4th annual edition of the AI
City Challenge has attracted 315 participating teams across 37 countries, who
leveraged city-scale real traffic data and high-quality synthetic data to
compete in four challenge tracks. Track 1 addressed video-based automatic
vehicle counting, where the evaluation is conducted on both algorithmic
effectiveness and computational efficiency. Track 2 addressed city-scale
vehicle re-identification with augmented synthetic data to substantially
increase the training set for the task. Track 3 addressed city-scale
multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly
detection. The evaluation system shows two leader boards, in which a general
leader board shows all submitted results, and a public leader board shows
results limited to our contest participation rules, that teams are not allowed
to use external data in their work. The public leader board shows results more
close to real-world situations where annotated data are limited. Our results
show promise that AI technology can enable smarter and safer transportation
systems.",arxiv
http://arxiv.org/abs/2003.01668v3,2020-03-11T23:30:56Z,2020-03-03T17:49:49Z,Model Assertions for Monitoring and Improving ML Models,"ML models are increasingly deployed in settings with real world interactions
such as vehicles, but unfortunately, these models can fail in systematic ways.
To prevent errors, ML engineering teams monitor and continuously improve these
models. We propose a new abstraction, model assertions, that adapts the
classical use of program assertions as a way to monitor and improve ML models.
Model assertions are arbitrary functions over a model's input and output that
indicate when errors may be occurring, e.g., a function that triggers if an
object rapidly changes its class in a video. We propose methods of using model
assertions at all stages of ML system deployment, including runtime monitoring,
validating labels, and continuously improving ML models. For runtime
monitoring, we show that model assertions can find high confidence errors,
where a model returns the wrong output with high confidence, which
uncertainty-based monitoring techniques would not detect. For training, we
propose two methods of using model assertions. First, we propose a bandit-based
active learning algorithm that can sample from data flagged by assertions and
show that it can reduce labeling costs by up to 40% over traditional
uncertainty-based methods. Second, we propose an API for generating
""consistency assertions"" (e.g., the class change example) and weak labels for
inputs where the consistency assertions fail, and show that these weak labels
can improve relative model quality by up to 46%. We evaluate model assertions
on four real-world tasks with video, LIDAR, and ECG data.",arxiv
http://arxiv.org/abs/1904.03814v2,2019-11-18T06:16:42Z,2019-04-08T03:21:11Z,Temporal Convolution for Real-time Keyword Spotting on Mobile Devices,"Keyword spotting (KWS) plays a critical role in enabling speech-based user
interactions on smart devices. Recent developments in the field of deep
learning have led to wide adoption of convolutional neural networks (CNNs) in
KWS systems due to their exceptional accuracy and robustness. The main
challenge faced by KWS systems is the trade-off between high accuracy and low
latency. Unfortunately, there has been little quantitative analysis of the
actual latency of KWS models on mobile devices. This is especially concerning
since conventional convolution-based KWS approaches are known to require a
large number of operations to attain an adequate level of performance. In this
paper, we propose a temporal convolution for real-time KWS on mobile devices.
Unlike most of the 2D convolution-based KWS approaches that require a deep
architecture to fully capture both low- and high-frequency domains, we exploit
temporal convolutions with a compact ResNet architecture. In Google Speech
Command Dataset, we achieve more than \textbf{385x} speedup on Google Pixel 1
and surpass the accuracy compared to the state-of-the-art model. In addition,
we release the implementation of the proposed and the baseline models including
an end-to-end pipeline for training models and evaluating them on mobile
devices.",arxiv
http://arxiv.org/abs/1904.01576v2,2019-04-11T16:00:14Z,2019-04-02T01:46:38Z,"BARISTA: Efficient and Scalable Serverless Serving System for Deep
  Learning Prediction Services","Pre-trained deep learning models are increasingly being used to offer a
variety of compute-intensive predictive analytics services such as fitness
tracking, speech and image recognition. The stateless and highly parallelizable
nature of deep learning models makes them well-suited for serverless computing
paradigm. However, making effective resource management decisions for these
services is a hard problem due to the dynamic workloads and diverse set of
available resource configurations that have their deployment and management
costs. To address these challenges, we present a distributed and scalable
deep-learning prediction serving system called Barista and make the following
contributions. First, we present a fast and effective methodology for
forecasting workloads by identifying various trends. Second, we formulate an
optimization problem to minimize the total cost incurred while ensuring bounded
prediction latency with reasonable accuracy. Third, we propose an efficient
heuristic to identify suitable compute resource configurations. Fourth, we
propose an intelligent agent to allocate and manage the compute resources by
horizontal and vertical scaling to maintain the required prediction latency.
Finally, using representative real-world workloads for urban transportation
service, we demonstrate and validate the capabilities of Barista.",arxiv
http://arxiv.org/abs/2107.05297v1,2021-07-12T10:14:45Z,2021-07-12T10:14:45Z,MMSys'21 Grand Challenge on Detecting Cheapfakes,"Cheapfake is a recently coined term that encompasses non-AI (""cheap"")
manipulations of multimedia content. Cheapfakes are known to be more prevalent
than deepfakes. Cheapfake media can be created using editing software for
image/video manipulations, or even without using any software, by simply
altering the context of an image/video by sharing the media alongside
misleading claims. This alteration of context is referred to as out-of-context
(OOC) misuse} of media. OOC media is much harder to detect than fake media,
since the images and videos are not tampered. In this challenge, we focus on
detecting OOC images, and more specifically the misuse of real photographs with
conflicting image captions in news items. The aim of this challenge is to
develop and benchmark models that can be used to detect whether given samples
(news image and associated captions) are OOC, based on the recently compiled
COSMOS dataset.",arxiv
http://arxiv.org/abs/2010.11700v1,2020-10-20T17:05:11Z,2020-10-20T17:05:11Z,"On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR
  Application","Augmented and virtual reality is being deployed in different fields of
applications. Such applications might involve accessing or processing critical
and sensitive information, which requires strict and continuous access control.
Given that Head-Mounted Displays (HMD) developed for such applications commonly
contains internal cameras for gaze tracking purposes, we evaluate the
suitability of such setup for verifying the users through iris recognition. In
this work, we first evaluate a set of iris recognition algorithms suitable for
HMD devices by investigating three well-established handcrafted feature
extraction approaches, and to complement it, we also present the analysis using
four deep learning models. While taking into consideration the minimalistic
hardware requirements of stand-alone HMD, we employ and adapt a recently
developed miniature segmentation model (EyeMMS) for segmenting the iris.
Further, to account for non-ideal and non-collaborative capture of iris, we
define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to
quantify the iris recognition performance. Motivated by the performance of iris
recognition, we also propose the continuous authentication of users in a
non-collaborative capture setting in HMD. Through the experiments on a publicly
available OpenEDS dataset, we show that performance with EER = 5% can be
achieved using deep learning methods in a general setting, along with high
accuracy for continuous user authentication.",arxiv
http://arxiv.org/abs/2010.13114v1,2020-10-25T13:26:48Z,2020-10-25T13:26:48Z,"Empowering Knowledge Distillation via Open Set Recognition for Robust 3D
  Point Cloud Classification","Real-world scenarios pose several challenges to deep learning based computer
vision techniques despite their tremendous success in research. Deeper models
provide better performance, but are challenging to deploy and knowledge
distillation allows us to train smaller models with minimal loss in
performance. The model also has to deal with open set samples from classes
outside the ones it was trained on and should be able to identify them as
unknown samples while classifying the known ones correctly. Finally, most
existing image recognition research focuses only on using two-dimensional
snapshots of the real world three-dimensional objects. In this work, we aim to
bridge these three research fields, which have been developed independently
until now, despite being deeply interrelated. We propose a joint Knowledge
Distillation and Open Set recognition training methodology for
three-dimensional object recognition. We demonstrate the effectiveness of the
proposed method via various experiments on how it allows us to obtain a much
smaller model, which takes a minimal hit in performance while being capable of
open set recognition for 3D point cloud data.",arxiv
http://arxiv.org/abs/2101.06702v3,2021-07-31T19:44:16Z,2021-01-17T16:19:47Z,"Deep Learning based Virtual Point Tracking for Real-Time Target-less
  Dynamic Displacement Measurement in Railway Applications","In the application of computer-vision based displacement measurement, an
optical target is usually required to prove the reference. In the case that the
optical target cannot be attached to the measuring objective, edge detection,
feature matching and template matching are the most common approaches in
target-less photogrammetry. However, their performance significantly relies on
parameter settings. This becomes problematic in dynamic scenes where
complicated background texture exists and varies over time. To tackle this
issue, we propose virtual point tracking for real-time target-less dynamic
displacement measurement, incorporating deep learning techniques and domain
knowledge. Our approach consists of three steps: 1) automatic calibration for
detection of region of interest; 2) virtual point detection for each video
frame using deep convolutional neural network; 3) domain-knowledge based rule
engine for point tracking in adjacent frames. The proposed approach can be
executed on an edge computer in a real-time manner (i.e. over 30 frames per
second). We demonstrate our approach for a railway application, where the
lateral displacement of the wheel on the rail is measured during operation. We
also implement an algorithm using template matching and line detection as the
baseline for comparison. The numerical experiments have been performed to
evaluate the performance and the latency of our approach in the harsh railway
environment with noisy and varying backgrounds.",arxiv
http://arxiv.org/abs/2108.05713v1,2021-08-08T11:29:16Z,2021-08-08T11:29:16Z,Towards real-world navigation with deep differentiable planners,"We train embodied neural networks to plan and navigate unseen complex 3D
environments, emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment, the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning, we focus on differentiable planners such as Value
Iteration Networks (VIN), which are trained offline from safe expert
demonstrations. Although they work well in small simulations, we address two
major limitations that hinder their deployment. First, we observed that current
differentiable planners struggle to plan long-term in environments with a high
branching complexity. While they should ideally learn to assign low rewards to
obstacles to avoid collisions, we posit that the constraints imposed on the
network are not strong enough to guarantee the network to learn sufficiently
large penalties for every possible collision. We thus impose a structural
constraint on the value iteration, which explicitly learns to model any
impossible actions. Secondly, we extend the model to work with a limited
perspective camera under translation and rotation, which is crucial for real
robot deployment. Many VIN-like planners assume a 360 degrees or overhead view
without rotation. In contrast, our method uses a memory-efficient lattice map
to aggregate CNN embeddings of partial observations, and models the rotational
dynamics explicitly using a 3D state-space grid (translation and rotation). Our
proposals significantly improve semantic navigation and exploration on several
2D and 3D environments, succeeding in settings that are otherwise challenging
for this class of methods. As far as we know, we are the first to successfully
perform differentiable planning on the difficult Active Vision Dataset,
consisting of real images captured from a robot.",arxiv
http://arxiv.org/abs/2012.04746v2,2021-04-02T22:28:33Z,2020-12-08T21:20:54Z,"Robust Neural Routing Through Space Partitions for Camera Relocalization
  in Dynamic Indoor Environments","Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks: (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environments. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30% on camera pose accuracy, while
running comparably fast for evaluation.",arxiv
http://arxiv.org/abs/2012.07086v1,2020-12-13T15:38:38Z,2020-12-13T15:38:38Z,"EfficientPose: Efficient Human Pose Estimation with Neural Architecture
  Search","Human pose estimation from image and video is a vital task in many multimedia
applications. Previous methods achieve great performance but rarely take
efficiency into consideration, which makes it difficult to implement the
networks on resource-constrained devices. Nowadays real-time multimedia
applications call for more efficient models for better interactions. Moreover,
most deep neural networks for pose estimation directly reuse the networks
designed for image classification as the backbone, which are not yet optimized
for the pose estimation task. In this paper, we propose an efficient framework
targeted at human pose estimation including two parts, the efficient backbone
and the efficient head. By implementing the differentiable neural architecture
search method, we customize the backbone network design for pose estimation and
reduce the computation cost with negligible accuracy degradation. For the
efficient head, we slim the transposed convolutions and propose a spatial
information correction module to promote the performance of the final
prediction. In experiments, we evaluate our networks on the MPII and COCO
datasets. Our smallest model has only 0.65 GFLOPs with 88.1% PCKh@0.5 on MPII
and our large model has only 2 GFLOPs while its accuracy is competitive with
the state-of-the-art large model, i.e., HRNet with 9.5 GFLOPs.",arxiv
http://arxiv.org/abs/2104.02306v1,2021-04-06T06:04:57Z,2021-04-06T06:04:57Z,Binary Neural Network for Speaker Verification,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",arxiv
http://arxiv.org/abs/1912.03884v1,2019-12-09T07:44:32Z,2019-12-09T07:44:32Z,"MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing","Deep learning methods have brought substantial advancements in speech
separation (SS). Nevertheless, it remains challenging to deploy
deep-learning-based models on edge devices. Thus, identifying an effective way
to compress these large models without hurting SS performance has become an
important research topic. Recently, TasNet and Conv-TasNet have been proposed.
They achieved state-of-the-art results on several standardized SS tasks.
Moreover, their low latency natures make them definitely suitable for real-time
on-device applications. In this study, we propose two parameter-sharing schemes
to lower the memory consumption on TasNet and Conv-TasNet. Accordingly, we
derive a novel so-called MiTAS (Mini TasNet). Our experimental results first
confirmed the robustness of our MiTAS on two types of perturbations in mixed
audio. We also designed a series of ablation experiments to analyze the
relation between SS performance and the amount of parameters in the model. The
results show that MiTAS is able to reduce the model size by a factor of four
while maintaining comparable SS performance with improved stability as compared
to TasNet and Conv-TasNet. This suggests that MiTAS is more suitable for
real-time low latency applications.",arxiv
http://arxiv.org/abs/1804.04065v1,2018-04-11T16:01:26Z,2018-04-11T16:01:26Z,Learning to Extract a Video Sequence from a Single Motion-Blurred Image,"We present a method to extract a video sequence from a single motion-blurred
image. Motion-blurred images are the result of an averaging process, where
instant frames are accumulated over time during the exposure of the sensor.
Unfortunately, reversing this process is nontrivial. Firstly, averaging
destroys the temporal ordering of the frames. Secondly, the recovery of a
single frame is a blind deconvolution task, which is highly ill-posed. We
present a deep learning scheme that gradually reconstructs a temporal ordering
by sequentially extracting pairs of frames. Our main contribution is to
introduce loss functions invariant to the temporal order. This lets a neural
network choose during training what frame to output among the possible
combinations. We also address the ill-posedness of deblurring by designing a
network with a large receptive field and implemented via resampling to achieve
a higher computational efficiency. Our proposed method can successfully
retrieve sharp image sequences from a single motion blurred image and can
generalize well on synthetic and real datasets captured with different cameras.",arxiv
http://arxiv.org/abs/2005.00828v1,2020-05-02T13:16:16Z,2020-05-02T13:16:16Z,DroTrack: High-speed Drone-based Object Tracking Under Uncertainty,"We present DroTrack, a high-speed visual single-object tracking framework for
drone-captured video sequences. Most of the existing object tracking methods
are designed to tackle well-known challenges, such as occlusion and cluttered
backgrounds. The complex motion of drones, i.e., multiple degrees of freedom in
three-dimensional space, causes high uncertainty. The uncertainty problem leads
to inaccurate location predictions and fuzziness in scale estimations. DroTrack
solves such issues by discovering the dependency between object representation
and motion geometry. We implement an effective object segmentation based on
Fuzzy C Means (FCM). We incorporate the spatial information into the membership
function to cluster the most discriminative segments. We then enhance the
object segmentation by using a pre-trained Convolution Neural Network (CNN)
model. DroTrack also leverages the geometrical angular motion to estimate a
reliable object scale. We discuss the experimental results and performance
evaluation using two datasets of 51,462 drone-captured frames. The combination
of the FCM segmentation and the angular scaling increased DroTrack precision by
up to $9\%$ and decreased the centre location error by $162$ pixels on average.
DroTrack outperforms all the high-speed trackers and achieves comparable
results in comparison to deep learning trackers. DroTrack offers high frame
rates up to 1000 frame per second (fps) with the best location precision, more
than a set of state-of-the-art real-time trackers.",arxiv
http://arxiv.org/abs/1911.00889v1,2019-11-03T14:02:58Z,2019-11-03T14:02:58Z,"eBrainII: A 3 kW Realtime Custom 3D DRAM integrated ASIC implementation
  of a Biologically Plausible Model of a Human Scale Cortex","The Artificial Neural Networks (ANNs) like CNN/DNN and LSTM are not
biologically plausible and in spite of their initial success, they cannot
attain the cognitive capabilities enabled by the dynamic hierarchical
associative memory systems of biological brains. The biologically plausible
spiking brain models, for e.g. cortex, basal ganglia and amygdala have a
greater potential to achieve biological brain like cognitive capabilities.
Bayesian Confidence Propagation Neural Network (BCPNN) is a biologically
plausible spiking model of cortex. A human scale model of BCPNN in real time
requires 162 TFlops/s, 50 TBs of synaptic weight storage to be accessed with a
bandwidth of 200 TBs. The spiking bandwidth is relatively modest at 250 GBs/s.
A hand optimized implementation of rodent scale BCPNN has been implemented on
Tesla K80 GPUs require 3 kW, we extrapolate from that a human scale network
will require 3 MW. These power numbers rule out such implementations for field
deployment as advanced cognition engines in embedded systems. The key
innovation that this paper reports is that it is feasible and affordable to
implement real time BCPNN as a custom tiled ASIC in 28 nm technology with
custom 3D DRAM - eBrain II - that consumes 3 kWs for human scale and 12 W for
rodent scale cortex model. Such implementations eminently fulfill the demands
for field deployment.",arxiv
http://arxiv.org/abs/1811.07807v2,2018-11-20T09:43:21Z,2018-11-19T17:10:44Z,Deeper Interpretability of Deep Networks,"Deep Convolutional Neural Networks (CNNs) have been one of the most
influential recent developments in computer vision, particularly for
categorization. There is an increasing demand for explainable AI as these
systems are deployed in the real world. However, understanding the information
represented and processed in CNNs remains in most cases challenging. Within
this paper, we explore the use of new information theoretic techniques
developed in the field of neuroscience to enable novel understanding of how a
CNN represents information. We trained a 10-layer ResNet architecture to
identify 2,000 face identities from 26M images generated using a rigorously
controlled 3D face rendering model that produced variations of intrinsic (i.e.
face morphology, gender, age, expression and ethnicity) and extrinsic factors
(i.e. 3D pose, illumination, scale and 2D translation). With our methodology,
we demonstrate that unlike human's network overgeneralizes face identities even
with extreme changes of face shape, but it is more sensitive to changes of
texture. To understand the processing of information underlying these
counterintuitive properties, we visualize the features of shape and texture
that the network processes to identify faces. Then, we shed a light into the
inner workings of the black box and reveal how hidden layers represent these
features and whether the representations are invariant to pose. We hope that
our methodology will provide an additional valuable tool for interpretability
of CNNs.",arxiv
http://arxiv.org/abs/1906.08864v1,2019-06-01T18:49:57Z,2019-06-01T18:49:57Z,"Accurate and Energy-Efficient Classification with Spiking Random Neural
  Network: Corrected and Expanded Version","Artificial Neural Network (ANN) based techniques have dominated
state-of-the-art results in most problems related to computer vision, audio
recognition, and natural language processing in the past few years, resulting
in strong industrial adoption from all leading technology companies worldwide.
One of the major obstacles that have historically delayed large scale adoption
of ANNs is the huge computational and power costs associated with training and
testing (deploying) them. In the mean-time, Neuromorphic Computing platforms
have recently achieved remarkable performance running more bio-realistic
Spiking Neural Networks at high throughput and very low power consumption
making them a natural alternative to ANNs. Here, we propose using the Random
Neural Network (RNN), a spiking neural network with both theoretical and
practical appealing properties, as a general purpose classifier that can match
the classification power of ANNs on a number of tasks while enjoying all the
features of a spiking neural network. This is demonstrated on a number of
real-world classification datasets.",arxiv
http://arxiv.org/abs/1603.07846v1,2016-03-25T08:46:02Z,2016-03-25T08:46:02Z,Deep Learning At Scale and At Ease,"Recently, deep learning techniques have enjoyed success in various multimedia
applications, such as image classification and multi-modal data analysis. Large
deep learning models are developed for learning rich representations of complex
data. There are two challenges to overcome before deep learning can be widely
adopted in multimedia and other applications. One is usability, namely the
implementation of different models and training algorithms must be done by
non-experts without much effort especially when the model is large and complex.
The other is scalability, that is the deep learning system must be able to
provision for a huge demand of computing resources for training large models
with massive datasets. To address these two challenges, in this paper, we
design a distributed deep learning platform called SINGA which has an intuitive
programming model based on the common layer abstraction of deep learning
models. Good scalability is achieved through flexible distributed training
architecture and specific optimization techniques. SINGA runs on GPUs as well
as on CPUs, and we show that it outperforms many other state-of-the-art deep
learning systems. Our experience with developing and training deep learning
models for real-life multimedia applications in SINGA shows that the platform
is both usable and scalable.",arxiv
http://arxiv.org/abs/2008.01160v2,2020-10-23T11:44:08Z,2020-08-03T19:56:04Z,A Spectral Energy Distance for Parallel Speech Synthesis,"Speech synthesis is an important practical generative modeling problem that
has seen great progress over the last few years, with likelihood-based
autoregressive neural models now outperforming traditional concatenative
systems. A downside of such autoregressive models is that they require
executing tens of thousands of sequential operations per second of generated
audio, making them ill-suited for deployment on specialized deep learning
hardware. Here, we propose a new learning method that allows us to train highly
parallel models of speech, without requiring access to an analytical likelihood
function. Our approach is based on a generalized energy distance between the
distributions of the generated and real audio. This spectral energy distance is
a proper scoring rule with respect to the distribution over
magnitude-spectrograms of the generated waveform audio and offers statistical
consistency guarantees. The distance can be calculated from minibatches without
bias, and does not involve adversarial learning, yielding a stable and
consistent method for training implicit generative models. Empirically, we
achieve state-of-the-art generation quality among implicit generative models,
as judged by the recently-proposed cFDSD metric. When combining our method with
adversarial techniques, we also improve upon the recently-proposed GAN-TTS
model in terms of Mean Opinion Score as judged by trained human evaluators.",arxiv
http://arxiv.org/abs/2106.15202v2,2021-11-21T08:43:15Z,2021-06-29T09:39:34Z,"Inconspicuous Adversarial Patches for Fooling Image Recognition Systems
  on Mobile Devices","Deep learning based image recognition systems have been widely deployed on
mobile devices in today's world. In recent studies, however, deep learning
models are shown vulnerable to adversarial examples. One variant of adversarial
examples, called adversarial patch, draws researchers' attention due to its
strong attack abilities. Though adversarial patches achieve high attack success
rates, they are easily being detected because of the visual inconsistency
between the patches and the original images. Besides, it usually requires a
large amount of data for adversarial patch generation in the literature, which
is computationally expensive and time-consuming. To tackle these challenges, we
propose an approach to generate inconspicuous adversarial patches with one
single image. In our approach, we first decide the patch locations basing on
the perceptual sensitivity of victim models, then produce adversarial patches
in a coarse-to-fine way by utilizing multiple-scale generators and
discriminators. The patches are encouraged to be consistent with the background
images with adversarial training while preserving strong attack abilities. Our
approach shows the strong attack abilities in white-box settings and the
excellent transferability in black-box settings through extensive experiments
on various models with different architectures and training methods. Compared
to other adversarial patches, our adversarial patches hold the most negligible
risks to be detected and can evade human observations, which is supported by
the illustrations of saliency maps and results of user evaluations. Lastly, we
show that our adversarial patches can be applied in the physical world.",arxiv
http://arxiv.org/abs/2105.05873v1,2021-05-12T18:00:14Z,2021-05-12T18:00:14Z,Out of the Box: Embodied Navigation in the Real World,"The research field of Embodied AI has witnessed substantial progress in
visual navigation and exploration thanks to powerful simulating platforms and
the availability of 3D data of indoor and photorealistic environments. These
two factors have opened the doors to a new generation of intelligent agents
capable of achieving nearly perfect PointGoal Navigation. However, such
architectures are commonly trained with millions, if not billions, of frames
and tested in simulation. Together with great enthusiasm, these results yield a
question: how many researchers will effectively benefit from these advances? In
this work, we detail how to transfer the knowledge acquired in simulation into
the real world. To that end, we describe the architectural discrepancies that
damage the Sim2Real adaptation ability of models trained on the Habitat
simulator and propose a novel solution tailored towards the deployment in
real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot
equipped with a single Intel RealSense camera. Different from previous work,
our testing scene is unavailable to the agent in simulation. The environment is
also inaccessible to the agent beforehand, so it cannot count on scene-specific
semantic priors. In this way, we reproduce a setting in which a research group
(potentially from other fields) needs to employ the agent visual navigation
capabilities as-a-Service. Our experiments indicate that it is possible to
achieve satisfying results when deploying the obtained model in the real world.
Our code and models are available at https://github.com/aimagelab/LoCoNav.",arxiv
http://arxiv.org/abs/2109.05821v1,2021-09-13T09:46:50Z,2021-09-13T09:46:50Z,Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.",arxiv
http://arxiv.org/abs/2008.12858v1,2020-08-28T21:44:24Z,2020-08-28T21:44:24Z,Real-world Video Adaptation with Reinforcement Learning,"Client-side video players employ adaptive bitrate (ABR) algorithms to
optimize user quality of experience (QoE). We evaluate recently proposed
RL-based ABR methods in Facebook's web-based video streaming platform.
Real-world ABR contains several challenges that requires customized designs
beyond off-the-shelf RL algorithms -- we implement a scalable neural network
architecture that supports videos with arbitrary bitrate encodings; we design a
training method to cope with the variance resulting from the stochasticity in
network conditions; and we leverage constrained Bayesian optimization for
reward shaping in order to optimize the conflicting QoE objectives. In a
week-long worldwide deployment with more than 30 million video streaming
sessions, our RL approach outperforms the existing human-engineered ABR
algorithms.",arxiv
http://arxiv.org/abs/1909.02511v2,2019-09-27T21:48:31Z,2019-09-05T16:31:40Z,"CT Data Curation for Liver Patients: Phase Recognition in Dynamic
  Contrast-Enhanced CT","As the demand for more descriptive machine learning models grows within
medical imaging, bottlenecks due to data paucity will exacerbate. Thus,
collecting enough large-scale data will require automated tools to harvest
data/label pairs from messy and real-world datasets, such as hospital PACS.
This is the focus of our work, where we present a principled data curation tool
to extract multi-phase CT liver studies and identify each scan's phase from a
real-world and heterogenous hospital PACS dataset. Emulating a typical
deployment scenario, we first obtain a set of noisy labels from our
institutional partners that are text mined using simple rules from DICOM tags.
We train a deep learning system, using a customized and streamlined 3D SE
architecture, to identify non-contrast, arterial, venous, and delay phase
dynamic CT liver scans, filtering out anything else, including other types of
liver contrast studies. To exploit as much training data as possible, we also
introduce an aggregated cross entropy loss that can learn from scans only
identified as ""contrast"". Extensive experiments on a dataset of 43K scans of
7680 patient imaging studies demonstrate that our 3DSE architecture, armed with
our aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest
up to 92.7% of studies, which significantly outperforms the text-mined and
standard-loss approach, and also outperforms other, and more complex, model
architectures.",arxiv
http://arxiv.org/abs/2107.01002v2,2021-09-27T13:42:44Z,2021-05-31T12:09:46Z,"WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise
  Labels","We introduce WiCluster, a new machine learning (ML) approach for passive
indoor positioning using radio frequency (RF) channel state information (CSI).
WiCluster can predict both a zone-level position and a precise 2D or 3D
position, without using any precise position labels during training. Prior
CSI-based indoor positioning work has relied on non-parametric approaches using
digital signal-processing (DSP) and, more recently, parametric approaches
(e.g., fully supervised ML methods). However these do not handle the complexity
of real-world environments well and do not meet requirements for large-scale
commercial deployments: the accuracy of DSP-based method deteriorates
significantly in non-line-of-sight conditions, while supervised ML methods need
large amounts of hard-to-acquire centimeter accuracy position labels. In
contrast, WiCluster is precise, requires weaker label-information that can be
easily collected, and works well in non-line-of-sight conditions. Our first
contribution is a novel dimensionality reduction method for charting. It
combines a triplet-loss with a multi-scale clustering-loss to map the
high-dimensional CSI representation to a 2D/3D latent space. Our second
contribution is two weakly supervised losses that map this latent space into a
Cartesian map, resulting in meter-accuracy position results. These losses only
require simple to acquire priors: a sketch of the floorplan, approximate
access-point locations and a few CSI packets that are labelled with the
corresponding zone in the floorplan. Thirdly, we report results and a
robustness study for 2D positioning in two single-floor office buildings and 3D
positioning in a two-story home.",arxiv
http://arxiv.org/abs/1910.07360v1,2019-10-16T14:11:24Z,2019-10-16T14:11:24Z,"Conservation AI: Live Stream Analysis for the Detection of Endangered
  Species Using Convolutional Neural Networks and Drone Technology","Many different species are adversely affected by poaching. In response to
this escalating crisis, efforts to stop poaching using hidden cameras, drones
and DNA tracking have been implemented with varying degrees of success. Limited
resources, costs and logistical limitations are often the cause of most
unsuccessful poaching interventions. The study presented in this paper outlines
a flexible and interoperable framework for the automatic detection of animals
and poaching activity to facilitate early intervention practices. Using a
robust deep learning pipeline, a convolutional neural network is trained and
implemented to detect rhinos and cars (considered an important tool in poaching
for fast access and artefact transportation in natural habitats) in the study,
that are found within live video streamed from drones Transfer learning with
the Faster RCNN Resnet 101 is performed to train a custom model with 350 images
of rhinos and 350 images of cars. Inference is performed using a frame sampling
technique to address the required trade-off control precision and processing
speed and maintain synchronisation with the live feed. Inference models are
hosted on a web platform using flask web serving, OpenCV and TensorFlow 1.13.
Video streams are transmitted from a DJI Mavic Pro 2 drone using the Real-Time
Messaging Protocol (RMTP). The best trained Faster RCNN model achieved a mAP of
0.83 @IOU 0.50 and 0.69 @IOU 0.75 respectively. In comparison an
SSD-mobilenetmodel trained under the same experimental conditions achieved a
mAP of 0.55 @IOU .50 and 0.27 @IOU 0.75.The results demonstrate that using a
FRCNN and off-the-shelf drones is a promising and scalable option for a range
of conservation projects.",arxiv
http://arxiv.org/abs/2103.13477v2,2021-03-26T02:49:43Z,2021-03-24T20:52:23Z,A Survey of Multimedia Technologies and Robust Algorithms,"Multimedia technologies are now more practical and deployable in real life,
and the algorithms are widely used in various researching areas such as deep
learning, signal processing, haptics, computer vision, robotics, and medical
multimedia processing. This survey provides an overview of multimedia
technologies and robust algorithms in multimedia data processing, medical
multimedia processing, human facial expression tracking and pose recognition,
and multimedia in education and training. This survey will also analyze and
propose a future research direction based on the overview of current robust
algorithms and multimedia technologies. We want to thank the research and
previous work done by the Multimedia Research Centre (MRC), the University of
Alberta, which is the inspiration and starting point for future research.",arxiv
http://arxiv.org/abs/1803.06077v2,2018-08-31T09:16:33Z,2018-03-16T05:29:12Z,"Real-time Detection, Tracking, and Classification of Moving and
  Stationary Objects using Multiple Fisheye Images","The ability to detect pedestrians and other moving objects is crucial for an
autonomous vehicle. This must be done in real-time with minimum system
overhead. This paper discusses the implementation of a surround view system to
identify moving as well as static objects that are close to the ego vehicle.
The algorithm works on 4 views captured by fisheye cameras which are merged
into a single frame. The moving object detection and tracking solution uses
minimal system overhead to isolate regions of interest (ROIs) containing moving
objects. These ROIs are then analyzed using a deep neural network (DNN) to
categorize the moving object. With deployment and testing on a real car in
urban environments, we have demonstrated the practical feasibility of the
solution. The video demos of our algorithm have been uploaded to Youtube:
https://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs",arxiv
http://arxiv.org/abs/2104.14236v1,2021-04-29T09:57:47Z,2021-04-29T09:57:47Z,Learning Multi-Attention Context Graph for Group-Based Re-Identification,"Learning to re-identify or retrieve a group of people across non-overlapped
camera systems has important applications in video surveillance. However, most
existing methods focus on (single) person re-identification (re-id), ignoring
the fact that people often walk in groups in real scenarios. In this work, we
take a step further and consider employing context information for identifying
groups of people, i.e., group re-id. We propose a novel unified framework based
on graph neural networks to simultaneously address the group-based re-id tasks,
i.e., group re-id and group-aware person re-id. Specifically, we construct a
context graph with group members as its nodes to exploit dependencies among
different people. A multi-level attention mechanism is developed to formulate
both intra-group and inter-group context, with an additional self-attention
module for robust graph-level representations by attentively aggregating
node-level features. The proposed model can be directly generalized to tackle
group-aware person re-id using node-level representations. Meanwhile, to
facilitate the deployment of deep learning models on these tasks, we build a
new group re-id dataset that contains more than 3.8K images with 1.5K annotated
groups, an order of magnitude larger than existing group re-id datasets.
Extensive experiments on the novel dataset as well as three existing datasets
clearly demonstrate the effectiveness of the proposed framework for both
group-based re-id tasks. The code is available at
https://github.com/daodaofr/group_reid.",arxiv
http://arxiv.org/abs/2001.05982v2,2020-06-04T15:13:47Z,2020-01-16T18:32:19Z,"A Common Operating Picture Framework Leveraging Data Fusion and Deep
  Learning","Organizations are starting to realize of the combined power of data and
data-driven algorithmic models to gain insights, situational awareness, and
advance their mission. A common challenge to gaining insights is connecting
inherently different datasets. These datasets (e.g. geocoded features, video
streams, raw text, social network data, etc.) per separate they provide very
narrow answers; however collectively they can provide new capabilities. In this
work, we present a data fusion framework for accelerating solutions for
Processing, Exploitation, and Dissemination (PED). Our platform is a collection
of services that extract information from several data sources (per separate)
by leveraging deep learning and other means of processing. This information is
fused by a set of analytical engines that perform data correlations, searches,
and other modeling operations to combine information from the disparate data
sources. As a result, events of interest are detected, geolocated, logged, and
presented into a common operating picture. This common operating picture allows
the user to visualize in real time all the data sources, per separate and their
collective cooperation. In addition, forensic activities have been implemented
and made available through the framework. Users can review archived results and
compare them to the most recent snapshot of the operational environment. In our
first iteration we have focused on visual data (FMV, WAMI, CCTV/PTZ-Cameras,
open source video, etc.) and AIS data streams (satellite and terrestrial
sources). As a proof-of-concept, in our experiments we show how FMV detections
can be combined with vessel tracking signals from AIS sources to confirm
identity, tip-and-cue aerial reconnaissance, and monitor vessel activity in an
area.",arxiv
http://arxiv.org/abs/1712.09347v1,2017-12-25T02:08:39Z,2017-12-25T02:08:39Z,"Smart Fog: Fog Computing Framework for Unsupervised Clustering Analytics
  in Wearable Internet of Things","The increasing use of wearables in smart telehealth generates heterogeneous
medical big data. Cloud and fog services process these data for assisting
clinical procedures. IoT based ehealthcare have greatly benefited from
efficient data processing. This paper proposed and evaluated use of low
resource machine learning on Fog devices kept close to the wearables for smart
healthcare. In state of the art telecare systems, the signal processing and
machine learning modules are deployed in the cloud for processing physiological
data. We developed a prototype of Fog-based unsupervised machine learning big
data analysis for discovering patterns in physiological data. We employed Intel
Edison and Raspberry Pi as Fog computer in proposed architecture. We performed
validation studies on real-world pathological speech data from in home
monitoring of patients with Parkinson's disease (PD). Proposed architecture
employed machine learning for analysis of pathological speech data obtained
from smartwatches worn by the patients with PD. Results showed that proposed
architecture is promising for low-resource clinical machine learning. It could
be useful for other applications within wearable IoT for smart telehealth
scenarios by translating machine learning approaches from the cloud backend to
edge computing devices such as Fog.",arxiv
http://arxiv.org/abs/2105.13598v3,2021-08-17T06:31:58Z,2021-05-28T05:54:59Z,End-to-End Deep Fault Tolerant Control,"Ideally, accurate sensor measurements are needed to achieve a good
performance in the closed-loop control of mechatronic systems. As a
consequence, sensor faults will prevent the system from working correctly,
unless a fault-tolerant control (FTC) architecture is adopted. As model-based
FTC algorithms for nonlinear systems are often challenging to design, this
paper focuses on a new method for FTC in the presence of sensor faults, based
on deep learning. The considered approach replaces the phases of fault
detection and isolation and controller design with a single recurrent neural
network, which has the value of past sensor measurements in a given time window
as input, and the current values of the control variables as output. This
end-to-end deep FTC method is applied to a mechatronic system composed of a
spherical inverted pendulum, whose configuration is changed via reaction
wheels, in turn actuated by electric motors. The simulation and experimental
results show that the proposed method can handle abrupt faults occurring in
link position/velocity sensors. The provided supplementary material includes a
video of real-world experiments and the software source code.",arxiv
http://arxiv.org/abs/2102.02638v1,2021-02-02T18:50:06Z,2021-02-02T18:50:06Z,"Autodidactic Neurosurgeon: Collaborative Deep Inference for Mobile Edge
  Intelligence via Online Learning","Recent breakthroughs in deep learning (DL) have led to the emergence of many
intelligent mobile applications and services, but in the meanwhile also pose
unprecedented computing challenges on resource-constrained mobile devices. This
paper builds a collaborative deep inference system between a
resource-constrained mobile device and a powerful edge server, aiming at
joining the power of both on-device processing and computation offloading. The
basic idea of this system is to partition a deep neural network (DNN) into a
front-end part running on the mobile device and a back-end part running on the
edge server, with the key challenge being how to locate the optimal partition
point to minimize the end-to-end inference delay. Unlike existing efforts on
DNN partitioning that rely heavily on a dedicated offline profiling stage to
search for the optimal partition point, our system has a built-in online
learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn
the optimal partition point on-the-fly. Therefore, ANS is able to closely
follow the changes of the system environment by generating new knowledge for
adaptive decision making. The core of ANS is a novel contextual bandit learning
algorithm, called $\mu$LinUCB, which not only has provable theoretical learning
performance guarantee but also is ultra-lightweight for easy real-world
implementation. We implement our system on a video stream object detection
testbed to validate the design of ANS and evaluate its performance. The
experiments show that ANS significantly outperforms state-of-the-art benchmarks
in terms of tracking system changes and reducing the end-to-end inference
delay.",arxiv
http://arxiv.org/abs/2003.10315v1,2020-03-23T15:04:30Z,2020-03-23T15:04:30Z,Adversarial Attacks on Monocular Depth Estimation,"Recent advances of deep learning have brought exceptional performance on many
computer vision tasks such as semantic segmentation and depth estimation.
However, the vulnerability of deep neural networks towards adversarial examples
have caused grave concerns for real-world deployment. In this paper, we present
to the best of our knowledge the first systematic study of adversarial attacks
on monocular depth estimation, an important task of 3D scene understanding in
scenarios such as autonomous driving and robot navigation. In order to
understand the impact of adversarial attacks on depth estimation, we first
define a taxonomy of different attack scenarios for depth estimation, including
non-targeted attacks, targeted attacks and universal attacks. We then adapt
several state-of-the-art attack methods for classification on the field of
depth estimation. Besides, multi-task attacks are introduced to further improve
the attack performance for universal attacks. Experimental results show that it
is possible to generate significant errors on depth estimation. In particular,
we demonstrate that our methods can conduct targeted attacks on given objects
(such as a car), resulting in depth estimation 3-4x away from the ground truth
(e.g., from 20m to 80m).",arxiv
http://arxiv.org/abs/2110.04378v1,2021-10-08T21:00:01Z,2021-10-08T21:00:01Z,Performance optimizations on deep noise suppression models,"We study the role of magnitude structured pruning as an architecture search
to speed up the inference time of a deep noise suppression (DNS) model. While
deep learning approaches have been remarkably successful in enhancing audio
quality, their increased complexity inhibits their deployment in real-time
applications. We achieve up to a 7.25X inference speedup over the baseline,
with a smooth model performance degradation. Ablation studies indicate that our
proposed network re-parameterization (i.e., size per layer) is the major driver
of the speedup, and that magnitude structured pruning does comparably to
directly training a model in the smaller size. We report inference speed
because a parameter reduction does not necessitate speedup, and we measure
model quality using an accurate non-intrusive objective speech quality metric.",arxiv
http://arxiv.org/abs/1706.05781v1,2017-06-19T04:42:14Z,2017-06-19T04:42:14Z,"Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
  Deep Neural Network Models with Keras","We introduce Kapre, Keras layers for audio and music signal preprocessing.
Music research using deep neural networks requires a heavy and tedious
preprocessing stage, for which audio processing parameters are often ignored in
parameter optimisation. To solve this problem, Kapre implements time-frequency
conversions, normalisation, and data augmentation as Keras layers. We report
simple benchmark results, showing real-time on-GPU preprocessing adds a
reasonable amount of computation.",arxiv
http://arxiv.org/abs/2106.02964v1,2021-06-05T21:15:34Z,2021-06-05T21:15:34Z,"A Review of Machine Learning Classification Using Quantum Annealing for
  Real-world Applications","Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.",arxiv
http://arxiv.org/abs/2109.08710v1,2021-09-17T18:31:31Z,2021-09-17T18:31:31Z,On-device neural speech synthesis,"Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and
WaveRNN, have made it possible to construct a fully neural network based TTS
system, by coupling the two components together. Such a system is conceptually
simple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an
intermediate feature, and directly generates speech samples. The system
achieves quality equal or close to natural speech. However, the high
computational cost of the system and issues with robustness have limited their
usage in real-world speech synthesis applications and products. In this paper,
we present key modeling improvements and optimization strategies that enable
deploying these models, not only on GPU servers, but also on mobile devices.
The proposed system can generate high-quality 24 kHz speech at 5x faster than
real time on server and 3x faster than real time on mobile devices.",arxiv
http://arxiv.org/abs/2108.03272v4,2021-11-03T18:51:07Z,2021-08-06T18:41:39Z,"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday
  Household Tasks","Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset are publicly available at
http://svl.stanford.edu/igibson/.",arxiv
http://arxiv.org/abs/2012.07938v1,2020-12-14T20:55:48Z,2020-12-14T20:55:48Z,NVIDIA SimNet^{TM}: an AI-accelerated multi-physics simulation framework,"We present SimNet, an AI-driven multi-physics simulation framework, to
accelerate simulations across a wide range of disciplines in science and
engineering. Compared to traditional numerical solvers, SimNet addresses a wide
range of use cases - coupled forward simulations without any training data,
inverse and data assimilation problems. SimNet offers fast turnaround time by
enabling parameterized system representation that solves for multiple
configurations simultaneously, as opposed to the traditional solvers that solve
for one configuration at a time. SimNet is integrated with parameterized
constructive solid geometry as well as STL modules to generate point clouds.
Furthermore, it is customizable with APIs that enable user extensions to
geometry, physics and network architecture. It has advanced network
architectures that are optimized for high-performance GPU computing, and offers
scalable performance for multi-GPU and multi-Node implementation with
accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this
paper we review the neural network solver methodology, the SimNet architecture,
and the various features that are needed for effective solution of the PDEs. We
present real-world use cases that range from challenging forward multi-physics
simulations with turbulence and complex 3D geometries, to industrial design
optimization and inverse problems that are not addressed efficiently by the
traditional solvers. Extensive comparisons of SimNet results with open source
and commercial solvers show good correlation.",arxiv
http://arxiv.org/abs/2109.09165v1,2021-09-19T16:59:01Z,2021-09-19T16:59:01Z,Traffic-Net: 3D Traffic Monitoring Using a Single Camera,"Computer Vision has played a major role in Intelligent Transportation Systems
(ITS) and traffic surveillance. Along with the rapidly growing automated
vehicles and crowded cities, the automated and advanced traffic management
systems (ATMS) using video surveillance infrastructures have been evolved by
the implementation of Deep Neural Networks. In this research, we provide a
practical platform for real-time traffic monitoring, including 3D
vehicle/pedestrian detection, speed detection, trajectory estimation,
congestion detection, as well as monitoring the interaction of vehicles and
pedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5
deep neural network model for vehicle/pedestrian detection and an enhanced SORT
tracking algorithm. For the first time, a hybrid satellite-ground based inverse
perspective mapping (SG-IPM) method for camera auto-calibration is also
developed which leads to an accurate 3D object detection and visualisation. We
also develop a hierarchical traffic modelling solution based on short- and
long-term temporal video data stream to understand the traffic flow,
bottlenecks, and risky spots for vulnerable road users. Several experiments on
real-world scenarios and comparisons with state-of-the-art are conducted using
various traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM
collected from highways, intersections, and urban areas under different
lighting and weather conditions.",arxiv
http://arxiv.org/abs/1907.11546v3,2020-07-13T08:23:37Z,2019-07-26T12:55:55Z,Compressing deep quaternion neural networks with targeted regularization,"In recent years, hyper-complex deep networks (such as complex-valued and
quaternion-valued neural networks) have received a renewed interest in the
literature. They find applications in multiple fields, ranging from image
reconstruction to 3D audio processing. Similar to their real-valued
counterparts, quaternion neural networks (QVNNs) require custom regularization
strategies to avoid overfitting. In addition, for many real-world applications
and embedded implementations, there is the need of designing sufficiently
compact networks, with few weights and neurons. However, the problem of
regularizing and/or sparsifying QVNNs has not been properly addressed in the
literature as of now. In this paper, we show how to address both problems by
designing targeted regularization strategies, which are able to minimize the
number of connections and neurons of the network during training. To this end,
we investigate two extensions of l1 and structured regularization to the
quaternion domain. In our experimental evaluation, we show that these tailored
strategies significantly outperform classical (real-valued) regularization
approaches, resulting in small networks especially suitable for low-power and
real-time applications.",arxiv
http://arxiv.org/abs/1912.10573v1,2019-12-23T01:11:33Z,2019-12-23T01:11:33Z,"Overcoming the Channel Estimation Barrier in Massive MIMO Communication
  Systems","A new wave of wireless services, including virtual reality, autonomous
driving and internet of things, is driving the design of new generations of
wireless systems to deliver ultra-high data rates, massive number of connected
devices and ultra low latency. Massive multiple-input multiple-output (MIMO) is
one of the critical underlying technologies that allow future wireless networks
to meet these service needs. This article discusses the application of deep
learning (DL) for massive MIMO channel estimation in wireless networks by
integrating the underlying characteristics of channels in future high-speed
cellular deployment. We develop important insights derived from the physical
radio frequency (RF) channel properties and present a comprehensive overview on
the application of DL for accurately estimating channel state information (CSI)
with low overhead. We provide examples of successful DL application in CSI
estimation for massive MIMO wireless systems and highlight several promising
directions for future research.",arxiv
http://arxiv.org/abs/1610.10042v2,2016-11-21T13:50:11Z,2016-10-31T18:08:54Z,ConfocalGN : a minimalistic confocal image simulator,"SUMMARY : We developed a user-friendly software to generate synthetic
confocal microscopy images from a ground truth specified as a 3D bitmap with
pixels of arbitrary size. The software can analyze a real confocal stack to
derivate noise parameters and will use them directly to generate new images
with similar noise characteristics. Such synthetic images can then be used to
assert the quality and robustness of an image analysis pipeline, as well as be
used to train machine-learning image analysis procedures. We illustrate the
approach with closed curves corresponding to the microtubule ring present in
blood platelet. AVAILABILITY AND IMPLEMENTATION: ConfocalGN is written in
Matlab but does not require any toolbox. The source code is distributed under
the GPL 3.0 licence on https://github.com/SergeDmi/ConfocalGN.",arxiv
http://arxiv.org/abs/1808.01356v1,2018-07-31T10:33:09Z,2018-07-31T10:33:09Z,"Deep Learning-Based Multiple Object Visual Tracking on Embedded System
  for IoT and Mobile Edge Computing Applications","Compute and memory demands of state-of-the-art deep learning methods are
still a shortcoming that must be addressed to make them useful at IoT
end-nodes. In particular, recent results depict a hopeful prospect for image
processing using Convolutional Neural Netwoks, CNNs, but the gap between
software and hardware implementations is already considerable for IoT and
mobile edge computing applications due to their high power consumption. This
proposal performs low-power and real time deep learning-based multiple object
visual tracking implemented on an NVIDIA Jetson TX2 development kit. It
includes a camera and wireless connection capability and it is battery powered
for mobile and outdoor applications. A collection of representative sequences
captured with the on-board camera, dETRUSC video dataset, is used to exemplify
the performance of the proposed algorithm and to facilitate benchmarking. The
results in terms of power consumption and frame rate demonstrate the
feasibility of deep learning algorithms on embedded platforms although more
effort to joint algorithm and hardware design of CNNs is needed.",arxiv
http://arxiv.org/abs/2003.00951v2,2021-10-19T13:38:48Z,2020-03-02T14:54:19Z,"DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro
  Hand Gestures and a Real-Time Recognition Framework","The use of hand gestures provides a natural alternative to cumbersome
interface devices for Human-Computer Interaction (HCI) systems. However,
real-time recognition of dynamic micro hand gestures from video streams is
challenging for in-vehicle scenarios since (i) the gestures should be performed
naturally without distracting the driver, (ii) micro hand gestures occur within
very short time intervals at spatially constrained areas, (iii) the performed
gesture should be recognized only once, and (iv) the entire architecture should
be designed lightweight as it will be deployed to an embedded system. In this
work, we propose an HCI system for dynamic recognition of driver micro hand
gestures, which can have a crucial impact in automotive sector especially for
safety related issues. For this purpose, we initially collected a dataset named
Driver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and
infrared modalities. The challenges for dynamic recognition of micro hand
gestures have been addressed by proposing a lightweight convolutional neural
network (CNN) based architecture which operates online efficiently with a
sliding window approach. For the CNN model, several 3-dimensional resource
efficient networks are applied and their performances are analyzed. Online
recognition of gestures has been performed with 3D-MobileNetV2, which provided
the best offline accuracy among the applied networks with similar computational
complexities. The final architecture is deployed on a driver simulator
operating in real-time. We make DriverMHG dataset and our source code publicly
available.",arxiv
http://arxiv.org/abs/1810.11846v2,2019-02-19T05:08:46Z,2018-10-28T17:59:57Z,LPCNet: Improving Neural Speech Synthesis Through Linear Prediction,"Neural speech synthesis models have recently demonstrated the ability to
synthesize high quality speech for text-to-speech and compression applications.
These new models often require powerful GPUs to achieve real-time operation, so
being able to reduce their complexity would open the way for many new
applications. We propose LPCNet, a WaveRNN variant that combines linear
prediction with recurrent neural networks to significantly improve the
efficiency of speech synthesis. We demonstrate that LPCNet can achieve
significantly higher quality than WaveRNN for the same network size and that
high quality LPCNet speech synthesis is achievable with a complexity under 3
GFLOPS. This makes it easier to deploy neural synthesis applications on
lower-power devices, such as embedded systems and mobile phones.",arxiv
http://arxiv.org/abs/2108.05962v1,2021-08-12T21:03:44Z,2021-08-12T21:03:44Z,DRQN-based 3D Obstacle Avoidance with a Limited Field of View,"In this paper, we propose a map-based end-to-end DRL approach for
three-dimensional (3D) obstacle avoidance in a partially observed environment,
which is applied to achieve autonomous navigation for an indoor mobile robot
using a depth camera with a narrow field of view. We first train a neural
network with LSTM units in a 3D simulator of mobile robots to approximate the
Q-value function in double DRQN. We also use a curriculum learning strategy to
accelerate and stabilize the training process. Then we deploy the trained model
to a real robot to perform 3D obstacle avoidance in its navigation. We evaluate
the proposed approach both in the simulated environment and on a robot in the
real world. The experimental results show that the approach is efficient and
easy to be deployed, and it performs well for 3D obstacle avoidance with a
narrow observation angle, which outperforms other existing DRL-based models by
15.5% on success rate.",arxiv
http://arxiv.org/abs/1911.00638v1,2019-11-02T03:41:37Z,2019-11-02T03:41:37Z,"Thompson Sampling for Contextual Bandit Problems with Auxiliary Safety
  Constraints","Recent advances in contextual bandit optimization and reinforcement learning
have garnered interest in applying these methods to real-world sequential
decision making problems. Real-world applications frequently have constraints
with respect to a currently deployed policy. Many of the existing
constraint-aware algorithms consider problems with a single objective (the
reward) and a constraint on the reward with respect to a baseline policy.
However, many important applications involve multiple competing objectives and
auxiliary constraints. In this paper, we propose a novel Thompson sampling
algorithm for multi-outcome contextual bandit problems with auxiliary
constraints. We empirically evaluate our algorithm on a synthetic problem.
Lastly, we apply our method to a real world video transcoding problem and
provide a practical way for navigating the trade-off between safety and
performance using Bayesian optimization.",arxiv
http://arxiv.org/abs/2002.06016v2,2020-03-16T15:58:55Z,2020-02-13T11:08:00Z,"DNN-Based Distributed Multichannel Mask Estimation for Speech
  Enhancement in Microphone Arrays","Multichannel processing is widely used for speech enhancement but several
limitations appear when trying to deploy these solutions to the real-world.
Distributed sensor arrays that consider several devices with a few microphones
is a viable alternative that allows for exploiting the multiple devices
equipped with microphones that we are using in our everyday life. In this
context, we propose to extend the distributed adaptive node-specific signal
estimation approach to a neural networks framework. At each node, a local
filtering is performed to send one signal to the other nodes where a mask is
estimated by a neural network in order to compute a global multi-channel Wiener
filter. In an array of two nodes, we show that this additional signal can be
efficiently taken into account to predict the masks and leads to better speech
enhancement performances than when the mask estimation relies only on the local
signals.",arxiv
http://arxiv.org/abs/1810.02648v3,2019-01-25T15:37:49Z,2018-10-05T12:39:37Z,LiveCap: Real-time Human Performance Capture from Monocular Video,"We present the first real-time human performance capture approach that
reconstructs dense, space-time coherent deforming geometry of entire humans in
general everyday clothing from just a single RGB video. We propose a novel
two-stage analysis-by-synthesis optimization whose formulation and
implementation are designed for high performance. In the first stage, a skinned
template model is jointly fitted to background subtracted input video, 2D and
3D skeleton joint positions found using a deep neural network, and a set of
sparse facial landmark detections. In the second stage, dense non-rigid 3D
deformations of skin and even loose apparel are captured based on a novel
real-time capable algorithm for non-rigid tracking using dense photometric and
silhouette constraints. Our novel energy formulation leverages automatically
identified material regions on the template to model the differing non-rigid
deformation behavior of skin and apparel. The two resulting non-linear
optimization problems per-frame are solved with specially-tailored
data-parallel Gauss-Newton solvers. In order to achieve real-time performance
of over 25Hz, we design a pipelined parallel architecture using the CPU and two
commodity GPUs. Our method is the first real-time monocular approach for
full-body performance capture. Our method yields comparable accuracy with
off-line performance capture techniques, while being orders of magnitude
faster.",arxiv
http://arxiv.org/abs/1910.06407v1,2019-10-14T20:19:15Z,2019-10-14T20:19:15Z,FireNet: Real-time Segmentation of Fire Perimeter from Aerial Video,"In this paper, we share our approach to real-time segmentation of fire
perimeter from aerial full-motion infrared video. We start by describing the
problem from a humanitarian aid and disaster response perspective.
Specifically, we explain the importance of the problem, how it is currently
resolved, and how our machine learning approach improves it. To test our models
we annotate a large-scale dataset of 400,000 frames with guidance from domain
experts. Finally, we share our approach currently deployed in production with
inference speed of 20 frames per second and an accuracy of 92 (F1 Score).",arxiv
http://arxiv.org/abs/1209.6393v1,2012-09-27T23:03:53Z,2012-09-27T23:03:53Z,Learning Robust Low-Rank Representations,"In this paper we present a comprehensive framework for learning robust
low-rank representations by combining and extending recent ideas for learning
fast sparse coding regressors with structured non-convex optimization
techniques. This approach connects robust principal component analysis (RPCA)
with dictionary learning techniques and allows its approximation via trainable
encoders. We propose an efficient feed-forward architecture derived from an
optimization algorithm designed to exactly solve robust low dimensional
projections. This architecture, in combination with different training
objective functions, allows the regressors to be used as online approximants of
the exact offline RPCA problem or as RPCA-based neural networks. Simple
modifications of these encoders can handle challenging extensions, such as the
inclusion of geometric data transformations. We present several examples with
real data from image, audio, and video processing. When used to approximate
RPCA, our basic implementation shows several orders of magnitude speedup
compared to the exact solvers with almost no performance degradation. We show
the strength of the inclusion of learning to the RPCA approach on a music
source separation application, where the encoders outperform the exact RPCA
algorithms, which are already reported to produce state-of-the-art results on a
benchmark database. Our preliminary implementation on an iPad shows
faster-than-real-time performance with minimal latency.",arxiv
http://arxiv.org/abs/1806.11430v3,2018-07-31T10:31:36Z,2018-06-29T14:18:24Z,Towards real-time unsupervised monocular depth estimation on CPU,"Unsupervised depth estimation from a single image is a very attractive
technique with several implications in robotic, autonomous navigation,
augmented reality and so on. This topic represents a very challenging task and
the advent of deep learning enabled to tackle this problem with excellent
results. However, these architectures are extremely deep and complex. Thus,
real-time performance can be achieved only by leveraging power-hungry GPUs that
do not allow to infer depth maps in application fields characterized by
low-power constraints. To tackle this issue, in this paper we propose a novel
architecture capable to quickly infer an accurate depth map on a CPU, even of
an embedded system, using a pyramid of features extracted from a single input
image. Similarly to state-of-the-art, we train our network in an unsupervised
manner casting depth estimation as an image reconstruction problem. Extensive
experimental results on the KITTI dataset show that compared to the top
performing approach our network has similar accuracy but a much lower
complexity (about 6% of parameters) enabling to infer a depth map for a KITTI
image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard
CPU. Moreover, by trading accuracy for efficiency, our network allows to infer
maps at about 2 Hz and 40 Hz respectively, still being more accurate than most
state-of-the-art slower methods. To the best of our knowledge, it is the first
method enabling such performance on CPUs paving the way for effective
deployment of unsupervised monocular depth estimation even on embedded systems.",arxiv
http://arxiv.org/abs/2103.14734v1,2021-03-26T21:03:33Z,2021-03-26T21:03:33Z,"Fully Automated 2D and 3D Convolutional Neural Networks Pipeline for
  Video Segmentation and Myocardial Infarction Detection in Echocardiography","Cardiac imaging known as echocardiography is a non-invasive tool utilized to
produce data including images and videos, which cardiologists use to diagnose
cardiac abnormalities in general and myocardial infarction (MI) in particular.
Echocardiography machines can deliver abundant amounts of data that need to be
quickly analyzed by cardiologists to help them make a diagnosis and treat
cardiac conditions. However, the acquired data quality varies depending on the
acquisition conditions and the patient's responsiveness to the setup
instructions. These constraints are challenging to doctors especially when
patients are facing MI and their lives are at stake. In this paper, we propose
an innovative real-time end-to-end fully automated model based on convolutional
neural networks (CNN) to detect MI depending on regional wall motion
abnormalities (RWMA) of the left ventricle (LV) from videos produced by
echocardiography. Our model is implemented as a pipeline consisting of a 2D CNN
that performs data preprocessing by segmenting the LV chamber from the apical
four-chamber (A4C) view, followed by a 3D CNN that performs a binary
classification to detect if the segmented echocardiography shows signs of MI.
We trained both CNNs on a dataset composed of 165 echocardiography videos each
acquired from a distinct patient. The 2D CNN achieved an accuracy of 97.18% on
data segmentation while the 3D CNN achieved 90.9% of accuracy, 100% of
precision and 95% of recall on MI detection. Our results demonstrate that
creating a fully automated system for MI detection is feasible and propitious.",arxiv
http://arxiv.org/abs/2001.00269v2,2020-04-01T05:40:23Z,2020-01-01T21:21:21Z,"A Smart, Efficient, and Reliable Parking Surveillance System with Edge
  Artificial Intelligence on IoT Devices","Cloud computing has been a main-stream computing service for years. Recently,
with the rapid development in urbanization, massive video surveillance data are
produced at an unprecedented speed. A traditional solution to deal with the big
data would require a large amount of computing and storage resources. With the
advances in Internet of things (IoT), artificial intelligence, and
communication technologies, edge computing offers a new solution to the problem
by processing the data partially or wholly on the edge of a surveillance
system. In this study, we investigate the feasibility of using edge computing
for smart parking surveillance tasks, which is a key component of Smart City.
The system processing pipeline is carefully designed with the consideration of
flexibility, online surveillance, data transmission, detection accuracy, and
system reliability. It enables artificial intelligence at the edge by
implementing an enhanced single shot multibox detector (SSD). A few more
algorithms are developed on both the edge and the server targeting optimal
system efficiency and accuracy. Thorough field tests were conducted in the
Angle Lake parking garage for three months. The experimental results are
promising that the final detection method achieves over 95% accuracy in
real-world scenarios with high efficiency and reliability. The proposed smart
parking surveillance system can be a solid foundation for future applications
of intelligent transportation systems.",arxiv
http://arxiv.org/abs/2004.09215v1,2020-04-20T11:36:02Z,2020-04-20T11:36:02Z,"CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture
  Recognition","Egocentric gestures are the most natural form of communication for humans to
interact with wearable devices such as VR/AR helmets and glasses. A major issue
in such scenarios for real-world applications is that may easily become
necessary to add new gestures to the system e.g., a proper VR system should
allow users to customize gestures incrementally. Traditional deep learning
methods require storing all previous class samples in the system and training
the model again from scratch by incorporating previous samples and new samples,
which costs humongous memory and significantly increases computation over time.
In this work, we demonstrate a lifelong 3D convolutional framework --
c(C)la(a)ss increment(t)al net(Net)work (CatNet), which considers temporal
information in videos and enables lifelong learning for egocentric gesture
video recognition by learning the feature representation of an exemplar set
selected from previous class samples. Importantly, we propose a two-stream
CatNet, which deploys RGB and depth modalities to train two separate networks.
We evaluate CatNets on a publicly available dataset -- EgoGesture dataset, and
show that CatNets can learn many classes incrementally over a long period of
time. Results also demonstrate that the two-stream architecture achieves the
best performance on both joint training and class incremental training compared
to 3 other one-stream architectures. The codes and pre-trained models used in
this work are provided at https://github.com/villawang/CatNet.",arxiv
http://arxiv.org/abs/1802.02961v1,2018-02-08T16:49:00Z,2018-02-08T16:49:00Z,Learning Sparse Wavelet Representations,"In this work we propose a method for learning wavelet filters directly from
data. We accomplish this by framing the discrete wavelet transform as a
modified convolutional neural network. We introduce an autoencoder wavelet
transform network that is trained using gradient descent. We show that the
model is capable of learning structured wavelet filters from synthetic and real
data. The learned wavelets are shown to be similar to traditional wavelets that
are derived using Fourier methods. Our method is simple to implement and easily
incorporated into neural network architectures. A major advantage to our model
is that we can learn from raw audio data.",arxiv
http://arxiv.org/abs/1802.00265v4,2019-01-16T09:04:06Z,2018-02-01T12:42:02Z,VR-Goggles for Robots: Real-to-sim Domain Adaptation for Visual Control,"In this paper, we deal with the reality gap from a novel perspective,
targeting transferring Deep Reinforcement Learning (DRL) policies learned in
simulated environments to the real-world domain for visual control tasks.
Instead of adopting the common solutions to the problem by increasing the
visual fidelity of synthetic images output from simulators during the training
phase, we seek to tackle the problem by translating the real-world image
streams back to the synthetic domain during the deployment phase, to make the
robot feel at home. We propose this as a lightweight, flexible, and efficient
solution for visual control, as 1) no extra transfer steps are required during
the expensive training of DRL agents in simulation; 2) the trained DRL agents
will not be constrained to being deployable in only one specific real-world
environment; 3) the policy training and the transfer operations are decoupled,
and can be conducted in parallel. Besides this, we propose a simple yet
effective shift loss that is agnostic to the downstream task, to constrain the
consistency between subsequent frames which is important for consistent policy
outputs. We validate the shift loss for artistic style transfer for videos and
domain adaptation, and validate our visual control approach in indoor and
outdoor robotics experiments.",arxiv
http://arxiv.org/abs/1811.10399v1,2018-11-26T14:38:25Z,2018-11-26T14:38:25Z,"A Convolutional Neural Network based Live Object Recognition System as
  Blind Aid","This paper introduces a live object recognition system that serves as a blind
aid. Visually impaired people heavily rely on their other senses such as touch
and auditory signals for understanding the environment around them. The act of
knowing what object is in front of the blind person without touching it (by
hand or some other tool) is very difficult. In some cases, the physical contact
between the person and object can be dangerous, and even lethal.
  This project employs a Convolutional Neural Network for recognition of
pre-trained objects on the ImageNet dataset. A camera, aligned with the
system's predetermined orientation serves as input to the computer system,
which has the object recognition Neural Network deployed to carry out real-time
object detection. Output from the network can then be parsed to present to the
visually impaired person either in the form of audio or Braille text.",arxiv
http://arxiv.org/abs/2003.04956v1,2020-03-10T20:26:26Z,2020-03-10T20:26:26Z,"SQUIRL: Robust and Efficient Learning from Video Demonstration of
  Long-Horizon Robotic Manipulation Tasks","Recent advances in deep reinforcement learning (RL) have demonstrated its
potential to learn complex robotic manipulation tasks. However, RL still
requires the robot to collect a large amount of real-world experience. To
address this problem, recent works have proposed learning from expert
demonstrations (LfD), particularly via inverse reinforcement learning (IRL),
given its ability to achieve robust performance with only a small number of
expert demonstrations. Nevertheless, deploying IRL on real robots is still
challenging due to the large number of robot experiences it requires. This
paper aims to address this scalability challenge with a robust,
sample-efficient, and general meta-IRL algorithm, SQUIRL, that performs a new
but related long-horizon task robustly given only a single video demonstration.
First, this algorithm bootstraps the learning of a task encoder and a
task-conditioned policy using behavioral cloning (BC). It then collects
real-robot experiences and bypasses reward learning by directly recovering a
Q-function from the combined robot and expert trajectories. Next, this
algorithm uses the Q-function to re-evaluate all cumulative experiences
collected by the robot to improve the policy quickly. In the end, the policy
performs more robustly (90%+ success) than BC on new tasks while requiring no
trial-and-errors at test time. Finally, our real-robot and simulated
experiments demonstrate our algorithm's generality across different state
spaces, action spaces, and vision-based manipulation tasks, e.g.,
pick-pour-place and pick-carry-drop.",arxiv
http://arxiv.org/abs/1805.01956v1,2018-05-04T22:45:08Z,2018-05-04T22:45:08Z,"Motion Planning Among Dynamic, Decision-Making Agents with Deep
  Reinforcement Learning","Robots that navigate among pedestrians use collision avoidance algorithms to
enable safe and efficient operation. Recent works present deep reinforcement
learning as a framework to model the complex interactions and cooperation.
However, they are implemented using key assumptions about other agents'
behavior that deviate from reality as the number of agents in the environment
increases. This work extends our previous approach to develop an algorithm that
learns collision avoidance among a variety of types of dynamic agents without
assuming they follow any particular behavior rules. This work also introduces a
strategy using LSTM that enables the algorithm to use observations of an
arbitrary number of other agents, instead of previous methods that have a fixed
observation size. The proposed algorithm outperforms our previous approach in
simulation as the number of agents increases, and the algorithm is demonstrated
on a fully autonomous robotic vehicle traveling at human walking speed, without
the use of a 3D Lidar.",arxiv
http://arxiv.org/abs/1812.01029v1,2018-12-03T19:05:25Z,2018-12-03T19:05:25Z,Sensitivity based Neural Networks Explanations,"Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque ""black boxes"". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.",arxiv
http://arxiv.org/abs/1808.06277v1,2018-08-20T00:54:29Z,2018-08-20T00:54:29Z,An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval,"Due to the rapid development of mobile Internet techniques, cloud computation
and popularity of online social networking and location-based services, massive
amount of multimedia data with geographical information is generated and
uploaded to the Internet. In this paper, we propose a novel type of cross-modal
multimedia retrieval called geo-multimedia cross-modal retrieval which aims to
search out a set of geo-multimedia objects based on geographical distance
proximity and semantic similarity between different modalities. Previous
studies for cross-modal retrieval and spatial keyword search cannot address
this problem effectively because they do not consider multimedia data with
geo-tags and do not focus on this type of query. In order to address this
problem efficiently, we present the definition of $k$NN geo-multimedia
cross-modal query at the first time and introduce relevant conceptions such as
cross-modal semantic representation space. To bridge the semantic gap between
different modalities, we propose a method named cross-modal semantic matching
which contains two important component, i.e., CorrProj and LogsTran, which aims
to construct a common semantic representation space for cross-modal semantic
similarity measurement. Besides, we designed a framework based on deep learning
techniques to implement common semantic representation space construction. In
addition, a novel hybrid indexing structure named GMR-Tree combining
geo-multimedia data and R-Tree is presented and a efficient $k$NN search
algorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on
real and synthetic dataset clearly demonstrates that our solution outperforms
the-state-of-the-art methods.",arxiv
http://arxiv.org/abs/1911.10751v1,2019-11-25T08:02:17Z,2019-11-25T08:02:17Z,"Deep Image-to-Video Adaptation and Fusion Networks for Action
  Recognition","Existing deep learning methods for action recognition in videos require a
large number of labeled videos for training, which is labor-intensive and
time-consuming. For the same action, the knowledge learned from different media
types, e.g., videos and images, may be related and complementary. However, due
to the domain shifts and heterogeneous feature representations between videos
and images, the performance of classifiers trained on images may be
dramatically degraded when directly deployed to videos. In this paper, we
propose a novel method, named Deep Image-to-Video Adaptation and Fusion
Networks (DIVAFN), to enhance action recognition in videos by transferring
knowledge from images using video keyframes as a bridge. The DIVAFN is a
unified deep learning model, which integrates domain-invariant representations
learning and cross-modal feature fusion into a unified optimization framework.
Specifically, we design an efficient cross-modal similarities metric to reduce
the modality shift among images, keyframes and videos. Then, we adopt an
autoencoder architecture, whose hidden layer is constrained to be the semantic
representations of the action class names. In this way, when the autoencoder is
adopted to project the learned features from different domains to the same
space, more compact, informative and discriminative representations can be
obtained. Finally, the concatenation of the learned semantic feature
representations from these three autoencoders are used to train the classifier
for action recognition in videos. Comprehensive experiments on four real-world
datasets show that our method outperforms some state-of-the-art domain
adaptation and action recognition methods.",arxiv
http://arxiv.org/abs/2108.04561v1,2021-08-10T10:15:54Z,2021-08-10T10:15:54Z,Evolution of NOMA Toward Next Generation Multiple Access (NGMA),"Due to the explosive growth in the number of wireless devices and diverse
wireless services, such as virtual/augmented reality and
Internet-of-Everything, next generation wireless networks face unprecedented
challenges caused by heterogeneous data traffic, massive connectivity, and
ultra-high bandwidth efficiency and ultra-low latency requirements. To address
these challenges, advanced multiple access schemes are expected to be
developed, namely next generation multiple access (NGMA), which are capable of
supporting massive numbers of users in a more resource- and
complexity-efficient manner than existing multiple access schemes. As the
research on NGMA is in a very early stage, in this paper, we explore the
evolution of NGMA with a particular focus on non-orthogonal multiple access
(NOMA), i.e., the transition from NOMA to NGMA. In particular, we first review
the fundamental capacity limits of NOMA, elaborate the new requirements for
NGMA, and discuss several possible candidate techniques. Moreover, given the
high compatibility and flexibility of NOMA, we provide an overview of current
research efforts on multi-antenna techniques for NOMA, promising future
application scenarios of NOMA, and the interplay between NOMA and other
emerging physical layer techniques. Furthermore, we discuss advanced
mathematical tools for facilitating the design of NOMA communication systems,
including conventional optimization approaches and new machine learning
techniques. Next, we propose a unified framework for NGMA based on multiple
antennas and NOMA, where both downlink and uplink transmission are considered,
thus setting the foundation for this emerging research area. Finally, several
practical implementation challenges for NGMA are highlighted as motivation for
future work.",arxiv
http://arxiv.org/abs/2010.08833v1,2020-10-17T17:48:04Z,2020-10-17T17:48:04Z,"Efficient and Compact Convolutional Neural Network Architectures for
  Non-temporal Real-time Fire Detection","Automatic visual fire detection is used to complement traditional fire
detection sensor systems (smoke/heat). In this work, we investigate different
Convolutional Neural Network (CNN) architectures and their variants for the
non-temporal real-time bounds detection of fire pixel regions in video (or
still) imagery. Two reduced complexity compact CNN architectures
(NasNet-A-OnFire and ShuffleNetV2-OnFire) are proposed through experimental
analysis to optimise the computational efficiency for this task. The results
improve upon the current state-of-the-art solution for fire detection,
achieving an accuracy of 95% for full-frame binary classification and 97% for
superpixel localisation. We notably achieve a classification speed up by a
factor of 2.3x for binary classification and 1.3x for superpixel localisation,
with runtime of 40 fps and 18 fps respectively, outperforming prior work in the
field presenting an efficient, robust and real-time solution for fire region
detection. Subsequent implementation on low-powered devices (Nvidia Xavier-NX,
achieving 49 fps for full-frame classification via ShuffleNetV2-OnFire)
demonstrates our architectures are suitable for various real-world deployment
applications.",arxiv
http://arxiv.org/abs/1912.03647v2,2020-08-11T03:42:21Z,2019-12-08T09:51:08Z,Compressing 3DCNNs Based on Tensor Train Decomposition,"Three dimensional convolutional neural networks (3DCNNs) have been applied in
many tasks, e.g., video and 3D point cloud recognition. However, due to the
higher dimension of convolutional kernels, the space complexity of 3DCNNs is
generally larger than that of traditional two dimensional convolutional neural
networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining
environments such as embedded devices, neural network compression is a
promising approach. In this work, we adopt the tensor train (TT) decomposition,
a straightforward and simple in situ training compression method, to shrink the
3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT
format, we investigate how to select appropriate TT ranks for achieving higher
compression ratio. We have also discussed the redundancy of 3D convolutional
kernels for compression, core significance and future directions of this work,
as well as the theoretical computation complexity versus practical executing
time of convolution in TT. In the light of multiple contrast experiments based
on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT
decomposition can compress 3DCNNs by around one hundred times without
significant accuracy loss, which will enable its applications in extensive real
world scenarios.",arxiv
http://arxiv.org/abs/2011.03790v1,2020-11-07T15:14:03Z,2020-11-07T15:14:03Z,"Rapid Pose Label Generation through Sparse Representation of Unknown
  Objects","Deep Convolutional Neural Networks (CNNs) have been successfully deployed on
robots for 6-DoF object pose estimation through visual perception. However,
obtaining labeled data on a scale required for the supervised training of CNNs
is a difficult task - exacerbated if the object is novel and a 3D model is
unavailable. To this end, this work presents an approach for rapidly generating
real-world, pose-annotated RGB-D data for unknown objects. Our method not only
circumvents the need for a prior 3D object model (textured or otherwise) but
also bypasses complicated setups of fiducial markers, turntables, and sensors.
With the help of a human user, we first source minimalistic labelings of an
ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then,
by solving an optimization problem, we combine these labels under a world frame
to recover a sparse, keypoint-based representation of the object. The sparse
representation leads to the development of a dense model and the pose labels
for each image frame in the set of scenes. We show that the sparse model can
also be efficiently used for scaling to a large number of new scenes. We
demonstrate the practicality of the generated labeled dataset by training a
pipeline for 6-DoF object pose estimation and a pixel-wise segmentation
network.",arxiv
http://arxiv.org/abs/1911.08563v1,2019-11-17T12:50:05Z,2019-11-17T12:50:05Z,"Robust Sub-Meter Level Indoor Localization With a Single WiFi Access
  Point-Regression Versus Classification","Precise indoor localization is an increasingly demanding requirement for
various emerging applications, like Virtual/Augmented reality and personalized
advertising. Current indoor environments are equipped with pluralities of WiFi
access points (APs), whose deployment is expected to be massive in the future
enabling highly precise localization approaches. Though the conventional
model-based localization schemes have achieved sub-meter level accuracy by
fusing multiple channel state information (CSI) observations, the corresponding
computational overhead is usually significant, especially in the current
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. In order to address this issue, model-free localization
techniques using deep learning frameworks have been lately proposed, where
mainly classification methods were applied. In this paper, instead of
classification based mechanism, we propose a logistic regression based scheme
with the deep learning framework, combined with Cram\'er-Rao lower bound (CRLB)
assisted robust training, which achieves more robust sub-meter level accuracy
(0.97m median distance error) in the standard laboratory environment and
maintains reasonable online prediction overhead under the single WiFi AP
settings.",arxiv
http://arxiv.org/abs/1208.6057v1,2012-08-30T00:58:22Z,2012-08-30T00:58:22Z,"Self-paced brain-computer interface control of ambulation in a virtual
  reality environment","Objective: Spinal cord injury (SCI) often leaves affected individuals unable
to ambulate. Electroencephalogramme (EEG) based brain-computer interface (BCI)
controlled lower extremity prostheses may restore intuitive and able-body-like
ambulation after SCI. To test its feasibility, the authors developed and tested
a novel EEG-based, data-driven BCI system for intuitive and self-paced control
of the ambulation of an avatar within a virtual reality environment (VRE).
  Approach: Eight able-bodied subjects and one with SCI underwent the following
10-min training session: subjects alternated between idling and walking
kinaesthetic motor imageries (KMI) while their EEG were recorded and analysed
to generate subject-specific decoding models. Subjects then performed a
goal-oriented online task, repeated over 5 sessions, in which they utilised the
KMI to control the linear ambulation of an avatar and make 10 sequential stops
at designated points within the VRE.
  Main results: The average offline training performance across subjects was
77.2 +/- 9.5%, ranging from 64.3% (p = 0.00176) to 94.5% (p = 6.26*10^-23),
with chance performance being 50%. The average online performance was 8.4 +/-
1.0 (out of 10) successful stops and 303 +/- 53 sec completion time (perfect =
211 sec). All subjects achieved performances significantly different than those
of random walk (p < 0.05) in 44 of the 45 online sessions.
  Significance: By using a data-driven machine learning approach to decode
users' KMI, this BCIVRE system enabled intuitive and purposeful self-paced
control of ambulation after only a 10-minute training. The ability to achieve
such BCI control with minimal training indicates that the implementation of
future BCI-lower extremity prosthesis systems may be feasible.",arxiv
http://arxiv.org/abs/2103.17088v1,2021-03-31T13:58:39Z,2021-03-31T13:58:39Z,"Deep Noise Suppression With Non-Intrusive PESQNet Supervision Enabling
  the Use of Real Training Data","Data-driven speech enhancement employing deep neural networks (DNNs) can
provide state-of-the-art performance even in the presence of non-stationary
noise. During the training process, most of the speech enhancement neural
networks are trained in a fully supervised way with losses requiring noisy
speech to be synthesized by clean speech and additive noise. However, in a real
implementation, only the noisy speech mixture is available, which leads to the
question, how such data could be advantageously employed in training. In this
work, we propose an end-to-end non-intrusive PESQNet DNN which estimates
perceptual evaluation of speech quality (PESQ) scores, allowing a
reference-free loss for real data. As a further novelty, we combine the PESQNet
loss with denoising and dereverberation loss terms, and train a complex
mask-based fully convolutional recurrent neural network (FCRN) in a ""weakly""
supervised way, each training cycle employing some synthetic data, some real
data, and again synthetic data to keep the PESQNet up-to-date. In a subjective
listening test, our proposed framework outperforms the Interspeech 2021 Deep
Noise Suppression (DNS) Challenge baseline overall by 0.09 MOS points and in
particular by 0.45 background noise MOS points.",arxiv
http://arxiv.org/abs/2102.03207v3,2021-06-23T03:08:42Z,2021-02-05T14:46:41Z,Real-time Denoising and Dereverberation with Tiny Recurrent U-Net,"Modern deep learning-based models have seen outstanding performance
improvement with speech enhancement tasks. The number of parameters of
state-of-the-art models, however, is often too large to be deployed on devices
for real-world applications. To this end, we propose Tiny Recurrent U-Net
(TRU-Net), a lightweight online inference model that matches the performance of
current state-of-the-art models. The size of the quantized version of TRU-Net
is 362 kilobytes, which is small enough to be deployed on edge devices. In
addition, we combine the small-sized model with a new masking method called
phase-aware $\beta$-sigmoid mask, which enables simultaneous denoising and
dereverberation. Results of both objective and subjective evaluations have
shown that our model can achieve competitive performance with the current
state-of-the-art models on benchmark datasets using fewer parameters by orders
of magnitude.",arxiv
http://arxiv.org/abs/2008.08198v2,2021-06-02T20:10:12Z,2020-08-18T23:57:07Z,BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning,"X-ray diffraction based microscopy techniques such as High Energy Diffraction
Microscopy rely on knowledge of the position of diffraction peaks with high
precision. These positions are typically computed by fitting the observed
intensities in area detector data to a theoretical peak shape such as
pseudo-Voigt. As experiments become more complex and detector technologies
evolve, the computational cost of such peak detection and shape fitting becomes
the biggest hurdle to the rapid analysis required for real-time feedback during
in-situ experiments. To this end, we propose BraggNN, a deep learning-based
method that can determine peak positions much more rapidly than conventional
pseudo-Voigt peak fitting. When applied to a test dataset, BraggNN gives errors
of less than 0.29 and 0.57 pixels, relative to the conventional method, for 75%
and 95% of the peaks, respectively. When applied to a real experimental
dataset, a 3D reconstruction that used peak positions computed by BraggNN
yields 15% better results on average as compared to a reconstruction obtained
using peak positions determined using conventional 2D pseudo-Voigt fitting.
Recent advances in deep learning method implementations and special-purpose
model inference accelerators allow BraggNN to deliver enormous performance
improvements relative to the conventional method, running, for example, more
than 200 times faster than a conventional method on a consumer-class GPU card
with out-of-the-box software.",arxiv
http://arxiv.org/abs/1801.03002v2,2019-02-20T12:44:10Z,2018-01-08T14:08:55Z,DeepStyle: Multimodal Search Engine for Fashion and Interior Design,"In this paper, we propose a multimodal search engine that combines visual and
textual cues to retrieve items from a multimedia database aesthetically similar
to the query. The goal of our engine is to enable intuitive retrieval of
fashion merchandise such as clothes or furniture. Existing search engines treat
textual input only as an additional source of information about the query image
and do not correspond to the real-life scenario where the user looks for 'the
same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those
shortcomings by using a joint neural network architecture to model contextual
dependencies between features of different modalities. We prove the robustness
of this approach on two different challenging datasets of fashion items and
furniture where our DeepStyle engine outperforms baseline methods by 18-21% on
the tested datasets. Our search engine is commercially deployed and available
through a Web-based application.",arxiv
http://arxiv.org/abs/2111.06953v1,2021-11-12T21:38:25Z,2021-11-12T21:38:25Z,"Distributed on-line reinforcement learning in a swarm of sterically
  interacting robots","While naturally occurring swarms thrive when crowded, physical interactions
in robotic swarms are either avoided or carefully controlled, thus limiting
their operational density. Designing behavioral strategies under such
circumstances remains a challenge, even though it may offer an opportunity for
exploring morpho-functional self-organized behaviors. In this paper, we
explicitly consider dense swarms of robots where physical interactions are
inevitable. We demonstrate experimentally that an a priori minor difference in
the mechanical design of the robots leads to important differences in their
dynamical behaviors when they evolve in crowded environments. We design
Morphobots, which are Kilobots augmented with a 3D-printed exoskeleton. The
exoskeleton not only significantly improves the motility and stability of the
Kilobots, it also allows to encode physically two contrasting dynamical
behaviors in response to an external force or a collision. This difference
translates into distinct performances during self-organized aggregation when
addressing a phototactic task. Having characterized the dynamical mechanism at
the root of these differences, we implement a decentralized on-line
evolutionary reinforcement learning algorithm in a swarm of Morphobots. We
demonstrate the learning efficiency and show that the learning reduces the
dependency on the morphology. We present a kinetic model that links the reward
function to an effective phototactic policy. Our results are of relevance for
the deployment of robust swarms of robots in a real environment, where robots
are deemed to collide, and to be exposed to external forces.",arxiv
http://arxiv.org/abs/2109.14549v1,2021-09-29T16:48:05Z,2021-09-29T16:48:05Z,"Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay
  Randomization","Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at https://mehooz.github.io/mmdr-wild/.",arxiv
http://arxiv.org/abs/1912.11584v1,2019-12-25T03:31:56Z,2019-12-25T03:31:56Z,"Exploration of the Applicability of Probabilistic Inference for Learning
  Control in Underactuated Autonomous Underwater Vehicles","Underwater vehicles are employed in the exploration of dynamic environments
where tuning of a specific controller for each task would be time-consuming and
unreliable as the controller depends on calculated mathematical coefficients in
idealised conditions. For such a case, learning task from experience can be a
useful alternative. This paper explores the capability of probabilistic
inference learning to control autonomous underwater vehicles that can be used
for different tasks without re-programming the controller. Probabilistic
inference learning uses a Gaussian process model of the real vehicle to learn
the correct policy with a small number of real field experiments. The use of
probabilistic reinforced learning looks for a simple implementation of
controllers without the burden of coefficients calculation, controller tuning
or system identification. A series of computational simulations were employed
to test the applicability of model-based reinforced learning in underwater
vehicles. Three simulation scenarios were evaluated: waypoint tracking, depth
control and 3D path tracking control. The 3D path tracking is done by coupling
together a line-of-sight law with probabilistic inference for learning control.
As a comparison study LOS-PILCO algorithm can perform better than a robust
LOS-PID. The results shows that probabilistic model based reinforced learning
is a possible solution to motion control of underactuated AUVs as can generate
capable policies with minimum quantity of episodes.",arxiv
http://arxiv.org/abs/2008.00376v1,2020-08-02T01:18:18Z,2020-08-02T01:18:18Z,"Velocity Regulation of 3D Bipedal Walking Robots with Uncertain Dynamics
  Through Adaptive Neural Network Controller","This paper presents a neural-network based adaptive feedback control
structure to regulate the velocity of 3D bipedal robots under dynamics
uncertainties. Existing Hybrid Zero Dynamics (HZD)-based controllers regulate
velocity through the implementation of heuristic regulators that do not
consider model and environmental uncertainties, which may significantly affect
the tracking performance of the controllers. In this paper, we address the
uncertainties in the robot dynamics from the perspective of the reduced
dimensional representation of virtual constraints and propose the integration
of an adaptive neural network-based controller to regulate the robot velocity
in the presence of model parameter uncertainties. The proposed approach yields
improved tracking performance under dynamics uncertainties. The shallow
adaptive neural network used in this paper does not require training a priori
and has the potential to be implemented on the real-time robotic controller. A
comparative simulation study of a 3D Cassie robot is presented to illustrate
the performance of the proposed approach under various scenarios.",arxiv
http://arxiv.org/abs/1808.09945v2,2020-12-03T15:51:08Z,2018-08-29T17:51:39Z,"Fixed-Point Convolutional Neural Network for Real-Time Video Processing
  in FPGA","Modern mobile neural networks with a reduced number of weights and parameters
do a good job with image classification tasks, but even they may be too complex
to be implemented in an FPGA for video processing tasks. The article proposes
neural network architecture for the practical task of recognizing images from a
camera, which has several advantages in terms of speed. This is achieved by
reducing the number of weights, moving from a floating-point to a fixed-point
arithmetic, and due to a number of hardware-level optimizations associated with
storing weights in blocks, a shift register, and an adjustable number of
convolutional blocks that work in parallel. The article also proposed methods
for adapting the existing data set for solving a different task. As the
experiments showed, the proposed neural network copes well with real-time video
processing even on the cheap FPGAs.",arxiv
http://arxiv.org/abs/2006.01250v6,2021-06-21T18:21:58Z,2020-05-09T09:41:46Z,RUHSNet: 3D Object Detection Using Lidar Data in Real Time,"In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects in
point cloud data. We compare the results with different backbone architectures
including the standard ones like VGG, ResNet, Inception with our backbone. Also
we present the optimization and ablation studies including designing an
efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking
and validating our results. Our work surpasses the state of the art in this
domain both in terms of average precision and speed running at > 30 FPS. This
makes it a feasible option to be deployed in real time applications including
self driving cars.",arxiv
http://arxiv.org/abs/2110.07742v1,2021-10-14T21:53:03Z,2021-10-14T21:53:03Z,"Beyond Classification: Directly Training Spiking Neural Networks for
  Semantic Segmentation","Spiking Neural Networks (SNNs) have recently emerged as the low-power
alternative to Artificial Neural Networks (ANNs) because of their sparse,
asynchronous, and binary event-driven processing. Due to their energy
efficiency, SNNs have a high possibility of being deployed for real-world,
resource-constrained systems such as autonomous vehicles and drones. However,
owing to their non-differentiable and complex neuronal dynamics, most previous
SNN optimization methods have been limited to image recognition. In this paper,
we explore the SNN applications beyond classification and present semantic
segmentation networks configured with spiking neurons. Specifically, we first
investigate two representative SNN optimization techniques for recognition
tasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic
segmentation datasets. We observe that, when converted from ANNs, SNNs suffer
from high latency and low performance due to the spatial variance of features.
Therefore, we directly train networks with surrogate gradient learning,
resulting in lower latency and higher performance than ANN-SNN conversion.
Moreover, we redesign two fundamental ANN segmentation architectures (i.e.,
Fully Convolutional Networks and DeepLab) for the SNN domain. We conduct
experiments on two public semantic segmentation benchmarks including the PASCAL
VOC2012 dataset and the DDD17 event-based dataset. In addition to showing the
feasibility of SNNs for semantic segmentation, we show that SNNs can be more
robust and energy-efficient compared to their ANN counterparts in this domain.",arxiv
http://arxiv.org/abs/1907.10148v3,2019-09-19T18:52:58Z,2019-07-23T21:41:54Z,"Conf-Net: Toward High-Confidence Dense 3D Point-Cloud with Error-Map
  Prediction","This work proposes a method for depth completion of sparse LiDAR data using a
convolutional neural network which can be used to generate semi-dense depth
maps and ""almost"" full 3D point-clouds with significantly lower root mean
squared error (RMSE) over state-of-the-art methods. We add an ""Error
Prediction"" unit to our network and present a novel and simple end-to-end
method that learns to predict an error-map of depth regression task. An
""almost"" dense high-confidence/low-variance point-cloud is more valuable for
safety-critical applications specifically real-world autonomous driving than a
full point-cloud with high error rate and high error variance. Using our
predicted error-map, we demonstrate that by up-filling a LiDAR point cloud from
18,000 points to 285,000 points, versus 300,000 points for full depth, we can
reduce the RMSE error from 1004 to 399. This error is approximately 60% less
than the state-of-the-art and 50% less than the state-of-the-art with RGB
guidance (we did not use RGB guidance in our algorithm). In addition to
analyzing our results on Kitti depth completion dataset, we also demonstrate
the ability of our proposed method to extend to new tasks by deploying our
""Error Prediction"" unit to improve upon the state-of-the-art for monocular
depth estimation. Codes and demo videos are available at
http://github.com/hekmak/Conf-net.",arxiv
http://arxiv.org/abs/2003.11117v1,2020-03-24T21:17:44Z,2020-03-24T21:17:44Z,"COVID-19 and Computer Audition: An Overview on What Speech & Sound
  Analysis Could Contribute in the SARS-CoV-2 Corona Crisis","At the time of writing, the world population is suffering from more than
10,000 registered COVID-19 disease epidemic induced deaths since the outbreak
of the Corona virus more than three months ago now officially known as
SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer
and control the epidemic by now labelled as pandemic. In this contribution, we
provide an overview on the potential for computer audition (CA), i.e., the
usage of speech and sound analysis by artificial intelligence to help in this
scenario. We first survey which types of related or contextually significant
phenomena can be automatically assessed from speech or sound. These include the
automatic recognition and monitoring of breathing, dry and wet coughing or
sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to
name but a few. Then, we consider potential use-cases for exploitation. These
include risk assessment and diagnosis based on symptom histograms and their
development over time, as well as monitoring of spread, social distancing and
its effects, treatment and recovery, and patient wellbeing. We quickly guide
further through challenges that need to be faced for real-life usage. We come
to the conclusion that CA appears ready for implementation of (pre-)diagnosis
and monitoring tools, and more generally provides rich and significant, yet so
far untapped potential in the fight against COVID-19 spread.",arxiv
http://arxiv.org/abs/1810.12541v1,2018-10-30T06:19:58Z,2018-10-30T06:19:58Z,"Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture
  Generation for Humanoid Robots","Co-speech gestures enhance interaction experiences between humans as well as
between humans and robots. Existing robots use rule-based speech-gesture
association, but this requires human labor and prior knowledge of experts to be
implemented. We present a learning-based co-speech gesture generation that is
learned from 52 h of TED talks. The proposed end-to-end neural network model
consists of an encoder for speech text understanding and a decoder to generate
a sequence of gestures. The model successfully produces various gestures
including iconic, metaphoric, deictic, and beat gestures. In a subjective
evaluation, participants reported that the gestures were human-like and matched
the speech content. We also demonstrate a co-speech gesture with a NAO robot
working in real time.",arxiv
http://arxiv.org/abs/2107.12744v2,2021-08-04T14:59:30Z,2021-07-27T11:38:44Z,"Real-Time Activity Recognition and Intention Recognition Using a
  Vision-based Embedded System","With the rapid increase in digital technologies, most fields of study include
recognition of human activity and intention recognition, which are essential in
smart environments. In this study, we equipped the activity recognition system
with the ability to recognize intentions by affecting the pace of movement of
individuals in the representation of images. Using this technology in various
environments such as elevators and automatic doors will lead to identifying
those who intend to pass the automatic door from those who are passing by. This
system, if applied in elevators and automatic doors, will save energy and
increase efficiency. For this study, data preparation is applied to combine the
spatial and temporal features with the help of digital image processing
principles. Nevertheless, unlike previous studies, only one AlexNet neural
network is used instead of two-stream convolutional neural networks. Our
embedded system was implemented with an accuracy of 98.78% on our intention
recognition dataset. We also examined our data representation approach on other
datasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of
78.48%, 97.95%, and 100%, respectively. The image recognition and neural
network models were simulated and implemented using Xilinx simulators for the
Xilinx ZCU102 board. The operating frequency of this embedded system is 333
MHz, and it works in real-time with 120 frames per second (fps).",arxiv
http://arxiv.org/abs/1905.12612v2,2019-10-15T08:10:25Z,2019-05-29T17:50:19Z,Learning Navigation Subroutines from Egocentric Videos,"Planning at a higher level of abstraction instead of low level torques
improves the sample efficiency in reinforcement learning, and computational
efficiency in classical planning. We propose a method to learn such
hierarchical abstractions, or subroutines from egocentric video data of experts
performing tasks. We learn a self-supervised inverse model on small amounts of
random interaction data to pseudo-label the expert egocentric videos with agent
actions. Visuomotor subroutines are acquired from these pseudo-labeled videos
by learning a latent intent-conditioned policy that predicts the inferred
pseudo-actions from the corresponding image observations. We demonstrate our
proposed approach in context of navigation, and show that we can successfully
learn consistent and diverse visuomotor subroutines from passive egocentric
videos. We demonstrate the utility of our acquired visuomotor subroutines by
using them as is for exploration, and as sub-policies in a hierarchical RL
framework for reaching point goals and semantic goals. We also demonstrate
behavior of our subroutines in the real world, by deploying them on a real
robotic platform. Project website:
https://ashishkumar1993.github.io/subroutines/.",arxiv
http://arxiv.org/abs/2109.07165v1,2021-09-15T09:00:56Z,2021-09-15T09:00:56Z,3D Annotation Of Arbitrary Objects In The Wild,"Recent years have produced a variety of learning based methods in the context
of computer vision and robotics. Most of the recently proposed methods are
based on deep learning, which require very large amounts of data compared to
traditional methods. The performance of the deep learning methods are largely
dependent on the data distribution they were trained on, and it is important to
use data from the robot's actual operating domain during training. Therefore,
it is not possible to rely on pre-built, generic datasets when deploying robots
in real environments, creating a need for efficient data collection and
annotation in the specific operating conditions the robots will operate in. The
challenge is then: how do we reduce the cost of obtaining such datasets to a
point where we can easily deploy our robots in new conditions, environments and
to support new sensors? As an answer to this question, we propose a data
annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry.
The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel
annotations of arbitrary objects without needing accurate 3D models of the
objects prior to data collection and annotation. Our results showcase almost
90% Intersection-over-Union (IoU) agreement on both semantic segmentation and
2D bounding box detection across a variety of objects and scenes, while
speeding up the annotation process by several orders of magnitude compared to
traditional manual annotation.",arxiv
http://arxiv.org/abs/1710.08637v1,2017-10-24T07:46:57Z,2017-10-24T07:46:57Z,"Improving Accuracy of Nonparametric Transfer Learning via Vector
  Segmentation","Transfer learning using deep neural networks as feature extractors has become
increasingly popular over the past few years. It allows to obtain
state-of-the-art accuracy on datasets too small to train a deep neural network
on its own, and it provides cutting edge descriptors that, combined with
nonparametric learning methods, allow rapid and flexible deployment of
performing solutions in computationally restricted settings. In this paper, we
are interested in showing that the features extracted using deep neural
networks have specific properties which can be used to improve accuracy of
downstream nonparametric learning methods. Namely, we demonstrate that for some
distributions where information is embedded in a few coordinates, segmenting
feature vectors can lead to better accuracy. We show how this model can be
applied to real datasets by performing experiments using three mainstream deep
neural network feature extractors and four databases, in vision and audio.",arxiv
http://arxiv.org/abs/2101.10278v1,2021-01-25T17:58:47Z,2021-01-25T17:58:47Z,"High-Quality Vocoding Design with Signal Processing for Speech Synthesis
  and Voice Conversion","This Ph.D. thesis focuses on developing a system for high-quality speech
synthesis and voice conversion. Vocoder-based speech analysis, manipulation,
and synthesis plays a crucial role in various kinds of statistical parametric
speech research. Although there are vocoding methods which yield close to
natural synthesized speech, they are typically computationally expensive, and
are thus not suitable for real-time implementation, especially in embedded
environments. Therefore, there is a need for simple and computationally
feasible digital signal processing algorithms for generating high-quality and
natural-sounding synthesized speech. In this dissertation, I propose a solution
to extract optimal acoustic features and a new waveform generator to achieve
higher sound quality and conversion accuracy by applying advances in deep
learning. The approach remains computationally efficient. This challenge
resulted in five thesis groups, which are briefly summarized below.",arxiv
http://arxiv.org/abs/2007.08224v2,2020-07-20T15:42:02Z,2020-07-16T09:50:23Z,SAILenv: Learning in Virtual Visual Environments Made Simple,"Recently, researchers in Machine Learning algorithms, Computer Vision
scientists, engineers and others, showed a growing interest in 3D simulators as
a mean to artificially create experimental settings that are very close to
those in the real world. However, most of the existing platforms to interface
algorithms with 3D environments are often designed to setup navigation-related
experiments, to study physical interactions, or to handle ad-hoc cases that are
not thought to be customized, sometimes lacking a strong photorealistic
appearance and an easy-to-use software interface. In this paper, we present a
novel platform, SAILenv, that is specifically designed to be simple and
customizable, and that allows researchers to experiment visual recognition in
virtual 3D scenes. A few lines of code are needed to interface every algorithm
with the virtual world, and non-3D-graphics experts can easily customize the 3D
environment itself, exploiting a collection of photorealistic objects. Our
framework yields pixel-level semantic and instance labeling, depth, and, to the
best of our knowledge, it is the only one that provides motion-related
information directly inherited from the 3D engine. The client-server
communication operates at a low level, avoiding the overhead of HTTP-based data
exchanges. We perform experiments using a state-of-the-art object detector
trained on real-world images, showing that it is able to recognize the
photorealistic 3D objects of our environment. The computational burden of the
optical flow compares favourably with the estimation performed using modern
GPU-based convolutional networks or more classic implementations. We believe
that the scientific community will benefit from the easiness and high-quality
of our framework to evaluate newly proposed algorithms in their own customized
realistic conditions.",arxiv
http://arxiv.org/abs/2101.11451v1,2021-01-27T14:35:52Z,2021-01-27T14:35:52Z,"Controlling by Showing: i-Mimic: A Video-based Method to Control Robotic
  Arms","A novel concept of vision-based intelligent control of robotic arms is
developed here in this work. This work enables the controlling of robotic arms
motion only with visual inputs, that is, controlling by showing the videos of
correct movements. This work can broadly be sub-divided into two segments. The
first part of this work is to develop an unsupervised vision-based method to
control robotic arm in 2-D plane, and the second one is with deep CNN in the
same task in 3-D plane. The first method is unsupervised, where our aim is to
perform mimicking of human arm motion in real-time by a manipulator. We
developed a network, namely the vision-to-motion optical network (DON), where
the input should be a video stream containing hand movements of human, the the
output would be out the velocity and torque information of the hand movements
shown in the videos. The output information of the DON is then fed to the
robotic arm by enabling it to generate motion according to the real hand
videos. The method has been tested with both live-stream video feed as well as
on recorded video obtained from a monocular camera even by intelligently
predicting the trajectory of human hand hand when it gets occluded. This is why
the mimicry of the arm incorporates some intelligence to it and becomes
intelligent mimic (i-mimic). Alongside the unsupervised method another method
has also been developed deploying the deep neural network technique with CNN
(Convolutional Neural Network) to perform the mimicking, where labelled
datasets are used for training. The same dataset, as used in the unsupervised
DON-based method, is used in the deep CNN method, after manual annotations.
Both the proposed methods are validated with off-line as well as with on-line
video datasets in real-time. The entire methodology is validated with real-time
1-link and simulated n-link manipulators alongwith suitable comparisons.",arxiv
http://arxiv.org/abs/1810.01140v2,2018-10-08T08:40:40Z,2018-10-02T09:45:15Z,"Training compact deep learning models for video classification using
  circulant matrices","In real world scenarios, model accuracy is hardly the only factor to
consider. Large models consume more memory and are computationally more
intensive, which makes them difficult to train and to deploy, especially on
mobile devices. In this paper, we build on recent results at the crossroads of
Linear Algebra and Deep Learning which demonstrate how imposing a structure on
large weight matrices can be used to reduce the size of the model. We propose
very compact models for video classification based on state-of-the-art network
architectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then
conduct thorough experiments using the large YouTube-8M video classification
dataset. As we will show, the circulant DBoF embedding achieves an excellent
trade-off between size and accuracy.",arxiv
http://arxiv.org/abs/1805.08692v1,2018-05-04T15:07:29Z,2018-05-04T15:07:29Z,"Assessing a mobile-based deep learning model for plant disease
  surveillance","Convolutional neural network models (CNNs) have made major advances in
computer vision tasks in the last five years. Given the challenge in collecting
real world datasets, most studies report performance metrics based on available
research datasets. In scenarios where CNNs are to be deployed on images or
videos from mobile devices, models are presented with new challenges due to
lighting, angle, and camera specifications, which are not accounted for in
research datasets. It is essential for assessment to also be conducted on real
world datasets if such models are to be reliably integrated with products and
services in society. Plant disease datasets can be used to test CNNs in real
time and gain insight into real world performance. We train a CNN object
detection model to identify foliar symptoms of diseases (or lack thereof) in
cassava (Manihot esculenta Crantz). We then deploy the model on a mobile app
and test its performance on mobile images and video of 720 diseased leaflets in
an agricultural field in Tanzania. Within each disease category we test two
levels of severity of symptoms - mild and pronounced, to assess the model
performance for early detection of symptoms. In both severities we see a
decrease in the F-1 score for real world images and video. The F-1 score
dropped by 32% for pronounced symptoms in real world images (the closest data
to the training data) due to a drop in model recall. If the potential of
smartphone CNNs are to be realized our data suggest it is crucial to consider
tuning precision and recall performance in order to achieve the desired
performance in real world settings. In addition, the varied performance related
to different input data (image or video) is an important consideration for the
design of CNNs in real world applications.",arxiv
http://arxiv.org/abs/1906.06969v1,2019-06-17T11:44:15Z,2019-06-17T11:44:15Z,Robotic Navigation using Entropy-Based Exploration,"Robotic navigation concerns the task in which a robot should be able to find
a safe and feasible path and traverse between two points in a complex
environment. We approach the problem of robotic navigation using reinforcement
learning and use deep $Q$-networks to train agents to solve the task of robotic
navigation. We compare the Entropy-Based Exploration (EBE) with the widely used
$\epsilon$-greedy exploration strategy by training agents using both of them in
simulation. The trained agents are then tested on different versions of the
environment to test the generalization ability of the learned policies. We also
implement the learned policies on a real robot in complex real environment
without any fine tuning and compare the effectiveness of the above-mentioned
exploration strategies in the real world setting. Video showing experiments on
TurtleBot3 platform is available at \url{https://youtu.be/NHT-EiN_4n8}.",arxiv
http://arxiv.org/abs/2007.07132v1,2020-07-14T15:51:52Z,2020-07-14T15:51:52Z,"A Deep Learning Approach for Low-Latency Packet Loss Concealment of
  Audio Signals in Networked Music Performance Applications","Networked Music Performance (NMP) is envisioned as a potential game changer
among Internet applications: it aims at revolutionizing the traditional concept
of musical interaction by enabling remote musicians to interact and perform
together through a telecommunication network. Ensuring realistic conditions for
music performance, however, constitutes a significant engineering challenge due
to extremely strict requirements in terms of audio quality and, most
importantly, network delay. To minimize the end-to-end delay experienced by the
musicians, typical implementations of NMP applications use un-compressed,
bidirectional audio streams and leverage UDP as transport protocol. Being
connection less and unreliable,audio packets transmitted via UDP which become
lost in transit are not re-transmitted and thus cause glitches in the receiver
audio playout. This article describes a technique for predicting lost packet
content in real-time using a deep learning approach. The ability of concealing
errors in real time can help mitigate audio impairments caused by packet
losses, thus improving the quality of audio playout in real-world scenarios.",arxiv
http://arxiv.org/abs/1310.3322v1,2013-10-12T01:16:32Z,2013-10-12T01:16:32Z,GPU-Framework for Teamwork Action Recognition,"Real time processing for teamwork action recognition is a challenge, due to
complex computational models to achieve high system performance. Hence, this
paper proposes a framework based on Graphical Processing Units (GPUs) to
achieve a significant speed up in the performance of role based activity
recognition of teamwork. The framework can be applied in various fields,
especially athletic and military applications. Furthermore, the framework can
be customized for many action recognition applications. The paper presents the
stages of the framework where GPUs are the main tool for performance
improvement. The speedup is achieved by performing video processing and Machine
learning algorithms on GPU. Video processing and machine learning algorithms
covers all computations involved in our framework. Video processing tasks on
involves GPU implementation of Motion detection, segmentation and object
tracking algorithms. In addition, our framework is integrated with GPUCV, a GPU
version of OpenCV functions. Machine learning tasks are supported under our
framework with GPU implementations of Support Vector Machine (SVM) for object
classification and feature discretization, Hidden Marcov Model (HMM) for
activity recognition phase, and ID3 algorithm for role recognition of team
members. The system was tested against UC-Teamwork dataset and speedup of 20X
has been achieved on NVidia 9500GT graphics card (32 500MHZ processors).",arxiv
http://arxiv.org/abs/2105.02957v1,2021-04-27T10:08:40Z,2021-04-27T10:08:40Z,"VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the
  Edge for the Internet of Multimedia Things","Efficient video processing is a critical component in many IoMT applications
to detect events of interest. Presently, many window optimization techniques
have been proposed in event processing with an underlying assumption that the
incoming stream has a structured data model. Videos are highly complex due to
the lack of any underlying structured data model. Video stream sources such as
CCTV cameras and smartphones are resource-constrained edge nodes. At the same
time, video content extraction is expensive and requires computationally
intensive Deep Neural Network (DNN) models that are primarily deployed at
high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage
allied windowing approach to accelerate video event analytics in an edge-cloud
paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the
query and resource-aware optimization for state-based complex event matching.
VID-WIN exploits the video content and DNN input knobs to accelerate the video
inference process across nodes. The paper proposes a novel content-driven
micro-batch resizing, queryaware caching and micro-batch based utility
filtering strategy of video frames under resource-constrained edge nodes to
improve the overall system throughput, latency, and network usage. Extensive
evaluations are performed over five real-world datasets. The experimental
results show that VID-WIN video event matching achieves ~2.3X higher throughput
with minimal latency and ~99% bandwidth reduction compared to other baselines
while maintaining query-level accuracy and resource bounds.",arxiv
http://arxiv.org/abs/2107.12563v1,2021-07-27T02:50:46Z,2021-07-27T02:50:46Z,Parallel Detection for Efficient Video Analytics at the Edge,"Deep Neural Network (DNN) trained object detectors are widely deployed in
many mission-critical systems for real time video analytics at the edge, such
as autonomous driving and video surveillance. A common performance requirement
in these mission-critical edge services is the near real-time latency of online
object detection on edge devices. However, even with well-trained DNN object
detectors, the online detection quality at edge may deteriorate for a number of
reasons, such as limited capacity to run DNN object detection models on
heterogeneous edge devices, and detection quality degradation due to random
frame dropping when the detection processing rate is significantly slower than
the incoming video frame rate. This paper addresses these problems by
exploiting multi-model multi-device detection parallelism for fast object
detection in edge systems with heterogeneous edge devices. First, we analyze
the performance bottleneck of running a well-trained DNN model at edge for real
time online object detection. We use the offline detection as a reference
model, and examine the root cause by analyzing the mismatch among the incoming
video streaming rate, video processing rate for object detection, and output
rate for real time detection visualization of video streaming. Second, we study
performance optimizations by exploiting multi-model detection parallelism. We
show that the model-parallel detection approach can effectively speed up the
FPS detection processing rate, minimizing the FPS disparity with the incoming
video frame rate on heterogeneous edge devices. We evaluate the proposed
approach using SSD300 and YOLOv3 on benchmark videos of different video stream
rates. The results show that exploiting multi-model detection parallelism can
speed up the online object detection processing rate and deliver near real-time
object detection performance for efficient video analytics at edge.",arxiv
http://arxiv.org/abs/2007.02351v1,2020-07-05T14:24:24Z,2020-07-05T14:24:24Z,Offline Model Guard: Secure and Private ML on Mobile Devices,"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time.",arxiv
http://arxiv.org/abs/1905.05212v1,2019-05-13T18:01:01Z,2019-05-13T18:01:01Z,"Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter
  pruning","Convolutional neural networks (CNNs) have emerged as the state-of-the-art in
multiple vision tasks including depth estimation. However, memory and computing
power requirements remain as challenges to be tackled in these models.
Monocular depth estimation has significant use in robotics and virtual reality
that requires deployment on low-end devices. Training a small model from
scratch results in a significant drop in accuracy and it does not benefit from
pre-trained large models. Motivated by the literature of model pruning, we
propose a lightweight monocular depth model obtained from a large trained
model. This is achieved by removing the least important features with a novel
joint end-to-end filter pruning. We propose to learn a binary mask for each
filter to decide whether to drop the filter or not. These masks are trained
jointly to exploit relations between filters at different layers as well as
redundancy within the same layer. We show that we can achieve around 5x
compression rate with small drop in accuracy on the KITTI driving dataset. We
also show that masking can improve accuracy over the baseline with fewer
parameters, even without enforcing compression loss.",arxiv
http://arxiv.org/abs/1612.06299v1,2016-12-19T18:12:20Z,2016-12-19T18:12:20Z,Simple Black-Box Adversarial Perturbations for Deep Networks,"Deep neural networks are powerful and popular learning models that achieve
state-of-the-art pattern recognition performance on many computer vision,
speech, and language processing tasks. However, these networks have also been
shown susceptible to carefully crafted adversarial perturbations which force
misclassification of the inputs. Adversarial examples enable adversaries to
subvert the expected system behavior leading to undesired consequences and
could pose a security risk when these systems are deployed in the real world.
  In this work, we focus on deep convolutional neural networks and demonstrate
that adversaries can easily craft adversarial examples even without any
internal knowledge of the target network. Our attacks treat the network as an
oracle (black-box) and only assume that the output of the network can be
observed on the probed inputs. Our first attack is based on a simple idea of
adding perturbation to a randomly selected single pixel or a small set of them.
We then improve the effectiveness of this attack by carefully constructing a
small set of pixels to perturb by using the idea of greedy local-search. Our
proposed attacks also naturally extend to a stronger notion of
misclassification. Our extensive experimental results illustrate that even
these elementary attacks can reveal a deep neural network's vulnerabilities.
The simplicity and effectiveness of our proposed schemes mean that they could
serve as a litmus test for designing robust networks.",arxiv
http://arxiv.org/abs/1904.02579v2,2019-10-15T15:45:57Z,2019-04-04T14:30:09Z,"Can a Robot Become a Movie Director? Learning Artistic Principles for
  Aerial Cinematography","Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human expert, our drone learns how to make
favorable viewpoint selections by experience. We propose a learning scheme that
exploits aesthetical features of retrospective shots in order to extract a
desirable policy for better prospective shots. We train our agent in realistic
AirSim simulations using both a hand-crafted reward function as well as reward
from direct human input. We then deploy the same agent on a real DJI M210 drone
in order to test the generalization capability of our approach to real world
conditions. To evaluate the success of our approach in the end, we conduct a
comprehensive user study in which participants rate the shot quality of our
methods. Videos of the system in action can be seen at
https://youtu.be/qmVw6mfyEmw.",arxiv
http://arxiv.org/abs/1705.02514v2,2017-10-31T01:58:50Z,2017-05-06T18:27:09Z,End-to-end Source Separation with Adaptive Front-Ends,"Source separation and other audio applications have traditionally relied on
the use of short-time Fourier transforms as a front-end frequency domain
representation step. The unavailability of a neural network equivalent to
forward and inverse transforms hinders the implementation of end-to-end
learning systems for these applications. We present an auto-encoder neural
network that can act as an equivalent to short-time front-end transforms. We
demonstrate the ability of the network to learn optimal, real-valued basis
functions directly from the raw waveform of a signal and further show how it
can be used as an adaptive front-end for supervised source separation. In terms
of separation performance, these transforms significantly outperform their
Fourier counterparts. Finally, we also propose a novel source to distortion
ratio based cost function for end-to-end source separation.",arxiv
http://arxiv.org/abs/1806.01248v2,2018-06-08T22:01:12Z,2018-06-04T17:39:58Z,"Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent
  Neural Network on Mobile Devices","Recurrent neural networks (RNNs) achieve cutting-edge performance on a
variety of problems. However, due to their high computational and memory
demands, deploying RNNs on resource constrained mobile devices is a challenging
task. To guarantee minimum accuracy loss with higher compression rate and
driven by the mobile resource requirement, we introduce a novel model
compression approach DirNet based on an optimized fast dictionary learning
algorithm, which 1) dynamically mines the dictionary atoms of the projection
dictionary matrix within layer to adjust the compression rate 2) adaptively
changes the sparsity of sparse codes cross the hierarchical layers.
Experimental results on language model and an ASR model trained with a 1000h
speech dataset demonstrate that our method significantly outperforms prior
approaches. Evaluated on off-the-shelf mobile devices, we are able to reduce
the size of original model by eight times with real-time model inference and
negligible accuracy loss.",arxiv
http://arxiv.org/abs/1902.03722v1,2019-02-11T04:20:20Z,2019-02-11T04:20:20Z,"A Minimal Template for Interactive Web-based Demonstrations of Musical
  Machine Learning","New machine learning algorithms are being developed to solve problems in
different areas, including music. Intuitive, accessible, and understandable
demonstrations of the newly built models could help attract the attention of
people from different disciplines and evoke discussions. However, we notice
that it has not been a common practice for researchers working on musical
machine learning to demonstrate their models in an interactive way. To address
this issue, we present in this paper an template that is specifically designed
to demonstrate symbolic musical machine learning models on the web. The
template comes with a small codebase, is open source, and is meant to be easy
to use by any practitioners to implement their own demonstrations. Moreover,
its modular design facilitates the reuse of the musical components and
accelerates the implementation. We use the template to build interactive
demonstrations of four exemplary music generation models. We show that the
built-in interactivity and real-time audio rendering of the browser make the
demonstration easier to understand and to play with. It also helps researchers
to gain insights into different models and to A/B test them.",arxiv
http://arxiv.org/abs/2008.04594v1,2020-08-11T09:13:54Z,2020-08-11T09:13:54Z,Multi-modal segmentation of 3D brain scans using neural networks,"Purpose: To implement a brain segmentation pipeline based on convolutional
neural networks, which rapidly segments 3D volumes into 27 anatomical
structures. To provide an extensive, comparative study of segmentation
performance on various contrasts of magnetic resonance imaging (MRI) and
computed tomography (CT) scans. Methods: Deep convolutional neural networks are
trained to segment 3D MRI (MPRAGE, DWI, FLAIR) and CT scans. A large database
of in total 851 MRI/CT scans is used for neural network training. Training
labels are obtained on the MPRAGE contrast and coregistered to the other
imaging modalities. The segmentation quality is quantified using the Dice
metric for a total of 27 anatomical structures. Dropout sampling is implemented
to identify corrupted input scans or low-quality segmentations. Full
segmentation of 3D volumes with more than 2 million voxels is obtained in less
than 1s of processing time on a graphical processing unit. Results: The best
average Dice score is found on $T_1$-weighted MPRAGE ($85.3\pm4.6\,\%$).
However, for FLAIR ($80.0\pm7.1\,\%$), DWI ($78.2\pm7.9\,\%$) and CT ($79.1\pm
7.9\,\%$), good-quality segmentation is feasible for most anatomical
structures. Corrupted input volumes or low-quality segmentations can be
detected using dropout sampling. Conclusion: The flexibility and performance of
deep convolutional neural networks enables the direct, real-time segmentation
of FLAIR, DWI and CT scans without requiring $T_1$-weighted scans.",arxiv
http://arxiv.org/abs/2005.09199v1,2020-05-19T03:52:21Z,2020-05-19T03:52:21Z,FrameProv: Towards End-To-End Video Provenance,"Video feeds are often deliberately used as evidence, as in the case of CCTV
footage; but more often than not, the existence of footage of a supposed event
is perceived as proof of fact in the eyes of the public at large. This reliance
represents a societal vulnerability given the existence of easy-to-use editing
tools and means to fabricate entire video feeds using machine learning. And, as
the recent barrage of fake news and fake porn videos have shown, this isn't
merely an academic concern, it is actively been exploited. I posit that this
exploitation is only going to get more insidious. In this position paper, I
introduce a long term project that aims to mitigate some of the most egregious
forms of manipulation by embedding trustworthy components in the video
transmission chain. Unlike earlier works, I am not aiming to do tamper
detection or other forms of forensics -- approaches I think are bound to fail
in the face of the reality of necessary editing and compression -- instead, the
aim here is to provide a way for the video publisher to prove the integrity of
the video feed as well as make explicit any edits they may have performed. To
do this, I present a novel data structure, a video-edit specification language
and supporting infrastructure that provides end-to-end video provenance, from
the camera sensor to the viewer. I have implemented a prototype of this system
and am in talks with journalists and video editors to discuss the best ways
forward with introducing this idea to the mainstream.",arxiv
http://arxiv.org/abs/2004.01030v1,2020-04-01T14:50:43Z,2020-04-01T14:50:43Z,"Objects of violence: synthetic data for practical ML in human rights
  investigations","We introduce a machine learning workflow to search for, identify, and
meaningfully triage videos and images of munitions, weapons, and military
equipment, even when limited training data exists for the object of interest.
This workflow is designed to expedite the work of OSINT (""open source
intelligence"") researchers in human rights investigations. It consists of three
components: automatic rendering and annotating of synthetic datasets that make
up for a lack of training data; training image classifiers from combined sets
of photographic and synthetic data; and mtriage, an open source software that
orchestrates these classifiers' deployment to triage public domain media, and
visualise predictions in a web interface. We show that synthetic data helps to
train classifiers more effectively, and that certain approaches yield better
results for different architectures. We then demonstrate our workflow in two
real-world human rights investigations: the use of the Triple-Chaser tear gas
grenade against civilians, and the verification of allegations of military
presence in Ukraine in 2014.",arxiv
http://arxiv.org/abs/1908.09089v1,2019-08-24T04:06:01Z,2019-08-24T04:06:01Z,"Web-enabled Intelligent System for Continuous Sensor Data Processing and
  Visualization","A large number of sensors deployed in recent years in various setups and
their data is readily available in dedicated databases or in the cloud. Of
particular interest is real-time data processing and 3D visualization in
web-based user interfaces that facilitate spatial information understanding and
sharing, hence helping the decision making process for all the parties
involved. In this research, we provide a prototype system for near real-time,
continuous X3D-based visualization of processed sensor data for two significant
applications: thermal monitoring for residential/commercial buildings and
nitrogen cycle monitoring in water beds for aquaponics systems. As sensors are
sparsely placed, in each application, where they collect data for large periods
(of up to one year), we employ a Finite Differences Method and a Neural
Networks model to approximate data distribution in the entire volume.",arxiv
http://arxiv.org/abs/1510.02055v1,2015-10-07T18:34:36Z,2015-10-07T18:34:36Z,"Diverse Large-Scale ITS Dataset Created from Continuous Learning for
  Real-Time Vehicle Detection","In traffic engineering, vehicle detectors are trained on limited datasets
resulting in poor accuracy when deployed in real world applications. Annotating
large-scale high quality datasets is challenging. Typically, these datasets
have limited diversity; they do not reflect the real-world operating
environment. There is a need for a large-scale, cloud based positive and
negative mining (PNM) process and a large-scale learning and evaluation system
for the application of traffic event detection. The proposed positive and
negative mining process addresses the quality of crowd sourced ground truth
data through machine learning review and human feedback mechanisms. The
proposed learning and evaluation system uses a distributed cloud computing
framework to handle data-scaling issues associated with large numbers of
samples and a high-dimensional feature space. The system is trained using
AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated
video frames. The trained real-time vehicle detector achieves an accuracy of at
least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on
approximately $7,500,000$ video frames. At the end of 2015, the dataset is
expect to have over one billion annotated video frames.",arxiv
http://arxiv.org/abs/2001.00391v1,2020-01-02T11:12:50Z,2020-01-02T11:12:50Z,"Temporal-Spatial Neural Filter: Direction Informed End-to-End
  Multi-channel Target Speech Separation","Target speech separation refers to extracting the target speaker's speech
from mixed signals. Despite the recent advances in deep learning based
close-talk speech separation, the applications to real-world are still an open
issue. Two main challenges are the complex acoustic environment and the
real-time processing requirement. To address these challenges, we propose a
temporal-spatial neural filter, which directly estimates the target speech
waveform from multi-speaker mixture in reverberant environments, assisted with
directional information of the speaker(s). Firstly, against variations brought
by complex environment, the key idea is to increase the acoustic representation
completeness through the jointly modeling of temporal, spectral and spatial
discriminability between the target and interference source. Specifically,
temporal, spectral, spatial along with the designed directional features are
integrated to create a joint acoustic representation. Secondly, to reduce the
latency, we design a fully-convolutional autoencoder framework, which is purely
end-to-end and single-pass. All the feature computation is implemented by the
network layers and operations to speed up the separation procedure. Evaluation
is conducted on simulated reverberant dataset WSJ0-2mix and WSJ0-3mix under
speaker-independent scenario. Experimental results demonstrate that the
proposed method outperforms state-of-the-art deep learning based multi-channel
approaches with fewer parameters and faster processing speed. Furthermore, the
proposed temporal-spatial neural filter can handle mixtures with varying and
unknown number of speakers and exhibits persistent performance even when
existing a direction estimation error. Codes and models will be released soon.",arxiv
http://arxiv.org/abs/2010.06676v1,2020-10-13T20:23:47Z,2020-10-13T20:23:47Z,On Front-end Gain Invariant Modeling for Wake Word Spotting,"Wake word (WW) spotting is challenging in far-field due to the complexities
and variations in acoustic conditions and the environmental interference in
signal transmission. A suite of carefully designed and optimized audio
front-end (AFE) algorithms help mitigate these challenges and provide better
quality audio signals to the downstream modules such as WW spotter. Since the
WW model is trained with the AFE-processed audio data, its performance is
sensitive to AFE variations, such as gain changes. In addition, when deploying
to new devices, the WW performance is not guaranteed because the AFE is unknown
to the WW model. To address these issues, we propose a novel approach to use a
new feature called $\Delta$LFBE to decouple the AFE gain variations from the WW
model. We modified the neural network architectures to accommodate the delta
computation, with the feature extraction module unchanged. We evaluate our WW
models using data collected from real household settings and showed the models
with the $\Delta$LFBE is robust to AFE gain changes. Specifically, when AFE
gain changes up to $\pm$12dB, the baseline CNN model lost up to relative 19.0%
in false alarm rate or 34.3% in false reject rate, while the model with
$\Delta$LFBE demonstrates no performance loss.",arxiv
http://arxiv.org/abs/2012.06380v1,2020-12-11T14:28:30Z,2020-12-11T14:28:30Z,Parallelized Rate-Distortion Optimized Quantization Using Deep Learning,"Rate-Distortion Optimized Quantization (RDOQ) has played an important role in
the coding performance of recent video compression standards such as H.264/AVC,
H.265/HEVC, VP9 and AV1. This scheme yields significant reductions in bit-rate
at the expense of relatively small increases in distortion. Typically, RDOQ
algorithms are prohibitively expensive to implement on real-time hardware
encoders due to their sequential nature and their need to frequently obtain
entropy coding costs. This work addresses this limitation using a neural
network-based approach, which learns to trade-off rate and distortion during
offline supervised training. As these networks are based solely on standard
arithmetic operations that can be executed on existing neural network hardware,
no additional area-on-chip needs to be reserved for dedicated RDOQ circuitry.
We train two classes of neural networks, a fully-convolutional network and an
auto-regressive network, and evaluate each as a post-quantization step designed
to refine cheap quantization schemes such as scalar quantization (SQ). Both
network architectures are designed to have a low computational overhead. After
training they are integrated into the HM 16.20 implementation of HEVC, and
their video coding performance is evaluated on a subset of the H.266/VVC SDR
common test sequences. Comparisons are made to RDOQ and SQ implementations in
HM 16.20. Our method achieves 1.64% BD-rate savings on luminosity compared to
the HM SQ anchor, and on average reaches 45% of the performance of the
iterative HM RDOQ algorithm.",arxiv
http://arxiv.org/abs/1904.12618v1,2019-04-19T10:48:18Z,2019-04-19T10:48:18Z,Deep Learning Based Automatic Video Annotation Tool for Self-Driving Car,"In a self-driving car, objection detection, object classification, lane
detection and object tracking are considered to be the crucial modules. In
recent times, using the real time video one wants to narrate the scene captured
by the camera fitted in our vehicle. To effectively implement this task, deep
learning techniques and automatic video annotation tools are widely used. In
the present paper, we compare the various techniques that are available for
each module and choose the best algorithm among them by using appropriate
metrics. For object detection, YOLO and Retinanet-50 are considered and the
best one is chosen based on mean Average Precision (mAP). For object
classification, we consider VGG-19 and Resnet-50 and select the best algorithm
based on low error rate and good accuracy. For lane detection, Udacity's
'Finding Lane Line' and deep learning based LaneNet algorithms are compared and
the best one that can accurately identify the given lane is chosen for
implementation. As far as object tracking is concerned, we compare Udacity's
'Object Detection and Tracking' algorithm and deep learning based Deep Sort
algorithm. Based on the accuracy of tracking the same object in many frames and
predicting the movement of objects, the best algorithm is chosen. Our automatic
video annotation tool is found to be 83% accurate when compared with a human
annotator. We considered a video with 530 frames each of resolution 1035 x 1800
pixels. At an average each frame had about 15 objects. Our annotation tool
consumed 43 minutes in a CPU based system and 2.58 minutes in a mid-level GPU
based system to process all four modules. But the same video took nearly 3060
minutes for one human annotator to narrate the scene in the given video. Thus
we claim that our proposed automatic video annotation tool is reasonably fast
(about 1200 times in a GPU system) and accurate.",arxiv
http://arxiv.org/abs/2005.01802v1,2020-05-04T19:20:09Z,2020-05-04T19:20:09Z,Learning-based Tracking of Fast Moving Objects,"Tracking fast moving objects, which appear as blurred streaks in video
sequences, is a difficult task for standard trackers as the object position
does not overlap in consecutive video frames and texture information of the
objects is blurred. Up-to-date approaches tuned for this task are based on
background subtraction with static background and slow deblurring algorithms.
In this paper, we present a tracking-by-segmentation approach implemented using
state-of-the-art deep learning methods that performs near-realtime tracking on
real-world video sequences. We implemented a physically plausible FMO sequence
generator to be a robust foundation for our training pipeline and demonstrate
the ease of fast generator and network adaptation for different FMO scenarios
in terms of foreground variations.",arxiv
http://arxiv.org/abs/1901.10584v1,2019-01-29T22:07:41Z,2019-01-29T22:07:41Z,"Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A
  Co-Design Approach","Deep neural networks have seen tremendous success for different modalities of
data including images, videos, and speech. This success has led to their
deployment in mobile and embedded systems for real-time applications. However,
making repeated inferences using deep networks on embedded systems poses
significant challenges due to constrained resources (e.g., energy and computing
power). To address these challenges, we develop a principled co-design
approach. Building on prior work, we develop a formalism referred to as
Coarse-to-Fine Networks (C2F Nets) that allow us to employ classifiers of
varying complexity to make predictions. We propose a principled optimization
algorithm to automatically configure C2F Nets for a specified trade-off between
accuracy and energy consumption for inference. The key idea is to select a
classifier on-the-fly whose complexity is proportional to the hardness of the
input example: simple classifiers for easy inputs and complex classifiers for
hard inputs. We perform comprehensive experimental evaluation using four
different C2F Net architectures on multiple real-world image classification
tasks. Our results show that optimized C2F Net can reduce the Energy Delay
Product (EDP) by 27 to 60 percent with no loss in accuracy when compared to the
baseline solution, where all predictions are made using the most complex
classifier in C2F Net.",arxiv
http://arxiv.org/abs/1810.07167v1,2018-10-16T17:49:43Z,2018-10-16T17:49:43Z,"Composable Action-Conditioned Predictors: Flexible Off-Policy Learning
  for Robot Navigation","A general-purpose intelligent robot must be able to learn autonomously and be
able to accomplish multiple tasks in order to be deployed in the real world.
However, standard reinforcement learning approaches learn separate
task-specific policies and assume the reward function for each task is known a
priori. We propose a framework that learns event cues from off-policy data, and
can flexibly combine these event cues at test time to accomplish different
tasks. These event cue labels are not assumed to be known a priori, but are
instead labeled using learned models, such as computer vision detectors, and
then `backed up' in time using an action-conditioned predictive model. We show
that a simulated robotic car and a real-world RC car can gather data and train
fully autonomously without any human-provided labels beyond those needed to
train the detectors, and then at test-time be able to accomplish a variety of
different tasks. Videos of the experiments and code can be found at
https://github.com/gkahn13/CAPs",arxiv
http://arxiv.org/abs/2106.03167v2,2021-06-16T00:51:02Z,2021-06-06T16:16:08Z,"Mathematical Vocoder Algorithm : Modified Spectral Inversion for
  Efficient Neural Speech Synthesis","In this work, we propose a new mathematical vocoder algorithm(modified
spectral inversion) that generates a waveform from acoustic features without
phase estimation. The main benefit of using our proposed method is that it
excludes the training stage of the neural vocoder from the end-to-end speech
synthesis model. Our implementation can synthesize high fidelity speech at
approximately 20 Mhz on CPU and 59.6MHz on GPU. This is 909 and 2,702 times
faster compared to real-time. Since the proposed methodology is not a
data-driven method, it is applicable to unseen voices and multiple languages
without any additional work. The proposed method is expected to adapt for
researching on neural network models capable of synthesizing speech at the
studio recording level.",arxiv
http://arxiv.org/abs/1804.03547v2,2018-04-11T15:20:31Z,2018-04-10T14:07:45Z,"A real-time and unsupervised face Re-Identification system for
  Human-Robot Interaction","In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.",arxiv
http://arxiv.org/abs/2108.00516v1,2021-08-01T18:14:46Z,2021-08-01T18:14:46Z,"BundleTrack: 6D Pose Tracking for Novel Objects without Instance or
  Category-Level 3D Models","Tracking the 6D pose of objects in video sequences is important for robot
manipulation. Most prior efforts, however, often assume that the target
object's CAD model, at least at a category-level, is available for offline
training or during online template matching. This work proposes BundleTrack, a
general framework for 6D pose tracking of novel objects, which does not depend
upon 3D models, either at the instance or category-level. It leverages the
complementary attributes of recent advances in deep learning for segmentation
and robust feature extraction, as well as memory-augmented pose graph
optimization for spatiotemporal consistency. This enables long-term, low-drift
tracking under various challenging scenarios, including significant occlusions
and object motions. Comprehensive experiments given two public benchmarks
demonstrate that the proposed approach significantly outperforms state-of-art,
category-level 6D tracking or dynamic SLAM methods. When compared against
state-of-art methods that rely on an object instance CAD model, comparable
performance is achieved, despite the proposed method's reduced information
requirements. An efficient implementation in CUDA provides a real-time
performance of 10Hz for the entire framework. Code is available at:
https://github.com/wenbowen123/BundleTrack",arxiv
http://arxiv.org/abs/1910.13348v1,2019-10-29T16:07:02Z,2019-10-29T16:07:02Z,"Sequential image processing methods for improving semantic video
  segmentation algorithms","Recently, semantic video segmentation gained high attention especially for
supporting autonomous driving systems. Deep learning methods made it possible
to implement real time segmentation and object identification algorithms on
videos. However, most of the available approaches process each video frame
independently disregarding their sequential relation in time. Therefore their
results suddenly miss some of the object segments in some of the frames even if
they were detected properly in the earlier frames. Herein we propose two
sequential probabilistic video frame analysis approaches to improve the
segmentation performance of the existing algorithms. Our experiments show that
using the information of the past frames we increase the performance and
consistency of the state of the art algorithms.",arxiv
http://arxiv.org/abs/2005.12074v2,2020-06-08T14:58:07Z,2020-05-25T12:34:47Z,Egocentric Human Segmentation for Mixed Reality,"The objective of this work is to segment human body parts from egocentric
video using semantic segmentation networks. Our contribution is two-fold: i) we
create a semi-synthetic dataset composed of more than 15, 000 realistic images
and associated pixel-wise labels of egocentric human body parts, such as arms
or legs including different demographic factors; ii) building upon the
ThunderNet architecture, we implement a deep learning semantic segmentation
algorithm that is able to perform beyond real-time requirements (16 ms for 720
x 720 images). It is believed that this method will enhance sense of presence
of Virtual Environments and will constitute a more realistic solution to the
standard virtual avatars.",arxiv
http://arxiv.org/abs/2108.09491v1,2021-08-21T11:16:49Z,2021-08-21T11:16:49Z,"Flikcer -- A Chrome Extension to Resolve Online Epileptogenic Visual
  Content with Real-Time Luminance Frequency Analysis","Video content with fast luminance variations, or with spatial patterns of
high contrast - referred to as epileptogenic visual content - may induce
seizures on viewers with photosensitive epilepsy, and even cause discomfort in
users not affected by this disease. Flikcer is a web app in the form of a
website and chrome extension which aims to resolve epileptic content in videos.
It provides the number of possible triggers for a seizure. It also provides the
timestamps for these triggers along with a safer version of the video, free to
download. The algorithm is written in Python and uses machine learning and
computer vision. A key aspect of the algorithm is its computational efficiency,
allowing real time implementation for public users.",arxiv
http://arxiv.org/abs/1709.09312v1,2017-09-27T02:42:29Z,2017-09-27T02:42:29Z,"A Simple Reinforcement Learning Mechanism for Resource Allocation in
  LTE-A Networks with Markov Decision Process and Q-Learning","Resource allocation is still a difficult issue to deal with in wireless
networks. The unstable channel condition and traffic demand for Quality of
Service (QoS) raise some barriers that interfere with the process. It is
significant that an optimal policy takes into account some resources available
to each traffic class while considering the spectral efficiency and other
related channel issues. Reinforcement learning is a dynamic and effective
method to support the accomplishment of resource allocation properly
maintaining QoS levels for applications. The technique can track the system
state as feedback to enhance the performance of a given task. Herein, it is
proposed a simple reinforcement learning mechanism introduced in LTE-A networks
and aimed to choose and limit the number of resources allocated for each
traffic class, regarding the QoS Class Identifier (QCI), at each Transmission
Time Interval (TTI) along the scheduling procedure. The proposed mechanism
implements a Markov Decision Process (MDP) solved by the Q-Learning algorithm
to find an optimal action-state decision policy. The results obtained from
simulation exhibit good performance, especially for the real-time Video
application.",arxiv
http://arxiv.org/abs/2001.05097v1,2020-01-15T01:31:01Z,2020-01-15T01:31:01Z,"Lightweight 3D Human Pose Estimation Network Training Using
  Teacher-Student Learning","We present MoVNect, a lightweight deep neural network to capture 3D human
pose using a single RGB camera. To improve the overall performance of the
model, we apply the teacher-student learning method based knowledge
distillation to 3D human pose estimation. Real-time post-processing makes the
CNN output yield temporally stable 3D skeletal information, which can be used
in applications directly. We implement a 3D avatar application running on
mobile in real-time to demonstrate that our network achieves both high accuracy
and fast inference time. Extensive evaluations show the advantages of our
lightweight model with the proposed training method over previous 3D pose
estimation methods on the Human3.6M dataset and mobile devices.",arxiv
http://arxiv.org/abs/2010.08600v2,2020-11-16T06:26:16Z,2020-10-16T19:40:08Z,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning","Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",arxiv
http://arxiv.org/abs/1905.06809v1,2019-05-16T14:50:28Z,2019-05-16T14:50:28Z,Occupancy Estimation Using Low-Cost Wi-Fi Sniffers,"Real-time measurements on the occupancy status of indoor and outdoor spaces
can be exploited in many scenarios (HVAC and lighting system control, building
energy optimization, allocation and reservation of spaces, etc.). Traditional
systems for occupancy estimation rely on environmental sensors (CO2,
temperature, humidity) or video cameras. In this paper, we depart from such
traditional approaches and propose a novel occupancy estimation system which is
based on the capture of Wi-Fi management packets from users' devices. The
system, implemented on a low-cost ESP8266 microcontroller, leverages a
supervised learning model to adapt to different spaces and transmits occupancy
information through the MQTT protocol to a web-based dashboard. Experimental
results demonstrate the validity of the proposed solution in four different
indoor university spaces.",arxiv
http://arxiv.org/abs/1903.02219v1,2019-03-06T07:39:11Z,2019-03-06T07:39:11Z,Training in Task Space to Speed Up and Guide Reinforcement Learning,"Recent breakthroughs in the reinforcement learning (RL) community have made
significant advances towards learning and deploying policies on real world
robotic systems. However, even with the current state-of-the-art algorithms and
computational resources, these algorithms are still plagued with high sample
complexity, and thus long training times, especially for high degree of freedom
(DOF) systems. There are also concerns arising from lack of perceived stability
or robustness guarantees from emerging policies. This paper aims at mitigating
these drawbacks by: (1) modeling a complex, high DOF system with a
representative simple one, (2) making explicit use of forward and inverse
kinematics without forcing the RL algorithm to ""learn"" them on its own, and (3)
learning locomotion policies in Cartesian space instead of joint space. In this
paper these methods are applied to JPL's Robosimian, but can be readily used on
any system with a base and end effector(s). These locomotion policies can be
produced in just a few minutes, trained on a single laptop. We compare the
robustness of the resulting learned policies to those of other control methods.
An accompanying video for this paper can be found at
https://youtu.be/xDxxSw5ahnc .",arxiv
http://arxiv.org/abs/2003.02372v1,2020-03-04T23:46:45Z,2020-03-04T23:46:45Z,Dynamic Experience Replay,"We present a novel technique called Dynamic Experience Replay (DER) that
allows Reinforcement Learning (RL) algorithms to use experience replay samples
not only from human demonstrations but also successful transitions generated by
RL agents during training and therefore improve training efficiency. It can be
combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and
their distributed versions. We build upon Ape-X DDPG and demonstrate our
approach on robotic tight-fitting joint assembly tasks, based on force/torque
and Cartesian pose observations. In particular, we run experiments on two
different tasks: peg-in-hole and lap-joint. In each case, we compare different
replay buffer structures and how DER affects them. Our ablation studies show
that Dynamic Experience Replay is a crucial ingredient that either largely
shortens the training time in these challenging environments or solves the
tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies
learned purely in simulation can be deployed successfully on the real robot.
The video presenting our experiments is available at
https://sites.google.com/site/dynamicexperiencereplay",arxiv
http://arxiv.org/abs/2105.10389v3,2021-11-09T22:30:45Z,2021-05-21T15:03:29Z,Learning Visible Connectivity Dynamics for Cloth Smoothing,"Robotic manipulation of cloth remains challenging for robotics due to the
complex dynamics of the cloth, lack of a low-dimensional state representation,
and self-occlusions. In contrast to previous model-based approaches that learn
a pixel-based dynamics model or a compressed latent vector dynamics, we propose
to learn a particle-based dynamics model from a partial point cloud
observation. To overcome the challenges of partial observability, we infer
which visible points are connected on the underlying cloth mesh. We then learn
a dynamics model over this visible connectivity graph. Compared to previous
learning-based approaches, our model poses strong inductive bias with its
particle based representation for learning the underlying cloth physics; it is
invariant to visual features; and the predictions can be more easily
visualized. We show that our method greatly outperforms previous
state-of-the-art model-based and model-free reinforcement learning methods in
simulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we
deploy the model trained in simulation on a Franka arm and show that the model
can successfully smooth different types of cloth from crumpled configurations.
Videos can be found on our project website.",arxiv
http://arxiv.org/abs/2005.11724v1,2020-05-24T11:37:48Z,2020-05-24T11:37:48Z,"Learning to Transfer Graph Embeddings for Inductive Graph based
  Recommendation","With the increasing availability of videos, how to edit them and present the
most interesting parts to users, i.e., video highlight, has become an urgent
need with many broad applications. As users'visual preferences are subjective
and vary from person to person, previous generalized video highlight extraction
models fail to tailor to users' unique preferences. In this paper, we study the
problem of personalized video highlight recommendation with rich visual
content. By dividing each video into non-overlapping segments, we formulate the
problem as a personalized segment recommendation task with many new segments in
the test stage. The key challenges of this problem lie in: the cold-start users
with limited video highlight records in the training data and new segments
without any user ratings at the test stage. In this paper, we propose an
inductive Graph based Transfer learning framework for personalized video
highlight Recommendation (TransGRec). TransGRec is composed of two parts: a
graph neural network followed by an item embedding transfer network.
Specifically, the graph neural network part exploits the higher-order proximity
between users and segments to alleviate the user cold-start problem. The
transfer network is designed to approximate the learned item embeddings from
graph neural networks by taking each item's visual content as input, in order
to tackle the new segment problem in the test phase. We design two detailed
implementations of the transfer learning optimization function, and we show how
the two parts of TransGRec can be efficiently optimized with different transfer
learning optimization functions. Extensive experimental results on a real-world
dataset clearly show the effectiveness of our proposed model.",arxiv
http://arxiv.org/abs/1710.07563v1,2017-10-20T15:05:41Z,2017-10-20T15:05:41Z,SEGCloud: Semantic Segmentation of 3D Point Clouds,"3D semantic scene labeling is fundamental to agents operating in the real
world. In particular, labeling raw 3D point sets from sensors provides
fine-grained semantics. Recent works leverage the capabilities of Neural
Networks (NNs), but are limited to coarse voxel predictions and do not
explicitly enforce global consistency. We present SEGCloud, an end-to-end
framework to obtain 3D point-level segmentation that combines the advantages of
NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields
(FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are
transferred back to the raw 3D points via trilinear interpolation. Then the
FC-CRF enforces global consistency and provides fine-grained semantics on the
points. We implement the latter as a differentiable Recurrent NN to allow joint
optimization. We evaluate the framework on two indoor and two outdoor 3D
datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance
comparable or superior to the state-of-the-art on all datasets.",arxiv
http://arxiv.org/abs/2107.11750v2,2021-07-30T08:42:18Z,2021-07-25T07:52:53Z,"Improving Variational Autoencoder based Out-of-Distribution Detection
  for Embedded Real-time Applications","Uncertainties in machine learning are a significant roadblock for its
application in safety-critical cyber-physical systems (CPS). One source of
uncertainty arises from distribution shifts in the input data between training
and test scenarios. Detecting such distribution shifts in real-time is an
emerging approach to address the challenge. The high dimensional input space in
CPS applications involving imaging adds extra difficulty to the task.
Generative learning models are widely adopted for the task, namely
out-of-distribution (OoD) detection. To improve the state-of-the-art, we
studied existing proposals from both machine learning and CPS fields. In the
latter, safety monitoring in real-time for autonomous driving agents has been a
focus. Exploiting the spatiotemporal correlation of motion in videos, we can
robustly detect hazardous motion around autonomous driving agents. Inspired by
the latest advances in the Variational Autoencoder (VAE) theory and practice,
we tapped into the prior knowledge in data to further boost OoD detection's
robustness. Comparison studies over nuScenes and Synthia data sets show our
methods significantly improve detection capabilities of OoD factors unique to
driving scenarios, 42% better than state-of-the-art approaches. Our model also
generalized near-perfectly, 97% better than the state-of-the-art across the
real-world and simulation driving data sets experimented. Finally, we
customized one proposed method into a twin-encoder model that can be deployed
to resource limited embedded devices for real-time OoD detection. Its execution
time was reduced over four times in low-precision 8-bit integer inference,
while detection capability is comparable to its corresponding floating-point
model.",arxiv
http://arxiv.org/abs/2103.14749v4,2021-11-07T13:04:04Z,2021-03-26T21:54:36Z,"Pervasive Label Errors in Test Sets Destabilize Machine Learning
  Benchmarks","We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
https://labelerrors.com and all label errors can be reproduced by
https://github.com/cleanlab/label-errors.",arxiv
http://arxiv.org/abs/1908.06619v1,2019-08-19T07:21:37Z,2019-08-19T07:21:37Z,"Demonstration of 3D ISAR Security Imaging at 24GHz with a Sparse MIMO
  Array","A 3D ISAR security imaging experiment at 24GHz is demonstrated with a sparse
MIMO array. The MIMO array is an 8Tx/16Rx linear array to achieve real-aperture
imaging along the vertical dimension. It is time-switching multiplexed with a
low-cost FMCW transceiver working at 22GHz-26GHz. A calibration procedure is
proposed to calibrate the channel imbalance across the MIMO array. The
experiment is conducted on human moving on a cart, where we take advantage of
the linear motion of human to form inverse synthetic aperture along the
horizontal dimension. To track the motion of human, a 3D depth camera is used
as an auxiliary sensor to capture the rough position of target to aid ISAR
imaging. The back projection imaging algorithm is implemented on GPU for
quasi-real-time operation. Finally, experiments are conducted with real human
with concealed objects and a preliminary automatic object recognition algorithm
based on convolutional neural networks are developed and evaluated on real
data.",arxiv
http://arxiv.org/abs/2101.10955v2,2021-11-14T17:09:25Z,2021-01-26T17:23:46Z,"RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated
  Content","Blind or no-reference video quality assessment of user-generated content
(UGC) has become a trending, challenging, heretofore unsolved problem. Accurate
and efficient video quality predictors suitable for this content are thus in
great demand to achieve more intelligent analysis and processing of UGC videos.
Previous studies have shown that natural scene statistics and deep learning
features are both sufficient to capture spatial distortions, which contribute
to a significant aspect of UGC video quality issues. However, these models are
either incapable or inefficient for predicting the quality of complex and
diverse UGC videos in practical applications. Here we introduce an effective
and efficient video quality model for UGC content, which we dub the Rapid and
Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably
to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.
RAPIQUE combines and leverages the advantages of both quality-aware scene
statistics features and semantics-aware deep convolutional features, allowing
us to design the first general and efficient spatial and temporal (space-time)
bandpass statistics model for video quality modeling. Our experimental results
on recent large-scale UGC video quality databases show that RAPIQUE delivers
top performances on all the datasets at a considerably lower computational
expense. We hope this work promotes and inspires further efforts towards
practical modeling of video quality problems for potential real-time and
low-latency applications. To promote public usage, an implementation of RAPIQUE
has been made freely available online: \url{https://github.com/vztu/RAPIQUE}.",arxiv
http://arxiv.org/abs/2105.00187v1,2021-05-01T08:02:59Z,2021-05-01T08:02:59Z,"One Detector to Rule Them All: Towards a General Deepfake Attack
  Detection Framework","Deep learning-based video manipulation methods have become widely accessible
to the masses. With little to no effort, people can quickly learn how to
generate deepfake (DF) videos. While deep learning-based detection methods have
been proposed to identify specific types of DFs, their performance suffers for
other types of deepfake methods, including real-world deepfakes, on which they
are not sufficiently trained. In other words, most of the proposed deep
learning-based detection methods lack transferability and generalizability.
Beyond detecting a single type of DF from benchmark deepfake datasets, we focus
on developing a generalized approach to detect multiple types of DFs, including
deepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW)
videos. To better cope with unknown and unseen deepfakes, we introduce a
Convolutional LSTM-based Residual Network (CLRNet), which adopts a unique model
training strategy and explores spatial as well as the temporal information in
deepfakes. Through extensive experiments, we show that existing defense methods
are not ready for real-world deployment. Whereas our defense method (CLRNet)
achieves far better generalization when detecting various benchmark deepfake
methods (97.57% on average). Furthermore, we evaluate our approach with a
high-quality DeepFake-in-the-Wild dataset, collected from the Internet
containing numerous videos and having more than 150,000 frames. Our CLRNet
model demonstrated that it generalizes well against high-quality DFW videos by
achieving 93.86% detection accuracy, outperforming existing state-of-the-art
defense methods by a considerable margin.",arxiv
http://arxiv.org/abs/2002.05123v4,2021-06-04T22:11:54Z,2020-02-12T17:58:12Z,"Over-the-Air Adversarial Flickering Attacks against Video Recognition
  Networks","Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.",arxiv
http://arxiv.org/abs/1912.07906v4,2020-04-28T23:24:12Z,2019-12-17T09:56:15Z,"Deep SCNN-based Real-time Object Detection for Self-driving Vehicles
  Using LiDAR Temporal Data","Real-time accurate detection of three-dimensional (3D) objects is a
fundamental necessity for self-driving vehicles. Most existing computer vision
approaches are based on convolutional neural networks (CNNs). Although the
CNN-based approaches can achieve high detection accuracy, their high energy
consumption is a severe drawback. To resolve this problem, novel energy
efficient approaches should be explored. Spiking neural network (SNN) is a
promising candidate because it has orders-of-magnitude lower energy consumption
than CNN. Unfortunately, the studying of SNN has been limited in small networks
only. The application of SNN for large 3D object detection networks has remain
largely open. In this paper, we integrate spiking convolutional neural network
(SCNN) with temporal coding into the YOLOv2 architecture for real-time object
detection. To take the advantage of spiking signals, we develop a novel data
preprocessing layer that translates 3D point-cloud data into spike time data.
We propose an analog circuit to implement the non-leaky integrate and fire
neuron used in our SCNN, from which the energy consumption of each spike is
estimated. Moreover, we present a method to calculate the network sparsity and
the energy consumption of the overall network. Extensive experiments have been
conducted based on the KITTI dataset, which show that the proposed network can
reach competitive detection accuracy as existing approaches, yet with much
lower average energy consumption. If implemented in dedicated hardware, our
network could have a mean sparsity of 56.24% and extremely low total energy
consumption of 0.247mJ only. Implemented in NVIDIA GTX 1080i GPU, we can
achieve 35.7 fps frame rate, high enough for real-time object detection.",arxiv
http://arxiv.org/abs/2012.02417v2,2020-12-07T02:34:05Z,2020-12-04T06:02:26Z,"Autonomous Navigation with Mobile Robots using Deep Learning and the
  Robot Operating System","Autonomous navigation is a long-standing field of robotics research, which
provides an essential capability for mobile robots to execute a series of tasks
on the same environments performed by human everyday. In this chapter, we
present a set of algorithms to train and deploy deep networks for autonomous
navigation of mobile robots using the Robot Operation System (ROS). We describe
three main steps to tackle this problem: i) collecting data in simulation
environments using ROS and Gazebo; ii) designing deep network for autonomous
navigation, and iii) deploying the learned policy on mobile robots in both
simulation and real-world. Theoretically, we present deep learning
architectures for robust navigation in normal environments (e.g., man-made
houses, roads) and complex environments (e.g., collapsed cities, or natural
caves). We further show that the use of visual modalities such as RGB, Lidar,
and point cloud is essential to improve the autonomy of mobile robots. Our
project website and demonstration video can be found at
https://sites.google.com/site/autonomousnavigationros.",arxiv
http://arxiv.org/abs/1611.08930v2,2017-03-28T03:15:07Z,2016-11-27T22:47:23Z,Deep attractor network for single-microphone speaker separation,"Despite the overwhelming success of deep learning in various speech
processing tasks, the problem of separating simultaneous speakers in a mixture
remains challenging. Two major difficulties in such systems are the arbitrary
source permutation and unknown number of sources in the mixture. We propose a
novel deep learning framework for single channel speech separation by creating
attractor points in high dimensional embedding space of the acoustic signals
which pull together the time-frequency bins corresponding to each source.
Attractor points in this study are created by finding the centroids of the
sources in the embedding space, which are subsequently used to determine the
similarity of each bin in the mixture to each source. The network is then
trained to minimize the reconstruction error of each source by optimizing the
embeddings. The proposed model is different from prior works in that it
implements an end-to-end training, and it does not depend on the number of
sources in the mixture. Two strategies are explored in the test time, K-means
and fixed attractor points, where the latter requires no post-processing and
can be implemented in real-time. We evaluated our system on Wall Street Journal
dataset and show 5.49\% improvement over the previous state-of-the-art methods.",arxiv
http://arxiv.org/abs/2104.11008v1,2021-04-22T12:10:38Z,2021-04-22T12:10:38Z,"Unsupervised anomaly detection for a Smart Autonomous Robotic Assistant
  Surgeon (SARAS)using a deep residual autoencoder","Anomaly detection in Minimally-Invasive Surgery (MIS) traditionally requires
a human expert monitoring the procedure from a console. Data scarcity, on the
other hand, hinders what would be a desirable migration towards autonomous
robotic-assisted surgical systems. Automated anomaly detection systems in this
area typically rely on classical supervised learning. Anomalous events in a
surgical setting, however, are rare, making it difficult to capture data to
train a detection model in a supervised fashion. In this work we thus propose
an unsupervised approach to anomaly detection for robotic-assisted surgery
based on deep residual autoencoders. The idea is to make the autoencoder learn
the 'normal' distribution of the data and detect abnormal events deviating from
this distribution by measuring the reconstruction error. The model is trained
and validated upon both the publicly available Cholec80 dataset, provided with
extra annotation, and on a set of videos captured on procedures using
artificial anatomies ('phantoms') produced as part of the Smart Autonomous
Robotic Assistant Surgeon (SARAS) project. The system achieves recall and
precision equal to 78.4%, 91.5%, respectively, on Cholec80 and of 95.6%, 88.1%
on the SARAS phantom dataset. The end-to-end system was developed and deployed
as part of the SARAS demonstration platform for real-time anomaly detection
with a processing time of about 25 ms per frame.",arxiv
http://arxiv.org/abs/1903.10883v1,2019-03-25T15:40:34Z,2019-03-25T15:40:34Z,Generalized Feedback Loop for Joint Hand-Object Pose Estimation,"We propose an approach to estimating the 3D pose of a hand, possibly handling
an object, given a depth image. We show that we can correct the mistakes made
by a Convolutional Neural Network trained to predict an estimate of the 3D pose
by using a feedback loop. The components of this feedback loop are also Deep
Networks, optimized using training data. This approach can be generalized to a
hand interacting with an object. Therefore, we jointly estimate the 3D pose of
the hand and the 3D pose of the object. Our approach performs en-par with
state-of-the-art methods for 3D hand pose estimation, and outperforms
state-of-the-art methods for joint hand-object pose estimation when using depth
images only. Also, our approach is efficient as our implementation runs in
real-time on a single GPU.",arxiv
http://arxiv.org/abs/2107.12137v2,2021-08-11T06:54:54Z,2021-07-26T12:18:23Z,AA3DNet: Attention Augmented Real Time 3D Object Detection,"In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects using
point cloud data. We present anchor design along with custom loss functions
used in this work. A combination of spatial and channel wise attention module
is used in this work. We use the Kitti 3D Birds Eye View dataset for
benchmarking and validating our results. Our method surpasses previous state of
the art in this domain both in terms of average precision and speed running at
> 30 FPS. Finally, we present the ablation study to demonstrate that the
performance of our network is generalizable. This makes it a feasible option to
be deployed in real time applications like self driving cars.",arxiv
http://arxiv.org/abs/2002.04597v1,2020-02-11T18:41:57Z,2020-02-11T18:41:57Z,WatchDog: Real-time Vehicle Tracking on Geo-distributed Edge Nodes,"Vehicle tracking, a core application to smart city video analytics, is
becoming more widely deployed than ever before thanks to the increasing number
of traffic cameras and recent advances of computer vision and machine learning.
Due to the constraints of bandwidth, latency, and privacy concerns, tracking
tasks are more preferable to run on edge devices sitting close to the cameras.
However, edge devices are provisioned with a fixed amount of compute budget,
making them incompetent to adapt to time-varying tracking workloads caused by
traffic dynamics. In coping with this challenge, we propose WatchDog, a
real-time vehicle tracking system fully utilizes edge nodes across the road
network. WatchDog leverages computer vision tasks with different
resource-accuracy trade-offs, and decompose and schedule tracking tasks
judiciously across edge devices based on the current workload to maximize the
number of tasks while ensuring a provable response time bound at each edge
device. Extensive evaluations have been conducted using real-world city-wide
vehicle trajectory datasets, showing a 100% tracking coverage with real-time
guarantee.",arxiv
http://arxiv.org/abs/1602.01208v3,2016-05-07T11:59:51Z,2016-02-03T06:56:51Z,"Spatial Concept Acquisition for a Mobile Robot that Integrates
  Self-Localization and Unsupervised Word Discovery from Spoken Sentences","In this paper, we propose a novel unsupervised learning method for the
lexical acquisition of words related to places visited by robots, from human
continuous speech signals. We address the problem of learning novel words by a
robot that has no prior knowledge of these words except for a primitive
acoustic model. Further, we propose a method that allows a robot to effectively
use the learned words and their meanings for self-localization tasks. The
proposed method is nonparametric Bayesian spatial concept acquisition method
(SpCoA) that integrates the generative model for self-localization and the
unsupervised word segmentation in uttered sentences via latent variables
related to the spatial concept. We implemented the proposed method SpCoA on
SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile
robot in a real environment. Further, we conducted experiments for evaluating
the performance of SpCoA. The experimental results showed that SpCoA enabled
the robot to acquire the names of places from speech sentences. They also
revealed that the robot could effectively utilize the acquired spatial concepts
and reduce the uncertainty in self-localization.",arxiv
http://arxiv.org/abs/1507.05695v1,2015-07-21T03:48:04Z,2015-07-21T03:48:04Z,"A neuromorphic hardware architecture using the Neural Engineering
  Framework for pattern recognition","We present a hardware architecture that uses the Neural Engineering Framework
(NEF) to implement large-scale neural networks on Field Programmable Gate
Arrays (FPGAs) for performing pattern recognition in real time. NEF is a
framework that is capable of synthesising large-scale cognitive systems from
subnetworks. We will first present the architecture of the proposed neural
network implemented using fixed-point numbers and demonstrate a routine that
computes the decoding weights by using the online pseudoinverse update method
(OPIUM) in a parallel and distributed manner. The proposed system is
efficiently implemented on a compact digital neural core. This neural core
consists of 64 neurons that are instantiated by a single physical neuron using
a time-multiplexing approach. As a proof of concept, we combined 128 identical
neural cores together to build a handwritten digit recognition system using the
MNIST database and achieved a recognition rate of 96.55%. The system is
implemented on a state-of-the-art FPGA and can process 5.12 million digits per
second. The architecture is not limited to handwriting recognition, but is
generally applicable as an extremely fast pattern recognition processor for
various kinds of patterns such as speech and images.",arxiv
http://arxiv.org/abs/2104.06535v1,2021-04-13T22:34:33Z,2021-04-13T22:34:33Z,NPE: An FPGA-based Overlay Processor for Natural Language Processing,"In recent years, transformer-based models have shown state-of-the-art results
for Natural Language Processing (NLP). In particular, the introduction of the
BERT language model brought with it breakthroughs in tasks such as question
answering and natural language inference, advancing applications that allow
humans to interact naturally with embedded devices. FPGA-based overlay
processors have been shown as effective solutions for edge image and video
processing applications, which mostly rely on low precision linear matrix
operations. In contrast, transformer-based NLP techniques employ a variety of
higher precision nonlinear operations with significantly higher frequency. We
present NPE, an FPGA-based overlay processor that can efficiently execute a
variety of NLP models. NPE offers software-like programmability to the end user
and, unlike FPGA designs that implement specialized accelerators for each
nonlinear function, can be upgraded for future NLP models without requiring
reconfiguration. We demonstrate that NPE can meet real-time conversational AI
latency targets for the BERT language model with $4\times$ lower power than
CPUs and $6\times$ lower power than GPUs. We also show NPE uses $3\times$ fewer
FPGA resources relative to comparable BERT network-specific accelerators in the
literature. NPE provides a cost-effective and power-efficient FPGA-based
solution for Natural Language Processing at the edge.",arxiv
http://arxiv.org/abs/2010.15824v2,2020-11-03T16:19:59Z,2020-10-29T17:57:12Z,Passport-aware Normalization for Deep Model Protection,"Despite tremendous success in many application scenarios, deep learning faces
serious intellectual property (IP) infringement threats. Considering the cost
of designing and training a good model, infringements will significantly
infringe the interests of the original model owner. Recently, many impressive
works have emerged for deep model IP protection. However, they either are
vulnerable to ambiguity attacks, or require changes in the target network
structure by replacing its original normalization layers and hence cause
significant performance drops. To this end, we propose a new passport-aware
normalization formulation, which is generally applicable to most existing
normalization layers and only needs to add another passport-aware branch for IP
protection. This new branch is jointly trained with the target model but
discarded in the inference stage. Therefore it causes no structure change in
the target model. Only when the model IP is suspected to be stolen by someone,
the private passport-aware branch is added back for ownership verification.
Through extensive experiments, we verify its effectiveness in both image and 3D
point recognition models. It is demonstrated to be robust not only to common
attack techniques like fine-tuning and model compression, but also to ambiguity
attacks. By further combining it with trigger-set based methods, both black-box
and white-box verification can be achieved for enhanced security of deep
learning models deployed in real systems. Code can be found at
https://github.com/ZJZAC/Passport-aware-Normalization.",arxiv
http://arxiv.org/abs/1910.09667v1,2019-10-21T21:44:15Z,2019-10-21T21:44:15Z,"Combining Benefits from Trajectory Optimization and Deep Reinforcement
  Learning","Recent breakthroughs both in reinforcement learning and trajectory
optimization have made significant advances towards real world robotic system
deployment. Reinforcement learning (RL) can be applied to many problems without
needing any modeling or intuition about the system, at the cost of high sample
complexity and the inability to prove any metrics about the learned policies.
Trajectory optimization (TO) on the other hand allows for stability and
robustness analyses on generated motions and trajectories, but is only as good
as the often over-simplified derived model, and may have prohibitively
expensive computation times for real-time control. This paper seeks to combine
the benefits from these two areas while mitigating their drawbacks by (1)
decreasing RL sample complexity by using existing knowledge of the problem with
optimal control, and (2) providing an upper bound estimate on the
time-to-arrival of the combined learned-optimized policy, allowing online
policy deployment at any point in the training process by using the TO as a
worst-case scenario action. This method is evaluated for a car model, with
applicability to any mobile robotic system. A video showing policy execution
comparisons can be found at https://youtu.be/mv2xw83NyWU .",arxiv
http://arxiv.org/abs/1808.03841v1,2018-08-11T17:33:40Z,2018-08-11T17:33:40Z,"Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement
  Learning for Safe and Efficient Navigation in Complex Scenarios","In this paper, we present a decentralized sensor-level collision avoidance
policy for multi-robot systems, which shows promising results in practical
applications. In particular, our policy directly maps raw sensor measurements
to an agent's steering commands in terms of the movement velocity. As a first
step toward reducing the performance gap between decentralized and centralized
methods, we present a multi-scenario multi-stage training framework to learn an
optimal policy. The policy is trained over a large number of robots in rich,
complex environments simultaneously using a policy gradient based reinforcement
learning algorithm. The learning algorithm is also integrated into a hybrid
control framework to further improve the policy's robustness and effectiveness.
  We validate the learned sensor-level collision avoidance policy in a variety
of simulated and real-world scenarios with thorough performance evaluations for
large-scale multi-robot systems. The generalization of the learned policy is
verified in a set of unseen scenarios including the navigation of a group of
heterogeneous robots and a large-scale scenario with 100 robots. Although the
policy is trained using simulation data only, we have successfully deployed it
on physical robots with shapes and dynamics characteristics that are different
from the simulated agents, in order to demonstrate the controller's robustness
against the sim-to-real modeling error. Finally, we show that the
collision-avoidance policy learned from multi-robot navigation tasks provides
an excellent solution to the safe and effective autonomous navigation for a
single robot working in a dense real human crowd. Our learned policy enables a
robot to make effective progress in a crowd without getting stuck. Videos are
available at https://sites.google.com/view/hybridmrca",arxiv
http://arxiv.org/abs/2008.00706v1,2020-08-03T08:19:57Z,2020-08-03T08:19:57Z,LiDAR point-cloud processing based on projection methods: a comparison,"An accurate and rapid-response perception system is fundamental for
autonomous vehicles to operate safely. 3D object detection methods handle point
clouds given by LiDAR sensors to provide accurate depth and position
information for each detection, together with its dimensions and
classification. The information is then used to track vehicles and other
obstacles in the surroundings of the autonomous vehicle, and also to feed
control units that guarantee collision avoidance and motion planning. Nowadays,
object detection systems can be divided into two main categories. The first
ones are the geometric based, which retrieve the obstacles using geometric and
morphological operations on the 3D points. The seconds are the deep
learning-based, which process the 3D points, or an elaboration of the 3D
point-cloud, with deep learning techniques to retrieve a set of obstacles. This
paper presents a comparison between those two approaches, presenting one
implementation of each class on a real autonomous vehicle. Accuracy of the
estimates of the algorithms has been evaluated with experimental tests carried
in the Monza ENI circuit. The position of the ego vehicle and the obstacle is
given by GPS sensors with RTK correction, which guarantees an accurate ground
truth for the comparison. Both algorithms have been implemented on ROS and run
on a consumer laptop.",arxiv
http://arxiv.org/abs/2111.09412v1,2021-10-03T14:03:22Z,2021-10-03T14:03:22Z,"Meta-Reinforcement Learning via Buffering Graph Signatures for Live
  Video Streaming Events","In this study, we present a meta-learning model to adapt the predictions of
the network's capacity between viewers who participate in a live video
streaming event. We propose the MELANIE model, where an event is formulated as
a Markov Decision Process, performing meta-learning on reinforcement learning
tasks. By considering a new event as a task, we design an actor-critic learning
scheme to compute the optimal policy on estimating the viewers' high-bandwidth
connections. To ensure fast adaptation to new connections or changes among
viewers during an event, we implement a prioritized replay memory buffer based
on the Kullback-Leibler divergence of the reward/throughput of the viewers'
connections. Moreover, we adopt a model-agnostic meta-learning framework to
generate a global model from past events. As viewers scarcely participate in
several events, the challenge resides on how to account for the low structural
similarity of different events. To combat this issue, we design a graph
signature buffer to calculate the structural similarities of several streaming
events and adjust the training of the global model accordingly. We evaluate the
proposed model on the link weight prediction task on three real-world datasets
of live video streaming events. Our experiments demonstrate the effectiveness
of our proposed model, with an average relative gain of 25% against
state-of-the-art strategies. For reproduction purposes, our evaluation datasets
and implementation are publicly available at
https://github.com/stefanosantaris/melanie",arxiv
http://arxiv.org/abs/1908.02999v1,2019-08-08T10:19:48Z,2019-08-08T10:19:48Z,Learning Vision-based Flight in Drone Swarms by Imitation,"Decentralized drone swarms deployed today either rely on sharing of positions
among agents or detecting swarm members with the help of visual markers. This
work proposes an entirely visual approach to coordinate markerless drone swarms
based on imitation learning. Each agent is controlled by a small and efficient
convolutional neural network that takes raw omnidirectional images as inputs
and predicts 3D velocity commands that match those computed by a flocking
algorithm. We start training in simulation and propose a simple yet effective
unsupervised domain adaptation approach to transfer the learned controller to
the real world. We further train the controller with data collected in our
motion capture hall. We show that the convolutional neural network trained on
the visual inputs of the drone can learn not only robust inter-agent collision
avoidance but also cohesion of the swarm in a sample-efficient manner. The
neural controller effectively learns to localize other agents in the visual
input, which we show by visualizing the regions with the most influence on the
motion of an agent. We remove the dependence on sharing positions among swarm
members by taking only local visual information into account for control. Our
work can therefore be seen as the first step towards a fully decentralized,
vision-based swarm without the need for communication or visual markers.",arxiv
http://arxiv.org/abs/2011.02838v1,2020-10-11T15:04:34Z,2020-10-11T15:04:34Z,"Real-time parameter inference in reduced-order flame models with
  heteroscedastic Bayesian neural network ensembles","The estimation of model parameters with uncertainties from observed data is a
ubiquitous inverse problem in science and engineering. In this paper, we
suggest an inexpensive and easy to implement parameter estimation technique
that uses a heteroscedastic Bayesian Neural Network trained using anchored
ensembling. The heteroscedastic aleatoric error of the network models the
irreducible uncertainty due to parameter degeneracies in our inverse problem,
while the epistemic uncertainty of the Bayesian model captures uncertainties
which may arise from an input observation's out-of-distribution nature. We use
this tool to perform real-time parameter inference in a 6 parameter G-equation
model of a ducted, premixed flame from observations of acoustically excited
flames. We train our networks on a library of 2.1 million simulated flame
videos. Results on the test dataset of simulated flames show that the network
recovers flame model parameters, with the correlation coefficient between
predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated
uncertainty estimates. The trained neural networks are then used to infer model
parameters from real videos of a premixed Bunsen flame captured using a
high-speed camera in our lab. Re-simulation using inferred parameters shows
excellent agreement between the real and simulated flames. Compared to Ensemble
Kalman Filter-based tools that have been proposed for this problem in the
combustion literature, our neural network ensemble achieves better
data-efficiency and our sub-millisecond inference times represent a savings on
computational costs by several orders of magnitude. This allows us to calibrate
our reduced-order flame model in real-time and predict the thermoacoustic
instability behaviour of the flame more accurately.",arxiv
http://arxiv.org/abs/1903.02550v1,2019-03-06T00:27:08Z,2019-03-06T00:27:08Z,"Towards a Uniform Architecture for the Efficient Implementation of 2D
  and 3D Deconvolutional Neural Networks on FPGAs","Three-dimensional deconvolution is widely used in many computer vision
applications. However, most previous works have only focused on accelerating 2D
deconvolutional neural networks (DCNNs) on FPGAs, while the acceleration of 3D
DCNNs has not been studied in depth as they have higher computational
complexity and sparsity than 2D DCNNs. In this paper, we focus on the
acceleration of both 2D and 3D DCNNs on FPGAs by proposing efficient schemes
for mapping 2D and 3D DCNNs on a uniform architecture. By implementing our
design on the Xilinx VC709 platform for four real-life 2D and 3D DCNNs, we can
achieve up to 3.0 TOPS with high hardware efficiency. Comparisons with CPU and
GPU solutions demonstrate that we can achieve an improvement of up to 63.3X in
throughput relative to a CPU solution and an improvement of up to 8.3X in
energy efficiency compared to a GPU solution.",arxiv
http://arxiv.org/abs/2001.01026v2,2020-04-25T22:20:46Z,2020-01-04T03:12:38Z,Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings,"We introduce a new video synthesis task: synthesizing time lapse videos
depicting how a given painting might have been created. Artists paint using
unique combinations of brushes, strokes, and colors. There are often many
possible ways to create a given painting. Our goal is to learn to capture this
rich range of possibilities.
  Creating distributions of long-term videos is a challenge for learning-based
video synthesis methods. We present a probabilistic model that, given a single
image of a completed painting, recurrently synthesizes steps of the painting
process. We implement this model as a convolutional neural network, and
introduce a novel training scheme to enable learning from a limited dataset of
painting time lapses. We demonstrate that this model can be used to sample many
time steps, enabling long-term stochastic video synthesis. We evaluate our
method on digital and watercolor paintings collected from video websites, and
show that human raters find our synthetic videos to be similar to time lapse
videos produced by real artists. Our code is available at
https://xamyzhao.github.io/timecraft.",arxiv
http://arxiv.org/abs/1906.04232v1,2019-06-10T19:04:09Z,2019-06-10T19:04:09Z,"BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour
  Extraction","Ultrasound imaging is safe, relatively affordable, and capable of real-time
performance. One application of this technology is to visualize and to
characterize human tongue shape and motion during a real-time speech to study
healthy or impaired speech production. Due to the noisy nature of ultrasound
images with low-contrast characteristic, it might require expertise for
non-expert users to recognize organ shape such as tongue surface (dorsum). To
alleviate this difficulty for quantitative analysis of tongue shape and motion,
tongue surface can be extracted, tracked, and visualized instead of the whole
tongue region. Delineating the tongue surface from each frame is a cumbersome,
subjective, and error-prone task. Furthermore, the rapidity and complexity of
tongue gestures have made it a challenging task, and manual segmentation is not
a feasible solution for real-time applications. Employing the power of
state-of-the-art deep neural network models and training techniques, it is
feasible to implement new fully-automatic, accurate, and robust segmentation
methods with the capability of real-time performance, applicable for tracking
of the tongue contours during the speech. This paper presents two novel deep
neural network models named BowNet and wBowNet benefits from the ability of
global prediction of decoding-encoding models, with integrated multi-scale
contextual information, and capability of full-resolution (local) extraction of
dilated convolutions. Experimental results using several ultrasound tongue
image datasets revealed that the combination of both localization and
globalization searching could improve prediction result significantly.
Assessment of BowNet models using both qualitatively and quantitatively studies
showed them outstanding achievements in terms of accuracy and robustness in
comparison with similar techniques.",arxiv
http://arxiv.org/abs/2106.10305v1,2021-06-18T18:30:22Z,2021-06-18T18:30:22Z,"Multi-Task Learning for User Engagement and Adoption in Live Video
  Streaming Events","Nowadays, live video streaming events have become a mainstay in viewer's
communication in large international enterprises. Provided that viewers are
distributed worldwide, the main challenge resides on how to schedule the
optimal event's time so as to improve both the viewer's engagement and
adoption. In this paper we present a multi-task deep reinforcement learning
model to select the time of a live video streaming event, aiming to optimize
the viewer's engagement and adoption at the same time. We consider the
engagement and adoption of the viewers as independent tasks and formulate a
unified loss function to learn a common policy. In addition, we account for the
fact that each task might have different contribution to the training strategy
of the agent. Therefore, to determine the contribution of each task to the
agent's training, we design a Transformer's architecture for the state-action
transitions of each task. We evaluate our proposed model on four real-world
datasets, generated by the live video streaming events of four large
enterprises spanning from January 2019 until March 2021. Our experiments
demonstrate the effectiveness of the proposed model when compared with several
state-of-the-art strategies. For reproduction purposes, our evaluation datasets
and implementation are publicly available at
https://github.com/stefanosantaris/merlin.",arxiv
http://arxiv.org/abs/2102.05334v2,2021-09-02T07:50:28Z,2021-02-10T09:16:09Z,"Enhancing Real-World Adversarial Patches through 3D Modeling of Complex
  Target Scenes","Adversarial examples have proven to be a concerning threat to deep learning
models, particularly in the image domain. However, while many studies have
examined adversarial examples in the real world, most of them relied on 2D
photos of the attack scene. As a result, the attacks proposed may have limited
effectiveness when implemented in realistic environments with 3D objects or
varied conditions. There are few studies on adversarial learning that use 3D
objects, and in many cases, other researchers are unable to replicate the
real-world evaluation process. In this study, we present a framework that uses
3D modeling to craft adversarial patches for an existing real-world scene. Our
approach uses a 3D digital approximation of the scene as a simulation of the
real world. With the ability to add and manipulate any element in the digital
scene, our framework enables the attacker to improve the adversarial patch's
impact in real-world settings. We use the framework to create a patch for an
everyday scene and evaluate its performance using a novel evaluation process
that ensures that our results are reproducible in both the digital space and
the real world. Our evaluation results show that the framework can generate
adversarial patches that are robust to different settings in the real world.",arxiv
http://arxiv.org/abs/2005.07115v6,2021-06-29T15:20:19Z,2020-05-14T16:33:13Z,CoSimGNN: Towards Large-scale Graph Similarity Computation,"The ability to compute similarity scores between graphs based on metrics such
as Graph Edit Distance (GED) is important in many real-world applications, such
as 3D action recognition and biological molecular identification. Computing
exact GED values is typically an NP-hard problem and traditional algorithms
usually achieve an unsatisfactory trade-off between accuracy and efficiency.
Recently, Graph Neural Networks (GNNs) provide a data-driven solution for this
task, which is more efficient while maintaining prediction accuracy in small
graph (around 10 nodes per graph) similarity computation. Existing GNN-based
methods, which either respectively embed two graphs (lack of low-level
cross-graph interactions) or deploy cross-graph interactions for whole graph
pairs (redundant and time-consuming), are still not able to achieve competitive
results when the number of nodes in graphs increases. In this paper, we focus
on similarity computation for large-scale graphs and propose the
""embedding-coarsening-matching"" framework, which first embeds and coarsens
large graphs to coarsened graphs with denser local topology and then deploys
fine-grained interactions on the coarsened graphs for the final similarity
scores.",arxiv
http://arxiv.org/abs/2105.10707v1,2021-05-22T12:19:03Z,2021-05-22T12:19:03Z,"Adversarial Attacks and Mitigation for Anomaly Detectors of
  Cyber-Physical Systems","The threats faced by cyber-physical systems (CPSs) in critical infrastructure
have motivated research into a multitude of attack detection mechanisms,
including anomaly detectors based on neural network models. The effectiveness
of anomaly detectors can be assessed by subjecting them to test suites of
attacks, but less consideration has been given to adversarial attackers that
craft noise specifically designed to deceive them. While successfully applied
in domains such as images and audio, adversarial attacks are much harder to
implement in CPSs due to the presence of other built-in defence mechanisms such
as rule checkers(or invariant checkers). In this work, we present an
adversarial attack that simultaneously evades the anomaly detectors and rule
checkers of a CPS. Inspired by existing gradient-based approaches, our
adversarial attack crafts noise over the sensor and actuator values, then uses
a genetic algorithm to optimise the latter, ensuring that the neural network
and the rule checking system are both deceived.We implemented our approach for
two real-world critical infrastructure testbeds, successfully reducing the
classification accuracy of their detectors by over 50% on average, while
simultaneously avoiding detection by rule checkers. Finally, we explore whether
these attacks can be mitigated by training the detectors on adversarial
samples.",arxiv
http://arxiv.org/abs/1803.06312v2,2018-04-17T02:26:35Z,2018-03-16T16:59:47Z,EVA$^2$: Exploiting Temporal Redundancy in Live Computer Vision,"Hardware support for deep convolutional neural networks (CNNs) is critical to
advanced computer vision in mobile and embedded devices. Current designs,
however, accelerate generic CNNs; they do not exploit the unique
characteristics of real-time vision. We propose to use the temporal redundancy
in natural video to avoid unnecessary computation on most frames. A new
algorithm, activation motion compensation, detects changes in the visual input
and incrementally updates a previously-computed output. The technique takes
inspiration from video compression and applies well-known motion estimation
techniques to adapt to visual changes. We use an adaptive key frame rate to
control the trade-off between efficiency and vision quality as the input
changes. We implement the technique in hardware as an extension to existing
state-of-the-art CNN accelerator designs. The new unit reduces the average
energy per frame by 54.2%, 61.7%, and 87.6% for three CNNs with less than 1%
loss in vision accuracy.",arxiv
http://arxiv.org/abs/1910.06017v1,2019-10-14T09:52:40Z,2019-10-14T09:52:40Z,"OmniTrack: Real-time detection and tracking of objects, text and logos
  in video","The automatic detection and tracking of general objects (like persons,
animals or cars), text and logos in a video is crucial for many video
understanding tasks, and usually real-time processing as required. We propose
OmniTrack, an efficient and robust algorithm which is able to automatically
detect and track objects, text as well as brand logos in real-time. It combines
a powerful deep learning based object detector (YoloV3) with high-quality
optical flow methods. Based on the reference YoloV3 C++ implementation, we did
some important performance optimizations which will be described. The major
steps in the training procedure for the combined detector for text and logo
will be presented. We will describe then the OmniTrack algorithm, consisting of
the phases preprocessing, feature calculation, prediction, matching and update.
Several performance optimizations have been implemented there as well, like
doing the object detection and optical flow calculation asynchronously.
Experiments show that the proposed algorithm runs in real-time for standard
definition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.",arxiv
http://arxiv.org/abs/1907.02526v1,2019-07-03T21:25:21Z,2019-07-03T21:25:21Z,"Convolutional Neural Network-based Speech Enhancement for Cochlear
  Implant Recipients","Attempts to develop speech enhancement algorithms with improved speech
intelligibility for cochlear implant (CI) users have met with limited success.
To improve speech enhancement methods for CI users, we propose to perform
speech enhancement in a cochlear filter-bank feature space, a feature-set
specifically designed for CI users based on CI auditory stimuli. We leverage a
convolutional neural network (CNN) to extract both stationary and
non-stationary components of environmental acoustics and speech. We propose
three CNN architectures: (1) vanilla CNN that directly generates the enhanced
signal; (2) spectral-subtraction-style CNN (SS-CNN) that first predicts noise
and then generates the enhanced signal by subtracting noise from the noisy
signal; (3) Wiener-style CNN (Wiener-CNN) that generates an optimal mask for
suppressing noise. An important problem of the proposed networks is that they
introduce considerable delays, which limits their real-time application for CI
users. To address this, this study also considers causal variations of these
networks. Our experiments show that the proposed networks (both causal and
non-causal forms) achieve significant improvement over existing baseline
systems. We also found that causal Wiener-CNN outperforms other networks, and
leads to the best overall envelope coefficient measure (ECM). The proposed
algorithms represent a viable option for implementation on the CCi-MOBILE
research platform as a pre-processor for CI users in naturalistic environments.",arxiv
http://arxiv.org/abs/2111.02041v1,2021-11-03T07:00:20Z,2021-11-03T07:00:20Z,"A Comparative Study of Speaker Role Identification in Air Traffic
  Communication Using Deep Learning Approaches","Automatic spoken instruction understanding (SIU) of the controller-pilot
conversations in the air traffic control (ATC) requires not only recognizing
the words and semantics of the speech but also determining the role of the
speaker. However, few of the published works on the automatic understanding
systems in air traffic communication focus on speaker role identification
(SRI). In this paper, we formulate the SRI task of controller-pilot
communication as a binary classification problem. Furthermore, the text-based,
speech-based, and speech and text based multi-modal methods are proposed to
achieve a comprehensive comparison of the SRI task. To ablate the impacts of
the comparative approaches, various advanced neural network architectures are
applied to optimize the implementation of text-based and speech-based methods.
Most importantly, a multi-modal speaker role identification network (MMSRINet)
is designed to achieve the SRI task by considering both the speech and textual
modality features. To aggregate modality features, the modal fusion module is
proposed to fuse and squeeze acoustic and textual representations by modal
attention mechanism and self-attention pooling layer, respectively. Finally,
the comparative approaches are validated on the ATCSpeech corpus collected from
a real-world ATC environment. The experimental results demonstrate that all the
comparative approaches are worked for the SRI task, and the proposed MMSRINet
shows the competitive performance and robustness than the other methods on both
seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively.",arxiv
http://arxiv.org/abs/1805.07925v3,2019-04-25T08:02:35Z,2018-05-21T07:30:26Z,"Batch-Instance Normalization for Adaptively Style-Invariant Neural
  Networks","Real-world image recognition is often challenged by the variability of visual
styles including object textures, lighting conditions, filter effects, etc.
Although these variations have been deemed to be implicitly handled by more
training data and deeper networks, recent advances in image style transfer
suggest that it is also possible to explicitly manipulate the style
information. Extending this idea to general visual recognition problems, we
present Batch-Instance Normalization (BIN) to explicitly normalize unnecessary
styles from images. Considering certain style features play an essential role
in discriminative tasks, BIN learns to selectively normalize only disturbing
styles while preserving useful styles. The proposed normalization module is
easily incorporated into existing network architectures such as Residual
Networks, and surprisingly improves the recognition performance in various
scenarios. Furthermore, experiments verify that BIN effectively adapts to
completely different tasks like object classification and style transfer, by
controlling the trade-off between preserving and removing style variations. BIN
can be implemented with only a few lines of code using popular deep learning
frameworks.",arxiv
http://arxiv.org/abs/1806.08085v1,2018-06-21T07:03:36Z,2018-06-21T07:03:36Z,"Inference of Quantized Neural Networks on Heterogeneous All-Programmable
  Devices","Neural networks have established as a generic and powerful means to approach
challenging problems such as image classification, object detection or decision
making. Their successful employment foots on an enormous demand of compute. The
quantization of network parameters and the processed data has proven a valuable
measure to reduce the challenges of network inference so effectively that the
feasible scope of applications is expanded even into the embedded domain. This
paper describes the making of a real-time object detection in a live video
stream processed on an embedded all-programmable device. The presented case
illustrates how the required processing is tamed and parallelized across both
the CPU cores and the programmable logic and how the most suitable resources
and powerful extensions, such as NEON vectorization, are leveraged for the
individual processing steps. The crafted result is an extended Darknet
framework implementing a fully integrated, end-to-end solution from video
capture over object annotation to video output applying neural network
inference at different quantization levels running at 16~frames per second on
an embedded Zynq UltraScale+ (XCZU3EG) platform.",arxiv
http://arxiv.org/abs/2011.05671v1,2020-11-11T10:00:12Z,2020-11-11T10:00:12Z,"VStreamDRLS: Dynamic Graph Representation Learning with Self-Attention
  for Enterprise Distributed Video Streaming Solutions","Live video streaming has become a mainstay as a standard communication
solution for several enterprises worldwide. To efficiently stream high-quality
live video content to a large amount of offices, companies employ distributed
video streaming solutions which rely on prior knowledge of the underlying
evolving enterprise network. However, such networks are highly complex and
dynamic. Hence, to optimally coordinate the live video distribution, the
available network capacity between viewers has to be accurately predicted. In
this paper we propose a graph representation learning technique on weighted and
dynamic graphs to predict the network capacity, that is the weights of
connections/links between viewers/nodes. We propose VStreamDRLS, a graph neural
network architecture with a self-attention mechanism to capture the evolution
of the graph structure of live video streaming events. VStreamDRLS employs the
graph convolutional network (GCN) model over the duration of a live video
streaming event and introduces a self-attention mechanism to evolve the GCN
parameters. In doing so, our model focuses on the GCN weights that are relevant
to the evolution of the graph and generate the node representation,
accordingly. We evaluate our proposed approach on the link prediction task on
two real-world datasets, generated by enterprise live video streaming events.
The duration of each event lasted an hour. The experimental results demonstrate
the effectiveness of VStreamDRLS when compared with state-of-the-art
strategies. Our evaluation datasets and implementation are publicly available
at https://github.com/stefanosantaris/vstreamdrls",arxiv
http://arxiv.org/abs/1807.00139v1,2018-06-30T08:31:54Z,2018-06-30T08:31:54Z,Harnessing constrained resources in service industry via video analytics,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance.",arxiv
http://arxiv.org/abs/1710.08135v1,2017-10-23T08:12:45Z,2017-10-23T08:12:45Z,"An iterative closest point method for measuring the level of similarity
  of 3d log scans in wood industry","In the Canadian's lumber industry, simulators are used to predict the lumbers
resulting from the sawing of a log at a given sawmill. Giving a log or several
logs' 3D scans as input, simulators perform a real-time job to predict the
lumbers. These simulators, however, tend to be slow at processing large volume
of wood. We thus explore an alternative approximation techniques based on the
Iterative Closest Point (ICP) algorithm to identify the already processed log
to which an unseen log resembles the most. The main benefit of the ICP approach
is that it can easily handle 3D scans with a variable number of points. We
compare this ICP-based nearest neighbor predictor, to predictors built using
machine learning algorithms such as the K-nearest-neighbor (kNN) and Random
Forest (RF). The implemented ICP-based predictor enabled us to identify key
points in using the 3D scans directly for distance calculation. The long-term
goal of this ongoing research is to integrated ICP distance calculations and
machine learning.",arxiv
http://arxiv.org/abs/1904.05343v2,2020-03-26T02:51:10Z,2019-04-10T17:53:38Z,StegaStamp: Invisible Hyperlinks in Physical Photographs,"Printed and digitally displayed photos have the ability to hide imperceptible
digital data that can be accessed through internet-connected imaging systems.
Another way to think about this is physical photographs that have unique QR
codes invisibly embedded within them. This paper presents an architecture,
algorithms, and a prototype implementation addressing this vision. Our key
technical contribution is StegaStamp, a learned steganographic algorithm to
enable robust encoding and decoding of arbitrary hyperlink bitstrings into
photos in a manner that approaches perceptual invisibility. StegaStamp
comprises a deep neural network that learns an encoding/decoding algorithm
robust to image perturbations approximating the space of distortions resulting
from real printing and photography. We demonstrates real-time decoding of
hyperlinks in photos from in-the-wild videos that contain variation in
lighting, shadows, perspective, occlusion and viewing distance. Our prototype
system robustly retrieves 56 bit hyperlinks after error correction - sufficient
to embed a unique code within every photo on the internet.",arxiv
http://arxiv.org/abs/2007.10243v1,2020-07-20T16:32:27Z,2020-07-20T16:32:27Z,Inter-Homines: Distance-Based Risk Estimation for Human Safety,"In this document, we report our proposal for modeling the risk of possible
contagiousity in a given area monitored by RGB cameras where people freely move
and interact. Our system, called Inter-Homines, evaluates in real-time the
contagion risk in a monitored area by analyzing video streams: it is able to
locate people in 3D space, calculate interpersonal distances and predict risk
levels by building dynamic maps of the monitored area. Inter-Homines works both
indoor and outdoor, in public and private crowded areas. The software is
applicable to already installed cameras or low-cost cameras on industrial PCs,
equipped with an additional embedded edge-AI system for temporary measurements.
From the AI-side, we exploit a robust pipeline for real-time people detection
and localization in the ground plane by homographic transformation based on
state-of-the-art computer vision algorithms; it is a combination of a people
detector and a pose estimator. From the risk modeling side, we propose a
parametric model for a spatio-temporal dynamic risk estimation, that, validated
by epidemiologists, could be useful for safety monitoring the acceptance of
social distancing prevention measures by predicting the risk level of the
scene.",arxiv
http://arxiv.org/abs/2107.07502v2,2021-11-10T07:31:56Z,2021-07-15T17:54:36Z,MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.",arxiv
http://arxiv.org/abs/1811.07112v2,2019-04-10T09:59:47Z,2018-11-17T07:09:13Z,Augmented LiDAR Simulator for Autonomous Driving,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads
is a critical task. Deep-learning based methods using annotated LiDAR data have
been the most widely adopted approach for this. Unfortunately, annotating 3D
point cloud is a very challenging, time- and money-consuming task. In this
paper, we propose a novel LiDAR simulator that augments real point cloud with
synthetic obstacles (e.g., cars, pedestrians, and other movable objects).
Unlike previous simulators that entirely rely on CG models and game engines,
our augmented simulator bypasses the requirement to create high-fidelity
background CAD models. Instead, we can simply deploy a vehicle with a LiDAR
scanner to sweep the street of interests to obtain the background point cloud,
based on which annotated point cloud can be automatically generated. This
unique ""scan-and-simulate"" capability makes our approach scalable and
practical, ready for large-scale industrial applications. In this paper, we
describe our simulator in detail, in particular the placement of obstacles that
is critical for performance enhancement. We show that detectors with our
simulated LiDAR point cloud alone can perform comparably (within two percentage
points) with these trained with real data. Mixing real and simulated data can
achieve over 95% accuracy.",arxiv
http://arxiv.org/abs/1912.10557v3,2020-08-07T05:37:40Z,2019-12-22T23:02:18Z,"Algorithm Unrolling: Interpretable, Efficient Deep Learning for Signal
  and Image Processing","Deep neural networks provide unprecedented performance gains in many real
world problems in signal and image processing. Despite these gains, future
development and practical deployment of deep networks is hindered by their
blackbox nature, i.e., lack of interpretability, and by the need for very large
training sets. An emerging technique called algorithm unrolling or unfolding
offers promise in eliminating these issues by providing a concrete and
systematic connection between iterative algorithms that are used widely in
signal processing and deep neural networks. Unrolling methods were first
proposed to develop fast neural network approximations for sparse coding. More
recently, this direction has attracted enormous attention and is rapidly
growing both in theoretic investigations and practical applications. The
growing popularity of unrolled deep networks is due in part to their potential
in developing efficient, high-performance and yet interpretable network
architectures from reasonable size training sets. In this article, we review
algorithm unrolling for signal and image processing. We extensively cover
popular techniques for algorithm unrolling in various domains of signal and
image processing including imaging, vision and recognition, and speech
processing. By reviewing previous works, we reveal the connections between
iterative algorithms and neural networks and present recent theoretical
results. Finally, we provide a discussion on current limitations of unrolling
and suggest possible future research directions.",arxiv
http://arxiv.org/abs/2005.08337v1,2020-05-17T18:46:23Z,2020-05-17T18:46:23Z,A Survey on Unknown Presentation Attack Detection for Fingerprint,"Fingerprint recognition systems are widely deployed in various real-life
applications as they have achieved high accuracy. The widely used applications
include border control, automated teller machine (ATM), and attendance
monitoring systems. However, these critical systems are prone to spoofing
attacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed
by presenting gummy fingers made from different materials such as silicone,
gelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex.
Biometrics Researchers have developed Presentation Attack Detection (PAD)
methods as a countermeasure to PA. PAD is usually done by training a machine
learning classifier for known attacks for a given dataset, and they achieve
high accuracy in this task. However, generalizing to unknown attacks is an
essential problem from applicability to real-world systems, mainly because
attacks cannot be exhaustively listed in advance. In this survey paper, we
present a comprehensive survey on existing PAD algorithms for fingerprint
recognition systems, specifically from the standpoint of detecting unknown PAD.
We categorize PAD algorithms, point out their advantages/disadvantages, and
future directions for this area.",arxiv
http://arxiv.org/abs/1806.04533v2,2018-06-21T02:20:59Z,2018-06-11T03:40:06Z,"Cross-dataset Person Re-Identification Using Similarity Preserved
  Generative Adversarial Networks","Person re-identification (Re-ID) aims to match the image frames which contain
the same person in the surveillance videos. Most of the Re-ID algorithms
conduct supervised training in some small labeled datasets, so directly
deploying these trained models to the real-world large camera networks may lead
to a poor performance due to underfitting. The significant difference between
the source training dataset and the target testing dataset makes it challenging
to incrementally optimize the model. To address this challenge, we propose a
novel solution by transforming the unlabeled images in the target domain to fit
the original classifier by using our proposed similarity preserved generative
adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the
generative adversarial networks with the cycle consistency constraint to
transform the unlabeled images in the target domain to the style of the source
domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is
measured by a siamese deep convolutional neural network, to preserve the
similarity of the transformed images of the same person. Comprehensive
experiments based on multiple real surveillance datasets are conducted, and the
results show that our algorithm is better than the state-of-the-art
cross-dataset unsupervised person Re-ID algorithms.",arxiv
http://arxiv.org/abs/1909.10914v1,2019-09-23T09:20:44Z,2019-09-23T09:20:44Z,Sensor-Augmented Neural Adaptive Bitrate Video Streaming on UAVs,"Recent advances in unmanned aerial vehicle (UAV) technology have
revolutionized a broad class of civil and military applications. However, the
designs of wireless technologies that enable real-time streaming of
high-definition video between UAVs and ground clients present a conundrum. Most
existing adaptive bitrate (ABR) algorithms are not optimized for the
air-to-ground links, which usually fluctuate dramatically due to the dynamic
flight states of the UAV. In this paper, we present SA-ABR, a new
sensor-augmented system that generates ABR video streaming algorithms with the
assistance of various kinds of inherent sensor data that are used to pilot
UAVs. By incorporating the inherent sensor data with network observations,
SA-ABR trains a deep reinforcement learning (DRL) model to extract salient
features from the flight state information and automatically learn an ABR
algorithm to adapt to the varying UAV channel capacity through the training
process. SA-ABR does not rely on any assumptions or models about UAV's flight
states or the environment, but instead, it makes decisions by exploiting
temporal properties of past throughput through the long short-term memory
(LSTM) to adapt itself to a wide range of highly dynamic environments. We have
implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare
SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the
results show that our system outperforms the best known existing ABR algorithm
by 21.4% in terms of the average quality of experience (QoE) reward.",arxiv
http://arxiv.org/abs/2107.13619v1,2021-07-28T19:53:05Z,2021-07-28T19:53:05Z,"A Deep Graph Reinforcement Learning Model for Improving User Experience
  in Live Video Streaming","In this paper we present a deep graph reinforcement learning model to predict
and improve the user experience during a live video streaming event,
orchestrated by an agent/tracker. We first formulate the user experience
prediction problem as a classification task, accounting for the fact that most
of the viewers at the beginning of an event have poor quality of experience due
to low-bandwidth connections and limited interactions with the tracker. In our
model we consider different factors that influence the quality of user
experience and train the proposed model on diverse state-action transitions
when viewers interact with the tracker. In addition, provided that past events
have various user experience characteristics we follow a gradient boosting
strategy to compute a global model that learns from different events. Our
experiments with three real-world datasets of live video streaming events
demonstrate the superiority of the proposed model against several baseline
strategies. Moreover, as the majority of the viewers at the beginning of an
event has poor experience, we show that our model can significantly increase
the number of viewers with high quality experience by at least 75% over the
first streaming minutes. Our evaluation datasets and implementation are
publicly available at https://publicresearch.z13.web.core.windows.net",arxiv
http://arxiv.org/abs/2007.16170v1,2020-07-31T16:43:10Z,2020-07-31T16:43:10Z,Diet deep generative audio models with structured lottery,"Deep learning models have provided extremely successful solutions in most
audio application fields. However, the high accuracy of these models comes at
the expense of a tremendous computation cost. This aspect is almost always
overlooked in evaluating the quality of proposed models. However, models should
not be evaluated without taking into account their complexity. This aspect is
especially critical in audio applications, which heavily relies on specialized
embedded hardware with real-time constraints. In this paper, we build on recent
observations that deep models are highly overparameterized, by studying the
lottery ticket hypothesis on deep generative audio models. This hypothesis
states that extremely efficient small sub-networks exist in deep models and
would provide higher accuracy than larger models if trained in isolation.
However, lottery tickets are found by relying on unstructured masking, which
means that resulting models do not provide any gain in either disk size or
inference time. Instead, we develop here a method aimed at performing
structured trimming. We show that this requires to rely on global selection and
introduce a specific criterion based on mutual information. First, we confirm
the surprising result that smaller models provide higher accuracy than their
large counterparts. We further show that we can remove up to 95% of the model
weights without significant degradation in accuracy. Hence, we can obtain very
light models for generative audio across popular methods such as Wavenet, SING
or DDSP, that are up to 100 times smaller with commensurate accuracy. We study
the theoretical bounds for embedding these models on Raspberry Pi and Arduino,
and show that we can obtain generative models on CPU with equivalent quality as
large GPU models. Finally, we discuss the possibility of implementing deep
generative audio models on embedded platforms.",arxiv
http://arxiv.org/abs/2005.07976v2,2020-10-30T13:38:45Z,2020-05-16T12:58:31Z,"Target Speech Extraction Based on Blind Source Separation and
  X-vector-based Speaker Selection Trained with Data Augmentation","Extracting the desired speech from a mixture is a meaningful and challenging
task. The end-to-end DNN-based methods, though attractive, face the problem of
generalization. In this paper, we explore a sequential approach for target
speech extraction by combining blind source separation (BSS) with the x-vector
based speaker recognition (SR) module. Two promising BSS methods based on
source independence assumption, independent low-rank matrix analysis (ILRMA)
and multi-channel variational autoencoder (MVAE), are utilized and compared.
ILRMA employs nonnegative matrix factorization (NMF) to capture spectral
structures of source signals and MVAE utilizes the strong modeling power of
deep neural networks (DNN). However, the investigation of MVAE has been limited
to the training with very few speakers and the speech signals of test speakers
are usually included. We extend the training of MVAE using clean speech signals
of 500 speakers to evaluate its generalization to unseen speakers. To improve
the correct extraction rate, two data augmentation strategies are implemented
to train the SR module. The performance of the proposed cascaded approach is
investigated with test data constructed with real room impulse responses under
varied environments.",arxiv
http://arxiv.org/abs/2110.06674v1,2021-10-13T12:18:09Z,2021-10-13T12:18:09Z,Truthful AI: Developing and governing AI that does not lie,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI ""lies"" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.",arxiv
http://arxiv.org/abs/2106.15409v2,2021-06-30T20:05:23Z,2021-06-28T08:07:31Z,"Efficient Realistic Data Generation Framework leveraging Deep
  Learning-based Human Digitization","The performance of supervised deep learning algorithms depends significantly
on the scale, quality and diversity of the data used for their training.
Collecting and manually annotating large amount of data can be both
time-consuming and costly tasks to perform. In the case of tasks related to
visual human-centric perception, the collection and distribution of such data
may also face restrictions due to legislation regarding privacy. In addition,
the design and testing of complex systems, e.g., robots, which often employ
deep learning-based perception models, may face severe difficulties as even
state-of-the-art methods trained on real and large-scale datasets cannot always
perform adequately due to not having been adapted to the visual differences
between the virtual and the real world data. As an attempt to tackle and
mitigate the effect of these issues, we present a method that automatically
generates realistic synthetic data with annotations for a) person detection, b)
face recognition, and c) human pose estimation. The proposed method takes as
input real background images and populates them with human figures in various
poses. Instead of using hand-made 3D human models, we propose the use of models
generated through deep learning methods, further reducing the dataset creation
costs, while maintaining a high level of realism. In addition, we provide
open-source and easy to use tools that implement the proposed pipeline,
allowing for generating highly-realistic synthetic datasets for a variety of
tasks. A benchmarking and evaluation in the corresponding tasks shows that
synthetic data can be effectively used as a supplement to real data.",arxiv
http://arxiv.org/abs/1805.00330v1,2018-04-24T22:02:10Z,2018-04-24T22:02:10Z,"Real-Time Human Detection as an Edge Service Enabled by a Lightweight
  CNN","Edge computing allows more computing tasks to take place on the decentralized
nodes at the edge of networks. Today many delay sensitive, mission-critical
applications can leverage these edge devices to reduce the time delay or even
to enable real time, online decision making thanks to their onsite presence.
Human objects detection, behavior recognition and prediction in smart
surveillance fall into that category, where a transition of a huge volume of
video streaming data can take valuable time and place heavy pressure on
communication networks. It is widely recognized that video processing and
object detection are computing intensive and too expensive to be handled by
resource limited edge devices. Inspired by the depthwise separable convolution
and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural
Network (LCNN) is introduced in this paper. By narrowing down the classifier's
searching space to focus on human objects in surveillance video frames, the
proposed LCNN algorithm is able to detect pedestrians with an affordable
computation workload to an edge device. A prototype has been implemented on an
edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance
is achieved using real world surveillance video streams. The experimental study
has validated the design of LCNN and shown it is a promising approach to
computing intensive applications at the edge.",arxiv
http://arxiv.org/abs/2104.06826v1,2021-04-14T12:57:40Z,2021-04-14T12:57:40Z,Towards Unsupervised Fine-Tuning for Edge Video Analytics,"Judging by popular and generic computer vision challenges, such as the
ImageNet or PASCAL VOC, neural networks have proven to be exceptionally
accurate in recognition tasks. However, state-of-the-art accuracy often comes
at a high computational price, requiring equally state-of-the-art and high-end
hardware acceleration to achieve anything near real-time performance. At the
same time, use cases such as smart cities or autonomous vehicles require an
automated analysis of images from fixed cameras in real-time. Due to the huge
and constant amount of network bandwidth these streams would generate, we
cannot rely on offloading compute to the omnipresent and omnipotent cloud.
Therefore, a distributed Edge Cloud must be in charge to process images
locally. However, the Edge Cloud is, by nature, resource-constrained, which
puts a limit on the computational complexity of the models executed in the
edge. Nonetheless, there is a need for a meeting point between the Edge Cloud
and accurate real-time video analytics. In this paper, we propose a method for
improving accuracy of edge models without any extra compute cost by means of
automatic model specialization. First, we show how the sole assumption of
static cameras allows us to make a series of considerations that greatly
simplify the scope of the problem. Then, we present Edge AutoTuner, a framework
that implements and brings these considerations together to automate the
end-to-end fine-tuning of models. Finally, we show that complex neural networks
- able to generalize better - can be effectively used as teachers to annotate
datasets for the fine-tuning of lightweight neural networks and tailor them to
the specific edge context, which boosts accuracy at constant computational
cost, and do so without any human interaction. Results show that our method can
automatically improve accuracy of pre-trained models by an average of 21%.",arxiv
http://arxiv.org/abs/2104.02541v1,2021-04-06T14:31:23Z,2021-04-06T14:31:23Z,"Instantaneous Stereo Depth Estimation of Real-World Stimuli with a
  Neuromorphic Stereo-Vision Setup","The stereo-matching problem, i.e., matching corresponding features in two
different views to reconstruct depth, is efficiently solved in biology. Yet, it
remains the computational bottleneck for classical machine vision approaches.
By exploiting the properties of event cameras, recently proposed Spiking Neural
Network (SNN) architectures for stereo vision have the potential of simplifying
the stereo-matching problem. Several solutions that combine event cameras with
spike-based neuromorphic processors already exist. However, they are either
simulated on digital hardware or tested on simplified stimuli. In this work, we
use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a
brain-inspired event-based stereo-matching architecture implemented on a
mixed-signal neuromorphic processor with real-world data. Our experiments show
that this SNN architecture, composed of coincidence detectors and disparity
sensitive neurons, is able to provide a coarse estimate of the input disparity
instantaneously, thereby detecting the presence of a stimulus moving in depth
in real-time.",arxiv
http://arxiv.org/abs/2001.01043v2,2020-04-06T03:26:24Z,2020-01-04T06:05:23Z,"SurveilEdge: Real-time Video Query based on Collaborative Cloud-Edge
  Deep Learning","The real-time query of massive surveillance video data plays a fundamental
role in various smart urban applications such as public safety and intelligent
transportation. Traditional cloud-based approaches are not applicable because
of high transmission latency and prohibitive bandwidth cost, while edge devices
are often incapable of executing complex vision algorithms with low latency and
high accuracy due to restricted resources. Given the infeasibility of both
cloud-only and edge-only solutions, we present SurveilEdge, a collaborative
cloud-edge system for real-time queries of large-scale surveillance video
streams. Specifically, we design a convolutional neural network (CNN) training
scheme to reduce the training time with high accuracy, and an intelligent task
allocator to balance the load among different computing nodes and to achieve
the latency-accuracy tradeoff for real-time queries. We implement SurveilEdge
on a prototype with multiple edge devices and a public Cloud, and conduct
extensive experiments using realworld surveillance video datasets. Evaluation
results demonstrate that SurveilEdge manages to achieve up to 7x less bandwidth
cost and 5.4x faster query response time than the cloud-only solution; and can
improve query accuracy by up to 43.9% and achieve 15.8x speedup respectively,
in comparison with edge-only approaches.",arxiv
http://arxiv.org/abs/2004.09548v1,2020-04-20T18:07:55Z,2020-04-20T18:07:55Z,AANet: Adaptive Aggregation Network for Efficient Stereo Matching,"Despite the remarkable progress made by learning based stereo matching
algorithms, one key challenge remains unsolved. Current state-of-the-art stereo
models are mostly based on costly 3D convolutions, the cubic computational
complexity and high memory consumption make it quite expensive to deploy in
real-world applications. In this paper, we aim at completely replacing the
commonly used 3D convolutions to achieve fast inference speed while maintaining
comparable accuracy. To this end, we first propose a sparse points based
intra-scale cost aggregation method to alleviate the well-known edge-fattening
issue at disparity discontinuities. Further, we approximate traditional
cross-scale cost aggregation algorithm with neural network layers to handle
large textureless regions. Both modules are simple, lightweight, and
complementary, leading to an effective and efficient architecture for cost
aggregation. With these two modules, we can not only significantly speed up
existing top-performing models (e.g., $41\times$ than GC-Net, $4\times$ than
PSMNet and $38\times$ than GA-Net), but also improve the performance of fast
stereo models (e.g., StereoNet). We also achieve competitive results on Scene
Flow and KITTI datasets while running at 62ms, demonstrating the versatility
and high efficiency of the proposed method. Our full framework is available at
https://github.com/haofeixu/aanet .",arxiv
http://arxiv.org/abs/2101.06409v1,2021-01-16T09:00:34Z,2021-01-16T09:00:34Z,Shape Back-Projection In 3D Scenes,"In this work, we propose a novel framework shape back-projection for
computationally efficient point cloud processing in a probabilistic manner. The
primary component of the technique is shape histogram and a back-projection
procedure. The technique measures similarity between 3D surfaces, by analyzing
their geometrical properties. It is analogous to color back-projection which
measures similarity between images, simply by looking at their color
distributions. In the overall process, first, shape histogram of a sample
surface (e.g. planar) is computed, which captures the profile of surface
normals around a point in form of a probability distribution. Later, the
histogram is back-projected onto a test surface and a likelihood score is
obtained. The score depicts that how likely a point in the test surface behaves
similar to the sample surface, geometrically. Shape back-projection finds its
application in binary surface classification, high curvature edge detection in
unorganized point cloud, automated point cloud labeling for 3D-CNNs
(convolutional neural network) etc. The algorithm can also be used for
real-time robotic operations such as autonomous object picking in warehouse
automation, ground plane extraction for autonomous vehicles and can be deployed
easily on computationally limited platforms (UAVs).",arxiv
http://arxiv.org/abs/1709.06734v2,2018-07-06T13:20:51Z,2017-09-20T06:36:14Z,Enhancing Quality for HEVC Compressed Videos,"The latest High Efficiency Video Coding (HEVC) standard has been increasingly
applied to generate video streams over the Internet. However, HEVC compressed
videos may incur severe quality degradation, particularly at low bit-rates.
Thus, it is necessary to enhance the visual quality of HEVC videos at the
decoder side. To this end, this paper proposes a Quality Enhancement
Convolutional Neural Network (QE-CNN) method that does not require any
modification of the encoder to achieve quality enhancement for HEVC. In
particular, our QE-CNN method learns QE-CNN-I and QE-CNN-P models to reduce the
distortion of HEVC I and P frames, respectively. The proposed method differs
from the existing CNN-based quality enhancement approaches, which only handle
intra-coding distortion and are thus not suitable for P frames. Our
experimental results validate that our QE-CNN method is effective in enhancing
quality for both I and P frames of HEVC videos. To apply our QE-CNN method in
time-constrained scenarios, we further propose a Time-constrained Quality
Enhancement Optimization (TQEO) scheme. Our TQEO scheme controls the
computational time of QE-CNN to meet a target, meanwhile maximizing the quality
enhancement. Next, the experimental results demonstrate the effectiveness of
our TQEO scheme from the aspects of time control accuracy and quality
enhancement under different time constraints. Finally, we design a prototype to
implement our TQEO scheme in a real-time scenario.",arxiv
http://arxiv.org/abs/1808.03506v4,2019-03-05T23:50:01Z,2018-08-10T12:30:03Z,"ChipNet: Real-Time LiDAR Processing for Drivable Region Segmentation on
  an FPGA","This paper presents a field-programmable gate array (FPGA) design of a
segmentation algorithm based on convolutional neural network (CNN) that can
process light detection and ranging (LiDAR) data in real-time. For autonomous
vehicles, drivable region segmentation is an essential step that sets up the
static constraints for planning tasks. Traditional drivable region segmentation
algorithms are mostly developed on camera data, so their performance is
susceptible to the light conditions and the qualities of road markings. LiDAR
sensors can obtain the 3D geometry information of the vehicle surroundings with
high precision. However, it is a computational challenge to process a large
amount of LiDAR data in real-time. In this paper, a convolutional neural
network model is proposed and trained to perform semantic segmentation using
data from the LiDAR sensor. An efficient hardware architecture is proposed and
implemented on an FPGA that can process each LiDAR scan in 17.59 ms, which is
much faster than the previous works. Evaluated using Ford and KITTI road
detection benchmarks, the proposed solution achieves both high accuracy in
performance and real-time processing in speed.",arxiv
http://arxiv.org/abs/1611.01235v1,2016-11-04T01:10:07Z,2016-11-04T01:10:07Z,"A Self-Driving Robot Using Deep Convolutional Neural Networks on
  Neuromorphic Hardware","Neuromorphic computing is a promising solution for reducing the size, weight
and power of mobile embedded systems. In this paper, we introduce a realization
of such a system by creating the first closed-loop battery-powered
communication system between an IBM TrueNorth NS1e and an autonomous
Android-Based Robotics platform. Using this system, we constructed a dataset of
path following behavior by manually driving the Android-Based robot along steep
mountain trails and recording video frames from the camera mounted on the robot
along with the corresponding motor commands. We used this dataset to train a
deep convolutional neural network implemented on the TrueNorth NS1e. The NS1e,
which was mounted on the robot and powered by the robot's battery, resulted in
a self-driving robot that could successfully traverse a steep mountain path in
real time. To our knowledge, this represents the first time the TrueNorth NS1e
neuromorphic chip has been embedded on a mobile platform under closed-loop
control.",arxiv
http://arxiv.org/abs/2108.05045v2,2021-09-09T02:54:03Z,2021-08-11T06:08:25Z,Semi-Supervised Domain Generalizable Person Re-Identification,"Existing person re-identification (re-id) methods are stuck when deployed to
a new unseen scenario despite the success in cross-camera person matching.
Recent efforts have been substantially devoted to domain adaptive person re-id
where extensive unlabeled data in the new scenario are utilized in a
transductive learning manner. However, for each scenario, it is required to
first collect enough data and then train such a domain adaptive re-id model,
thus restricting their practical application. Instead, we aim to explore
multiple labeled datasets to learn generalized domain-invariant representations
for person re-id, which is expected universally effective for each new-coming
re-id scenario. To pursue practicability in real-world systems, we collect all
the person re-id datasets (20 datasets) in this field and select the three most
frequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen
target domains. In addition, we develop DataHunter that collects over 300K+
weak annotated images named YouTube-Human from YouTube street-view videos,
which joins 17 remaining full labeled datasets to form multiple source domains.
On such a large and challenging benchmark called FastHuman (~440K+ labeled
images), we further propose a simple yet effective Semi-Supervised Knowledge
Distillation (SSKD) framework. SSKD effectively exploits the weakly annotated
data by assigning soft pseudo labels to YouTube-Human to improve models'
generalization ability. Experiments on several protocols verify the
effectiveness of the proposed SSKD framework on domain generalizable person
re-id, which is even comparable to supervised learning on the target domains.
Lastly, but most importantly, we hope the proposed benchmark FastHuman could
bring the next development of domain generalizable person re-id algorithms.",arxiv
http://arxiv.org/abs/2005.13131v1,2020-05-27T02:17:54Z,2020-05-27T02:17:54Z,"Efficient Pig Counting in Crowds with Keypoints Tracking and
  Spatial-aware Temporal Response Filtering","Pig counting is a crucial task for large-scale pig farming, which is usually
completed by human visually. But this process is very time-consuming and
error-prone. Few studies in literature developed automated pig counting method.
Existing methods only focused on pig counting using single image, and its
accuracy is challenged by several factors, including pig movements, occlusion
and overlapping. Especially, the field of view of a single image is very
limited, and could not meet the requirements of pig counting for large pig
grouping houses. To that end, we presented a real-time automated pig counting
system in crowds using only one monocular fisheye camera with an inspection
robot. Our system showed that it produces accurate results surpassing human.
Our pipeline began with a novel bottom-up pig detection algorithm to avoid
false negatives due to overlapping, occlusion and deformation of pigs. A deep
convolution neural network (CNN) is designed to detect keypoints of pig body
part and associate the keypoints to identify individual pigs. After that, an
efficient on-line tracking method is used to associate pigs across video
frames. Finally, a novel spatial-aware temporal response filtering (STRF)
method is proposed to predict the counts of pigs, which is effective to
suppress false positives caused by pig or camera movements or tracking
failures. The whole pipeline has been deployed in an edge computing device, and
demonstrated the effectiveness.",arxiv
http://arxiv.org/abs/2102.07764v2,2021-02-17T18:56:39Z,2021-02-15T18:59:07Z,End-to-End Egospheric Spatial Memory,"Spatial memory, or the ability to remember and recall specific locations and
objects, is central to autonomous agents' ability to carry out tasks in real
environments. However, most existing artificial memory modules are not very
adept at storing spatial information. We propose a parameter-free module,
Egospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere
around the agent, enabling expressive 3D representations. ESM can be trained
end-to-end via either imitation or reinforcement learning, and improves both
training efficiency and final performance against other memory baselines on
both drone and manipulator visuomotor control tasks. The explicit egocentric
geometry also enables us to seamlessly combine the learned controller with
other non-learned modalities, such as local obstacle avoidance. We further show
applications to semantic segmentation on the ScanNet dataset, where ESM
naturally combines image-level and map-level inference modalities. Through our
broad set of experiments, we show that ESM provides a general computation graph
for embodied spatial reasoning, and the module forms a bridge between real-time
mapping systems and differentiable memory architectures. Implementation at:
https://github.com/ivy-dl/memory.",arxiv
http://arxiv.org/abs/2108.13680v2,2021-09-07T01:18:16Z,2021-08-31T08:37:58Z,Learning Practically Feasible Policies for Online 3D Bin Packing,"We tackle the Online 3D Bin Packing Problem, a challenging yet practically
useful variant of the classical Bin Packing Problem. In this problem, the items
are delivered to the agent without informing the full sequence information.
Agent must directly pack these items into the target bin stably without
changing their arrival order, and no further adjustment is permitted. Online
3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt
deep reinforcement learning, in particular, the on-policy actor-critic
framework, to solve this MDP with constrained action space. To learn a
practically feasible packing policy, we propose three critical designs. First,
we propose an online analysis of packing stability based on a novel stacking
tree. It attains a high analysis accuracy while reducing the computational
complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL
training. Second, we propose a decoupled packing policy learning for different
dimensions of placement which enables high-resolution spatial discretization
and hence high packing precision. Third, we introduce a reward function that
dictates the robot to place items in a far-to-near order and therefore
simplifies the collision avoidance in movement planning of the robotic arm.
Furthermore, we provide a comprehensive discussion on several key implemental
issues. The extensive evaluation demonstrates that our learned policy
outperforms the state-of-the-art methods significantly and is practically
usable for real-world applications.",arxiv
http://arxiv.org/abs/2101.04240v2,2021-01-15T22:46:36Z,2021-01-11T23:58:56Z,"Lesion2Vec: Deep Metric Learning for Few-Shot Multiple Lesions
  Recognition in Wireless Capsule Endoscopy Video","Effective and rapid detection of lesions in the Gastrointestinal tract is
critical to gastroenterologist's response to some life-threatening diseases.
Wireless Capsule Endoscopy (WCE) has revolutionized traditional endoscopy
procedure by allowing gastroenterologists visualize the entire GI tract
non-invasively. Once the tiny capsule is swallowed, it sequentially capture
images of the GI tract at about 2 to 6 frames per second (fps). A single video
can last up to 8 hours producing between 30,000 to 100,000 images. Automating
the detection of frames containing specific lesion in WCE video would relieve
gastroenterologists the arduous task of reviewing the entire video before
making diagnosis. While the WCE produces large volume of images, only about 5\%
of the frames contain lesions that aid the diagnosis process. Convolutional
Neural Network (CNN) based models have been very successful in various image
classification tasks. However, they suffer excessive parameters, are sample
inefficient and rely on very large amount of training data. Deploying a CNN
classifier for lesion detection task will require time-to-time fine-tuning to
generalize to any unforeseen category. In this paper, we propose a metric-based
learning framework followed by a few-shot lesion recognition in WCE data.
Metric-based learning is a meta-learning framework designed to establish
similarity or dissimilarity between concepts while few-shot learning (FSL) aims
to identify new concepts from only a small number of examples. We train a
feature extractor to learn a representation for different small bowel lesions
using metric-based learning. At the testing stage, the category of an unseen
sample is predicted from only a few support examples, thereby allowing the
model to generalize to a new category that has never been seen before. We
demonstrated the efficacy of this method on real patient capsule endoscopy
data.",arxiv
http://arxiv.org/abs/1502.04221v1,2015-02-14T16:14:05Z,2015-02-14T16:14:05Z,"A Row-parallel 8$\times$8 2-D DCT Architecture Using Algebraic Integer
  Based Exact Computation","An algebraic integer (AI) based time-multiplexed row-parallel architecture
and two final-reconstruction step (FRS) algorithms are proposed for the
implementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The
architecture directly realizes an error-free 2-D DCT without using FRSs between
row-column transforms, leading to an 8$\times$8 2-D DCT which is entirely free
of quantization errors in AI basis. As a result, the user-selectable accuracy
for each of the coefficients in the FRS facilitates each of the 64 coefficients
to have its precision set independently of others, avoiding the leakage of
quantization noise between channels as is the case for published DCT designs.
The proposed FRS uses two approaches based on (i) optimized Dempster-Macleod
multipliers and (ii) expansion factor scaling. This architecture enables
low-noise high-dynamic range applications in digital video processing that
requires full control of the finite-precision computation of the 2-D DCT. The
proposed architectures and FRS techniques are experimentally verified and
validated using hardware implementations that are physically realized and
verified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using
the two proposed FRS schemes, have been designed, simulated, physically
implemented and measured. The maximum clock rate and block-rate achieved among
8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a
pixel rate of 8$\times$307.787$\approx$2.462 GHz if eventually embedded in a
real-time video-processing system. The equivalent frame rate is about 1187.35
Hz for the image size of 1920$\times$1080. All implementations are functional
on a Xilinx Virtex-6 XC6VLX240T FPGA device.",arxiv
http://arxiv.org/abs/1903.09254v4,2019-04-05T22:52:17Z,2019-03-21T22:03:25Z,"CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle
  Tracking and Re-Identification","Urban traffic optimization using traffic cameras as sensors is driving the
need to advance state-of-the-art multi-target multi-camera (MTMC) tracking.
This work introduces CityFlow, a city-scale traffic camera dataset consisting
of more than 3 hours of synchronized HD videos from 40 cameras across 10
intersections, with the longest distance between two simultaneous cameras being
2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in
terms of spatial coverage and the number of cameras/videos in an urban
environment. The dataset contains more than 200K annotated bounding boxes
covering a wide range of scenes, viewing angles, vehicle models, and urban
traffic flow conditions. Camera geometry and calibration information are
provided to aid spatio-temporal analysis. In addition, a subset of the
benchmark is made available for the task of image-based vehicle
re-identification (ReID). We conducted an extensive experimental evaluation of
baselines/state-of-the-art approaches in MTMC tracking, multi-target
single-camera (MTSC) tracking, object detection, and image-based ReID on this
dataset, analyzing the impact of different network architectures, loss
functions, spatio-temporal models and their combinations on task effectiveness.
An evaluation server is launched with the release of our benchmark at the 2019
AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to
compare the performance of their newest techniques. We expect this dataset to
catalyze research in this field, propel the state-of-the-art forward, and lead
to deployed traffic optimization(s) in the real world.",arxiv
http://arxiv.org/abs/1910.09430v2,2020-02-06T16:28:34Z,2019-10-21T15:06:03Z,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.de",arxiv
http://arxiv.org/abs/1711.02757v1,2017-11-07T22:42:09Z,2017-11-07T22:42:09Z,Real-Time Road Segmentation Using LiDAR Data Processing on an FPGA,"This paper presents the FPGA design of a convolutional neural network (CNN)
based road segmentation algorithm for real-time processing of LiDAR data. For
autonomous vehicles, it is important to perform road segmentation and obstacle
detection such that the drivable region can be identified for path planning.
Traditional road segmentation algorithms are mainly based on image data from
cameras, which is subjected to the light condition as well as the quality of
road markings. LiDAR sensor can obtain the 3D geometry information of the
vehicle surroundings with very high accuracy. However, it is a computational
challenge to process a large amount of LiDAR data at real-time. In this work, a
convolutional neural network model is proposed and trained to perform semantic
segmentation using the LiDAR sensor data. Furthermore, an efficient hardware
design is implemented on the FPGA that can process each LiDAR scan in 16.9ms,
which is much faster than the previous works. Evaluated using KITTI road
benchmarks, the proposed solution achieves high accuracy of road segmentation.",arxiv
http://arxiv.org/abs/2107.05307v2,2021-07-14T14:42:34Z,2021-07-12T10:35:05Z,Real-Time Super-Resolution System of 4K-Video Based on Deep Learning,"Video super-resolution (VSR) technology excels in reconstructing low-quality
video, avoiding unpleasant blur effect caused by interpolation-based
algorithms. However, vast computation complexity and memory occupation hampers
the edge of deplorability and the runtime inference in real-life applications,
especially for large-scale VSR task. This paper explores the possibility of
real-time VSR system and designs an efficient and generic VSR network, termed
EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for
temporal coherence. In order to pursue faster VSR processing ability up to 4K
resolution, this paper tries to choose lightweight network structure and
efficient upsampling method to reduce the computation required by EGVSR network
under the guarantee of high visual quality. Besides, we implement the batch
normalization computation fusion, convolutional acceleration algorithm and
other neural network acceleration techniques on the actual hardware platform to
optimize the inference process of EGVSR network. Finally, our EGVSR achieves
the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the
most advanced VSR network at present, we achieve 85.04% reduction of
computation density and 7.92x performance speedups. In terms of visual quality,
the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,
etc.) on the public test dataset Vid4 and surpasses other state-of-the-art
methods in overall performance score. The source code of this project can be
found on https://github.com/Thmen/EGVSR.",arxiv
http://arxiv.org/abs/2001.03855v1,2020-01-12T05:25:02Z,2020-01-12T05:25:02Z,"Hyperparameters optimization for Deep Learning based emotion prediction
  for Human Robot Interaction","To enable humanoid robots to share our social space we need to develop
technology for easy interaction with the robots using multiple modes such as
speech, gestures and share our emotions with them. We have targeted this
research towards addressing the core issue of emotion recognition problem which
would require less computation resources and much lesser number of network
hyperparameters which will be more adaptive to be computed on low resourced
social robots for real time communication. More specifically, here we have
proposed an Inception module based Convolutional Neural Network Architecture
which has achieved improved accuracy of upto 6% improvement over the existing
network architecture for emotion classification when combinedly tested over
multiple datasets when tried over humanoid robots in real - time. Our proposed
model is reducing the trainable Hyperparameters to an extent of 94% as compared
to vanilla CNN model which clearly indicates that it can be used in real time
based application such as human robot interaction. Rigorous experiments have
been performed to validate our methodology which is sufficiently robust and
could achieve high level of accuracy. Finally, the model is implemented in a
humanoid robot, NAO in real time and robustness of the model is evaluated.",arxiv
http://arxiv.org/abs/2106.01674v1,2021-06-03T08:23:24Z,2021-06-03T08:23:24Z,"JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale
  Online Inference at Baidu","In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.",arxiv
http://arxiv.org/abs/2104.06237v1,2021-04-13T14:31:37Z,2021-04-13T14:31:37Z,"Learning to recover orientations from projections in single-particle
  cryo-EM","A major challenge in single-particle cryo-electron microscopy (cryo-EM) is
that the orientations adopted by the 3D particles prior to imaging are unknown;
yet, this knowledge is essential for high-resolution reconstruction. We present
a method to recover these orientations directly from the acquired set of 2D
projections. Our approach consists of two steps: (i) the estimation of
distances between pairs of projections, and (ii) the recovery of the
orientation of each projection from these distances. In step (i), pairwise
distances are estimated by a Siamese neural network trained on synthetic
cryo-EM projections from resolved bio-structures. In step (ii), orientations
are recovered by minimizing the difference between the distances estimated from
the projections and the distances induced by the recovered orientations. We
evaluated the method on synthetic cryo-EM datasets. Current results demonstrate
that orientations can be accurately recovered from projections that are shifted
and corrupted with a high level of noise. The accuracy of the recovery depends
on the accuracy of the distance estimator. While not yet deployed in a real
experimental setup, the proposed method offers a novel learning-based take on
orientation recovery in SPA. Our code is available at
https://github.com/JelenaBanjac/protein-reconstruction",arxiv
http://arxiv.org/abs/2012.09812v2,2021-03-26T11:12:28Z,2020-12-17T18:22:32Z,ViNG: Learning Open-World Navigation with Visual Goals,"We propose a learning-based navigation system for reaching visually indicated
goals and demonstrate this system on a real mobile robot platform. Learning
provides an appealing alternative to conventional methods for robotic
navigation: instead of reasoning about environments in terms of geometry and
maps, learning can enable a robot to learn about navigational affordances,
understand what types of obstacles are traversable (e.g., tall grass) or not
(e.g., walls), and generalize over patterns in the environment. However, unlike
conventional planning algorithms, it is harder to change the goal for a learned
policy during deployment. We propose a method for learning to navigate towards
a goal image of the desired destination. By combining a learned policy with a
topological graph constructed out of previously observed data, our system can
determine how to reach this visually indicated goal even in the presence of
variable appearance and lighting. Three key insights, waypoint proposal, graph
pruning and negative mining, enable our method to learn to navigate in
real-world environments using only offline data, a setting where prior methods
struggle. We instantiate our method on a real outdoor ground robot and show
that our system, which we call ViNG, outperforms previously-proposed methods
for goal-conditioned reinforcement learning, including other methods that
incorporate reinforcement learning and search. We also study how \sysName
generalizes to unseen environments and evaluate its ability to adapt to such an
environment with growing experience. Finally, we demonstrate ViNG on a number
of real-world applications, such as last-mile delivery and warehouse
inspection. We encourage the reader to visit the project website for videos of
our experiments and demonstrations sites.google.com/view/ving-robot.",arxiv
http://arxiv.org/abs/2007.00491v1,2020-07-01T13:49:56Z,2020-07-01T13:49:56Z,"Optimisation of a Siamese Neural Network for Real-Time Energy Efficient
  Object Tracking","In this paper the research on optimisation of visual object tracking using a
Siamese neural network for embedded vision systems is presented. It was assumed
that the solution shall operate in real-time, preferably for a high resolution
video stream, with the lowest possible energy consumption. To meet these
requirements, techniques such as the reduction of computational precision and
pruning were considered. Brevitas, a tool dedicated for optimisation and
quantisation of neural networks for FPGA implementation, was used. A number of
training scenarios were tested with varying levels of optimisations - from
integer uniform quantisation with 16 bits to ternary and binary networks. Next,
the influence of these optimisations on the tracking performance was evaluated.
It was possible to reduce the size of the convolutional filters up to 10 times
in relation to the original network. The obtained results indicate that using
quantisation can significantly reduce the memory and computational complexity
of the proposed network while still enabling precise tracking, thus allow to
use it in embedded vision systems. Moreover, quantisation of weights positively
affects the network training by decreasing overfitting.",arxiv
http://arxiv.org/abs/1910.06540v1,2019-10-15T05:44:28Z,2019-10-15T05:44:28Z,"Real-time monitoring of driver drowsiness on mobile platforms using 3D
  neural networks","Driver drowsiness increases crash risk, leading to substantial road trauma
each year. Drowsiness detection methods have received considerable attention,
but few studies have investigated the implementation of a detection approach on
a mobile phone. Phone applications reduce the need for specialised hardware and
hence, enable a cost-effective roll-out of the technology across the driving
population. While it has been shown that three-dimensional (3D) operations are
more suitable for spatiotemporal feature learning, current methods for
drowsiness detection commonly use frame-based, multi-step approaches. However,
computationally expensive techniques that achieve superior results on action
recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create
bottlenecks for real-time, safety-critical applications on mobile devices.
Here, we show how depthwise separable 3D convolutions, combined with an early
fusion of spatial and temporal information, can achieve a balance between high
prediction accuracy and real-time inference requirements. In particular,
increased accuracy is achieved when assessment requires motion information, for
example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based
smartphone application shows the true impact of various approaches on inference
times and demonstrates the effectiveness of real-time monitoring based on
out-of-sample data to alert a drowsy driver. Our model is pre-trained on
ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness
Detection dataset. Fine-tuning on large naturalistic driving datasets could
further improve accuracy to obtain robust in-vehicle performance. Overall, our
research is a step towards practical deep learning applications, potentially
preventing micro-sleeps and reducing road trauma.",arxiv
http://arxiv.org/abs/1802.03515v5,2018-11-11T04:53:47Z,2018-02-10T03:56:19Z,Vehicle Pose and Shape Estimation through Multiple Monocular Vision,"In this paper, we present an accurate approach to estimate vehicles' pose and
shape from off-board multiview images. The images are taken by monocular
cameras and have small overlaps. We utilize state-of-the-art convolutional
neural networks (CNNs) to extract vehicles' semantic keypoints and introduce a
Cross Projection Optimization (CPO) method to estimate the 3D pose. During the
iterative CPO process, an adaptive shape adjustment method named Hierarchical
Wireframe Constraint (HWC) is implemented to estimate the shape. Our approach
is evaluated under both simulated and real-world scenes for performance
verification. It's shown that our algorithm outperforms other existing
monocular and stereo methods for vehicles' pose and shape estimation. This
approach provides a new and robust solution for off-board visual vehicle
localization and tracking, which can be applied to massive surveillance camera
networks for intelligent transportation.",arxiv
http://arxiv.org/abs/2007.00493v1,2020-07-01T13:50:42Z,2020-07-01T13:50:42Z,"Optimisation of the PointPillars network for 3D object detection in
  point clouds","In this paper we present our research on the optimisation of a deep neural
network for 3D object detection in a point cloud. Techniques like quantisation
and pruning available in the Brevitas and PyTorch tools were used. We performed
the experiments for the PointPillars network, which offers a reasonable
compromise between detection accuracy and calculation complexity. The aim of
this work was to propose a variant of the network which we will ultimately
implement in an FPGA device. This will allow for real-time LiDAR data
processing with low energy consumption. The obtained results indicate that even
a significant quantisation from 32-bit floating point to 2-bit integer in the
main part of the algorithm, results in 5%-9% decrease of the detection
accuracy, while allowing for almost a 16-fold reduction in size of the model.",arxiv
http://arxiv.org/abs/1812.03622v2,2019-03-01T03:29:53Z,2018-12-10T04:53:14Z,3D Scene Parsing via Class-Wise Adaptation,"We propose the method that uses only computer graphics datasets to parse the
real world 3D scenes. 3D scene parsing based on semantic segmentation is
required to implement the categorical interaction in the virtual world.
Convolutional Neural Networks (CNNs) have recently shown state-of-theart
performance on computer vision tasks including semantic segmentation. However,
collecting and annotating a huge amount of data are needed to train CNNs.
Especially in the case of semantic segmentation, annotating pixel by pixel
takes a significant amount of time and often makes mistakes. In contrast,
computer graphics can generate a lot of accurate annotated data and easily
scale up by changing camera positions, textures and lights. Despite these
advantages, models trained on computer graphics datasets cannot perform well on
real data, which is known as the domain shift. To address this issue, we first
present that depth modal and synthetic noise are effective to reduce the domain
shift. Then, we develop the class-wise adaptation which obtains domain
invariant features of CNNs. To reduce the domain shift, we create computer
graphics rooms with a lot of props, and provide photo-realistic rendered
images.We also demonstrate the application which is combined semantic
segmentation with Simultaneous Localization and Mapping (SLAM). Our application
performs accurate 3D scene parsing in real-time on an actual room.",arxiv
http://arxiv.org/abs/1909.12217v4,2021-01-25T21:19:51Z,2019-09-26T16:15:37Z,"Visual Exploration and Energy-aware Path Planning via Reinforcement
  Learning","Visual exploration and smart data collection via autonomous vehicles is an
attractive topic in various disciplines. Disturbances like wind significantly
influence both the power consumption of the flying robots and the performance
of the camera. We propose a reinforcement learning approach which combines the
effects of the power consumption and the object detection modules to develop a
policy for object detection in large areas with limited battery life. The
learning model enables dynamic learning of the negative rewards of each action
based on the drag forces that is resulted by the motion of the flying robot
with respect to the wind field. The algorithm is implemented in a near-real
world simulation environment both for the planar motion and flight in different
altitudes. The trained agent often performed a trade-off between detecting the
objects with high accuracy and increasing the area coverage within its battery
life. The developed exploration policy outperformed the complete coverage
algorithm by minimizing the traveled path while finding the target objects. The
performance of the algorithms under various wind fields was evaluated in planar
and 3D motion. During an exploration task with sparsely distributed goals and
within a UAV's battery life, the proposed architecture could detect more than
twice the amount of goal objects compared to the coverage path planning
algorithm in moderate wind field. In high wind intensities, the energy-aware
algorithm could detect 4 times the amount of goal objects when compared to its
complete coverage counterpart.",arxiv
http://arxiv.org/abs/2005.11487v1,2020-05-23T07:36:23Z,2020-05-23T07:36:23Z,Self-Training for Domain Adaptive Scene Text Detection,"Though deep learning based scene text detection has achieved great progress,
well-trained detectors suffer from severe performance degradation for different
domains. In general, a tremendous amount of data is indispensable to train the
detector in the target domain. However, data collection and annotation are
expensive and time-consuming. To address this problem, we propose a
self-training framework to automatically mine hard examples with pseudo-labels
from unannotated videos or images. To reduce the noise of hard examples, a
novel text mining module is implemented based on the fusion of detection and
tracking results. Then, an image-to-video generation method is designed for the
tasks that videos are unavailable and only images can be used. Experimental
results on standard benchmarks, including ICDAR2015, MSRA-TD500, ICDAR2017 MLT,
demonstrate the effectiveness of our self-training method. The simple Mask
R-CNN adapted with self-training and fine-tuned on real data can achieve
comparable or even superior results with the state-of-the-art methods.",arxiv
http://arxiv.org/abs/1904.08500v1,2019-04-01T05:38:59Z,2019-04-01T05:38:59Z,"Machine Vision for Natural Gas Methane Emissions Detection Using an
  Infrared Camera","It is crucial to reduce natural gas methane emissions, which can potentially
offset the climate benefits of replacing coal with gas. Optical gas imaging
(OGI) is a widely-used method to detect methane leaks, but is labor-intensive
and cannot provide leak detection results without operators' judgment. In this
paper, we develop a computer vision approach to OGI-based leak detection using
convolutional neural networks (CNN) trained on methane leak images to enable
automatic detection. First, we collect ~1 M frames of labeled video of methane
leaks from different leaking equipment for building CNN model, covering a wide
range of leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m).
Second, we examine different background subtraction methods to extract the
methane plume in the foreground. Third, we then test three CNN model variants,
collectively called GasNet, to detect plumes in videos taken at other pieces of
leaking equipment. We assess the ability of GasNet to perform leak detection by
comparing it to a baseline method that uses optical-flow based change detection
algorithm. We explore the sensitivity of results to the CNN structure, with a
moderate-complexity variant performing best across distances. We find that the
detection accuracy can reach as high as 99%, the overall detection accuracy can
exceed 95% for a case across all leak sizes and imaging distances. Binary
detection accuracy exceeds 97% for large leaks (~710 gCH4/h) imaged closely
(~5-7 m). At closer imaging distances (~5-10 m), CNN-based models have greater
than 94% accuracy across all leak sizes. At farthest distances (~13-16 m),
performance degrades rapidly, but it can achieve above 95% accuracy to detect
large leaks (>950 gCH4/h). The GasNet-based computer vision approach could be
deployed in OGI surveys to allow automatic vigilance of methane leak detection
with high detection accuracy in the real world.",arxiv
http://arxiv.org/abs/1508.01292v3,2015-11-23T20:01:06Z,2015-08-06T07:01:55Z,Compact Convolutional Neural Network Cascade for Face Detection,"The problem of faces detection in images or video streams is a classical
problem of computer vision. The multiple solutions of this problem have been
proposed, but the question of their optimality is still open. Many algorithms
achieve a high quality face detection, but at the cost of high computational
complexity. This restricts their application in the real-time systems. This
paper presents a new solution of the frontal face detection problem based on
compact convolutional neural networks cascade. The test results on FDDB dataset
show that it is competitive with state-of-the-art algorithms. This proposed
detector is implemented using three technologies: SSE/AVX/AVX2 instruction sets
for Intel CPUs, Nvidia CUDA, OpenCL. The detection speed of our approach
considerably exceeds all the existing CPU-based and GPU-based algorithms.
Because of high computational efficiency, our detector can processing 4K Ultra
HD video stream in real time (up to 27 fps) on mobile platforms (Intel Ivy
Bridge CPUs and Nvidia Kepler GPUs) in searching objects with the dimension
60x60 pixels or higher. At the same time its performance weakly dependent on
the background and number of objects in scene. This is achieved by the
asynchronous computation of stages in the cascade.",arxiv
http://arxiv.org/abs/1712.02294v4,2018-07-12T14:11:40Z,2017-12-06T17:20:21Z,Joint 3D Proposal Generation and Object Detection from View Aggregation,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",arxiv
http://arxiv.org/abs/1705.06599v1,2017-05-17T03:04:43Z,2017-05-17T03:04:43Z,Localized LRR on Grassmann Manifolds: An Extrinsic View,"Subspace data representation has recently become a common practice in many
computer vision tasks. It demands generalizing classical machine learning
algorithms for subspace data. Low-Rank Representation (LRR) is one of the most
successful models for clustering vectorial data according to their subspace
structures. This paper explores the possibility of extending LRR for subspace
data on Grassmann manifolds. Rather than directly embedding the Grassmann
manifolds into the symmetric matrix space, an extrinsic view is taken to build
the LRR self-representation in the local area of the tangent space at each
Grassmannian point, resulting in a localized LRR method on Grassmann manifolds.
A novel algorithm for solving the proposed model is investigated and
implemented. The performance of the new clustering algorithm is assessed
through experiments on several real-world datasets including MNIST handwritten
digits, ballet video clips, SKIG action clips, DynTex++ dataset and highway
traffic video clips. The experimental results show the new method outperforms a
number of state-of-the-art clustering methods",arxiv
http://arxiv.org/abs/1910.13676v1,2019-10-30T05:13:33Z,2019-10-30T05:13:33Z,Multi Modal Semantic Segmentation using Synthetic Data,"Semantic understanding of scenes in three-dimensional space (3D) is a
quintessential part of robotics oriented applications such as autonomous
driving as it provides geometric cues such as size, orientation and true
distance of separation to objects which are crucial for taking mission critical
decisions. As a first step, in this work we investigate the possibility of
semantically classifying different parts of a given scene in 3D by learning the
underlying geometric context in addition to the texture cues BUT in the absence
of labelled real-world datasets. To this end we generate a large number of
synthetic scenes, their pixel-wise labels and corresponding 3D representations
using CARLA software framework. We then build a deep neural network that learns
underlying category specific 3D representation and texture cues from color
information of the rendered synthetic scenes. Further on we apply the learned
model on different real world datasets to evaluate its performance. Our
preliminary investigation of results show that the neural network is able to
learn the geometric context from synthetic scenes and effectively apply this
knowledge to classify each point of a 3D representation of a scene in
real-world.",arxiv
http://arxiv.org/abs/1902.09868v2,2019-03-12T14:20:04Z,2019-02-26T11:23:54Z,"RepNet: Weakly Supervised Training of an Adversarial Reprojection
  Network for 3D Human Pose Estimation","This paper addresses the problem of 3D human pose estimation from single
images. While for a long time human skeletons were parameterized and fitted to
the observation by satisfying a reprojection error, nowadays researchers
directly use neural networks to infer the 3D pose from the observations.
However, most of these approaches ignore the fact that a reprojection
constraint has to be satisfied and are sensitive to overfitting. We tackle the
overfitting problem by ignoring 2D to 3D correspondences. This efficiently
avoids a simple memorization of the training data and allows for a weakly
supervised training. One part of the proposed reprojection network (RepNet)
learns a mapping from a distribution of 2D poses to a distribution of 3D poses
using an adversarial training approach. Another part of the network estimates
the camera. This allows for the definition of a network layer that performs the
reprojection of the estimated 3D pose back to 2D which results in a
reprojection loss function. Our experiments show that RepNet generalizes well
to unknown data and outperforms state-of-the-art methods when applied to unseen
data. Moreover, our implementation runs in real-time on a standard desktop PC.",arxiv
http://arxiv.org/abs/2002.10718v2,2020-06-26T07:43:00Z,2020-02-25T08:04:31Z,"Denoising IMU Gyroscopes with Deep Learning for Open-Loop Attitude
  Estimation","This paper proposes a learning method for denoising gyroscopes of Inertial
Measurement Units (IMUs) using ground truth data, and estimating in real time
the orientation (attitude) of a robot in dead reckoning. The obtained algorithm
outperforms the state-of-the-art on the (unseen) test sequences. The obtained
performances are achieved thanks to a well-chosen model, a proper loss function
for orientation increments, and through the identification of key points when
training with high-frequency inertial data. Our approach builds upon a neural
network based on dilated convolutions, without requiring any recurrent neural
network. We demonstrate how efficient our strategy is for 3D attitude
estimation on the EuRoC and TUM-VI datasets. Interestingly, we observe our dead
reckoning algorithm manages to beat top-ranked visual-inertial odometry systems
in terms of attitude estimation although it does not use vision sensors. We
believe this paper offers new perspectives for visual-inertial localization and
constitutes a step toward more efficient learning methods involving IMUs. Our
open-source implementation is available at
https://github.com/mbrossar/denoise-imu-gyro.",arxiv
http://arxiv.org/abs/2108.03173v1,2021-08-04T09:03:53Z,2021-08-04T09:03:53Z,"Incremental learning of LSTM framework for sensor fusion in attitude
  estimation","This paper presents a novel method for attitude estimation of an object in 3D
space by incremental learning of the Long-Short Term Memory (LSTM) network.
Gyroscope, accelerometer, and magnetometer are few widely used sensors in
attitude estimation applications. Traditionally, multi-sensor fusion methods
such as the Extended Kalman Filter and Complementary Filter are employed to
fuse the measurements from these sensors. However, these methods exhibit
limitations in accounting for the uncertainty, unpredictability, and dynamic
nature of the motion in real-world situations. In this paper, the inertial
sensors data are fed to the LSTM network which are then updated incrementally
to incorporate the dynamic changes in motion occurring in the run time. The
robustness and efficiency of the proposed framework is demonstrated on the
dataset collected from a commercially available inertial measurement unit. The
proposed framework offers a significant improvement in the results compared to
the traditional method, even in the case of a highly dynamic environment. The
LSTM framework-based attitude estimation approach can be deployed on a standard
AI-supported processing module for real-time applications.",arxiv
http://arxiv.org/abs/1710.09975v1,2017-10-27T03:32:48Z,2017-10-27T03:32:48Z,"A Single-Channel Architecture for Algebraic Integer Based 8$\times$8 2-D
  DCT Computation","An area efficient row-parallel architecture is proposed for the real-time
implementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine
transform (DCT) for image and video processing. The proposed architecture
computes 8$\times$8 2-D DCT transform based on the Arai DCT algorithm. An
improved fast algorithm for AI based 1-D DCT computation is proposed along with
a single channel 2-D DCT architecture. The design improves on the 4-channel AI
DCT architecture that was published recently by reducing the number of integer
channels to one and the number of 8-point 1-D DCT cores from 5 down to 2. The
architecture offers exact computation of 8$\times$8 blocks of the 2-D DCT
coefficients up to the FRS, which converts the coefficients from the AI
representation to fixed-point format using the method of expansion factors.
Prototype circuits corresponding to FRS blocks based on two expansion factors
are realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6
XC6VLX240T device. Post place-and-route results show a 20% reduction in terms
of area compared to the 2-D DCT architecture requiring five 1-D AI cores. The
area-time and area-time${}^2$ complexity metrics are also reduced by 23% and
22% respectively for designs with 8-bit input word length. The digital
realizations are simulated up to place and route for ASICs using 45 nm CMOS
standard cells. The maximum estimated clock rate is 951 MHz for the CMOS
realizations indicating 7.608$\cdot$10$^9$ pixels/seconds and a 8$\times$8
block rate of 118.875 MHz.",arxiv
http://arxiv.org/abs/1802.10408v3,2018-09-24T20:22:52Z,2018-02-28T13:49:18Z,"A Neurorobotic Experiment for Crossmodal Conflict Resolution in Complex
  Environments","Crossmodal conflict resolution is crucial for robot sensorimotor coupling
through the interaction with the environment, yielding swift and robust
behaviour also in noisy conditions. In this paper, we propose a neurorobotic
experiment in which an iCub robot exhibits human-like responses in a complex
crossmodal environment. To better understand how humans deal with multisensory
conflicts, we conducted a behavioural study exposing 33 subjects to congruent
and incongruent dynamic audio-visual cues. In contrast to previous studies
using simplified stimuli, we designed a scenario with four animated avatars and
observed that the magnitude and extension of the visual bias are related to the
semantics embedded in the scene, i.e., visual cues that are congruent with
environmental statistics (moving lips and vocalization) induce the strongest
bias. We implement a deep learning model that processes stereophonic sound,
facial features, and body motion to trigger a discrete behavioural response.
After training the model, we exposed the iCub to the same experimental
conditions as the human subjects, showing that the robot can replicate similar
responses in real time. Our interdisciplinary work provides important insights
into how crossmodal conflict resolution can be modelled in robots and
introduces future research directions for the efficient combination of sensory
observations with internally generated knowledge and expectations.",arxiv
http://arxiv.org/abs/1802.01144v2,2018-02-12T06:49:38Z,2018-02-04T15:25:52Z,"Human Action Adverb Recognition: ADHA Dataset and A Three-Stream Hybrid
  Model","We introduce the first benchmark for a new problem --- recognizing human
action adverbs (HAA): ""Adverbs Describing Human Actions"" (ADHA). This is the
first step for computer vision to change over from pattern recognition to real
AI. We demonstrate some key features of ADHA: a semantically complete set of
adverbs describing human actions, a set of common, describable human actions,
and an exhaustive labeling of simultaneously emerging actions in each video. We
commit an in-depth analysis on the implementation of current effective models
in action recognition and image captioning on adverb recognition, and the
results show that such methods are unsatisfactory. Moreover, we propose a novel
three-stream hybrid model to deal the HAA problem, which achieves a better
result.",arxiv
http://arxiv.org/abs/1808.03941v2,2019-05-05T03:42:19Z,2018-08-12T13:30:27Z,"Denoising of 3-D Magnetic Resonance Images Using a Residual
  Encoder-Decoder Wasserstein Generative Adversarial Network","Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images
is a critical step in medical image analysis. Over the past few years, many
algorithms with impressive performances have been proposed. In this paper,
inspired by the idea of deep learning, we introduce an MRI denoising method
based on the residual encoder-decoder Wasserstein generative adversarial
network (RED-WGAN). Specifically, to explore the structure similarity between
neighboring slices, a 3D configuration is utilized as the basic processing
unit. Residual autoencoders combined with deconvolution operations are
introduced into the generator network. Furthermore, to alleviate the
oversmoothing shortcoming of the traditional mean squared error (MSE) loss
function, the perceptual similarity, which is implemented by calculating the
distances in the feature space extracted by a pretrained VGG-19 network, is
incorporated with the MSE and adversarial losses to form the new loss function.
Extensive experiments are implemented to assess the performance of the proposed
method. The experimental results show that the proposed RED-WGAN achieves
performance superior to several state-of-the-art methods in both simulated and
real clinical data. In particular, our method demonstrates powerful abilities
in both noise suppression and structure preservation.",arxiv
http://arxiv.org/abs/1606.01393v1,2016-06-04T16:51:37Z,2016-06-04T16:51:37Z,"Automated Image Captioning for Rapid Prototyping and Resource
  Constrained Environments","Significant performance gains in deep learning coupled with the exponential
growth of image and video data on the Internet have resulted in the recent
emergence of automated image captioning systems. Ensuring scalability of
automated image captioning systems with respect to the ever increasing volume
of image and video data is a significant challenge. This paper provides a
valuable insight in that the detection of a few significant (top) objects in an
image allows one to extract other relevant information such as actions (verbs)
in the image. We expect this insight to be useful in the design of scalable
image captioning systems. We address two parameters by which the scalability of
image captioning systems could be quantified, i.e., the traditional algorithmic
time complexity which is important given the resource limitations of the user
device and the system development time since the programmers' time is a
critical resource constraint in many real-world scenarios. Additionally, we
address the issue of how word embeddings could be used to infer the verb
(action) from the nouns (objects) in a given image in a zero-shot manner. Our
results show that it is possible to attain reasonably good performance on
predicting actions and captioning images using our approaches with the added
advantage of simplicity of implementation.",arxiv
http://arxiv.org/abs/1911.01921v1,2019-11-05T16:31:23Z,2019-11-05T16:31:23Z,DLA: Dense-Layer-Analysis for Adversarial Example Detection,"In recent years Deep Neural Networks (DNNs) have achieved remarkable results
and even showed super-human capabilities in a broad range of domains. This led
people to trust in DNNs' classifications and resulting actions even in
security-sensitive environments like autonomous driving.
  Despite their impressive achievements, DNNs are known to be vulnerable to
adversarial examples. Such inputs contain small perturbations to intentionally
fool the attacked model.
  In this paper, we present a novel end-to-end framework to detect such attacks
during classification without influencing the target model's performance.
Inspired by recent research in neuron-coverage guided testing we show that
dense layers of DNNs carry security-sensitive information. With a secondary DNN
we analyze the activation patterns of the dense layers during classification
runtime, which enables effective and real-time detection of adversarial
examples.
  Our prototype implementation successfully detects adversarial examples in
image, natural language, and audio processing. Thereby, we cover a variety of
target DNNs, including Long Short Term Memory (LSTM) architectures. In
addition, to effectively defend against state-of-the-art attacks, our approach
generalizes between different sets of adversarial examples. Thus, our method
most likely enables us to detect even future, yet unknown attacks. Finally,
during white-box adaptive attacks, we show our method cannot be easily
bypassed.",arxiv
http://arxiv.org/abs/2007.08389v2,2020-08-27T00:33:27Z,2020-07-16T15:07:14Z,"Device-Robust Acoustic Scene Classification Based on Two-Stage
  Categorization and Data Augmentation","In this technical report, we present a joint effort of four groups, namely
GT, USTC, Tencent, and UKE, to tackle Task 1 - Acoustic Scene Classification
(ASC) in the DCASE 2020 Challenge. Task 1 comprises two different sub-tasks:
(i) Task 1a focuses on ASC of audio signals recorded with multiple (real and
simulated) devices into ten different fine-grained classes, and (ii) Task 1b
concerns with classification of data into three higher-level classes using
low-complexity solutions. For Task 1a, we propose a novel two-stage ASC system
leveraging upon ad-hoc score combination of two convolutional neural networks
(CNNs), classifying the acoustic input according to three classes, and then ten
classes, respectively. Four different CNN-based architectures are explored to
implement the two-stage classifiers, and several data augmentation techniques
are also investigated. For Task 1b, we leverage upon a quantization method to
reduce the complexity of two of our top-accuracy three-classes CNN-based
architectures. On Task 1a development data set, an ASC accuracy of 76.9\% is
attained using our best single classifier and data augmentation. An accuracy of
81.9\% is then attained by a final model fusion of our two-stage ASC
classifiers. On Task 1b development data set, we achieve an accuracy of 96.7\%
with a model size smaller than 500KB. Code is available:
https://github.com/MihawkHu/DCASE2020_task1.",arxiv
http://arxiv.org/abs/1909.11799v4,2021-08-07T16:30:21Z,2019-09-25T22:28:47Z,"Manifold Oblique Random Forests: Towards Closing the Gap on
  Convolutional Deep Networks","Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification.",arxiv
http://arxiv.org/abs/1908.11312v1,2019-08-29T15:53:40Z,2019-08-29T15:53:40Z,"Flexible Conditional Image Generation of Missing Data with Learned
  Mental Maps","Real-world settings often do not allow acquisition of high-resolution
volumetric images for accurate morphological assessment and diagnostic. In
clinical practice it is frequently common to acquire only sparse data (e.g.
individual slices) for initial diagnostic decision making. Thereby, physicians
rely on their prior knowledge (or mental maps) of the human anatomy to
extrapolate the underlying 3D information. Accurate mental maps require years
of anatomy training, which in the first instance relies on normative learning,
i.e. excluding pathology. In this paper, we leverage Bayesian Deep Learning and
environment mapping to generate full volumetric anatomy representations from
none to a small, sparse set of slices. We evaluate proof of concept
implementations based on Generative Query Networks (GQN) and Conditional BRUNO
using abdominal CT and brain MRI as well as in a clinical application involving
sparse, motion-corrupted MR acquisition for fetal imaging. Our approach allows
to reconstruct 3D volumes from 1 to 4 tomographic slices, with a SSIM of 0.7+
and cross-correlation of 0.8+ compared to the 3D ground truth.",arxiv
http://arxiv.org/abs/2111.02156v1,2021-11-03T11:53:57Z,2021-11-03T11:53:57Z,"Continual Learning of Semantic Segmentation using Complementary 2D-3D
  Data Representations","Semantic segmentation networks are usually pre-trained and not updated during
deployment. As a consequence, misclassifications commonly occur if the
distribution of the training data deviates from the one encountered during the
robot's operation. We propose to mitigate this problem by adapting the neural
network to the robot's environment during deployment, without any need for
external supervision. Leveraging complementary data representations, we
generate a supervision signal, by probabilistically accumulating consecutive 2D
semantic predictions in a volumetric 3D map. We then retrain the network on
renderings of the accumulated semantic map, effectively resolving ambiguities
and enforcing multi-view consistency through the 3D representation. To preserve
the previously-learned knowledge while performing network adaptation, we employ
a continual learning strategy based on experience replay. Through extensive
experimental evaluation, we show successful adaptation to real-world indoor
scenes both on the ScanNet dataset and on in-house data recorded with an RGB-D
sensor. Our method increases the segmentation performance on average by 11.8%
compared to the fixed pre-trained neural network, while effectively retaining
knowledge from the pre-training dataset.",arxiv
http://arxiv.org/abs/1904.05191v3,2019-04-16T07:53:34Z,2019-04-10T13:55:10Z,"Weakly-Supervised White and Grey Matter Segmentation in 3D Brain
  Ultrasound","Although the segmentation of brain structures in ultrasound helps initialize
image based registration, assist brain shift compensation, and provides
interventional decision support, the task of segmenting grey and white matter
in cranial ultrasound is very challenging and has not been addressed yet. We
train a multi-scale fully convolutional neural network simultaneously for two
classes in order to segment real clinical 3D ultrasound data. Parallel pathways
working at different levels of resolution account for high frequency speckle
noise and global 3D image features. To ensure reproducibility, the publicly
available RESECT dataset is utilized for training and cross-validation. Due to
the absence of a ground truth, we train with weakly annotated label. We
implement label transfer from MRI to US, which is prone to a residual but
inevitable registration error. To further improve results, we perform transfer
learning using synthetic US data. The resulting method leads to excellent Dice
scores of 0.7080, 0.8402 and 0.9315 for grey matter, white matter and
background. Our proposed methodology sets an unparalleled standard for white
and grey matter segmentation in 3D intracranial ultrasound.",arxiv
http://arxiv.org/abs/1909.05776v1,2019-09-12T16:14:37Z,2019-09-12T16:14:37Z,"I-SAFE: Instant Suspicious Activity identiFication at the Edge using
  Fuzzy Decision Making","Urban imagery usually serves as forensic analysis and by design is available
for incident mitigation. As more imagery collected, it is harder to narrow down
to certain frames among thousands of video clips to a specific incident. A
real-time, proactive surveillance system is desirable, which could instantly
detect dubious personnel, identify suspicious activities, or raise momentous
alerts. The recent proliferation of the edge computing paradigm allows more
data-intensive tasks to be accomplished by smart edge devices with lightweight
but powerful algorithms. This paper presents a forensic surveillance strategy
by introducing an Instant Suspicious Activity identiFication at the Edge
(I-SAFE) using fuzzy decision making. A fuzzy control system is proposed to
mimic the decision-making process of a security officer. Decisions are made
based on video features extracted by a lightweight Deep Machine Learning (DML)
model. Based on the requirements from the first-line law enforcement officers,
several features are selected and fuzzified to cope with the state of
uncertainty that exists in the officers' decision-making process. Using
features in the edge hierarchy minimizes the communication delay such that
instant alerting is achieved. Additionally, leveraging the Microservices
architecture, the I-SAFE scheme possesses good scalability given the increasing
complexities at the network edge. Implemented as an edge-based application and
tested using exemplary and various labeled dataset surveillance videos, the
I-SAFE scheme raises alerts by identifying the suspicious activity in an
average of 0.002 seconds. Compared to four other state-of-the-art methods over
two other data sets, the experimental study verified the superiority of the
I-SAFE decentralized method.",arxiv
http://arxiv.org/abs/2010.12025v3,2021-05-07T08:59:17Z,2020-10-22T20:16:36Z,Combination of Deep Speaker Embeddings for Diarisation,"Significant progress has recently been made in speaker diarisation after the
introduction of d-vectors as speaker embeddings extracted from neural network
(NN) speaker classifiers for clustering speech segments. To extract
better-performing and more robust speaker embeddings, this paper proposes a
c-vector method by combining multiple sets of complementary d-vectors derived
from systems with different NN components. Three structures are used to
implement the c-vectors, namely 2D self-attentive, gated additive, and bilinear
pooling structures, relying on attention mechanisms, a gating mechanism, and a
low-rank bilinear pooling mechanism respectively. Furthermore, a neural-based
single-pass speaker diarisation pipeline is also proposed in this paper, which
uses NNs to achieve voice activity detection, speaker change point detection,
and speaker embedding extraction. Experiments and detailed analyses are
conducted on the challenging AMI and NIST RT05 datasets which consist of real
meetings with 4--10 speakers and a wide range of acoustic conditions. For
systems trained on the AMI training set, relative speaker error rate (SER)
reductions of 13% and 29% are obtained by using c-vectors instead of d-vectors
on the AMI dev and eval sets respectively, and a relative reduction of 15% in
SER is observed on RT05, which shows the robustness of the proposed methods. By
incorporating VoxCeleb data into the training set, the best c-vector system
achieved 7%, 17% and16% relative SER reduction compared to the d-vector on the
AMI dev, eval, and RT05 sets respectively",arxiv
http://arxiv.org/abs/2010.13070v1,2020-10-25T08:55:40Z,2020-10-25T08:55:40Z,Dynamic Adversarial Patch for Evading Object Detection Models,"Recent research shows that neural networks models used for computer vision
(e.g., YOLO and Fast R-CNN) are vulnerable to adversarial evasion attacks. Most
of the existing real-world adversarial attacks against object detectors use an
adversarial patch which is attached to the target object (e.g., a carefully
crafted sticker placed on a stop sign). This method may not be robust to
changes in the camera's location relative to the target object; in addition, it
may not work well when applied to nonplanar objects such as cars. In this
study, we present an innovative attack method against object detectors applied
in a real-world setup that addresses some of the limitations of existing
attacks. Our method uses dynamic adversarial patches which are placed at
multiple predetermined locations on a target object. An adversarial learning
algorithm is applied in order to generate the patches used. The dynamic attack
is implemented by switching between optimized patches dynamically, according to
the camera's position (i.e., the object detection system's position). In order
to demonstrate our attack in a real-world setup, we implemented the patches by
attaching flat screens to the target object; the screens are used to present
the patches and switch between them, depending on the current camera location.
Thus, the attack is dynamic and adjusts itself to the situation to achieve
optimal results. We evaluated our dynamic patch approach by attacking the
YOLOv2 object detector with a car as the target object and succeeded in
misleading it in up to 90% of the video frames when filming the car from a wide
viewing angle range. We improved the attack by generating patches that consider
the semantic distance between the target object and its classification. We also
examined the attack's transferability among different car models and were able
to mislead the detector 71% of the time.",arxiv
http://arxiv.org/abs/2008.10454v2,2020-09-04T07:55:11Z,2020-08-24T13:55:14Z,"FOCAL: A Forgery Localization Framework based on Video Coding
  Self-Consistency","Forgery operations on video contents are nowadays within the reach of anyone,
thanks to the availability of powerful and user-friendly editing software.
Integrity verification and authentication of videos represent a major interest
in both journalism (e.g., fake news debunking) and legal environments dealing
with digital evidence (e.g., a court of law). While several strategies and
different forensics traces have been proposed in recent years, latest solutions
aim at increasing the accuracy by combining multiple detectors and features.
This paper presents a video forgery localization framework that verifies the
self-consistency of coding traces between and within video frames, by fusing
the information derived from a set of independent feature descriptors. The
feature extraction step is carried out by means of an explainable convolutional
neural network architecture, specifically designed to look for and classify
coding artifacts. The overall framework was validated in two typical forgery
scenarios: temporal and spatial splicing. Experimental results show an
improvement to the state-of-the-art on temporal splicing localization and also
promising performance in the newly tackled case of spatial splicing, on both
synthetic and real-world videos.",arxiv
http://arxiv.org/abs/1610.07104v1,2016-10-22T23:08:01Z,2016-10-22T23:08:01Z,Independent Component Analysis by Entropy Maximization with Kernels,"Independent component analysis (ICA) is the most popular method for blind
source separation (BSS) with a diverse set of applications, such as biomedical
signal processing, video and image analysis, and communications. Maximum
likelihood (ML), an optimal theoretical framework for ICA, requires knowledge
of the true underlying probability density function (PDF) of the latent
sources, which, in many applications, is unknown. ICA algorithms cast in the ML
framework often deviate from its theoretical optimality properties due to poor
estimation of the source PDF. Therefore, accurate estimation of source PDFs is
critical in order to avoid model mismatch and poor ICA performance. In this
paper, we propose a new and efficient ICA algorithm based on entropy
maximization with kernels, (ICA-EMK), which uses both global and local
measuring functions as constraints to dynamically estimate the PDF of the
sources with reasonable complexity. In addition, the new algorithm performs
optimization with respect to each of the cost function gradient directions
separately, enabling parallel implementations on multi-core computers. We
demonstrate the superior performance of ICA-EMK over competing ICA algorithms
using simulated as well as real-world data.",arxiv
http://arxiv.org/abs/2004.14674v2,2020-05-02T12:33:08Z,2020-04-30T10:28:08Z,SS3D: Single Shot 3D Object Detector,"Single stage deep learning algorithm for 2D object detection was made popular
by Single Shot MultiBox Detector (SSD) and it was heavily adopted in several
embedded applications. PointPillars is a state of the art 3D object detection
algorithm that uses a Single Shot Detector adapted for 3D object detection. The
main downside of PointPillars is that it has a two stage approach with learned
input representation based on fully connected layers followed by the Single
Shot Detector for 3D detection. In this paper we present Single Shot 3D Object
Detection (SS3D) - a single stage 3D object detection algorithm which combines
straight forward, statistically computed input representation and a Single Shot
Detector (based on PointPillars). Computing the input representation is
straight forward, does not involve learning and does not have much
computational cost. We also extend our method to stereo input and show that,
aided by additional semantic segmentation input; our method produces similar
accuracy as state of the art stereo based detectors. Achieving the accuracy of
two stage detectors using a single stage approach is important as single stage
approaches are simpler to implement in embedded, real-time applications. With
LiDAR as well as stereo input, our method outperforms PointPillars. When using
LiDAR input, our input representation is able to improve the AP3D of Cars
objects in the moderate category from 74.99 to 76.84. When using stereo input,
our input representation is able to improve the AP3D of Cars objects in the
moderate category from 38.13 to 45.13. Our results are also better than other
popular 3D object detectors such as AVOD and F-PointNet.",arxiv
http://arxiv.org/abs/1601.06473v2,2016-01-26T04:43:20Z,2016-01-25T03:31:24Z,Teaching Robots to Do Object Assembly using Multi-modal 3D Vision,"The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios.",arxiv
http://arxiv.org/abs/1801.01949v2,2018-08-22T09:28:14Z,2018-01-06T00:41:10Z,"Face Flashing: a Secure Liveness Detection Protocol based on Light
  Reflections","Face authentication systems are becoming increasingly prevalent, especially
with the rapid development of Deep Learning technologies. However, human facial
information is easy to be captured and reproduced, which makes face
authentication systems vulnerable to various attacks. Liveness detection is an
important defense technique to prevent such attacks, but existing solutions did
not provide clear and strong security guarantees, especially in terms of time.
  To overcome these limitations, we propose a new liveness detection protocol
called Face Flashing that significantly increases the bar for launching
successful attacks on face authentication systems. By randomly flashing
well-designed pictures on a screen and analyzing the reflected light, our
protocol has leveraged physical characteristics of human faces: reflection
processing at the speed of light, unique textual features, and uneven 3D
shapes. Cooperating with working mechanism of the screen and digital cameras,
our protocol is able to detect subtle traces left by an attacking process.
  To demonstrate the effectiveness of Face Flashing, we implemented a prototype
and performed thorough evaluations with large data set collected from
real-world scenarios. The results show that our Timing Verification can
effectively detect the time gap between legitimate authentications and
malicious cases. Our Face Verification can also differentiate 2D plane from 3D
objects accurately. The overall accuracy of our liveness detection system is
98.8\%, and its robustness was evaluated in different scenarios. In the worst
case, our system's accuracy decreased to a still-high 97.3\%.",arxiv
http://arxiv.org/abs/1808.00259v1,2018-08-01T10:50:47Z,2018-08-01T10:50:47Z,Drone Detection Using Depth Maps,"Obstacle avoidance is a key feature for safe Unmanned Aerial Vehicle (UAV)
navigation. While solutions have been proposed for static obstacle avoidance,
systems enabling avoidance of dynamic objects, such as drones, are hard to
implement due to the detection range and field-of-view (FOV) requirements, as
well as the constraints for integrating such systems on-board small UAVs. In
this work, a dataset of 6k synthetic depth maps of drones has been generated
and used to train a state-of-the-art deep learning-based drone detection model.
While many sensing technologies can only provide relative altitude and azimuth
of an obstacle, our depth map-based approach enables full 3D localization of
the obstacle. This is extremely useful for collision avoidance, as 3D
localization of detected drones is key to perform efficient collision-free path
planning. The proposed detection technique has been validated in several real
depth map sequences, with multiple types of drones flying at up to 2 m/s,
achieving an average precision of 98.7%, an average recall of 74.7% and a
record detection range of 9.5 meters.",arxiv
http://arxiv.org/abs/2104.03657v1,2021-04-08T10:18:52Z,2021-04-08T10:18:52Z,"Dynamic Object Aware LiDAR SLAM based on Automatic Generation of
  Training Data","Highly dynamic environments, with moving objects such as cars or humans, can
pose a performance challenge for LiDAR SLAM systems that assume largely static
scenes. To overcome this challenge and support the deployment of robots in real
world scenarios, we propose a complete solution for a dynamic object aware
LiDAR SLAM algorithm. This is achieved by leveraging a real-time capable neural
network that can detect dynamic objects, thus allowing our system to deal with
them explicitly. To efficiently generate the necessary training data which is
key to our approach, we present a novel end-to-end occupancy grid based
pipeline that can automatically label a wide variety of arbitrary dynamic
objects. Our solution can thus generalize to different environments without the
need for expensive manual labeling and at the same time avoids assumptions
about the presence of a predefined set of known objects in the scene. Using
this technique, we automatically label over 12000 LiDAR scans collected in an
urban environment with a large amount of pedestrians and use this data to train
a neural network, achieving an average segmentation IoU of 0.82. We show that
explicitly dealing with dynamic objects can improve the LiDAR SLAM odometry
performance by 39.6% while yielding maps which better represent the
environments. A supplementary video as well as our test data are available
online.",arxiv
http://arxiv.org/abs/1810.11348v1,2018-10-26T14:37:45Z,2018-10-26T14:37:45Z,Security Event Recognition for Visual Surveillance,"With rapidly increasing deployment of surveillance cameras, the reliable
methods for automatically analyzing the surveillance video and recognizing
special events are demanded by different practical applications. This paper
proposes a novel effective framework for security event analysis in
surveillance videos. First, convolutional neural network (CNN) framework is
used to detect objects of interest in the given videos. Second, the owners of
the objects are recognized and monitored in real-time as well. If anyone moves
any object, this person will be verified whether he/she is its owner. If not,
this event will be further analyzed and distinguished between two different
scenes: moving the object away or stealing it. To validate the proposed
approach, a new video dataset consisting of various scenarios is constructed
for more complex tasks. For comparison purpose, the experiments are also
carried out on the benchmark databases related to the task on abandoned luggage
detection. The experimental results show that the proposed approach outperforms
the state-of-the-art methods and effective in recognizing complex security
events.",arxiv
http://arxiv.org/abs/2008.01567v3,2021-02-20T01:06:06Z,2020-08-01T02:32:43Z,"Multi-Slice Fusion for Sparse-View and Limited-Angle 4D CT
  Reconstruction","Inverse problems spanning four or more dimensions such as space, time and
other independent parameters have become increasingly important.
State-of-the-art 4D reconstruction methods use model based iterative
reconstruction (MBIR), but depend critically on the quality of the prior
modeling. Recently, plug-and-play (PnP) methods have been shown to be an
effective way to incorporate advanced prior models using state-of-the-art
denoising algorithms. However, state-of-the-art denoisers such as BM4D and deep
convolutional neural networks (CNNs) are primarily available for 2D or 3D
images and extending them to higher dimensions is difficult due to algorithmic
complexity and the increased difficulty of effective training.
  In this paper, we present multi-slice fusion, a novel algorithm for 4D
reconstruction, based on the fusion of multiple low-dimensional denoisers. Our
approach uses multi-agent consensus equilibrium (MACE), an extension of
plug-and-play, as a framework for integrating the multiple lower-dimensional
models. We apply our method to 4D cone-beam X-ray CT reconstruction for non
destructive evaluation (NDE) of samples that are dynamically moving during
acquisition. We implement multi-slice fusion on distributed, heterogeneous
clusters in order to reconstruct large 4D volumes in reasonable time and
demonstrate the inherent parallelizable nature of the algorithm. We present
simulated and real experimental results on sparse-view and limited-angle CT
data to demonstrate that multi-slice fusion can substantially improve the
quality of reconstructions relative to traditional methods, while also being
practical to implement and train.",arxiv
http://arxiv.org/abs/2005.08465v1,2020-05-18T05:49:48Z,2020-05-18T05:49:48Z,Context-aware and Scale-insensitive Temporal Repetition Counting,"Temporal repetition counting aims to estimate the number of cycles of a given
repetitive action. Existing deep learning methods assume repetitive actions are
performed in a fixed time-scale, which is invalid for the complex repetitive
actions in real life. In this paper, we tailor a context-aware and
scale-insensitive framework, to tackle the challenges in repetition counting
caused by the unknown and diverse cycle-lengths. Our approach combines two key
insights: (1) Cycle lengths from different actions are unpredictable that
require large-scale searching, but, once a coarse cycle length is determined,
the variety between repetitions can be overcome by regression. (2) Determining
the cycle length cannot only rely on a short fragment of video but a contextual
understanding. The first point is implemented by a coarse-to-fine cycle
refinement method. It avoids the heavy computation of exhaustively searching
all the cycle lengths in the video, and, instead, it propagates the coarse
prediction for further refinement in a hierarchical manner. We secondly propose
a bidirectional cycle length estimation method for a context-aware prediction.
It is a regression network that takes two consecutive coarse cycles as input,
and predicts the locations of the previous and next repetitive cycles. To
benefit the training and evaluation of temporal repetition counting area, we
construct a new and largest benchmark, which contains 526 videos with diverse
repetitive actions. Extensive experiments show that the proposed network
trained on a single dataset outperforms state-of-the-art methods on several
benchmarks, indicating that the proposed framework is general enough to capture
repetition patterns across domains.",arxiv
http://arxiv.org/abs/1909.10964v2,2019-10-19T13:57:12Z,2019-09-24T14:45:43Z,A System-Level Solution for Low-Power Object Detection,"Object detection has made impressive progress in recent years with the help
of deep learning. However, state-of-the-art algorithms are both computation and
memory intensive. Though many lightweight networks are developed for a
trade-off between accuracy and efficiency, it is still a challenge to make it
practical on an embedded device. In this paper, we present a system-level
solution for efficient object detection on a heterogeneous embedded device. The
detection network is quantized to low bits and allows efficient implementation
with shift operators. In order to make the most of the benefits of low-bit
quantization, we design a dedicated accelerator with programmable logic. Inside
the accelerator, a hybrid dataflow is exploited according to the heterogeneous
property of different convolutional layers. We adopt a straightforward but
resource-friendly column-prior tiling strategy to map the computation-intensive
convolutional layers to the accelerator that can support arbitrary feature
size. Other operations can be performed on the low-power CPU cores, and the
entire system is executed in a pipelined manner. As a case study, we evaluate
our object detection system on a real-world surveillance video with input size
of 512x512, and it turns out that the system can achieve an inference speed of
18 fps at the cost of 6.9W (with display) with an mAP of 66.4 verified on the
PASCAL VOC 2012 dataset.",arxiv
http://arxiv.org/abs/2109.05542v2,2021-10-26T14:36:25Z,2021-09-12T15:51:41Z,"Unsupervised Domain Adaptive Learning via Synthetic Data for Person
  Re-identification","Person re-identification (re-ID) has gained more and more attention due to
its widespread applications in intelligent video surveillance. Unfortunately,
the mainstream deep learning methods still need a large quantity of labeled
data to train models, and annotating data is an expensive work in real-world
scenarios. In addition, due to domain gaps between different datasets, the
performance is dramatically decreased when re-ID models pre-trained on
label-rich datasets (source domain) are directly applied to other unlabeled
datasets (target domain). In this paper, we attempt to remedy these problems
from two aspects, namely data and methodology. Firstly, we develop a data
collector to automatically generate synthetic re-ID samples in a computer game,
and construct a data labeler to simultaneously annotate them, which free humans
from heavy data collections and annotations. Based on them, we build two
synthetic person re-ID datasets with different scales, ""GSPR"" and ""mini-GSPR""
datasets. Secondly, we propose a synthesis-based multi-domain collaborative
refinement (SMCR) network, which contains a synthetic pretraining module and
two collaborative-refinement modules to implement sufficient learning for the
valuable knowledge from multiple domains. Extensive experiments show that our
proposed framework obtains significant performance improvements over the
state-of-the-art methods on multiple unsupervised domain adaptation tasks of
person re-ID.",arxiv
http://arxiv.org/abs/1910.11818v2,2019-11-01T15:31:34Z,2019-10-25T16:00:05Z,"Real-time Memory Efficient Large-pose Face Alignment via Deep
  Evolutionary Network","There is an urgent need to apply face alignment in a memory-efficient and
real-time manner due to the recent explosion of face recognition applications.
However, impact factors such as large pose variation and computational
inefficiency, still hinder its broad implementation. To this end, we propose a
computationally efficient deep evolutionary model integrated with 3D Diffusion
Heap Maps (DHM). First, we introduce a sparse 3D DHM to assist the initial
modeling process under extreme pose conditions. Afterward, a simple and
effective CNN feature is extracted and fed to Recurrent Neural Network (RNN)
for evolutionary learning. To accelerate the model, we propose an efficient
network structure to accelerate the evolutionary learning process through a
factorization strategy. Extensive experiments on three popular alignment
databases demonstrate the advantage of the proposed models over the
state-of-the-art, especially under large-pose conditions. Notably, the
computational speed of our model is 6 times faster than the state-of-the-art on
CPU and 14 times on GPU. We also discuss and analyze the limitations of our
models and future research work.",arxiv
http://arxiv.org/abs/2008.02321v2,2021-02-25T03:04:37Z,2020-08-05T19:00:36Z,"Can I Pour into It? Robot Imagining Open Containability Affordance of
  Previously Unseen Objects via Physical Simulations","Open containers, i.e., containers without covers, are an important and
ubiquitous class of objects in human life. In this letter, we propose a novel
method for robots to ""imagine"" the open containability affordance of a
previously unseen object via physical simulations. We implement our imagination
method on a UR5 manipulator. The robot autonomously scans the object with an
RGB-D camera. The scanned 3D model is used for open containability imagination
which quantifies the open containability affordance by physically simulating
dropping particles onto the object and counting how many particles are retained
in it. This quantification is used for open-container vs. non-open-container
binary classification (hereafter referred to as open container classification).
If the object is classified as an open container, the robot further imagines
pouring into the object, again using physical simulations, to obtain the
pouring position and orientation for real robot autonomous pouring. We evaluate
our method on open container classification and autonomous pouring of granular
material on a dataset containing 130 previously unseen objects with 57 object
categories. Although our proposed method uses only 11 objects for simulation
calibration (training), its open container classification aligns well with
human judgements. In addition, our method endows the robot with the capability
to autonomously pour into the 55 containers in the dataset with a very high
success rate. We also compare to a deep learning method. Results show that our
method achieves the same performance as the deep learning method on open
container classification and outperforms it on autonomous pouring. Moreover,
our method is fully explainable.",arxiv
http://arxiv.org/abs/2011.05705v1,2020-11-11T11:16:52Z,2020-11-11T11:16:52Z,"EGAD: Evolving Graph Representation Learning with Self-Attention and
  Knowledge Distillation for Live Video Streaming Events","In this study, we present a dynamic graph representation learning model on
weighted graphs to accurately predict the network capacity of connections
between viewers in a live video streaming event. We propose EGAD, a neural
network architecture to capture the graph evolution by introducing a
self-attention mechanism on the weights between consecutive graph convolutional
networks. In addition, we account for the fact that neural architectures
require a huge amount of parameters to train, thus increasing the online
inference latency and negatively influencing the user experience in a live
video streaming event. To address the problem of the high online inference of a
vast number of parameters, we propose a knowledge distillation strategy. In
particular, we design a distillation loss function, aiming to first pretrain a
teacher model on offline data, and then transfer the knowledge from the teacher
to a smaller student model with less parameters. We evaluate our proposed model
on the link prediction task on three real-world datasets, generated by live
video streaming events. The events lasted 80 minutes and each viewer exploited
the distribution solution provided by the company Hive Streaming AB. The
experiments demonstrate the effectiveness of the proposed model in terms of
link prediction accuracy and number of required parameters, when evaluated
against state-of-the-art approaches. In addition, we study the distillation
performance of the proposed model in terms of compression ratio for different
distillation strategies, where we show that the proposed model can achieve a
compression ratio up to 15:100, preserving high link prediction accuracy. For
reproduction purposes, our evaluation datasets and implementation are publicly
available at https://stefanosantaris.github.io/EGAD.",arxiv
http://arxiv.org/abs/1710.08526v1,2017-10-23T22:09:45Z,2017-10-23T22:09:45Z,Video Labeling for Automatic Video Surveillance in Security Domains,"Beyond traditional security methods, unmanned aerial vehicles (UAVs) have
become an important surveillance tool used in security domains to collect the
required annotated data. However, collecting annotated data from videos taken
by UAVs efficiently, and using these data to build datasets that can be used
for learning payoffs or adversary behaviors in game-theoretic approaches and
security applications, is an under-explored research question. This paper
presents VIOLA, a novel labeling application that includes (i) a workload
distribution framework to efficiently gather human labels from videos in a
secured manner; (ii) a software interface with features designed for labeling
videos taken by UAVs in the domain of wildlife security. We also present the
evolution of VIOLA and analyze how the changes made in the development process
relate to the efficiency of labeling, including when seemingly obvious
improvements did not lead to increased efficiency. VIOLA enables collecting
massive amounts of data with detailed information from challenging security
videos such as those collected aboard UAVs for wildlife security. VIOLA will
lead to the development of new approaches that integrate deep learning for
real-time detection and response.",arxiv
http://arxiv.org/abs/2105.01948v1,2021-05-05T09:36:30Z,2021-05-05T09:36:30Z,"Similarity Measures for Location-Dependent MMIMO, 5G Base Stations
  On/Off Switching Using Radio Environment Map","The Massive Multiple-Input Multiple-Output (MMIMO) technique together with
Heterogeneous Network (Het-Net) deployment enables high throughput of 5G and
beyond networks. However, a high number of antennas and a high number of Base
Stations (BSs) can result in significant power consumption. Previous studies
have shown that the energy efficiency (EE) of such a network can be effectively
increased by turning off some BSs depending on User Equipments (UEs) positions.
Such mapping is obtained by using Reinforcement Learning. Its results are
stored in a so-called Radio Environment Map (REM). However, in a real network,
the number of UEs' positions patterns would go to infinity. This paper aims to
determine how to match the current set of UEs' positions to the most similar
pattern, i.e., providing the same optimal active BSs set, saved in REM. We
compare several state-of-the-art distance metrics using a computer simulator:
an accurate 3D-Ray-Tracing model of the radio channel and an advanced
system-level simulator of MMIMO Het-Net. The results have shown that the
so-called Sum of Minimums Distance provides the best matching between REM data
and UEs' positions, enabling up to 56% EE improvement over the scenario without
EE optimization.",arxiv
http://arxiv.org/abs/1509.02069v2,2017-02-09T14:25:13Z,2015-09-07T15:00:55Z,"EMMIXcskew: an R Package for the Fitting of a Mixture of Canonical
  Fundamental Skew t-Distributions","This paper presents an R package EMMIXcskew for the fitting of the canonical
fundamental skew t-distribution (CFUST) and finite mixtures of this
distribution (FM-CFUST) via maximum likelihood (ML). The CFUST distribution
provides a flexible family of models to handle non-normal data, with parameters
for capturing skewness and heavy-tails in the data. It formally encompasses the
normal, t, and skew-normal distributions as special and/or limiting cases. A
few other versions of the skew t-distributions are also nested within the CFUST
distribution. In this paper, an Expectation-Maximization (EM) algorithm is
described for computing the ML estimates of the parameters of the FM-CFUST
model, and different strategies for initializing the algorithm are discussed
and illustrated. The methodology is implemented in the EMMIXcskew package, and
examples are presented using two real datasets. The EMMIXcskew package contains
functions to fit the FM-CFUST model, including procedures for generating
different initial values. Additional features include random sample generation
and contour visualization in 2D and 3D.",arxiv
http://arxiv.org/abs/2109.14797v2,2021-10-02T00:05:08Z,2021-09-30T01:54:44Z,"Emergency Vehicles Audio Detection and Localization in Autonomous
  Driving","Emergency vehicles in service have right-of-way over all other vehicles.
Hence, all other vehicles are supposed to take proper actions to yield
emergency vehicles with active sirens. As this task requires the cooperation
between ears and eyes for human drivers, it also needs audio detection as a
supplement to vision-based algorithms for fully autonomous driving vehicles. In
urban driving scenarios, we need to know both the existence of emergency
vehicles and their relative positions to us to decide the proper actions. We
present a novel system from collecting the real-world siren data to the
deployment of models using only two cost-efficient microphones. We are able to
achieve promising performance for each task separately, especially within the
crucial 10m to 50m distance range to react (the size of our ego vehicle is
around 5m in length and 2m in width). The recall rate to determine the
existence of sirens is 99.16% , the median and mean angle absolute error is
9.64{\deg} and 19.18{\deg} respectively, and the median and mean distance
absolute error of 9.30m and 10.58m respectively within that range. We also
benchmark various machine learning approaches that can determine the siren
existence and sound source localization which includes direction and distance
simultaneously within 50ms of latency.",arxiv
http://arxiv.org/abs/1706.06696v1,2017-06-20T22:53:16Z,2017-06-20T22:53:16Z,"The NAO Backpack: An Open-hardware Add-on for Fast Software Development
  with the NAO Robot","We present an open-source accessory for the NAO robot, which enables to test
computationally demanding algorithms in an external platform while preserving
robot's autonomy and mobility. The platform has the form of a backpack, which
can be 3D printed and replicated, and holds an ODROID XU4 board to process
algorithms externally with ROS compatibility. We provide also a software bridge
between the B-Human's framework and ROS to have access to the robot's sensors
close to real-time. We tested the platform in several robotics applications
such as data logging, visual SLAM, and robot vision with deep learning
techniques. The CAD model, hardware specifications and software are available
online for the benefit of the community:
https://github.com/uchile-robotics/nao-backpack",arxiv
http://arxiv.org/abs/2107.11355v2,2021-08-18T03:25:24Z,2021-07-23T17:19:23Z,Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency,"Deep learning-based 3D object detection has achieved unprecedented success
with the advent of large-scale autonomous driving datasets. However, drastic
performance degradation remains a critical challenge for cross-domain
deployment. In addition, existing 3D domain adaptive detection methods often
assume prior access to the target domain annotations, which is rarely feasible
in the real world. To address this challenge, we study a more realistic
setting, unsupervised 3D domain adaptive detection, which only utilizes source
domain annotations. 1) We first comprehensively investigate the major
underlying factors of the domain gap in 3D detection. Our key insight is that
geometric mismatch is the key factor of domain shift. 2) Then, we propose a
novel and unified framework, Multi-Level Consistency Network (MLC-Net), which
employs a teacher-student paradigm to generate adaptive and reliable
pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level
consistency to facilitate cross-domain transfer. Extensive experiments
demonstrate that MLC-Net outperforms existing state-of-the-art methods
(including those using additional target domain information) on standard
benchmarks. Notably, our approach is detector-agnostic, which achieves
consistent gains on both single- and two-stage 3D detectors.",arxiv
http://arxiv.org/abs/2011.14579v2,2021-06-18T02:06:59Z,2020-11-30T06:55:05Z,"End-to-End 3D Point Cloud Learning for Registration Task Using Virtual
  Correspondences","3D Point cloud registration is still a very challenging topic due to the
difficulty in finding the rigid transformation between two point clouds with
partial correspondences, and it's even harder in the absence of any initial
estimation information. In this paper, we present an end-to-end deep-learning
based approach to resolve the point cloud registration problem. Firstly, the
revised LPD-Net is introduced to extract features and aggregate them with the
graph network. Secondly, the self-attention mechanism is utilized to enhance
the structure information in the point cloud and the cross-attention mechanism
is designed to enhance the corresponding information between the two input
point clouds. Based on which, the virtual corresponding points can be generated
by a soft pointer based method, and finally, the point cloud registration
problem can be solved by implementing the SVD method. Comparison results in
ModelNet40 dataset validate that the proposed approach reaches the
state-of-the-art in point cloud registration tasks and experiment resutls in
KITTI dataset validate the effectiveness of the proposed approach in real
applications.Our source code is available at
\url{https://github.com/qiaozhijian/VCR-Net.git}",arxiv
http://arxiv.org/abs/2012.03325v1,2020-12-06T17:06:33Z,2020-12-06T17:06:33Z,EasyPBR: A Lightweight Physically-Based Renderer,"Modern rendering libraries provide unprecedented realism, producing real-time
photorealistic 3D graphics on commodity hardware. Visual fidelity, however,
comes at the cost of increased complexity and difficulty of usage, with many
rendering parameters requiring a deep understanding of the pipeline. We propose
EasyPBR as an alternative rendering library that strikes a balance between
ease-of-use and visual quality. EasyPBR consists of a deferred renderer that
implements recent state-of-the-art approaches in physically based rendering. It
offers an easy-to-use Python and C++ interface that allows high-quality images
to be created in only a few lines of code or directly through a graphical user
interface. The user can choose between fully controlling the rendering pipeline
or letting EasyPBR automatically infer the best parameters based on the current
scene composition. The EasyPBR library can help the community to more easily
leverage the power of current GPUs to create realistic images. These can then
be used as synthetic data for deep learning or for creating animations for
academic purposes.",arxiv
