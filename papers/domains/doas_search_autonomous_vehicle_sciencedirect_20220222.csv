id,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1016/j.jag.2021.102652,Journal,International Journal of Applied Earth Observation and Geoinformation,scopus,2022-02-01,sciencedirect,Developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big UAV imagery data,https://api.elsevier.com/content/abstract/scopus_id/85122505895,"The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.",autonomous vehicle
10.1016/j.compag.2021.106574,Journal,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,Perennial ryegrass biomass retrieval through multispectral UAV data,https://api.elsevier.com/content/abstract/scopus_id/85122407603,"Frequent biomass measurement is a key activity for optimal perennial ryegrass (Lolium perenne) management in intensive forage-based dairy operations. Due to the necessary high-frequency (i.e., weekly or monthly) pasture monitoring and continuous trend of larger dairy farms, such activity is perceived as an operational bottleneck. Consequently, substantial effort is directed to the development of accurate and automated technological solutions for biomass assessment. The popularization of unmanned aerial vehicles (UAVs) combined with multispectral cameras should allow for an optimal observational system able to deploy machine learning algorithms for near real-time biomass dry-matter (DM) mapping. For successful operation, these systems should deliver radiometrically accurate orthomosaics and robust models able to generalize across different periods. Nevertheless, the accuracy of radiometric calibration and generalization ability of these models is seldom evaluated. Also, such pipelines should require minimum processing power and allow for fast deployment. This study has established a two-year experiment comparing reflectance measurements between a handheld spectrometer and a commercial multispectral UAV camera. Different algorithms based on regression-tree architecture were contrasted regarding accuracy, speed, and model size. Model performances were validated, providing error-metrics for baseline accuracy and temporal validation. The results have shown that the standard procedure for multispectral imagery radiometric calibration is sub-optimal, requiring further post-processing and presenting low correlation with handheld measurements across spectral bands and dates. Nevertheless, after post-calibration, the use of spectral imagery has presented better baseline error than the point-based sensors, respectively displaying an average of 397.3 and 464.2 kg DM/ha when employed alongside the best performing algorithm (i.e., Cubist). When trained and validated across different years, model performance was largely reduced and deemed unfit for operational purposes. The Cubist/M5 family of algorithms have exhibited advantageous characteristics such as compact model structure, allowing for a higher level of model interpretability, while displaying a smaller size and faster deployment than the Random Forest, Boosted, and Bagged Regression Trees algorithms.",autonomous vehicle
10.1016/j.aap.2021.106473,Journal,Accident Analysis and Prevention,scopus,2022-02-01,sciencedirect,Mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors,https://api.elsevier.com/content/abstract/scopus_id/85118989110,"Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.",autonomous vehicle
10.1016/j.inffus.2021.09.004,Journal,Information Fusion,scopus,2022-02-01,sciencedirect,Multimodal Earth observation data fusion: Graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",autonomous vehicle
10.1016/j.isatra.2022.01.014,Journal,ISA Transactions,scopus,2022-01-01,sciencedirect,"Intelligent framework for automated failure prediction, detection, and classification of mission critical autonomous flights",https://api.elsevier.com/content/abstract/scopus_id/85123893673,"Autonomous flights are the major industry contributors towards next-generation developments in pervasive and ubiquitous computing. Modern aerial vehicles are designed to receive actuator commands from the primary autopilot software as input to regulate their servos for adjusting control surfaces. Due to real-time interaction with the actual physical environment, there exists a high risk of control surface failures for engine, rudder, elevators, and ailerons etc. If not anticipated and then timely controlled, failures occurring during the flight can have severe and cataclysmic consequences, which may result in mid-air collision or ultimate crash. Humongous amount of sensory data being generated throughout mission-critical flights, makes it an ideal candidate for applying advanced data-driven machine learning techniques to identify intelligent insights related to failures for instant recovery from emergencies. In this paper, we present a novel framework based on machine learning techniques for failure prediction, detection, and classification for autonomous aerial vehicles. The proposed framework utilizes long short-term memory recurrent neural network architecture to analyze time series data and has been applied at the AirLab Failure and Anomaly flight dataset, which is a comprehensive publicly available dataset of various fault types in fixed-wing autonomous aerial vehicles’ control surfaces. The proposed framework is able to predict failure with an average accuracy of 93% and the average time-to-predict a failure is 19 s before the actual occurrence of the failure, which is 10 s better than current state-of-the-art. Failure detection accuracy is 100% and average detection time is 0.74 s after happening of failure, which is 1.28 s better than current state-of-the-art. Failure classification accuracy of proposed framework is 100%. The performance analysis shows the strength of the proposed methodology to be used as a real-time failure prediction and a pseudo-real-time failure detection along with a failure classification framework for eventual deployment with actual mission-critical autonomous flights.",autonomous vehicle
10.1016/j.trc.2021.103499,Journal,Transportation Research Part C: Emerging Technologies,scopus,2022-01-01,sciencedirect,Do autonomous vehicles drive like humans? A Turing approach and an application to SAE automation Level 2 cars,https://api.elsevier.com/content/abstract/scopus_id/85120490088,"Fully automated vehicles (AVs) are set to become a reality in future decades and changes are to be expected in user perceptions and behavior. While AV acceptability has been widely studied, changes in human drivers’ behavior and in passengers’ reactions have received less attention. It is not yet possible to ascertain the risk of driver behavioral changes such as overreaction, and the corresponding safety problems, in mixed traffic with partially AVs. Nor has there been proper investigation of the potential unease of car occupants trained for human control, when exposed to automatic maneuvers. The conjecture proposed in this paper is that automation Level 2 vehicles do not induce potentially adverse effects in traditional vehicle drivers’ behavior or in occupants’ reactions, provided that they are indistinguishable from human-driven vehicles. To this end, the paper proposes a Turing approach to test the “humanity” of automation Level 2 vehicles. The proposed test was applied to the results of an experimental campaign carried out in Italy: 546 car passengers were interviewed on board Level 2 cars in which they could not see the driver. They were asked whether a specific driving action (braking, accelerating, lane keeping) had been performed by the human driver or by the automatic on-board software under different traffic conditions (congestion and speed). Estimation results show that in most cases the interviewees were unable to distinguish the Artificial Intelligence (AI) from the human driver by observing random responses with a 95% significance level (proportion of success statistically equal to 50%). However, in the case of moderate braking and lane keeping at >100 km/h and in high traffic congestion, respondents recognized AI control from the human driver above pure chance, with 62–69% correct response rates. These findings, if confirmed in other case studies, could significantly impact on AVs acceptability, also contributing to their design as well as to long-debated ethical questions. AI driving software could be designed and tested for “humanity”, as long as safety is guaranteed, and autonomous cars could be allowed to circulate as long as they cannot be distinguished from human-driven vehicles in recurrent driving conditions.",autonomous vehicle
10.1016/j.dsp.2021.103290,Journal,Digital Signal Processing: A Review Journal,scopus,2022-01-01,sciencedirect,Deep residual learning-based cognitive model for detection and classification of transmitted signal patterns in 5G smart city networks,https://api.elsevier.com/content/abstract/scopus_id/85118634214,"Primary user (PU) signal detection or classification is a critical component of cognitive radio (CR) related wireless communication applications. In CR, the PU detection methods are mostly based on statistical models, and their detection performance heavily relies on the accuracy of assumed models. In this paper, we design a novel detector, dubbed as PU-Net, that dynamically learns the PU activity patterns in a cognitive 5G smart city, where a network of unmanned aerial vehicles (UAVs) is deployed as flying base stations to serve the Internet-of-Things (IoT) users. Unlike the traditional schemes, the PU-Net is free from signal-noise model assumptions and is leveraged through deep residual learning integrated with atrous spatial pyramid pooling (ASPP) to sense the PU's transmitted signal patterns in the network. The PU-Net detects and classifies the active and idle PU states by exploiting the multilevel spatial-temporal features in the signal and noise frames. The proposed model is trained using locally synthesized Rayleigh channel-impaired data with large variability of modulated signals and different noise floor regimes. Additionally, the PU-Net model is blind-tested and evaluated on real-world over-the-air signals and with variable-length frames and varying channel effects at secondary users (SUs). With extensive experiments, it is shown that PU-Net outperforms other benchmark detectors, obtaining an accuracy of 0.9974, with 0.9978 recall and 0.9970 precision in detecting and classifying the PU transmitted signal patterns. Correspondingly, the proposed PU-Net can be adopted for IoT/UAV-assisted communication systems in optimizing spectrum efficiency and resolving the coexistence issues in 5G and beyond networks.",autonomous vehicle
10.1016/j.engappai.2021.104514,Journal,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,Instance-based defense against adversarial attacks in Deep Reinforcement Learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",autonomous vehicle
10.1016/j.inffus.2021.07.004,Journal,Information Fusion,scopus,2022-01-01,sciencedirect,SaccadeFork: A lightweight multi-sensor fusion-based target detector,https://api.elsevier.com/content/abstract/scopus_id/85112374720,"Commercialization of self-driving applications requires precision and reliability of the perception system due to the highly dynamic and complex road environment. Early perception systems either rely on the camera or on LiDAR for moving obstacle detection. With the development of vehicular sensors and deep learning technologies, the multi-view and sensor fusion based convolutional neural network (CNN) model for detection tasks has become a popular research area. In this paper, we present a novel multi-sensor fusion-based CNN model–SaccadeFork–that integrates the image and upsampled LiDAR point clouds as the input. SaccadeFork includes two modules: (1) a lightweight backbone that consists of hourglass convolution feature extraction module and a parallel dilation convolution module for adaptation of the system to different target sizes; (2) an anchor-based detection head. The model also considers deployment of resource-limited edge devices in the vehicle. Two refinement strategies, i.e., Mixup and Swish activation function are also adopted to improve the model. Comparison with a series of latest models on public dataset of KITTI shows that SaccadeFork can achieve the optimal detection accuracy on vehicles and pedestrians under different scenarios. The final model is also deployed and tested on a local dataset collected based on edge devices and low-cost sensor solutions, and the results show that the model can achieve real-time efficiency and high detection accuracy.",autonomous vehicle
