doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/TIE.2021.3090707,Active Object Detection Based on a Novel Deep Q-Learning Network and Long-Term Learning Strategy for the Service Robot,IEEE,Journals,"This article focuses on active object detection (AOD), one of the greatest challenges in the robotics field. A novel deep-Q-learning-network-based approach is proposed to utilize more useful status information for enhancing the training efficiency and testing accuracy of AOD by adding the cropped target object (TGOJ) from the current state as a new input. Different from the existing researches, a novel reward function, combing the area factor and distance factor of the bounding box, is designed to make the robot not only get closer to the TGOJ but also obtain a better observation viewpoint. Moreover, to overcome the differences between the training dataset and new environments as well as improving the adaptation of the AOD model, a reward-based long-term learning strategy including a novel training strategy is presented. The comparable experiments and the ablation study have been implemented in an AOD dataset, proving that our method owns better performance and efficiency than the comparable methods. Meanwhile, the experiments in the real-world scenario with a robot indicate the validity of the proposed method.",https://ieeexplore.ieee.org/document/9464751/,IEEE Transactions on Industrial Electronics,June 2022,ieeexplore
10.1109/LRA.2022.3146515,Kineverse: A Symbolic Articulation Model Framework for Model-Agnostic Mobile Manipulation,IEEE,Journals,"Service robots in the future need to execute abstract instructions such as “fetch the milk from the fridge”. To translate such instructions into actionable plans, robots require in-depth background knowledge. With regards to interactions with doors and drawers, robots require articulation models that they can use for state estimation and motion planning. Existing frameworks model articulated connections as abstract concepts such as <italic>prismatic</italic>, or <italic>revolute</italic>, but do not provide a parameterized model of these connections for computation. In this letter, we introduce a novel framework that uses symbolic mathematical expressions to model articulated structures – robots and objects alike – in a unified and extensible manner. We provide a theoretical description of this framework, and the operations that are supported by its models, and introduce an architecture to exchange our models in robotic applications, making them as flexible as any other environmental observation. To demonstrate the utility of our approach, we employ our practical implementation <italic>Kineverse</italic> for solving common robotics tasks from state estimation and mobile manipulation, and use it further in real-world mobile robot manipulation.",https://ieeexplore.ieee.org/document/9695204/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2022.3146945,"TACTO: A Fast, Flexible, and Open-Source Simulator for High-Resolution Vision-Based Tactile Sensors",IEEE,Journals,"Simulators perform an important role in prototyping, debugging, and benchmarking new advances in robotics and learning for control. Although many physics engines exist, some aspects of the real world are harder than others to simulate. One of the aspects that have so far eluded accurate simulation is touch sensing. To address this gap, we present TACTO – a fast, flexible, and open-source simulator for vision-based tactile sensors. This simulator allows to render realistic high-resolution touch readings at hundreds of frames per second, and can be easily configured to simulate different vision-based tactile sensors, including DIGIT and OmniTact. In this letter, we detail the principles that drove the implementation of TACTO and how they are reflected in its architecture. We demonstrate TACTO on a perceptual task, by learning to predict grasp stability using touch from 1 million grasps, and on a marble manipulation control task. Moreover, we provide a proof-of-concept that TACTO can be successfully used for Sim2Real applications. We believe that TACTO is a step towards the widespread adoption of touch sensing in robotic applications, and to enable machine learning practitioners interested in multi-modal learning and control.",https://ieeexplore.ieee.org/document/9697425/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2021.3123374,Uncertainty for Identifying Open-Set Errors in Visual Object Detection,IEEE,Journals,"Deployed into an open world, object detectors are prone to open-set errors, false positive detections of object classes not present in the training dataset.We propose GMM-Det, a real-time method for extracting epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified by their low log-probability under all Gaussian Mixture Models. We test two common detector architectures, Faster R-CNN and RetinaNet, across three varied datasets spanning robotics and computer vision. Our results show that GMM-Det consistently outperforms existing uncertainty techniques for identifying and rejecting open-set detections, especially at the low-error-rate operating point required for safety-critical applications. GMM-Det maintains object detection performance, and introduces only minimal computational overhead. We also introduce a methodology for converting existing object detection datasets into specific <italic>open-set</italic> datasets to evaluate open-set performance in object detection.",https://ieeexplore.ieee.org/document/9591346/,IEEE Robotics and Automation Letters,Jan. 2022,ieeexplore
10.1109/TRO.2021.3084374,Cat-Like Jumping and Landing of Legged Robots in Low Gravity Using Deep Reinforcement Learning,IEEE,Journals,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",https://ieeexplore.ieee.org/document/9453856/,IEEE Transactions on Robotics,Feb. 2022,ieeexplore
10.1109/TFUZZ.2020.3033141,Fuzzy Double Deep Q-Network-Based Gait Pattern Controller for Humanoid Robots,IEEE,Journals,"In this article, the adaptive-network-based fuzzy inference system (ANFIS) is combined with the double deep <italic>Q</italic>-network (DDQN) to realize a fuzzy DDQN (FDDQN) such that a humanoid robot can generate a linear inverted pendulum model-based gait pattern in real time. The FDDQN not only allows the humanoid robot to correct the gait pattern instantly but also improves its stability. The proposed scheme is designed and implemented in a toddler-sized humanoid robot called Louis. First, four pressure sensors are installed on the bottom of the sole and one inertial measurement unit is set up on the trunk of the robot. A wireless communication chip is employed to transfer the data to a computer to determine the required parameters for the robot. Next, a control system based on the Linux operating system is developed. The values of the center of pressure and acceleration obtained with the ANFIS are adopted to train the DDQN. The proposed neural network comprises four layers, and the model is cautiously selected to avoid overfitting. The proposed scheme is verified using a robot simulator and then real-time-tested on Louis. The experimental results indicate that the FDDQN can provide the robot timely feedback during walking as well as helps it in adjusting the gait pattern independently. The balancing of the robot through effective dynamic feedback is similar to the balancing ability of an infant learning to walk.",https://ieeexplore.ieee.org/document/9237162/,IEEE Transactions on Fuzzy Systems,Jan. 2022,ieeexplore
10.1109/SII52469.2022.9708826,Evaluation of Variable Impedance- and Hybrid Force/MotionControllers for Learning Force Tracking Skills,IEEE,Conferences,"For robots to perform real-world force interaction tasks with human level dexterity, it is crucial to develop adaptable and compliant force controllers. Learning techniques, especially reinforcement learning, provide a platform to develop adaptable controllers for complex robotic tasks. This paper presents an evaluation of two prominent force control methods, variable impedance control and hybrid force-motion control in a robot learning framework. The controllers are evaluated on a Franka Emika Panda robotic manipulator for a robotic interaction task demanding force and motion tracking using a model-based reinforcement learning algorithm, PILCO. Utilizing the learning framework to find the optimal controller parameters has significantly improved the performance of the controllers. The implementation of the controllers integrated with the robot learning framework is available on https://github.com/martihmy/Compliant_control.",https://ieeexplore.ieee.org/document/9708826/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore
10.1109/LRA.2022.3142439,Anytime 3D Object Reconstruction Using Multi-Modal Variational Autoencoder,IEEE,Journals,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository<sup>1</sup>.",https://ieeexplore.ieee.org/document/9681277/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2022.3142433,Enabling Low-Cost Full Surface Tactile Skin for Human Robot Interaction,IEEE,Journals,"Realizing full coverage, low-maintenance, and low-cost tactile skin is a <italic>de facto</italic> design dream since the invention of robots. It ensures safety and enables collaborative work protocols for human robot interactions (HRI). The on-robot tactile capability is realized by deploying an array of external sensors or inferring from proprioceptive information that comes with the robot, such as motor torque. However, these methods may be cumbersome, introduce extra management cost, expensive, lack real-world robustness, or require special robot designs. In this letter, we present <italic>SonicSkin</italic>, a low-cost ($2) and easy to deploy system that localizes the on-robot human touch and estimates the touch pressure without actually attaching sensors at potential touch locations. The system requires only a single pair of piezoelectric transducers (<italic>i.e.</italic> one transmitter and one receiver) attached on the target robot and turns the robot itself into a versatile sensor. We present a set of novel algorithms to progressively address the unique challenges posed by our system design. We put together an end-to-end <italic>SonicSkin</italic> system on a Jaco robot arm that runs in real-time, and conducted an extensive real-world study including 57019 actual evaluation datapoints under various challenging conditions from 12 human subjects. <italic>SonicSkin</italic> achieves less than 2 cm localization error for 96.4% of touches, with more than 96.7% cross-correlation similarity between the predicted touch pressure and the ground truth touch pressure.",https://ieeexplore.ieee.org/document/9681158/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/ACCESS.2022.3145969,FastMDE: A Fast CNN Architecture for Monocular Depth Estimation at High Resolution,IEEE,Journals,"A depth map helps robots and autonomous vehicles (AVs) visualize the three-dimensional world to navigate and localize neighboring obstacles. However, it is difficult to develop a deep learning model that can estimate the depth map from a single image in real-time. This study proposes a fast monocular depth estimation model named <italic>FastMDE</italic> by optimizing the deep convolutional neural network according to the encoder-decoder architecture. The decoder needs to obtain partial and semantic feature maps from the encoding phase to improve the depth estimation accuracy. Therefore, we designed FastMDE with two effective strategies. The first one involved redesigning the skip connection with the features of the squeeze-excitation module to obtain partial and semantic feature maps of the encoding phase. The second strategy involved redesigning the decoder by using the fusion dense block to permit the usage of high-resolution features that were learned earlier in the network before upsampling. The proposed FastMDE model utilizes only 4.1 M parameters, which is much lesser than the parameters utilized by state-of-art models. Thus, FastDME has a higher accuracy and lower latency than previous models. This study also demonstrates that MDE can leverage deep neural networks in real-time (i.e., 30 fps) with the Linux embedded board Nvidia Jetson Xavier NX. The model can facilitate the development and applications with superior performances and easy deployment on an embedded platform.",https://ieeexplore.ieee.org/document/9690863/,IEEE Access,2022,ieeexplore
10.1109/LRA.2022.3146515,Kineverse: A Symbolic Articulation Model Framework for Model-Agnostic Mobile Manipulation,IEEE,Journals,"Service robots in the future need to execute abstract instructions such as “fetch the milk from the fridge”. To translate such instructions into actionable plans, robots require in-depth background knowledge. With regards to interactions with doors and drawers, robots require articulation models that they can use for state estimation and motion planning. Existing frameworks model articulated connections as abstract concepts such as <italic>prismatic</italic>, or <italic>revolute</italic>, but do not provide a parameterized model of these connections for computation. In this letter, we introduce a novel framework that uses symbolic mathematical expressions to model articulated structures – robots and objects alike – in a unified and extensible manner. We provide a theoretical description of this framework, and the operations that are supported by its models, and introduce an architecture to exchange our models in robotic applications, making them as flexible as any other environmental observation. To demonstrate the utility of our approach, we employ our practical implementation <italic>Kineverse</italic> for solving common robotics tasks from state estimation and mobile manipulation, and use it further in real-world mobile robot manipulation.",https://ieeexplore.ieee.org/document/9695204/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/TMRB.2021.3129113,Learning a Generic Olfactory Search Strategy From Silk Moths by Deep Inverse Reinforcement Learning,IEEE,Journals,"Despite their simple nervous systems, insects efficiently search for and find sources of odorants. Hence, it is necessary to model and implement such behavior in artificial agents (robots), to enable them to detect dangerous substances such as drugs, gas leaks, and explosives. Previous studies have approached behavioral modeling with either statistical or machine-learning methods. In this study, we determined the behavior trajectories of male silk moths using a virtual reality (VR) system. We then modeled these trajectories as a Markov decision process (MDP) and employed inverse reinforcement learning (IRL) to learn their reward function. Furthermore, we estimated the optimal policy from the learned reward function. We then conducted olfactory search simulations and determined that the IRL-based policy could locate odor sources with a high success rate. This was also investigated under environmental conditions different from those faced by real moths on the VR system. The obtained results indicate that IRL can generically represent olfactory search strategies that are adaptable to various environments.",https://ieeexplore.ieee.org/document/9619462/,IEEE Transactions on Medical Robotics and Bionics,Feb. 2022,ieeexplore
10.1109/LRA.2021.3129136,OCRTOC: A Cloud-Based Competition and Benchmark for Robotic Grasping and Manipulation,IEEE,Journals,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is <uri>www.ocrtoc.org</uri>, and the OCRTOC software package is available at <uri>https://github.com/OCRTOC/OCRTOC_software_package</uri>.",https://ieeexplore.ieee.org/document/9619915/,IEEE Robotics and Automation Letters,Jan. 2022,ieeexplore
10.1109/LRA.2022.3140793,Vision-Based Self-Adaptive Gripping in a Trimodal Robotic Sorting End-Effector,IEEE,Journals,"Recyclable waste management, which includes sorting as a key process, is a crucial component of maintaining a sustainable ecosystem. The use of robots in sorting could significantly facilitate the production of secondary raw materials from waste in the sense of a recycling economy. However, due to the complex and heterogeneous types of the recyclable items, the conventional robotic gripping end-effectors, which typically come with a fixed structure, are unlikely to hold onto the full range of items to enable separation and recycling. To this end, a trimodal adaptive end-effector is proposed that can be integrated with robotic manipulators to improve their gripping versatility. The end-effector can deploy effective modes of gripping to different objects in response to their size and porosity via gripping mechanisms based on Nano Polyurethane (PU) adhesive gels, pumpless vacuum suction, and radially deployable claws. While the end-effector's mechanical design allows the three gripping modes to be deployed independently or in conjunction with one another, this work aims at deploying modes that are effective for gripping onto the recyclable item. In order to decide on the suitable modes of gripping a real-time vision system is designed to measure the size and porosity of the recyclable items and advise on a suitable combination of gripping modes to be deployed. Integrated current sensors provide an indication of successful gripping and releasing of the recyclable items. The results of the experiments confirmed the ability of our vision-based approach in identifying suitable gripping modes in real-time, the deployment of the relevant mechanisms and successful gripping onto a maximum of 84.8% (single-mode), 90.9% (dual-mode) and 96.9% (triple-mode) of a specified set of recyclable items.",https://ieeexplore.ieee.org/document/9672720/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2022.3143289,VisuoTactile 6D Pose Estimation of an In-Hand Object Using Vision and Tactile Sensor Data,IEEE,Journals,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",https://ieeexplore.ieee.org/document/9682507/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/SII52469.2022.9708882,Reinforcement Learning based Hierarchical Control for Path Tracking of a Wheeled Bipedal Robot with Sim-to-Real Framework,IEEE,Conferences,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",https://ieeexplore.ieee.org/document/9708882/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore
10.1109/TIE.2021.3090707,Active Object Detection Based on a Novel Deep Q-Learning Network and Long-Term Learning Strategy for the Service Robot,IEEE,Journals,"This article focuses on active object detection (AOD), one of the greatest challenges in the robotics field. A novel deep-Q-learning-network-based approach is proposed to utilize more useful status information for enhancing the training efficiency and testing accuracy of AOD by adding the cropped target object (TGOJ) from the current state as a new input. Different from the existing researches, a novel reward function, combing the area factor and distance factor of the bounding box, is designed to make the robot not only get closer to the TGOJ but also obtain a better observation viewpoint. Moreover, to overcome the differences between the training dataset and new environments as well as improving the adaptation of the AOD model, a reward-based long-term learning strategy including a novel training strategy is presented. The comparable experiments and the ablation study have been implemented in an AOD dataset, proving that our method owns better performance and efficiency than the comparable methods. Meanwhile, the experiments in the real-world scenario with a robot indicate the validity of the proposed method.",https://ieeexplore.ieee.org/document/9464751/,IEEE Transactions on Industrial Electronics,June 2022,ieeexplore
10.1109/LRA.2022.3142433,Enabling Low-Cost Full Surface Tactile Skin for Human Robot Interaction,IEEE,Journals,"Realizing full coverage, low-maintenance, and low-cost tactile skin is a <italic>de facto</italic> design dream since the invention of robots. It ensures safety and enables collaborative work protocols for human robot interactions (HRI). The on-robot tactile capability is realized by deploying an array of external sensors or inferring from proprioceptive information that comes with the robot, such as motor torque. However, these methods may be cumbersome, introduce extra management cost, expensive, lack real-world robustness, or require special robot designs. In this letter, we present <italic>SonicSkin</italic>, a low-cost ($2) and easy to deploy system that localizes the on-robot human touch and estimates the touch pressure without actually attaching sensors at potential touch locations. The system requires only a single pair of piezoelectric transducers (<italic>i.e.</italic> one transmitter and one receiver) attached on the target robot and turns the robot itself into a versatile sensor. We present a set of novel algorithms to progressively address the unique challenges posed by our system design. We put together an end-to-end <italic>SonicSkin</italic> system on a Jaco robot arm that runs in real-time, and conducted an extensive real-world study including 57019 actual evaluation datapoints under various challenging conditions from 12 human subjects. <italic>SonicSkin</italic> achieves less than 2 cm localization error for 96.4% of touches, with more than 96.7% cross-correlation similarity between the predicted touch pressure and the ground truth touch pressure.",https://ieeexplore.ieee.org/document/9681158/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2022.3146900,Open Simulation Environment for Learning and Practice of Robot-Assisted Surgical Suturing,IEEE,Journals,"Automation has the potential to improve the standard of care but is difficult to realize due to perceptual challenges, especially in soft-tissue surgery. Machine learning can provide solutions, but typically requires large amounts of training data, which is time-consuming to collect. Even with shared platforms, hardware differences can prevent effective sharing of data between institutions. This letter proposes a standardized simulation platform for training and testing algorithms to control surgical robotic systems, which is built upon an open-source simulator, the Asynchronous Multi-Body Framework (AMBF), to enable quick prototyping of different scenes. An illustrative example of a suturing task on a phantom is presented and has formed the basis of a challenge, released to the community. The top-level contribution is the open-source release of a dynamic simulation environment that enables realistic suturing on a phantom, but supporting contributions include its extendable architectural design and a series of algorithmic optimizations to achieve real-time control and collision detection, realistic behavior of the needle and suture, and generation of multi-modal ground-truth data, including labeled depth data. These capabilities enable simulation-based surgical training and support research in machine learning for surgical scene perception and autonomous action.",https://ieeexplore.ieee.org/document/9697399/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/SII52469.2022.9708826,Evaluation of Variable Impedance- and Hybrid Force/MotionControllers for Learning Force Tracking Skills,IEEE,Conferences,"For robots to perform real-world force interaction tasks with human level dexterity, it is crucial to develop adaptable and compliant force controllers. Learning techniques, especially reinforcement learning, provide a platform to develop adaptable controllers for complex robotic tasks. This paper presents an evaluation of two prominent force control methods, variable impedance control and hybrid force-motion control in a robot learning framework. The controllers are evaluated on a Franka Emika Panda robotic manipulator for a robotic interaction task demanding force and motion tracking using a model-based reinforcement learning algorithm, PILCO. Utilizing the learning framework to find the optimal controller parameters has significantly improved the performance of the controllers. The implementation of the controllers integrated with the robot learning framework is available on https://github.com/martihmy/Compliant_control.",https://ieeexplore.ieee.org/document/9708826/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore
10.1109/SII52469.2022.9708896,Integration of a reconfigurable robotic workcell for assembly operations in automotive industry,IEEE,Conferences,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",https://ieeexplore.ieee.org/document/9708896/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore
10.1109/SII52469.2022.9708882,Reinforcement Learning based Hierarchical Control for Path Tracking of a Wheeled Bipedal Robot with Sim-to-Real Framework,IEEE,Conferences,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",https://ieeexplore.ieee.org/document/9708882/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore
10.1109/TIE.2021.3090707,Active Object Detection Based on a Novel Deep Q-Learning Network and Long-Term Learning Strategy for the Service Robot,IEEE,Journals,"This article focuses on active object detection (AOD), one of the greatest challenges in the robotics field. A novel deep-Q-learning-network-based approach is proposed to utilize more useful status information for enhancing the training efficiency and testing accuracy of AOD by adding the cropped target object (TGOJ) from the current state as a new input. Different from the existing researches, a novel reward function, combing the area factor and distance factor of the bounding box, is designed to make the robot not only get closer to the TGOJ but also obtain a better observation viewpoint. Moreover, to overcome the differences between the training dataset and new environments as well as improving the adaptation of the AOD model, a reward-based long-term learning strategy including a novel training strategy is presented. The comparable experiments and the ablation study have been implemented in an AOD dataset, proving that our method owns better performance and efficiency than the comparable methods. Meanwhile, the experiments in the real-world scenario with a robot indicate the validity of the proposed method.",https://ieeexplore.ieee.org/document/9464751/,IEEE Transactions on Industrial Electronics,June 2022,ieeexplore
10.1109/LRA.2022.3142439,Anytime 3D Object Reconstruction Using Multi-Modal Variational Autoencoder,IEEE,Journals,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository<sup>1</sup>.",https://ieeexplore.ieee.org/document/9681277/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/TRO.2021.3084374,Cat-Like Jumping and Landing of Legged Robots in Low Gravity Using Deep Reinforcement Learning,IEEE,Journals,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",https://ieeexplore.ieee.org/document/9453856/,IEEE Transactions on Robotics,Feb. 2022,ieeexplore
10.1109/JAS.2021.1003907,Domain-Invariant Similarity Activation Map Contrastive Learning for Retrieval-Based Long-Term Visual Localization,IEEE,Journals,"Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.",https://ieeexplore.ieee.org/document/9358457/,IEEE/CAA Journal of Automatica Sinica,February 2022,ieeexplore
10.1109/LRA.2022.3142433,Enabling Low-Cost Full Surface Tactile Skin for Human Robot Interaction,IEEE,Journals,"Realizing full coverage, low-maintenance, and low-cost tactile skin is a <italic>de facto</italic> design dream since the invention of robots. It ensures safety and enables collaborative work protocols for human robot interactions (HRI). The on-robot tactile capability is realized by deploying an array of external sensors or inferring from proprioceptive information that comes with the robot, such as motor torque. However, these methods may be cumbersome, introduce extra management cost, expensive, lack real-world robustness, or require special robot designs. In this letter, we present <italic>SonicSkin</italic>, a low-cost ($2) and easy to deploy system that localizes the on-robot human touch and estimates the touch pressure without actually attaching sensors at potential touch locations. The system requires only a single pair of piezoelectric transducers (<italic>i.e.</italic> one transmitter and one receiver) attached on the target robot and turns the robot itself into a versatile sensor. We present a set of novel algorithms to progressively address the unique challenges posed by our system design. We put together an end-to-end <italic>SonicSkin</italic> system on a Jaco robot arm that runs in real-time, and conducted an extensive real-world study including 57019 actual evaluation datapoints under various challenging conditions from 12 human subjects. <italic>SonicSkin</italic> achieves less than 2 cm localization error for 96.4% of touches, with more than 96.7% cross-correlation similarity between the predicted touch pressure and the ground truth touch pressure.",https://ieeexplore.ieee.org/document/9681158/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2022.3145971,Focus on Impact: Indoor Exploration With Intrinsic Motivation,IEEE,Journals,"Exploration of indoor environments has recently experienced a significant interest, also thanks to the introduction of deep neural agents built in a hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on simulated environments. Current state-of-the-art methods employ a dense extrinsic reward that requires the complete a priori knowledge of the layout of the training environment to learn an effective exploration policy. However, such information is expensive to gather in terms of time and resources. In this work, we propose to train the model with a purely intrinsic reward signal to guide exploration, which is based on the impact of the robot’s actions on its internal representation of the environment. So far, impact-based rewards have been employed for simple tasks and in procedurally generated synthetic environments with countable states. Since the number of states observable by the agent in realistic indoor environments is non-countable, we include a neural-based density model and replace the traditional count-based regularization with an estimated pseudo-count of previously visited states. The proposed exploration approach outperforms DRL-based competitors relying on intrinsic rewards and surpasses the agents trained with a dense extrinsic reward computed with the environment layouts. We also show that a robot equipped with the proposed approach seamlessly adapts to point-goal navigation and real-world deployment.",https://ieeexplore.ieee.org/document/9691914/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/TFUZZ.2020.3033141,Fuzzy Double Deep Q-Network-Based Gait Pattern Controller for Humanoid Robots,IEEE,Journals,"In this article, the adaptive-network-based fuzzy inference system (ANFIS) is combined with the double deep <italic>Q</italic>-network (DDQN) to realize a fuzzy DDQN (FDDQN) such that a humanoid robot can generate a linear inverted pendulum model-based gait pattern in real time. The FDDQN not only allows the humanoid robot to correct the gait pattern instantly but also improves its stability. The proposed scheme is designed and implemented in a toddler-sized humanoid robot called Louis. First, four pressure sensors are installed on the bottom of the sole and one inertial measurement unit is set up on the trunk of the robot. A wireless communication chip is employed to transfer the data to a computer to determine the required parameters for the robot. Next, a control system based on the Linux operating system is developed. The values of the center of pressure and acceleration obtained with the ANFIS are adopted to train the DDQN. The proposed neural network comprises four layers, and the model is cautiously selected to avoid overfitting. The proposed scheme is verified using a robot simulator and then real-time-tested on Louis. The experimental results indicate that the FDDQN can provide the robot timely feedback during walking as well as helps it in adjusting the gait pattern independently. The balancing of the robot through effective dynamic feedback is similar to the balancing ability of an infant learning to walk.",https://ieeexplore.ieee.org/document/9237162/,IEEE Transactions on Fuzzy Systems,Jan. 2022,ieeexplore
10.1109/LRA.2022.3146515,Kineverse: A Symbolic Articulation Model Framework for Model-Agnostic Mobile Manipulation,IEEE,Journals,"Service robots in the future need to execute abstract instructions such as “fetch the milk from the fridge”. To translate such instructions into actionable plans, robots require in-depth background knowledge. With regards to interactions with doors and drawers, robots require articulation models that they can use for state estimation and motion planning. Existing frameworks model articulated connections as abstract concepts such as <italic>prismatic</italic>, or <italic>revolute</italic>, but do not provide a parameterized model of these connections for computation. In this letter, we introduce a novel framework that uses symbolic mathematical expressions to model articulated structures – robots and objects alike – in a unified and extensible manner. We provide a theoretical description of this framework, and the operations that are supported by its models, and introduce an architecture to exchange our models in robotic applications, making them as flexible as any other environmental observation. To demonstrate the utility of our approach, we employ our practical implementation <italic>Kineverse</italic> for solving common robotics tasks from state estimation and mobile manipulation, and use it further in real-world mobile robot manipulation.",https://ieeexplore.ieee.org/document/9695204/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2021.3129136,OCRTOC: A Cloud-Based Competition and Benchmark for Robotic Grasping and Manipulation,IEEE,Journals,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is <uri>www.ocrtoc.org</uri>, and the OCRTOC software package is available at <uri>https://github.com/OCRTOC/OCRTOC_software_package</uri>.",https://ieeexplore.ieee.org/document/9619915/,IEEE Robotics and Automation Letters,Jan. 2022,ieeexplore
10.1109/LRA.2022.3141150,REVE-CE: Remote Embodied Visual Referring Expression in Continuous Environment,IEEE,Journals,"Ithas always been a great challenge for the robot to navigate in the visual world following natural language instructions. Recently, several tasks such as the Vision-and-Language Navigation (VLN) and Remote Embodied Visual Referring Expression in Real Indoor Environments (REVERIE) are proposed trying to solve this challenge. And the most significant difference between VLN and REVERIE tasks is that REVERIE uses a higher guidance level instruction. However, the navigation process of REVERIE is implemented in a discrete environment, which is unrealistic in real world scenarios. To make the REVERIE task more consistent with the real physical world, we develop a new task of Remote Embodied Visual Referring Expression in Continuous Environment, namely REVE-CE, in which the agent executes a much longer sequence of low-level actions given language instructions. Furthermore, we propose a multi-branch cross modal attention (MBCMA) framework to solve the proposed REVE-CE task. Extensive experiments are conducted demonstrating that the proposed framework greatly outperforms the state-of-the-art VLN baselines and a new benchmark for the proposed REVE-CE task is built.",https://ieeexplore.ieee.org/document/9674225/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2022.3143289,VisuoTactile 6D Pose Estimation of an In-Hand Object Using Vision and Tactile Sensor Data,IEEE,Journals,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",https://ieeexplore.ieee.org/document/9682507/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
