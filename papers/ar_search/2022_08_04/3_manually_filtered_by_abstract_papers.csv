id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract
1,unknown,http://arxiv.org/abs/2201.11067v3,arxiv,arxiv,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement',http://arxiv.org/abs/2201.11067v3,2022-01-26,roma: resource orchestration for microservices-based 5g applications,"With the growth of 5G, Internet of Things (IoT), edge computing and cloud
computing technologies, the infrastructure (compute and network) available to
emerging applications (AR/VR, autonomous driving, industry 4.0, etc.) has
become quite complex. There are multiple tiers of computing (IoT devices, near
edge, far edge, cloud, etc.) that are connected with different types of
networking technologies (LAN, LTE, 5G, MAN, WAN, etc.). Deployment and
management of applications in such an environment is quite challenging. In this
paper, we propose ROMA, which performs resource orchestration for
microservices-based 5G applications in a dynamic, heterogeneous, multi-tiered
compute and network fabric. We assume that only application-level requirements
are known, and the detailed requirements of the individual microservices in the
application are not specified. As part of our solution, ROMA identifies and
leverages the coupling relationship between compute and network usage for
various microservices and solves an optimization problem in order to
appropriately identify how each microservice should be deployed in the complex,
multi-tiered compute and network fabric, so that the end-to-end application
requirements are optimally met. We implemented two real-world 5G applications
in video surveillance and intelligent transportation system (ITS) domains.
Through extensive experiments, we show that ROMA is able to save up to 90%, 55%
and 44% compute and up to 80%, 95% and 75% network bandwidth for the
surveillance (watchlist) and transportation application (person and car
detection), respectively. This improvement is achieved while honoring the
application performance requirements, and it is over an alternative scheme that
employs a static and overprovisioned resource allocation strategy by ignoring
the resource coupling relationships."
2,unknown,10.1007/978-3-030-69893-5_21,Springer,springer,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement',http://dx.doi.org/10.1007/978-3-030-69893-5_21,2021-01-01,application design and service provisioning for multi-access edge cloud (mec),"The edge cloud is attractive to provide low latency services to mobile users. It overcomes computation, storage, and energy limitations of mobile devices through computation offloading. It also avoids long delays in migration of big data from the point of their generation by IoT devices to the centralized data centers. Context-aware edge cloud design provides mobile users with more personalized and customized services that improve their over-all experience. It manages the cloud infrastructure for resource provisioning, scheduling, and load balancing. The latency constraints of MEC applications need light-weight container service in the edge cloud. Kubernetes container orchestration is popular in the industry that is supported by all major edge cloud platforms. Container migration is important for ensuring low latency to new mobile applications of connected vehicles and drones. In this chapter we present the current state of research and development in the application design and service provisioning for edge cloud."
3,unknown,10.1109/vtc2021-fall52928.2021.9625307,IEEE,ieeexplore,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement',https://ieeexplore.ieee.org/document/9625307/,2021-09-30,machine learning based mmwave orchestration for edge gaming qoe enhancement,"Millimeter wave (mmWave) is a crucial component in 5G and beyond 5G communications. However, the dense deployment of mmWave transceivers imposes a heavy burden on the management of radio access network (RAN). This challenge increases the need for autonomous network management methods leveraging machine learning (ML) techniques. In particular, mmWave beam selection is a critical issue for the management of RAN due to the large training overhead on mmWave transceivers. To this end, a new beam tracking method based on sequence-to-sequence (Seq2Seq) learning is proposed. Besides, thanks to edge computing technologies, network management algorithms and delay-sensitive user applications can be hosted on edge servers in close proximity. Due to limited resources on the edge server, the resource allocation problem for beam tracking and edge gaming is investigated with the aim of maximizing game quality of experience (QoE). Simulation results verify the effectiveness of the proposed orchestration scheme."
4,unknown,10.1007/s13369-022-06563-5,Springer,springer,augmented reality,'augmented reality' AND 'edge' AND 'orchestration' AND 'placement',http://dx.doi.org/10.1007/s13369-022-06563-5,2022-01-29,optimized resource allocation for fog network using neuro-fuzzy offloading approach,"Fog computing has emerged as one of the most important Internet infrastructures for improving service quality, particularly in real-time applications. Due to the convergence in technologies, the scope of the Internet of things (IoT) has evolved to a new dimension, it expands from data collection to device interconnections, and to pre-processing. This acceleration involves cloud and fog computing layers into the system which plays an integral role in IoT data storage and computing. Due to the diversity present in IoT devices, selection of computation devices and allocation of resources are major challenges to be addressed for efficient utilization of resources. In this paper, we presented the offloading and resource allocation model to address the solution to the above challenge. Firstly, a 5-layered neuro-fuzzy model is introduced to retrieve the fuzzy sets and rules which further passes to the fuzzy inference system to model an orchestration decision system. Additionally, to improve the system performance, we have presented the modified least loaded resource allocation algorithm which is adaptively required to reduce the failure rate of the applications. To showcase the efficacy of the model, 4 healthcare applications (augmented reality, patient pre-monitoring, record analysis, and billing systems) are evaluated with their heterogeneous parameters. The simulation findings show that our suggested model improves system performance by lowering network latency by 2.23–9.68 %, computation delay by 3.40–13.66 %, and system performance by 1.03–11.55%. The simulation results demonstrated the suggested model’s resilience in terms of network latency, computation time, and failure rate."
