doi,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1109/ccnc49033.2022.9700725,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,mitigating location-based attacks using predication models in vehicular ad-hoc networks,https://ieeexplore.ieee.org/document/9700725/,"The modern world is constantly in a state of technological revolution. Everyday new technological ideas, inventions, and threats emerge. With modern computer software and hardware advancements, we have the emergence of the Internet of Things (IoT). In conjunction, modern car companies have a push from public demand for a fully-autonomous car. To accomplish autonomy, small, and secure Vehicular Ad-Hoc Networks (VANETs) it is necessary to ensure that the systems that rely on connected vehicle data is reliable and accurate. In the event there is a malicious actor manipulating the data through replica and injection attacks or there is a hardware failure yielding inaccurate location information, it is necessary to explore efficient methods for predicting connected vehicles locations such that these systems, which rely on accurate information are not impacted. This study analyzes multiple clustering and prediction models to discover how effectively a multi-layered machine learning approach is able to meet the real-time requirement of future generation smart cities.",autonomous vehicle
10.1109/ccnc49033.2022.9700636,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,balancing latency and accuracy on deep video analytics at the edge,https://ieeexplore.ieee.org/document/9700636/,"Real-time deep video analytic at the edge is an enabling technology for emerging applications, such as vulnerable road user detection for autonomous driving, which requires highly accurate results of model inference within a low latency. In this paper, we investigate the accuracy-latency trade-off in the design and implementation of real-time deep video analytic at the edge. Without loss of generality, we select the widely used YOLO-based object detection and WebRTC-based video streaming for case study. Here, the latency consists of both networking latency caused by video streaming and the processing latency for video encoding/decoding and model inference. We conduct extensive measurements to figure out how the dynamically changing settings of video streaming affect the achieved latency, the quality of video, and further the accuracy of model inference. Based on the findings, we propose a mechanism for adapting video streaming settings (i.e. bitrate, resolution) online to optimize the accuracy of video analytic within latency constraints. The mechanism has proved, through a simulated setup, to be efficient in searching the optimal settings.",autonomous vehicle
10.1109/iccece54139.2022.9712792,preprocessed,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),IEEE,2022-01-16 00:00:00,ieeexplore,design of deep learning based autonomous driving control algorithm,https://ieeexplore.ieee.org/document/9712792/,"In recent years, with the continuous development of the field of artificial intelligence, autonomous driving technology has gained widespread attention. In order to meet the purpose of changing driving behavior and completing driving tasks in real time without human intervention. In this paper, we study the design and implementation process of end-to-end autonomous driving algorithms based on computer vision and deep learning, and explain the elements of algorithm design from a theoretical perspective. The method of continuous steering angle prediction for autonomous driving based on convolutional neural network is proposed, as well as the method of network pre-training and overfitting prevention to improve the training effect and generalization ability. The difference with the traditional end-to-end control methods is that the traditional methods study the problem abstractly as a classification problem, describing the motion in terms of direction with a coarser granularity. The method proposed in this paper treats it as a regression problem, describing the motion in terms of steering angles, which provides a more accurate description of the motion and is more adaptive.",autonomous vehicle
10.1109/wacv51458.2022.00206,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,plugging self-supervised monocular depth into unsupervised domain adaptation for semantic segmentation,https://ieeexplore.ieee.org/document/9707096/,"Although recent semantic segmentation methods have made remarkable progress, they still rely on large amounts of annotated training data, which are often infeasible to collect in the autonomous driving scenario. Previous works usually tackle this issue with Unsupervised Domain Adaptation (UDA), which entails training a network on synthetic images and applying the model to real ones while minimizing the discrepancy between the two domains. Yet, these techniques do not consider additional information that may be obtained from other tasks. Differently, we propose to exploit self-supervised monocular depth estimation to improve UDA for semantic segmentation. On one hand, we deploy depth to realize a plug-in component which can inject complementary geometric cues into any existing UDA method. We further rely on depth to generate a large and varied set of samples to Self-Train the final model. Our whole proposal allows for achieving state-of-the-art performance (58.8 mIoU) in the GTA5 → CS benchmark. Code is available at https://github.com/CVLAB-Unibo/d4-dbst.",autonomous vehicle
10.1109/jas.2021.1003907,preprocessed,IEEE/CAA Journal of Automatica Sinica,IEEE,2022-02-01 00:00:00,ieeexplore,domain-invariant similarity activation map contrastive learning for retrieval-based long-term visual localization,https://ieeexplore.ieee.org/document/9358457/,"Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.",autonomous vehicle
10.1007/s11042-021-11437-3,preprocessed,Multimedia Tools and Applications,Springer,2022-01-01 00:00:00,springer,deep reinforcement learning based control for autonomous vehicles in carla,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11437-3,"Nowadays, Artificial Intelligence (AI) is growing by leaps and bounds in almost all fields of technology, and Autonomous Vehicles (AV) research is one more of them. This paper proposes the using of algorithms based on Deep Learning (DL) in the control layer of an autonomous vehicle. More specifically, Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) are implemented in order to compare results between them. The aim of this work is to obtain a trained model, applying a DRL algorithm, able of sending control commands to the vehicle to navigate properly and efficiently following a determined route. In addition, for each of the algorithms, several agents are presented as a solution, so that each of these agents uses different data sources to achieve the vehicle control commands. For this purpose, an open-source simulator such as CARLA is used, providing to the system with the ability to perform a multitude of tests without any risk into an hyper-realistic urban simulation environment, something that is unthinkable in the real world. The results obtained show that both DQN and DDPG reach the goal, but DDPG obtains a better performance. DDPG perfoms trajectories very similar to classic controller as LQR. In both cases RMSE is lower than 0.1m following trajectories with a range 180-700m. To conclude, some conclusions and future works are commented.",autonomous vehicle
http://arxiv.org/abs/2202.04224v1,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,intelligent autonomous intersection management,http://arxiv.org/abs/2202.04224v1,"Connected Autonomous Vehicles will make autonomous intersection management a
reality replacing traditional traffic signal control. Autonomous intersection
management requires time and speed adjustment of vehicles arriving at an
intersection for collision-free passing through the intersection. Due to its
computational complexity, this problem has been studied only when vehicle
arrival times towards the vicinity of the intersection are known beforehand,
which limits the applicability of these solutions for real-time deployment. To
solve the real-time autonomous traffic intersection management problem, we
propose a reinforcement learning (RL) based multiagent architecture and a novel
RL algorithm coined multi-discount Q-learning. In multi-discount Q-learning, we
introduce a simple yet effective way to solve a Markov Decision Process by
preserving both short-term and long-term goals, which is crucial for
collision-free speed control. Our empirical results show that our RL-based
multiagent solution can achieve near-optimal performance efficiently when
minimizing the travel time through an intersection.",autonomous vehicle
http://arxiv.org/abs/2202.02352v1,preprocessed,arxiv,arxiv,2022-02-04 00:00:00,arxiv,"learning interpretable, high-performing policies for continuous control
  problems",http://arxiv.org/abs/2202.02352v1,"Gradient-based approaches in reinforcement learning (RL) have achieved
tremendous success in learning policies for continuous control problems. While
the performance of these approaches warrants real-world adoption in domains,
such as in autonomous driving and robotics, these policies lack
interpretability, limiting deployability in safety-critical and
legally-regulated domains. Such domains require interpretable and verifiable
control policies that maintain high performance. We propose Interpretable
Continuous Control Trees (ICCTs), a tree-based model that can be optimized via
modern, gradient-based, RL approaches to produce high-performing, interpretable
policies. The key to our approach is a procedure for allowing direct
optimization in a sparse decision-tree-like representation. We validate ICCTs
against baselines across six domains, showing that ICCTs are capable of
learning interpretable policy representations that parity or outperform
baselines by up to 33$\%$ in autonomous driving scenarios while achieving a
$300$x-$600$x reduction in the number of policy parameters against deep
learning baselines.",autonomous vehicle
http://arxiv.org/abs/2202.00091v1,preprocessed,arxiv,arxiv,2022-01-31 00:00:00,arxiv,"query efficient decision based sparse attacks against black-box deep
  learning models",http://arxiv.org/abs/2202.00091v1,"Despite our best efforts, deep learning models remain highly vulnerable to
even tiny adversarial perturbations applied to the inputs. The ability to
extract information from solely the output of a machine learning model to craft
adversarial perturbations to black-box models is a practical threat against
real-world systems, such as autonomous cars or machine learning models exposed
as a service (MLaaS). Of particular interest are sparse attacks. The
realization of sparse attacks in black-box models demonstrates that machine
learning models are more vulnerable than we believe. Because these attacks aim
to minimize the number of perturbed pixels measured by l_0 norm-required to
mislead a model by solely observing the decision (the predicted label) returned
to a model query; the so-called decision-based attack setting. But, such an
attack leads to an NP-hard optimization problem. We develop an evolution-based
algorithm-SparseEvo-for the problem and evaluate against both convolutional
deep neural networks and vision transformers. Notably, vision transformers are
yet to be investigated under a decision-based attack setting. SparseEvo
requires significantly fewer model queries than the state-of-the-art sparse
attack Pointwise for both untargeted and targeted attacks. The attack
algorithm, although conceptually simple, is also competitive with only a
limited query budget against the state-of-the-art gradient-based whitebox
attacks in standard computer vision tasks such as ImageNet. Importantly, the
query efficient SparseEvo, along with decision-based attacks, in general, raise
new questions regarding the safety of deployed systems and poses new directions
to study and understand the robustness of machine learning models.",autonomous vehicle
http://arxiv.org/abs/2201.05797v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"finding label and model errors in perception data with learned
  observation assertions",http://arxiv.org/abs/2201.05797v1,"ML is being deployed in complex, real-world scenarios where errors have
impactful consequences. In these systems, thorough testing of the ML pipelines
is critical. A key component in ML deployment pipelines is the curation of
labeled training data. Common practice in the ML literature assumes that labels
are the ground truth. However, in our experience in a large autonomous vehicle
development center, we have found that vendors can often provide erroneous
labels, which can lead to downstream safety risks in trained models.
  To address these issues, we propose a new abstraction, learned observation
assertions, and implement it in a system called Fixy. Fixy leverages existing
organizational resources, such as existing (possibly noisy) labeled datasets or
previously trained ML models, to learn a probabilistic model for finding errors
in human- or model-generated labels. Given user-provided features and these
existing resources, Fixy learns feature distributions that specify likely and
unlikely values (e.g., that a speed of 30mph is likely but 300mph is unlikely).
It then uses these feature distributions to score labels for potential errors.
We show that FIxy can automatically rank potential errors in real datasets with
up to 2$\times$ higher precision compared to recent work on model assertions
and standard techniques such as uncertainty sampling.",autonomous vehicle
http://arxiv.org/abs/2201.05518v1,preprocessed,arxiv,arxiv,2022-01-14 00:00:00,arxiv,ugv-uav object geolocation in unstructured environments,http://arxiv.org/abs/2201.05518v1,"A robotic system of multiple unmanned ground vehicles (UGVs) and unmanned
aerial vehicles (UAVs) has the potential for advancing autonomous object
geolocation performance. Much research has focused on algorithmic improvements
on individual components, such as navigation, motion planning, and perception.
In this paper, we present a UGV-UAV object detection and geolocation system,
which performs perception, navigation, and planning autonomously in real scale
in unstructured environment. We designed novel sensor pods equipped with
multispectral (visible, near-infrared, thermal), high resolution (181.6 Mega
Pixels), stereo (near-infrared pair), wide field of view (192 degree HFOV)
array. We developed a novel on-board software-hardware architecture to process
the high volume sensor data in real-time, and we built a custom AI subsystem
composed of detection, tracking, navigation, and planning for autonomous
objects geolocation in real-time.
  This research is the first real scale demonstration of such high speed data
processing capability. Our novel modular sensor pod can boost relevant computer
vision and machine learning research. Our novel hardware-software architecture
is a solid foundation for system-level and component-level research. Our system
is validated through data-driven offline tests as well as a series of field
tests in unstructured environments. We present quantitative results as well as
discussions on key robotic system level challenges which manifest when we build
and test the system. This system is the first step toward a UGV-UAV cooperative
reconnaissance system in the future.",autonomous vehicle
10.1016/j.jag.2021.102652,preprocessed,International Journal of Applied Earth Observation and Geoinformation,scopus,2022-02-01,sciencedirect,developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big uav imagery data,https://api.elsevier.com/content/abstract/scopus_id/85122505895,"The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.",autonomous vehicle
10.1016/j.compag.2021.106574,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,perennial ryegrass biomass retrieval through multispectral uav data,https://api.elsevier.com/content/abstract/scopus_id/85122407603,"Frequent biomass measurement is a key activity for optimal perennial ryegrass (Lolium perenne) management in intensive forage-based dairy operations. Due to the necessary high-frequency (i.e., weekly or monthly) pasture monitoring and continuous trend of larger dairy farms, such activity is perceived as an operational bottleneck. Consequently, substantial effort is directed to the development of accurate and automated technological solutions for biomass assessment. The popularization of unmanned aerial vehicles (UAVs) combined with multispectral cameras should allow for an optimal observational system able to deploy machine learning algorithms for near real-time biomass dry-matter (DM) mapping. For successful operation, these systems should deliver radiometrically accurate orthomosaics and robust models able to generalize across different periods. Nevertheless, the accuracy of radiometric calibration and generalization ability of these models is seldom evaluated. Also, such pipelines should require minimum processing power and allow for fast deployment. This study has established a two-year experiment comparing reflectance measurements between a handheld spectrometer and a commercial multispectral UAV camera. Different algorithms based on regression-tree architecture were contrasted regarding accuracy, speed, and model size. Model performances were validated, providing error-metrics for baseline accuracy and temporal validation. The results have shown that the standard procedure for multispectral imagery radiometric calibration is sub-optimal, requiring further post-processing and presenting low correlation with handheld measurements across spectral bands and dates. Nevertheless, after post-calibration, the use of spectral imagery has presented better baseline error than the point-based sensors, respectively displaying an average of 397.3 and 464.2 kg DM/ha when employed alongside the best performing algorithm (i.e., Cubist). When trained and validated across different years, model performance was largely reduced and deemed unfit for operational purposes. The Cubist/M5 family of algorithms have exhibited advantageous characteristics such as compact model structure, allowing for a higher level of model interpretability, while displaying a smaller size and faster deployment than the Random Forest, Boosted, and Bagged Regression Trees algorithms.",autonomous vehicle
10.1016/j.aap.2021.106473,preprocessed,Accident Analysis and Prevention,scopus,2022-02-01,sciencedirect,mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors,https://api.elsevier.com/content/abstract/scopus_id/85118989110,"Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.",autonomous vehicle
10.1016/j.inffus.2021.09.004,preprocessed,Information Fusion,scopus,2022-02-01,sciencedirect,multimodal earth observation data fusion: graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",autonomous vehicle
10.1016/j.isatra.2022.01.014,preprocessed,ISA Transactions,scopus,2022-01-01,sciencedirect,"intelligent framework for automated failure prediction, detection, and classification of mission critical autonomous flights",https://api.elsevier.com/content/abstract/scopus_id/85123893673,"Autonomous flights are the major industry contributors towards next-generation developments in pervasive and ubiquitous computing. Modern aerial vehicles are designed to receive actuator commands from the primary autopilot software as input to regulate their servos for adjusting control surfaces. Due to real-time interaction with the actual physical environment, there exists a high risk of control surface failures for engine, rudder, elevators, and ailerons etc. If not anticipated and then timely controlled, failures occurring during the flight can have severe and cataclysmic consequences, which may result in mid-air collision or ultimate crash. Humongous amount of sensory data being generated throughout mission-critical flights, makes it an ideal candidate for applying advanced data-driven machine learning techniques to identify intelligent insights related to failures for instant recovery from emergencies. In this paper, we present a novel framework based on machine learning techniques for failure prediction, detection, and classification for autonomous aerial vehicles. The proposed framework utilizes long short-term memory recurrent neural network architecture to analyze time series data and has been applied at the AirLab Failure and Anomaly flight dataset, which is a comprehensive publicly available dataset of various fault types in fixed-wing autonomous aerial vehicles’ control surfaces. The proposed framework is able to predict failure with an average accuracy of 93% and the average time-to-predict a failure is 19 s before the actual occurrence of the failure, which is 10 s better than current state-of-the-art. Failure detection accuracy is 100% and average detection time is 0.74 s after happening of failure, which is 1.28 s better than current state-of-the-art. Failure classification accuracy of proposed framework is 100%. The performance analysis shows the strength of the proposed methodology to be used as a real-time failure prediction and a pseudo-real-time failure detection along with a failure classification framework for eventual deployment with actual mission-critical autonomous flights.",autonomous vehicle
10.1016/j.trc.2021.103499,preprocessed,Transportation Research Part C: Emerging Technologies,scopus,2022-01-01,sciencedirect,do autonomous vehicles drive like humans? a turing approach and an application to sae automation level 2 cars,https://api.elsevier.com/content/abstract/scopus_id/85120490088,"Fully automated vehicles (AVs) are set to become a reality in future decades and changes are to be expected in user perceptions and behavior. While AV acceptability has been widely studied, changes in human drivers’ behavior and in passengers’ reactions have received less attention. It is not yet possible to ascertain the risk of driver behavioral changes such as overreaction, and the corresponding safety problems, in mixed traffic with partially AVs. Nor has there been proper investigation of the potential unease of car occupants trained for human control, when exposed to automatic maneuvers. The conjecture proposed in this paper is that automation Level 2 vehicles do not induce potentially adverse effects in traditional vehicle drivers’ behavior or in occupants’ reactions, provided that they are indistinguishable from human-driven vehicles. To this end, the paper proposes a Turing approach to test the “humanity” of automation Level 2 vehicles. The proposed test was applied to the results of an experimental campaign carried out in Italy: 546 car passengers were interviewed on board Level 2 cars in which they could not see the driver. They were asked whether a specific driving action (braking, accelerating, lane keeping) had been performed by the human driver or by the automatic on-board software under different traffic conditions (congestion and speed). Estimation results show that in most cases the interviewees were unable to distinguish the Artificial Intelligence (AI) from the human driver by observing random responses with a 95% significance level (proportion of success statistically equal to 50%). However, in the case of moderate braking and lane keeping at >100 km/h and in high traffic congestion, respondents recognized AI control from the human driver above pure chance, with 62–69% correct response rates. These findings, if confirmed in other case studies, could significantly impact on AVs acceptability, also contributing to their design as well as to long-debated ethical questions. AI driving software could be designed and tested for “humanity”, as long as safety is guaranteed, and autonomous cars could be allowed to circulate as long as they cannot be distinguished from human-driven vehicles in recurrent driving conditions.",autonomous vehicle
10.1016/j.dsp.2021.103290,preprocessed,Digital Signal Processing: A Review Journal,scopus,2022-01-01,sciencedirect,deep residual learning-based cognitive model for detection and classification of transmitted signal patterns in 5g smart city networks,https://api.elsevier.com/content/abstract/scopus_id/85118634214,"Primary user (PU) signal detection or classification is a critical component of cognitive radio (CR) related wireless communication applications. In CR, the PU detection methods are mostly based on statistical models, and their detection performance heavily relies on the accuracy of assumed models. In this paper, we design a novel detector, dubbed as PU-Net, that dynamically learns the PU activity patterns in a cognitive 5G smart city, where a network of unmanned aerial vehicles (UAVs) is deployed as flying base stations to serve the Internet-of-Things (IoT) users. Unlike the traditional schemes, the PU-Net is free from signal-noise model assumptions and is leveraged through deep residual learning integrated with atrous spatial pyramid pooling (ASPP) to sense the PU's transmitted signal patterns in the network. The PU-Net detects and classifies the active and idle PU states by exploiting the multilevel spatial-temporal features in the signal and noise frames. The proposed model is trained using locally synthesized Rayleigh channel-impaired data with large variability of modulated signals and different noise floor regimes. Additionally, the PU-Net model is blind-tested and evaluated on real-world over-the-air signals and with variable-length frames and varying channel effects at secondary users (SUs). With extensive experiments, it is shown that PU-Net outperforms other benchmark detectors, obtaining an accuracy of 0.9974, with 0.9978 recall and 0.9970 precision in detecting and classifying the PU transmitted signal patterns. Correspondingly, the proposed PU-Net can be adopted for IoT/UAV-assisted communication systems in optimizing spectrum efficiency and resolving the coexistence issues in 5G and beyond networks.",autonomous vehicle
10.1016/j.engappai.2021.104514,preprocessed,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,instance-based defense against adversarial attacks in deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",autonomous vehicle
10.1016/j.inffus.2021.07.004,preprocessed,Information Fusion,scopus,2022-01-01,sciencedirect,saccadefork: a lightweight multi-sensor fusion-based target detector,https://api.elsevier.com/content/abstract/scopus_id/85112374720,"Commercialization of self-driving applications requires precision and reliability of the perception system due to the highly dynamic and complex road environment. Early perception systems either rely on the camera or on LiDAR for moving obstacle detection. With the development of vehicular sensors and deep learning technologies, the multi-view and sensor fusion based convolutional neural network (CNN) model for detection tasks has become a popular research area. In this paper, we present a novel multi-sensor fusion-based CNN model–SaccadeFork–that integrates the image and upsampled LiDAR point clouds as the input. SaccadeFork includes two modules: (1) a lightweight backbone that consists of hourglass convolution feature extraction module and a parallel dilation convolution module for adaptation of the system to different target sizes; (2) an anchor-based detection head. The model also considers deployment of resource-limited edge devices in the vehicle. Two refinement strategies, i.e., Mixup and Swish activation function are also adopted to improve the model. Comparison with a series of latest models on public dataset of KITTI shows that SaccadeFork can achieve the optimal detection accuracy on vehicles and pedestrians under different scenarios. The final model is also deployed and tested on a local dataset collected based on edge devices and low-cost sensor solutions, and the results show that the model can achieve real-time efficiency and high detection accuracy.",autonomous vehicle
10.1109/jiot.2021.3096637,preprocessed,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,an integrated framework for health state monitoring in a smart factory employing iot and big data techniques,https://ieeexplore.ieee.org/document/9481251/,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",health
10.1109/tie.2021.3065616,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-03-01 00:00:00,ieeexplore,health management of dry-type transformer based on broad learning system,https://ieeexplore.ieee.org/document/9380956/,"This article presents a novel health management method of the dry-type transformer to diagnose the early unhealthy behavior and evaluate the transformer's health condition by health score. The health condition diagnosis implemented by a proposed dynamic-weighted-feed-back broad learning system (BLS) (DW-FB-BLS) method, which helps to determine the BLS network structure effectively, and adjusts the weight of features in the online application to avoid reduction of accuracy caused by concept drift. Then, a rational score rule is set to evaluate the health condition of the dry-type transformer by health score, which allows intuitive presentation and preservation of transformer's health condition over a long period. Finally, the effectiveness and validity of the proposed method are verified based on the real field data of dry-type transformer. Satisfactory results for unhealthy behavior diagnosis and health evaluation are obtained, it shows that health management of this article can reflect the real health condition of dry-type transformer appropriately.",health
10.1109/comsnets53615.2022.9668420,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,ml-based device-agnostic human activity detection with wifi sniffer traffic,https://ieeexplore.ieee.org/document/9668420/,"Human Activity Detection plays a pivotal role in smoothly managing the health care for the elderly and those with chronic health conditions in smart home environments. Even though there are several technological advancements made in this area, solutions like smartwatches are costly to afford and the solutions that rely on sensors and cameras suffer from privacy concerns. While wireless channel state information-based approaches can address some of these limitations, these approaches either require special hardware to be deployed or modifications at the WiFi access point. In this paper, we propose to detect human activities in a device-agnostic manner by leveraging passively captured passively captured WiFi MAC-layer traffic with the help of a sniffer. In that way, elderly people and those who suffer from chronic health conditions do not need to put any sensors on their body. This approach is not only cost-effective, but it is also easy to deploy without requiring any changes at the WiFi access point or installing special sensors in the environment. We train and test six off-the-shelf machine learning models on 15+ hours worth of WiFi MAC-layer traffic collected in a home environment. We present a proof-of-concept system prototype that employs this approach. We are able to detect six activities - (a) Walking vs Sitting, (b) Sleeping vs Awake, and (c) Using Phone vs Not Using Phone in three different real-world scenarios. Our evaluation reveals that WiFi MAC-layer traffic has special signatures to detect human activities and we are able to achieve 92.49 % detection accuracy in the best case.",health
10.1109/access.2021.3114590,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,correlation-aware sport training evaluation for players with trust based on mahalanobis distance,https://ieeexplore.ieee.org/document/9543705/,"With the widely-adopted idea of health and longevity, sports have been becoming one of the most popular entertainment ways of the public. For the majority of sports, players need to know about their concrete physical conditions in a real time manner so as to pursue a good sport score or ranking in a competition or a race. Generally, we can achieve the above goal through analyzing and evaluating the daily training scores of each player. However, there are often multiple physical trainings for players and various correlations are existent among them, which significantly decrease the fairness and trust of player training score evaluation and ranking since traditional multi-dimensional data integration solutions are often based on a strong hypothesis, i.e., the involved multiple dimensions are independent with each other. In view of this shortcoming, we introduce the Mahalanobis Distance into the multi-dimensional player training score evaluation and further propose a <underline>c</underline>orrelation-aware <underline>p</underline>layer training score <underline>e</underline>valuation method with trust (abbreviated as CPE<sub>MD</sub>) based on <underline>M</underline>ahalanobis <underline>D</underline>istance. As Mahalanobis Distance can eliminate the hidden linear correlations among the involved multiple dimensions, we can guarantee the fairness and trust of Mahalanobis Distance-based player training score evaluation and ranking results. At last, we use a case study to show the feasibility of CPE<sub>MD</sub> in this paper.",health
10.1109/tie.2021.3068681,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-03-01 00:00:00,ieeexplore,degradation estimation and prediction of electronic packages using data-driven approach,https://ieeexplore.ieee.org/document/9390342/,"Recent trends in automotive electronics such as automated driving will increase the number and complexity of electronics used in safety-relevant applications. Applications in logistics or ridesharing will require a specific year of service rather than the conventional mileage usage. Reliable operations of the electronic systems must be assured at all times, regardless of the usage condition. A more dynamic and on-demand way of assuring the system availability will have to be developed. This article proposes a thermomechanical stress-based prognostics method as a potential solution. The goal is achieved by several novel advancements. On the experimental front, a key microelectronics package is developed to directly apply the prognostics and health management concept using a piezoresistive silicon-based stress sensor. Additional hardware for safe and secure data transmission and data processing is also developed, which is critically required for recording <italic>in situ</italic> and real-time data. On the data management front, proper data-driven approaches have to be identified to handle the unique dataset from the stress sensor employed in this study. The approaches effectively handle the massive amount of data that reveals the important information and automation of the prognostic process and thus to be able to detect, classify, locate, and predict the failure. The statistical techniques for diagnostics and the machine learning algorithms for health assessment and prognostics are also determined to implement the approaches in a simple, fast, but accurate way within the capacity of limited computing power. The proposed prognostics approach is implemented with actual microelectronics packages subjected to harsh accelerated testing conditions. The results corroborate the validity of the proposed prognostics approach.",health
10.1109/jbhi.2021.3119325,preprocessed,IEEE Journal of Biomedical and Health Informatics,IEEE,2022-01-01 00:00:00,ieeexplore,mix-and-interpolate: a training strategy to deal with source-biased medical data,https://ieeexplore.ieee.org/document/9568732/,"Till March 31st, 2021, the coronavirus disease 2019 (COVID-19) had reportedly infected more than 127 million people and caused over 2.5 million deaths worldwide. Timely diagnosis of COVID-19 is crucial for management of individual patients as well as containment of the highly contagious disease. Having realized the clinical value of non-contrast chest computed tomography (CT) for diagnosis of COVID-19, deep learning (DL) based automated methods have been proposed to aid the radiologists in reading the huge quantities of CT exams as a result of the pandemic. In this work, we address an overlooked problem for training deep convolutional neural networks for COVID-19 classification using real-world multi-source data, namely, the <italic>data source bias</italic> problem. The data source bias problem refers to the situation in which certain sources of data comprise only a single class of data, and training with such source-biased data may make the DL models learn to distinguish data sources instead of COVID-19. To overcome this problem, we propose MIx-aNd-Interpolate (MINI), a conceptually simple, easy-to-implement, efficient yet effective training strategy. The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performance upon the baseline (which does not deal with the source bias), and 2) MINI is superior to competing methods in terms of the extent of improvement.",health
10.1109/tsmc.2020.3018102,preprocessed,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2022-02-01 00:00:00,ieeexplore,novel fast and automatic condition monitoring strategy based on small amount of labeled data,https://ieeexplore.ieee.org/document/9194082/,"Signal-based automatic condition monitoring techniques are one of the effective ways to ensure the operational safety of modern industrial systems. Currently, the main challenge is to increase their effectiveness under noisy environment and with limited labeled data. In this article, a novel fast and automatic signal-based fault diagnosis procedure that does not require any kinds of machine learning algorithms is proposed. This procedure is based on the measurement of the similarity in the frequency domain between the collected data and a small amount of labeled reference signals (LRSs). The LRSs are obtained under various known operation conditions using fast Fourier transform (FFT) algorithm. The proposed approach is suitable for a real-time implementation and capable of successfully overcoming the challenge of condition monitoring of rotating machines under noisy environment and with limited labeled data. The merits, fastness, and also robustness against noise of the proposed technique are demonstrated experimentally through different practical applications, as well as compared to state-of-the-art procedures on a database of vibration signals measured under a variety of machine health and working conditions.",health
10.1109/jbhi.2021.3088750,preprocessed,IEEE Journal of Biomedical and Health Informatics,IEEE,2022-01-01 00:00:00,ieeexplore,stroke risk prediction with hybrid deep transfer learning framework,https://ieeexplore.ieee.org/document/9453166/,"Stroke has become a leading cause of death and long-term disability in the world with no effective treatment. Deep learning-based approaches have the potential to outperform existing stroke risk prediction models, but they rely on large well-labeled data. Due to the strict privacy protection policy in health-care systems, stroke data is usually distributed among different hospitals in small pieces. In addition, the positive and negative instances of such data are extremely imbalanced. Transfer learning can solve small data issue by exploiting the knowledge of a correlated domain, especially when multiple source of data are available. In this work, we propose a novel Hybrid Deep Transfer Learning-based Stroke Risk Prediction (HDTL-SRP) scheme to exploit the knowledge structure from multiple correlated sources (i.e., external stroke data, chronic diseases data, such as hypertension and diabetes). The proposed framework has been extensively tested in synthetic and real-world scenarios, and it outperforms the state-of-the-art stroke risk prediction models. It also shows the potential of real-world deployment among multiple hospitals aided with 5 G/B5G infrastructures.",health
10.1109/access.2022.3141913,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,decentralized federated learning for healthcare networks: a case study on tumor segmentation,https://ieeexplore.ieee.org/document/9676574/,"Smart healthcare relies on artificial intelligence (AI) functions for learning and analysis of patient data. Since large and diverse datasets for training of Machine Learning (ML) models can rarely be found in individual medical centers, classical centralized AI requires moving privacy-sensitive data from medical institutions to data centers that process the fused information. Training on data centers thus requires higher communication resource/energy demands while violating privacy. This is considered today as a significant bottleneck in pursuing scientific collaboration across trans-national clinical medical research centers. Recently, federated learning (FL) has emerged as a distributed AI approach that enables the cooperative training of ML models, without the need of sharing patient data. This paper dives into the analysis of different FL methods and proposes a real-time distributed networking framework based on the Message Queuing Telemetry Transport (MQTT) protocol. In particular, we design a number of solutions for ML over networks, based on FL tools relying on a parameter server (PS) and fully decentralized paradigms driven by consensus methods. The proposed approach is validated in the context of brain tumor segmentation, using a modified version of the popular U-NET model with representative clinical datasets obtained from the daily clinical workflow. The FL process is implemented on multiple physically separated machines located in different countries and communicating over the Internet. The real-time test-bed is used to obtain measurements of training accuracy vs. latency trade-offs, and to highlight key operational conditions that affect the performance in real deployments.",health
10.1109/tii.2021.3093905,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,"modeling, detecting, and mitigating threats against industrial healthcare systems: a combined software defined networking and reinforcement learning approach",https://ieeexplore.ieee.org/document/9470933/,"The rise of the Internet of Medical Things introduces the healthcare ecosystem in a new digital era with multiple benefits, such as remote medical assistance, real-time monitoring, and pervasive control. However, despite the valuable healthcare services, this progression raises significant cybersecurity and privacy concerns. In this article, we focus our attention on the IEC 60 870-5-104 protocol, which is widely adopted in industrial healthcare systems. First, we investigate and assess the severity of the IEC 60 870-5-104 cyberattacks by providing a quantitative threat model, which relies on Attack Defence Trees and Common Vulnerability Scoring System v3.1. Next, we introduce an intrusion detection and prevention system (IDPS), which is capable of discriminating and mitigating automatically the IEC 60 870-5-104 cyberattacks. The proposed IDPS takes full advantage of the machine learning (ML) and software defined networking (SDN) technologies. ML is used to detect the IEC 60 870-5-104 cyberattacks, utilizing 1) Transmission Control Protocol/Internet Protocol network flow statistics and 2) IEC 60 870-5-104 payload flow statistics. On the other side, the automated mitigation is transformed into a multiarmed bandit problem, which is solved through a reinforcement learning method called Thomson sampling and SDN. The evaluation analysis demonstrates the efficiency of the proposed IDPS in terms of intrusion detection accuracy and automated mitigation performance. The detection accuracy and the F1 score of the proposed IDPS reach 0.831 and 0.8258, respectively, while the mitigation accuracy is calculated at 0.923.",health
10.1007/978-3-030-64573-1_164,preprocessed,Artificial Intelligence in Medicine,Springer,2022-01-01 00:00:00,springer,aim in endoscopy procedures,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-64573-1_164,"Artificial intelligence (AI) is revolutionizing the way medicine is practiced. In this context, the application of AI algorithms in endoscopy is gaining increasing attention so that modern endoscopy is moving towards more and more assisted/automatic solutions. Several approaches have been carried out in order to improve accuracy in diagnosis and surgical procedures. In this chapter, a general overview of the main contributions in the field is surveyed. Four main categories of applications were identified, namely, (i) detection and diagnosis during endoscopic procedure, (ii) informative frame selection, (iii) mosaicking and surface reconstruction, (iv) augmented reality systems for intraoperative assistance and surgeon training. Discussions on future research directions and implementation in clinical practice are provided.",health
10.1007/978-3-030-80928-7_10,preprocessed,Machine Learning for Critical Internet of Medical Things,Springer,2022-01-01 00:00:00,springer,aiiomt: iomt-based system-enabled artificial intelligence for enhanced smart healthcare systems,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80928-7_10,"The healthcare system has been on the frontline in recent years, researchers have tried to find solutions to different illnesses and sickness by applying various modern methods. But the major difference among them is that in recent years, other powerful new tools have emerged, which could be used as an instrument in the healthcare system and keeping it within reasonable limits. One of those technological tools is the Internet of Medical Things (IoMT) and Artificial intelligence (AI). Recently, AI enabled with IoMT-based systems is causing a paradigm shift in the healthcare zone, and the applicability might yield profit especially in diagnosis, prediction, and treatment of different diseases outbreak. The application of AI enabled with IoT-based systems in the healthcare system can be expediting the diagnoses and monitoring of disease and minimizes the burden of medical processes. Therefore, this chapter reviews the applicability of AiIoMT-based system in healthcare systems and the research challenges in deployment of AiIoMT system. The chapter also proposed an AiIoMT-based framework for diagnosis and monitoring of patients in real time. The model was tested using cytology image dataset and evaluated based on accuracy, sensitivity, specificity, F-score, and precision. The findings show a greater diagnosis accuracy of 99.5%, which shows that the AI model is a promising algorithm for the diagnosis of diseases in an IoMT-based system. The diagnosis, prediction, treatment, screening, and medication in the healthcare system have significantly improved with the continuing expansion in the methods having seriously reduced human intervention in medical practice.",health
http://arxiv.org/abs/2202.10336v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,artificial intelligence for the metaverse: a survey,http://arxiv.org/abs/2202.10336v1,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.",health
http://arxiv.org/abs/2202.04361v2,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,"molecular-scale integration of multi-modal sensing and neuromorphic
  computing with organic electrochemical transistors",http://arxiv.org/abs/2202.04361v2,"Abstract: Bionic learning with fused sensing, memory and processing functions
outperforms artificial neural networks running on silicon chips in terms of
efficiency and footprint. However, digital hardware implementation of bionic
learning suffers from device heterogeneity in sensors and processing cores,
which incurs large hardware, energy and time overheads. Here, we present a
universal solution to simultaneously perform multi-modal sensing, memory and
processing using organic electrochemical transistors with designed architecture
and tailored channel morphology, selective ion injection into the
crystalline/amorphous regions. The resultant device work as either a volatile
receptor that shows multi-modal sensing, or a non-volatile synapse that
features record-high 10-bit analog states, low switching stochasticity and good
retention without the integration of any extra devices. Homogeneous integration
of such devices enables bionic learning functions such as conditioned reflex
and real-time cardiac disease diagnose via reservoir computing, illustrating
the promise for future smart edge health informatics.",health
http://arxiv.org/abs/2202.02559v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,"digital twin of wireless systems: overview, taxonomy, challenges, and
  opportunities",http://arxiv.org/abs/2202.02559v1,"Future wireless services must be focused on improving the quality of life by
enabling various applications, such as extended reality, brain-computer
interaction, and healthcare. These applications have diverse performance
requirements (e.g., user-defined quality of experience metrics, latency, and
reliability) that are challenging to be fulfilled by existing wireless systems.
To meet the diverse requirements of the emerging applications, the concept of a
digital twin has been recently proposed. A digital twin uses a virtual
representation along with security-related technologies (e.g., blockchain),
communication technologies (e.g., 6G), computing technologies (e.g., edge
computing), and machine learning, so as to enable the smart applications. In
this tutorial, we present a comprehensive overview on digital twins for
wireless systems. First, we present an overview of fundamental concepts (i.e.,
design aspects, high-level architecture, and frameworks) of digital twin of
wireless systems. Second, a comprehensive taxonomy is devised for both
different aspects. These aspects are twins for wireless and wireless for twins.
For the twins for wireless aspect, we consider parameters, such as twin objects
design, prototyping, deployment trends, physical devices design, interface
design, incentive mechanism, twins isolation, and decoupling. On the other
hand, for wireless for twins, parameters such as, twin objects access aspects,
security and privacy, and air interface design are considered. Finally, open
research challenges and opportunities are presented along with causes and
possible solutions.",health
http://arxiv.org/abs/2202.01176v1,preprocessed,arxiv,arxiv,2022-02-02 00:00:00,arxiv,epidemic dreams: dreaming about health during the covid-19 pandemic,http://arxiv.org/abs/2202.01176v1,"The continuity hypothesis of dreams suggests that the content of dreams is
continuous with the dreamer's waking experiences. Given the unprecedented
nature of the experiences during COVID-19, we studied the continuity hypothesis
in the context of the pandemic. We implemented a deep-learning algorithm that
can extract mentions of medical conditions from text and applied it to two
datasets collected during the pandemic: 2,888 dream reports (dreaming life
experiences), and 57M tweets mentioning the pandemic (waking life experiences).
The health expressions common to both sets were typical COVID-19 symptoms
(e.g., cough, fever, and anxiety), suggesting that dreams reflected people's
real-world experiences. The health expressions that distinguished the two sets
reflected differences in thought processes: expressions in waking life
reflected a linear and logical thought process and, as such, described
realistic symptoms or related disorders (e.g., nasal pain, SARS, H1N1); those
in dreaming life reflected a thought process closer to the visual and emotional
spheres and, as such, described either conditions unrelated to the virus (e.g.,
maggots, deformities, snakebites), or conditions of surreal nature (e.g., teeth
falling out, body crumbling into sand). Our results confirm that dream reports
represent an understudied yet valuable source of people's health experiences in
the real world.",health
http://arxiv.org/abs/2202.01034v1,preprocessed,arxiv,arxiv,2022-02-02 00:00:00,arxiv,"maintaining fairness across distribution shift: do we have viable
  solutions for real-world applications?",http://arxiv.org/abs/2202.01034v1,"Fairness and robustness are often considered as orthogonal dimensions when
evaluating machine learning models. However, recent work has revealed
interactions between fairness and robustness, showing that fairness properties
are not necessarily maintained under distribution shift. In healthcare
settings, this can result in e.g. a model that performs fairly according to a
selected metric in ""hospital A"" showing unfairness when deployed in ""hospital
B"". While a nascent field has emerged to develop provable fair and robust
models, it typically relies on strong assumptions about the shift, limiting its
impact for real-world applications. In this work, we explore the settings in
which recently proposed mitigation strategies are applicable by referring to a
causal framing. Using examples of predictive models in dermatology and
electronic health records, we show that real-world applications are complex and
often invalidate the assumptions of such methods. Our work hence highlights
technical, practical, and engineering gaps that prevent the development of
robustly fair machine learning models for real-world applications. Finally, we
discuss potential remedies at each step of the machine learning pipeline.",health
http://arxiv.org/abs/2201.07711v1,preprocessed,arxiv,arxiv,2022-01-19 00:00:00,arxiv,enhancing the security & privacy of wearable brain-computer interfaces,http://arxiv.org/abs/2201.07711v1,"Brain computing interfaces (BCI) are used in a plethora of
safety/privacy-critical applications, ranging from healthcare to smart
communication and control. Wearable BCI setups typically involve a head-mounted
sensor connected to a mobile device, combined with ML-based data processing.
Consequently, they are susceptible to a multiplicity of attacks across the
hardware, software, and networking stacks used that can leak users' brainwave
data or at worst relinquish control of BCI-assisted devices to remote
attackers. In this paper, we: (i) analyse the whole-system security and privacy
threats to existing wearable BCI products from an operating system and
adversarial machine learning perspective; and (ii) introduce Argus, the first
information flow control system for wearable BCI applications that mitigates
these attacks. Argus' domain-specific design leads to a lightweight
implementation on Linux ARM platforms suitable for existing BCI use-cases. Our
proof of concept attacks on real-world BCI devices (Muse, NeuroSky, and
OpenBCI) led us to discover more than 300 vulnerabilities across the stacks of
six major attack vectors. Our evaluation shows Argus is highly effective in
tracking sensitive dataflows and restricting these attacks with an acceptable
memory and performance overhead (<15%).",health
http://arxiv.org/abs/2201.07888v1,preprocessed,arxiv,arxiv,2022-01-16 00:00:00,arxiv,"adaptive energy management for self-sustainable wearables in mobile
  health",http://arxiv.org/abs/2201.07888v1,"Wearable devices that integrate multiple sensors, processors, and
communication technologies have the potential to transform mobile health for
remote monitoring of health parameters. However, the small form factor of the
wearable devices limits the battery size and operating lifetime. As a result,
the devices require frequent recharging, which has limited their widespread
adoption. Energy harvesting has emerged as an effective method towards
sustainable operation of wearable devices. Unfortunately, energy harvesting
alone is not sufficient to fulfill the energy requirements of wearable devices.
This paper studies the novel problem of adaptive energy management towards the
goal of self-sustainable wearables by using harvested energy to supplement the
battery energy and to reduce manual recharging by users. To solve this problem,
we propose a principled algorithm referred as AdaEM. There are two key ideas
behind AdaEM. First, it uses machine learning (ML) methods to learn predictive
models of user activity and energy usage patterns. These models allow us to
estimate the potential of energy harvesting in a day as a function of the user
activities. Second, it reasons about the uncertainty in predictions and
estimations from the ML models to optimize the energy management decisions
using a dynamic robust optimization (DyRO) formulation. We propose a
light-weight solution for DyRO to meet the practical needs of deployment. We
validate the AdaEM approach on a wearable device prototype consisting of solar
and motion energy harvesting using real-world data of user activities.
Experiments show that AdaEM achieves solutions that are within 5% of the
optimal with less than 0.005% execution time and energy overhead.",health
http://arxiv.org/abs/2201.05115v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,functional anomaly detection: a benchmark study,http://arxiv.org/abs/2201.05115v1,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",health
http://arxiv.org/abs/2202.00478v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,"neurahealthnlp: an automated screening pipeline to detect undiagnosed
  cognitive impairment in electronic health records with deep learning and
  natural language processing",http://arxiv.org/abs/2202.00478v1,"Dementia related cognitive impairment (CI) affects over 55 million people
worldwide and is growing rapidly at the rate of one new case every 3 seconds.
With a recurring failure of clinical trials, early diagnosis is crucial, but
75% of dementia cases go undiagnosed globally with up to 90% in
low-and-middle-income countries. Current diagnostic methods are notoriously
complex, involving manual review of medical notes, numerous cognitive tests,
expensive brain scans or spinal fluid tests. Information relevant to CI is
often found in the electronic health records (EHRs) and can provide vital clues
for early diagnosis, but a manual review by experts is tedious and error prone.
This project develops a novel state-of-the-art automated screening pipeline for
scalable and high-speed discovery of undetected CI in EHRs. To understand the
linguistic context from complex language structures in EHR, a database of 8,656
sequences was constructed to train attention-based deep learning natural
language processing model to classify sequences. A patient level prediction
model based on logistic regression was developed using the sequence level
classifier. The deep learning system achieved 93% accuracy and AUC = 0.98 to
identify patients who had no earlier diagnosis, dementia-related diagnosis
code, or dementia-related medications in their EHR. These patients would have
otherwise gone undetected or detected too late. The EHR screening pipeline was
deployed in NeuraHealthNLP, a web application for automated and real-time CI
screening by simply uploading EHRs in a browser. NeuraHealthNLP is cheaper,
faster, more accessible, and outperforms current clinical methods including
text-based analytics and machine learning approaches. It makes early diagnosis
viable in regions with scarce health care services but accessible internet or
cellular services.",health
http://arxiv.org/abs/2201.04967v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,"adherence forecasting for guided internet-delivered cognitive behavioral
  therapy: a minimally data-sensitive approach",http://arxiv.org/abs/2201.04967v1,"Internet-delivered psychological treatments (IDPT) are seen as an effective
and scalable pathway to improving the accessibility of mental healthcare.
Within this context, treatment adherence is an especially relevant challenge to
address due to the reduced interaction between healthcare professionals and
patients, compared to more traditional interventions. In parallel, there are
increasing regulations when using peoples' personal data, especially in the
digital sphere. In such regulations, data minimization is often a core tenant
such as within the General Data Protection Regulation (GDPR). Consequently,
this work proposes a deep-learning approach to perform automatic adherence
forecasting, while only relying on minimally sensitive login/logout data. This
approach was tested on a dataset containing 342 patients undergoing guided
internet-delivered cognitive behavioral therapy (G-ICBT) treatment. The
proposed Self-Attention Network achieved over 70% average balanced accuracy,
when only 1/3 of the treatment duration had elapsed. As such, this study
demonstrates that automatic adherence forecasting for G-ICBT, is achievable
using only minimally sensitive data, thus facilitating the implementation of
such tools within real-world IDPT platforms.",health
http://arxiv.org/abs/2201.01943v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,"machine learning: algorithms, models, and applications",http://arxiv.org/abs/2201.01943v1,"Recent times are witnessing rapid development in machine learning algorithm
systems, especially in reinforcement learning, natural language processing,
computer and robot vision, image processing, speech, and emotional processing
and understanding. In tune with the increasing importance and relevance of
machine learning models, algorithms, and their applications, and with the
emergence of more innovative uses cases of deep learning and artificial
intelligence, the current volume presents a few innovative research works and
their applications in real world, such as stock trading, medical and healthcare
systems, and software automation. The chapters in the book illustrate how
machine learning and deep learning algorithms and models are designed,
optimized, and deployed. The volume will be useful for advanced graduate and
doctoral students, researchers, faculty members of universities, practicing
data scientists and data engineers, professionals, and consultants working on
the broad areas of machine learning, deep learning, and artificial
intelligence.",health
10.1016/j.physc.2021.1354007,preprocessed,Physica C: Superconductivity and its Applications,scopus,2022-02-15,sciencedirect,optical fibre based quench detection in hts applications using machine learning classifiers,https://api.elsevier.com/content/abstract/scopus_id/85122309535,"A Mach-Zehnder Interferometer (MZI) based optical fibre sensing technique, developed and patented by EPFL, is an efficient and economical way to detect hotspots in High Temperature Superconductor (HTS) applications. Due to the MZI sensitivity being a composite of strain sensitive and temperature sensitive contributions, the MZI gives an instantaneous response to a quench (within 10 ms), because of the quick strain transfer to the optical fibre. However, the MZI output signal can also manifest the environmental noise caused by mechanical vibrations, bubbling in the cryostat and temperature variations, along with the response to the quench. This presents the problems of false alarms and indiscernible response to a quench. Discrete wavelet transform (DWT) has been proven to be a useful tool for feature extraction in different fields requiring signal categorization and hence holds the potential to enable quench recognition in the MZI output. This paper proposes an effective approach of performing DWT based feature extraction on experimental data and subsequently using the extracted features for the MZI response classification using two machine learning based classification techniques: k-nearest neighbours (KNN) and Artificial Neural Network (ANN). For this manuscript, experiments were performed using MZI for quench detection in an HTS tape. Feature extraction was then implemented on these experimental measurements using discrete wavelet coefficients extracted at different decomposition levels from the MZI output; these features were then used to train the KNN and ANN models for identifying quench in the MZI signal. This method could be a valuable supplement to the MZI technique by enabling the development of a real time application that can process the MZI output data as well as eliminate the occurrences of false alarms; thereby facilitating reliable quench detection. With this development, the MZI technique would become an even more attractive solution for the health monitoring of HTS applications.",health
10.1016/j.comnet.2021.108661,preprocessed,Computer Networks,scopus,2022-02-11,sciencedirect,evaluating federated learning for intrusion detection in internet of things: review and challenges,https://api.elsevier.com/content/abstract/scopus_id/85121903251,"The application of Machine Learning (ML) techniques to the well-known intrusion detection systems (IDS) is key to cope with increasingly sophisticated cybersecurity attacks through an effective and efficient detection process. In the context of the Internet of Things (IoT), most ML-enabled IDS approaches use centralized approaches where IoT devices share their data with data centers for further analysis. To mitigate privacy concerns associated with centralized approaches, in recent years the use of Federated Learning (FL) has attracted a significant interest in different sectors, including healthcare and transport systems. However, the development of FL-enabled IDS for IoT is in its infancy, and still requires research efforts from various areas, in order to identify the main challenges for the deployment in real-world scenarios. In this direction, our work evaluates a FL-enabled IDS approach based on a multiclass classifier considering different data distributions for the detection of different attacks in an IoT scenario. In particular, we use three different settings that are obtained by partitioning the recent ToN_IoT dataset according to IoT devices’ IP address and types of attack. Furthermore, we evaluate the impact of different aggregation functions according to such setting by using the recent IBMFL framework as FL implementation. Additionally, we identify a set of challenges and future directions based on the existing literature and the analysis of our evaluation results.",health
10.1016/j.ces.2021.117205,preprocessed,Chemical Engineering Science,scopus,2022-02-02,sciencedirect,"developments of leak detection, diagnostics, and prediction algorithms in multiphase flows",https://api.elsevier.com/content/abstract/scopus_id/85118896502,"Leak detection, diagnostics, and prediction constitute a crucial phase of the flow assurance risk management process for onshore and offshore pipelines. There are a variety of techniques and algorithms that can be deployed to address each aspect. To date, most review papers have concentrated on steady-state and single-phase flow conditions. The goal of the current review is therefore to carry out a thorough analysis of the available leak detection and diagnosis methods by focusing on (i) multiphase flow and transient flow conditions, (ii) model-based and data-driven techniques, (iii) prediction tools, and (iv) performance measures. Detailed assessment of leak detection methods based on accuracy, complexity, data requirement, and cost of installation are discussed. Data-driven techniques are utterly dependent on qualitative and quantitative data available from pipeline systems. Contrastingly data-driven techniques, model-based techniques require less data to achieve leak detection, provided that a nearly accurate base model is available. Different methodologies and technologies can be combined in order to produce the best detection and diagnosis outputs. In many cases, statistical analysis was combined with the Real Time Transient Method (RTTM), which helped to minimize false alarms. The material in this review can be used as a robust guide for the design of diagnostic systems and further research.",health
10.1016/j.cjca.2021.11.009,preprocessed,Canadian Journal of Cardiology,scopus,2022-02-01,sciencedirect,a primer on the present state and future prospects for machine learning and artificial intelligence applications in cardiology,https://api.elsevier.com/content/abstract/scopus_id/85122266625,"The artificial intelligence (AI) revolution is well underway, including in the medical field, and has dramatically transformed our lives. An understanding of the basics of AI applications, their development, and challenges to their clinical implementation is important for clinicians to fully appreciate the possibilities of AI. Such a foundation would ensure that clinicians have a good grasp and realistic expectations for AI in medicine and prevent discrepancies between the promised and real-world impact. When quantifying the track record for AI applications in cardiology, we found that a substantial number of AI systems are never deployed in clinical practice, although there certainly are many success stories. Successful implementations shared the following: they came from clinical areas where large amount of training data was available; were deployable into a single diagnostic modality; prediction models generally had high performance in external validation; and most were developed as part of collaborations with medical device manufacturers who had substantial experience with implementation of new clinical technology. When looking into the current processes used for developing AI-based systems, we suggest that expanding the analytic framework to address potential deployment and implementation issues at project outset will improve the rate of successful implementation, and will be a necessary next step for AI to achieve its full potential in cardiovascular medicine.",health
10.1016/j.compbiomed.2021.105144,preprocessed,Computers in Biology and Medicine,scopus,2022-02-01,sciencedirect,domain generalization on medical imaging classification using episodic training with task augmentation,https://api.elsevier.com/content/abstract/scopus_id/85121969937,"Medical imaging datasets usually exhibit domain shift due to the variations of scanner vendors, imaging protocols, etc. This raises the concern about the generalization capacity of machine learning models. Domain generalization (DG), which aims to learn a model from multiple source domains such that it can be directly generalized to unseen test domains, seems particularly promising to medical imaging community. To address DG, recent model-agnostic meta-learning (MAML) has been introduced, which transfers the knowledge from previous training tasks to facilitate the learning of novel testing tasks. However, in clinical practice, there are usually only a few annotated source domains available, which decreases the capacity of training task generation and thus increases the risk of overfitting to training tasks in the paradigm. In this paper, we propose a novel DG scheme of episodic training with task augmentation on medical imaging classification. Based on meta-learning, we develop the paradigm of episodic training to construct the knowledge transfer from episodic training-task simulation to the real testing task of DG. Motivated by the limited number of source domains in real-world medical deployment, we consider the unique task-level overfitting and we propose task augmentation to enhance the variety during training task generation to alleviate it. With the established learning framework, we further exploit a novel meta-objective to regularize the deep embedding of training domains. To validate the effectiveness of the proposed method, we perform experiments on histopathological images and abdominal CT images.",health
10.1016/j.scs.2021.103559,preprocessed,Sustainable Cities and Society,scopus,2022-02-01,sciencedirect,assessment of sustainable development objectives in smart labs: technology and sustainability at the service of society,https://api.elsevier.com/content/abstract/scopus_id/85120052266,"Sustainable development is the working basis of engineering research and cities are becoming increasingly flexible, inclusive and intelligent. In this context, there is a need for environments that emulate real-life spaces in which cutting-edge technologies can be implemented for subsequent deployment in society. Smart Labs or Living Labs are spaces for innovation, research and experimentation that integrate systems, devices and methodologies focused on people and their environments. The technologies studied and developed in such labs can then be deployed in human spaces to provide intelligence, comfort, health and sustainability. Health and wellness, energy and environment, artificial intelligence, big data and digital rights are some of the disciplines being studied. At the same time, the UN 2030 Agenda provides a comprehensive framework to promote human well-being through the Sustainable Development Goals. In this work, an evaluation model of its indicators in smart environments is performed through a mixed review methodology. The objective of this work is the analysis and implementation of the SDGs in Smart Labs through a literature review and a case study of UJAmI, the smart laboratory of the University of Jaén. The results provide quantitative and qualitative data on the present and future of the smart devices implemented in the UJAmI lab, providing a roadmap for future developments.",health
10.1016/j.jiac.2021.10.027,preprocessed,Journal of Infection and Chemotherapy,scopus,2022-02-01,sciencedirect,a study of quality assessment in sars-cov-2 pathogen nucleic acid amplification tests performance; from the results of external quality assessment survey of clinical laboratories in the tokyo metropolitan government external quality assessment program in 2020,https://api.elsevier.com/content/abstract/scopus_id/85119258737,"Introduction
                  The Tokyo Metropolitan Government (TMG) conducted an external quality assessment (EQA) survey of pathogen nucleic acid amplification tests (NAATs) as a TMG EQA program for SARS-CoV-2 for clinical laboratories in Tokyo.
               
                  Methods
                  We diluted and prepared a standard product manufactured by Company A to about 2,500 copies/mL to make a positive control and distribute it with a negative control. The participants reported the use of the NAATs methods for SARS-CoV-2, the name of the real-time RT-PCR kit, the name of the detection device, the target gene(s), nucleic acid extraction kit, Threshold Cycle value in the case of RT-PCR and the Threshold time value and Differential calculation value in the case of Loop-Mediated Isothermal Amplification (LAMP) method.
               
                  Results
                  As a result, 17 laboratories using fully automated equipment and 34 laboratories using the RT-PCR method reported generally appropriate results in this EQA survey. On the other hand, among the laboratories that adopted the LAMP method, there were a plurality of laboratories that judged positive samples to be negative.
               
                  Conclusion
                  The false negative result is considered to be due to the fact that the amount of virus genome contained in the quality control reagent used this time was below the detection limit of the LAMP method combined with the rapid extraction reagent for influenza virus. On the other hand, false positive results are considered to be due to the non-specific reaction of the NAATs. The EQA program must be continued for the proper implementation of the pathogen NAATs.",health
10.1016/j.ress.2021.108119,preprocessed,Reliability Engineering and System Safety,scopus,2022-02-01,sciencedirect,prognostics and health management (phm): where are we and where do we (need to) go in theory and practice,https://api.elsevier.com/content/abstract/scopus_id/85117331443,"We are performing the digital transition of industry, living the 4th industrial revolution, building a new World in which the digital, physical and human dimensions are interrelated in complex socio-cyber-physical systems. For the sustainability of these transformations, knowledge, information and data must be integrated within model-based and data-driven approaches of Prognostics and Health Management (PHM) for the assessment and prediction of structures, systems and components (SSCs) evolutions and process behaviors, so as to allow anticipating failures and avoiding accidents, thus, aiming at improved safe and reliable design, operation and maintenance.
                  There is already a plethora of methods available for many potential applications and more are being developed: yet, there are still a number of critical problems which impede full deployment of PHM and its benefits in practice. In this respect, this paper does not aim at providing a survey of existing works for an introduction to PHM nor at providing new tools or methods for its further development; rather, it aims at pointing out main challenges and directions of advancements, for full deployment of condition-based and predictive maintenance in practice.",health
10.1016/j.future.2021.08.030,preprocessed,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,a wearable-based posture recognition system with ai-assisted approach for healthcare iot,https://api.elsevier.com/content/abstract/scopus_id/85115908462,"Human posture recognition is a challenging task in the medical healthcare industry, when pursuing intelligence, accuracy, security, privacy, and efficiency, etc. Currently, the main posture recognition methods are captured-behaviors-based visual image analysis and wearable devices-based signal analysis. However, these methods suffer from issues such as high misjudgment rate, high-cost and low-efficiency. To address these issues, we propose a collaborative AI-IoT-based solution (namely, WMHPR) that embeds with advanced AI-assisted approach. In WMHPR, we propose the multi-posture recognition (MPR), an offline algorithm is implemented on wearable hardware, to identify posture based on multi-dimensions data. Meanwhile, an AI-based algorithm running on the cloud server (online), named Cascade-AdaBoosting-CART (CACT), is proposed to further enhance the reliability and accuracy of MPR. We recruit 20 volunteers for real-life experiments to evaluate the effectiveness, and the results show our solution is significantly outstanding in terms of accuracy and reliability while comparing with other typical algorithms.",health
10.1016/j.future.2021.09.010,preprocessed,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,xsru-iomt: explainable simple recurrent units for threat detection in internet of medical things networks,https://api.elsevier.com/content/abstract/scopus_id/85115376405,"The Internet of Medical Things (IoMT) is increasingly replacing the traditional healthcare systems. However, less focus has been paid to their security against cyber-threats in the implementation of the IoMT and its networks. One of the key reasons can be the challenging task of optimizing typical security solutions to the IoMT networks. And despite the rising admiration of machine learning and deep learning methods in the cyber-security domain (e.g., a threat detection system), most of these methods are acknowledged as a black-box model. The explainable AI (XAI) has become progressively vital to understand the employed learning models to improve trust level and empower security experts to interpret the prediction decisions. The authors propose a highly efficient model named XSRU-IoMT, for effective and timely detection of sophisticated attack vectors in IoMT networks. The proposed model is developed using novel bidirectional simple recurrent units (SRU) using the phenomenon of skip connections to eradicate the vanishing gradient problem and achieve a fast training process in recurrent networks. We also explore the concepts of XAI to improve trust level by providing explanations of the predictive decisions and enabling humans and security experts to understand the causal reasoning and underlying data evidence. The evaluation results on the ToN_IoT dataset demonstrate the effectiveness and superiority of the proposed XSRU-IoMT model as compared to the state-of-the-art compelling detection models, suggesting its usefulness as a viable deployment model in real-IoMT networks.",health
10.1016/j.aej.2021.06.024,preprocessed,Alexandria Engineering Journal,scopus,2022-02-01,sciencedirect,"automatic diagnosis of covid-19 disease using deep convolutional neural network with multi-feature channel from respiratory sound data: cough, voice, and breath",https://api.elsevier.com/content/abstract/scopus_id/85109458695,"The problem of respiratory sound classification has received good attention from the clinical scientists and medical researcher’s community in the last year to the diagnosis of COVID-19 disease. The Artificial Intelligence (AI) based models deployed into the real-world to identify the COVID-19 disease from human-generated sounds such as voice/speech, dry cough, and breath. The CNN (Convolutional Neural Network) is used to solve many real-world problems with Artificial Intelligence (AI) based machines. We have proposed and implemented a multi-channeled Deep Convolutional Neural Network (DCNN) for automatic diagnosis of COVID-19 disease from human respiratory sounds like a voice, dry cough, and breath, and it will give better accuracy and performance than previous models. We have applied multi-feature channels such as the data De-noising Auto Encoder (DAE) technique, GFCC (Gamma-tone Frequency Cepstral Coefficients), and IMFCC (Improved Multi-frequency Cepstral Coefficients) methods on augmented data to extract the deep features for the input of the CNN. The proposed approach improves system performance to the diagnosis of COVID-19 disease and provides better results on the COVID-19 respiratory sound dataset.",health
10.1016/j.vaccine.2021.12.014,preprocessed,Vaccine,scopus,2022-01-28,sciencedirect,humoral response to the sars-cov-2 bnt162b2 mrna vaccine: real-world data from a large cohort of healthcare workers,https://api.elsevier.com/content/abstract/scopus_id/85121675841,"Background
                  The SARS-CoV-2 pandemic was responsible for the death of millions of people around the world, which accelerated the study of vaccines. The BNT162b2 mRNA COVID-19 is a messenger RNA vaccine that encodes the spike protein of the virus. However, the duration of the protection conferred by this vaccine and factors associated with immune responses require validation in large cohorts.
               
                  Methods
                  Here, we present data of humoral immune response to vaccination in4264 healthcare workers, tested before (T0) and 15 and 90 days (T1 and T2, respectively) following vaccination.Peripheral blood was collected for immunological analysis using the Quant SARS-CoV-2 IgG II Chemiluminescent Microparticle Immunoassay (CMIA) to determine anti-spike IgG, receptor binding domain (RBD), S1 subunit of SARS-CoV-2.
               
                  Findings
                  At T0, 96·8% (n = 4129) of participants had IgG antibodies non-reactive to anti-SARS-CoV-2. Fifteen days after completing the vaccination, the IgG overall median titer was significantly elevated (21·7x103
                     AU/mL). Both for uni- and multivariate logistic regression analyses women presented higher antibody levels than men, independent of age. Titers were significantly altered among age groups, decreasing by each increase in 10-year of age. At 3 months after completing the vaccination, anti-SARS-CoV-2 IgG titers were 6·3-fold diminished.
                  This real-world post-vaccination data confirmed production of a frequent and elevated anti-SARS-CoV-2 IgG titers, associated with high protection rates. Females and younger participants had higher titer 15 days after vaccination, and despite the significant reduction from 15-to-90 days, those with higher pre-vaccination titers maintained higher levels throughout the remaining timepoints.
               
                  Interpretation
                  These findings support the need to track humoral immunity kinetics to uncover viral susceptibility and eventually implement re-vaccination, particularly in groups prone to lower humoral immune response.
               
                  Funding
                  No external funding was received to conduct this study.",health
10.1016/j.ijhydene.2022.01.145,preprocessed,International Journal of Hydrogen Energy,scopus,2022-01-01,sciencedirect,real-time data-driven fault diagnosis of proton exchange membrane fuel cell system based on binary encoding convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85124312531,"The performance of proton exchange Membrane fuel cell (PEMFC) fault diagnosis system plays an important role in normal operation of PEMFC. Therefore, a new fault diagnosis algorithm based on binary matrix encoding neural network called BinE-CNN is proposed. In BinE-CNN, high-dimensional features are extracted through binary encoding, and the feature maps are transferred to a convolutional neural network (CNN) to realize seven-category fault classification. For development of BinE-CNN, a PEMFC model is modeled to generate simulative datasets. Simulative test precision and Frames per second (FPS) of BinE-CNN have reached respectively 0.973 and 999.8 (better than support vector machines (SVM), long short-term memory neural network (LSTM), etc.). In experimental verification section, fault datasets are collected during bench test. After that, BinE-CNN is deployed on vehicle control unit (VCU) to verify its engineering value (real-time and precision). The result meet both requirements, with time cost of 96.15 ms and precision of 0.931.",health
10.1016/j.medin.2021.12.005,preprocessed,Medicina Intensiva,scopus,2022-01-01,sciencedirect,impact of aspergillus spp. isolation in the first 24 hours of admission in critically ill patients with severe influenza virus pneumonia,https://api.elsevier.com/content/abstract/scopus_id/85124136387,"Objective
                  To determine the incidence and impact of Aspergillus spp. isolation (AI) on ICU mortality in critically ill patients with severe influenza pneumonia during the first 24h of admission.
               
                  Design
                  Secondary analysis of an observational and prospective cohort study.
               
                  Setting
                  ICUs voluntary participating in the Spanish severe Influenza pneumonia registry, between June 2009 and June 2019.
               
                  Patients
                  Consecutive patients admitted to the ICU with diagnosis of severe influenza pneumonia, confirmed by real-time polymerase chain reaction.
               
                  Interventions
                  None.
               
                  Main variables of interest
                  Incidence of AI in respiratory samples. Demographic variables, comorbidities, need for mechanical ventilation and the presence of shock according at admission. Acute Physiology and Chronic Health Evaluation II (APACHE II) scale calculated on ICU admission.
               
                  Results
                  3702 patients were analyzed in this study. AI incidence was 1.13% (n
                     =42). Hematological malignancies (OR 4.39, 95% CI 1.92–10.04); HIV (OR 3.83, 95% CI 1.08–13.63), and other immunosuppression situations (OR 4.87, 95% CI 1.99–11.87) were factors independently associated with the presence of Aspergillus spp. The automatic CHAID decision tree showed that hematologic disease with an incidence of 3.3% was the most closely AI related variable. Hematological disease (OR 2.62 95% CI 1.95–3.51), immunosuppression (OR 2.05 95% CI 1.46–2.88) and AI (OR 3.24, 95% CI 1.60–6.53) were variables independently associated with ICU mortality.
               
                  Conclusions
                  Empirical antifungal treatment in our population may only be justified in immunocompromised patients. In moderate-high risk cases, active search for Aspergillus spp. should be implemented.",health
10.1016/j.fmre.2021.12.005,preprocessed,Fundamental Research,scopus,2022-01-01,sciencedirect,ai-aided on-chip nucleic acid assay for smart diagnosis of infectious disease,https://api.elsevier.com/content/abstract/scopus_id/85122505591,"Global pandemics such as COVID-19 have resulted in significant global social and economic disruption. Although polymerase chain reaction (PCR) is recommended as the standard test for identifying the SARS-CoV-2, conventional assays are time-consuming. In parallel, although artificial intelligence (AI) has been employed to contain the disease, the implementation of AI in PCR analytics, which may enhance the cognition of diagnostics, is quite rare. The information that the amplification curve reveals can reflect the dynamics of reactions. Here, we present a novel AI-aided on-chip approach by integrating deep learning with microfluidic paper-based analytical devices (µPADs) to detect synthetic RNA templates of the SARS-CoV-2 ORF1ab gene. The µPADs feature a multilayer structure by which the devices are compatible with conventional PCR instruments. During analysis, real-time PCR data were synchronously fed to three unsupervised learning models with deep neural networks, including RNN, LSTM, and GRU. Of these, the GRU is found to be most effective and accurate. Based on the experimentally obtained datasets, qualitative forecasting can be made as early as 13 cycles, which significantly enhances the efficiency of the PCR tests by 67.5% (∼40 min). Also, an accurate prediction of the end-point value of PCR curves can be obtained by GRU around 20 cycles. To further improve PCR testing efficiency, we also propose AI-aided dynamic evaluation criteria for determining critical cycle numbers, which enables real-time quantitative analysis of PCR tests. The presented approach is the first to integrate AI for on-chip PCR data analysis. It is capable of forecasting the final output and the trend of qPCR in addition to the conventional end-point Cq calculation. It is also capable of fully exploring the dynamics and intrinsic features of each reaction. This work leverages methodologies from diverse disciplines to provide perspectives and insights beyond the scope of a single scientific field. It is universally applicable and can be extended to multiple areas of fundamental research.",health
10.1016/j.animal.2021.100432,preprocessed,Animal,scopus,2022-01-01,sciencedirect,a machine vision system to predict individual cow feed intake of different feeds in a cowshed,https://api.elsevier.com/content/abstract/scopus_id/85122307007,"Data on individual feed intake of dairy cows, an important variable for farm management, are currently unavailable in commercial dairies. A real-time machine vision system including models that are able to adapt to multiple types of feed was developed to predict individual feed intake of dairy cows. Using a Red-Green-Blue-Depth (RGBD) camera, images of feed piles of two different feed types (lactating cows' feed and heifers' feed) were acquired in a research dairy farm, for a range of feed weights under varied configurations and illuminations. Several models were developed to predict individual feed intake: two Transfer Learning (TL) models based on Convolutional Neural Networks (CNNs), one CNN model trained on both feed types, and one Multilayer Perceptron and Convolutional Neural Network model trained on both feed types, along with categorical data. We also implemented a statistical method to compare these four models using a Linear Mixed Model and a Generalised Linear Mixed Model, showing that all models are significantly different. The TL models performed best and were trained on both feeds with TL methods. These models achieved Mean Absolute Errors (MAEs) of 0.12 and 0.13 kg per meal with RMSE of 0.18 and 0.17 kg per meal for the two different feeds, when tested on varied data collected manually in a cowshed. Testing the model with actual cows’ meals data automatically collected by the system in the cowshed resulted in a MAE of 0.14 kg per meal and RMSE of 0.19 kg per meal. These results suggest the potential of measuring individual feed intake of dairy cows in a cowshed using RGBD cameras and Deep Learning models that can be applied and tuned to different types of feed.",health
10.1016/j.ebiom.2021.103774,preprocessed,eBioMedicine,scopus,2022-01-01,sciencedirect,accuracy and ease-of-use of seven point-of-care sars-cov-2 antigen-detecting tests: a multi-centre clinical evaluation,https://api.elsevier.com/content/abstract/scopus_id/85121647709,"Background
                  Antigen-detecting rapid diagnostic tests (Ag-RDTs) for SARS-CoV-2 are important diagnostic tools. We assessed clinical performance and ease-of-use of seven Ag-RDTs in a prospective, manufacturer-independent, multi-centre cross-sectional diagnostic accuracy study to inform global decision makers.
               
                  Methods
                  Unvaccinated participants suspected of a first SARS-CoV-2 infection were recruited at six sites (Germany, Brazil). Ag-RDTs were evaluated sequentially, with collection of paired swabs for routine reverse transcription polymerase chain reaction (RT-PCR) testing and Ag-RDT testing. Performance was compared to RT-PCR overall and in sub-group analyses (viral load, symptoms, symptoms duration). To understandusability a System Usability Scale (SUS) questionnaire and ease-of-use (EoU) assessment were performed.
               
                  Findings
                  7471 participants were included in the analysis. Sensitivities across Ag-RDTs ranged from 70·4%-90·1%, specificities were above 97·2% for all Ag-RDTs but one (93·1%).Ag-RDTs, Mologic, Bionote, Standard Q, showed diagnostic accuracy in line with WHO targets (> 80% sensitivity, > 97% specificity). All tests showed high sensitivity in the first three days after symptom onset (≥87·1%) and in individuals with viral loads≥ 6 log10SARS-CoV2 RNA copies/mL (≥ 88·7%). Usability varied, with Rapigen, Bionote and Standard Q reaching very good scores; 90, 88 and 84/100, respectively.
               
                  Interpretation
                  Variability in test performance is partially explained by variable viral loads in population evaluated over the course of the pandemic. All Ag-RDTs reach high sensitivity early in the disease and in individuals with high viral loads, supporting their role in identifying transmission relevant infections. For easy-to-use tests, performance shown will likely be maintained in routine implementation.
               
                  Funding
                  Ministry of Science, Research and Arts, State of Baden-Wuerttemberg, Germany, internal funds from Heidelberg University Hospital, University Hospital Charité − Universitätsmedizin Berlin, UK Department of International Development, WHO, Unitaid.",health
10.1016/j.measurement.2021.110491,preprocessed,Measurement: Journal of the International Measurement Confederation,scopus,2022-01-01,sciencedirect,chxcapsnet: deep capsule network with transfer learning for evaluating pneumonia in paediatric chest radiographs,https://api.elsevier.com/content/abstract/scopus_id/85121269671,"Pneumonia is the primary cause of death in children under the age of 5 years. Faster and more accurate laboratory testing aids in the prescription of appropriate treatment for children suspected of having pneumonia, lowering mortality. In this work, we implement a deep neural network model to efficiently evaluate pediatric pneumonia from chest radio graph images. Our network uses a combination of convolutional and capsule layers to capture abstract details as well as low level hidden features from the radio graphic images, allowing the model to generate more generic predictions. Furthermore, we employ transfer learning approach to extract spatial features from the raw input radio graph images, allowing the model to save resources while enhancing performance. The capsule layer weights of the network are updated using the dynamic routing algorithm. The proposed model is evaluated using benchmark pneumonia dataset Kermany et al. 2018, and the outcomes of our experimental studies indicate that the capsules employed in the network enhance the learning of disease level features that are essential in diagnosing pneumonia. According to our comparison studies, the proposed model with Convolution base from InceptionV3 attached with Capsule layers at the end surpasses several existing models by achieving an accuracy of 94.84%. The proposed model is superior in terms of various performance measures such as accuracy and recall, and is well suited to real-time pediatric pneumonia diagnosis, substituting manual chest radiography examination.",health
10.1016/s2589-7500(21)00211-9,preprocessed,The Lancet Digital Health,scopus,2022-01-01,sciencedirect,"deep learning-based classification of kidney transplant pathology: a retrospective, multicentre, proof-of-concept study",https://api.elsevier.com/content/abstract/scopus_id/85120858490,"Background
                  Histopathological assessment of transplant biopsies is currently the standard method to diagnose allograft rejection and can help guide patient management, but it is one of the most challenging areas of pathology, requiring considerable expertise, time, and effort. We aimed to analyse the utility of deep learning to preclassify histology of kidney allograft biopsies into three main broad categories (ie, normal, rejection, and other diseases) as a potential biopsy triage system focusing on transplant rejection.
               
                  Methods
                  We performed a retrospective, multicentre, proof-of-concept study using 5844 digital whole slide images of kidney allograft biopsies from 1948 patients. Kidney allograft biopsy samples were identified by a database search in the Departments of Pathology of the Amsterdam UMC, Amsterdam, Netherlands (1130 patients) and the University Medical Center Utrecht, Utrecht, Netherlands (717 patients). 101 consecutive kidney transplant biopsies were identified in the archive of the Institute of Pathology, RWTH Aachen University Hospital, Aachen, Germany. Convolutional neural networks (CNNs) were trained to classify allograft biopsies as normal, rejection, or other diseases. Three times cross-validation (1847 patients) and deployment on an external real-world cohort (101 patients) were used for validation. Area under the receiver operating characteristic curve (AUROC) was used as the main performance metric (the primary endpoint to assess CNN performance).
               
                  Findings
                  Serial CNNs, first classifying kidney allograft biopsies as normal (AUROC 0·87 [ten times bootstrapped CI 0·85–0·88]) and disease (0·87 [0·86–0·88]), followed by a second CNN classifying biopsies classified as disease into rejection (0·75 [0·73–0·76]) and other diseases (0·75 [0·72–0·77]), showed similar AUROC in cross-validation and deployment on independent real-world data (first CNN normal AUROC 0·83 [0·80–0·85], disease 0·83 [0·73–0·91]; second CNN rejection 0·61 [0·51–0·70], other diseases 0·61 [0·50–0·74]). A single CNN classifying biopsies as normal, rejection, or other diseases showed similar performance in cross-validation (normal AUROC 0·80 [0·73–0·84], rejection 0·76 [0·66–0·80], other diseases 0·50 [0·36–0·57]) and generalised well for normal and rejection classes in the real-world data. Visualisation techniques highlighted rejection-relevant areas of biopsies in the tubulointerstitium.
               
                  Interpretation
                  This study showed that deep learning-based classification of transplant biopsies could support pathological diagnostics of kidney allograft rejection.
               
                  Funding
                  European Research Council; German Research Foundation; German Federal Ministries of Education and Research, Health, and Economic Affairs and Energy; Dutch Kidney Foundation; Human(e) AI Research Priority Area of the University of Amsterdam; and Max-Eder Programme of German Cancer Aid.",health
10.1016/j.ergon.2021.103234,preprocessed,International Journal of Industrial Ergonomics,scopus,2022-01-01,sciencedirect,industrial intelligence in the care of workers’ mental health: a review of status and challenges,https://api.elsevier.com/content/abstract/scopus_id/85120173556,"Mental health is a current concern because people worldwide have been committed to disorders that impair lives as a whole, affecting emotional states, behaviors, and body responses. These disorders decrease worker's productivity, impact industries economically, and cause serious psycho-physical conditions. However, technological advances have leveraged the industry to a novel phase where digitalization and automation provide a new reality. Hence, this industrial transformation may contribute to assists human beings in the workplace with a focus on mental health. This article presents a systematic literature review to investigate studies regarding technologies employed in the care of worker's mental health and the industrial role in this scenario. Three general, three focused, and three descriptive questions highlight the academic progress of industrial concern on mental health, implemented systems and cases, and research challenges. As a result, the review discussed 31 studies, extracted from an initial corpus of 25269, ranging from January 2010 to November 2020. The studies approached stress as the most frequent mental issue in the industry and Support Vector Machine (SVM) as the most used machine learning algorithm, where biomarkers presented the primary data extractors to deal with this theme. Moreover, information fusion methods improved the accuracy of specific cases. However, a growing interest in mental health care has emerged only in recent years, and several challenges require efforts before applying systems in real industrial environments.",health
10.1016/j.suscom.2021.100622,preprocessed,Sustainable Computing: Informatics and Systems,scopus,2022-01-01,sciencedirect,internet of things for sustaining a smart and secure healthcare system,https://api.elsevier.com/content/abstract/scopus_id/85119703892,"The thyroid is a key endocrine gland in the human body that regulates several bodily processes, including protein synthesis, energy consumption, and the body’s reaction to other hormones. Segmentation and volume regeneration of the thyroid is particularly important for identifying thyroid-related diseases since the majority of these problems result in a change in the thyroid’s shape and scale over time. There is an urgent need for research on the disease’s origins and spread. The Internet of Things, cloud computing, and artificial intelligence all provide real-time processing for a variety of applications in the healthcare sector. In healthcare and biomedicine applications, machine learning algorithms are increasingly being utilized to make critical choices. Thyroid patients urgently need a robust and latency-sensitive Quality of Service framework. This paper aims to integrate fog computing and artificial intelligence with smart health to provide a dependable platform for thyroid infection early detection. To identify thyroid patients, a novel ensemble-based classifier is proposed. The thyroid dataset is obtained from the UCI library and the simulation is carried out utilizing Python programming. To increase the framework’s security, encryption and decryption methods are suggested. The suggested framework’s performance is assessed in terms of latency, network use, RAM utilization, and energy consumption. On the other side, the suggested classifier’s accuracy, precision, specificity, sensitivity and F1 score are all assessed. The result demonstrates that the suggested framework and classifier perform consistently better than conventional frameworks and classifiers.",health
10.1016/j.bspc.2021.103123,preprocessed,Biomedical Signal Processing and Control,scopus,2022-01-01,sciencedirect,real-time application based cnn architecture for automatic usct bone image segmentation,https://api.elsevier.com/content/abstract/scopus_id/85114454344,"Artificial Intelligence (AI) in medical image analysis has achieved excellent success in automatic diagnosis in the same way as clinician, especially in the ultrasound field. In this work, we develop a new segmentation application based on various Convolutional Neural Network (CNN) models for Ultrasonic Computed Tomographic (USCT) images. To evaluate the proposed segmentation system, we use different state-of-the-art models for better segmentation performances to train and test the suggested system. We ensure in this work a USCT data augmentation technique based on the Haar wavelet transform and the improved k-means algorithms. Thus, we offer a free dataset for USCT researchers. Moreover, the proposed CNN system is trained and tested using the networks of Adadelta and Adam optimizers. The whole system is implemented on a CPU and a GPU for complexity analysis. High segmentation accuracy has been achieved using the Adadelta optimizer, reaching 99.24%, 99.19%, 99.13% and 99.10% for VGG-Segnet, VGG-Unet, Fully CNN (FCN)-8 and FCN-32 models, respectively. To obtain better results, we use the Adam optimizer to train and test different architectures, and we obtain more competitive results attaining 99.55%, 99.31%, 99.35% and 99.45% for VGG-Segnet, VGG-Unet, FCN-8 and FCN-32, respectively. The achieved results outperform the state of the art in terms of accuracy and time speed up. Moreover, our proposed CNN segmentation confirms the low computational complexity of the system. In addition, our system proves to be a good candidate for medical real-time applications thanks to its implementation on the GPU.",health
10.1109/sii52469.2022.9708896,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,integration of a reconfigurable robotic workcell for assembly operations in automotive industry,https://ieeexplore.ieee.org/document/9708896/,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",industry
10.1109/ccnc49033.2022.9700522,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,a deep reinforcement learning-based resource management scheme for sdn-mec-supported xr applications,https://ieeexplore.ieee.org/document/9700522/,"The Multi-Access Edge Computing (MEC) paradigm provides a promising solution for efficient computing services at edge nodes, such as base stations (BS), access points (AP), etc. By offloading highly intensive computational tasks to MEC servers, critical benefits in terms of reducing energy consumption at mobile devices and lowering processing latency can be achieved to support high Quality of Service (QoS) to many applications. Among the services which would benefit from MEC deployments are eXtended Reality (XR) applications which are receiving increasing attention from both academia and industry. XR applications have high resource requirements, mostly in terms of network bandwidth, computation and storage. Often these resources are not available in classic network architectures and especially not when XR applications are run by mobile devices. This paper leverages the concepts of Software Defined Networking (SDN) and Network Function Virtualization (NFV) to propose an innovative resource management scheme considering heterogeneous QoS requirements at the MEC server level. The resource assignment is formulated by employing a Deep Reinforcement Learning (DRL) technique to support high quality of XR services. The simulation results show how our proposed solution outperforms other state-of-the-art resource management-based schemes.",industry
10.1109/jiot.2021.3079440,preprocessed,IEEE Internet of Things Journal,IEEE,2015-01-15 20:22:00,ieeexplore,deep-learning-enabled automatic optical inspection for module-level defects in lcd,https://ieeexplore.ieee.org/document/9429707/,"Liquid crystal display (LCD) defects detection on module level is increasingly important for flat-panel displays (FPD) industry to increase the production capacity via machine vision technology. However, it is an overwhelmingly challenging issue due to various difficulties. This article discloses a practical automatic optical inspection (AOI) system consisting of hardware structure and software algorithm to detect module-level defects. The AOI system is the core component to build a distributed integrated inspection system with the help of the Internet of Things (IoT). Starting from the analysis of the challenges encountered in module-level defects inspection, a delicate photograph scheme is proposed to reveal different kinds of defects. In order to robustly work on the module-level defects detection with complex situations, a novel framework based on YOLOV3 detection unit is proposed in this article, including the preprocessing module, detection module, defects definition module, and interferences elimination module. To the best of our knowledge, this is the first work that designs a practical AOI system for module-level defects detection. In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on the manufacturing lines. The evaluation of the detection performance of the AOI system in comparison with a manual scheme indicates that the proposed system is practical for module-level defects detection. Currently, the proposed system has been deployed in a real-world LCD manufacturing line from a major player in the world.",industry
10.1109/tcyb.2020.2964011,preprocessed,IEEE Transactions on Cybernetics,IEEE,2022-01-01 00:00:00,ieeexplore,hierarchical granular computing-based model and its reinforcement structural learning for construction of long-term prediction intervals,https://ieeexplore.ieee.org/document/8972350/,"As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.",industry
10.1109/access.2021.3138990,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"microgrid digital twins: concepts, applications, and future trends",https://ieeexplore.ieee.org/document/9663369/,"Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the <italic>digital twinning</italic> concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs’ operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed.",industry
10.1109/tii.2021.3086149,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,toward a web-based digital twin thermal power plant,https://ieeexplore.ieee.org/document/9446630/,"As a crucial part of cyber-physical systems, a digital twin can process data, visualize processes, and send commands to the control system, which can be used for the research on thermal power plants that are vital for providing energy for manufacturing and industry, and also daily consumptions. This article introduces the methodologies and techniques toward a web-based digital twin thermal power plant. To implement a web-based digital twin thermal power plant, the architecture, modeling, control algorithm, rule model, and physical-digital twin control are explored. The potential functionalities of the web-based digital twin including real-time monitoring, visualization and interactions, and provided services for physical thermal plants and universities are also presented. A case study has been provided to illustrate the web-based digital twin power plant. The research in this article can provide potential solutions for web-based digital twin research and education.",industry
10.1109/tii.2021.3093388,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-04-01 00:00:00,ieeexplore,an adaptive deep learning framework for fast recognition of integrated circuit markings,https://ieeexplore.ieee.org/document/9468418/,"Fast recognition of integrated circuit (IC) markings is an essential but challenging task in electronic device manufacturing lines. This article develops an adaptive deep learning framework to facilitate the fast marking recognition of IC chips. The proposed framework contains four deep learning components, namely, chip segmentation, orientation correction, character extraction, and character recognition. The four components utilize different convolutional neural network structures to guarantee excellent adaptivity to a wide range of IC types and mitigate the influence of the low-quality chip images. In particular, the character extraction model is comprised of two improved label generation strategies and a proposed border correction method, so as to accommodate tiny scale chips and compactly printed markings. Experiments from the chip image dataset of a real laptop manufacturing line reached a recognition Precision of 91.73% and the Recall of 92.93%. The results demonstrate the superiority of the proposed framework to the state-of-the-art models and the effectiveness of handling a great diversity of chips with different scales, shapes, text fonts, marking colors, and layouts.",industry
10.1109/tii.2021.3131355,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,dynamic network slicing orchestration for remote adaptation and configuration in industrial iot,https://ieeexplore.ieee.org/document/9629333/,"As an emerging and prospective paradigm, the industrial Internet of Things (IIoT) enable intelligent manufacturing through the interconnection and interaction of industrial production elements. The traditional approach that transmits data in a single physical network is undesirable because such a scheme cannot meet the network requirements of different industrial applications. To address this problem, in this article, we propose a network slicing orchestration system for remote adaptation and configuration in smart factories. We exploit software-defined networking and network functions virtualization to slice the physical network into multiple virtual networks. Different applications can use a dedicated network that meets its requirements with limited network resources with this scheme. To optimize network resource allocation and adapt to the dynamic network environments, we propose two heuristic algorithms with the assistance of artificial intelligence and the theoretical analysis of the network slicing system. We conduct numerical simulations to learn the performance of the proposed algorithms. Our experimental results show the effectiveness and efficiency of our proposed algorithms when multiple network services are concurrently running in the IIoT. Finally, we use a case study to verify the feasibility of the proposed network slicing orchestration system on a real smart manufacturing testbed.",industry
10.1109/tii.2021.3128972,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-05-01 00:00:00,ieeexplore,guest editorial: security and privacy of federated learning solutions for industrial iot applications,https://ieeexplore.ieee.org/document/9619939/,"The Industrial Internet of Things (IoT) typically consists of several thousands of heterogeneous devices, such as sensors, actuators, access points, machinery, end-users' handheld equipment, and supply chain. In such an industrial environment, a multitude of data is generated from massive IoT devices, e.g., sensors for monitoring the environment, reading temperature, and gauging pressure. Most of the data are from delay-sensitive and computation-intensive applications, such as real-time manufacturing and automated diagnostics, which require big data analytics with low latency. Machine learning (ML) has been witnessed as an efficient solution for big data analytics. The majority of such ML algorithms are centralized methods, meaning that they first gather data from different users for use as a training dataset, which is placed on the ML server, and then build a model to classify the new data samples by applying the ML algorithms to this training dataset. However, the access to these datasets in the centralized ML methods raises concerns about data privacy for users. Federated learning (FL) was designed to protect data privacy to address a part of these issues. In FL, each participant uses a global training model without uploading their private data to a third-party server. Compared with the conventional ML, FL can preserve data security, especially in terms of participant data during the learning process. In particular, FL can also help in updating server-side data for the global model, and the participant is not required to provide their data. However, in FL, individual computing units may show abnormal actions, such as faulty software, hardware invasions, unreliable communication channels, and malicious samples deliberately crafting the model. To mitigate these challenges, we require robust policies to control the learning phases in FL. Motivated by the abovementioned issues, this special section solicits original research and practical contributions that advance the security and privacy of the FL solutions for industrial IoT applications as follows.",industry
10.1109/access.2022.3140595,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,integrating artificial intelligence internet of things and 5g for next-generation smartgrid: a survey of trends challenges and prospect,https://ieeexplore.ieee.org/document/9672084/,"Smartgrid is a paradigm that was introduced into the conventional electricity network to enhance the way generation, transmission, and distribution networks interrelate. It involves the use of Information and Communication Technology (ICT) and other solution in fault and intrusion detection, mere monitoring of energy generation, transmission, and distribution. However, on one hand, the actual and earlier smartgrid, do not integrate more advanced features such as automatic decision making, security, scalability, self-healing and awareness, real-time monitoring, cross-layer compatibility, etc. On the other hand, the emergence of the digitalization of the communication infrastructure to support the economic sector which among them are energy generation and distribution grid with Artificial Intelligence (AI) and large-scale Machine to Machine (M2M) communication. With the future Massive Internet of Things (MIoT) as one of the pillars of 5G/6G network factory, it is the enabler to support the next generation smart grid by providing the needed platform that integrates, in addition to the communication infrastructure, the AI and IoT support, providing a multitenant system. This paper aim at presenting a comprehensive review of next smart grid research trends and technological background, discuss a futuristic next-generation smart grid driven by artificial intelligence (AI) and leverage by IoT and 5G. In addition, it discusses the challenges of next-generation smart-grids as it relate to the integration of AI, IoT and 5G for better smart grid architecture. Also, proffers possible solutions to some of the challenges and standards to support this novel trend. A corresponding future work will dwell on the implementation of the discussed integration of AI, IoT and 5G for next-generation smart grid, using Matlab, NS2/NS3, Open-daylight and Mininet as soft tools and compare with related literature.",industry
10.1109/tii.2021.3077865,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,a data stream cleaning system using edge intelligence for smart city industrial environments,https://ieeexplore.ieee.org/document/9424956/,"Cities are becoming smarter because of recent advances in artificial intelligence and the Internet of Things. However, heterogeneous data source in smart cities are continuously producing low-quality data, and ever-growing applications have greater real-time requirements. Therefore, this article proposes a data stream cleaning system (named DSCS) using edge intelligence to utilize the advantages of cloud servers and edge devices. The DSCS in edge nodes consists of a dynamic protocol interpreter, a structure parser, and a cleaning model activator. Meanwhile, a cloud server, which has pools of protocol and structured programs and cleaning models, supports the edge nodes to adapt massive heterogeneous data sources. To validate the proposed data cleaning system, we applied it to two scenarios: monitoring the injection molding machines, and base stations. The DSCS can have a stable processing time when the number of accessed edge devices is increased, as well as a good cleaning effect.",industry
10.1109/access.2022.3145236,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,colonization by algorithms in the fourth industrial revolution,https://ieeexplore.ieee.org/document/9690490/,"Data gathering and information processing have evolved to where it is almost unfathomable how much exists in digital form today. The generation thereof also no longer involves an explicit instruction from human to machine but can happen in real-time without human intervention. Artificial intelligence, machine learning, and cognitive computing are being utilized to mine data from a variety of sources. One such (profitable) source is human beings. Digital algorithms are designed to harness the power of technology to gather information. There has always been a sense of secrecy regarding some information (classified, top secret, confidential, etc.) but the Fourth Industrial Revolution has created the means to gather extremely large amounts of data, unknown to its sources. Anthropological value systems should become a fundamental foundation of digital algorithms. Such an approach could prevent software from exploiting its sources, especially minorities. Value systems together with ethics are guided by people’s culture. In ethically aligned algorithm design, value systems and digital technologies intersect and govern how algorithms are developed, the way data is engaged, and further the discipline of digital humanities.",industry
10.1109/tii.2021.3081417,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,early classification of industrial alarm floods based on semisupervised learning,https://ieeexplore.ieee.org/document/9435070/,"Early classification of ongoing alarm floods in industrial monitoring systems is crucial to provide a safe and efficient operation. It can provide online decision support for plant operators to take timely action, without waiting for the end of an alarm flood. In this article, a data-driven approach is proposed to address the early classification problem with unlabeled historical data. To prioritize earlier activated alarms and take advantage of the triggering time information of alarms, a vector representation called exponentially attenuated component (EAC) is used to represent alarm floods. This makes alarm sequences fit for different powerful machine learning algorithms, which can be easily implemented online with acceptable computational complexities. A method based on the time information of unlabeled historical alarm floods is formulated to determine the attenuation coefficient for EAC representation. With the Gaussian mixture model, an efficient semisupervised approach is proposed to provide an early classification of alarm floods using unlabeled historical data. It includes two phases: offline clustering and online classification, where the clustering step is automated in terms of choosing the optimal number of clusters by applying an efficient cluster validity index. The efficiency of the proposed method is validated by the Tennessee Eastman process benchmark and a real industrial dataset.",industry
10.1109/tii.2021.3130279,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,imitation learning based heavy-hitter scheduling scheme in software-defined industrial networks,https://ieeexplore.ieee.org/document/9626609/,"To realize flexible networking and on-demand topology reconstructing, software-defined industrial networks (SDINs) are increasingly embracing the flat structure. Similar to software defined networks (SDN), SDIN suffers from low traffic scheduling efficiency caused by large and imbalanced flows, known as the heavy hitters problem. Due to such heavy hitters, industrial networks may fail to satisfy application’s QoS requirements, which results in more severe damages. To improve flow scheduling efficiency under heavy hitters, this article introduces a novel imitation learning-based flow scheduling (ILFS) method. ILFS utilizes P4-based In-band Network Telemetry (INT) to collect fine-grained, real-time traffic data from SDIN’s data plane. In the control plane, it integrates the Generative Adversarial Imitation Learning (GAIL) model with a soft actor critic to preserve the experiences of flow, thereby better scheduling large flows. Our experiments thoroughly compare ILFS’s performance with several state-of-the-art traffic scheduling strategies. The results indicate that ILFS successfully controls the link bandwidth the utilization between 10<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and 80<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and significantly improves the average network throughput and link utilization rate.",industry
10.1109/tii.2021.3124848,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,qos and privacy-aware routing for 5g-enabled industrial internet of things: a federated reinforcement learning approach,https://ieeexplore.ieee.org/document/9601174/,"The development and maturity of the fifth-generation (5G) wireless communication technology provides the industrial Internet of Things (IIoT) with ultra-reliable and low-latency communications and massive machine-type communications, and forms a novel IIoT architecture, 5G-IIoT. However, massive data transfer between interconnecting industrial devices also brings new challenges for the 5G-IIoT routing process in terms of latency, load balancing, and data privacy, which affect the development of 5G-IIoT applications. Moreover, the existing research works on IIoT routing mostly focus on the latency and the reliability of the routing, disregarding the privacy security in the routing process. To solve these problems, in this article, we propose a quality of service (QoS) and data privacy-aware routing protocol, named QoSPR, for 5G-IIoT. Specifically, we improve the community detection algorithm info-map to divide the routing area into optimal subdomains, based on which the deep reinforcement learning algorithm is applied to build the gateway deployment model for latency reduction and load-balancing improvement. To eliminate areal differences, while considering the privacy preservation of the routing data, the federated reinforcement learning is applied to obtain the universal gateway deployment model. Then, based on the gateway deployment, the QoS and data privacy-aware routing is accomplished by establishing communications along the load-balancing routes of the minimum latencies. The validation experiment is conducted on real datasets. The experiment results show that as a data privacy-aware routing protocol, the QoSPR can significantly reduce both average latency and maximum latency, while maintaining excellent load balancing in 5G-IIoT.",industry
10.1109/tii.2021.3077005,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,verifiable data mining against malicious adversaries in industrial internet of things,https://ieeexplore.ieee.org/document/9422191/,"With the large-scaled data generated from various interconnected machines and networks, Industrial Internet of Things (IIoT) provides unprecedented opportunities for facilitating data mining for industrial applications. The current IIoT architecture tends to adopt cloud computing for further timely mining IIoT data, however, the openness of security-critical IIoT becomes challenging in terms of unbearable privacy issues. Most existing privacy-preserving data mining (PPDM) techniques are designed to resist honest-but-curious adversaries (i.e., cloud servers and data users). Due to the complexity and openness in IIoT, PPDM is significantly difficult with the presence of malicious adversaries in IIoT who may incur incorrect learned models and inference results. To solve the aforementioned issues, we propose a framework to extend existing PPDM to guard linear regression against malicious behaviors (hereafter referred to as GuardLR). To prevent dishonest computations of cloud servers and inconsistent inputs of data users, we first design a privacy-preserving verifiable learning scheme for linear regression, which guarantees the correctness of learning. In this article, to avoid malicious clouds from returning incorrect inference results, we design a privacy-preserving prediction scheme with lightweight verification. Our formal security analysis shows that GuardLR achieves privacy, completeness, and soundness. Empirical experiments using real-world datasets also demonstrate that GuardLR has high computational efficiency and accuracy.",industry
10.1109/tnse.2021.3075428,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,ai-assisted energy-efficient and intelligent routing for reconfigurable wireless networks,https://ieeexplore.ieee.org/document/9416866/,"Intelligent network management for reconfigurable wireless networks such as 5G and beyond applications is crucial for many industrial applications, and has been the subject of ongoing research. This paper proposes an Artificial Intelligence(AI)-Assisted energy-efficient and intelligent routing, based on both energy efficiency prioritization and AI theory, in order to meet the exacting demands particularly in a real-world scenario. Specifically, to achieve network intelligence and quality of service (QoS), we use the AI theory to enhance routing adaptivity for intelligent network management in reconfigurable wireless networks. The software-defined networking idea is used to achieve this goal from a network-level perspective. To facilitate self-awareness, self-study, self-decision making, and self-configuration, we construct a mathematical model to convert the energy-efficient and intelligent routing problem into a multi-constraint optimal problem. Then an AI-assisted intelligent routing algorithm is designed to dynamically and adaptively change link weighs, which allows us to achieve optimal energy efficiency. Findings from our simulation suggest the potential of our proposed approach.",industry
10.1109/tnse.2021.3055835,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,cloud versus edge deployment strategies of real-time face recognition inference,https://ieeexplore.ieee.org/document/9350171/,"Choosing the appropriate deployment strategy for any Deep Learning (DL) project in a production environment has always been the most challenging problem for industrial practitioners. There are several conflicting constraints and controversial approaches when it comes to deployment. Among these problems, the deployment on cloud versus the deployment on edge represents a common dilemma. In a nutshell, each approach provides benefits where the other would have limitations. This paper presents a real-world case study on deploying a face recognition application using MTCNN detector and FaceNet recognizer. We report the challenges faced to decide on the best deployment strategy. We propose three inference architectures for the deployment, including cloud-based, edge-based, and hybrid. Furthermore, we evaluate the performance of face recognition inference on different cloud-based and edge-based GPU platforms. We consider different models of Jetson boards for the edge (Nano, TX2, Xavier NX, Xavier AGX) and various GPUs for the cloud (GTX 1080, RTX 2080Ti, RTX 2070, and RTX 8000). We also investigate the effect of deep learning model optimization using TensorRT and TFLite compared to a standard Tensorflow GPU model, and the effect of input resolution. We provide a benchmarking study for all these devices in terms of frames per second, execution times, energy and memory usages. After conducting a total of 294 experiments, the results demonstrate that the TensorRT optimization provides the fastest execution on all cloud and edge devices, at the expense of significantly larger energy consumption (up to +40% and +35% for edge and cloud devices, respectively, compared to Tensorflow). Whereas TFLite is the most efficient framework in terms of memory and power consumption, while providing significantly less (-4% to -62%) processing acceleration than TensorRT. <italic>Practitioners Note:</italic> The study reported in this paper presents the real-challenges that we faced during our development and deployment of a face-recognition application both on the edge and on the cloud, and the solutions we have developed to solve these problems. The code, results, and interactive analytic dashboards of this paper will be put public upon publication.",industry
10.1109/tii.2021.3075464,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-04-01 00:00:00,ieeexplore,dnnoff: offloading dnn-based intelligent iot applications in mobile edge computing,https://ieeexplore.ieee.org/document/9416166/,"A deep neural network (DNN) has become increasingly popular in industrial Internet of Things scenarios. Due to high demands on computational capability, it is hard for DNN-based applications to directly run on intelligent end devices with limited resources. Computation offloading technology offers a feasible solution by offloading some computation-intensive tasks to the cloud or edges. Supporting such capability is not easy due to two aspects: <italic>Adaptability:</italic> offloading should dynamically occur among computation nodes. <italic>Effectiveness:</italic> it needs to be determined which parts are worth offloading. This article proposes a novel approach, called DNNOff. For a given DNN-based application, DNNOff first rewrites the source code to implement a special program structure supporting on-demand offloading and, at runtime, automatically determines the offloading scheme. We evaluated DNNOff on a real-world intelligent application, with three DNN models. Our results show that, compared with other approaches, DNNOff saves response time by 12.4–66.6% on average.",industry
10.1109/access.2022.3149050,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,design and implementation of traffic generation model and spectrum requirement calculator for private 5g network,https://ieeexplore.ieee.org/document/9703352/,"This paper proposes a neural 5G traffic generation model and a methodology for calculating the spectrum requirements of private 5G networks to provide various industrial communication services. To accurately calculate the spectral requirements, it is necessary to analyze the actual data volume and traffic type of industrial cases. However, because there is currently no suitable traffic model to test loads in private 5G networks, we have developed a generative adversarial network (GAN)-based traffic generator that can generate realistic traffic by learning actual traffic traces collected by mobile network operators. In addition, in the case of industrial applications, probability-based traffic models were used in parallel as there were not enough real data to be learned. The proposed 5G traffic generation model is combined with the proposed 5G spectrum calculation methodology, enabling more accurate spectrum requirements calculation through traffic simulation similar to a real-life environment. In this paper, the spectrum requirements are calculated differently according to two types of duplexing, namely frequency division duplexing (FDD) and time division duplexing (TDD). As a guide for companies aiming to provide advanced wireless connectivity for a wide variety of vertical industries using 5G networks, eight use cases defined in the 5G Alliance for Connected Industries and Automation (ACIA) white paper were simulated. The spectrum requirements were calculated under various simulation conditions considering varying traffic loads, deployment scenarios, and duplexing types. Various simulation results confirmed that a bandwidth of at least 22.0 MHz to a maximum of 397.8 MHz is required depending on the deployment scenario.",industry
10.1109/tmech.2021.3065522,preprocessed,IEEE/ASME Transactions on Mechatronics,IEEE,2022-02-01 00:00:00,ieeexplore,federated transfer learning for intelligent fault diagnostics using deep adversarial networks with data privacy,https://ieeexplore.ieee.org/document/9376674/,"Intelligent data-driven machinery fault diagnosis methods have been popularly developed in the past years. While fairly high diagnosis accuracies have been obtained, large amounts of labeled training data are mostly required, which are difficult to collect in practice. The promising collaborative model training solution with multiple users poses high demands on data privacy due to conflict of interests. Furthermore, in the real industries, the data from different users can be usually collected from different machine operating conditions. The domain shift phenomenon and data privacy concern make the joint model training scheme quite challenging. To address this issue, a federated transfer learning method for fault diagnosis is proposed in this article. Different models can be used by different users to enhance data privacy. A federal initialization stage is introduced to keep similar data structures in distributed feature extractions, and a federated communication stage is further implemented using deep adversarial learning. A prediction consistency scheme is also adopted to increase model robustness. Experiments on two real-world datasets suggest the proposed federated transfer learning method is promising for real industrial applications.",industry
10.1109/tie.2021.3057030,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-02-01 00:00:00,ieeexplore,kdnet-rul: a knowledge distillation framework to compress deep neural networks for machine remaining useful life prediction,https://ieeexplore.ieee.org/document/9351733/,"Machine remaining useful life (RUL) prediction is vital in improving the reliability of industrial systems and reducing maintenance cost. Recently, long short-term memory (LSTM) based algorithms have achieved state-of-the-art performance for RUL prediction due to their strong capability of modeling sequential sensory data. In many cases, the RUL prediction algorithms are required to be deployed on edge devices to support real-time decision making, reduce the data communication cost, and preserve the data privacy. However, the powerful LSTM-based methods which have high complexity cannot be deployed to edge devices with limited computational power and memory. To solve this problem, we propose a knowledge distillation framework, entitled KDnet-RUL, to compress a complex LSTM-based method for RUL prediction. Specifically, it includes a generative adversarial network based knowledge distillation (GAN-KD) for disparate architecture knowledge transfer, a learning-during-teaching based knowledge distillation (LDT-KD) for identical architecture knowledge transfer, and a sequential distillation upon LDT-KD for complicated datasets. We leverage simple and complicated datasets to verify the effectiveness of the proposed KDnet-RUL. The results demonstrate that the proposed method significantly outperforms state-of-the-art KD methods. The compressed model with 12.8 times less weights and 46.2 times less total float point operations even achieves a comparable performance with the complex LSTM model for RUL prediction.",industry
10.1109/tpds.2021.3104255,preprocessed,IEEE Transactions on Parallel and Distributed Systems,IEEE,2022-06-01 00:00:00,ieeexplore,taskflow: a lightweight parallel and heterogeneous task graph computing system,https://ieeexplore.ieee.org/document/9511796/,"Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.",industry
10.26599/tst.2020.9010055,preprocessed,Tsinghua Science and Technology,TUP,2022-04-01 00:00:00,ieeexplore,underground pipeline surveillance with an algorithm based on statistical time-frequency acoustic features,https://ieeexplore.ieee.org/document/9552663/,"Underground pipeline networks suffer from severe damage by earth-moving devices due to rapid urbanization. Thus, designing a round-the-clock intelligent surveillance system has become crucial and urgent. In this study, we develop an acoustic signal-based excavation device recognition system for underground pipeline protection. The front-end hardware system is equipped with an acoustic sensor array, an Analog-to-Digital Converter (ADC) module (ADS1274), and an industrial processor Advanced RISC Machine (ARM) cortex-A8 for signal collection and algorithm implementation. Then, a novel Statistical Time-Frequency acoustic Feature (STFF) is proposed, and a fast Extreme Learning Machine (ELM) is adopted as the classifier. Experiments on real recorded data show that the proposed STFF achieves better discriminative capability than the conventional acoustic cepstrum features. In addition, the surveillance platform is applicable for encountering big data owing to the fast learning speed of ELM.",industry
10.1007/978-3-030-42462-6_123,preprocessed,The Palgrave Handbook of Climate Resilient Societies,Springer,2021-01-01 00:00:00,springer,water 4.0: enhancing climate resilience,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-42462-6_123,"For this chapter, water 4.0 is defined as the industry 4.0 concept applied to the water sector. As industry 4.0 reflects the fourth industrial revolution , water 4.0 reflects the fourth water revolution . Based on the literature review and case studies, this chapter examines a proposition that water 4.0 will increase not only the sector’s economic effectiveness but also sustainability including climate resilience. Relevant technologies include digital twins , visualization, wireless monitoring sensors, industrial internet of things (IoT/IIoT), cloud computing, and predictive or prescriptive analytics but also blockchain , drones, and cybersecurity. For water 4.0 becoming a reality, water utility companies need not only collect more data but also to have proper analytical tools in place to convert data into information supporting optimal decisions. The current tools should preferably be replaced by machine learning algorithms that are nonlinear, nonstationary, and dynamic and thus aligned closely with the real world. It has been suggested in this chapter that such disruptive technologies be introduced through an ISO 55001-based asset management system (AMS). ISO 19650 series supplements ISO 55001 and contains additional requirements for the AMS development by focusing particularly on asset information. For this purpose, the series provides assistance with big data and digital twins . Two approaches are applicable to the implementation of water 4.0 through AMS: adaptability and more traditional continuous improvement with the former considered in this chapter as preferred but requires a sufficient level of asset management maturity. Therefore, it might be prudent that every organization sets their own water 4.0 -related standards and objectives in their own AMS and considers the preferred level of adaptability . Adaptability is arguably required for water 4.0 with adaptation bringing the greatest value .",industry
http://arxiv.org/abs/2202.10075v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"icsml: industrial control systems machine learning inference framework
  natively executing on iec 61131-3 languages",http://arxiv.org/abs/2202.10075v1,"Industrial Control Systems (ICS) have played a catalytic role in enabling the
4th Industrial Revolution. ICS devices like Programmable Logic Controllers
(PLCs), automate, monitor and control critical processes in industrial, energy
and commercial environments. The convergence of traditional Operational
Technology (OT) with Information Technology (IT) has opened a new and unique
threat landscape. This has inspired defense research that focuses heavily on
Machine Learning (ML) based anomaly detection methods that run on external IT
hardware which means an increase in costs and the further expansion of the
threat landscape. To remove this requirement, we introduce the ICS Machine
Learning inference framework (ICSML) which enables the execution of ML models
natively on the PLC. ICSML is implemented in IEC 61131-3 code and works around
the limitations imposed by the domain-specific languages, providing a complete
set of components for the creation of fully fledged ML models in a way similar
to established ML frameworks. We then demonstrate a complete end-to-end
methodology for creating ICS ML models using an external framework for training
and ICSML for the PLC implementation. To evaluate our contributions we run a
series of benchmarks studying memory and performance and compare our solution
to the TFLite inference framework. Finally, to demonstrate the abilities of
ICSML and to verify its non-intrusive nature, we develop and evaluate a case
study of a real defense for process aware attacks against a Multi Stage Flash
(MSF) desalination plant.",industry
http://arxiv.org/abs/2202.09549v1,preprocessed,arxiv,arxiv,2022-02-19 00:00:00,arxiv,"learning to detect slip with barometric tactile sensors and a temporal
  convolutional neural network",http://arxiv.org/abs/2202.09549v1,"The ability to perceive object slip via tactile feedback enables humans to
accomplish complex manipulation tasks including maintaining a stable grasp.
Despite the utility of tactile information for many applications, tactile
sensors have yet to be widely deployed in industrial robotics settings; part of
the challenge lies in identifying slip and other events from the tactile data
stream. In this paper, we present a learning-based method to detect slip using
barometric tactile sensors. These sensors have many desirable properties
including high durability and reliability, and are built from inexpensive,
off-the-shelf components. We train a temporal convolution neural network to
detect slip, achieving high detection accuracies while displaying robustness to
the speed and direction of the slip motion. Further, we test our detector on
two manipulation tasks involving a variety of common objects and demonstrate
successful generalization to real-world scenarios not seen during training. We
argue that barometric tactile sensing technology, combined with data-driven
learning, is suitable for many manipulation tasks such as slip compensation.",industry
http://arxiv.org/abs/2202.09113v1,preprocessed,arxiv,arxiv,2022-02-18 00:00:00,arxiv,how to manage tiny machine learning at scale: an industrial perspective,http://arxiv.org/abs/2202.09113v1,"Tiny machine learning (TinyML) has gained widespread popularity where machine
learning (ML) is democratized on ubiquitous microcontrollers, processing sensor
data everywhere in real-time. To manage TinyML in the industry, where mass
deployment happens, we consider the hardware and software constraints, ranging
from available onboard sensors and memory size to ML-model architectures and
runtime platforms. However, Internet of Things (IoT) devices are typically
tailored to specific tasks and are subject to heterogeneity and limited
resources. Moreover, TinyML models have been developed with different
structures and are often distributed without a clear understanding of their
working principles, leading to a fragmented ecosystem. Considering these
challenges, we propose a framework using Semantic Web technologies to enable
the joint management of TinyML models and IoT devices at scale, from modeling
information to discovering possible combinations and benchmarking, and
eventually facilitate TinyML component exchange and reuse. We present an
ontology (semantic schema) for neural network models aligned with the World
Wide Web Consortium (W3C) Thing Description, which semantically describes IoT
devices. Furthermore, a Knowledge Graph of 23 publicly available ML models and
six IoT devices were used to demonstrate our concept in three case studies, and
we shared the code and examples to enhance reproducibility:
https://github.com/Haoyu-R/How-to-Manage-TinyML-at-Scale",industry
http://arxiv.org/abs/2202.08897v1,preprocessed,arxiv,arxiv,2022-02-17 00:00:00,arxiv,"implementing spiking neural networks on neuromorphic architectures: a
  review",http://arxiv.org/abs/2202.08897v1,"Recently, both industry and academia have proposed several different
neuromorphic systems to execute machine learning applications that are designed
using Spiking Neural Networks (SNNs). With the growing complexity on design and
technology fronts, programming such systems to admit and execute a machine
learning application is becoming increasingly challenging. Additionally,
neuromorphic systems are required to guarantee real-time performance, consume
lower energy, and provide tolerance to logic and memory failures. Consequently,
there is a clear need for system software frameworks that can implement machine
learning applications on current and emerging neuromorphic systems, and
simultaneously address performance, energy, and reliability. Here, we provide a
comprehensive overview of such frameworks proposed for both, platform-based
design and hardware-software co-design. We highlight challenges and
opportunities that the future holds in the area of system software technology
for neuromorphic computing.",industry
http://arxiv.org/abs/2202.06149v1,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,"automatic issue classifier: a transfer learning framework for
  classifying issue reports",http://arxiv.org/abs/2202.06149v1,"Issue tracking systems are used in the software industry for the facilitation
of maintenance activities that keep the software robust and up to date with
ever-changing industry requirements. Usually, users report issues that can be
categorized into different labels such as bug reports, enhancement requests,
and questions related to the software. Most of the issue tracking systems make
the labelling of these issue reports optional for the issue submitter, which
leads to a large number of unlabeled issue reports. In this paper, we present a
state-of-the-art method to classify the issue reports into their respective
categories i.e. bug, enhancement, and question. This is a challenging task
because of the common use of informal language in the issue reports. Existing
studies use traditional natural language processing approaches adopting
key-word based features, which fail to incorporate the contextual relationship
between words and therefore result in a high rate of false positives and false
negatives. Moreover, previous works utilize a uni-label approach to classify
the issue reports however, in reality, an issue-submitter can tag one issue
report with more than one label at a time. This paper presents our approach to
classify the issue reports in a multi-label setting. We use an off-the-shelf
neural network called RoBERTa and fine-tune it to classify the issue reports.
We validate our approach on issue reports belonging to numerous industrial
projects from GitHub. We were able to achieve promising F-1 scores of 81%, 74%,
and 80% for bug reports, enhancements, and questions, respectively. We also
develop an industry tool called Automatic Issue Classifier (AIC), which
automatically assigns labels to newly reported issues on GitHub repositories
with high accuracy.",industry
http://arxiv.org/abs/2202.04834v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"geometric digital twinning of industrial facilities: retrieval of
  industrial shapes",http://arxiv.org/abs/2202.04834v1,"This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",industry
http://arxiv.org/abs/2202.03028v1,preprocessed,arxiv,arxiv,2022-02-07 00:00:00,arxiv,quark: a framework for quantum computing application benchmarking,http://arxiv.org/abs/2202.03028v1,"Quantum computing (QC) is anticipated to provide a speedup over classical HPC
approaches for specific problems in optimization, simulation, and machine
learning. With the advances in quantum computing toward practical applications,
the need to analyze and compare different quantum solutions increases. While
different low-level benchmarks for QC exist, these benchmarks do not provide
sufficient insights into real-world application-level performance. We propose
an application-centric benchmark method and the QUantum computing Application
benchmaRK (QUARK) framework to foster the investigation and creation of
application benchmarks for QC. This paper establishes three significant
contributions: (1) it makes a case for application-level benchmarks and
provides an in-depth ""pen and paper"" benchmark formulation of two reference
problems: robot path and vehicle option optimization from the industrial
domain; (2) it proposes the open-source QUARK framework for designing,
implementing, executing, and analyzing benchmarks; (3) it provides multiple
reference implementations for these two reference problems based on different
known, and where needed, extended, classical and quantum algorithmic approaches
and analyzes their performance on different types of infrastructures.",industry
http://arxiv.org/abs/2202.02813v2,preprocessed,arxiv,arxiv,2022-02-06 00:00:00,arxiv,a coding framework and benchmark towards compressed video understanding,http://arxiv.org/abs/2202.02813v2,"Most video understanding methods are learned on high-quality videos. However,
in real-world scenarios, the videos are first compressed before the
transportation and then decompressed for understanding. The decompressed videos
may have lost the critical information to the downstream tasks. To address this
issue, we propose the first coding framework for compressed video
understanding, where another learnable analytic bitstream is simultaneously
transported with the original video bitstream. With the dedicatedly designed
self-supervised optimization target and dynamic network architectures, this new
stream largely boosts the downstream tasks yet with a small bit cost. By only
one-time training, our framework can be deployed for multiple downstream tasks.
Our framework also enjoys the best of both two worlds, (1) high efficiency of
industrial video codec and (2) flexible coding capability of neural networks
(NNs). Finally, we build a rigorous benchmark for compressed video
understanding on three popular tasks over seven large-scale datasets and four
different compression levels. The proposed Understanding oriented Video Coding
framework UVC consistently demonstrates significantly stronger performances
than the baseline industrial codec.",industry
http://arxiv.org/abs/2201.12170v3,preprocessed,arxiv,arxiv,2022-01-28 00:00:00,arxiv,"unsupervised single-shot depth estimation using perceptual
  reconstruction",http://arxiv.org/abs/2201.12170v3,"Real-time estimation of actual object depth is a module that is essential to
performing various autonomous system tasks such as 3D reconstruction, scene
understanding and condition assessment of machinery parts. During the last
decade of machine learning, extensive deployment of deep learning methods to
computer vision tasks has yielded approaches that succeed in achieving
realistic depth synthesis out of a simple RGB modality. While most of these
models are based on paired depth data or availability of video sequences and
stereo images, methods for single-view depth synthesis in a fully unsupervised
setting have hardly been explored. This study presents the most recent advances
in the field of generative neural networks, leveraging them to perform fully
unsupervised single-shot depth synthesis. Two generators for RGB-to-depth and
depth-to-RGB transfer are implemented and simultaneously optimized using the
Wasserstein-1 distance and a novel perceptual reconstruction term. To ensure
that the proposed method is plausible, we comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database and the SURREAL dataset that records body depth. The success observed
in this study suggests the great potential for unsupervised single-shot depth
estimation in real-world applications.",industry
http://arxiv.org/abs/2201.06735v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,ai augmented digital metal component,http://arxiv.org/abs/2201.06735v1,"The aim of this work is to propose a new paradigm that imparts intelligence
to metal parts with the fusion of metal additive manufacturing and artificial
intelligence (AI). Our digital metal part classifies the status with real time
data processing with convolutional neural network (CNN). The training data for
the CNN is collected from a strain gauge embedded in metal parts by laser
powder bed fusion process. We implement this approach using additive
manufacturing, demonstrate a self-cognitive metal part recognizing partial
screw loosening, malfunctioning, and external impacting object. The results
indicate that metal part can recognize subtle change of multiple fixation state
under repetitive compression with 89.1% accuracy with test sets. The proposed
strategy showed promising potential in contributing to the hyper-connectivity
for next generation of digital metal based mechanical systems",industry
http://arxiv.org/abs/2201.06616v2,preprocessed,arxiv,arxiv,2022-01-17 00:00:00,arxiv,improving the quality control of seismic data through active learning,http://arxiv.org/abs/2201.06616v2,"In image denoising problems, the increasing density of available images makes
an exhaustive visual inspection impossible and therefore automated methods
based on machine-learning must be deployed for this purpose. This is
particulary the case in seismic signal processing. Engineers/geophysicists have
to deal with millions of seismic time series. Finding the sub-surface
properties useful for the oil industry may take up to a year and is very costly
in terms of computing/human resources. In particular, the data must go through
different steps of noise attenuation. Each denoise step is then ideally
followed by a quality control (QC) stage performed by means of human expertise.
To learn a quality control classifier in a supervised manner, labeled training
data must be available, but collecting the labels from human experts is
extremely time-consuming. We therefore propose a novel active learning
methodology to sequentially select the most relevant data, which are then given
back to a human expert for labeling. Beyond the application in geophysics, the
technique we promote in this paper, based on estimates of the local error and
its uncertainty, is generic. Its performance is supported by strong empirical
evidence, as illustrated by the numerical experiments presented in this
article, where it is compared to alternative active learning strategies both on
synthetic and real seismic datasets.",industry
http://arxiv.org/abs/2201.06599v1,preprocessed,arxiv,arxiv,2022-01-17 00:00:00,arxiv,"who supervises the supervisor? model monitoring in production using deep
  feature embeddings with applications to workpiece inspection",http://arxiv.org/abs/2201.06599v1,"The automation of condition monitoring and workpiece inspection plays an
essential role in maintaining high quality as well as high throughput of the
manufacturing process. To this end, the recent rise of developments in machine
learning has lead to vast improvements in the area of autonomous process
supervision. However, the more complex and powerful these models become, the
less transparent and explainable they generally are as well. One of the main
challenges is the monitoring of live deployments of these machine learning
systems and raising alerts when encountering events that might impact model
performance. In particular, supervised classifiers are typically build under
the assumption of stationarity in the underlying data distribution. For
example, a visual inspection system trained on a set of material surface
defects generally does not adapt or even recognize gradual changes in the data
distribution - an issue known as ""data drift"" - such as the emergence of new
types of surface defects. This, in turn, may lead to detrimental
mispredictions, e.g. samples from new defect classes being classified as
non-defective. To this end, it is desirable to provide real-time tracking of a
classifier's performance to inform about the putative onset of additional error
classes and the necessity for manual intervention with respect to classifier
re-training. Here, we propose an unsupervised framework that acts on top of a
supervised classification system, thereby harnessing its internal deep feature
representations as a proxy to track changes in the data distribution during
deployment and, hence, to anticipate classifier performance degradation.",industry
http://arxiv.org/abs/2201.04263v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,the human factor in ai safety,http://arxiv.org/abs/2201.04263v1,"AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.",industry
http://arxiv.org/abs/2201.02028v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,"a light in the dark: deep learning practices for industrial computer
  vision",http://arxiv.org/abs/2201.02028v1,"In recent years, large pre-trained deep neural networks (DNNs) have
revolutionized the field of computer vision (CV). Although these DNNs have been
shown to be very well suited for general image recognition tasks, application
in industry is often precluded for three reasons: 1) large pre-trained DNNs are
built on hundreds of millions of parameters, making deployment on many devices
impossible, 2) the underlying dataset for pre-training consists of general
objects, while industrial cases often consist of very specific objects, such as
structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal
issues for companies. As a remedy, we study neural networks for CV that we
train from scratch. For this purpose, we use a real-world case from a solar
wafer manufacturer. We find that our neural networks achieve similar
performances as pre-trained DNNs, even though they consist of far fewer
parameters and do not rely on third-party datasets.",industry
10.1016/j.compag.2022.106688,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,implementation of a decision support system for prediction of the total soluble solids of industrial tomato using machine learning models,https://api.elsevier.com/content/abstract/scopus_id/85122635918,"Tomato is the second most important vegetable in the world, both in terms of production and consumption. Especially for the cultivation of industrial tomato, harvest is conducted when the total soluble solids, a major quality characteristic, are as high as possible. Advancements in technology have made Decision Support Systems simpler and more applicable in an everyday basis. Data Analysis, combined with Machine Learning algorithms are considered the future of sustainable agriculture, allowing farmers to be advised about the best possible decisions for their cultivation. Farmers need to adopt this kind of technology in order to be able to know when the quality of tomatoes is at its peak, in order to gather their product from the field. The implementation of a Decision Support System to predict the total soluble solids was conducted,based on data from previous years, including quality data (pH, Bostwick, L, a/b, Mean Weight, °Brix), the type of hybrid used, weather data and soil data from the fields. Data derived from fields in 6 different regions in the northwestern Peloponnese, Greece over 6 cultivation periods, created a dataset of 33 different inputs. Thirteen different algorithms were put into evaluation in order to find the best one in terms of speed and efficiency. In this research, we developed a Decision Support System using the K-nearest algorithm, which proved to be the best for our dataset. The predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. This DSS which is using real time weather data as an input is expected to be a valuable tool for the farmers.",industry
10.1016/j.autcon.2021.104088,preprocessed,Automation in Construction,scopus,2022-02-01,sciencedirect,vision-based high-precision intelligent monitoring for shield tail clearance,https://api.elsevier.com/content/abstract/scopus_id/85120874971,"Real-time shield tail clearance measurement and monitoring is a key task during shield tunneling construction. The shield tail clearance measurement and monitoring technology development is still in its infancy, the current methods are mainly designed manually based on intuition. In order to fill the gap between the requirement of shield tail clearance measurement and monitoring and the limitations of the current methods, this paper systematically studies the existing mechanisms related to shield tail clearance measurement and monitoring, and develops a high-precision intelligent monitoring system for shield tail clearance. The proposed monitoring system includes four components: 1) two types of shield tail clearance calculation models, 2) the integrated hardware of the monitoring system which is composed of a data acquisition unit, a signal transmission unit and a control unit, 3) the region of interest (ROI) extraction method based on deep neural network, and the image processing algorithms for image enhancement and feature extraction, 4) the custom-developed software built on mature integrated development environment (IDE). After the calculation model of shield tail clearance is established, the system uses monitoring devices equipped with industrial cameras to obtain the on-site image, and then applies image processing technologies along with deep learning approach to extract the key features, which are brought into the model to calculate the values of shield tail clearance, finally displays these values and simulates the current tunneling attitude of the shield machine in real time. The experimental results show that the system proposed in this paper achieves the goal of high precision measuring and real-time monitoring of the shield tail clearance.",industry
10.1016/j.jisa.2021.103046,preprocessed,Journal of Information Security and Applications,scopus,2022-02-01,sciencedirect,aicrit: a unified framework for real-time anomaly detection in water treatment plants,https://api.elsevier.com/content/abstract/scopus_id/85119422439,"Industrial Control Systems (ICS) in public infrastructure, such as water treatment and distribution plants, have become a target of sophisticated cyber-attacks. Given the ever-present insider and other threats in such systems, there is a need to deploy mechanisms for defense and incidence response beyond the traditional. In this work we present AICrit that operates over the physical constraints and domain norms for accurate and timely detection of process anomalies. AICrit learns system-wide normal behavior using design knowledge and machine learning algorithms to recognize abnormal or irregular behavioral patterns resulting due to process anomalies. AICrit was implemented and evaluated in SWaT by launching several real-time stealthy and coordinated attacks. Experimental results attest to the effectiveness of AICrit in the timely detection of process anomalies with a low occurrence of false alarms. The underlying methodology used in the design of AICrit is generic and applicable to other ICS in various domains such as power, energy, and transportation.",industry
10.1016/j.apenergy.2021.118127,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,data-driven control of room temperature and bidirectional ev charging using deep reinforcement learning: simulations and experiments,https://api.elsevier.com/content/abstract/scopus_id/85118721393,"The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.",industry
10.1016/j.ssci.2021.105529,preprocessed,Safety Science,scopus,2022-02-01,sciencedirect,a novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry
10.1016/j.eswa.2021.116045,preprocessed,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,posimnet-r: an immunologic resilient approach to position routers in industrial wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/85117584055,"Industry 4.0 has increased the interest in employing Industrial Wireless Sensor Network (IWSN) technologies in industrial automation. The advantages range from ease of installation and maintenance to reduced deployment time and infrastructure costs. However, industrial automation has critical requirements regarding network infrastructure, such as reliability and failure tolerance. Therefore, it is imperative to have an adequate placement of sensor and router nodes, to obtain a network with multiple paths, allowing the data to reach management systems within a reasonable time, even in the event of failures. The placement of router nodes has to consider latency, network lifespan, connectivity, and failure tolerance aspects in a possibly hostile environment, with classified areas and obstacles such as silos, tanks and buildings. We present a new approach, called POSIMNET-R, to place IWSN routing nodes in an industrial configuration, which circumvents forbidden areas and obstacles, based on Artificial Immunological Networks. The resulting network offers low failure rates and path redundancy criteria. The results have shown that POSIMNET-R was capable of providing a reliable network with multiple paths and resilience of the used routers equal to 81.50% in the basic case study and 73.66% in the real case scenario.",industry
10.1016/j.comcom.2021.10.036,preprocessed,Computer Communications,scopus,2022-01-15,sciencedirect,lstm-mfcn: a time series classifier based on multi-scale spatial–temporal features,https://api.elsevier.com/content/abstract/scopus_id/85119299619,"Time series classification (TSC) task attracts huge interests, since they correspond to the real-world problems in a wide variety of fields, such as industry monitoring. Deep learning methods, especially CNN and FCN, shows competitive performance in TSC task by their virtue of good adaption for raw time series and self-adapting extraction of features. Then various variants of CNN are proposed so as to make further breakthrough by the better perception to characteristics of data. Among them, LSTM-FCN and GRU-FCN who learn spatial and temporal features simultaneously are the most remarkable ones, achieving state of the art results. Therefore, inspired by their success and in consideration of the discriminative features implied in time series are diverse in size, a multimodal network LSTM-MFCN composed of multi-scale FCN (MFCN) and LSTM are proposed in this work. The gate-based network LSTM naturally fits to various terms time dependencies, and FCN with multi-scale sets of filters are capable to perceive spatial features of different range from time series curves. Besides, dilation convolution is deployed to build multi-scale receptive fields in larger level without increasing the parameters to be trained. The full perception of large multi-scale spatial–temporal features lead LSTM-MFCN to possess comprehensive and thorough grasp to time series, thus achieve even better accuracies. Finally, two representative architectures are presented specifically and their experiments on UCR datasets reveals the effectiveness and superiority of proposed LSTM-MFCN.",industry
10.1016/j.energy.2021.122359,preprocessed,Energy,scopus,2022-01-15,sciencedirect,fuzzy inference system application for oil-water flow patterns identification,https://api.elsevier.com/content/abstract/scopus_id/85117714992,"Prediction of oil-water two-phase flow pattern provides an effective solution for reducing oil production costs. In this research, the fuzzy inference system (FIS) is utilized to predict fluid flow patterns and establish a new adaptable prediction model. This paper takes No. 10 industrial white oil and tap water as the research objects to simulate fluids, and analyzes the changes of the pipeline angle, the total flow of oil-water two-phase flow and the convective pattern of water cut. A data set containing 60 samples was used to create the model, and the Mamdani fuzzy model was established using MATLAB software. The results show that compared with the BP neural network algorithm, the model set forth in the present paper has higher accuracy and reliability, and can achieve real-time monitoring and effectively reduce errors, especially in the case of decision-making. In addition, the fuzzy model is demonstrated that in the entire production logging process of non-vertical wells, the use of a fuzzy inference system to predict fluid flow patterns can greatly save production costs while ensuring the safe operation of production equipment.",industry
10.1016/j.aca.2021.339411,preprocessed,Analytica Chimica Acta,scopus,2022-01-01,sciencedirect,a video processing and machine vision-based automatic analyzer to determine sequentially total suspended and settleable solids in wastewater,https://api.elsevier.com/content/abstract/scopus_id/85123884378,"The monitoring of total suspended (TSS) and settleable (SetS) solids in wastewater is essential to maintain the quality parameters for aquatic biota because they can transport pollutants and block light penetration. Determining them by their respective reference methods, however, is laborious, expensive, and time consuming. To overcome this, we developed a new analytical instrument called Solids in Wastewater's Machine Vision-based Automatic Analyzer (SWAMVA), which is equiped with an automatic sampler and a software for real-time digital movie capture to quantify sequentially the TSS and SetS contents in wastewater samples. The machine vision algorithm (MVA) coupled with the Red color plane (derived from color histograms in the Red-Green-Blue (RGB) system) showed the best prediction results with R2 of 0.988 and 0.964, and relative error of prediction (REP) of 6.133 and 9.115% for TSS and SetS, respectively. The constructed models were validated by Analysis of Variance (ANOVA), and the accuracy and precision of the predictions by the t- and F-tests, respectively, at a 0.05 significance level. The elliptical joint confidence region (EJCR) test confirmed the accuracy, while the coefficient of variation (CV) of 6.529 and 10.908% confirmed the good precisions, respectively. Compared with the reference method (Standard Methods For the Examination of Water and Wastewater), the proposed method reduced the analysis volume from 1.5 L to just 15 mL and the analysis time from 12 h to 24 s per sample. Therefore, SWAMVA can be considered an important alternative to the determination of TSS and SetS in wastewater as an automatic, fast, and low-cost analytical tool, following the principles of Green Chemistry and exploiting Industry 4.0 features such as intelligent processing, miniaturization, and machine vision.",industry
10.1016/j.jmsy.2022.01.010,preprocessed,Journal of Manufacturing Systems,scopus,2022-01-01,sciencedirect,"towards edge computing in intelligent manufacturing: past, present and future",https://api.elsevier.com/content/abstract/scopus_id/85123859503,"Industry 4.0 (I4.0) is the fourth industrial revolution and a synonym for intelligent manufacturing. It drives the convergence of several cutting-edge technologies to provoke autonomous, fully integrated, collaborated, highly automated, and customized industries. Edge Computing (EC), a highly distributed framework, emerged a couple of years ago and embraced the industry to leverage the benefit of low latency and near real-time performance. It brings computation and storage in the close proximity of end devices and reduces the cloud overhead. In addition to improved operational efficiency, storage, and latency, EC further reduces the cost, improves productivity with higher quality maintenance and customer satisfaction. At the digital-to-digital stage of the Physical-Digital-Physical (PDP) loop, adapting EC can furnish tremendous benefits and further accelerate the next stages of the loop. This survey identifies the past and present works oriented towards Intelligent Manufacturing integrated with the EC platform and categorizes the research based on architecture, intelligence platform, edge objectives, and application. Herein, the authors have incorporated; (1) The progress in I4.0 following the PDP loop; (2) The discussion on EC in I4.0 and their Research Trend; (3) Methods to bring intelligence to the edge. To the best of our knowledge, it is the first review article that focuses on the applications and objectives of EC in Intelligent Manufacturing. It also outlines the optimum solutions to bring intelligence to the edge by overcoming the resource and complexity-bound with accuracy and latency constraints for the decision-making processes. Future directions include the less explored research areas, challenges in edge deployment in industries, and the integration of trending technologies such as Blockchain, Software Defined Networking, and 5 G with EC to excite the EC researchers. A few collaborative edge scenarios are discussed for the promotion and application of EC in I4.0. Nevertheless, efficient edge deployments face many challenges since studies are still limited to conceptual levels or design steps, and future orientation to application strategies for Smart Manufacturing is required.",industry
10.1016/j.cie.2021.107824,preprocessed,Computers and Industrial Engineering,scopus,2022-01-01,sciencedirect,new perspectives and results for smart operators in industry 4.0: a human-centered approach,https://api.elsevier.com/content/abstract/scopus_id/85122422552,"Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators.
                  To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept.
                  Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework.",industry
10.1016/j.softx.2021.100956,preprocessed,SoftwareX,scopus,2022-01-01,sciencedirect,tx2_fcnn_node: an open-source ros compatible tool for monocular depth reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85121968187,"We present tx2_fcnn_node – a Robot Operating System (ROS) compatible tool that is aimed at seamless integration of various monocular depth reconstruction neural networks to the robotic software based on ROS (which is a de-facto standard in the area of robotics). Our tool simplifies the process of deploying, evaluating, and comparing depth reconstruction neural networks both on real robots and in simulation. We complement our software with a set of the precompiled neural networks which can be used off the shelf, with some of them being able to demonstrate near real-time performance when running onboard compact embedded platforms, e.g. Nvidia Jetson TX2, that are often used nowadays both in academia and industry.",industry
10.1016/j.suscom.2021.100650,preprocessed,Sustainable Computing: Informatics and Systems,scopus,2022-01-01,sciencedirect,a conceptual framework for the implementation of industry 4.0 in legal informatics,https://api.elsevier.com/content/abstract/scopus_id/85121918847,"The growing number of applications of Industry 4.0 in the field of legal informatics offers huge opportunities for data scientists and academic researchers. The term Industry 4.0 is defined as “the fourth industrial revolution that connects embedded systems to Cyber-Physical-Systems”. It emphasizes the end-to-end digitalization of all physical resources and integrating digital environments with the value chain organizations. Industry 4.0 comprises a variety of technologies such as Cyber-Physical-Systems, Big Data, Internet of Things, Artificial Intelligence, Cloud Computing and Cybersecurity. It has been found that the implementation of these technologies may be useful in achieving the objective of legal informatics. It can help lawmakers to align jurisprudence by providing modern computational technologies to improve and advance the traditional legal justice system. Further, the integration of legal informatics with Industry 4.0 will be strengthening the legal justice system by providing decision-making support, data transparency, real-time monitoring, cost-effective solution, and triple-bottom-line performance in the future. Therefore, the article aims to determine the implementation patterns of Industry 4.0 technologies in legislative institutions and administrations. The study proposes a conceptual framework that integrates Industry 4.0 with legal informatics. The findings show that implementing Industry 4.0 technologies such as Artificial Intelligence, Big Data, and Cloud Computing plays a vital role for legal firms that are currently in the nascent stage of development.",industry
10.1016/j.compag.2021.106635,preprocessed,Computers and Electronics in Agriculture,scopus,2022-01-01,sciencedirect,intelligent iot-multiagent precision irrigation approach for improving water use efficiency in irrigation systems at farm and district scales,https://api.elsevier.com/content/abstract/scopus_id/85121511874,"The fourth industrial revolution in agriculture seeks the automation of traditional practices, using modern smart technologies. Advances in electronics, computation and the internet of things are integrated for improving field inputs management. The aim of this paper is to present the design and implementation of an intelligent IoT-multiagent precision irrigation approach for improving water use efficiency in irrigation systems. The study site was the large-scale irrigation and drainage district of Chicamocha and Firavitoba (Usochicamocha) located in Boyacá - Colombia, where water is distributed from the Chicamocha riverbed. In the proposed system, irrigation is supervised and controlled in each field by an intelligent irrigation agent that autonomously prescribes and applies water amounts with agronomical criteria. The methodology was applied with real (cyber-physical) and virtual (simulated) intelligent agents and was extended to eleven pump stations that supply water to 5911 fields. Using a MQTT protocol, hundreds of irrigation intelligent agents report water prescriptions and crop characteristics to a master agent in each pump station, who creates a regional irrigation map to manage georeferenced field information and performs negotiation of water resources between agents according to supply availability. Field maps and intelligent irrigation agents can be visualized using devices with internet access. Results demonstrated that irrigation amounts were correctly applied on the fields, thus improving the water use efficiency. This technology is a novel support to decision-making in water resources management applications at field and district scales.",industry
10.1016/j.enconman.2021.115030,preprocessed,Energy Conversion and Management,scopus,2022-01-01,sciencedirect,deep reinforcement learning based energy management strategy of fuel cell hybrid railway vehicles considering fuel cell aging,https://api.elsevier.com/content/abstract/scopus_id/85119905389,"In the rail transportation industry, growing energy and environmental awareness requires the use of alternatives to combustion engines. These include hybrid electrically driven railway vehicles powered by fuel cells and batteries. The cost of hydrogen consumption and the lifetime of fuel cells are currently the main challenges that need to be addressed before widespread deployment of fuel cell railway vehicles can be realized. With this in mind, this work focuses on the energy management system with emphasis on optimizing the energy distribution to reduce the overall operational cost. The presented energy management strategy (EMS) aims at minimizing hydrogen consumption and fuel cell aging costs while achieving a favorable balance between battery charging and discharging. In order to take fuel cell aging into account in energy management and mitigate fuel cell aging trough power distribution, an online fuel cell aging estimation model based on four operation modes is introduced and applied. Moreover, the advanced deep reinforcement learning method Twin Delayed Deep Deterministic Policy Gradient (TD3) is used to obtain a promising EMS. To improve the adaptability of the strategy, a stochastic training environment, which is based on real measured speed profiles considering passenger numbers is used for training. Assuming different environmental and passenger transport volumes, the results confirm that the proposed TD3-EMS achieves battery charge-sustaining at low hydrogen consumption while slowing down fuel cell degradation.",industry
10.1016/j.ijpe.2021.108339,preprocessed,International Journal of Production Economics,scopus,2022-01-01,sciencedirect,age-based preventive maintenance with multiple printing options,https://api.elsevier.com/content/abstract/scopus_id/85118549755,"In today's economic context, production systems must be readily available and machinery downtime kept to a minimum. Maintenance and spare parts inventory management play a vital role in achieving these goals, and preventive maintenance has increasingly been considered in maintenance policies. Additive manufacturing (AM) has recently been combined with preventive maintenance, and thus represents an emerging research direction. However, few studies have as yet been conducted in this research stream, and we intend to fill this gap. Our study makes three main contributions. First, we address the main limitations of two current models (i.e., assuming that no failure occurs during the replenishment lead time of the spare parts). Second, we propose a new maintenance policy that considers two printing options with different levels of reliability and unitary purchase costs. Third, we develop a decision support system (DSS) to assist managers in deciding whether to implement a preventive maintenance policy that includes AM or conventional manufacturing (CM) parts. We take an interdisciplinary approach to conducting a parametrical analysis where we consider real data on the reliability of CM and AM parts, in addition to the impact of post-processing operations and optimization routines. We find that AM-based preventive maintenance policies are favored when the MTTF and the backorder costs are low and when the failure and maintenance costs are high. These findings have been incorporated into the DSS, which provides thresholds for every parameter to guide practitioners in choosing between AM and CM parts for preventive maintenance, without requiring time-expensive calculations.",industry
10.1016/j.cose.2021.102500,preprocessed,Computers and Security,scopus,2022-01-01,sciencedirect,antiviruses under the microscope: a hands-on perspective,https://api.elsevier.com/content/abstract/scopus_id/85118529412,"AntiViruses (AVs) are the main defense line against attacks for most users and much research has been done about them, especially proposing new detection procedures that work in academic prototypes. However, as most current and commercial AVs are closed-source solutions, in practice, little is known about their real internals: information such as what is a typical AV database size, the detection methods effectively used in each operation mode, and how often on average the AVs are updated are still unknown. This prevents research work from meeting the industrial practices more thoroughly. To fill this gap, in this work, we systematize the knowledge about AVs. To do so, we first surveyed the literature and identified existing knowledge gaps in AV internals’ working. Further, we bridged these gaps by analyzing popular (Windows, Linux, and Android) AV solutions to check their operations in practice. Our methodology encompassed multiple techniques, from tracing to fuzzing. We detail current AV’s architecture, including their multiple components, such as browser extensions and injected libraries, regarding their implementation, monitoring features, and self-protection capabilities. We discovered, for instance, a great disparity in the set of API functions hooked by the distinct AV’s libraries, which might have a significant impact in the viability of academically-proposed detection models (e.g., machine learning-based ones).",industry
10.1016/j.compind.2021.103556,preprocessed,Computers in Industry,scopus,2022-01-01,sciencedirect,c-ports: a proposal for a comprehensive standardization and implementation plan of digital services offered by the “port of the future”,https://api.elsevier.com/content/abstract/scopus_id/85118477493,"In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called “Port of the Future”. How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel & Marine Navigation, (ii) e-Freight & (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C-Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed.",industry
10.1016/j.dss.2021.113653,preprocessed,Decision Support Systems,scopus,2022-01-01,sciencedirect,ai-based industrial full-service offerings: a model for payment structure selection considering predictive power,https://api.elsevier.com/content/abstract/scopus_id/85114151068,"Artificial Intelligence and servitization reshape the way that manufacturing companies derive value. Aiming to sustain competitive advantage and intensify customer loyalty, full-service providers offer the use of their products as a service to achieve continuous revenues. For this purpose, companies implement AI classification algorithms to enable high levels of service at controllable costs. However, traditional asset sellers who become service providers require previously atypical payment structures, as classic payment methods involving a one-time fee for production costs and profit margins are unsuitable. In addition, a low predictive power of the implemented classification algorithm can lead to misclassifications, which diminish the achievable level of service and the intended net present value of the resultant service. While previous works focus solely on the costs of such misclassifications, our decision model highlights implications for payment structures, service levels, and – ultimately – the net present value of such data-driven service offerings. Our research suggests that predictive power can be a major factor in selecting a suitable payment structure and the overall design of service level agreements. Therefore, we compare common payment structures for data-driven services and investigate their relationship to predictive power. We develop our model using a design science methodology and iteratively evaluate our results using a four-step approach that includes interviews with industry experts and the application of our model to a real-world use case. In summary, our research extends the existing knowledge of servitization and data-driven services in the manufacturing industry through a quantitative decision model.",industry
10.1109/ccnc49033.2022.9700515,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,cave-vr and unity game engine for visualizing city scale 3d meshes,https://ieeexplore.ieee.org/document/9700515/,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",smart cities
10.1109/ccnc49033.2022.9700676,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,qos-aware priority-based task offloading for deep learning services at the edge,https://ieeexplore.ieee.org/document/9700676/,"Emerging Edge Computing (EC) technology has shown promise for many delay-sensitive Deep Learning (DL) based applications of smart cities in terms of improved Quality-of-Service (QoS). EC requires judicious decisions which jointly consider the limited capacity of the edge servers and provided QoS of DL-dependent services. In a smart city environment, tasks may have varying priorities in terms of when and how to serve them; thus, priorities of the tasks have to be considered when making resource management decisions. In this paper, we focus on finding optimal offloading decisions in a three-tier user-edge-cloud architecture while considering different priority classes for the DL-based services and making a trade-off between a task’s completion time and the provided accuracy by the DL-based service. We cast the optimization problem as an Integer Linear Program (ILP) where the objective is to maximize a function called gain of system (GoS) defined based on provided QoS and priority of the tasks. We prove the problem is NP-hard. We then propose an efficient offloading algorithm, called PGUS, that is shown to achieve near-optimal results in terms of the provided GoS. Finally, we compare our proposed algorithm, PGUS, with heuristics and a state-of-the-art algorithm, called GUS, using both numerical analysis and real-world implementation. Our results show that PGUS outperforms GUS by a factor of 45% in average in terms of serving the top 25% higher priority classes of the tasks while still keeping the overall percentage of the dropped tasks minimal and the overall gain of system maximized.",smart cities
10.1109/jiot.2021.3100068,preprocessed,IEEE Internet of Things Journal,IEEE,2022-03-01 00:00:00,ieeexplore,astcn: an attentive spatial–temporal convolutional network for flow prediction,https://ieeexplore.ieee.org/document/9511315/,"Flow prediction attracts intensive research interests, since it can offer essential support to many crucial problems in public safety and smart city, e.g., epidemic spread prediction and medical resource allocation optimization. Among all the models in flow prediction, deep learning models (e.g., convolutional neural networks, recurrent neural networks, and graph neural networks) are popular and outperform other statistics and machine learning models, since they can learn intrinsic structures and extract features from spatial–temporal (ST) data. However, most of them set strict temporal periods in the prediction or separate the interaction between spatial and temporal correlations. Therefore, the prediction accuracy is affected. To overcome the difficulties, we propose a flow prediction network attentive spatial–temporal convolutional network (ASTCN), which can effectively handle large-scale flow data and learn complex features. In ASTCN, we leverage an attention mechanism to overcome the previous problem of strict temporal periods, and can effectively fuse ST data with multiple factors from different time-series sources. Furthermore, we propose a causal 3-D convolutional layer based on temporal convolutional networks (TCNs). It can simultaneously extract both spatial and temporal features to improve the prediction accuracy. We comprehensively conducted our experiments based on real-world data sets. Experimental results show that ASTCN outperforms the state-of-the-art methods by at least 3.78% in root mean square error. Therefore, ASTCN is a potential solution to other large-scale ST problems.",smart cities
10.1109/jiot.2021.3097768,preprocessed,IEEE Internet of Things Journal,IEEE,2022-03-01 00:00:00,ieeexplore,user-aware and flexible proactive caching using lstm and ensemble learning in iot-mec networks,https://ieeexplore.ieee.org/document/9488291/,"To meet the stringent demands of emerging Internet-of-Things (IoT) applications, such as smart home, smart city, and virtual reality in 5G/6G IoT networks, edge content caching for mobile/multiaccess edge computing (MEC) has been identified as a promising approach to improve the quality of services in terms of latency and energy consumption. However, the limitations of cache capacity make it difficult to develop an effective common caching framework that satisfies diverse user preferences. In this article, we propose a new content caching strategy that maximizes the cache hit ratio through flexible prediction in dynamically changing network and user environments. It is based on a hierarchical deep learning architecture: long short-term memory (LSTM)-based local learning and ensemble-based meta-learning. First, as a local learning model, we employ an LSTM method with seasonal-trend decomposition using loess (STL)-based preprocessing. It identifies the attributes for demand prediction on the contents in various demographic user groups. Second, as a metalearning model, we employ a regression-based ensemble learning method, which uses an online convex optimization framework and exhibits sublinear “regret” performance. It orchestrates the obtained multiple demographic user preferences into a unified caching strategy in real time. Extensive experiments were conducted on the popular MovieLens data sets. It was shown that the proposed control provides up to a 30% higher cache hit ratio than conventional representative algorithms and a near-optimal cache hit ratio within approximately 9% of the optimal caching scheme with perfect prior knowledge of content popularity. The proposed learning and caching control can be implemented as a core function of the 5G/6G standard’s network data analytic function (NWDAF) module.",smart cities
10.1109/tnse.2021.3050781,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,vfchain: enabling verifiable and auditable federated learning via blockchain systems,https://ieeexplore.ieee.org/document/9321132/,"Advanced artificial intelligence techniques, such as federated learning, has been applied to broad areas, e.g., image classification, speech recognition, smart city, and healthcare. Despite intensive research on federated learning, existing schemes are vulnerable to attacks and can hardly meet the security requirements for real-world applications. The problem of designing a secure federated learning framework to ensure the correctness of training procedure has not been sufficiently studied and remains open. In this paper, we propose VFChain, a verifiable and auditable federated learning framework based on the blockchain system. First, to provide the verifiability, a committee is selected through the blockchain to collectively aggregate models and record verifiable proofs in the blockchain. Then, to provide the auditability, a novel authenticated data structure is proposed for blockchain to improve the search efficiency of verifiable proofs and support a secure rotation of committee. Finally, to further improve the search efficiency, an optimization scheme is proposed to support multiple-model learning tasks. We implement VFChain and conduct extensive experiments by utilizing the popular deep learning models over the public real-world dataset. The evaluation results demonstrate the effectiveness of our proposed VFChain system.",smart cities
10.1109/tkde.2020.2985954,preprocessed,IEEE Transactions on Knowledge and Data Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,incorporating multi-source urban data for personalized and context-aware multi-modal transportation recommendation,https://ieeexplore.ieee.org/document/9063461/,"Transportation recommendation is one important map service in navigation applications. Previous transportation recommendation solutions fail to deliver satisfactory user experience because their recommendations only consider routes in one transportation mode (uni-modal, e.g., taxi, bus, cycle) and largely overlook situational context. In this work, we propose <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq1-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>, a multi-task deep learning based recommendation system that offers multi-modal transportation planning and is adaptive to various situational context (e.g., nearby point-of-interest (POI) distribution and weather). We leverage the availability of existing routing engines and big urban data, and design a novel two-level framework that integrates uni-modal and multi-modal (e.g., taxi-bus, bus-cycle) routes as well as heterogeneous urban data for intelligent multi-modal transportation recommendation. In addition to urban context features constructed from multi-source urban data, we learn the latent representations of users, origin-destination (OD) pairs and transportation modes based on user implicit feedbacks, which captures the collaborative transportation mode preferences of users and OD pairs. Moreover, we propose two models to recommend the proper route among various uni-modal and multi-modal transportation routes: (1) a light-weight gradient boosting decision tree (GBDT) based recommendation model; and (2) a multi-task wide and deep learning (MTWDL) based recommendation model. We also optimize the framework to support real-time, large-scale route query and recommendation. We deploy <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq2-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> on Baidu Maps,<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref><fn id=""fn1""><label>1.</label><p><uri>https://maps.baidu.com/</uri>.</p> </fn> one of the world's largest map services. Real-world urban-scale experiments demonstrate the effectiveness and efficiency of our proposed system. Since its deployment in August 2018, <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq3-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> has answered over a hundred million route recommendation queries made by over ten million distinct users. The GBDT based model and MTWDL based model achieve 82.8 and 96.6 percent relative improvement of user click ratio, respectively.",smart cities
10.1109/scset55041.2022.00053,preprocessed,2022 International Seminar on Computer Science and Engineering Technology (SCSET),IEEE,2022-01-09 00:00:00,ieeexplore,research on leak location method of water supply network based on deep neural network model,https://ieeexplore.ieee.org/document/9700905/,"The water supply network is one of the important infrastructure in urban construction. It has strong theoretical and practical significance to realize the real-time monitoring and leak location of the water supply network. In this paper, based on the similarity of water supply network node pressure, fuzzy C-means clustering algorithm is used to realize the selection of finite monitoring points. On this basis, a depth neural network model is constructed according to the pressure changes of the monitoring points before and after the leakage of the water supply network, so as to locate the leakage points. In the experimental part, hydraulics simulation was conducted by using EPANETH pipe network adjustment software according to the layout structure of water supply network, and the pressure of all nodes was obtained. A deep neural network model was established by Keras in Tensorflow framework. After model training and testing, the training error was controlled within the effective range of 5 %. Finally, the model is applied to the actual leakage problem of underground water supply network in Langxi County of Xuancheng City, and the accurate location of the leakage point is realized. The experiment proves the feasibility and accuracy of the method proposed in this paper.",smart cities
10.1109/tgrs.2021.3056624,preprocessed,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,attention-based multiscale residual adaptation network for cross-scene classification,https://ieeexplore.ieee.org/document/9377566/,"In recent years, classification has obtained ever-rising attention and has been applied to many areas in the field of remote sensing, including land use, forest monitoring, urban planning, and vegetation management. Due to the lack of labeled data and the poor generalization ability of supervised models, cross-scene classification is proposed for better utilization of the existing knowledge. Existing adaptation methods for cross-scene classification only consider the marginal distribution, while the conditional distribution is equally important in real applications. In addition, approaches based on deep learning align the distribution of features extracted from a single-scale structure, leading to the loss of information. To overcome the above drawbacks, an Attention-based Multiscale Residual Adaptation Network (AMRAN) is proposed for cross-scene classification tasks. In the proposed AMRAN, both the marginal and conditional distributions are taken into consideration for more comprehensive alignment. Besides, the attention mechanism and the multiscale strategy are used to extract more robust features and more complete information, respectively. Experimental results between four existing scene classification data sets demonstrate that AMRAN has a significant improvement compared with the state-of-the-art deep adaptation methods.",smart cities
10.1109/access.2021.3137031,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,autonomous detection and deterrence of pigeons on buildings by drones,https://ieeexplore.ieee.org/document/9656717/,"Pigeons may transmit diseases to humans and cause damages to buildings, monuments, and other infrastructure. Therefore, several control strategies have been developed, but they have been found to be either ineffective or harmful to animals and often depend on human operation. This study proposes a system capable of autonomously detecting and deterring pigeons on building roofs using a drone. The presence and position of pigeons were detected in real time by a neural network using images taken by a video camera located on the roof. Moreover, a drone was utilized to deter the animals. Field experiments were conducted in a real-world urban setting to assess the proposed system by comparing the number of animals and their stay durations for over five days against the 21-day-trial experiment without the drone. During the five days of experiments, the drone was automatically deployed 55 times and was significantly effective in reducing the number of birds and their stay durations without causing any harm to them. In conclusion, this study has proven the effectiveness of this system in deterring birds, and this approach can be seen as a fully autonomous alternative to the already existing methods.",smart cities
10.1109/lgrs.2020.3030839,preprocessed,IEEE Geoscience and Remote Sensing Letters,IEEE,2000-01-01 00:00:00,ieeexplore,hybrid attention networks for flow and pressure forecasting in water distribution systems,https://ieeexplore.ieee.org/document/9241394/,"Multivariate geo-sensory time series prediction is challenging because of the complex spatial and temporal correlations. In urban water distribution systems (WDSs), numerous spatial-correlated sensors have been deployed to continuously collect hydraulic data. Forecasts of the monitored flow and pressure time series are of vital importance for operational decision making, alerts, and anomaly detection. To address this issue, we proposed a hybrid dual-stage spatial–temporal attention-based recurrent neural networks (hDS-RNN). Our model consists of two stages: a spatial attention-based encoder and a temporal attention-based decoder. Specifically, a hybrid spatial attention mechanism that employs inputs along the temporal and spatial axes is proposed. Experiments on a real-world data set are conducted, which demonstrate that our model outperformed seven baseline models in flow and pressure predictions in WDS.",smart cities
10.1109/jstars.2022.3142898,preprocessed,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,wh-mavs: a novel dataset and deep learning benchmark for multiple land use and land cover applications,https://ieeexplore.ieee.org/document/9681304/,"Over the past decade, many excellent data sharing efforts have enriched the remote sensing scene classification (SC) methods. These datasets have achieved great success in complex high-level semantic information interpretation. However, most existing datasets are collected from standard and ungeoreferenced image patches for algorithm training and evaluation. These datasets do not fit for practical applications and cannot be directly applied in further geographical study. Accordingly, we provide a large range high-resolution SC dataset with multiple time phases, called “<bold>W</bold>u<bold>h</bold>an <bold>M</bold>ulti<bold>a</bold>pplication <bold>V</bold>HR <bold>S</bold>cene classification dataset (WH-MAVS).” It facilitates the study of SC and scene change detection (SCD) algorithms. Moreover, it can also be directly employed to perform a variety of real-life land use application tasks. To the best of our knowledge, this is the first free, publicly available, georeferenced, and annotated dataset to cover almost an entire megacity. The WH-MAVS was collected and annotated from Google Earth imagery with the same spatial resolution and uniform nonoverlapping patch size, covering the central area of Wuhan, China. The total number of scene samples is 47 137, which belong to 14 classes with 23 567 labeled patches for each time phase in 2014 and 2016, respectively. The geographic coordinates of all samples in both time phases exhibit one-to-one correspondence with 23 202 unchanged image patches of scene categories and 365 changed ones. The distribution of the number of samples in each class is highly imbalanced; moreover, there are large intraclass differences and indistinguishable interclass variances. These characteristics are closer to the real land use/land cover application tasks and introduce further challenges to the related algorithm research. In addition, we conducted benchmark experiments on SC and SCD based on the WH-MAVS dataset with widely used deep learning models. DenseNet169 was found to achieve the best performance. The overall accuracies are 91.07% and 92.09%, respectively, in the 2014 and 2016 validation sets of WH-MAVS. Furthermore, SCD obtained by DenseNet169 has a binary change detection accuracy of 89.56% and a multiple (from–to) change detection accuracy of 86.70%. Over and above the research value of the algorithm, it is also proven to have practical applications in fields such as urban planning, landscape pattern analysis, and urban dynamic monitoring and analysis.",smart cities
10.1109/ccnc49033.2022.9700579,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,demo: an experimental environment based on mini-pcs for federated learning research,https://ieeexplore.ieee.org/document/9700579/,"There is a growing research interest in Federated Learning (FL), a promising approach for data privacy preservation and proximity of training to the network edge, where data is generated. Resource consumption for Machine Learning (ML) training and inference is important for edge nodes, but most of the proposed protocols and algorithms for FL are evaluated by simulations. In this demo paper, we present an environment based on distributed mini-PCs to enable experimental study of FL protocols and algorithms. We have installed low-capacity mini-PCs within a wireless city-level mesh network and deployed container-based FL components on these nodes. We show the deployed FL clients and server at different nodes in the city and demonstrate how an FL experiment can be set and run in a real environment.",smart cities
10.1109/wacv51458.2022.00308,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,multi-branch neural networks for video anomaly detection in adverse lighting and weather conditions,https://ieeexplore.ieee.org/document/9706717/,"Automated anomaly detection in surveillance videos has attracted much interest as it provides a scalable alternative to manual monitoring. Most existing approaches achieve good performance on clean benchmark datasets recorded in well-controlled environments. However, detecting anomalies is much more challenging in the real world. Adverse weather conditions like rain or changing brightness levels cause a significant shift in the input data distribution, which in turn can lead to the detector model incorrectly reporting high anomaly scores. Additionally, surveillance cameras are usually deployed in evolving environments such as a city street of which the appearance changes over time because of seasonal changes or roadworks. The anomaly detection model will need to be updated periodically to deal with these issues. In this paper, we introduce a multi-branch model that is equipped with a trainable preprocessing step and multiple identical branches for detecting anomalies during day and night as well as in sunny and rainy conditions. We experimentally validate our approach on a distorted version of the Avenue dataset and provide qualitative results on real-world surveillance camera data. Experimental results show that our method outperforms the existing methods in terms of detection accuracy while being faster and more robust on scenes with varying visibility.",smart cities
10.1109/tii.2021.3091597,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,optimal sizing and efficient routing of electric vehicles for a vehicle-on-demand system,https://ieeexplore.ieee.org/document/9462468/,"Due to the steep rise in global population, urbanization, and industrialization, most of the cities in the world today are witnessing increased carbon footprints and reduced per capita space. In such a scenario, vehicle sharing and carpooling systems, specifically with electric vehicles (EV), can significantly help due to the reduced cost of ownership, maintenance, and parking space. In this article, we study the challenging problem of optimal sizing and efficient routing for an electric vehicle-on-demand system. Users demand EVs at the pooling stations at different time instances with individual deadlines to reach the destinations. The objective is to fulfill all the demands respecting the deadlines with minimum investment, which essentially translates to minimizing the total number of EVs. We define the problem formally using mixed-integer linear programming formulation and propose a set of intelligent and efficient heuristic algorithms to solve it efficiently. The proposed algorithms’ performances are tested and validated in a simulated environment on a reasonable size city network with many EV demands. The results obtained show that the proposed heuristic algorithms are competent by reducing 200–360 EVs per day on a network of 282 charging ports, indicating their scalability to be implemented in real-world scenarios.",smart cities
10.1109/tits.2020.3029537,preprocessed,IEEE Transactions on Intelligent Transportation Systems,IEEE,2022-02-01 00:00:00,ieeexplore,spatial positioning token (sptoken) for smart mobility,https://ieeexplore.ieee.org/document/9238413/,"We introduce a permissioned distributed ledger technology (DLT) design for crowdsourced smart mobility applications. This architecture is based on a directed acyclic graph architecture (similar to the IOTA tangle) and uses both Proof-of-Work and Proof-of-Position mechanisms to provide protection against spam attacks and malevolent actors. In addition to enabling individuals to retain ownership of their data and to monetize it, the architecture is also suitable for distributed privacy-preserving machine learning algorithms, is lightweight, and can be implemented in simple internet-of-things (IoT) devices. To demonstrate its efficacy, we apply this framework to reinforcement learning settings where a third party is interested in acquiring information from agents. In particular, one may be interested in sampling an unknown vehicular traffic flow in a city, using a DLT-type architecture and without perturbing the density, with the idea of realizing a set of virtual tokens as surrogates of real vehicles to explore geographical areas of interest. These tokens, whose authenticated position determines write access to the ledger, are thus used to emulate the probing actions of commanded (real) vehicles on a given planned route by “jumping” from a passing-by vehicle to another to complete the planned trajectory. Consequently, the environment stays unaffected (i.e., the autonomy of participating vehicles is not influenced by the algorithm), regardless of the number of emitted tokens. The design of such a DLT architecture is presented, and numerical results from large-scale simulations are provided to validate the proposed approach.",smart cities
10.1109/tits.2020.3015542,preprocessed,IEEE Transactions on Intelligent Transportation Systems,IEEE,2022-02-01 00:00:00,ieeexplore,taxi demand prediction using parallel multi-task learning model,https://ieeexplore.ieee.org/document/9172100/,"Accurate and real-time taxi demand prediction can help managers pre-allocate taxi resources in cities, which assists drivers quickly finding passengers and reduce passengers’ waiting time. Most of the existing studies focus on mining spatial-temporal characteristics of taxi demand distributions, while lacking in modeling the correlations between taxi pick-up demand and the drop-off demand from the perspective of multi-task learning. In this article, we propose a multi-task learning model containing three parallel LSTM layers to co-predict taxi pick-up and drop-off demands, and compare the performance of single demand prediction methodology and that of two demands’ co-prediction methodology. Experimental results on real-world datasets demonstrate that the pick-up demand and the drop-off demand do depend on each other, and the effectiveness of the proposed co-prediction methods.",smart cities
10.1109/tii.2021.3071771,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,a utility-based subcontract method for sensing task in mobile crowd sensing,https://ieeexplore.ieee.org/document/9399247/,"In mobile crowd sensing, the mobile terminal integrates a variety of widely distributed sensing devices and communication ports. Sensing devices and communication ports can collect and share all kinds of perception data. However, inherent contradictions exist among perceived ability, communication port, and moving rule while collecting real-time and accurate sensing information. This article mainly focused on recruited and selected mobile nodes and assigned sensing tasks to improve the quality of sensing information. The optimization of the implementation stage of the sensing task is beyond the scope of this study. This article proposes a utility-based sensing task decomposition and subcontract algorithm, which is a method of sensing data acquisition that establishes direct collaboration between mobile nodes. A mobility model based on Markov chain is established to forecast the spatial distribution of sensing nodes. A utility function is designed to estimate the sensing task execution capacity of sensing nodes based on spatial distribution and tempo-spatial coverage of the collected data. The sensing tasks are then decomposed and subcontracted to neighboring nodes according to the utilities of the neighboring nodes to the decomposed sensing tasks. This method improves the quality of sensing data in terms of sensing data coverage and finished ratio of sensing task.",smart cities
10.1109/access.2022.3146728,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,assistive devices analysis for visually impaired persons: a review on taxonomy,https://ieeexplore.ieee.org/document/9693966/,"Visually impaired persons (VIPs) comprise a significant portion of the population and they are present in all corners of the world. In recent times, the technology proved its presence in every domain of life and innovative devices are assisting humans in all fields especially, artificial intelligence has dominated and outperformed the rest of the trades. VIPs need assistance in performing daily life tasks like object/obstacle detection and recognition, navigation, and mobility, particularly in indoor and outdoor environments. Moreover, the protection and safety of these people are of prime concern. Several devices and applications have been developed for the assistance of VIPs. Firstly, these devices take input from the surrounding environment through different sensors e.g. infrared radiation, ultrasonic, imagery sensor, etc. In the second stage, state of the art machine learning techniques process these signals and extract useful information. Finally, feedback is provided to the user through auditory and/or vibratory means. It is observed that most of the existing devices are constrained in their abilities. The paper presents a comprehensive comparative analysis of the state-of-the-art assistive devices for VIPs. These techniques are categorized based on their functionality and working principles. The main attributes, challenges, and limitations of these techniques have also been highlighted. Moreover, a score-based quantitative analysis of these devices is performed to highlight their feature enrichment capability for each category. It may help to select an appropriate device for a particular scenario.",smart cities
10.1109/tnse.2021.3072911,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,prioritized content determination and dissemination using reinforcement learning in dtns,https://ieeexplore.ieee.org/document/9403937/,"In a battlefield, several groups of soldiers are deployed with different mission goals by the command and control center (CC). To continue the missions appropriately and get a better understanding of the situation, the soldiers, as well as the CC, need to collect information of interest generated in different battle zones. However, due to the damaged network infrastructure in the hostile areas, it is a challenge to determine the topics of interest associated with the events and missions, and efficiently forward the associated content to the CC. Hence, the devices of the soldiers (nodes) generate, store and forward content hop by hop using a Delay Tolerant Network (DTN). While forwarding content, nodes avoid congestion so that meaningful content related to prioritized mission goals can be disseminated. In this dynamic surrounding, any sudden but important event-related content should also be sent to the CC with the help of intermediate nodes regardless of their own mission interests. We design a scheme to forward contents generated by the nodes to the CC using Reinforcement Learning (RL) while maximizing the number of interesting data in the respective nodes' buffer, and avoiding congestion. In this forwarding process, we focus on identifying the trending topics/keywords among changing missions and their related data at the node level, and the changes of interest of the nodes based on their mobility and connectivity patterns. Experiments are conducted using real datasets and ONE simulator to show the effectiveness of Reinforcement Learning (RL) on the prioritized content dissemination in a DTN.",smart cities
10.1109/ojits.2021.3139393,preprocessed,IEEE Open Journal of Intelligent Transportation Systems,IEEE,2000-01-01 00:00:00,ieeexplore,napc: a neural algorithm for automated passenger counting in public transport on a privacy-friendly dataset,https://ieeexplore.ieee.org/document/9665722/,"Real-time load information in public transport is of high importance for both passengers and service providers. Neural algorithms have shown a high performance on various object counting tasks and play a continually growing methodological role in developing automated passenger counting systems. However, the publication of public-space video footage is often contradicted by legal and ethical considerations to protect the passengers’ privacy. This work proposes an end-to-end Long Short-Term Memory network with a problem-adapted cost function that learned to count boarding and alighting passengers on a publicly available, comprehensive dataset of approx.13,000 manually annotated low-resolution 3D LiDAR video recordings (depth information only) from the doorways of a regional train. These depth recordings do not allow the identification of single individuals. For each door opening phase, the trained models predict the correct passenger count (ranging from 0 to 67) in approx.96% of boarding and alighting, respectively. Repeated training with different training and validation sets confirms the independence of this result from a specific test set.",smart cities
10.1109/jsen.2021.3132460,preprocessed,IEEE Sensors Journal,IEEE,2001-02-01 20:22:00,ieeexplore,automatic rail component detection based on attnconv-net,https://ieeexplore.ieee.org/document/9634063/,"The automatic detection of major rail components using railway images is beneficial to ensure the rail transport safety. In this paper, we propose an attention-powered deep convolutional network (AttnConv-net) to detect multiple rail components including the rail, clips, and bolts. The proposed method consists of a deep convolutional neural network (DCNN) as the backbone, cascading attention blocks (CAB), and two feed forward networks (FFN). Two types of positional embedding are applied to enrich information in latent features extracted from the backbone. Based on processed latent features, the CAB aims to learn the local context of rail components including their categories and component boundaries. Final categories and bounding boxes are generated via two FFN implemented in parallel. To enhance the detection of small components, various data augmentation methods are employed in training process. The effectiveness of the proposed AttnConv-net is validated with one real dataset and another synthesized dataset. Compared with classic convolutional neural network based methods, our proposed method simplifies the detection pipeline by eliminating the need of prior- and post-processing, which offers a new speed-quality solution to enable faster and more accurate image-based rail component detections.",smart cities
http://arxiv.org/abs/2202.08982v1,preprocessed,arxiv,arxiv,2022-02-18 00:00:00,arxiv,"pgcn: progressive graph convolutional networks for spatial-temporal
  traffic forecasting",http://arxiv.org/abs/2202.08982v1,"The complex spatial-temporal correlations in transportation networks make the
traffic forecasting problem challenging. Since transportation system inherently
possesses graph structures, much research efforts have been put with graph
neural networks. Recently, constructing adaptive graphs to the data has shown
promising results over the models relying on a single static graph structure.
However, the graph adaptations are applied during the training phases, and do
not reflect the data used during the testing phases. Such shortcomings can be
problematic especially in traffic forecasting since the traffic data often
suffers from the unexpected changes and irregularities in the time series. In
this study, we propose a novel traffic forecasting framework called Progressive
Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by
progressively adapting to input data during the training and the testing
phases. Specifically, we implemented the model to construct progressive
adjacency matrices by learning trend similarities among graph nodes. Then, the
model is combined with the dilated causal convolution and gated activation unit
to extract temporal features. With residual and skip connections, PGCN performs
the traffic prediction. When applied to four real-world traffic datasets of
diverse geometric nature, the proposed model achieves state-of-the-art
performance with consistency in all datasets. We conclude that the ability of
PGCN to progressively adapt to input data enables the model to generalize in
different study sites with robustness.",smart cities
http://arxiv.org/abs/2202.07147v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,"graph meta-reinforcement learning for transferable autonomous
  mobility-on-demand",http://arxiv.org/abs/2202.07147v1,"Autonomous Mobility-on-Demand (AMoD) systems represent an attractive
alternative to existing transportation paradigms, currently challenged by
urbanization and increasing travel needs. By centrally controlling a fleet of
self-driving vehicles, these systems provide mobility service to customers and
are currently starting to be deployed in a number of cities around the world.
Current learning-based approaches for controlling AMoD systems are limited to
the single-city scenario, whereby the service operator is allowed to take an
unlimited amount of operational decisions within the same transportation
system. However, real-world system operators can hardly afford to fully
re-train AMoD controllers for every city they operate in, as this could result
in a high number of poor-quality decisions during training, making the
single-city strategy a potentially impractical solution. To address these
limitations, we propose to formalize the multi-city AMoD problem through the
lens of meta-reinforcement learning (meta-RL) and devise an actor-critic
algorithm based on recurrent graph neural networks. In our approach, AMoD
controllers are explicitly trained such that a small amount of experience
within a new city will produce good system performance. Empirically, we show
how control policies learned through meta-RL are able to achieve near-optimal
performance on unseen cities by learning rapidly adaptable policies, thus
making them more robust not only to novel environments, but also to
distribution shifts common in real-world operations, such as special events,
unexpected congestion, and dynamic pricing schemes.",smart cities
http://arxiv.org/abs/2202.06639v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"on the complexity of object detection on real-world public
  transportation images for social distancing measurement",http://arxiv.org/abs/2202.06639v1,"Social distancing in public spaces has become an essential aspect in helping
to reduce the impact of the COVID-19 pandemic. Exploiting recent advances in
machine learning, there have been many studies in the literature implementing
social distancing via object detection through the use of surveillance cameras
in public spaces. However, to date, there has been no study of social distance
measurement on public transport. The public transport setting has some unique
challenges, including some low-resolution images and camera locations that can
lead to the partial occlusion of passengers, which make it challenging to
perform accurate detection. Thus, in this paper, we investigate the challenges
of performing accurate social distance measurement on public transportation. We
benchmark several state-of-the-art object detection algorithms using real-world
footage taken from the London Underground and bus network. The work highlights
the complexity of performing social distancing measurement on images from
current public transportation onboard cameras. Further, exploiting domain
knowledge of expected passenger behaviour, we attempt to improve the quality of
the detections using various strategies and show improvement over using vanilla
object detection alone.",smart cities
http://arxiv.org/abs/2202.06608v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"unscene: toward unsupervised scenario extraction for automated driving
  systems from urban naturalistic road traffic data",http://arxiv.org/abs/2202.06608v1,"Scenario-based testing is a promising approach to solve the challenge of
proving the safe behavior of vehicles equipped with automated driving systems
(ADS). Since an infinite number of concrete scenarios can theoretically occur
in real-world road traffic, the extraction of relevant scenarios that are
sensitive regarding the safety-related behavior of ADS-equipped vehicles is a
key aspect for the successful verification and validation of these systems.
Therefore, this paper provides a method for extracting multimodal urban traffic
scenarios from naturalistic road traffic data in an unsupervised manner for
minimizing the amount of (potentially biased) prior expert knowledge needed.
Rather than an (expensive) rule-based assignment by extracting concrete
scenarios into predefined functional scenarios, the presented method deploys an
unsupervised machine learning pipeline. It includes principal feature analysis,
feature extraction with so-called scenario grids, dimensionality reduction by
principal component analysis, scenario clustering as well as cluster
validation. The approach allows exploring the unknown natures of the data and
interpreting them as scenarios that experts could not have anticipated. The
method is demonstrated and evaluated for naturalistic road traffic data at
urban intersections from the inD and the Silicon Valley dataset. The findings
encourage the use of this type of data as well as unsupervised machine learning
approaches as important pillar for a systematic construction of a relevant
scenario database with sufficient coverage for testing ADS.",smart cities
http://arxiv.org/abs/2202.05334v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"learning the pedestrian-vehicle interaction for pedestrian trajectory
  prediction",http://arxiv.org/abs/2202.05334v1,"In this paper, we study the interaction between pedestrians and vehicles and
propose a novel neural network structure called the Pedestrian-Vehicle
Interaction (PVI) extractor for learning the pedestrian-vehicle interaction. We
implement the proposed PVI extractor on both sequential approaches (long
short-term memory (LSTM) models) and non-sequential approaches (convolutional
models). We use the Waymo Open Dataset that contains real-world urban traffic
scenes with both pedestrian and vehicle annotations. For the LSTM-based models,
our proposed model is compared with Social-LSTM and Social-GAN, and using our
proposed PVI extractor reduces the average displacement error (ADE) and the
final displacement error (FDE) by 7.46% and 5.24%, respectively. For the
convolutional-based models, our proposed model is compared with Social-STGCNN
and Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and
FDE by 2.10% and 1.27%, respectively. The results show that the
pedestrian-vehicle interaction influences pedestrian behavior, and the models
using the proposed PVI extractor can capture the interaction between
pedestrians and vehicles, and thereby outperform the compared methods.",smart cities
http://arxiv.org/abs/2202.05118v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"reinforcement learning in the wild: scalable rl dispatching algorithm
  deployed in ridehailing marketplace",http://arxiv.org/abs/2202.05118v1,"In this study, a real-time dispatching algorithm based on reinforcement
learning is proposed and for the first time, is deployed in large scale.
Current dispatching methods in ridehailing platforms are dominantly based on
myopic or rule-based non-myopic approaches. Reinforcement learning enables
dispatching policies that are informed of historical data and able to employ
the learned information to optimize returns of expected future trajectories.
Previous studies in this field yielded promising results, yet have left room
for further improvements in terms of performance gain, self-dependency,
transferability, and scalable deployment mechanisms. The present study proposes
a standalone RL-based dispatching solution that is equipped with multiple
mechanisms to ensure robust and efficient on-policy learning and inference
while being adaptable for full-scale deployment. A new form of value updating
based on temporal difference is proposed that is more adapted to the inherent
uncertainty of the problem. For the driver-order assignment, a customized
utility function is proposed that when tuned based on the statistics of the
market, results in remarkable performance improvement and interpretability. In
addition, for reducing the risk of cancellation after drivers' assignment, an
adaptive graph pruning strategy based on the multi-arm bandit problem is
introduced. The method is evaluated using offline simulation with real data and
yields notable performance improvement. In addition, the algorithm is deployed
online in multiple cities under DiDi's operation for A/B testing and is
launched in one of the major international markets as the primary mode of
dispatch. The deployed algorithm shows over 1.3% improvement in total driver
income from A/B testing. In addition, by causal inference analysis, as much as
5.3% improvement in major performance metrics is detected after full-scale
deployment.",smart cities
http://arxiv.org/abs/2202.04628v2,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,"reinforcement learning with sparse rewards using guidance from offline
  demonstration",http://arxiv.org/abs/2202.04628v2,"A major challenge in real-world reinforcement learning (RL) is the sparsity
of reward feedback. Often, what is available is an intuitive but sparse reward
function that only indicates whether the task is completed partially or fully.
However, the lack of carefully designed, fine grain feedback implies that most
existing RL algorithms fail to learn an acceptable policy in a reasonable time
frame. This is because of the large number of exploration actions that the
policy has to perform before it gets any useful feedback that it can learn
from. In this work, we address this challenging problem by developing an
algorithm that exploits the offline demonstration data generated by a
sub-optimal behavior policy for faster and efficient online RL in such sparse
reward settings. The proposed algorithm, which we call the Learning Online with
Guidance Offline (LOGO) algorithm, merges a policy improvement step with an
additional policy guidance step by using the offline demonstration data. The
key idea is that by obtaining guidance from - not imitating - the offline data,
LOGO orients its policy in the manner of the sub-optimal policy, while yet
being able to learn beyond and approach optimality. We provide a theoretical
analysis of our algorithm, and provide a lower bound on the performance
improvement in each learning episode. We also extend our algorithm to the even
more challenging incomplete observation setting, where the demonstration data
contains only a censored version of the true state observation. We demonstrate
the superior performance of our algorithm over state-of-the-art approaches on a
number of benchmark environments with sparse rewards and censored state.
Further, we demonstrate the value of our approach via implementing LOGO on a
mobile robot for trajectory tracking and obstacle avoidance, where it shows
excellent performance.",smart cities
http://arxiv.org/abs/2202.03917v1,preprocessed,arxiv,arxiv,2022-02-08 00:00:00,arxiv,edge-based fever screening system over private 5g,http://arxiv.org/abs/2202.03917v1,"Edge computing and 5G have made it possible to perform analytics closer to
the source of data and achieve super-low latency response times, which is not
possible with centralized cloud deployment. In this paper, we present a novel
fever-screening system, which uses edge machine learning techniques and
leverages private 5G to accurately identify and screen individuals with fever
in real-time. Particularly, we present deep-learning based novel techniques for
fusion and alignment of cross-spectral visual and thermal data streams at the
edge. Our novel Cross-Spectral Generative Adversarial Network (CS-GAN)
synthesizes visual images that have the key, representative object level
features required to uniquely associate objects across visual and thermal
spectrum. Two key features of CS-GAN are a novel, feature-preserving loss
function that results in high-quality pairing of corresponding cross-spectral
objects, and dual bottleneck residual layers with skip connections (a new,
network enhancement) to not only accelerate real-time inference, but to also
speed up convergence during model training at the edge. To the best of our
knowledge, this is the first technique that leverages 5G networks and limited
edge resources to enable real-time feature-level association of objects in
visual and thermal streams (30 ms per full HD frame on an Intel Core i7-8650
4-core, 1.9GHz mobile processor). To the best of our knowledge, this is also
the first system to achieve real-time operation, which has enabled fever
screening of employees and guests in arenas, theme parks, airports and other
critical facilities. By leveraging edge computing and 5G, our fever screening
system is able to achieve 98.5% accuracy and is able to process about 5X more
people when compared to a centralized cloud deployment.",smart cities
http://arxiv.org/abs/2202.02653v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,"millisecond speed deep learning based proton dose calculation with monte
  carlo accuracy",http://arxiv.org/abs/2202.02653v1,"Next generation online and real-time adaptive radiotherapy workflows require
precise particle transport simulations in sub-second times, which is unfeasible
with current analytical pencil beam algorithms (PBA) or stochastic Monte Carlo
(MC) methods. We present a data-driven millisecond speed dose calculation
algorithm (DoTA) accurately predicting the dose deposited by mono-energetic
proton pencil beams for arbitrary energies and patient geometries. Given the
forward-scattering nature of protons, we frame 3D particle transport as
modeling a sequence of 2D geometries in the beam's eye view. DoTA combines
convolutional neural networks extracting spatial features (e.g., tissue and
density contrasts) with a transformer self-attention backbone that routes
information between the sequence of geometry slices and a vector representing
the beam's energy, and is trained to predict low noise MC simulations of proton
beamlets using 80,000 different head and neck, lung, and prostate geometries.
Predicting beamlet doses in 5 ms with a very high gamma pass rate of 99.37%
(1%, 3 mm) compared to the ground truth MC calculations, DoTA significantly
improves upon analytical pencil beam algorithms both in precision and speed.
Offering MC accuracy 100 times faster than PBAs for pencil beams, our model
calculates full treatment plan doses in 10 to 15 s depending on the number of
beamlets, achieving a 99.70% (2%, 2 mm) gamma pass rate across 9 test patients.
Outperforming all previous analytical pencil beam and deep learning based
approaches, DoTA represents a new state of the art in data-driven dose
calculation and can directly compete with the speed of even commercial GPU MC
approaches. Providing the sub-second speed required for adaptive treatments,
straightforward implementations could offer similar benefits to other steps of
the radiotherapy workflow or other modalities such as helium or carbon
treatments.",smart cities
http://arxiv.org/abs/2202.01862v1,preprocessed,arxiv,arxiv,2022-02-03 00:00:00,arxiv,practical imitation learning in the real world via task consistency loss,http://arxiv.org/abs/2202.01862v1,"Recent work in visual end-to-end learning for robotics has shown the promise
of imitation learning across a variety of tasks. Such approaches are expensive
both because they require large amounts of real world training demonstrations
and because identifying the best model to deploy in the real world requires
time-consuming real-world evaluations. These challenges can be mitigated by
simulation: by supplementing real world data with simulated demonstrations and
using simulated evaluations to identify high performing policies. However, this
introduces the well-known ""reality gap"" problem, where simulator inaccuracies
decorrelate performance in simulation from that of reality. In this paper, we
build on top of prior work in GAN-based domain adaptation and introduce the
notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages
sim and real alignment both at the feature and action-prediction levels. We
demonstrate the effectiveness of our approach by teaching a mobile manipulator
to autonomously approach a door, turn the handle to open the door, and enter
the room. The policy performs control from RGB and depth images and generalizes
to doors not encountered in training data. We achieve 80% success across ten
seen and unseen scenes using only ~16.2 hours of teleoperated demonstrations in
sim and real. To the best of our knowledge, this is the first work to tackle
latched door opening from a purely end-to-end learning approach, where the task
of navigation and manipulation are jointly modeled by a single neural network.",smart cities
http://arxiv.org/abs/2201.09419v1,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,"automated machine learning for secure key rate in discrete-modulated
  continuous-variable quantum key distribution",http://arxiv.org/abs/2201.09419v1,"Continuous-variable quantum key distribution (CV QKD) with discrete
modulation has attracted increasing attention due to its experimental
simplicity, lower-cost implementation and compatibility with classical optical
communication. Correspondingly, some novel numerical methods have been proposed
to analyze the security of these protocols against collective attacks, which
promotes key rates over one hundred kilometers of fiber distance. However,
numerical methods are limited by their calculation time and resource
consumption, for which they cannot play more roles on mobile platforms in
quantum networks. To improve this issue, a neural network model predicting key
rates in nearly real time has been proposed previously. Here, we go further and
show a neural network model combined with Bayesian optimization. This model
automatically designs the best architecture of neural network computing key
rates in real time. We demonstrate our model with two variants of CV QKD
protocols with quaternary modulation. The results show high reliability with
secure probability as high as $99.15\%-99.59\%$, considerable tightness and
high efficiency with speedup of approximately $10^7$ in both cases. This
inspiring model enables the real-time computation of unstructured quantum key
distribution protocols' key rate more automatically and efficiently, which has
met the growing needs of implementing QKD protocols on moving platforms.",smart cities
http://arxiv.org/abs/2201.05858v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"smart parking space detection under hazy conditions using convolutional
  neural networks: a novel approach",http://arxiv.org/abs/2201.05858v1,"Limited urban parking space combined with urbanization has necessitated the
development of smart parking systems that can communicate the availability of
parking slots to the end users. Towards this, various deep learning based
solutions using convolutional neural networks have been proposed for parking
space occupation detection. Though these approaches are robust to partial
obstructions and lighting conditions, their performance is found to degrade in
the presence of haze conditions. Looking in this direction, this paper
investigates the use of dehazing networks that improves the performance of
parking space occupancy classifier under hazy conditions. Additionally,
training procedures are proposed for dehazing networks to maximize the
performance of the system on both hazy and non-hazy conditions. The proposed
system is deployable as part of existing smart parking systems where limited
number of cameras are used to monitor hundreds of parking spaces. To validate
our approach, we have developed a custom hazy parking system dataset from
real-world task-driven test set of RESIDE-\b{eta} dataset. The proposed
approach is tested against existing state-of-the-art parking space detectors on
CNRPark-EXT and hazy parking system datasets. Experimental results indicate
that there is a significant accuracy improvement of the proposed approach on
the hazy parking system dataset.",smart cities
http://arxiv.org/abs/2201.05024v2,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"real-time gpu-accelerated machine learning based multiuser detection for
  5g and beyond",http://arxiv.org/abs/2201.05024v2,"Adaptive partial linear beamforming meets the need of 5G and future 6G
applications for high flexibility and adaptability. Choosing an appropriate
tradeoff between conflicting goals opens the recently proposed multiuser (MU)
detection method. Due to their high spatial resolution, nonlinear beamforming
filters can significantly outperform linear approaches in stationary scenarios
with massive connectivity. However, a dramatic decrease in performance can be
expected in high mobility scenarios because they are very susceptible to
changes in the wireless channel. The robustness of linear filters is required,
considering these changes. One way to respond appropriately is to use online
machine learning algorithms. The theory of algorithms based on the adaptive
projected subgradient method (APSM) is rich, and they promise accurate tracking
capabilities in dynamic wireless environments. However, one of the main
challenges comes from the real-time implementation of these algorithms, which
involve projections on time-varying closed convex sets. While the projection
operations are relatively simple, their vast number poses a challenge in
ultralow latency (ULL) applications where latency constraints must be satisfied
in every radio frame. Taking non-orthogonal multiple access (NOMA) systems as
an example, this paper explores the acceleration of APSM-based algorithms
through massive parallelization. The result is a GPU-accelerated real-time
implementation of an orthogonal frequency-division multiplexing (OFDM)-based
transceiver that enables detection latency of less than one millisecond and
therefore complies with the requirements of 5G and beyond. To meet the
stringent physical layer latency requirements, careful co-design of hardware
and software is essential, especially in virtualized wireless systems with
hardware accelerators.",smart cities
http://arxiv.org/abs/2201.04349v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,video intelligence as a component of a global security system,http://arxiv.org/abs/2201.04349v1,"This paper describes the evolution of our research from video analytics to a
global security system with focus on the video surveillance component. Indeed
video surveillance has evolved from a commodity security tool up to the most
efficient way of tracking perpetrators when terrorism hits our modern urban
centers. As number of cameras soars, one could expect the system to leverage
the huge amount of data carried through the video streams to provide fast
access to video evidences, actionable intelligence for monitoring real-time
events and enabling predictive capacities to assist operators in their
surveillance tasks. This research explores a hybrid platform for video
intelligence capture, automated data extraction, supervised Machine Learning
for intelligently assisted urban video surveillance; Extension to other
components of a global security system are discussed. Applying Knowledge
Management principles in this research helps with deep problem understanding
and facilitates the implementation of efficient information and experience
sharing decision support systems providing assistance to people on the field as
well as in operations centers. The originality of this work is also the
creation of ""common"" human-machine and machine to machine language and a
security ontology.",smart cities
http://arxiv.org/abs/2201.03808v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,mobilefaceswap: a lightweight framework for video face swapping,http://arxiv.org/abs/2201.03808v1,"Advanced face swapping methods have achieved appealing results. However, most
of these methods have many parameters and computations, which makes it
challenging to apply them in real-time applications or deploy them on edge
devices like mobile phones. In this work, we propose a lightweight
Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by
dynamically adjusting the model parameters according to the identity
information. In particular, we design an efficient Identity Injection Module
(IIM) by introducing two dynamic neural network techniques, including the
weights prediction and weights modulation. Once the IDN is updated, it can be
applied to swap faces given any target image or video. The presented IDN
contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it
capable for real-time video face swapping on mobile phones. In addition, we
introduce a knowledge distillation-based method for stable training, and a loss
reweighting module is employed to obtain better synthesized results. Finally,
our method achieves comparable results with the teacher models and other
state-of-the-art methods.",smart cities
10.1016/j.rse.2021.112809,preprocessed,Remote Sensing of Environment,scopus,2022-02-01,sciencedirect,methanet – an ai-driven approach to quantifying methane point-source emission from high-resolution 2-d plume imagery,https://api.elsevier.com/content/abstract/scopus_id/85120521703,"Methane is one of the most important anthropogenic greenhouse gases with a significant impact on the Earth's radiation budget and tropospheric background ozone. Despite a well-constrained global budget, quantification of local and regional methane emissions has proven challenging. Recent advancements in airborne remote sensing instruments such as from the next-generation Airborne Visible/Infrared Imaging Spectrometer (AVIRIS-NG) provide 2-D observations of CH4 plume column enhancements at an unprecedented resolution of 1–5 m over large geographic areas. Quantifying an emission rate from observed plumes is a critical step for understanding local emission distributions and prioritizing mitigation efforts. However, there exists no method that can predict emission rates from detected plumes in real-time without ancillary data reliably. In order to predict methane point-source emissions directly from high resolution 2-D plume images without relying on other local measurements such as background wind speeds, we trained a convolutional neural network model called MethaNet. The training data was derived from large eddy simulations of methane plumes and realistic measurement noise over agricultural, desert and urban environments. Our model has a mean absolute percentage error for predicting unseen plumes under 17%, a significant improvement from previous methods that require wind information. Using MethaNet, a validation against a natural gas controlled-release experiment agrees to within the precision error estimate. Our results support the basis for the applicability of using deep learning techniques to quantify CH4 point sources in an automated manner over large geographical areas, not only for present and future airborne field campaigns but also for upcoming space-based observations in this decade.",smart cities
10.1016/j.cageo.2021.105010,preprocessed,Computers and Geosciences,scopus,2022-02-01,sciencedirect,geospatialvr: a web-based virtual reality framework for collaborative environmental simulations,https://api.elsevier.com/content/abstract/scopus_id/85120435814,"This research introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be served on any web platform and accessed via desktop and mobile devices and virtual reality headsets. The framework can generate realistic simulations of desired locations entailing the terrain, elevation model, infrastructures, dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, occupancy, traffic, weather). These layers enable in-situ visualization of useful data to aid public, scientists, officials, and decision-makers in acquiring a bird's eye view of the current, historical, or forecasted condition of a community. The framework incorporates multiuser support to allow different stakeholders to remotely work on the same VR environment and observe other users' actions and 3D positions via avatars in real-time, and thus, presenting the potential to be utilized as a virtual incident command center or a meeting room. GeospatialVR's purpose is to enhance existing web-based cyberinfrastructure systems with the integration of immersive geospatial capabilities to assist the development of next-generation information and decision support systems powered by virtual reality. Finally, several case studies have been developed for flooding, wildfire, transportation, and public safety.",smart cities
10.1016/j.apenergy.2021.118136,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,an online energy management system for ac/dc residential microgrids supported by non-intrusive load monitoring,https://api.elsevier.com/content/abstract/scopus_id/85119066285,"Traditional electric energy systems are experiencing a major revolution and the main drivers of this revolution are green transition and digitalization. In this paper, an advanced system-level EMS is proposed for residential AC/DC microgrids (MGs) by taking advantage of the innovations offered by digitalization. The proposed EMS supports green transition as it is designed for an MG that includes renewable energy sources (RESs), batteries, and electric vehicles. In addition, the electricity consumption behaviors of residential users have been automatically extracted to create a more flexible MG. Deep learning-supported Non-intrusive load monitoring (NILM) algorithm is deployed to analyze and disaggregate the aggregated consumption signal of each household in the MG. A two-level EMS is designed that coordinates both households and MG components using optimization, forecasting, and NILM modules. The proposed system-level EMS has been tested in a laboratory environment in real-time. Experiments are performed considering different optimization periods and the effectiveness of the proposed EMS has been shown for different optimization horizons. Compared to a peak shaving strategy as a benchmark, the proposed EMS for 24-hour horizon provides a 12.36% reduction in the residential MG daily operation cost.",smart cities
10.1016/j.snb.2021.130958,preprocessed,Sensors and Actuators B: Chemical,scopus,2022-01-15,sciencedirect,from air quality sensors to sensor networks: things we need to learn,https://api.elsevier.com/content/abstract/scopus_id/85118500250,"As a potential complement to traditional regulatory instruments, low-cost air quality sensors (LCAQS) can be deployed in dense monitoring networks to provide timely and comprehensive snapshots of pollutant concentrations and their spatial and temporal variability at various scales with relatively less cost and labor. However, a lack of practical guidance and a limited understanding of sensor data quality hinder the widespread application of this emerging technology. We leveraged air quality data collected from state and local monitoring agencies in metropolitan areas of the United States to evaluate how low-cost sensors could be deployed across the U.S. We found that ozone, as a secondary pollutant, is more homogeneous than other pollutants at various scales. PM2.5, CO, and NO2 displayed homogeneities that varied by city, making it challenging to design a uniform network that was suitable across geographies. Our low-cost sensor data in New York City indicated that PM2.5 sensors track well with light-scattering reference methods, particularly at low concentrations. The same phenomenon was also found after thoroughly evaluating sensor evaluation reports from the Air Quality Sensor Performance Evaluation Center (AQ-SPEC). Furthermore, LCAQS data collected during wildfire episodes in Portland, OR show that a real-time (i.e. in situ) machine learning calibration process is a promising approach to address the data quality challenges persisting in LCAQS applications. Our research highlights the urgency and importance of practical guidance for deploying LCAQS.",smart cities
10.1016/j.jsr.2021.12.010,preprocessed,Journal of Safety Research,scopus,2022-01-01,sciencedirect,learning to interpret novel ehmi: the effect of vehicle kinematics and ehmi familiarity on pedestrian’ crossing behavior,https://api.elsevier.com/content/abstract/scopus_id/85123375400,"Introduction: In current urban traffic, pedestrians attempting to cross the road at un-signalized locations are thought to mostly use implicit communication, such as deceleration cues, to interpret a vehicle’s intention to yield. There is less reliance on explicit driver- or vehicle-based messages, such as hand/head movements, or flashing lights/beeping horns. With the impending deployment of Automated Vehicles (AV), especially those at SAE Level 4 and 5, where the driver is no longer in control of the vehicle, there has been a surge in interest in the value of new forms of communication for AVs, for example, via different types of external Human Machine Interfaces (eHMIs). However, there is still much to be understood about how quickly a novel eHMI affects pedestrian crossing decisions, and whether it provides any additional aid, above and beyond implicit/kinematic information from the vehicle. The aim of this between-participant study, funded by the H2020 interACT project, was to investigate how the combination of kinematic information from a vehicle (e.g., Speed and Deceleration), and eHMI designs, play a role in assisting the crossing decision of pedestrians in a cave-based pedestrian simulator. Method: Using an existing, well-recognized, message for yielding (Flashing Headlights - FH) as a benchmark, this study also investigated how quickly a novel eHMI (Slow Pulsing Light Band – SPLB) was learned. To investigate the effect of eHMI visibility on crossing decisions, the distance at which each eHMI was perceivable was also measured. Results: Results showed that, compared to SPLB, the FH led to earlier crossings during vehicle deceleration, especially at lower approaching speeds, and smaller time gaps. However, although FH was visible earlier than SPLB, this visibility does not appear to be the only reason for earlier crossings, with message familiarity thought to play a role. Participants were found to learn the meaning conveyed by FH relatively quickly, crossing around 1 second earlier in its presence (compared to the no eHMI condition), across the three blocks of trials. On the other hand, it took participants at least one block of 12 trials for the new SPLB signal to affect crossing, which only accelerated crossing initiations by around 200 ms, compared to the no eHMI condition. The role of comprehension, long-term exposure, and familiarity of novel messages in this context is therefore important, if AVs are to provide safe, trustworthy communication messages, which will enhance traffic flow and efficiency.",smart cities
10.1016/j.isprsjprs.2021.10.015,preprocessed,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2022-01-01,sciencedirect,changemask: deep multi-task encoder-transformer-decoder architecture for semantic change detection,https://api.elsevier.com/content/abstract/scopus_id/85119995073,"Multi-temporal high spatial resolution earth observation makes it possible to detect complex urban land surface changes, which is a significant and challenging task in remote sensing communities. Previous works mainly focus on binary change detection (BCD) based on modern technologies, e.g., deep fully convolutional network (FCN), whereas the deep network architecture for semantic change detection (SCD) is insufficiently explored in current literature. In this paper, we propose a deep multi-task encoder-transformer-decoder architecture (ChangeMask) designed by exploring two important inductive biases: sematic-change causal relationship and temporal symmetry. ChangeMask decouples the SCD into a temporal-wise semantic segmentation and a BCD, and then integrates these two tasks into a general encoder-transformer-decoder framework. In the encoder part, we design a semantic-aware encoder to model the semantic-change causal relationship. This encoder is only used to learn semantic representation and then learn change representation from semantic representation via a later transformer module. In this way, change representation can constrain semantic representation during training, which introduces a regularization to reduce the risk of overfitting. To learn a robust change representation from semantic representation, we propose a temporal-symmetric transformer (TST) to guarantee temporal symmetry for change representation and keep it discriminative. Based on the above semantic representation and change representation, we adopt simple multi-task decoders to output semantic change map. Benefiting from the differentiable building blocks, ChangeMask can be trained by a multi-task loss function, which significantly simplifies the whole pipeline of applying ChangeMask. The comprehensive experimental results on two large-scale SCD datasets confirm the effectiveness and superiority of ChangeMask in SCD. Besides, to demonstrate the potential value in real-world applications, e.g., automatic urban analysis and decision-making, we deploy the ChangeMask to map a large geographic area covering 30 km2 with 300 million pixels. Code will be made available.",smart cities
10.1016/j.scs.2021.103364,preprocessed,Sustainable Cities and Society,scopus,2022-01-01,sciencedirect,blockchain-enabled secure framework for energy-efficient smart parking in sustainable city environment,https://api.elsevier.com/content/abstract/scopus_id/85117732239,"In the smart city environment, parking vehicle management is an essential requirement for citizens in the current situation because every city is rapidly growing as a crowded environment. Specific planning, operation, and thinking can address this problem with the Internet of things (IoT) and Information Communication Technologies (ICT). Existing research provides various solutions and methods for parking systems in the smart city. However, smart parking has many challenges, such as centralization, communication bandwidth, energy efficiency, integrity, security, and privacy. Inspired by Blockchain and AI technology, we propose a Blockchain-enabled Secure Framework for Energy-Efficient Smart Parking in Sustainable City Environment. The transport layer implements the ECC algorithm to encrypt and decrypt the parking zones data for secure communication. The RSU-based-Blockchain network offers authentication and verification of data at the security layer in a distributed manner. Virtualization technology is used for data storage and provides an energy-efficient environment using virtual machines. With Deep LSTM networks, we analyze the parking zone's data and offer the best parking space to the drivers with the best location and timing. We evaluate the proposed architecture using quantitative, qualitative analysis and provide the driver's best parking space.",smart cities
10.1016/j.apenergy.2021.117853,preprocessed,Applied Energy,scopus,2022-01-01,sciencedirect,transferable representation modelling for real-time energy management of the plug-in hybrid vehicle based on k-fold fuzzy learning and gaussian process regression,https://api.elsevier.com/content/abstract/scopus_id/85114985028,"Electric vehicles, including plug-in hybrids, are important for achieving net-zero emission and will dominate road transportation in the future. Energy management, which optimizes the onboard energy usage, is a critical functionality of electric vehicles. It is usually developed following the model-based routine, which is conventionally costly and time-consuming and is hard to meet the increasing market competition in the digital era. To reduce the development workload for the energy management controller, this paper studies an innovative transfer learning routine. A new transferable representation control model is proposed by incorporating two promising artificial intelligence technologies, adaptive neural fuzzy inference system and Gaussian process regression, where the former applies k-fold cross valudation to build a neural fuzzy system for real-time implementation of offline optimization result, and the later connects the neural fuzzy system with a ‘deeper’ architecture to transfer the offline optimization knowledge learnt at source domain to new target domains. By introducing a concept of control utility that evaluates vehicle energy efficiency with a penalty on usage of battery energy, experimental evaluations based on the hardware-in-the-loop testing platform are conducted. Competitive real-time control ultility values (as much as 90% of offline benchmarking results) can be achieved by the proposed control method. They are over 27% higher than that achieved by the neural-network-based model.",smart cities
10.1109/jsac.2021.3118405,preprocessed,IEEE Journal on Selected Areas in Communications,IEEE,2022-02-01 00:00:00,ieeexplore,"learning-based prediction, rendering and transmission for interactive virtual reality in ris-assisted terahertz networks",https://ieeexplore.ieee.org/document/9565222/,"The quality of experience (QoE) requirements of wireless virtual reality (VR) can only be satisfied with high data rate, high reliability, and low VR interaction latency. This high data rate over short transmission distances may be achieved via the abundant bandwidth in the terahertz (THz) band. However, THz waves experience severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with programmable reflecting elements. Meanwhile, the low VR interaction latency can be achieved with the mobile edge computing (MEC) network architecture due to its computation capabilities. Motivated by these considerations, in this paper, we propose an MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by taking into account the uplink viewpoint prediction and position transmission, the MEC rendering, and the downlink transmission. We propose two methods, which are referred to as centralized online gated recurrent unit (GRU) and distributed federated averaging (FedAvg), to predict the viewpoints of the VR users. In the uplink, an algorithm that integrates online long-short term memory (LSTM) and convolutional neural networks (CNN) is deployed to predict the locations and the line-of-sight and non-line-of-sight statuses of the VR users over time. In the downlink, we develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS under latency constraints. Simulation results show that our proposed learning architecture achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.",multimedia
10.1109/tnsre.2022.3147772,preprocessed,IEEE Transactions on Neural Systems and Rehabilitation Engineering,IEEE,2000-01-01 00:00:00,ieeexplore,improving automatic control of upper-limb prosthesis wrists using gaze-centered eye tracking and deep learning,https://ieeexplore.ieee.org/document/9698069/,"Many upper-limb prostheses lack proper wrist rotation functionality, leading to users performing poor compensatory strategies, leading to overuse or abandonment. In this study, we investigate the validity of creating and implementing a data-driven predictive control strategy in object grasping tasks performed in virtual reality. We propose the idea of using gaze-centered vision to predict the wrist rotations of a user and implement a user study to investigate the impact of using this predictive control. We demonstrate that using this vision-based predictive system leads to a decrease in compensatory movement in the shoulder, as well as task completion time. We discuss the cases in which the virtual prosthesis with the predictive model implemented did and did not make a physical improvement in various arm movements. We also discuss the cognitive value in implementing such predictive control strategies into prosthetic controllers. We find that gaze-centered vision provides information about the intent of the user when performing object reaching and that the performance of prosthetic hands improves greatly when wrist prediction is implemented. Lastly, we address the limitations of this study in the context of both the study itself as well as any future physical implementations.",multimedia
10.1109/tmrb.2021.3129113,preprocessed,IEEE Transactions on Medical Robotics and Bionics,IEEE,2022-02-01 00:00:00,ieeexplore,learning a generic olfactory search strategy from silk moths by deep inverse reinforcement learning,https://ieeexplore.ieee.org/document/9619462/,"Despite their simple nervous systems, insects efficiently search for and find sources of odorants. Hence, it is necessary to model and implement such behavior in artificial agents (robots), to enable them to detect dangerous substances such as drugs, gas leaks, and explosives. Previous studies have approached behavioral modeling with either statistical or machine-learning methods. In this study, we determined the behavior trajectories of male silk moths using a virtual reality (VR) system. We then modeled these trajectories as a Markov decision process (MDP) and employed inverse reinforcement learning (IRL) to learn their reward function. Furthermore, we estimated the optimal policy from the learned reward function. We then conducted olfactory search simulations and determined that the IRL-based policy could locate odor sources with a high success rate. This was also investigated under environmental conditions different from those faced by real moths on the VR system. The obtained results indicate that IRL can generically represent olfactory search strategies that are adaptable to various environments.",multimedia
10.1109/wacvw54805.2022.00069,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),IEEE,2022-01-08 00:00:00,ieeexplore,aa3dnet: attention augmented real time 3d object detection,https://ieeexplore.ieee.org/document/9707544/,"In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Bird’s Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at &gt;30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",multimedia
10.1109/wacv51458.2022.00380,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,fast-clocs: fast camera-lidar object candidates fusion for 3d object detection,https://ieeexplore.ieee.org/document/9706631/,"When compared to single modality approaches, fusion-based object detection methods often require more complex models to integrate heterogeneous sensor data, and use more GPU memory and computational resources. This is particularly true for camera-LiDAR based multimodal fusion, which may require three separate deep-learning networks and/or processing pipelines that are designated for the visual data, LiDAR data, and for some form of a fusion framework. In this paper, we propose Fast Camera-LiDAR Object Candidates (Fast-CLOCs) fusion network that can run high-accuracy fusion-based 3D object detection in near real-time. Fast-CLOCs operates on the output candidates before Non-Maximum Suppression (NMS) of any 3D detector, and adds a lightweight 3D detector-cued 2D image detector (3D-Q-2D) to extract visual features from the image domain to improve 3D detections significantly. The 3D detection candidates are shared with the proposed 3D-Q-2D image detector as proposals to reduce the network complexity drastically. The superior experimental results of our Fast-CLOCs on the challenging KITTI and nuScenes datasets illustrate that our Fast-CLOCs outperforms state-of-the-art fusion-based 3D object detection approaches. We will release the code upon publication.",multimedia
10.1109/wacv51458.2022.00037,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,occlusion resistant network for 3d face reconstruction,https://ieeexplore.ieee.org/document/9706716/,"3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of 0.77 against 5.84 of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of 6.67 in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.",multimedia
10.1109/lra.2022.3142439,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,anytime 3d object reconstruction using multi-modal variational autoencoder,https://ieeexplore.ieee.org/document/9681277/,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository<sup>1</sup>.",multimedia
10.1109/access.2022.3140332,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,spatial-importance-based computation scheme for real-time object detection from 3d sensor data,https://ieeexplore.ieee.org/document/9668921/,"Three-dimensional (3D) sensor networks using multiple light-detection-and-ranging (LIDAR) sensors are good for smart monitoring of spots, such as intersections, with high potential risk of road-traffic accidents. The image sensors must share the strictly limited computation capacity of an edge computer. To have the computation speeds required from real-time applications, the system must have a short computation delay while maintaining the quality of the output, e.g., the accuracy of the object detection. This paper proposes a spatial-importance-based computation scheme that can be implemented on an edge computer of image-sensor networks composed of 3D sensors. The scheme considers regions where objects exist as more likely to be ones of higher spatial importance. It processes point-cloud data from each region according to the spatial importance of that region. By prioritizing regions with high spatial importance, it shortens the computation delay involved in the object detection. A point-cloud dataset obtained by a moving car equipped with a LIDAR unit was used to numerically evaluate the proposed scheme. The results indicate that the scheme shortens the delay in object detection.",multimedia
10.1109/lsp.2022.3144074,preprocessed,IEEE Signal Processing Letters,IEEE,2000-01-01 00:00:00,ieeexplore,rethinking lightweight: multiple angle strategy for efficient video action recognition,https://ieeexplore.ieee.org/document/9684992/,"Video action recognition task involves modeling spatiotemporal information, and efficiency is critical to capture spatiotemporal dependencies in the video. Most existing models rely on optical flow information to capture the dynamic visual tempos between consecutive video frames. Although impressive performance can be achieved by combining optical flow with RGB, the time-consuming nature of optical flow computation cannot be ignored. Moreover, 3D CNN has successfully modeled spatiotemporal information, yet the enormous computational volume is unsuitable for real-time action recognition. In this letter, we propose a novel lightweight video feature extraction strategy that achieves better recognition performance with lower FLOPs. In particular, we perform convolution on the video cube from three orthogonal angles to learn its appearance and motion features. Compared with the computational volume of 3D CNN, our proposed method is more economical and thus meets the lightweight requirements. Extensive experimental results on public Something Something-V1<inline-formula><tex-math notation=""LaTeX"">$\&amp;$</tex-math></inline-formula>V2 and Diving48 datasets show our approach achieves the state-of-the-art performance.",multimedia
10.1109/lra.2022.3147337,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,sim2air - synthetic aerial dataset for uav monitoring,https://ieeexplore.ieee.org/document/9699390/,"In this letter, we propose a novel approach to generate a synthetic aerial dataset for application in UAV monitoring. We propose to accentuate shape-based object representation by applying texture randomization. A diverse dataset with photorealism in all parameters such as shape, pose, lighting, scale, viewpoint, etc. except for atypical textures is created in a 3D modelling software Blender. Our approach specifically targets two conditions in aerial images where texture of objects is difficult to detect, namely challenging illumination and objects occupying only a small portion of the image. Experimental evaluation of YOLO and Faster R-CNN detectors trained on synthetic data with randomized textures confirmed our approach by increasing the mAP value (17 and 3.7 percentage points for YOLO; 20 and 1.1 percentage points for Faster R-CNN) on two test datasets of real images, both containing UAV-to-UAV images with motion blur. Testing on different domains, we conclude that the more the generalisation ability is put to the test, the more apparent are the advantages of the shape-based representation.",multimedia
10.1109/lra.2021.3116700,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments,https://ieeexplore.ieee.org/document/9555228/,"Obstacle avoidance for robotic manipulators can be challenging when they operate in unstructured environments. This problem is probed with the sim-to-real (sim2real) deep reinforcement learning, such that a moving policy of the robotic arm is learnt in a simulator and then adapted to the real world. However, the problem of sim2real adaptation is notoriously difficult. To this end, this work proposes (1) a unified representation of obstacles and targets to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model combining the unified representation with the deep reinforcement learning control module that can be trained by interacting with the environment. Such a representation is agnostic to the shape and appearance of the underlying objects, which simplifies and unifies the scene representation in both simulated and real worlds. We implement this idea with a vision-based actor-critic framework by devising a bounding box predictor module. The predictor estimates the 3D bounding boxes of obstacles and targets from the RGB-D input. The features extracted by the predictor are fed into the policy network, and all the modules are jointly trained. This makes the policy learn object-aware scene representation, which leads to a data-efficient learning of the obstacle avoidance policy. Our experiments in simulated environment and the real-world show that the end-to-end model of the unified representation achieves better sim2real adaption and scene generalization than state-of-the-art techniques.",multimedia
10.1109/jiot.2021.3089080,preprocessed,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,audio-visual autoencoding for privacy-preserving video streaming,https://ieeexplore.ieee.org/document/9453730/,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",multimedia
10.1109/wacv51458.2022.00135,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,detecting tear gas canisters with limited training data,https://ieeexplore.ieee.org/document/9706699/,"Human rights investigations often entail triaging large volumes of open source images and video in order to find moments that are relevant to a given investigation and warrant further inspection. Searching for instances of tear gas usage online manually is laborious and time-consuming. In this paper, we study various object detection models for their potential use in the discovery and identification of tear gas canisters for human rights monitors. CNN based object detection typically requires large volumes of training data, and prior to our work, an appropriate dataset of tear gas canisters did not exist. We benchmark methods for training object detectors using limited labelled data: we fine-tune different object detection models on the limited labelled data and compare performance to a few shot detector and augmentation strategies using synthetic data. We provide a dataset for evaluating and training tear gas canister detectors and indicate how such detectors can be deployed in real-world contexts for investigating human rights violations. Our experiments show that various techniques can improve results, including fine-tuning state of the art detectors, using few shot detectors, and including synthetic data as part of the training set.",multimedia
10.1109/comsnets53615.2022.9668364,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,open-air off-street vehicle parking management system using deep neural networks: a case study,https://ieeexplore.ieee.org/document/9668364/,"Smart parking solution aims to output real-time parking occupancy information. It helps to reduce parking bay search time, traffic, fuel consumption, and thereby vehicular emissions with increased road safety. A computer vision-based solution using camera video data is most reliable and rational since it allows monitoring the entire open-air parking area at once. A real-time parking solution (cloud-based, server processing, or onboard processing) helps bring the occupancy information to the end-user. It comes with many challenges such as viewing angles, lighting conditions, model optimization, reducing inference time, and many more real-world challenges. Hence, this paper presents a case study on real-time open-air off-street intelligent parking management using a deep neural network. Also, most of the earlier research works focus on day-time data and do not discuss the night data. So, in this work, we perform experiments on realtime 24-hour data from an input camera video source mounted to monitor parking at IIT Hyderabad (IITH) parking lot. Our experiments demonstrate the real-world challenges and can help improve parking performance, deployment at IITH, and relevant parking systems in general.",multimedia
10.1109/comsnets53615.2022.9668498,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,tele-driving an electric vehicle over a private lte network,https://ieeexplore.ieee.org/document/9668498/,"We demonstrate tele-driving operation for an electric vehicle capable of stopping itself in case of system failure over a captive LTE network deployed in a university campus. Our electronically controlled vehicle is driven remotely by an operator from a control room which receives the multi-camera real-time video feed from the vehicle over this network. Our primary contribution includes the responsive emergency braking mechanism for the vehicle, modular vehicle design based on CAN bus, low latency LTE MAC scheduler design, and modifications to popular video tool, FFMPEG to support low latency real time video streaming. Our demonstration shows complete integration of the different components, i.e., the vehicle, the LTE network and the remote driving application. Another salient feature of our system is the O-RAN compliant RAN awareness module and KPI (Key Performance Indicator) application which enables real-time network performance monitoring.",multimedia
10.1109/access.2022.3147519,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a deep learning-based approach for inappropriate content detection and classification of youtube videos,https://ieeexplore.ieee.org/document/9696242/,"The exponential growth of videos on YouTube has attracted billions of viewers among which the majority belongs to a young demographic. Malicious uploaders also find this platform as an opportunity to spread upsetting visual content, such as using animated cartoon videos to share inappropriate content with children. Therefore, an automatic real-time video content filtering mechanism is highly suggested to be integrated into social media platforms. In this study, a novel deep learning-based architecture is proposed for the detection and classification of inappropriate content in videos. For this, the proposed framework employs an ImageNet pre-trained convolutional neural network (CNN) model known as EfficientNet-B7 to extract video descriptors, which are then fed to bidirectional long short-term memory (BiLSTM) network to learn effective video representations and perform multiclass video classification. An attention mechanism is also integrated after BiLSTM to apply attention probability distribution in the network. These models are evaluated on a manually annotated dataset of 111,156 cartoon clips collected from YouTube videos. Experimental results demonstrated that EfficientNet-BiLSTM (accuracy = 95.66%) performs better than attention mechanism-based EfficientNet-BiLSTM (accuracy = 95.30%) framework. Secondly, the traditional machine learning classifiers perform relatively poor than deep learning classifiers. Overall, the architecture of EfficientNet and BiLSTM with 128 hidden units yielded state-of-the-art performance (f1 score = 0.9267). Furthermore, the performance comparison against existing state-of-the-art approaches verified that BiLSTM on top of CNN captures better contextual information of video descriptors in network architecture, and hence achieved better results in child inappropriate video content detection and classification.",multimedia
10.1109/tmm.2021.3050086,preprocessed,IEEE Transactions on Multimedia,IEEE,2000-01-01 00:00:00,ieeexplore,improving robustness of dash against unpredictable network variations,https://ieeexplore.ieee.org/document/9317786/,"Most video players use adaptive bitrate (ABR) algorithms to provide good quality-of-experience (QoE) in dynamic network conditions. To deal with the adaptation challenges, many ABR algorithms select bitrate by optimizing a defined QoE function. Within the framework, various algorithms mainly differ in how the optimization problem is solved, including prediction-based approaches and learn-based approaches. However, these algorithms suffer from limited performance in the current popular mobile streaming which has limited resources and rapidly changing link rates. Existing machine-learning approaches face deployment difficulties on mobile devices, and prediction-based approaches that rely on throughput prediction experience large buffer occupancy variations in cellular networks, resulting in rebuffering frequently. To provide a robust and lightweight ABR algorithm for mobile streaming, this work improves the robustness of prediction-based scheme against unpredictable network variations and develops RBC (Robust Bitrate Controller) algorithm. Rather than optimizing QoE over the entire buffer capacity, RBC creates buffer margins to absorb the impact of throughput jitters and solves QoE maximization on the narrowed buffer range. The amount of buffer margin is dynamically adjusted based on the real-time throughput fluctuation to ensure sufficient de-jitter space. For online lightweight deployment, RBC provides a closed-form solution of the desired bitrate with small computation complexity by using adaptive control approach. Trace-driven experiments and real-world tests show that RBC effectively reduces the playback freezing and gains an improvement in overall QoE.",multimedia
10.1109/tcsvt.2021.3066675,preprocessed,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,2022-02-01 00:00:00,ieeexplore,spatio-temporal online matrix factorization for multi-scale moving objects detection,https://ieeexplore.ieee.org/document/9380454/,"Detecting moving objects from the video sequences has been treated as a challenging computer vision task, since the problems of dynamic background, multi-scale moving objects and various noise interference impact the corresponding feasibility and efficiency. In this paper, a novel spatio-temporal online matrix factorization (STOMF) method is proposed to detect multi-scale moving objects under dynamic background. To accommodate a wide range of the real noise distractions, we apply a specific mixture of exponential power (MoEP) distributions to the framework of low-rank matrix factorization (LRMF). For the optimization of solution algorithm, a temporal difference motion prior (TDMP) model is proposed, which estimates the motion matrix and calculates the weight matrix. Moreover, a partial spatial motion information (PSMI) post-processing method is further designed to implement multi-scale objects extraction in varieties of complex dynamic scenes, which utilizes partial background and motion information. The superiority of the STOMF method is validated by massive experiments on practical datasets, as compared with state-of-the-art moving objects detection approaches.",multimedia
10.1109/iccece54139.2022.9712814,preprocessed,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),IEEE,2022-01-16 00:00:00,ieeexplore,a lightweight sar image recognition algorithm based on deep convolutional neural network,https://ieeexplore.ieee.org/document/9712814/,"Synthetic Aperture Radar (SAR) can provide large-scale, all-time, all-weather imaging and therefore plays an important role in both military reconnaissance and battlefield perception. In recent years, due to the outstanding performance of deep convolutional neural networks (CNNs) on image recognition, the approaches that apply CNNs to SAR target recognition has attracted widespread attention among domestic and foreign scholars. The CNNs can achieve high accuracy, yet they often contain huge number of parameters and occupy too much memory to be deployed on devices with memory constraint. In this work, we build light-weight SAR image recognition models by performing iterative pruning and retraining on three representative architectures. The algorithm can lead to a 50% reduction in the number of parameters, while still obtaining a high accuracy of 98.5%. Our algorithm and results provide a potential direction for building light-weight SAR image model that can be deployed for real-world applications.",multimedia
10.1109/tim.2021.3132332,preprocessed,IEEE Transactions on Instrumentation and Measurement,IEEE,2000-01-01 00:00:00,ieeexplore,finger vein recognition algorithm based on lightweight deep convolutional neural network,https://ieeexplore.ieee.org/document/9633979/,"Even though the deep neural networks have strong feature representation capability and high recognition accuracy in finger vein recognition, the deep models are computationally intensive and poor in timeliness. To address these issues, this article proposes a lightweight algorithm for finger vein image recognition and matching. The proposed algorithm uses a lightweight convolutional model in the backbone network and employs a triplet loss function to train the model, which not only improves the matching accuracy, but also satisfies the real-time matching requirements. In addition, the Mini-region of interest (RoI) and finger vein pattern feature extraction also effectively solve the problems of large amounts of calculation and background noise. Moreover, the present model recognizes new categories based on the feature vector space constructed by the finger vein recognition system, so that new categories can be recognized without retraining the model. The results show that the finger vein recognition and matching algorithm proposed in this article achieves 99.3% and 99.6% in recognition accuracy and 14.2 and 16.5 ms in matching time for the dataset Shandong University Machine Learning and Applications Laboratory-Homologous Multimodal Biometric Traits (SDUMLA-HMT) and Peking University Finger Vein Dataset (PKU-FVD), respectively. These metrics show that our approach is time-saving and more effective than previous algorithms. Compared with the state-of-the-art finger vein recognition algorithm, the proposed algorithm improves 1.45% in recognition accuracy while saving 45.7% in recognition time.",multimedia
10.1109/taslp.2021.3129994,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,soundstream: an end-to-end neural audio codec,https://ieeexplore.ieee.org/document/9625818/,"We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.",multimedia
10.1109/access.2021.3139537,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"automatic adaptation of open educational resources: an approach from a multilevel methodology based on students’ preferences, educational special needs, artificial intelligence and accessibility metadata",https://ieeexplore.ieee.org/document/9669174/,"The need for adaptive e-learning environments that respond to learning variability is now a fundamental requirement in education, as it helps to ensure that students learn and pass their courses within a set time frame. Although guidelines, techniques and methods have been established in recent years to contribute to the development of accessible and adaptable e-learning environments that promote digital inclusion, their implementation is challenging due to the lack of knowledge of an adequate way to do it and because it is considered more of a technological competence for scholars in the area. In this context, automated support for adapting material that responds to the correct use of accessibility metadata not only provides a way to improve the description of adapted educational resources, but also facilitates their search according to the needs and preferences of students, particularly those with disabilities. In this article, we carry out a multilevel methodological proposal for the automatic adaptation of open educational resources, in order to provide a tool that contributes to the accessibility and correct use of their metadata in e-learning environments. A research is conducted with students with disabilities to establish their real needs and preferences, highlighting the need to strengthen the adequate description and coherent alternative text in images, the correct subtitling in videos and the conversion of audio to text, data that are relevant to our proposal. The research conducted aims to contribute with an automated support tool in the generation of accessible educational resources that are correctly labeled for search and reuse. This research also aims to support researchers in artificial intelligence applications to address challenges and opportunities in the field of virtual education, in addition to providing an overview that could help those who generate educational resources and maintain their interest in making them accessible.",multimedia
10.1109/access.2022.3140901,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a two-stage deep neuroevolutionary technique for self-adaptive speech enhancement,https://ieeexplore.ieee.org/document/9672141/,"This paper presents a novel self-adaptive approach for speech enhancement in the context of highly nonstationary noise. A two-stage deep neuroevolutionary technique for speech enhancement is proposed. The first stage is composed of a deep neural network (DNN) method for speech enhancement. Two DNN methods were tested at this stage, namely, both a deep complex convolution recurrent network (DCCRN) and a residual long short-term memory neural network (ResLSTM). The ResLSTM method was combined with a minimum mean-square error method to perform a preliminary enhancement. The ResLSTM network is used as an <italic>a priori</italic> signal-to-noise ratio (SNR) estimator. The second stage implements a self-adaptive multiband spectral subtraction enhancement method using tuning optimization based on a genetic algorithm. The proposed two-stage technique is evaluated using objective measures of speech quality and intelligibility. The experiments are carried out using the NOIZEUS noisy speech corpus using conditions of real-world stationary, colored, and nonstationary noise sources at multiple SNR levels. These experiments demonstrate the advantage of building a cooperative approach using evolutionary and deep learning-based techniques that are capable of achieving robust speech enhancement in adverse conditions. Indeed, the experimental tests show that the proposed two-stage technique outperformed a baseline implementation using a state-of-the-art deep learning approach by an average 13% and 6% improvement for six noise conditions at a −5 dB and a 0 dB input SNR, respectively.",multimedia
10.1109/taslp.2021.3126947,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,end-to-end neural based modification of noisy speech for speech-in-noise intelligibility improvement,https://ieeexplore.ieee.org/document/9611022/,"Intelligibility of speech can be significantly reduced when it is presented in adverse near-end listening conditions, like background noise. Multiple approaches have been suggested to improve the perception of speech in such conditions. However, most of these approaches were designed to work with clean input speech. Therefore, they have serious limitations when deployed in real world applications like telephony and hearing aids, where noisy input speech is quite common. In this paper we present an end-to-end neural network approach for the above problem, which effectively reduces the input noise and improves the intelligibility for listeners in adverse conditions. To that end, a convolutional neural network topology with variable dilation factors is proposed and evaluated both in a causal and a non-causal configuration using raw speech as input. A Teacher-Student training strategy is employed, where the Teacher is a well-established speech-in-noise intelligibility enhancer based on spectral shaping followed by dynamic range compression (SSDRC). The evaluation is performed both objectively using the speech intelligibility in bits metric (SIIB), and subjectively on the Greek Harvard corpus. A noise robust multi-band version of SSDRC was used as a baseline. Compared with the baseline, at 0 dB input SNR, the suggested neural network system achieved about 380% and 230% relative SIIB improvements in fluctuating and stationary backgrounds, respectively. Subjectively, the suggested model increased listeners’ keyword correct rate in stationary noise from 25% to 60% at 0 dB input SNR, and from about 52% to 75% at 5 dB input SNR, compared with the baseline.",multimedia
10.1109/taslp.2021.3133190,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,vace-wpe: virtual acoustic channel expansion based on neural networks for weighted prediction error-based speech dereverberation,https://ieeexplore.ieee.org/document/9640471/,"Speech dereverberation is an important issue for many real-world speech processing applications. Among the techniques developed, the weighted prediction error (WPE) algorithm has been widely adopted and advanced over the last decade, which blindly cancels out the late reverberation component from the reverberant mixture of microphone signals. In this study, we extend the neural-network-based virtual acoustic channel expansion (VACE) framework for the WPE-based speech dereverberation, a variant of the WPE that we recently proposed to enable the use of dual-channel WPE algorithm in a single-microphone speech dereverberation scenario. Based on the previous study, some ablation studies are conducted regarding the constituents of the VACE-WPE in an offline processing scenario. These studies reveal the characteristics of the system, thereby simplifying the architecture and leading to the introduction of new strategies for training the neural network for the VACE. Experimental results demonstrate that VACE-WPE (our PyTorch implementation and pre-trained models are available from <uri>https://github.com/dreadbird06/vace_wpe</uri>) considerably outperforms its single-channel counterpart in simulated noisy reverberant environments in terms of objective speech quality and is superior to the single-channel WPE as well as several fully neural speech dereverberation methods when employed as the front-end for the far-field automatic speech recognizer.",multimedia
10.1007/978-3-030-85365-5_17,preprocessed,"Advances in Deep Learning, Artificial Intelligence and Robotics",Springer,2022-01-01 00:00:00,springer,robust model for rural education using deep learning and robotics,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85365-5_17,"Rural Education is important for the overall development of villages. National Achievement Survey (NAS) has surveyed and reported in many States of India consistent decline in the learning levels of students in mathematics, language and science from class III to class VIII studying in the government school system. Smart Villages is only possible if the literacy level and infrastructure improves considerably. This paper aims to perform a reality check of the situation by comparing different rural areas of various countries including India and study of related work done. The paper proposes a Robust Model for Rural Education by developing an intelligent humanoid robot using the Deep Learning approach, Human recognition, Object Recognition and Speech Recognition. The data set consists of Primary and Secondary Student data of around 10,000 Students (5 years) from 5 villages. The Proposed Model would be compared with existing models on the parameters of Learnability, Decision making, Flexibility and Cost-effectiveness. The implementation of this Model will help in decreasing the drop out rate, evaluate Students and give them a Learning platform based on their characteristics, increase adaptive and self paced learning. This Model can also be executed for Rural Adult Education and Skill building so that the Smart Village concept can become a reality.",multimedia
10.1007/s00530-021-00881-8,preprocessed,Multimedia Systems,Springer,2022-01-29 00:00:00,springer,an object detection-based few-shot learning approach for multimedia quality assessment,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00881-8,"A large portion of the global population generates various multimedia data such as texts, images, videos, etc. One of the most common categories which influences the public at large is visual multimedia content. Due to the different social media platforms (e.g., Whatsapp, Twitter, Facebook, Instagram, and YouTube), these materials are passed without censorship and national boundaries. Multimedia data containing any violent or vulgar objects could trigger public unrest, and thus, it is a serious threat to the law and order of the land. Children and teenagers use social media like never before in previous generations and create lots of multimedia data. It is important to assess the quality of multimedia content without any bias and prejudices. Although the mainstream social media platforms use different filters and moderation using human experts, it is impossible to verify the terabytes of uploaded images and videos. Thus, it is inevitable to automate the content assessment phase without incurring an increase in upload time. This study aims to prevent uploading or to tag an image/video with a reasonable percentage of a gun as content. In this paper, object detection architectures such as Faster RCNN, EfficientDet, and YOLOv5 have been used to demonstrate how these techniques can efficiently detect human faces and different types of guns in given multimedia data (images/videos). The models are tested on various test images and video clips. A comparative analysis has also been discussed based on mean average precision and frames per second metric. The YOLOv5 provides the best-performing results as high as 80.39% and 35.22% at $$\text{mAP}_{0.5}$$ mAP 0.5 and $$\text{mAP}_{[0.50:0.95]}$$ mAP [ 0.50 : 0.95 ] , respectively. A face recognition task requires thousands of samples and the usual deep learning models are data-driven. On the contrary, a few-shot learning approach has been implemented to recognize the detected faces categorizing the content as real or reel.",multimedia
10.1007/s00371-021-02347-4,preprocessed,The Visual Computer,Springer,2022-01-13 00:00:00,springer,a detailed analysis of image and video forgery detection techniques,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02347-4,"With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site’s or can be as malicious as to defame or hurt one’s reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one’s identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery.",multimedia
10.1007/978-3-030-92127-9_68,preprocessed,"11th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions and Artificial Intelligence - ICSCCW-2021",Springer,2022-01-01 00:00:00,springer,application of digital twin theory for improvement of natural gas treatment unit,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92127-9_68,"This paper describes fundamental principles of Digital Twins theory and provides exact investigation results in application of Digital Twins theory in upstream branch of oil and gas industry, namely based on example of natural gas treatment plant’s performance increase. As one of key process units of natural gas treatment which allows to implement most powerful functions of digital twin gas sweetening unit with set of membranes is considered as object of investigation. On the base of membrane technology manipulated variables are defined as inputs to digital twin model. Some theoretical results as well as real references of model’s engine calculations are reflected in the paper. Details of technical dashboards to visualize calculated results of running model based on manipulated variables are presented including monthly key performance indicators report dashboard, process flow diagram dashboard and high-level management dashboard. Paper also demonstrates data flow between digital twin model and real process unit and also inside digital twin model.",multimedia
10.1007/978-3-030-93564-1_38,preprocessed,7th International Conference on Advancements of Medicine and Health Care through Technology,Springer,2022-01-01 00:00:00,springer,programing a robotic ambulance with virtual reality,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93564-1_38,"By applying artificial intelligence and virtual reality, this study presents results and challenges for robotic ambulances. Programming vehicle dynamics and testing protocol is intended to support task of developing an autonomous ambulance and engineering efforts. Testing parameters are controlled with automated driver. Using software will diminish human input in driving. Actual values of robotic ambulance in testing are displacement, speed, and acceleration. Ambulance’s velocity in testing on a virtual track with corners and straight lines is an important kinematic parameter that influences safety of transport and some program sections. Accelerations are also important to be programmed. Objective of this paper is to highlight sequences of programming robotic ambulance using virtual reality and artificial intelligence. Results are consisting in testing scenarios, ambulance automated driving program on virtual track, refined program code, solutions for challenges.",multimedia
10.1007/978-3-030-95405-5_9,preprocessed,Advanced Data Mining and Applications,Springer,2022-01-01 00:00:00,springer,smart online exam proctoring assist for cheating detection,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95405-5_9,"Online exams are the most preferred mode of exams in online learning environment. This mode of exam has been even more prevalent and a necessity in the event of a forced closure of face-to-face teaching such as the recent Covid-19 pandemic. Naturally, conducting online exams poses much greater challenge to preserving academic integrity compared to conducting on-site face-to-face exams. As there is no human proctor for policing the examinee on site, the chances of cheating are high. Various online exam proctoring tools are being used by educational institutes worldwide, which offer different solutions to reduce the chances of cheating. The most common technique followed by these tools is recording of video and audio of the examinee during the whole duration of exam. These videos can be analyzed later by human examiner to detect possible cheating case. However, viewing hours of exam videos for each student can be impractical for a large class and thus detecting cheating would be next to impossible. Although some AI-based tools are being used by some proctoring software to raise flags, they are not always very useful. In this paper we propose a cheating detection technique that analyzes an exam video to extract four types of event data, which are then fed to a pre-trained classification model for detecting cheating activity. We formulate the cheating detection problem as a multivariate time-series classification problem by transforming each video into a multivariate time-series representing the time-varying event data extracted from each frame of the video. We have developed a real dataset of cheating videos and conduct extensive experiments with varying video lengths, different deep learning and traditional machine learning models and feature sets, achieving prediction accuracy as high as 97.7%.",multimedia
10.1007/978-981-16-5689-7_12,preprocessed,Advances in Data and Information Sciences,Springer,2022-01-01 00:00:00,springer,applications of high dimensional neural networks: a survey,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5689-7_12,"The evolution of artificial neural networks has always been inspired by enormous power of human brain. This survey can be an eye-opener for researchers as its diverse applications of HDNNs in present scenario shows an intelligent way to mimic human brain without creating a complex neuronal architecture having large number of layers. HDNN’s urgency is evident because in science many quantities are measured not by single values for each of them but a group of values defines 1 single unit as: signal has two values: amplitude and phase. The deficiency in present literature on the questions allied with HDNN application looks like sluggish down research focal point and growth in the area. Hence, there exists a call for state-of-the-art addressing high-dimensional problems in neural networks. The study equips readers with a lucid acquaintance of the existing and novel inclination in HDNN replicas. A lot of applications of HDNNs in various disciplines like: healthcare, climate, security, speech recognition, computer vision, music signal processing, production, stock, science, etc. are covered here to confirm advancement in HDNNs. Study divulges that HDNNs is prevalently known as: CVNN, QVNN, 3D VVNN, and OVNN. This paper also reveals that HDNNs have outperformed real valued neural networks in terms of resource utilization, training data set requirement, and accuracy of results. To see a comparative picture of significance and possible implementation of different HDNNs few charts are provided. A motivational message and suggestions for future researches in this area of High-Dimensionality will conclude this paper.",multimedia
http://arxiv.org/abs/2202.08227v1,preprocessed,arxiv,arxiv,2022-02-16 00:00:00,arxiv,ditto: building digital twins of articulated objects from interaction,http://arxiv.org/abs/2202.08227v1,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",multimedia
http://arxiv.org/abs/2202.06483v2,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,bifsmn: binary neural network for keyword spotting,http://arxiv.org/abs/2202.06483v2,"The deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications. However, computational resources for these
networks are significantly constrained since they usually run on-call on edge
devices. In this paper, we present BiFSMN, an accurate and extreme-efficient
binary neural network for KWS. We first construct a High-frequency Enhancement
Distillation scheme for the binarization-aware training, which emphasizes the
high-frequency information from the full-precision network's representation
that is more crucial for the optimization of the binarized network. Then, to
allow the instant and adaptive accuracy-efficiency trade-offs at runtime, we
also propose a Thinnable Binarization Architecture to further liberate the
acceleration potential of the binarized network from the topology perspective.
Moreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8
devices which fully utilizes registers and increases instruction throughput to
push the limit of deployment efficiency. Extensive experiments show that BiFSMN
outperforms existing binarization methods by convincing margins on various
datasets and is even comparable with the full-precision counterpart (e.g., less
than 3% drop on Speech Commands V1-12). We highlight that benefiting from the
thinnable architecture and the optimized 1-bit implementation, BiFSMN can
achieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge
hardware.",multimedia
http://arxiv.org/abs/2202.05940v1,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,automatic curriculum generation for learning adaptation in networking,http://arxiv.org/abs/2202.05940v1,"As deep reinforcement learning (RL) showcases its strengths in networking and
systems, its pitfalls also come to the public's attention--when trained to
handle a wide range of network workloads and previously unseen deployment
environments, RL policies often manifest suboptimal performance and poor
generalizability.
  To tackle these problems, we present Genet, a new training framework for
learning better RL-based network adaptation algorithms. Genet is built on the
concept of curriculum learning, which has proved effective against similar
issues in other domains where RL is extensively employed. At a high level,
curriculum learning gradually presents more difficult environments to the
training, rather than choosing them randomly, so that the current RL model can
make meaningful progress in training. However, applying curriculum learning in
networking is challenging because it remains unknown how to measure the
""difficulty"" of a network environment.
  Instead of relying on handcrafted heuristics to determine the environment's
difficulty level, our insight is to utilize traditional rule-based (non-RL)
baselines: If the current RL model performs significantly worse in a network
environment than the baselines, then the model's potential to improve when
further trained in this environment is substantial. Therefore, Genet
automatically searches for the environments where the current model falls
significantly behind a traditional baseline scheme and iteratively promotes
these environments as the training progresses. Through evaluating Genet on
three use cases--adaptive video streaming, congestion control, and load
balancing, we show that Genet produces RL policies which outperform both
regularly trained RL policies and traditional baselines in each context, not
only under synthetic workloads but also in real environments.",multimedia
http://arxiv.org/abs/2202.05811v1,preprocessed,arxiv,arxiv,2022-02-11 00:00:00,arxiv,overhead image factors for underwater sonar-based slam,http://arxiv.org/abs/2202.05811v1,"Simultaneous localization and mapping (SLAM) is a critical capability for any
autonomous underwater vehicle (AUV). However, robust, accurate state estimation
is still a work in progress when using low-cost sensors. We propose enhancing a
typical low-cost sensor package using widely available and often free prior
information; overhead imagery. Given an AUV's sonar image and a partially
overlapping, globally-referenced overhead image, we propose using a
convolutional neural network (CNN) to generate a synthetic overhead image
predicting the above-surface appearance of the sonar image contents. We then
use this synthetic overhead image to register our observations to the provided
global overhead image. Once registered, the transformation is introduced as a
factor into a pose SLAM factor graph. We use a state-of-the-art simulation
environment to perform validation over a series of benchmark trajectories and
quantitatively show the improved accuracy of robot state estimation using the
proposed approach. We also show qualitative outcomes from a real AUV field
deployment. Video attachment: https://youtu.be/_uWljtp58ks",multimedia
http://arxiv.org/abs/2202.04971v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"asrpu: a programmable accelerator for low-power automatic speech
  recognition",http://arxiv.org/abs/2202.04971v1,"The outstanding accuracy achieved by modern Automatic Speech Recognition
(ASR) systems is enabling them to quickly become a mainstream technology. ASR
is essential for many applications, such as speech-based assistants, dictation
systems and real-time language translation. However, highly accurate ASR
systems are computationally expensive, requiring on the order of billions of
arithmetic operations to decode each second of audio, which conflicts with a
growing interest in deploying ASR on edge devices. On these devices, hardware
acceleration is key for achieving acceptable performance. However, ASR is a
rich and fast-changing field, and thus, any overly specialized hardware
accelerator may quickly become obsolete.
  In this paper, we tackle those challenges by proposing ASRPU, a programmable
accelerator for on-edge ASR. ASRPU contains a pool of general-purpose cores
that execute small pieces of parallel code. Each of these programs computes one
part of the overall decoder (e.g. a layer in a neural network). The accelerator
automates some carefully chosen parts of the decoder to simplify the
programming without sacrificing generality. We provide an analysis of a modern
ASR system implemented on ASRPU and show that this architecture can achieve
real-time decoding with a very low power budget.",multimedia
http://arxiv.org/abs/2202.02656v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,a survey of top-down approaches for human pose estimation,http://arxiv.org/abs/2202.02656v1,"Human pose estimation in two-dimensional images videos has been a hot topic
in the computer vision problem recently due to its vast benefits and potential
applications for improving human life, such as behaviors recognition, motion
capture and augmented reality, training robots, and movement tracking. Many
state-of-the-art methods implemented with Deep Learning have addressed several
challenges and brought tremendous remarkable results in the field of human pose
estimation. Approaches are classified into two kinds: the two-step framework
(top-down approach) and the part-based framework (bottom-up approach). While
the two-step framework first incorporates a person detector and then estimates
the pose within each box independently, detecting all body parts in the image
and associating parts belonging to distinct persons is conducted in the
part-based framework. This paper aims to provide newcomers with an extensive
review of deep learning methods-based 2D images for recognizing the pose of
people, which only focuses on top-down approaches since 2016. The discussion
through this paper presents significant detectors and estimators depending on
mathematical background, the challenges and limitations, benchmark datasets,
evaluation metrics, and comparison between methods.",multimedia
http://arxiv.org/abs/2201.10910v1,preprocessed,arxiv,arxiv,2022-01-26 00:00:00,arxiv,"a bayesian based deep unrolling algorithm for single-photon lidar
  systems",http://arxiv.org/abs/2201.10910v1,"Deploying 3D single-photon Lidar imaging in real world applications faces
multiple challenges including imaging in high noise environments. Several
algorithms have been proposed to address these issues based on statistical or
learning-based frameworks. Statistical methods provide rich information about
the inferred parameters but are limited by the assumed model correlation
structures, while deep learning methods show state-of-the-art performance but
limited inference guarantees, preventing their extended use in critical
applications. This paper unrolls a statistical Bayesian algorithm into a new
deep learning architecture for robust image reconstruction from single-photon
Lidar data, i.e., the algorithm's iterative steps are converted into neural
network layers. The resulting algorithm benefits from the advantages of both
statistical and learning based frameworks, providing best estimates with
improved network interpretability. Compared to existing learning-based
solutions, the proposed architecture requires a reduced number of trainable
parameters, is more robust to noise and mismodelling effects, and provides
richer information about the estimates including uncertainty measures. Results
on synthetic and real data show competitive results regarding the quality of
the inference and computational complexity when compared to state-of-the-art
algorithms.",multimedia
http://arxiv.org/abs/2201.10369v1,preprocessed,arxiv,arxiv,2022-01-25 00:00:00,arxiv,winograd convolution for deep neural networks: efficient point selection,http://arxiv.org/abs/2201.10369v1,"Convolutional neural networks (CNNs) have dramatically improved the accuracy
of tasks such as object recognition, image segmentation and interactive speech
systems. CNNs require large amounts of computing resources because
ofcomputationally intensive convolution layers. Fast convolution algorithms
such as Winograd convolution can greatly reduce the computational cost of these
layers at a cost of poor numeric properties, such that greater savings in
computation exponentially increase floating point errors.
  A defining feature of each Winograd convolution algorithm is a set of
real-value points where polynomials are sampled. The choice of points impacts
the numeric accuracy of the algorithm, but the optimal set of points for small
convolutions remains unknown. Existing work considers only small integers and
simple fractions as candidate points. In this work, we propose a novel approach
to point selection using points of the form {-1/c , -c, c, 1/c } using the full
range of real-valued numbers for c. We show that groups of this form cause
cancellations in the Winograd transform matrices that reduce numeric error. We
find empirically that the error for different values of c forms a rough curve
across the range of real-value numbers helping to localize the values of c that
reduce error and that lower errors can be achieved with non-obvious real-valued
evaluation points instead of integers or simple fractions. We study a range of
sizes for small convolutions and achieve reduction in error ranging from 2% to
around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in
cases when we select a subset of our proposed points which will always lead to
a lower error. Finally we implement a complete Winograd convolution layer and
use it to run deep convolution neural networks on real datasets and show that
our proposed points reduce error, ranging from 22% to 63%.",multimedia
http://arxiv.org/abs/2201.09550v1,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,crowd tracking and monitoring middleware via map-reduce,http://arxiv.org/abs/2201.09550v1,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques.",multimedia
http://arxiv.org/abs/2201.10947v1,preprocessed,arxiv,arxiv,2022-01-22 00:00:00,arxiv,"enabling deep learning on edge devices through filter pruning and
  knowledge transfer",http://arxiv.org/abs/2201.10947v1,"Deep learning models have introduced various intelligent applications to edge
devices, such as image classification, speech recognition, and augmented
reality. There is an increasing need of training such models on the devices in
order to deliver personalized, responsive, and private learning. To address
this need, this paper presents a new solution for deploying and training
state-of-the-art models on the resource-constrained devices. First, the paper
proposes a novel filter-pruning-based model compression method to create
lightweight trainable models from large models trained in the cloud, without
much loss of accuracy. Second, it proposes a novel knowledge transfer method to
enable the on-device model to update incrementally in real time or near real
time using incremental learning on new data and enable the on-device model to
learn the unseen categories with the help of the in-cloud model in an
unsupervised fashion. The results show that 1) our model compression method can
remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy
of over 90% on CIFAR-10; 2) our knowledge transfer method enables the
compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good
accuracy on old categories; 3) it allows the compressed models to converge
within real time (three to six minutes) on the edge for incremental learning
tasks; 4) it enables the model to classify unseen categories of data (78.92%
Top-1 accuracy) that it is never trained with.",multimedia
http://arxiv.org/abs/2201.08619v1,preprocessed,arxiv,arxiv,2022-01-21 00:00:00,arxiv,"dangerous cloaking: natural trigger based backdoor attacks on object
  detectors in the physical world",http://arxiv.org/abs/2201.08619v1,"Deep learning models have been shown to be vulnerable to recent backdoor
attacks. A backdoored model behaves normally for inputs containing no
attacker-secretly-chosen trigger and maliciously for inputs with the trigger.
To date, backdoor attacks and countermeasures mainly focus on image
classification tasks. And most of them are implemented in the digital world
with digital triggers. Besides the classification tasks, object detection
systems are also considered as one of the basic foundations of computer vision
tasks. However, there is no investigation and understanding of the backdoor
vulnerability of the object detector, even in the digital world with digital
triggers. For the first time, this work demonstrates that existing object
detectors are inherently susceptible to physical backdoor attacks. We use a
natural T-shirt bought from a market as a trigger to enable the cloaking
effect--the person bounding-box disappears in front of the object detector. We
show that such a backdoor can be implanted from two exploitable attack
scenarios into the object detector, which is outsourced or fine-tuned through a
pretrained model. We have extensively evaluated three popular object detection
algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building
upon 19 videos shot in real-world scenes, we confirm that the backdoor attack
is robust against various factors: movement, distance, angle, non-rigid
deformation, and lighting. Specifically, the attack success rate (ASR) in most
videos is 100% or close to it, while the clean data accuracy of the backdoored
model is the same as its clean counterpart. The latter implies that it is
infeasible to detect the backdoor behavior merely through a validation set. The
averaged ASR still remains sufficiently high to be 78% in the transfer learning
attack scenarios evaluated on CenterNet. See the demo video on
https://youtu.be/Q3HOF4OobbY.",multimedia
http://arxiv.org/abs/2201.08197v1,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,"enhancement or super-resolution: learning-based adaptive video streaming
  with client-side video processing",http://arxiv.org/abs/2201.08197v1,"The rapid development of multimedia and communication technology has resulted
in an urgent need for high-quality video streaming. However, robust video
streaming under fluctuating network conditions and heterogeneous client
computing capabilities remains a challenge. In this paper, we consider an
enhancement-enabled video streaming network under a time-varying wireless
network and limited computation capacity. ""Enhancement"" means that the client
can improve the quality of the downloaded video segments via image processing
modules. We aim to design a joint bitrate adaptation and client-side
enhancement algorithm toward maximizing the quality of experience (QoE). We
formulate the problem as a Markov decision process (MDP) and propose a deep
reinforcement learning (DRL)-based framework, named ENAVS. As video streaming
quality is mainly affected by video compression, we demonstrate that the video
enhancement algorithm outperforms the super-resolution algorithm in terms of
signal-to-noise ratio and frames per second, suggesting a better solution for
client processing in video streaming. Ultimately, we implement ENAVS and
demonstrate extensive testbed results under real-world bandwidth traces and
videos. The simulation shows that ENAVS is capable of delivering 5%-14% more
QoE under the same bandwidth and computing power conditions as conventional ABR
streaming.",multimedia
http://arxiv.org/abs/2201.08102v2,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,safe deep rl in 3d environments using human feedback,http://arxiv.org/abs/2201.08102v2,"Agents should avoid unsafe behaviour during both training and deployment.
This typically requires a simulator and a procedural specification of unsafe
behaviour. Unfortunately, a simulator is not always available, and procedurally
specifying constraints can be difficult or impossible for many real-world
tasks. A recently introduced technique, ReQueST, aims to solve this problem by
learning a neural simulator of the environment from safe human trajectories,
then using the learned simulator to efficiently learn a reward model from human
feedback. However, it is yet unknown whether this approach is feasible in
complex 3D environments with feedback obtained from real humans - whether
sufficient pixel-based neural simulator quality can be achieved, and whether
the human data requirements are viable in terms of both quantity and quality.
In this paper we answer this question in the affirmative, using ReQueST to
train an agent to perform a 3D first-person object collection task using data
entirely from human contractors. We show that the resulting agent exhibits an
order of magnitude reduction in unsafe behaviour compared to standard
reinforcement learning.",multimedia
http://arxiv.org/abs/2201.07312v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,model-driven cluster resource management for ai workloads in edge clouds,http://arxiv.org/abs/2201.07312v1,"Since emerging edge applications such as Internet of Things (IoT) analytics
and augmented reality have tight latency constraints, hardware AI accelerators
have been recently proposed to speed up deep neural network (DNN) inference run
by these applications. Resource-constrained edge servers and accelerators tend
to be multiplexed across multiple IoT applications, introducing the potential
for performance interference between latency-sensitive workloads. In this
paper, we design analytic models to capture the performance of DNN inference
workloads on shared edge accelerators, such as GPU and edgeTPU, under different
multiplexing and concurrency behaviors. After validating our models using
extensive experiments, we use them to design various cluster resource
management algorithms to intelligently manage multiple applications on edge
accelerators while respecting their latency constraints. We implement a
prototype of our system in Kubernetes and show that our system can host 2.3X
more DNN applications in heterogeneous multi-tenant edge clusters with no
latency violations when compared to traditional knapsack hosting algorithms.",multimedia
http://arxiv.org/abs/2201.07232v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,"real-time x-ray phase-contrast imaging using spinnet -- a speckle-based
  phase-contrast imaging neural network",http://arxiv.org/abs/2201.07232v1,"X-ray phase-contrast imaging has become indispensable for visualizing samples
with low absorption contrast. In this regard, speckle-based techniques have
shown significant advantages in spatial resolution, phase sensitivity, and
implementation flexibility compared with traditional methods. However, their
computational cost has hindered their wider adoption. By exploiting the power
of deep learning, we developed a new speckle-based phase-contrast imaging
neural network (SPINNet) that boosts the phase retrieval speed by at least two
orders of magnitude compared to existing methods. To achieve this performance,
we combined SPINNet with a novel coded-mask-based technique, an enhanced
version of the speckle-based method. Using this scheme, we demonstrate a
simultaneous reconstruction of absorption and phase images on the order of 100
ms, where a traditional correlation-based analysis would take several minutes
even with a cluster. In addition to significant improvement in speed, our
experimental results show that the imaging resolution and phase retrieval
quality of SPINNet outperform existing single-shot speckle-based methods.
Furthermore, we successfully demonstrate its application in 3D X-ray
phase-contrast tomography. Our result shows that SPINNet could enable many
applications requiring high-resolution and fast data acquisition and
processing, such as in-situ and in-operando 2D and 3D phase-contrast imaging
and real-time at-wavelength metrology and wavefront sensing.",multimedia
http://arxiv.org/abs/2201.10978v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,machine learning for food review and recommendation,http://arxiv.org/abs/2201.10978v1,"Food reviews and recommendations have always been important for online food
service websites. However, reviewing and recommending food is not simple as it
is likely to be overwhelmed by disparate contexts and meanings. In this paper,
we use different deep learning approaches to address the problems of sentiment
analysis, automatic review tag generation, and retrieval of food reviews. We
propose to develop a web-based food review system at Nanyang Technological
University (NTU) named NTU Food Hunter, which incorporates different deep
learning approaches that help users with food selection. First, we implement
the BERT and LSTM deep learning models into the system for sentiment analysis
of food reviews. Then, we develop a Part-of-Speech (POS) algorithm to
automatically identify and extract adjective-noun pairs from the review content
for review tag generation based on POS tagging and dependency parsing. Finally,
we also train a RankNet model for the re-ranking of the retrieval results to
improve the accuracy in our Solr-based food reviews search system. The
experimental results show that our proposed deep learning approaches are
promising for the applications of real-world problems.",multimedia
http://arxiv.org/abs/2201.06912v1,preprocessed,arxiv,arxiv,2022-01-14 00:00:00,arxiv,digital twin: from concept to practice,http://arxiv.org/abs/2201.06912v1,"Recent technological developments and advances in Artificial Intelligence
(AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT),
virtually making it possible to introduce automation into all aspects of work
processes. Given these possibilities that DT can offer, practitioners are
facing increasingly difficult decisions regarding what capabilities to select
while deploying a DT in practice. The lack of research in this field has not
helped either. It has resulted in the rebranding and reuse of emerging
technological capabilities like prediction, simulation, AI, and Machine
Learning (ML) as necessary constituents of DT. Inappropriate selection of
capabilities in a DT can result in missed opportunities, strategic
misalignments, inflated expectations, and risk of it being rejected as just
hype by the practitioners. To alleviate this challenge, this paper proposes the
digitalization framework, designed and developed by following a Design Science
Research (DSR) methodology over a period of 18 months. The framework can help
practitioners select an appropriate level of sophistication in a DT by weighing
the pros and cons for each level, deciding evaluation criteria for the digital
twin system, and assessing the implications of the selected DT on the
organizational processes and strategies, and value creation. Three real-life
case studies illustrate the application and usefulness of the framework.",multimedia
http://arxiv.org/abs/2201.05184v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"achieving ai-enabled robust end-to-end quality of experience over radio
  access networks",http://arxiv.org/abs/2201.05184v1,"Emerging applications such as Augmented Reality, the Internet of Vehicles and
Remote Surgery require both computing and networking functions working in
harmony. The End-to-end (E2E) quality of experience (QoE) for these
applications depends on the synchronous allocation of networking and computing
resources. However, the relationship between the resources and the E2E QoE
outcomes is typically stochastic and non-linear. In order to make efficient
resource allocation decisions, it is essential to model these relationships.
This article presents a novel machine-learning based approach to learn these
relationships and concurrently orchestrate both resources for this purpose. The
machine learning models further help make robust allocation decisions regarding
stochastic variations and simplify robust optimization to a conventional
constrained optimization. When resources are insufficient to accommodate all
application requirements, our framework supports executing some of the
applications with minimal degradation (graceful degradation) of E2E QoE. We
also show how we can implement the learning and optimization methods in a
distributed fashion by the Software-Defined Network (SDN) and Kubernetes
technologies. Our results show that deep learning-based modelling achieves E2E
QoE with approximately 99.8\% accuracy, and our robust joint-optimization
technique allocates resources efficiently when compared to existing
differential services alternatives.",multimedia
http://arxiv.org/abs/2201.04833v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"snapshotnet: self-supervised feature learning for point cloud data
  segmentation using minimal labeled data",http://arxiv.org/abs/2201.04833v1,"Manually annotating complex scene point cloud datasets is both costly and
error-prone. To reduce the reliance on labeled data, a new model called
SnapshotNet is proposed as a self-supervised feature learning approach, which
directly works on the unlabeled point cloud data of a complex 3D scene. The
SnapshotNet pipeline includes three stages. In the snapshot capturing stage,
snapshots, which are defined as local collections of points, are sampled from
the point cloud scene. A snapshot could be a view of a local 3D scan directly
captured from the real scene, or a virtual view of such from a large 3D point
cloud dataset. Snapshots could also be sampled at different sampling rates or
fields of view (FOVs), thus multi-FOV snapshots, to capture scale information
from the scene. In the feature learning stage, a new pre-text task called
multi-FOV contrasting is proposed to recognize whether two snapshots are from
the same object or not, within the same FOV or across different FOVs. Snapshots
go through two self-supervised learning steps: the contrastive learning step
with both part and scale contrasting, followed by a snapshot clustering step to
extract higher level semantic features. Then a weakly-supervised segmentation
stage is implemented by first training a standard SVM classifier on the learned
features with a small fraction of labeled snapshots. The trained SVM is used to
predict labels for input snapshots and predicted labels are converted into
point-wise label assignments for semantic segmentation of the entire scene
using a voting procedure. The experiments are conducted on the Semantic3D
dataset and the results have shown that the proposed method is capable of
learning effective features from snapshots of complex scene data without any
labels. Moreover, the proposed method has shown advantages when comparing to
the SOA method on weakly-supervised point cloud semantic segmentation.",multimedia
http://arxiv.org/abs/2201.04195v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,matching-based service offloading for compute-less driven iot networks,http://arxiv.org/abs/2201.04195v1,"With the advent of the Internet of Things (IoT) and 5G networks, edge
computing is offering new opportunities for business model and use cases
innovations. Service providers can now virtualize the cloud beyond the data
center to meet the latency, data sovereignty, reliability, and interoperability
requirements. Yet, many new applications (e.g., augmented reality, virtual
reality, artificial intelligence) are computation-intensive and
delay-sensitivity. These applications are invoked heavily with similar inputs
that could lead to the same output. Compute-less networks aim to implement a
network with a minimum amount of computation and communication. This can be
realized by offloading prevalent services to the edge and thus minimizing
communication in the core network and eliminating redundant computations using
the computation reuse concept. In this paper, we present matching-based
services offloading schemes for compute-less IoT networks. We adopt the
matching theory to match service offloading to the appropriate edge server(s).
Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme
that aims to offload the most invoked and highly reusable services to the
appropriate edge servers. We further extend WHISTLE to provide horizontal
one-to-many computation reuse sharing among edge servers which leads to
bouncing less computation back to the cloud. We evaluate the efficiency and
effectiveness of WHISTLE with a real-world dataset. The obtained findings show
that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the
computation up to 77%, and decrease the communication up to 71%. Theoretical
analyses also prove the stability of the designed schemes.",multimedia
http://arxiv.org/abs/2201.04014v2,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,captcha attack: turning captchas against humanity,http://arxiv.org/abs/2201.04014v2,"Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
  In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.",multimedia
http://arxiv.org/abs/2201.03804v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,"ci-avsr: a cantonese audio-visual speech dataset for in-car command
  recognition",http://arxiv.org/abs/2201.03804v1,"With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",multimedia
http://arxiv.org/abs/2201.03335v2,preprocessed,arxiv,arxiv,2022-01-10 00:00:00,arxiv,"deepke: a deep learning based knowledge extraction toolkit for knowledge
  base population",http://arxiv.org/abs/2201.03335v2,"We present a new open-source and extensible knowledge extraction toolkit,
called DeepKE (Deep learning based Knowledge Extraction), supporting standard
fully supervised, low-resource few-shot and document-level scenarios. DeepKE
implements various information extraction tasks, including named entity
recognition, relation extraction and attribute extraction. With a unified
framework, DeepKE allows developers and researchers to customize datasets and
models to extract information from unstructured texts according to their
requirements. Specifically, DeepKE not only provides various functional modules
and model implementation for different tasks and scenarios but also organizes
all components by consistent frameworks to maintain sufficient modularity and
extensibility. Besides, we present an online platform in
http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has
been equipped with Google Colab tutorials and comprehensive documents for
beginners. We release the source code at https://github.com/zjunlp/DeepKE, with
a demo video.",multimedia
http://arxiv.org/abs/2201.02503v1,preprocessed,arxiv,arxiv,2022-01-07 00:00:00,arxiv,"a review of deep learning techniques for markerless human motion on
  synthetic datasets",http://arxiv.org/abs/2201.02503v1,"Markerless motion capture has become an active field of research in computer
vision in recent years. Its extensive applications are known in a great variety
of fields, including computer animation, human motion analysis, biomedical
research, virtual reality, and sports science. Estimating human posture has
recently gained increasing attention in the computer vision community, but due
to the depth of uncertainty and the lack of the synthetic datasets, it is a
challenging task. Various approaches have recently been proposed to solve this
problem, many of which are based on deep learning. They are primarily focused
on improving the performance of existing benchmarks with significant advances,
especially 2D images. Based on powerful deep learning techniques and recently
collected real-world datasets, we explored a model that can predict the
skeleton of an animation based solely on 2D images. Frames generated from
different real-world datasets with synthesized poses using different body
shapes from simple to complex. The implementation process uses DeepLabCut on
its own dataset to perform many necessary steps, then use the input frames to
train the model. The output is an animated skeleton for human movement. The
composite dataset and other results are the ""ground truth"" of the deep model.",multimedia
http://arxiv.org/abs/2201.02279v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,de-rendering 3d objects in the wild,http://arxiv.org/abs/2201.02279v1,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.",multimedia
http://arxiv.org/abs/2201.01144v2,preprocessed,arxiv,arxiv,2022-01-04 00:00:00,arxiv,digital twin network: opportunities and challenges,http://arxiv.org/abs/2201.01144v2,"The proliferation of emergent network applications (e.g., AR/VR, telesurgery,
real-time communications) is increasing the difficulty of managing modern
communication networks. These applications typically have stringent
requirements (e.g., ultra-low deterministic latency), making it more difficult
for network operators to manage their network resources efficiently. In this
article, we propose the Digital Twin Network (DTN) as a key enabler for
efficient network management in modern networks. We describe the general
architecture of the DTN and argue that recent trends in Machine Learning (ML)
enable building a DTN that efficiently and accurately mimics real-world
networks. In addition, we explore the main ML technologies that enable
developing the components of the DTN architecture. Finally, we describe the
open challenges that the research community has to address in the upcoming
years in order to enable the deployment of the DTN in real-world scenarios.",multimedia
http://arxiv.org/abs/2201.00768v1,preprocessed,arxiv,arxiv,2022-01-03 00:00:00,arxiv,"robust natural language processing: recent advances, challenges, and
  future directions",http://arxiv.org/abs/2201.00768v1,"Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.",multimedia
http://arxiv.org/abs/2201.00309v1,preprocessed,arxiv,arxiv,2022-01-02 00:00:00,arxiv,"optimizing machine learning inference queries with correlative proxy
  models",http://arxiv.org/abs/2201.00309v1,"We consider accelerating machine learning (ML) inference queries on
unstructured datasets. Expensive operators such as feature extractors and
classifiers are deployed as user-defined functions(UDFs), which are not
penetrable with classic query optimization techniques such as predicate
push-down. Recent optimization schemes (e.g., Probabilistic Predicates or PP)
assume independence among the query predicates, build a proxy model for each
predicate offline, and rewrite a new query by injecting these cheap proxy
models in the front of the expensive ML UDFs. In such a manner, unlikely inputs
that do not satisfy query predicates are filtered early to bypass the ML UDFs.
We show that enforcing the independence assumption in this context may result
in sub-optimal plans. In this paper, we propose CORE, a query optimizer that
better exploits the predicate correlations and accelerates ML inference
queries. Our solution builds the proxy models online for a new query and
leverages a branch-and-bound search process to reduce the building costs.
Results on three real-world text, image and video datasets show that CORE
improves the query throughput by up to 63% compared to PP and up to 80%
compared to running the queries as it is.",multimedia
10.1016/j.enconman.2022.115217,preprocessed,Energy Conversion and Management,scopus,2022-02-15,sciencedirect,robopv: an integrated software package for autonomous aerial monitoring of large scale pv plants,https://api.elsevier.com/content/abstract/scopus_id/85122793867,"In this paper, a novel software package, called RoboPV, is introduced for autonomous aerial monitoring of PV plants. RoboPV automatically performs aerial monitoring of PV plants, from optimal trajectory planning to image processing and pattern recognition for real-time fault detection and analysis. RoboPV consists of four integrated components: boundary area detection, path planning, dynamic processing, and fault detection. To design an optimal flight path, aerial images of PV plants, which have been collected from experimental flights, are given as inputs to a developed encoder-decoder deep learning architecture to extract boundary points of PV plants automatically. Then, a novel path planning algorithm is conducted by RoboPV to design an optimal flight path with full coverage of whole regions of the PV plant. Aerial images are analysed in real-time during the flight by a high precise neural network trained for automatic fault detection. In this study, several decision-making and maneuver algorithms were developed for various real-world flight conditions to improve the performance of RoboPV during an autonomous aerial inspection. RoboPV is a modular processing library that can be installed on any micro-computer processor with a low computational power. Moreover, supporting the MAVLink communication protocol enables RoboPV to connect with an intelligent Pixhawk flight autopilot and navigate a wide range of multi-rotors. To demonstrate the performance of RoboPV, a six degrees of freedom dynamic model of a multi-rotor is developed in a SIMULINK environment with a defined aerial monitoring mission on three different real megawatt-scale PV plants. The results prove that RoboPV can execute the autonomous aerial inspection with an overall accuracy of 93% for large-scale PV plants.",multimedia
10.1016/j.ymssp.2021.108284,preprocessed,Mechanical Systems and Signal Processing,scopus,2022-02-15,sciencedirect,real-time model calibration with deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85112506465,"The real-time, and accurate inference of model parameters is of great importance in many scientific and engineering disciplines that use computational models (such as a digital twin) for the analysis and prediction of complex physical processes. However, fast and accurate inference for processes of complex systems cannot easily be achieved in real-time with state-of-the-art methods under noisy real-world conditions with the requirement of a real-time response. The primary reason is that the inference of model parameters with traditional techniques based on optimization or sampling often suffers from computational and statistical challenges, resulting in a trade-off between accuracy and deployment time. In this paper, we propose a novel framework for inference of model parameters based on reinforcement learning. The proposed methodology is demonstrated and evaluated on two different physics-based models of turbofan engines. The experimental results demonstrate that the proposed methodology outperforms all other tested methods in terms of speed and robustness, with high inference accuracy.",multimedia
10.1016/j.vrih.2022.01.004,preprocessed,Virtual Reality and Intelligent Hardware,scopus,2022-02-01,sciencedirect,virtual-reality-based digital twin of office spaces with social distance measurement feature,https://api.elsevier.com/content/abstract/scopus_id/85124517698,"Background
                  Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
               
                  Methods
                  In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
               
                  Results
                  We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
               
                  Conclusions
                  A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.",multimedia
10.1016/j.pmcj.2022.101540,preprocessed,Pervasive and Mobile Computing,scopus,2022-02-01,sciencedirect,lwtool: a data processing toolkit for building a real-time pressure mapping smart textile software system,https://api.elsevier.com/content/abstract/scopus_id/85123030397,"Pressure mapping smart textile is a new type of sensing modality that transforms the pressure distribution over surfaces into digital ”image” and ”video”, that has rich application scenarios in Human Activity Recognition (HAR), because all human activities are linked with force change over certain surfaces. To speed up its application exploration, we propose a toolkit named LwTool for the data processing, including: (a) a feature library, including 1830 ready-to-use temporal and spatial features, (b) a hierarchical feature selection framework that automatically picks out the best features for a new application from the feature library. As real-time processing capability is important for instant user feedback, we emphasize not only on good recognition result but also on reducing time cost when selecting features. Our library and algorithms are validated on Smart-Toy and Smart-Bedsheet applications, an 89.7% accuracy for Smart-Toy and an 83.8% accuracy for Smart-Bedsheet can be achieved (10-fold cross-validation) using our feature library. Adopting the feature selection algorithm, the processing speed is increased by more than 3 times while maintaining high accuracy for both two applications. We believe our method could be a general and powerful toolkit in building real-time recognition software systems for pressure mapping smart textile.",multimedia
10.1016/j.cviu.2021.103339,preprocessed,Computer Vision and Image Understanding,scopus,2022-02-01,sciencedirect,snapshotnet: self-supervised feature learning for point cloud data segmentation using minimal labeled data,https://api.elsevier.com/content/abstract/scopus_id/85122523188,"Manually annotating complex scene point cloud datasets is both costly and error-prone. To reduce the reliance on labeled data, a new model called SnapshotNet is proposed as a self-supervised feature learning approach, which directly works on the unlabeled point cloud data of a complex 3D scene. The SnapshotNet pipeline includes three stages. In the snapshot capturing stage, snapshots, which are defined as local collections of points, are sampled from the point cloud scene. A snapshot could be a view of a local 3D scan directly captured from the real scene, or a virtual view of such from a large 3D point cloud dataset. Snapshots could also be sampled at different sampling rates or fields of view (FOVs), thus multi-FOV snapshots, to capture scale information from the scene. In the feature learning stage, a new pre-text task called multi-FOV contrasting is proposed to recognize whether two snapshots are from the same object or not, within the same FOV or across different FOVs. Snapshots go through two self-supervised learning steps: the contrastive learning step with both part contrasting and scale contrasting, followed by a snapshot clustering step to extract higher level semantic features. Then a weakly-supervised segmentation stage is implemented by first training a standard SVM classifier on the learned features with a small fraction of labeled snapshots. Then trained SVM is further used to predict labels for input snapshots and predicted labels are converted into point-wise label assignments for semantic segmentation of the entire scene using a voting procedure. The experiments are conducted on the Semantic3D dataset and the results have shown that the proposed method is capable of learning effective features from snapshots of complex scene data without any labels. Moreover, the proposed weakly-supervised method has shown advantages when comparing to the state of the art method on weakly-supervised point cloud semantic segmentation.",multimedia
10.1016/j.compag.2021.106644,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,ric-net: a plant disease classification model based on the fusion of inception and residual structure and embedded attention mechanism,https://api.elsevier.com/content/abstract/scopus_id/85121931762,"In this paper, we proposed a convolutional neural network based on Inception and residual structure with an embedded modified convolutional block attention module (CBAM), aiming to improve the classification of plant leaf diseases. Corn, potatoes and tomatoes are the most cultivated grains in southern China. The leaves of the three crops are very fragile and sensitive and are susceptible to leaf diseases, such as leaf blight of corn, late blight of potato and mosaic virus of tomato. These diseases cannot be identified at early stages. Therefore, an efficient solution is proposed by deep learning techniques to detect the disease categories of crops, which can effectively prevent the spread of diseases and ensure the normal growth of plants. In this experiment, our model achieved an overall accuracy of 99.55% for the identification of the three diseases of corn, potato and tomato. In addition, we tested the three plants individually. The classification accuracy of our model on corn, potato and tomato was 98.44%, 99.43% and 95.20%, respectively. We have also developed a web-based real-time plant disease classification system and deployed our model. The system had good performance in time and accuracy evaluation metrics. The results of this study showed that our model had fewer parameters, shorter training time, and higher recognition accuracy compared to existing image classification models.",multimedia
10.1016/j.cose.2021.102539,preprocessed,Computers and Security,scopus,2022-02-01,sciencedirect,deep face fuzzy vault: implementation and performance,https://api.elsevier.com/content/abstract/scopus_id/85119176199,"Biometric technologies, especially face recognition, have become an essential part of identity management systems worldwide. In deployments of biometrics, secure storage of biometric information is necessary in order to protect the users’ privacy. In this context, biometric cryptosystems are designed to meet key requirements of biometric information protection enabling a privacy-preserving storage and comparison of biometric data, e.g. feature vectors extracted from facial images. Until now, biometric cryptosystems have hardly been applied to state-of-the-art biometric recognition systems utilizing deep convolutional neural networks.
                  This work investigates the application of a well-known biometric cryptosystem, i.e. the improved fuzzy vault scheme, to facial feature vectors extracted through deep convolutional neural networks. To this end, a feature transformation method is introduced which maps fixed-length real-valued deep feature vectors to integer-valued feature sets. As part of said feature transformation, a detailed analysis of different feature quantisation and binarisation techniques is conducted. At key binding, obtained feature sets are locked in an unlinkable improved fuzzy vault. For key retrieval, the efficiency of different polynomial reconstruction techniques is investigated. The proposed feature transformation method and template protection scheme are agnostic of the biometric characteristic and, thus, can be applied to virtually any biometric features computed by a deep neural network. In experiments, an unlinkable improved deep face fuzzy vault-based template protection scheme is constructed employing features extracted with a state-of-the-art deep convolutional neural network trained with the additive angular margin loss (ArcFace). For the best configuration, a false non-match rate below 1% at a false match rate of 0.01%, is achieved in cross-database experiments on the FERET and FRGCv2 face databases. On average, a security level of up to approximately 28 bits is obtained. This work presents an effective face-based fuzzy vault scheme providing privacy protection of facial reference data as well as digital key derivation from face.",multimedia
10.1016/j.eswa.2021.116073,preprocessed,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,efficient machine learning approach for volunteer eye-blink detection in real-time using webcam,https://api.elsevier.com/content/abstract/scopus_id/85117617175,"The progressive diminishment of motor capacities due to Amyotrophic Lateral Sclerosis (ALS) causes a severe communication deficit. The development of Alternative Communication software aids ALS patients in overcoming communication issues and the detection of communication signals plays a big role in this task. In this paper, volunteer eye-blinking is proposed as human–computer interaction signal and an intelligent Computer Vision detector was built for handling the captured data in real-time using a generic webcam. The eye-blink detection was treated as an extension of the eye-state classification, and the base pipeline used is delineated as follows: face detection, face alignment, region-of-interest (ROI) extraction, and eye-state classification. Furthermore, this pipeline was complemented with auxiliary models: a rotation compensator, a ROIs evaluator, and a moving average filter. Two new datasets were created: the Youtube Eye-state Classification (YEC) dataset, built from the AVSpeech dataset by extracting face images; and the Autonomus Blink Dataset (ABD), built completely as a result of the present work. The YEC allowed training the eye-classification task; ABD was specifically idealized taking into consideration volunteer eye-blinking detection. The proposed models, a Convolutional Neural Network (CNN) and a Support Vector Machine (SVM), were trained by the YEC dataset and performance evaluation experiments for both models were conducted across different databases: CeW, ZJU, Eyeblink, Talking Face (public datasets) and ABD. The impact of the proposed auxiliary models was evaluated and the CNN and SVM models were compared for the eye-state classification task. Promising results were obtained: 97.44% accuracy for the eye-state classification task on the CeW dataset and 92.63% F1-Score for the eye-blink detection task on the ABD dataset.",multimedia
10.1016/j.apacoust.2021.108439,preprocessed,Applied Acoustics,scopus,2022-01-15,sciencedirect,"camnet: a controllable acoustic model for efficient, expressive, high-quality text-to-speech",https://api.elsevier.com/content/abstract/scopus_id/85116891718,"Spoken language is becoming one of the key components of human–machine interaction, both to send information to the machine – e.g. voice control – and to receive from it – e.g. virtual assistants. In this scenario, text-to-speech (TTS) models have become an essential artificial intelligence capacity. Even though this interaction can be based on neutral style speech, generating speech with different styles, pitches and speaking rates may improve user experience. With this in view, this paper presents CAMNet, a controllable acoustic model for efficient, expressive, high-quality TTS. CAMNet is based on deep convolutional TTS (DCTTS), a state-of-art acoustic model which is efficient and produces neutral speech. DCTTS was first adapted to generate Bark cepstrum acoustic features in order to integrate well with the LPCNet (linear prediction coefficient) neural vocoder and to remove the reduction factor which demanded the presence of an upsampling network before the vocoder – i.e. the CAMNet output can be directly fed into LPCNet. Next, style transfer functionality was added by means of a novel characterisation of the prosodic information from the Bark cepstrum acoustic features and a new approach to inject this information into the convolutional layers. Finally, controllability is provided via a variational auto-encoder module which creates a smoothed disentangled latent space which allows interpolation and extrapolation of reference styles as well as independent and simultaneous control of two generative factors: pitch and speaking rate. Moreover, this controllability is implemented using a simple offset-based approach. To sum up, CAMNet is an efficient acoustic model which provides a simple but consistent controllability on coarse-grained expression, pitch and speaking rate while still providing high-quality synthesised speech.",multimedia
10.1016/j.buildenv.2021.108532,preprocessed,Building and Environment,scopus,2022-01-01,sciencedirect,personal thermal comfort models using digital twins: preference prediction with bim-extracted spatial–temporal proximity data from build2vec,https://api.elsevier.com/content/abstract/scopus_id/85119963452,"Conventional thermal preference prediction in buildings has limitations due to the difficulty in capturing all environmental and personal factors. New model features can improve the ability of a machine learning model to classify a person’s thermal preference. The spatial context of a building can provide information to models about the windows, walls, heating and cooling sources, air diffusers, and other factors that create micro-environments that influence thermal comfort. Due to spatial heterogeneity, it is impractical to position sensors at a high enough resolution to capture all conditions. This research aims to build upon an existing vector-based spatial model, called Build2Vec, for predicting spatial–temporal occupants’ indoor environmental preferences. Build2Vec utilizes the spatial data from the Building Information Model (BIM) and indoor localization in a real-world setting. This framework uses longitudinal intensive thermal comfort subjective feedback from smart watch-based ecological momentary assessments (EMA). The aggregation of these data is combined into a graph network structure (i.e., objects and relations) and used as input for a classification model to predict occupant thermal preference. The results of a test implementation show 14%–28% accuracy improvement over a set of baselines that use conventional thermal preference prediction input variables.",multimedia
10.1016/j.jobe.2021.103571,preprocessed,Journal of Building Engineering,scopus,2022-01-01,sciencedirect,an integrated building energy performance evaluation method: from parametric modeling to ga-nn based energy consumption prediction modeling,https://api.elsevier.com/content/abstract/scopus_id/85119285403,"Building energy performance evaluation, as an important process in a sustainable building design, has important consequences for global energy conservation and environmental protection. The traditional methods to perform this evaluation are usually time-consuming and computationally complex, and have high requirements for designers’ professional knowledge on architectural physics and software operation skills. To solve these problems and provide rapid, user-friendly, and more accurate prediction results, this study presents an efficient building energy performance evaluation method which integrates building information modeling, energy simulation, and energy consumption prediction together. This method follows a three-stage research framework: Stage 1 proposes a rapid 3D building energy modeling process according to the parameterized setting, Stage 2 generates numerous simulation results automatically by EnergyPlus, and Stage 3 develops the user-friendly building energy consumption prediction model with the help of the Genetic Algorithm-Neural Network (GA-NN) and provides the energy performance level of the building design after the prediction. A case study is carried out to present the overall process and verify the accuracy of the proposed three-stage building energy performance evaluation method. This study contributes to the improvement of both the extensive dataset establishment and the operational efficiency of building energy consumption prediction. It can provide designers with a real-time, user-friendly, and reliable building energy consumption prediction tool and an energy performance assessment basis in the design phase of construction projects.",multimedia
10.1016/j.eswa.2021.115973,preprocessed,Expert Systems with Applications,scopus,2022-01-01,sciencedirect,deep correlation mining for multi-task image clustering,https://api.elsevier.com/content/abstract/scopus_id/85116928779,"Multi-task clustering (MTC) aims to enhance the performance of each individual task by leveraging the correlation information among them. Existing MTC algorithms usually first extract the feature representations of each task and then learn the relationships among multiple tasks for clustering. However, the multi-task correlations are not embedded into the feature learning in existing MTC. In addition, many real applications, such as image clustering, always perform visual feature extraction and clustering assignment separately, which often results in local optimal clustering resolutions. In this study, an end-to-end MTC framework, named Deep correlation mining for Multi-Task image Clustering (DMTC), is proposed to explore multi-task correlations and conduct image clustering simultaneously. Specifically, DMTC consists of two sub-networks: a between-task network (B-net) and a within-task network (W-net), which learn the correlations among multiple tasks and the relationships in each individual task, respectively, based on a deep convolutional network. To optimize B-net, an optimization procedure is proposed as follows: (1) DMTC builds a pseudo-graph to discover similar samples among tasks and obtain the positive pairs of possible related tasks. (2) A discriminator is designed to calculate the mutual information between the deep and shallow representations of related tasks, which can estimate the relatedness between each pair of related tasks. After that, the trained parameters in B-net are transferred to the within-task networks (W-net) as their initialized parameters, in which the above optimization procedure is performed again to obtain the final cluster partition by end-to-end training. Experimental results on NUS-Wide, Caltech-256, Cifar-100 and Pascal VOC demonstrate that our proposed DMTC method
                        1
                     
                     
                        1
                        The source code is available in https://github.com/Xiaoqiang-Yan/DMTC.
                     compares favorably to the state-of-the-art methods.",multimedia
10.1016/j.autcon.2021.103996,preprocessed,Automation in Construction,scopus,2022-01-01,sciencedirect,implementation experiments on convolutional neural network training using synthetic images for 3d pose estimation of an excavator on real images,https://api.elsevier.com/content/abstract/scopus_id/85116888055,"Remote and descriptive visualization of spatio-temporal information of excavator activities may increase awareness about jobsite hazards and operational performance in earthwork operations. One of the emerging approaches to collect this information is to extract the 3D pose of an excavator from the video frames using a convolutional neural network (CNN). However, this method requires labeling the training datasets, which are difficult to prepare because of conditions unsuitable for installing the motion capture sensors. This study investigates the performance of a CNN for estimating the 3D pose when trained on a synthetic dataset. In particular, a kinematic constraint is proposed to update the model parameters efficiently during training. The results show that the proposed method estimated the 3D poses of a real excavator with an average pose error of 9.63°. Hence, the proposed data augmentation method could help address the training data issues and improves the learning of real data complexity.",multimedia
10.1016/j.sigpro.2021.108317,preprocessed,Signal Processing,scopus,2022-01-01,sciencedirect,selective fixed-filter active noise control based on convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85114690529,"Active noise control (ANC) technology is increasingly ubiquitous in wearable audio devices, or hearables. Owing to its low computational complexity, high robustness, and exemplary performance in dealing with dynamic noise, the fixed-coefficient control filter strategy plays a central role in portable ANC implementation. Unlike its traditional adaptive counterpart, the fixed-filter strategy is unable to attain optimal noise reduction for different types of noise. Hence, we propose a selective fixed-filter ANC method based on a simplified two-dimensional convolution neural network (2D CNN), which is implemented on a co-processor (e.g., in a mobile phone), to derive the most suitable control filter for different noise types. To further reduce classification complexity, we designed a lightweight one-dimensional CNN (1D CNN), which can directly classify noise types in time domain. A numerical simulation based on measured paths in headphones demonstrates the proposed algorithm’s efficacy in attenuating real-world non-stationary noise over conventional adaptive algorithms.",multimedia
10.1016/j.csl.2021.101275,preprocessed,Computer Speech and Language,scopus,2022-01-01,sciencedirect,feature learning for efficient asr-free keyword spotting in low-resource languages,https://api.elsevier.com/content/abstract/scopus_id/85113158279,"We consider feature learning for a computationally efficient method of keyword spotting that can be applied in severely under-resourced settings. The objective is to support humanitarian relief programmes by the United Nations (UN) in parts of Africa in which almost no language resources are available. To allow a keyword spotting system to be rapidly developed in such a language, we rely on a small and easily-compiled set of isolated keywords. Using the isolated keywords as templates, we apply dynamic time warping (DTW) to a much larger corpus of in-domain but untranscribed speech. The resulting DTW alignment scores are used to train a convolutional neural network (CNN) which is orders of magnitude more computationally efficient than DTW and therefore suitable for real-time application. We optimise this ASR-free neural network keyword spotting procedure by identifying acoustic features that provide robust performance in this almost zero-resource setting. First, we consider the benefits of incorporating information from well-resourced but unrelated languages by incorporating a multilingual bottleneck feature (BNF) extractor. Next, we consider using features extracted from an autoencoder (AE) trained on in-domain but untranscribed data. Finally, we consider features obtained from a correspondence autoencoder (CAE) which is initialised with the AE and subsequently fine-tuned on the small set of in-domain labelled data. Experiments in South African English and Luganda, a low-resource language, demonstrate that, on their own, both the BNF and CAE features can achieve a 5% relative performance improvement over baseline MFCCs. However, by using BNFs as input to the CAE, even better performance is achieved, resulting in a more than 27% relative improvement over MFCCs in ROC area-under-the-curve (AUC) and more than twice as many top-10 retrievals. We also show that, using these features, the CNN-DTW keyword spotter performs almost as well as the DTW keyword spotter while comfortably outperforming a baseline CNN trained only on the keyword templates. We conclude that a CNN-DTW keyword spotter using BNF-derived CAE features represents a computationally efficient approach with very competitive performance that is suited to rapid deployment in a severely under-resourced scenario.",multimedia
10.1016/j.petrol.2021.109332,preprocessed,Journal of Petroleum Science and Engineering,scopus,2022-01-01,sciencedirect,end-to-end neural network approach to 3d reservoir simulation and adaptation,https://api.elsevier.com/content/abstract/scopus_id/85112357560,"Reservoir simulation and adaptation (also known as history matching) are typically considered as separate problems. While a set of models are aimed at the solution of the forward simulation problem assuming all initial geological parameters are known, the other set of models adjust geological parameters under the fixed forward simulation model to fit production data. This results in many difficulties for both reservoir engineers and developers of new efficient computation schemes. We present a unified approach to reservoir simulation and adaptation problems. A single neural network model allows a forward pass from initial geological parameters of the 3D reservoir model through dynamic state variables to well’s production rates and backward gradient propagation to any model inputs and variables. The model fitting and geological parameters adaptation both become the optimization problem over specific parts of the same neural network model. Standard gradient-based optimization schemes can be used to find the optimal solution. Using real-world oilfield model and historical production rates we demonstrate that the suggested approach allows reservoir simulation and history matching with a benefit of several orders of magnitude simulation speed-up. Finally, to propagate this research we open-source a Python-based framework DeepField that allows standard processing of reservoir models and reproducing the approach presented in this paper.",multimedia
10.1016/j.patcog.2021.108205,preprocessed,Pattern Recognition,scopus,2022-01-01,sciencedirect,tracking more than 100 arbitrary objects at 25 fps through deep learning,https://api.elsevier.com/content/abstract/scopus_id/85111593606,"Most video analytics applications rely on object detectors to localize objects in frames. However, when real-time is a requirement, running the detector at all the frames is usually not possible. This is somewhat circumvented by instantiating visual object trackers between detector calls, but this does not scale with the number of objects. To tackle this problem, we present SiamMT, a new deep learning multiple visual object tracking solution that applies single-object tracking principles to multiple arbitrary objects in real-time. To achieve this, SiamMT reuses feature computations, implements a novel crop-and-resize operator, and defines a new and efficient pairwise similarity operator. SiamMT naturally scales up to several dozens of targets, reaching 25 fps with 122 simultaneous objects for VGA videos, or up to 100 simultaneous objects in HD720 video. SiamMT has been validated on five large real-time benchmarks, achieving leading performance against current state-of-the-art trackers.",multimedia
10.1109/access.2022.3147955,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,how to boost the performance of recommender systems by social trust? studying the challenges and proposing a solution,https://ieeexplore.ieee.org/document/9698036/,"With the increasing number of items in electronic retailers, news websites, etc., finding interesting items concerning the taste of users is becoming more challenging. Recommender Systems (RS) are a well-known solution to this issue. Collaborative filtering (CF) is a widely accepted and popular technique to implement an RS. However, cold-start and data sparsity problems reduce the performance of CF methods. One promising solution for these issues is to use the social trust information. However, how to properly use social trust information is a hot and still open question. In this paper, we propose a similarity measure and a simple link prediction method to address this question and employ them in trust-aware matrix factorization. Especially, our proposed similarity measure is asymmetric to consider the nature of social relationships. Also, to have a more accurate similarity estimation, we have considered both the user’s historical ratings and trust relations, and we have determined the weight of each source. Finally, we have used the item-based model and the level of interest a user’s trustee have for an item to improve the performance of the proposed method for sparse datasets. We conduct extensive performance evaluations in terms of rate prediction and interesting items found. Experimental results on three real-world datasets demonstrate the effectiveness of the proposed method, especially in terms of Mean Absolute Error.",science
10.1109/ccnc49033.2022.9700613,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,first experimental results on real-time cleaning activity monitoring system,https://ieeexplore.ieee.org/document/9700613/,"The COVID-19 pandemic has presented social challenges to establish the new normal lifestyle in our daily lives. The goal of this paper is to enable easy and low-cost monitoring of cleaning activity to keep a clean environment for preventing infection. Although human activity recognition has been a hot research topic in pervasive computing, existing schemes have not been optimized for monitoring cleaning activities. To address this issue, this paper provides an initial concept and preliminary experimental results of cleaning activity recognition using accelerometer data and RFID tags. In the proposed scheme, machine learning technologies and short range wireless communication are employed for recognizing the time and place of wiping as an example of cleaning activities, because it is an important activity for shared places to avoid infection. This paper reports the evaluation results on the recognition accuracy using the proof-of-concept (PoC) implementation to clarify the required sampling rate and time-window size for further experiments. Also, a real-time feedback system is implemented to provide the monitoring results for users. The proposed scheme contributes for efficient monitoring of cleaning activities for creating the new normal era.",science
10.1109/wacv51458.2022.00391,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,unveiling real-life effects of online photo sharing,https://ieeexplore.ieee.org/document/9706741/,"Social networks give free access to their services in exchange for the right to exploit their users’ data. Data sharing is done in an initial context which is chosen by the users. However, data are used by social networks and third parties in different contexts which are often not transparent. In order to unveil such usages, we propose an approach which focuses on the effects of data sharing in impactful real-life situations. Focus is put on visual content because of its strong influence in shaping online user profiles. The approach relies on three components: (1) a set of visual objects with associated situation impact ratings obtained by crowdsourcing, (2) a corresponding set of object detectors for mining users’ photos and (3) a ground truth dataset made of 500 visual user profiles which are manually rated per situation. These components are combined in LERV UP, a method which learns to rate visual user profiles in each situation. LERV UP exploits a new image descriptor which aggregates object ratings and object detections at user level and an attention mechanism which boosts highly-rated objects to prevent them from being overwhelmed by low-rated ones. Performance is evaluated per situation by measuring the correlation between the automatic ranking of profile ratings and a manual ground truth. Results indicate that LERV UP is effective since a strong correlation of the two rankings is obtained. A practical implementation of the approach in a mobile app which raises user awareness about shared data usage is also discussed.",science
10.1109/access.2021.3137636,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a deep learning-based framework for phishing website detection,https://ieeexplore.ieee.org/document/9661323/,"Phishing attackers spread phishing links through e-mail, text messages, and social media platforms. They use social engineering skills to trick users into visiting phishing websites and entering crucial personal information. In the end, the stolen personal information is used to defraud the trust of regular websites or financial institutions to obtain illegal benefits. With the development and applications of machine learning technology, many machine learning-based solutions for detecting phishing have been proposed. Some solutions are based on the features extracted by rules, and some of the features need to rely on third-party services, which will cause instability and time-consuming issues in the prediction service. In this paper, we propose a deep learning-based framework for detecting phishing websites. We have implemented the framework as a browser plug-in capable of determining whether there is a phishing risk in real-time when the user visits a web page and gives a warning message. The real-time prediction service combines multiple strategies to improve accuracy, reduce false alarm rates, and reduce calculation time, including whitelist filtering, blacklist interception, and machine learning (ML) prediction. In the ML prediction module, we compared multiple machine learning models using several datasets. From the experimental results, the RNN-GRU model obtained the highest accuracy of 99.18%, demonstrating the feasibility of the proposed solution.",science
10.1109/access.2021.3140175,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"a metaverse: taxonomy, components, applications, and open challenges",https://ieeexplore.ieee.org/document/9667507/,"Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to connectivity with reality using virtual currency. The integration of enhanced social activities and neural-net methods requires a new definition of Metaverse suitable for the present, different from the previous Metaverse. This paper divides the concepts and essential techniques necessary for realizing the Metaverse into three components (i.e., hardware, software, and contents) and three approaches (i.e., user interaction, implementation, and application) rather than marketing or hardware approach to conduct a comprehensive analysis. Furthermore, we describe essential methods based on three components and techniques to Metaverse’s representative Ready Player One, Roblox, and Facebook research in the domain of films, games, and studies. Finally, we summarize the limitations and directions for implementing the immersive Metaverse as social influences, constraints, and open challenges.",science
10.1109/tc.2021.3057082,preprocessed,IEEE Transactions on Computers,IEEE,2022-03-01 00:00:00,ieeexplore,<sc>lime</sc>: low-cost and incremental learning for dynamic heterogeneous information networks,https://ieeexplore.ieee.org/document/9353275/,"Understanding the interconnected relationships of large-scale information networks like social, scholar and Internet of Things networks is vital for tasks like recommendation and fraud detection. The vast majority of the real-world networks are inherently heterogeneous and dynamic, containing many different types of nodes and edges and can change drastically over time. The dynamicity and heterogeneity make it extremely challenging to reason about the network structure. Unfortunately, existing approaches are inadequate in modeling real-life dynamical networks as they either have strong assumption of a given stochastic process or fail to capture the heterogeneity of network structure, and they all require extensive computational resources. We introduce <sc>Lime</sc>, a better approach for modeling dynamic and heterogeneous information networks. <sc>Lime</sc> is designed to extract high-quality network representation with significantly lower memory resources and computational time over the state-of-the-arts. Unlike prior work that uses a vector to encode each network node, we exploit the semantic relationships among network nodes to encode multiple nodes with similar semantics in shared vectors. By using many fewer node vectors, our approach significantly reduces the required memory space for encoding large-scale networks. To effectively trade information sharing for reduced memory footprint, we employ the recursive neural network (RsNN) with carefully designed optimization strategies to explore the node semantics in a novel cuboid space. We then go further by showing, for the first time, how an effective incremental learning approach can be developed – with the help of RsNN, our cuboid structure, and a set of novel optimization techniques – to allow a learning framework to quickly and efficiently adapt to a constantly evolving network. We evaluate <sc>Lime</sc> by applying it to three representative network-based tasks, node classification, node clustering and anomaly detection, performing on three large-scale datasets. We compare <sc>Lime</sc> against eleven prior state-of-the-art approaches for learning network representation. Our extensive experiments demonstrate that <sc>Lime</sc> not only reduces the memory footprint by over 80 percent and the processing time over 2x when learning network representation but also delivers comparable performance for downstream processing tasks. We show that our incremental learning method can boost the learning time by up to 20x without compromising the quality of the learned network representation.",science
10.1109/tifs.2021.3131026,preprocessed,IEEE Transactions on Information Forensics and Security,IEEE,2000-01-01 00:00:00,ieeexplore,poligraph: intrusion-tolerant and distributed fake news detection system,https://ieeexplore.ieee.org/document/9627681/,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally (<inline-formula> <tex-math notation=""LaTeX"">${4\%}$ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">${7\%}$ </tex-math></inline-formula>) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach.",science
10.1109/tii.2021.3117861,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-05-01 00:00:00,ieeexplore,toward fairness-aware time-sensitive asynchronous federated learning for critical energy infrastructure,https://ieeexplore.ieee.org/document/9560090/,"Critical energy infrastructure (CEI) systems are vital to underpin the national economy and social development, but vulnerable to cyber attack and data privacy leakage when distributed machine learning technologies are deployed on them. Although federated learning (FL) has promoted distributed collaborative learning while keeping natural compliance with the privacy protection, it is tremendously difficult to schedule edge nodes of CEI collaboratively when asynchronous FL tasks are applied in CEI system, since the CEI system must make an irrevocable immediate decision on whether to hire a participant who arrives and departs dynamically without knowing future information. In this article, we tackle this issue by designing fairness-aware and time-sensitive task allocation mechanisms in asynchronous FL for CEI. First, we design an optimal multidimensional contract to guarantee the reliability, honesty, and fairness, and maximize the learning accuracy for the fixed deadline scenario. Second, we design a multimetric participant recruitment mechanism to control time consumption for the limited budget scenario, prove that the problem of optimizing this mechanism is NP-hard, and propose an <inline-formula><tex-math notation=""LaTeX"">$e$</tex-math></inline-formula>-approximation algorithm accordingly. Finally, extensive experiments using both real-world data and simulated data further demonstrate the effectiveness and efficiency of our proposed mechanisms compared to the state-of-the-art approaches.",science
10.1109/tgrs.2021.3087186,preprocessed,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,3-d gabor convolutional neural network for hyperspectral image classification,https://ieeexplore.ieee.org/document/9460777/,"Due to the detailed spectral information through hundreds of narrow spectral bands provided by hyperspectral image (HSI) data, it can be employed to accurately classify diverse materials of interest, which is one of the core applications of hyperspectral remote sensing technology. In recent years, with the rapid development of deep learning, convolutional neural networks (CNNs) have been successfully applied in many fields, including HSI classification. However, the random gradient descent-based parameter updating scheme is too general and leading to the inefficiency of CNN models. Moreover, the high dimensionality and limited training samples of HSI data also exacerbate the overfitting problem. To tackle these issues, in this article, a novel deep network with multilayer and multibranch architecture, named 3-D Gabor CNN (3DG-CNN), is proposed for HSI classification. More precisely, since the predefined 3-D Gabor filters in multiple scales and orientations could well characterize the internal spatial–spectral structure of HSI data from various perspectives, the 3-D Gabor-modulated kernels (3-D GMKs) are employed to replace the random initialization kernels. Moreover, the specially designed multibranch architecture enables the network to better integrating the scalable property of 3-D Gabor filters; thus, the representative ability and robustness of the extracted features can be greatly improved. Alternatively, the number of network parameters is substantially reduced due to the incorporation of 3-D Gabor modulation, relieving the training complexity and also alleviating the training process from overfitting. Experimental results on four real HSI datasets (including two newly released ones in the literature) have demonstrated that the proposed 3DG-CNN model can achieve better performance than several widely used machine-learning-based and deep-learning-based approaches. For the sake of reproducibility, the codes of the proposed 3DG-CNN model are available at <uri>http://jiasen.tech/papers/</uri>.",science
10.1109/tcbb.2021.3082915,preprocessed,IEEE/ACM Transactions on Computational Biology and Bioinformatics,IEEE,2022-02-01 00:00:00,ieeexplore,graphplas: refined classification of plasmid sequences using assembly graphs,https://ieeexplore.ieee.org/document/9439922/,"Plasmids are extra-chromosomal genetic materials with important markers that affect the function and behaviour of the microorganisms supporting their environmental adaptations. Hence the identification and recovery of such plasmid sequences from assemblies is a crucial task in metagenomics analysis. In the past, machine learning approaches have been developed to separate chromosomes and plasmids. However, there is always a compromise between precision and recall in the existing classification approaches. The similarity of compositions between chromosomes and their plasmids makes it difficult to separate plasmids and chromosomes with high accuracy. However, high confidence classifications are accurate with a significant compromise of recall, and vice versa. Hence, the requirement exists to have more sophisticated approaches to separate plasmids and chromosomes accurately while retaining an acceptable trade-off between precision and recall. We present GraphPlas, a novel approach for plasmid recovery using coverage, composition and assembly graph topology. We evaluated GraphPlas on simulated and real short read assemblies with varying compositions of plasmids and chromosomes. Our experiments show that GraphPlas is able to significantly improve accuracy in detecting plasmid and chromosomal contigs on top of popular state-of-the-art plasmid detection tools. <p>The source code is freely available at: <uri>https://github.com/anuradhawick/GraphPlas</uri>.</p>",science
10.1109/lra.2022.3140793,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,vision-based self-adaptive gripping in a trimodal robotic sorting end-effector,https://ieeexplore.ieee.org/document/9672720/,"Recyclable waste management, which includes sorting as a key process, is a crucial component of maintaining a sustainable ecosystem. The use of robots in sorting could significantly facilitate the production of secondary raw materials from waste in the sense of a recycling economy. However, due to the complex and heterogeneous types of the recyclable items, the conventional robotic gripping end-effectors, which typically come with a fixed structure, are unlikely to hold onto the full range of items to enable separation and recycling. To this end, a trimodal adaptive end-effector is proposed that can be integrated with robotic manipulators to improve their gripping versatility. The end-effector can deploy effective modes of gripping to different objects in response to their size and porosity via gripping mechanisms based on Nano Polyurethane (PU) adhesive gels, pumpless vacuum suction, and radially deployable claws. While the end-effector's mechanical design allows the three gripping modes to be deployed independently or in conjunction with one another, this work aims at deploying modes that are effective for gripping onto the recyclable item. In order to decide on the suitable modes of gripping a real-time vision system is designed to measure the size and porosity of the recyclable items and advise on a suitable combination of gripping modes to be deployed. Integrated current sensors provide an indication of successful gripping and releasing of the recyclable items. The results of the experiments confirmed the ability of our vision-based approach in identifying suitable gripping modes in real-time, the deployment of the relevant mechanisms and successful gripping onto a maximum of 84.8% (single-mode), 90.9% (dual-mode) and 96.9% (triple-mode) of a specified set of recyclable items.",science
http://arxiv.org/abs/2202.10335v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,explainability in machine learning: a pedagogical perspective,http://arxiv.org/abs/2202.10335v1,"Given the importance of integrating of explainability into machine learning,
at present, there are a lack of pedagogical resources exploring this.
Specifically, we have found a need for resources in explaining how one can
teach the advantages of explainability in machine learning. Often pedagogical
approaches in the field of machine learning focus on getting students prepared
to apply various models in the real world setting, but much less attention is
given to teaching students the various techniques one could employ to explain a
model's decision-making process. Furthermore, explainability can benefit from a
narrative structure that aids one in understanding which techniques are
governed by which questions about the data.
  We provide a pedagogical perspective on how to structure the learning process
to better impart knowledge to students and researchers in machine learning,
when and how to implement various explainability techniques as well as how to
interpret the results. We discuss a system of teaching explainability in
machine learning, by exploring the advantages and disadvantages of various
opaque and transparent machine learning models, as well as when to utilize
specific explainability techniques and the various frameworks used to structure
the tools for explainability. Among discussing concrete assignments, we will
also discuss ways to structure potential assignments to best help students
learn to use explainability as a tool alongside any given machine learning
application.
  Data science professionals completing the course will have a birds-eye view
of a rapidly developing area and will be confident to deploy machine learning
more widely. A preliminary analysis on the effectiveness of a recently
delivered course following the structure presented here is included as evidence
supporting our pedagogical approach.",science
http://arxiv.org/abs/2202.10144v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"inferring network structure with unobservable nodes from time series
  data",http://arxiv.org/abs/2202.10144v1,"Network structures play important roles in social, technological and
biological systems. However, the observable nodes and connections in real cases
are often incomplete or unavailable due to measurement errors, private
protection issues, or other problems. Therefore, inferring the complete network
structure is useful for understanding human interactions and complex dynamics.
The existing studies have not fully solved the problem of inferring network
structure with partial information about connections or nodes. In this paper,
we tackle the problem by utilizing time-series data generated by network
dynamics. We regard the network inference problem based on dynamical time
series data as a problem of minimizing errors for predicting states of
observable nodes and proposed a novel data-driven deep learning model called
Gumbel-softmax Inference for Network (GIN) to solve the problem under
incomplete information. The GIN framework includes three modules: a dynamics
learner, a network generator, and an initial state generator to infer the
unobservable parts of the network. We implement experiments on artificial and
empirical social networks with discrete and continuous dynamics. The
experiments show that our method can infer the unknown parts of the structure
and the initial states of the observable nodes with up to 90\% accuracy. The
accuracy declines linearly with the increase of the fractions of unobservable
nodes. Our framework may have wide applications where the network structure is
hard to obtain and the time series data is rich.",science
http://arxiv.org/abs/2202.08450v1,preprocessed,arxiv,arxiv,2022-02-17 00:00:00,arxiv,"design-bench: benchmarks for data-driven offline model-based
  optimization",http://arxiv.org/abs/2202.08450v1,"Black-box model-based optimization (MBO) problems, where the goal is to find
a design input that maximizes an unknown objective function, are ubiquitous in
a wide range of domains, such as the design of proteins, DNA sequences,
aircraft, and robots. Solving model-based optimization problems typically
requires actively querying the unknown objective function on design proposals,
which means physically building the candidate molecule, aircraft, or robot,
testing it, and storing the result. This process can be expensive and time
consuming, and one might instead prefer to optimize for the best design using
only the data one already has. This setting -- called offline MBO -- poses
substantial and different algorithmic challenges than more commonly studied
online techniques. A number of recent works have demonstrated success with
offline MBO for high-dimensional optimization problems using high-capacity deep
neural networks. However, the lack of standardized benchmarks in this emerging
field is making progress difficult to track. To address this, we present
Design-Bench, a benchmark for offline MBO with a unified evaluation protocol
and reference implementations of recent methods. Our benchmark includes a suite
of diverse and realistic tasks derived from real-world optimization problems in
biology, materials science, and robotics that present distinct challenges for
offline MBO. Our benchmark and reference implementations are released at
github.com/rail-berkeley/design-bench and
github.com/rail-berkeley/design-baselines.",science
http://arxiv.org/abs/2202.07785v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,predictability and surprise in large generative models,http://arxiv.org/abs/2202.07785v1,"Large-scale pre-training has recently emerged as a technique for creating
capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG,
Gopher, and many others. In this paper, we highlight a counterintuitive
property of such models and discuss the policy implications of this property.
Namely, these generative models have an unusual combination of predictable loss
on a broad training distribution (as embodied in their ""scaling laws""), and
unpredictable specific capabilities, inputs, and outputs. We believe that the
high-level predictability and appearance of useful capabilities drives rapid
development of such models, while the unpredictable qualities make it difficult
to anticipate the consequences of model deployment. We go through examples of
how this combination can lead to socially harmful behavior with examples from
the literature and real world observations, and we also perform two novel
experiments to illustrate our point about harms from unpredictability.
Furthermore, we analyze how these conflicting properties combine to give model
developers various motivations for deploying these models, and challenges that
can hinder deployment. We conclude with a list of possible interventions the AI
community may take to increase the chance of these models having a beneficial
impact. We intend this paper to be useful to policymakers who want to
understand and regulate AI systems, technologists who care about the potential
policy impact of their work, and academics who want to analyze, critique, and
potentially develop large generative models.",science
http://arxiv.org/abs/2202.07475v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"a real-time system for detecting landslide reports on social media using
  artificial intelligence",http://arxiv.org/abs/2202.07475v1,"This paper presents an online system that leverages social media data in real
time to identify landslide-related information automatically using
state-of-the-art artificial intelligence techniques. The designed system can
(i) reduce the information overload by eliminating duplicate and irrelevant
content, (ii) identify landslide images, (iii) infer geolocation of the images,
and (iv) categorize the user type (organization or person) of the account
sharing the information. The system was deployed in February 2020 online at
https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter
data stream and has been running continuously since then to provide
time-critical information to partners such as British Geological Survey and
European Mediterranean Seismological Centre. We trust this system can both
contribute to harvesting of global landslide data for further research and
support global landslide maps to facilitate emergency response and decision
making.",science
http://arxiv.org/abs/2202.00617v1,preprocessed,arxiv,arxiv,2022-02-01 00:00:00,arxiv,"a general, evolution-inspired reward function for social robotics",http://arxiv.org/abs/2202.00617v1,"The field of social robotics will likely need to depart from a paradigm of
designed behaviours and imitation learning and adopt modern reinforcement
learning (RL) methods to enable robots to interact fluidly and efficaciously
with humans. In this paper, we present the Social Reward Function as a
mechanism to provide (1) a real-time, dense reward function necessary for the
deployment of RL agents in social robotics, and (2) a standardised objective
metric for comparing the efficacy of different social robots. The Social Reward
Function is designed to closely mimic those genetically endowed social
perception capabilities of humans in an effort to provide a simple, stable and
culture-agnostic reward function. Presently, datasets used in social robotics
are either small or significantly out-of-domain with respect to social
robotics. The use of the Social Reward Function will allow larger in-domain
datasets to be collected close to the behaviour policy of social robots, which
will allow both further improvements to reward functions and to the behaviour
policies of social robots. We believe this will be the key enabler to
developing efficacious social robots in the future.",science
http://arxiv.org/abs/2201.08475v1,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,gengnn: a generic fpga framework for graph neural network acceleration,http://arxiv.org/abs/2201.08475v1,"Graph neural networks (GNNs) have recently exploded in popularity thanks to
their broad applicability to ubiquitous graph-related problems such as quantum
chemistry, drug discovery, and high energy physics. However, meeting demand for
novel GNN models and fast inference simultaneously is challenging because of
the gap between the difficulty in developing efficient FPGA accelerators and
the rapid pace of creation of new GNN models. Prior art focuses on the
acceleration of specific classes of GNNs but lacks the generality to work
across existing models or to extend to new and emerging GNN models. In this
work, we propose a generic GNN acceleration framework using High-Level
Synthesis (HLS), named GenGNN, with two-fold goals. First, we aim to deliver
ultra-fast GNN inference without any graph pre-processing for real-time
requirements. Second, we aim to support a diverse set of GNN models with the
extensibility to flexibly adapt to new models. The framework features an
optimized message-passing structure applicable to all models, combined with a
rich library of model-specific components. We verify our implementation
on-board on the Xilinx Alveo U50 FPGA and observe a speed-up of up to 25x
against CPU (6226R) baseline and 13x against GPU (A6000) baseline. Our HLS code
will be open-source on GitHub upon acceptance.",science
http://arxiv.org/abs/2201.03413v1,preprocessed,arxiv,arxiv,2022-01-10 00:00:00,arxiv,systems challenges for trustworthy embodied systems,http://arxiv.org/abs/2201.03413v1,"A new generation of increasingly autonomous and self-learning systems, which
we call embodied systems, is about to be developed. When deploying these
systems into a real-life context we face various engineering challenges, as it
is crucial to coordinate the behavior of embodied systems in a beneficial
manner, ensure their compatibility with our human-centered social values, and
design verifiably safe and reliable human-machine interaction. We are arguing
that raditional systems engineering is coming to a climacteric from embedded to
embodied systems, and with assuring the trustworthiness of dynamic federations
of situationally aware, intent-driven, explorative, ever-evolving, largely
non-predictable, and increasingly autonomous embodied systems in uncertain,
complex, and unpredictable real-world contexts. We are also identifying a
number of urgent systems challenges for trustworthy embodied systems, including
robust and human-centric AI, cognitive architectures, uncertainty
quantification, trustworthy self-integration, and continual analysis and
assurance.",science
http://arxiv.org/abs/2201.03550v1,preprocessed,arxiv,arxiv,2022-01-09 00:00:00,arxiv,"machine learning enabling high-throughput and remote operations at
  large-scale user facilities",http://arxiv.org/abs/2201.03550v1,"Imaging, scattering, and spectroscopy are fundamental in understanding and
discovering new functional materials. Contemporary innovations in automation
and experimental techniques have led to these measurements being performed much
faster and with higher resolution, thus producing vast amounts of data for
analysis. These innovations are particularly pronounced at user facilities and
synchrotron light sources. Machine learning (ML) methods are regularly
developed to process and interpret large datasets in real-time with
measurements. However, there remain conceptual barriers to entry for the
facility general user community, whom often lack expertise in ML, and technical
barriers for deploying ML models. Herein, we demonstrate a variety of
archetypal ML models for on-the-fly analysis at multiple beamlines at the
National Synchrotron Light Source II (NSLS-II). We describe these examples
instructively, with a focus on integrating the models into existing
experimental workflows, such that the reader can easily include their own ML
techniques into experiments at NSLS-II or facilities with a common
infrastructure. The framework presented here shows how with little effort,
diverse ML models operate in conjunction with feedback loops via integration
into the existing Bluesky Suite for experimental orchestration and data
management.",science
http://arxiv.org/abs/2201.02734v1,preprocessed,arxiv,arxiv,2022-01-02 00:00:00,arxiv,building human-like communicative intelligence: a grounded perspective,http://arxiv.org/abs/2201.02734v1,"Modern Artificial Intelligence (AI) systems excel at diverse tasks, from
image classification to strategy games, even outperforming humans in many of
these domains. After making astounding progress in language learning in the
recent decade, AI systems, however, seem to approach the ceiling that does not
reflect important aspects of human communicative capacities. Unlike human
learners, communicative AI systems often fail to systematically generalize to
new data, suffer from sample inefficiency, fail to capture common-sense
semantic knowledge, and do not translate to real-world communicative
situations. Cognitive Science offers several insights on how AI could move
forward from this point. This paper aims to: (1) suggest that the dominant
cognitively-inspired AI directions, based on nativist and symbolic paradigms,
lack necessary substantiation and concreteness to guide progress in modern AI,
and (2) articulate an alternative, ""grounded"", perspective on AI advancement,
inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research.
I review results on 4E research lines in Cognitive Science to distinguish the
main aspects of naturalistic learning conditions that play causal roles for
human language development. I then use this analysis to propose a list of
concrete, implementable components for building ""grounded"" linguistic
intelligence. These components include embodying machines in a
perception-action cycle, equipping agents with active exploration mechanisms so
they can build their own curriculum, allowing agents to gradually develop motor
abilities to promote piecemeal language development, and endowing the agents
with adaptive feedback from their physical and social environment. I hope that
these ideas can direct AI research towards building machines that develop
human-like language abilities through their experiences with the world.",science
10.1016/j.engstruct.2021.113824,preprocessed,Engineering Structures,scopus,2022-02-15,sciencedirect,"explainable machine learning using real, synthetic and augmented fire tests to predict fire resistance and spalling of rc columns",https://api.elsevier.com/content/abstract/scopus_id/85122261194,"This paper presents the development of systematic machine learning (ML) approach to enable explainable and rapid assessment of fire resistance and fire-induced spalling of reinforced concrete (RC) columns. The developed approach comprises an ensemble of three novel ML algorithms namely; random forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL). These algorithms are trained to account for a wide collection of geometric characteristics and material properties, as well as loading conditions to examine fire performance of normal and high strength RC columns by analyzing a comprehensive database of fire tests comprising of over 494 observations. The developed ensemble is also capable of presenting quantifiable insights to ML predictions; thus, breaking free from the notion of “black-box” ML and establishing a solid step towards transparent and explainable ML. Most importantly, this work tackles the scarcity of available fire tests by proposing new techniques to leverage the use of real, synthetic, and augmented fire test observations. The developed ML ensemble has been calibrated and validated for standard and design fire exposures and one-, two-, three- and four-sided fire exposures thus; covering a wide range of practical scenarios present during fire incidents. When fully deployed, the developed ensemble can analyze over 5,000 RC columns in under 60 s; thus, providing an attractive solution for researchers and practitioners. The presented approach can also be easily extended for evaluating fire resistance and spalling of other structural members under varying fire scenarios and loading conditions and hence paves the way to modernize the state of this research area and practice.",science
10.1016/j.comcom.2021.11.011,preprocessed,Computer Communications,scopus,2022-02-01,sciencedirect,ran energy efficiency and failure rate through ann traffic predictions processing,https://api.elsevier.com/content/abstract/scopus_id/85120657977,"In this paper, we focus on the application of ML tools to resource management in a portion of a Radio Access Network (RAN) and, in particular, to Base Station (BS) activation and deactivation, aiming at reducing energy consumption while providing enough capacity to satisfy the variable traffic demand generated by end users. In order to properly decide on BS (de)activation, traffic predictions are needed, and Artificial Neural Networks (ANN) are used for this purpose. Since critical BS (de)activation decisions are not taken in proximity of minima and maxima of the traffic patterns, high accuracy in the traffic estimation is not required at those times, but only close to the times when a decision is taken. This calls for careful processing of the ANN traffic predictions to increase the probability of correct decision. Numerical performance results in terms of energy saving and traffic lost due to incorrect BS deactivations are obtained by simulating algorithms for traffic predictions processing, using real traffic as input. Results suggest that good performance trade-offs can be achieved even in presence of non-negligible traffic prediction errors, if these forecasts are properly processed. The impact of forecast processing for dynamic resource allocation on the BS failure rate is also investigated. Results reveal that conservative approaches better prevent BSs from hardware failure. Nevertheless, the deployment of newer devices, designed for fast dynamic networks, allows the adoption of approaches which frequently activate and deactivate BSs, thus achieving higher energy saving.",science
10.1016/j.simpat.2021.102446,preprocessed,Simulation Modelling Practice and Theory,scopus,2022-02-01,sciencedirect,modelling argumentation in short text: a case of social media debate,https://api.elsevier.com/content/abstract/scopus_id/85120649661,"The technological leaps of artificial intelligence (AI) and the rise of machine learning have triggered significant progress in a plethora of natural language processing (NLP) and natural language understanding tasks. One of these tasks is argumentation mining which has received significant interest in recent years and is regarded as a key domain for future decision-making systems, behaviour modelling, and natural language understanding problems. Until recently, natural language modelling tasks, such as computational argumentation schemes, were often tested in controlled environments, such as persuasive essays, reducing unexpected behaviours that could occur in real-life settings, like a public debate on social media. Additionally, the growing demand for enhancing the trust and the explainability of the AI services has dictated the design and adoption of modelling schemes to increase the confidence in the outcomes of the AI solutions. This paper attempts to explore modelling argumentation in short text and proposes a novel framework for argumentation detection under the name Abstract Framework for Argumentation Detection (AFAD). Moreover, different proof-of-concept implementations are provided to examine the applicability of the proposed framework to very short text developing a rule-based mechanism and compare the results with data-driven solutions. Eventually, a combination of the deployed methods is applied increasing the correct predictions in the minority class on an imbalanced dataset. The findings suggest that the modelling process provides solid grounds for technical research while the hybrid solutions have the potential to be applied to a wide range of NLP-related tasks offering a deeper understanding of human language and reasoning.",science
10.1016/j.apenergy.2021.118085,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,ship energy management system development and experimental evaluation utilizing marine loading cycles based on machine learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85120648001,"In order to develop energy management systems for hybrid ship propulsion plants that are truly optimal and robust, it is important that the test conditions in experimental facilities are as close as possible to real world applications. In this context, a framework for the design and experimental evaluation of power-split control systems for ship propulsion is proposed. Using machine learning, data from ship operation are processed and 20 loading patterns are recognized; representative templates are extracted to be used as marine loading cycles in the energy management system development and testing. A ship propulsion model with wave disturbance is utilized to simulate realistic loading scenarios on the experimental facility. A predictive energy management system is presented, that controls the diesel engine and the electric motor/generator based on a strategy that defines the trade-off between fuel consumption and NOx emissions minimization. In addition the propeller load characteristics that are estimated and a speed predictor are utilized to aid the optimization within the 10 s prediction time window. A parametric simulation study is performed for the trade-off evaluation between fuel consumption and NOx emissions reduction potential of the control scheme. Finally, utilizing an extracted loading cycle, the energy management system is experimentally implemented and tested in real-time operation, where it has to cope with environmental disturbance rejection and follow the desired speed profile while performing the power-split control in respect to the fuel to NOx weighting strategy. Based on the experimental results in a hybrid diesel–electric marine powertrain with a 260 kW diesel engine and a 90 kW electric machine, fuel consumption and NOx emissions reduction by 6% and 8.5% respectively, were achieved over the tested profile. In this framework, the capabilities of the energy management system in realistic operation conditions can be exploited and evaluated.",science
10.1016/j.jwpe.2021.102452,preprocessed,Journal of Water Process Engineering,scopus,2022-02-01,sciencedirect,polyamine-modified polyacrylonitrile fibers for efficient removal of u(vi) from real fluorine-contained low-level radioactive wastewater,https://api.elsevier.com/content/abstract/scopus_id/85119972768,"It is of great significance to develop an adsorbent with high adsorption capacity and excellent resistance to anion and cation interference toward the removal of U(VI). Herein, a novel polyamine-modified polyacrylonitrile-based fiber (PANPA) has been synthesized through hydrothermal method, which can validly remove U(VI) from solution. Combined with mesoscopic, spectral characterization and simulation method, the removal behavior and mechanism of U(VI) from high fluorine uranium-containing wastewater by PANPA are systematically investigated. The results show that, based on the strong coordination principle of polyamine group and UO2
                     2+, PANPA can selectively remove U(VI) from wastewater. In addition, the q
                     
                        max
                      of 459.27 mg g−1 was more than that of many other adsorbent materials. More importantly, PANPA is not affected by high concentration of F−, and exhibits higher distribution coefficient (559,900 mL g−1) and removal efficiency (99.5%) to U(VI) than other coexisting ions in real wastewater. Furthermore, the column experiment was also implemented to remove U(VI). The results indicate that PANPA is a promising material to effectively remove U(VI) from real wastewater produced during the fabrication of nuclear fuel elements.",science
10.1016/j.saa.2021.120347,preprocessed,Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy,scopus,2022-01-15,sciencedirect,rapid discrimination of curcuma longa and curcuma xanthorrhiza using direct analysis in real time mass spectrometry and near infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115004546,"This study describes a newly developed method for the fast and straightforward differentiation of two turmeric species using Direct Analysis in Real Time mass spectrometry and miniaturized Near Infrared spectroscopy. Multivariate analyses (PCA and LDA) were performed on the mass spectrometric data, thus creating a powerful model for the discrimination of Curcumalonga and Curcumaxanthorrhiza. Cross-validation of the model revealed correctness-scores of 100% with 20-fold as well as leave-one-out validation techniques. To further estimate the models prediction power, seven retail samples of turmeric powder were analyzed and assorted to a species. Looking for a fast, non-invasive, cost-efficient and laboratory independent method, miniaturized NIR spectrometers offer an alternative for quality control of turmeric species. However, different technologies implemented to compensate for their small size, lead to different applicability of these spectrometers. Therefore, we investigated the three handheld spectrometers microPHAZIR, MicroNIR 2200 and MicroNIR 1700ES for their application in spice analysis in hyphenation to PCA, LDA and ANN methods used for the discriminant analysis. While microPHAZIR proved to be the most valuable device for differentiating C.longa and C.xanthorrhiza, MicroNIR 1700ES offered the worst results. These findings are interpreted on the basis of a quantum chemical simulation of the NIR spectrum of curcumin as the representative constituent. It was found that the information accessible to MicroNIR 1700ES that is relevant to the analyzed constituents is located in the spectral region prone to interferences with the matrix, likely limiting the performance of this spectrometer in this analytical scenario.",science
10.1016/j.dss.2021.113665,preprocessed,Decision Support Systems,scopus,2022-01-01,sciencedirect,a constraint programming model for making recommendations in personal process management: a design science research approach,https://api.elsevier.com/content/abstract/scopus_id/85115634506,"Decision-making in everyday life has an essential role in effectively completing personal tasks and processes. The complexity of these processes and the resulting cognitive load of managing them may vary significantly. To decrease the cognitive load created by such decision-making efforts and to obtain better outcomes, recommendation systems carry significant potential. In order to investigate the benefits provided by decision support systems (DSS) in personal process management (PPM), we first build a constraint programming (CP) model and a prototype context-aware-mobile application employing this CP model. Then, we evaluate the application and the model via two exemplary real-world scenarios. The scenarios form the core of the experiments conducted with 50 participants. We compare the participants’ planning performances with and without the PPM system with quantitative metrics such as planning times and scenario objective values. In addition, System Usability Scale (SUS) questionnaires and open-ended questions provide qualitative evaluation results. Throughout the study, we apply the Design Science Research methodology to rigorously conduct research activities by proof of concept, proof of use, and proof of value. The empirical results clearly show that our proposed model for PPM is effective, and the developed prototype solution generates positive participant comments as well as a high SUS score. Overall, the prototype PPM system with CP implementation leads to better planning in less time in the planning phase, and it lets the user do fast replanning in the execution phase, which is invaluable in dynamically changing situations such as daily activities.",science
10.1016/j.postharvbio.2021.111741,preprocessed,Postharvest Biology and Technology,scopus,2022-01-01,sciencedirect,multi-output 1-dimensional convolutional neural networks for simultaneous prediction of different traits of fruit based on near-infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115232057,"In spectral data predictive modelling of fresh fruit, often the models are calibrated to predict multiple responses. A common method to deal with such a multi-response predictive modelling is the partial least-squares (PLS2) regression. Recently, deep learning (DL) has shown to outperform partial least-squares (PLS) approaches for single fruit traits prediction. The DL can also be adapted to perform multi-response modelling. This study presents an implementation of DL modelling for multi-response prediction for spectral data of fresh fruit. To show this, a real NIR data set related to SSC and MC measurements in pear fruit was used. Since DL models perform better with larger data sets, a data augmentation procedure was performed prior to data modelling. Furthermore, a comparative study was also performed between two of the most used DL architectures for spectral analysis, their multi-output and single-output variants and a classic baseline model using PLS2. A key point to note that all the DL modelling presented in this study is performed using novel automated optimisation tools such as Bayesian optimisation and Hyperband. The results showed that DL models can be easily adapted by changing the output of the fully connected layers to perform multi-response modelling. In comparison to the PLS2, the multi-response DL model showed ∼13 % lower root mean squared error (RMSE), showing the ease and superiority of handling multi-response by DL models for spectral calibration.",science
10.1109/tie.2021.3090707,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-06-01 00:00:00,ieeexplore,active object detection based on a novel deep q-learning network and long-term learning strategy for the service robot,https://ieeexplore.ieee.org/document/9464751/,"This article focuses on active object detection (AOD), one of the greatest challenges in the robotics field. A novel deep-Q-learning-network-based approach is proposed to utilize more useful status information for enhancing the training efficiency and testing accuracy of AOD by adding the cropped target object (TGOJ) from the current state as a new input. Different from the existing researches, a novel reward function, combing the area factor and distance factor of the bounding box, is designed to make the robot not only get closer to the TGOJ but also obtain a better observation viewpoint. Moreover, to overcome the differences between the training dataset and new environments as well as improving the adaptation of the AOD model, a reward-based long-term learning strategy including a novel training strategy is presented. The comparable experiments and the ablation study have been implemented in an AOD dataset, proving that our method owns better performance and efficiency than the comparable methods. Meanwhile, the experiments in the real-world scenario with a robot indicate the validity of the proposed method.",robotics
10.1109/lra.2022.3146515,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,kineverse: a symbolic articulation model framework for model-agnostic mobile manipulation,https://ieeexplore.ieee.org/document/9695204/,"Service robots in the future need to execute abstract instructions such as “fetch the milk from the fridge”. To translate such instructions into actionable plans, robots require in-depth background knowledge. With regards to interactions with doors and drawers, robots require articulation models that they can use for state estimation and motion planning. Existing frameworks model articulated connections as abstract concepts such as <italic>prismatic</italic>, or <italic>revolute</italic>, but do not provide a parameterized model of these connections for computation. In this letter, we introduce a novel framework that uses symbolic mathematical expressions to model articulated structures – robots and objects alike – in a unified and extensible manner. We provide a theoretical description of this framework, and the operations that are supported by its models, and introduce an architecture to exchange our models in robotic applications, making them as flexible as any other environmental observation. To demonstrate the utility of our approach, we employ our practical implementation <italic>Kineverse</italic> for solving common robotics tasks from state estimation and mobile manipulation, and use it further in real-world mobile robot manipulation.",robotics
10.1109/lra.2022.3146945,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,"tacto: a fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors",https://ieeexplore.ieee.org/document/9697425/,"Simulators perform an important role in prototyping, debugging, and benchmarking new advances in robotics and learning for control. Although many physics engines exist, some aspects of the real world are harder than others to simulate. One of the aspects that have so far eluded accurate simulation is touch sensing. To address this gap, we present TACTO – a fast, flexible, and open-source simulator for vision-based tactile sensors. This simulator allows to render realistic high-resolution touch readings at hundreds of frames per second, and can be easily configured to simulate different vision-based tactile sensors, including DIGIT and OmniTact. In this letter, we detail the principles that drove the implementation of TACTO and how they are reflected in its architecture. We demonstrate TACTO on a perceptual task, by learning to predict grasp stability using touch from 1 million grasps, and on a marble manipulation control task. Moreover, we provide a proof-of-concept that TACTO can be successfully used for Sim2Real applications. We believe that TACTO is a step towards the widespread adoption of touch sensing in robotic applications, and to enable machine learning practitioners interested in multi-modal learning and control.",robotics
10.1109/lra.2021.3123374,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,uncertainty for identifying open-set errors in visual object detection,https://ieeexplore.ieee.org/document/9591346/,"Deployed into an open world, object detectors are prone to open-set errors, false positive detections of object classes not present in the training dataset.We propose GMM-Det, a real-time method for extracting epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified by their low log-probability under all Gaussian Mixture Models. We test two common detector architectures, Faster R-CNN and RetinaNet, across three varied datasets spanning robotics and computer vision. Our results show that GMM-Det consistently outperforms existing uncertainty techniques for identifying and rejecting open-set detections, especially at the low-error-rate operating point required for safety-critical applications. GMM-Det maintains object detection performance, and introduces only minimal computational overhead. We also introduce a methodology for converting existing object detection datasets into specific <italic>open-set</italic> datasets to evaluate open-set performance in object detection.",robotics
10.1109/tro.2021.3084374,preprocessed,IEEE Transactions on Robotics,IEEE,2022-02-01 00:00:00,ieeexplore,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,https://ieeexplore.ieee.org/document/9453856/,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",robotics
10.1109/tfuzz.2020.3033141,preprocessed,IEEE Transactions on Fuzzy Systems,IEEE,2022-01-01 00:00:00,ieeexplore,fuzzy double deep q-network-based gait pattern controller for humanoid robots,https://ieeexplore.ieee.org/document/9237162/,"In this article, the adaptive-network-based fuzzy inference system (ANFIS) is combined with the double deep <italic>Q</italic>-network (DDQN) to realize a fuzzy DDQN (FDDQN) such that a humanoid robot can generate a linear inverted pendulum model-based gait pattern in real time. The FDDQN not only allows the humanoid robot to correct the gait pattern instantly but also improves its stability. The proposed scheme is designed and implemented in a toddler-sized humanoid robot called Louis. First, four pressure sensors are installed on the bottom of the sole and one inertial measurement unit is set up on the trunk of the robot. A wireless communication chip is employed to transfer the data to a computer to determine the required parameters for the robot. Next, a control system based on the Linux operating system is developed. The values of the center of pressure and acceleration obtained with the ANFIS are adopted to train the DDQN. The proposed neural network comprises four layers, and the model is cautiously selected to avoid overfitting. The proposed scheme is verified using a robot simulator and then real-time-tested on Louis. The experimental results indicate that the FDDQN can provide the robot timely feedback during walking as well as helps it in adjusting the gait pattern independently. The balancing of the robot through effective dynamic feedback is similar to the balancing ability of an infant learning to walk.",robotics
10.1109/sii52469.2022.9708826,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,evaluation of variable impedance- and hybrid force/motioncontrollers for learning force tracking skills,https://ieeexplore.ieee.org/document/9708826/,"For robots to perform real-world force interaction tasks with human level dexterity, it is crucial to develop adaptable and compliant force controllers. Learning techniques, especially reinforcement learning, provide a platform to develop adaptable controllers for complex robotic tasks. This paper presents an evaluation of two prominent force control methods, variable impedance control and hybrid force-motion control in a robot learning framework. The controllers are evaluated on a Franka Emika Panda robotic manipulator for a robotic interaction task demanding force and motion tracking using a model-based reinforcement learning algorithm, PILCO. Utilizing the learning framework to find the optimal controller parameters has significantly improved the performance of the controllers. The implementation of the controllers integrated with the robot learning framework is available on https://github.com/martihmy/Compliant_control.",robotics
10.1109/lra.2022.3142433,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,enabling low-cost full surface tactile skin for human robot interaction,https://ieeexplore.ieee.org/document/9681158/,"Realizing full coverage, low-maintenance, and low-cost tactile skin is a <italic>de facto</italic> design dream since the invention of robots. It ensures safety and enables collaborative work protocols for human robot interactions (HRI). The on-robot tactile capability is realized by deploying an array of external sensors or inferring from proprioceptive information that comes with the robot, such as motor torque. However, these methods may be cumbersome, introduce extra management cost, expensive, lack real-world robustness, or require special robot designs. In this letter, we present <italic>SonicSkin</italic>, a low-cost ($2) and easy to deploy system that localizes the on-robot human touch and estimates the touch pressure without actually attaching sensors at potential touch locations. The system requires only a single pair of piezoelectric transducers (<italic>i.e.</italic> one transmitter and one receiver) attached on the target robot and turns the robot itself into a versatile sensor. We present a set of novel algorithms to progressively address the unique challenges posed by our system design. We put together an end-to-end <italic>SonicSkin</italic> system on a Jaco robot arm that runs in real-time, and conducted an extensive real-world study including 57019 actual evaluation datapoints under various challenging conditions from 12 human subjects. <italic>SonicSkin</italic> achieves less than 2 cm localization error for 96.4% of touches, with more than 96.7% cross-correlation similarity between the predicted touch pressure and the ground truth touch pressure.",robotics
10.1109/access.2022.3145969,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,fastmde: a fast cnn architecture for monocular depth estimation at high resolution,https://ieeexplore.ieee.org/document/9690863/,"A depth map helps robots and autonomous vehicles (AVs) visualize the three-dimensional world to navigate and localize neighboring obstacles. However, it is difficult to develop a deep learning model that can estimate the depth map from a single image in real-time. This study proposes a fast monocular depth estimation model named <italic>FastMDE</italic> by optimizing the deep convolutional neural network according to the encoder-decoder architecture. The decoder needs to obtain partial and semantic feature maps from the encoding phase to improve the depth estimation accuracy. Therefore, we designed FastMDE with two effective strategies. The first one involved redesigning the skip connection with the features of the squeeze-excitation module to obtain partial and semantic feature maps of the encoding phase. The second strategy involved redesigning the decoder by using the fusion dense block to permit the usage of high-resolution features that were learned earlier in the network before upsampling. The proposed FastMDE model utilizes only 4.1 M parameters, which is much lesser than the parameters utilized by state-of-art models. Thus, FastDME has a higher accuracy and lower latency than previous models. This study also demonstrates that MDE can leverage deep neural networks in real-time (i.e., 30 fps) with the Linux embedded board Nvidia Jetson Xavier NX. The model can facilitate the development and applications with superior performances and easy deployment on an embedded platform.",robotics
10.1109/lra.2021.3129136,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,ocrtoc: a cloud-based competition and benchmark for robotic grasping and manipulation,https://ieeexplore.ieee.org/document/9619915/,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is <uri>www.ocrtoc.org</uri>, and the OCRTOC software package is available at <uri>https://github.com/OCRTOC/OCRTOC_software_package</uri>.",robotics
10.1109/lra.2022.3143289,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,visuotactile 6d pose estimation of an in-hand object using vision and tactile sensor data,https://ieeexplore.ieee.org/document/9682507/,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",robotics
10.1109/sii52469.2022.9708882,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,reinforcement learning based hierarchical control for path tracking of a wheeled bipedal robot with sim-to-real framework,https://ieeexplore.ieee.org/document/9708882/,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",robotics
10.1109/lra.2022.3146900,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,open simulation environment for learning and practice of robot-assisted surgical suturing,https://ieeexplore.ieee.org/document/9697399/,"Automation has the potential to improve the standard of care but is difficult to realize due to perceptual challenges, especially in soft-tissue surgery. Machine learning can provide solutions, but typically requires large amounts of training data, which is time-consuming to collect. Even with shared platforms, hardware differences can prevent effective sharing of data between institutions. This letter proposes a standardized simulation platform for training and testing algorithms to control surgical robotic systems, which is built upon an open-source simulator, the Asynchronous Multi-Body Framework (AMBF), to enable quick prototyping of different scenes. An illustrative example of a suturing task on a phantom is presented and has formed the basis of a challenge, released to the community. The top-level contribution is the open-source release of a dynamic simulation environment that enables realistic suturing on a phantom, but supporting contributions include its extendable architectural design and a series of algorithmic optimizations to achieve real-time control and collision detection, realistic behavior of the needle and suture, and generation of multi-modal ground-truth data, including labeled depth data. These capabilities enable simulation-based surgical training and support research in machine learning for surgical scene perception and autonomous action.",robotics
10.1109/lra.2022.3145971,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,focus on impact: indoor exploration with intrinsic motivation,https://ieeexplore.ieee.org/document/9691914/,"Exploration of indoor environments has recently experienced a significant interest, also thanks to the introduction of deep neural agents built in a hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on simulated environments. Current state-of-the-art methods employ a dense extrinsic reward that requires the complete a priori knowledge of the layout of the training environment to learn an effective exploration policy. However, such information is expensive to gather in terms of time and resources. In this work, we propose to train the model with a purely intrinsic reward signal to guide exploration, which is based on the impact of the robot’s actions on its internal representation of the environment. So far, impact-based rewards have been employed for simple tasks and in procedurally generated synthetic environments with countable states. Since the number of states observable by the agent in realistic indoor environments is non-countable, we include a neural-based density model and replace the traditional count-based regularization with an estimated pseudo-count of previously visited states. The proposed exploration approach outperforms DRL-based competitors relying on intrinsic rewards and surpasses the agents trained with a dense extrinsic reward computed with the environment layouts. We also show that a robot equipped with the proposed approach seamlessly adapts to point-goal navigation and real-world deployment.",robotics
10.1109/lra.2022.3141150,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,reve-ce: remote embodied visual referring expression in continuous environment,https://ieeexplore.ieee.org/document/9674225/,"Ithas always been a great challenge for the robot to navigate in the visual world following natural language instructions. Recently, several tasks such as the Vision-and-Language Navigation (VLN) and Remote Embodied Visual Referring Expression in Real Indoor Environments (REVERIE) are proposed trying to solve this challenge. And the most significant difference between VLN and REVERIE tasks is that REVERIE uses a higher guidance level instruction. However, the navigation process of REVERIE is implemented in a discrete environment, which is unrealistic in real world scenarios. To make the REVERIE task more consistent with the real physical world, we develop a new task of Remote Embodied Visual Referring Expression in Continuous Environment, namely REVE-CE, in which the agent executes a much longer sequence of low-level actions given language instructions. Furthermore, we propose a multi-branch cross modal attention (MBCMA) framework to solve the proposed REVE-CE task. Extensive experiments are conducted demonstrating that the proposed framework greatly outperforms the state-of-the-art VLN baselines and a new benchmark for the proposed REVE-CE task is built.",robotics
http://arxiv.org/abs/2202.08004v1,preprocessed,arxiv,arxiv,2022-02-16 00:00:00,arxiv,deep koopman operator with control for nonlinear systems,http://arxiv.org/abs/2202.08004v1,"Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. We then design an auxiliary control network to encode
the nonlinear state-dependent control term to model the nonlinearity in control
input. For linear control, this encoded term is considered the new control
variable instead, ensuring the linearity of the embedding space. Then we deploy
Linear Quadratic Regulator (LQR) on the linear embedding space to derive the
optimal control policy and decode the actual control input from the control
net. Experimental results demonstrate that our approach outperforms other
existing methods, reducing the prediction error by order-of-magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and 7 Dof robotic manipulator.",robotics
http://arxiv.org/abs/2202.07064v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"towards hardware implementation of wta for cpg-based control of a
  spiking robotic arm",http://arxiv.org/abs/2202.07064v1,"Biological nervous systems typically perform the control of numerous degrees
of freedom for example in animal limbs. Neuromorphic engineers study these
systems by emulating them in hardware for a deeper understanding and its
possible application to solve complex problems in engineering and robotics.
Central-Pattern-Generators (CPGs) are part of neuro-controllers, typically used
at their last steps to produce rhythmic patterns for limbs movement. Different
patterns and gaits typically compete through winner-take-all (WTA) circuits to
produce the right movements. In this work we present a WTA circuit implemented
in a Spiking-Neural-Network (SNN) processor to produce such patterns for
controlling a robotic arm in real-time. The robot uses spike-based
proportional-integrativederivative (SPID) controllers to keep a commanded joint
position from the winner population of neurons of the WTA circuit. Experiments
demonstrate the feasibility of robotic control with spiking circuits following
brain-inspiration.",robotics
http://arxiv.org/abs/2202.06003v2,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,robust learning from observation with model misspecification,http://arxiv.org/abs/2202.06003v2,"Imitation learning (IL) is a popular paradigm for training policies in
robotic systems when specifying the reward function is difficult. However,
despite the success of IL algorithms, they impose the somewhat unrealistic
requirement that the expert demonstrations must come from the same domain in
which a new imitator policy is to be learned. We consider a practical setting,
where (i) state-only expert demonstrations from the real (deployment)
environment are given to the learner, (ii) the imitation learner policy is
trained in a simulation (training) environment whose transition dynamics is
slightly different from the real environment, and (iii) the learner does not
have any access to the real environment during the training phase beyond the
batch of demonstrations given. Most of the current IL methods, such as
generative adversarial imitation learning and its state-only variants, fail to
imitate the optimal expert behavior under the above setting. By leveraging
insights from the Robust reinforcement learning (RL) literature and building on
recent adversarial imitation approaches, we propose a robust IL algorithm to
learn policies that can effectively transfer to the real environment without
fine-tuning. Furthermore, we empirically demonstrate on continuous-control
benchmarks that our method outperforms the state-of-the-art state-only IL
method in terms of the zero-shot transfer performance in the real environment
and robust performance under different testing conditions.",robotics
http://arxiv.org/abs/2201.09857v2,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,"stops: short-term-based volatility-controlled policy search and its
  global convergence",http://arxiv.org/abs/2201.09857v2,"It remains challenging to deploy existing risk-averse approaches to
real-world applications. The reasons are multi-fold, including the lack of
global optimality guarantee and the necessity of learning from long-term
consecutive trajectories. Long-term consecutive trajectories are prone to
involving visiting hazardous states, which is a major concern in the
risk-averse setting. This paper proposes \textbf{\ul{S}}hort-\textbf{\ul{T}}erm
V\textbf{\ul{O}}latility-controlled \textbf{\ul{P}}olicy \textbf{\ul{S}}earch
(STOPS), a novel algorithm that solves risk-averse problems by learning from
short-term trajectories instead of long-term trajectories. Short-term
trajectories are more flexible to generate, and can avoid the danger of
hazardous state visitations. By using an actor-critic scheme with an
overparameterized two-layer neural network, our algorithm finds a globally
optimal policy at a sublinear rate with proximal policy optimization and
natural policy gradient, with effectiveness comparable to the state-of-the-art
convergence rate of risk-neutral policy-search methods. The algorithm is
evaluated on challenging Mujoco robot simulation tasks under the mean-variance
evaluation metric. Both theoretical analysis and experimental results
demonstrate a state-of-the-art level of STOPS' performance among existing
risk-averse policy search methods.",robotics
http://arxiv.org/abs/2201.05753v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"parameter identification and motion control for articulated rigid body
  robots using differentiable position-based dynamics",http://arxiv.org/abs/2201.05753v1,"Simulation modeling of robots, objects, and environments is the backbone for
all model-based control and learning. It is leveraged broadly across dynamic
programming and model-predictive control, as well as data generation for
imitation, transfer, and reinforcement learning. In addition to fidelity, key
features of models in these control and learning contexts are speed, stability,
and native differentiability. However, many popular simulation platforms for
robotics today lack at least one of the features above. More recently,
position-based dynamics (PBD) has become a very popular simulation tool for
modeling complex scenes of rigid and non-rigid object interactions, due to its
speed and stability, and is starting to gain significant interest in robotics
for its potential use in model-based control and learning. Thus, in this paper,
we present a mathematical formulation for coupling position-based dynamics
(PBD) simulation and optimal robot design, model-based motion control and
system identification. Our framework breaks down PBD definitions and
derivations for various types of joint-based articulated rigid bodies. We
present a back-propagation method with automatic differentiation, which can
integrate both positional and angular geometric constraints. Our framework can
critically provide the native gradient information and perform gradient-based
optimization tasks. We also propose articulated joint model representations and
simulation workflow for our differentiable framework. We demonstrate the
capability of the framework in efficient optimal robot design, accurate
trajectory torque estimation and supporting spring stiffness estimation, where
we achieve minor errors. We also implement impedance control in real robots to
demonstrate the potential of our differentiable framework in human-in-the-loop
applications.",robotics
http://arxiv.org/abs/2201.01369v1,preprocessed,arxiv,arxiv,2022-01-04 00:00:00,arxiv,"using simulation optimization to improve zero-shot policy transfer of
  quadrotors",http://arxiv.org/abs/2201.01369v1,"In this work, we show that it is possible to train low-level control policies
with reinforcement learning entirely in simulation and, then, deploy them on a
quadrotor robot without using real-world data to fine-tune. To render zero-shot
policy transfers feasible, we apply simulation optimization to narrow the
reality gap. Our neural network-based policies use only onboard sensor data and
run entirely on the embedded drone hardware. In extensive real-world
experiments, we compare three different control structures ranging from
low-level pulse-width-modulated motor commands to high-level attitude control
based on nested proportional-integral-derivative controllers. Our experiments
show that low-level controllers trained with reinforcement learning require a
more accurate simulation than higher-level control policies.",robotics
10.1016/j.artint.2021.103637,preprocessed,Artificial Intelligence,scopus,2022-02-01,sciencedirect,online perceptual learning and natural language acquisition for autonomous robots,https://api.elsevier.com/content/abstract/scopus_id/85120333069,"In this work, the problem of bootstrapping knowledge in language and vision for autonomous robots is addressed through novel techniques in grammar induction and word grounding to the perceptual world. In particular, we demonstrate a system, called OLAV, which is able, for the first time, to (1) learn to form discrete concepts from sensory data; (2) ground language (n-grams) to these concepts; (3) induce a grammar for the language being used to describe the perceptual world; and moreover to do all this incrementally, without storing all previous data. The learning is achieved in a loosely-supervised manner from raw linguistic and visual data. Moreover, the learnt model is transparent, rather than a black-box model and is thus open to human inspection. The visual data is collected using three different robotic platforms deployed in real-world and simulated environments and equipped with different sensing modalities, while the linguistic data is collected using online crowdsourcing tools and volunteers. The analysis performed on these robots demonstrates the effectiveness of the framework in learning visual concepts, language groundings and grammatical structure in these three online settings.",robotics
10.1016/j.automatica.2021.110007,preprocessed,Automatica,scopus,2022-01-01,sciencedirect,an analytic layer-wise deep learning framework with applications to robotics,https://api.elsevier.com/content/abstract/scopus_id/85118989490,"Deep learning (DL) has achieved great success in many applications, but it has been less well analyzed from the theoretical perspective. The unexplainable success of black-box DL models has raised questions among scientists and promoted the emergence of the field of explainable artificial intelligence (XAI). In robotics, it is particularly important to deploy DL algorithms in a predictable and stable manner as robots are active agents that need to interact safely with the physical world. This paper presents an analytic deep learning framework for fully connected neural networks, which can be applied for both regression problems and classification problems. Examples for regression and classification problems include online robot control and robot vision. We present two layer-wise learning algorithms such that the convergence of the learning systems can be analyzed. Firstly, an inverse layer-wise learning algorithm for multilayer networks with convergence analysis for each layer is presented to understand the problems of layer-wise deep learning. Secondly, a forward progressive learning algorithm where the deep networks are built progressively by using single hidden layer networks is developed to achieve better accuracy. It is shown that the progressive learning method can be used for fine-tuning of weights from convergence point of view. The effectiveness of the proposed framework is illustrated based on classical benchmark recognition tasks using the MNIST and CIFAR-10 datasets and the results show a good balance between performance and explainability. The proposed method is subsequently applied for online learning of robot kinematics and experimental results on kinematic control of UR5e robot with unknown model are presented.",robotics
10.1109/ccnc49033.2022.9700725,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,mitigating location-based attacks using predication models in vehicular ad-hoc networks,https://ieeexplore.ieee.org/document/9700725/,"The modern world is constantly in a state of technological revolution. Everyday new technological ideas, inventions, and threats emerge. With modern computer software and hardware advancements, we have the emergence of the Internet of Things (IoT). In conjunction, modern car companies have a push from public demand for a fully-autonomous car. To accomplish autonomy, small, and secure Vehicular Ad-Hoc Networks (VANETs) it is necessary to ensure that the systems that rely on connected vehicle data is reliable and accurate. In the event there is a malicious actor manipulating the data through replica and injection attacks or there is a hardware failure yielding inaccurate location information, it is necessary to explore efficient methods for predicting connected vehicles locations such that these systems, which rely on accurate information are not impacted. This study analyzes multiple clustering and prediction models to discover how effectively a multi-layered machine learning approach is able to meet the real-time requirement of future generation smart cities.",autonomous vehicle
10.1109/ccnc49033.2022.9700636,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,balancing latency and accuracy on deep video analytics at the edge,https://ieeexplore.ieee.org/document/9700636/,"Real-time deep video analytic at the edge is an enabling technology for emerging applications, such as vulnerable road user detection for autonomous driving, which requires highly accurate results of model inference within a low latency. In this paper, we investigate the accuracy-latency trade-off in the design and implementation of real-time deep video analytic at the edge. Without loss of generality, we select the widely used YOLO-based object detection and WebRTC-based video streaming for case study. Here, the latency consists of both networking latency caused by video streaming and the processing latency for video encoding/decoding and model inference. We conduct extensive measurements to figure out how the dynamically changing settings of video streaming affect the achieved latency, the quality of video, and further the accuracy of model inference. Based on the findings, we propose a mechanism for adapting video streaming settings (i.e. bitrate, resolution) online to optimize the accuracy of video analytic within latency constraints. The mechanism has proved, through a simulated setup, to be efficient in searching the optimal settings.",autonomous vehicle
10.1109/iccece54139.2022.9712792,preprocessed,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),IEEE,2022-01-16 00:00:00,ieeexplore,design of deep learning based autonomous driving control algorithm,https://ieeexplore.ieee.org/document/9712792/,"In recent years, with the continuous development of the field of artificial intelligence, autonomous driving technology has gained widespread attention. In order to meet the purpose of changing driving behavior and completing driving tasks in real time without human intervention. In this paper, we study the design and implementation process of end-to-end autonomous driving algorithms based on computer vision and deep learning, and explain the elements of algorithm design from a theoretical perspective. The method of continuous steering angle prediction for autonomous driving based on convolutional neural network is proposed, as well as the method of network pre-training and overfitting prevention to improve the training effect and generalization ability. The difference with the traditional end-to-end control methods is that the traditional methods study the problem abstractly as a classification problem, describing the motion in terms of direction with a coarser granularity. The method proposed in this paper treats it as a regression problem, describing the motion in terms of steering angles, which provides a more accurate description of the motion and is more adaptive.",autonomous vehicle
10.1109/wacv51458.2022.00206,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,plugging self-supervised monocular depth into unsupervised domain adaptation for semantic segmentation,https://ieeexplore.ieee.org/document/9707096/,"Although recent semantic segmentation methods have made remarkable progress, they still rely on large amounts of annotated training data, which are often infeasible to collect in the autonomous driving scenario. Previous works usually tackle this issue with Unsupervised Domain Adaptation (UDA), which entails training a network on synthetic images and applying the model to real ones while minimizing the discrepancy between the two domains. Yet, these techniques do not consider additional information that may be obtained from other tasks. Differently, we propose to exploit self-supervised monocular depth estimation to improve UDA for semantic segmentation. On one hand, we deploy depth to realize a plug-in component which can inject complementary geometric cues into any existing UDA method. We further rely on depth to generate a large and varied set of samples to Self-Train the final model. Our whole proposal allows for achieving state-of-the-art performance (58.8 mIoU) in the GTA5 → CS benchmark. Code is available at https://github.com/CVLAB-Unibo/d4-dbst.",autonomous vehicle
10.1109/jas.2021.1003907,preprocessed,IEEE/CAA Journal of Automatica Sinica,IEEE,2022-02-01 00:00:00,ieeexplore,domain-invariant similarity activation map contrastive learning for retrieval-based long-term visual localization,https://ieeexplore.ieee.org/document/9358457/,"Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.",autonomous vehicle
10.1007/s11042-021-11437-3,preprocessed,Multimedia Tools and Applications,Springer,2022-01-01 00:00:00,springer,deep reinforcement learning based control for autonomous vehicles in carla,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11437-3,"Nowadays, Artificial Intelligence (AI) is growing by leaps and bounds in almost all fields of technology, and Autonomous Vehicles (AV) research is one more of them. This paper proposes the using of algorithms based on Deep Learning (DL) in the control layer of an autonomous vehicle. More specifically, Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) are implemented in order to compare results between them. The aim of this work is to obtain a trained model, applying a DRL algorithm, able of sending control commands to the vehicle to navigate properly and efficiently following a determined route. In addition, for each of the algorithms, several agents are presented as a solution, so that each of these agents uses different data sources to achieve the vehicle control commands. For this purpose, an open-source simulator such as CARLA is used, providing to the system with the ability to perform a multitude of tests without any risk into an hyper-realistic urban simulation environment, something that is unthinkable in the real world. The results obtained show that both DQN and DDPG reach the goal, but DDPG obtains a better performance. DDPG perfoms trajectories very similar to classic controller as LQR. In both cases RMSE is lower than 0.1m following trajectories with a range 180-700m. To conclude, some conclusions and future works are commented.",autonomous vehicle
http://arxiv.org/abs/2202.04224v1,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,intelligent autonomous intersection management,http://arxiv.org/abs/2202.04224v1,"Connected Autonomous Vehicles will make autonomous intersection management a
reality replacing traditional traffic signal control. Autonomous intersection
management requires time and speed adjustment of vehicles arriving at an
intersection for collision-free passing through the intersection. Due to its
computational complexity, this problem has been studied only when vehicle
arrival times towards the vicinity of the intersection are known beforehand,
which limits the applicability of these solutions for real-time deployment. To
solve the real-time autonomous traffic intersection management problem, we
propose a reinforcement learning (RL) based multiagent architecture and a novel
RL algorithm coined multi-discount Q-learning. In multi-discount Q-learning, we
introduce a simple yet effective way to solve a Markov Decision Process by
preserving both short-term and long-term goals, which is crucial for
collision-free speed control. Our empirical results show that our RL-based
multiagent solution can achieve near-optimal performance efficiently when
minimizing the travel time through an intersection.",autonomous vehicle
http://arxiv.org/abs/2202.02352v1,preprocessed,arxiv,arxiv,2022-02-04 00:00:00,arxiv,"learning interpretable, high-performing policies for continuous control
  problems",http://arxiv.org/abs/2202.02352v1,"Gradient-based approaches in reinforcement learning (RL) have achieved
tremendous success in learning policies for continuous control problems. While
the performance of these approaches warrants real-world adoption in domains,
such as in autonomous driving and robotics, these policies lack
interpretability, limiting deployability in safety-critical and
legally-regulated domains. Such domains require interpretable and verifiable
control policies that maintain high performance. We propose Interpretable
Continuous Control Trees (ICCTs), a tree-based model that can be optimized via
modern, gradient-based, RL approaches to produce high-performing, interpretable
policies. The key to our approach is a procedure for allowing direct
optimization in a sparse decision-tree-like representation. We validate ICCTs
against baselines across six domains, showing that ICCTs are capable of
learning interpretable policy representations that parity or outperform
baselines by up to 33$\%$ in autonomous driving scenarios while achieving a
$300$x-$600$x reduction in the number of policy parameters against deep
learning baselines.",autonomous vehicle
http://arxiv.org/abs/2202.00091v1,preprocessed,arxiv,arxiv,2022-01-31 00:00:00,arxiv,"query efficient decision based sparse attacks against black-box deep
  learning models",http://arxiv.org/abs/2202.00091v1,"Despite our best efforts, deep learning models remain highly vulnerable to
even tiny adversarial perturbations applied to the inputs. The ability to
extract information from solely the output of a machine learning model to craft
adversarial perturbations to black-box models is a practical threat against
real-world systems, such as autonomous cars or machine learning models exposed
as a service (MLaaS). Of particular interest are sparse attacks. The
realization of sparse attacks in black-box models demonstrates that machine
learning models are more vulnerable than we believe. Because these attacks aim
to minimize the number of perturbed pixels measured by l_0 norm-required to
mislead a model by solely observing the decision (the predicted label) returned
to a model query; the so-called decision-based attack setting. But, such an
attack leads to an NP-hard optimization problem. We develop an evolution-based
algorithm-SparseEvo-for the problem and evaluate against both convolutional
deep neural networks and vision transformers. Notably, vision transformers are
yet to be investigated under a decision-based attack setting. SparseEvo
requires significantly fewer model queries than the state-of-the-art sparse
attack Pointwise for both untargeted and targeted attacks. The attack
algorithm, although conceptually simple, is also competitive with only a
limited query budget against the state-of-the-art gradient-based whitebox
attacks in standard computer vision tasks such as ImageNet. Importantly, the
query efficient SparseEvo, along with decision-based attacks, in general, raise
new questions regarding the safety of deployed systems and poses new directions
to study and understand the robustness of machine learning models.",autonomous vehicle
http://arxiv.org/abs/2201.05797v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"finding label and model errors in perception data with learned
  observation assertions",http://arxiv.org/abs/2201.05797v1,"ML is being deployed in complex, real-world scenarios where errors have
impactful consequences. In these systems, thorough testing of the ML pipelines
is critical. A key component in ML deployment pipelines is the curation of
labeled training data. Common practice in the ML literature assumes that labels
are the ground truth. However, in our experience in a large autonomous vehicle
development center, we have found that vendors can often provide erroneous
labels, which can lead to downstream safety risks in trained models.
  To address these issues, we propose a new abstraction, learned observation
assertions, and implement it in a system called Fixy. Fixy leverages existing
organizational resources, such as existing (possibly noisy) labeled datasets or
previously trained ML models, to learn a probabilistic model for finding errors
in human- or model-generated labels. Given user-provided features and these
existing resources, Fixy learns feature distributions that specify likely and
unlikely values (e.g., that a speed of 30mph is likely but 300mph is unlikely).
It then uses these feature distributions to score labels for potential errors.
We show that FIxy can automatically rank potential errors in real datasets with
up to 2$\times$ higher precision compared to recent work on model assertions
and standard techniques such as uncertainty sampling.",autonomous vehicle
http://arxiv.org/abs/2201.05518v1,preprocessed,arxiv,arxiv,2022-01-14 00:00:00,arxiv,ugv-uav object geolocation in unstructured environments,http://arxiv.org/abs/2201.05518v1,"A robotic system of multiple unmanned ground vehicles (UGVs) and unmanned
aerial vehicles (UAVs) has the potential for advancing autonomous object
geolocation performance. Much research has focused on algorithmic improvements
on individual components, such as navigation, motion planning, and perception.
In this paper, we present a UGV-UAV object detection and geolocation system,
which performs perception, navigation, and planning autonomously in real scale
in unstructured environment. We designed novel sensor pods equipped with
multispectral (visible, near-infrared, thermal), high resolution (181.6 Mega
Pixels), stereo (near-infrared pair), wide field of view (192 degree HFOV)
array. We developed a novel on-board software-hardware architecture to process
the high volume sensor data in real-time, and we built a custom AI subsystem
composed of detection, tracking, navigation, and planning for autonomous
objects geolocation in real-time.
  This research is the first real scale demonstration of such high speed data
processing capability. Our novel modular sensor pod can boost relevant computer
vision and machine learning research. Our novel hardware-software architecture
is a solid foundation for system-level and component-level research. Our system
is validated through data-driven offline tests as well as a series of field
tests in unstructured environments. We present quantitative results as well as
discussions on key robotic system level challenges which manifest when we build
and test the system. This system is the first step toward a UGV-UAV cooperative
reconnaissance system in the future.",autonomous vehicle
10.1016/j.jag.2021.102652,preprocessed,International Journal of Applied Earth Observation and Geoinformation,scopus,2022-02-01,sciencedirect,developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big uav imagery data,https://api.elsevier.com/content/abstract/scopus_id/85122505895,"The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.",autonomous vehicle
10.1016/j.compag.2021.106574,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,perennial ryegrass biomass retrieval through multispectral uav data,https://api.elsevier.com/content/abstract/scopus_id/85122407603,"Frequent biomass measurement is a key activity for optimal perennial ryegrass (Lolium perenne) management in intensive forage-based dairy operations. Due to the necessary high-frequency (i.e., weekly or monthly) pasture monitoring and continuous trend of larger dairy farms, such activity is perceived as an operational bottleneck. Consequently, substantial effort is directed to the development of accurate and automated technological solutions for biomass assessment. The popularization of unmanned aerial vehicles (UAVs) combined with multispectral cameras should allow for an optimal observational system able to deploy machine learning algorithms for near real-time biomass dry-matter (DM) mapping. For successful operation, these systems should deliver radiometrically accurate orthomosaics and robust models able to generalize across different periods. Nevertheless, the accuracy of radiometric calibration and generalization ability of these models is seldom evaluated. Also, such pipelines should require minimum processing power and allow for fast deployment. This study has established a two-year experiment comparing reflectance measurements between a handheld spectrometer and a commercial multispectral UAV camera. Different algorithms based on regression-tree architecture were contrasted regarding accuracy, speed, and model size. Model performances were validated, providing error-metrics for baseline accuracy and temporal validation. The results have shown that the standard procedure for multispectral imagery radiometric calibration is sub-optimal, requiring further post-processing and presenting low correlation with handheld measurements across spectral bands and dates. Nevertheless, after post-calibration, the use of spectral imagery has presented better baseline error than the point-based sensors, respectively displaying an average of 397.3 and 464.2 kg DM/ha when employed alongside the best performing algorithm (i.e., Cubist). When trained and validated across different years, model performance was largely reduced and deemed unfit for operational purposes. The Cubist/M5 family of algorithms have exhibited advantageous characteristics such as compact model structure, allowing for a higher level of model interpretability, while displaying a smaller size and faster deployment than the Random Forest, Boosted, and Bagged Regression Trees algorithms.",autonomous vehicle
10.1016/j.aap.2021.106473,preprocessed,Accident Analysis and Prevention,scopus,2022-02-01,sciencedirect,mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors,https://api.elsevier.com/content/abstract/scopus_id/85118989110,"Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.",autonomous vehicle
10.1016/j.inffus.2021.09.004,preprocessed,Information Fusion,scopus,2022-02-01,sciencedirect,multimodal earth observation data fusion: graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",autonomous vehicle
10.1016/j.isatra.2022.01.014,preprocessed,ISA Transactions,scopus,2022-01-01,sciencedirect,"intelligent framework for automated failure prediction, detection, and classification of mission critical autonomous flights",https://api.elsevier.com/content/abstract/scopus_id/85123893673,"Autonomous flights are the major industry contributors towards next-generation developments in pervasive and ubiquitous computing. Modern aerial vehicles are designed to receive actuator commands from the primary autopilot software as input to regulate their servos for adjusting control surfaces. Due to real-time interaction with the actual physical environment, there exists a high risk of control surface failures for engine, rudder, elevators, and ailerons etc. If not anticipated and then timely controlled, failures occurring during the flight can have severe and cataclysmic consequences, which may result in mid-air collision or ultimate crash. Humongous amount of sensory data being generated throughout mission-critical flights, makes it an ideal candidate for applying advanced data-driven machine learning techniques to identify intelligent insights related to failures for instant recovery from emergencies. In this paper, we present a novel framework based on machine learning techniques for failure prediction, detection, and classification for autonomous aerial vehicles. The proposed framework utilizes long short-term memory recurrent neural network architecture to analyze time series data and has been applied at the AirLab Failure and Anomaly flight dataset, which is a comprehensive publicly available dataset of various fault types in fixed-wing autonomous aerial vehicles’ control surfaces. The proposed framework is able to predict failure with an average accuracy of 93% and the average time-to-predict a failure is 19 s before the actual occurrence of the failure, which is 10 s better than current state-of-the-art. Failure detection accuracy is 100% and average detection time is 0.74 s after happening of failure, which is 1.28 s better than current state-of-the-art. Failure classification accuracy of proposed framework is 100%. The performance analysis shows the strength of the proposed methodology to be used as a real-time failure prediction and a pseudo-real-time failure detection along with a failure classification framework for eventual deployment with actual mission-critical autonomous flights.",autonomous vehicle
10.1016/j.trc.2021.103499,preprocessed,Transportation Research Part C: Emerging Technologies,scopus,2022-01-01,sciencedirect,do autonomous vehicles drive like humans? a turing approach and an application to sae automation level 2 cars,https://api.elsevier.com/content/abstract/scopus_id/85120490088,"Fully automated vehicles (AVs) are set to become a reality in future decades and changes are to be expected in user perceptions and behavior. While AV acceptability has been widely studied, changes in human drivers’ behavior and in passengers’ reactions have received less attention. It is not yet possible to ascertain the risk of driver behavioral changes such as overreaction, and the corresponding safety problems, in mixed traffic with partially AVs. Nor has there been proper investigation of the potential unease of car occupants trained for human control, when exposed to automatic maneuvers. The conjecture proposed in this paper is that automation Level 2 vehicles do not induce potentially adverse effects in traditional vehicle drivers’ behavior or in occupants’ reactions, provided that they are indistinguishable from human-driven vehicles. To this end, the paper proposes a Turing approach to test the “humanity” of automation Level 2 vehicles. The proposed test was applied to the results of an experimental campaign carried out in Italy: 546 car passengers were interviewed on board Level 2 cars in which they could not see the driver. They were asked whether a specific driving action (braking, accelerating, lane keeping) had been performed by the human driver or by the automatic on-board software under different traffic conditions (congestion and speed). Estimation results show that in most cases the interviewees were unable to distinguish the Artificial Intelligence (AI) from the human driver by observing random responses with a 95% significance level (proportion of success statistically equal to 50%). However, in the case of moderate braking and lane keeping at >100 km/h and in high traffic congestion, respondents recognized AI control from the human driver above pure chance, with 62–69% correct response rates. These findings, if confirmed in other case studies, could significantly impact on AVs acceptability, also contributing to their design as well as to long-debated ethical questions. AI driving software could be designed and tested for “humanity”, as long as safety is guaranteed, and autonomous cars could be allowed to circulate as long as they cannot be distinguished from human-driven vehicles in recurrent driving conditions.",autonomous vehicle
10.1016/j.dsp.2021.103290,preprocessed,Digital Signal Processing: A Review Journal,scopus,2022-01-01,sciencedirect,deep residual learning-based cognitive model for detection and classification of transmitted signal patterns in 5g smart city networks,https://api.elsevier.com/content/abstract/scopus_id/85118634214,"Primary user (PU) signal detection or classification is a critical component of cognitive radio (CR) related wireless communication applications. In CR, the PU detection methods are mostly based on statistical models, and their detection performance heavily relies on the accuracy of assumed models. In this paper, we design a novel detector, dubbed as PU-Net, that dynamically learns the PU activity patterns in a cognitive 5G smart city, where a network of unmanned aerial vehicles (UAVs) is deployed as flying base stations to serve the Internet-of-Things (IoT) users. Unlike the traditional schemes, the PU-Net is free from signal-noise model assumptions and is leveraged through deep residual learning integrated with atrous spatial pyramid pooling (ASPP) to sense the PU's transmitted signal patterns in the network. The PU-Net detects and classifies the active and idle PU states by exploiting the multilevel spatial-temporal features in the signal and noise frames. The proposed model is trained using locally synthesized Rayleigh channel-impaired data with large variability of modulated signals and different noise floor regimes. Additionally, the PU-Net model is blind-tested and evaluated on real-world over-the-air signals and with variable-length frames and varying channel effects at secondary users (SUs). With extensive experiments, it is shown that PU-Net outperforms other benchmark detectors, obtaining an accuracy of 0.9974, with 0.9978 recall and 0.9970 precision in detecting and classifying the PU transmitted signal patterns. Correspondingly, the proposed PU-Net can be adopted for IoT/UAV-assisted communication systems in optimizing spectrum efficiency and resolving the coexistence issues in 5G and beyond networks.",autonomous vehicle
10.1016/j.engappai.2021.104514,preprocessed,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,instance-based defense against adversarial attacks in deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",autonomous vehicle
10.1016/j.inffus.2021.07.004,preprocessed,Information Fusion,scopus,2022-01-01,sciencedirect,saccadefork: a lightweight multi-sensor fusion-based target detector,https://api.elsevier.com/content/abstract/scopus_id/85112374720,"Commercialization of self-driving applications requires precision and reliability of the perception system due to the highly dynamic and complex road environment. Early perception systems either rely on the camera or on LiDAR for moving obstacle detection. With the development of vehicular sensors and deep learning technologies, the multi-view and sensor fusion based convolutional neural network (CNN) model for detection tasks has become a popular research area. In this paper, we present a novel multi-sensor fusion-based CNN model–SaccadeFork–that integrates the image and upsampled LiDAR point clouds as the input. SaccadeFork includes two modules: (1) a lightweight backbone that consists of hourglass convolution feature extraction module and a parallel dilation convolution module for adaptation of the system to different target sizes; (2) an anchor-based detection head. The model also considers deployment of resource-limited edge devices in the vehicle. Two refinement strategies, i.e., Mixup and Swish activation function are also adopted to improve the model. Comparison with a series of latest models on public dataset of KITTI shows that SaccadeFork can achieve the optimal detection accuracy on vehicles and pedestrians under different scenarios. The final model is also deployed and tested on a local dataset collected based on edge devices and low-cost sensor solutions, and the results show that the model can achieve real-time efficiency and high detection accuracy.",autonomous vehicle
10.1109/jiot.2021.3096637,preprocessed,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,an integrated framework for health state monitoring in a smart factory employing iot and big data techniques,https://ieeexplore.ieee.org/document/9481251/,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",health
10.1109/tie.2021.3065616,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-03-01 00:00:00,ieeexplore,health management of dry-type transformer based on broad learning system,https://ieeexplore.ieee.org/document/9380956/,"This article presents a novel health management method of the dry-type transformer to diagnose the early unhealthy behavior and evaluate the transformer's health condition by health score. The health condition diagnosis implemented by a proposed dynamic-weighted-feed-back broad learning system (BLS) (DW-FB-BLS) method, which helps to determine the BLS network structure effectively, and adjusts the weight of features in the online application to avoid reduction of accuracy caused by concept drift. Then, a rational score rule is set to evaluate the health condition of the dry-type transformer by health score, which allows intuitive presentation and preservation of transformer's health condition over a long period. Finally, the effectiveness and validity of the proposed method are verified based on the real field data of dry-type transformer. Satisfactory results for unhealthy behavior diagnosis and health evaluation are obtained, it shows that health management of this article can reflect the real health condition of dry-type transformer appropriately.",health
10.1109/comsnets53615.2022.9668420,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,ml-based device-agnostic human activity detection with wifi sniffer traffic,https://ieeexplore.ieee.org/document/9668420/,"Human Activity Detection plays a pivotal role in smoothly managing the health care for the elderly and those with chronic health conditions in smart home environments. Even though there are several technological advancements made in this area, solutions like smartwatches are costly to afford and the solutions that rely on sensors and cameras suffer from privacy concerns. While wireless channel state information-based approaches can address some of these limitations, these approaches either require special hardware to be deployed or modifications at the WiFi access point. In this paper, we propose to detect human activities in a device-agnostic manner by leveraging passively captured passively captured WiFi MAC-layer traffic with the help of a sniffer. In that way, elderly people and those who suffer from chronic health conditions do not need to put any sensors on their body. This approach is not only cost-effective, but it is also easy to deploy without requiring any changes at the WiFi access point or installing special sensors in the environment. We train and test six off-the-shelf machine learning models on 15+ hours worth of WiFi MAC-layer traffic collected in a home environment. We present a proof-of-concept system prototype that employs this approach. We are able to detect six activities - (a) Walking vs Sitting, (b) Sleeping vs Awake, and (c) Using Phone vs Not Using Phone in three different real-world scenarios. Our evaluation reveals that WiFi MAC-layer traffic has special signatures to detect human activities and we are able to achieve 92.49 % detection accuracy in the best case.",health
10.1109/access.2021.3114590,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,correlation-aware sport training evaluation for players with trust based on mahalanobis distance,https://ieeexplore.ieee.org/document/9543705/,"With the widely-adopted idea of health and longevity, sports have been becoming one of the most popular entertainment ways of the public. For the majority of sports, players need to know about their concrete physical conditions in a real time manner so as to pursue a good sport score or ranking in a competition or a race. Generally, we can achieve the above goal through analyzing and evaluating the daily training scores of each player. However, there are often multiple physical trainings for players and various correlations are existent among them, which significantly decrease the fairness and trust of player training score evaluation and ranking since traditional multi-dimensional data integration solutions are often based on a strong hypothesis, i.e., the involved multiple dimensions are independent with each other. In view of this shortcoming, we introduce the Mahalanobis Distance into the multi-dimensional player training score evaluation and further propose a <underline>c</underline>orrelation-aware <underline>p</underline>layer training score <underline>e</underline>valuation method with trust (abbreviated as CPE<sub>MD</sub>) based on <underline>M</underline>ahalanobis <underline>D</underline>istance. As Mahalanobis Distance can eliminate the hidden linear correlations among the involved multiple dimensions, we can guarantee the fairness and trust of Mahalanobis Distance-based player training score evaluation and ranking results. At last, we use a case study to show the feasibility of CPE<sub>MD</sub> in this paper.",health
10.1109/tie.2021.3068681,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-03-01 00:00:00,ieeexplore,degradation estimation and prediction of electronic packages using data-driven approach,https://ieeexplore.ieee.org/document/9390342/,"Recent trends in automotive electronics such as automated driving will increase the number and complexity of electronics used in safety-relevant applications. Applications in logistics or ridesharing will require a specific year of service rather than the conventional mileage usage. Reliable operations of the electronic systems must be assured at all times, regardless of the usage condition. A more dynamic and on-demand way of assuring the system availability will have to be developed. This article proposes a thermomechanical stress-based prognostics method as a potential solution. The goal is achieved by several novel advancements. On the experimental front, a key microelectronics package is developed to directly apply the prognostics and health management concept using a piezoresistive silicon-based stress sensor. Additional hardware for safe and secure data transmission and data processing is also developed, which is critically required for recording <italic>in situ</italic> and real-time data. On the data management front, proper data-driven approaches have to be identified to handle the unique dataset from the stress sensor employed in this study. The approaches effectively handle the massive amount of data that reveals the important information and automation of the prognostic process and thus to be able to detect, classify, locate, and predict the failure. The statistical techniques for diagnostics and the machine learning algorithms for health assessment and prognostics are also determined to implement the approaches in a simple, fast, but accurate way within the capacity of limited computing power. The proposed prognostics approach is implemented with actual microelectronics packages subjected to harsh accelerated testing conditions. The results corroborate the validity of the proposed prognostics approach.",health
10.1109/jbhi.2021.3119325,preprocessed,IEEE Journal of Biomedical and Health Informatics,IEEE,2022-01-01 00:00:00,ieeexplore,mix-and-interpolate: a training strategy to deal with source-biased medical data,https://ieeexplore.ieee.org/document/9568732/,"Till March 31st, 2021, the coronavirus disease 2019 (COVID-19) had reportedly infected more than 127 million people and caused over 2.5 million deaths worldwide. Timely diagnosis of COVID-19 is crucial for management of individual patients as well as containment of the highly contagious disease. Having realized the clinical value of non-contrast chest computed tomography (CT) for diagnosis of COVID-19, deep learning (DL) based automated methods have been proposed to aid the radiologists in reading the huge quantities of CT exams as a result of the pandemic. In this work, we address an overlooked problem for training deep convolutional neural networks for COVID-19 classification using real-world multi-source data, namely, the <italic>data source bias</italic> problem. The data source bias problem refers to the situation in which certain sources of data comprise only a single class of data, and training with such source-biased data may make the DL models learn to distinguish data sources instead of COVID-19. To overcome this problem, we propose MIx-aNd-Interpolate (MINI), a conceptually simple, easy-to-implement, efficient yet effective training strategy. The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performance upon the baseline (which does not deal with the source bias), and 2) MINI is superior to competing methods in terms of the extent of improvement.",health
10.1109/tsmc.2020.3018102,preprocessed,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2022-02-01 00:00:00,ieeexplore,novel fast and automatic condition monitoring strategy based on small amount of labeled data,https://ieeexplore.ieee.org/document/9194082/,"Signal-based automatic condition monitoring techniques are one of the effective ways to ensure the operational safety of modern industrial systems. Currently, the main challenge is to increase their effectiveness under noisy environment and with limited labeled data. In this article, a novel fast and automatic signal-based fault diagnosis procedure that does not require any kinds of machine learning algorithms is proposed. This procedure is based on the measurement of the similarity in the frequency domain between the collected data and a small amount of labeled reference signals (LRSs). The LRSs are obtained under various known operation conditions using fast Fourier transform (FFT) algorithm. The proposed approach is suitable for a real-time implementation and capable of successfully overcoming the challenge of condition monitoring of rotating machines under noisy environment and with limited labeled data. The merits, fastness, and also robustness against noise of the proposed technique are demonstrated experimentally through different practical applications, as well as compared to state-of-the-art procedures on a database of vibration signals measured under a variety of machine health and working conditions.",health
10.1109/jbhi.2021.3088750,preprocessed,IEEE Journal of Biomedical and Health Informatics,IEEE,2022-01-01 00:00:00,ieeexplore,stroke risk prediction with hybrid deep transfer learning framework,https://ieeexplore.ieee.org/document/9453166/,"Stroke has become a leading cause of death and long-term disability in the world with no effective treatment. Deep learning-based approaches have the potential to outperform existing stroke risk prediction models, but they rely on large well-labeled data. Due to the strict privacy protection policy in health-care systems, stroke data is usually distributed among different hospitals in small pieces. In addition, the positive and negative instances of such data are extremely imbalanced. Transfer learning can solve small data issue by exploiting the knowledge of a correlated domain, especially when multiple source of data are available. In this work, we propose a novel Hybrid Deep Transfer Learning-based Stroke Risk Prediction (HDTL-SRP) scheme to exploit the knowledge structure from multiple correlated sources (i.e., external stroke data, chronic diseases data, such as hypertension and diabetes). The proposed framework has been extensively tested in synthetic and real-world scenarios, and it outperforms the state-of-the-art stroke risk prediction models. It also shows the potential of real-world deployment among multiple hospitals aided with 5 G/B5G infrastructures.",health
10.1109/access.2022.3141913,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,decentralized federated learning for healthcare networks: a case study on tumor segmentation,https://ieeexplore.ieee.org/document/9676574/,"Smart healthcare relies on artificial intelligence (AI) functions for learning and analysis of patient data. Since large and diverse datasets for training of Machine Learning (ML) models can rarely be found in individual medical centers, classical centralized AI requires moving privacy-sensitive data from medical institutions to data centers that process the fused information. Training on data centers thus requires higher communication resource/energy demands while violating privacy. This is considered today as a significant bottleneck in pursuing scientific collaboration across trans-national clinical medical research centers. Recently, federated learning (FL) has emerged as a distributed AI approach that enables the cooperative training of ML models, without the need of sharing patient data. This paper dives into the analysis of different FL methods and proposes a real-time distributed networking framework based on the Message Queuing Telemetry Transport (MQTT) protocol. In particular, we design a number of solutions for ML over networks, based on FL tools relying on a parameter server (PS) and fully decentralized paradigms driven by consensus methods. The proposed approach is validated in the context of brain tumor segmentation, using a modified version of the popular U-NET model with representative clinical datasets obtained from the daily clinical workflow. The FL process is implemented on multiple physically separated machines located in different countries and communicating over the Internet. The real-time test-bed is used to obtain measurements of training accuracy vs. latency trade-offs, and to highlight key operational conditions that affect the performance in real deployments.",health
10.1109/tii.2021.3093905,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,"modeling, detecting, and mitigating threats against industrial healthcare systems: a combined software defined networking and reinforcement learning approach",https://ieeexplore.ieee.org/document/9470933/,"The rise of the Internet of Medical Things introduces the healthcare ecosystem in a new digital era with multiple benefits, such as remote medical assistance, real-time monitoring, and pervasive control. However, despite the valuable healthcare services, this progression raises significant cybersecurity and privacy concerns. In this article, we focus our attention on the IEC 60 870-5-104 protocol, which is widely adopted in industrial healthcare systems. First, we investigate and assess the severity of the IEC 60 870-5-104 cyberattacks by providing a quantitative threat model, which relies on Attack Defence Trees and Common Vulnerability Scoring System v3.1. Next, we introduce an intrusion detection and prevention system (IDPS), which is capable of discriminating and mitigating automatically the IEC 60 870-5-104 cyberattacks. The proposed IDPS takes full advantage of the machine learning (ML) and software defined networking (SDN) technologies. ML is used to detect the IEC 60 870-5-104 cyberattacks, utilizing 1) Transmission Control Protocol/Internet Protocol network flow statistics and 2) IEC 60 870-5-104 payload flow statistics. On the other side, the automated mitigation is transformed into a multiarmed bandit problem, which is solved through a reinforcement learning method called Thomson sampling and SDN. The evaluation analysis demonstrates the efficiency of the proposed IDPS in terms of intrusion detection accuracy and automated mitigation performance. The detection accuracy and the F1 score of the proposed IDPS reach 0.831 and 0.8258, respectively, while the mitigation accuracy is calculated at 0.923.",health
10.1007/978-3-030-64573-1_164,preprocessed,Artificial Intelligence in Medicine,Springer,2022-01-01 00:00:00,springer,aim in endoscopy procedures,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-64573-1_164,"Artificial intelligence (AI) is revolutionizing the way medicine is practiced. In this context, the application of AI algorithms in endoscopy is gaining increasing attention so that modern endoscopy is moving towards more and more assisted/automatic solutions. Several approaches have been carried out in order to improve accuracy in diagnosis and surgical procedures. In this chapter, a general overview of the main contributions in the field is surveyed. Four main categories of applications were identified, namely, (i) detection and diagnosis during endoscopic procedure, (ii) informative frame selection, (iii) mosaicking and surface reconstruction, (iv) augmented reality systems for intraoperative assistance and surgeon training. Discussions on future research directions and implementation in clinical practice are provided.",health
10.1007/978-3-030-80928-7_10,preprocessed,Machine Learning for Critical Internet of Medical Things,Springer,2022-01-01 00:00:00,springer,aiiomt: iomt-based system-enabled artificial intelligence for enhanced smart healthcare systems,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80928-7_10,"The healthcare system has been on the frontline in recent years, researchers have tried to find solutions to different illnesses and sickness by applying various modern methods. But the major difference among them is that in recent years, other powerful new tools have emerged, which could be used as an instrument in the healthcare system and keeping it within reasonable limits. One of those technological tools is the Internet of Medical Things (IoMT) and Artificial intelligence (AI). Recently, AI enabled with IoMT-based systems is causing a paradigm shift in the healthcare zone, and the applicability might yield profit especially in diagnosis, prediction, and treatment of different diseases outbreak. The application of AI enabled with IoT-based systems in the healthcare system can be expediting the diagnoses and monitoring of disease and minimizes the burden of medical processes. Therefore, this chapter reviews the applicability of AiIoMT-based system in healthcare systems and the research challenges in deployment of AiIoMT system. The chapter also proposed an AiIoMT-based framework for diagnosis and monitoring of patients in real time. The model was tested using cytology image dataset and evaluated based on accuracy, sensitivity, specificity, F-score, and precision. The findings show a greater diagnosis accuracy of 99.5%, which shows that the AI model is a promising algorithm for the diagnosis of diseases in an IoMT-based system. The diagnosis, prediction, treatment, screening, and medication in the healthcare system have significantly improved with the continuing expansion in the methods having seriously reduced human intervention in medical practice.",health
http://arxiv.org/abs/2202.10336v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,artificial intelligence for the metaverse: a survey,http://arxiv.org/abs/2202.10336v1,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.",health
http://arxiv.org/abs/2202.04361v2,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,"molecular-scale integration of multi-modal sensing and neuromorphic
  computing with organic electrochemical transistors",http://arxiv.org/abs/2202.04361v2,"Abstract: Bionic learning with fused sensing, memory and processing functions
outperforms artificial neural networks running on silicon chips in terms of
efficiency and footprint. However, digital hardware implementation of bionic
learning suffers from device heterogeneity in sensors and processing cores,
which incurs large hardware, energy and time overheads. Here, we present a
universal solution to simultaneously perform multi-modal sensing, memory and
processing using organic electrochemical transistors with designed architecture
and tailored channel morphology, selective ion injection into the
crystalline/amorphous regions. The resultant device work as either a volatile
receptor that shows multi-modal sensing, or a non-volatile synapse that
features record-high 10-bit analog states, low switching stochasticity and good
retention without the integration of any extra devices. Homogeneous integration
of such devices enables bionic learning functions such as conditioned reflex
and real-time cardiac disease diagnose via reservoir computing, illustrating
the promise for future smart edge health informatics.",health
http://arxiv.org/abs/2202.02559v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,"digital twin of wireless systems: overview, taxonomy, challenges, and
  opportunities",http://arxiv.org/abs/2202.02559v1,"Future wireless services must be focused on improving the quality of life by
enabling various applications, such as extended reality, brain-computer
interaction, and healthcare. These applications have diverse performance
requirements (e.g., user-defined quality of experience metrics, latency, and
reliability) that are challenging to be fulfilled by existing wireless systems.
To meet the diverse requirements of the emerging applications, the concept of a
digital twin has been recently proposed. A digital twin uses a virtual
representation along with security-related technologies (e.g., blockchain),
communication technologies (e.g., 6G), computing technologies (e.g., edge
computing), and machine learning, so as to enable the smart applications. In
this tutorial, we present a comprehensive overview on digital twins for
wireless systems. First, we present an overview of fundamental concepts (i.e.,
design aspects, high-level architecture, and frameworks) of digital twin of
wireless systems. Second, a comprehensive taxonomy is devised for both
different aspects. These aspects are twins for wireless and wireless for twins.
For the twins for wireless aspect, we consider parameters, such as twin objects
design, prototyping, deployment trends, physical devices design, interface
design, incentive mechanism, twins isolation, and decoupling. On the other
hand, for wireless for twins, parameters such as, twin objects access aspects,
security and privacy, and air interface design are considered. Finally, open
research challenges and opportunities are presented along with causes and
possible solutions.",health
http://arxiv.org/abs/2202.01176v1,preprocessed,arxiv,arxiv,2022-02-02 00:00:00,arxiv,epidemic dreams: dreaming about health during the covid-19 pandemic,http://arxiv.org/abs/2202.01176v1,"The continuity hypothesis of dreams suggests that the content of dreams is
continuous with the dreamer's waking experiences. Given the unprecedented
nature of the experiences during COVID-19, we studied the continuity hypothesis
in the context of the pandemic. We implemented a deep-learning algorithm that
can extract mentions of medical conditions from text and applied it to two
datasets collected during the pandemic: 2,888 dream reports (dreaming life
experiences), and 57M tweets mentioning the pandemic (waking life experiences).
The health expressions common to both sets were typical COVID-19 symptoms
(e.g., cough, fever, and anxiety), suggesting that dreams reflected people's
real-world experiences. The health expressions that distinguished the two sets
reflected differences in thought processes: expressions in waking life
reflected a linear and logical thought process and, as such, described
realistic symptoms or related disorders (e.g., nasal pain, SARS, H1N1); those
in dreaming life reflected a thought process closer to the visual and emotional
spheres and, as such, described either conditions unrelated to the virus (e.g.,
maggots, deformities, snakebites), or conditions of surreal nature (e.g., teeth
falling out, body crumbling into sand). Our results confirm that dream reports
represent an understudied yet valuable source of people's health experiences in
the real world.",health
http://arxiv.org/abs/2202.01034v1,preprocessed,arxiv,arxiv,2022-02-02 00:00:00,arxiv,"maintaining fairness across distribution shift: do we have viable
  solutions for real-world applications?",http://arxiv.org/abs/2202.01034v1,"Fairness and robustness are often considered as orthogonal dimensions when
evaluating machine learning models. However, recent work has revealed
interactions between fairness and robustness, showing that fairness properties
are not necessarily maintained under distribution shift. In healthcare
settings, this can result in e.g. a model that performs fairly according to a
selected metric in ""hospital A"" showing unfairness when deployed in ""hospital
B"". While a nascent field has emerged to develop provable fair and robust
models, it typically relies on strong assumptions about the shift, limiting its
impact for real-world applications. In this work, we explore the settings in
which recently proposed mitigation strategies are applicable by referring to a
causal framing. Using examples of predictive models in dermatology and
electronic health records, we show that real-world applications are complex and
often invalidate the assumptions of such methods. Our work hence highlights
technical, practical, and engineering gaps that prevent the development of
robustly fair machine learning models for real-world applications. Finally, we
discuss potential remedies at each step of the machine learning pipeline.",health
http://arxiv.org/abs/2201.07711v1,preprocessed,arxiv,arxiv,2022-01-19 00:00:00,arxiv,enhancing the security & privacy of wearable brain-computer interfaces,http://arxiv.org/abs/2201.07711v1,"Brain computing interfaces (BCI) are used in a plethora of
safety/privacy-critical applications, ranging from healthcare to smart
communication and control. Wearable BCI setups typically involve a head-mounted
sensor connected to a mobile device, combined with ML-based data processing.
Consequently, they are susceptible to a multiplicity of attacks across the
hardware, software, and networking stacks used that can leak users' brainwave
data or at worst relinquish control of BCI-assisted devices to remote
attackers. In this paper, we: (i) analyse the whole-system security and privacy
threats to existing wearable BCI products from an operating system and
adversarial machine learning perspective; and (ii) introduce Argus, the first
information flow control system for wearable BCI applications that mitigates
these attacks. Argus' domain-specific design leads to a lightweight
implementation on Linux ARM platforms suitable for existing BCI use-cases. Our
proof of concept attacks on real-world BCI devices (Muse, NeuroSky, and
OpenBCI) led us to discover more than 300 vulnerabilities across the stacks of
six major attack vectors. Our evaluation shows Argus is highly effective in
tracking sensitive dataflows and restricting these attacks with an acceptable
memory and performance overhead (<15%).",health
http://arxiv.org/abs/2201.07888v1,preprocessed,arxiv,arxiv,2022-01-16 00:00:00,arxiv,"adaptive energy management for self-sustainable wearables in mobile
  health",http://arxiv.org/abs/2201.07888v1,"Wearable devices that integrate multiple sensors, processors, and
communication technologies have the potential to transform mobile health for
remote monitoring of health parameters. However, the small form factor of the
wearable devices limits the battery size and operating lifetime. As a result,
the devices require frequent recharging, which has limited their widespread
adoption. Energy harvesting has emerged as an effective method towards
sustainable operation of wearable devices. Unfortunately, energy harvesting
alone is not sufficient to fulfill the energy requirements of wearable devices.
This paper studies the novel problem of adaptive energy management towards the
goal of self-sustainable wearables by using harvested energy to supplement the
battery energy and to reduce manual recharging by users. To solve this problem,
we propose a principled algorithm referred as AdaEM. There are two key ideas
behind AdaEM. First, it uses machine learning (ML) methods to learn predictive
models of user activity and energy usage patterns. These models allow us to
estimate the potential of energy harvesting in a day as a function of the user
activities. Second, it reasons about the uncertainty in predictions and
estimations from the ML models to optimize the energy management decisions
using a dynamic robust optimization (DyRO) formulation. We propose a
light-weight solution for DyRO to meet the practical needs of deployment. We
validate the AdaEM approach on a wearable device prototype consisting of solar
and motion energy harvesting using real-world data of user activities.
Experiments show that AdaEM achieves solutions that are within 5% of the
optimal with less than 0.005% execution time and energy overhead.",health
http://arxiv.org/abs/2201.05115v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,functional anomaly detection: a benchmark study,http://arxiv.org/abs/2201.05115v1,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",health
http://arxiv.org/abs/2202.00478v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,"neurahealthnlp: an automated screening pipeline to detect undiagnosed
  cognitive impairment in electronic health records with deep learning and
  natural language processing",http://arxiv.org/abs/2202.00478v1,"Dementia related cognitive impairment (CI) affects over 55 million people
worldwide and is growing rapidly at the rate of one new case every 3 seconds.
With a recurring failure of clinical trials, early diagnosis is crucial, but
75% of dementia cases go undiagnosed globally with up to 90% in
low-and-middle-income countries. Current diagnostic methods are notoriously
complex, involving manual review of medical notes, numerous cognitive tests,
expensive brain scans or spinal fluid tests. Information relevant to CI is
often found in the electronic health records (EHRs) and can provide vital clues
for early diagnosis, but a manual review by experts is tedious and error prone.
This project develops a novel state-of-the-art automated screening pipeline for
scalable and high-speed discovery of undetected CI in EHRs. To understand the
linguistic context from complex language structures in EHR, a database of 8,656
sequences was constructed to train attention-based deep learning natural
language processing model to classify sequences. A patient level prediction
model based on logistic regression was developed using the sequence level
classifier. The deep learning system achieved 93% accuracy and AUC = 0.98 to
identify patients who had no earlier diagnosis, dementia-related diagnosis
code, or dementia-related medications in their EHR. These patients would have
otherwise gone undetected or detected too late. The EHR screening pipeline was
deployed in NeuraHealthNLP, a web application for automated and real-time CI
screening by simply uploading EHRs in a browser. NeuraHealthNLP is cheaper,
faster, more accessible, and outperforms current clinical methods including
text-based analytics and machine learning approaches. It makes early diagnosis
viable in regions with scarce health care services but accessible internet or
cellular services.",health
http://arxiv.org/abs/2201.04967v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,"adherence forecasting for guided internet-delivered cognitive behavioral
  therapy: a minimally data-sensitive approach",http://arxiv.org/abs/2201.04967v1,"Internet-delivered psychological treatments (IDPT) are seen as an effective
and scalable pathway to improving the accessibility of mental healthcare.
Within this context, treatment adherence is an especially relevant challenge to
address due to the reduced interaction between healthcare professionals and
patients, compared to more traditional interventions. In parallel, there are
increasing regulations when using peoples' personal data, especially in the
digital sphere. In such regulations, data minimization is often a core tenant
such as within the General Data Protection Regulation (GDPR). Consequently,
this work proposes a deep-learning approach to perform automatic adherence
forecasting, while only relying on minimally sensitive login/logout data. This
approach was tested on a dataset containing 342 patients undergoing guided
internet-delivered cognitive behavioral therapy (G-ICBT) treatment. The
proposed Self-Attention Network achieved over 70% average balanced accuracy,
when only 1/3 of the treatment duration had elapsed. As such, this study
demonstrates that automatic adherence forecasting for G-ICBT, is achievable
using only minimally sensitive data, thus facilitating the implementation of
such tools within real-world IDPT platforms.",health
http://arxiv.org/abs/2201.01943v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,"machine learning: algorithms, models, and applications",http://arxiv.org/abs/2201.01943v1,"Recent times are witnessing rapid development in machine learning algorithm
systems, especially in reinforcement learning, natural language processing,
computer and robot vision, image processing, speech, and emotional processing
and understanding. In tune with the increasing importance and relevance of
machine learning models, algorithms, and their applications, and with the
emergence of more innovative uses cases of deep learning and artificial
intelligence, the current volume presents a few innovative research works and
their applications in real world, such as stock trading, medical and healthcare
systems, and software automation. The chapters in the book illustrate how
machine learning and deep learning algorithms and models are designed,
optimized, and deployed. The volume will be useful for advanced graduate and
doctoral students, researchers, faculty members of universities, practicing
data scientists and data engineers, professionals, and consultants working on
the broad areas of machine learning, deep learning, and artificial
intelligence.",health
10.1016/j.physc.2021.1354007,preprocessed,Physica C: Superconductivity and its Applications,scopus,2022-02-15,sciencedirect,optical fibre based quench detection in hts applications using machine learning classifiers,https://api.elsevier.com/content/abstract/scopus_id/85122309535,"A Mach-Zehnder Interferometer (MZI) based optical fibre sensing technique, developed and patented by EPFL, is an efficient and economical way to detect hotspots in High Temperature Superconductor (HTS) applications. Due to the MZI sensitivity being a composite of strain sensitive and temperature sensitive contributions, the MZI gives an instantaneous response to a quench (within 10 ms), because of the quick strain transfer to the optical fibre. However, the MZI output signal can also manifest the environmental noise caused by mechanical vibrations, bubbling in the cryostat and temperature variations, along with the response to the quench. This presents the problems of false alarms and indiscernible response to a quench. Discrete wavelet transform (DWT) has been proven to be a useful tool for feature extraction in different fields requiring signal categorization and hence holds the potential to enable quench recognition in the MZI output. This paper proposes an effective approach of performing DWT based feature extraction on experimental data and subsequently using the extracted features for the MZI response classification using two machine learning based classification techniques: k-nearest neighbours (KNN) and Artificial Neural Network (ANN). For this manuscript, experiments were performed using MZI for quench detection in an HTS tape. Feature extraction was then implemented on these experimental measurements using discrete wavelet coefficients extracted at different decomposition levels from the MZI output; these features were then used to train the KNN and ANN models for identifying quench in the MZI signal. This method could be a valuable supplement to the MZI technique by enabling the development of a real time application that can process the MZI output data as well as eliminate the occurrences of false alarms; thereby facilitating reliable quench detection. With this development, the MZI technique would become an even more attractive solution for the health monitoring of HTS applications.",health
10.1016/j.comnet.2021.108661,preprocessed,Computer Networks,scopus,2022-02-11,sciencedirect,evaluating federated learning for intrusion detection in internet of things: review and challenges,https://api.elsevier.com/content/abstract/scopus_id/85121903251,"The application of Machine Learning (ML) techniques to the well-known intrusion detection systems (IDS) is key to cope with increasingly sophisticated cybersecurity attacks through an effective and efficient detection process. In the context of the Internet of Things (IoT), most ML-enabled IDS approaches use centralized approaches where IoT devices share their data with data centers for further analysis. To mitigate privacy concerns associated with centralized approaches, in recent years the use of Federated Learning (FL) has attracted a significant interest in different sectors, including healthcare and transport systems. However, the development of FL-enabled IDS for IoT is in its infancy, and still requires research efforts from various areas, in order to identify the main challenges for the deployment in real-world scenarios. In this direction, our work evaluates a FL-enabled IDS approach based on a multiclass classifier considering different data distributions for the detection of different attacks in an IoT scenario. In particular, we use three different settings that are obtained by partitioning the recent ToN_IoT dataset according to IoT devices’ IP address and types of attack. Furthermore, we evaluate the impact of different aggregation functions according to such setting by using the recent IBMFL framework as FL implementation. Additionally, we identify a set of challenges and future directions based on the existing literature and the analysis of our evaluation results.",health
10.1016/j.ces.2021.117205,preprocessed,Chemical Engineering Science,scopus,2022-02-02,sciencedirect,"developments of leak detection, diagnostics, and prediction algorithms in multiphase flows",https://api.elsevier.com/content/abstract/scopus_id/85118896502,"Leak detection, diagnostics, and prediction constitute a crucial phase of the flow assurance risk management process for onshore and offshore pipelines. There are a variety of techniques and algorithms that can be deployed to address each aspect. To date, most review papers have concentrated on steady-state and single-phase flow conditions. The goal of the current review is therefore to carry out a thorough analysis of the available leak detection and diagnosis methods by focusing on (i) multiphase flow and transient flow conditions, (ii) model-based and data-driven techniques, (iii) prediction tools, and (iv) performance measures. Detailed assessment of leak detection methods based on accuracy, complexity, data requirement, and cost of installation are discussed. Data-driven techniques are utterly dependent on qualitative and quantitative data available from pipeline systems. Contrastingly data-driven techniques, model-based techniques require less data to achieve leak detection, provided that a nearly accurate base model is available. Different methodologies and technologies can be combined in order to produce the best detection and diagnosis outputs. In many cases, statistical analysis was combined with the Real Time Transient Method (RTTM), which helped to minimize false alarms. The material in this review can be used as a robust guide for the design of diagnostic systems and further research.",health
10.1016/j.cjca.2021.11.009,preprocessed,Canadian Journal of Cardiology,scopus,2022-02-01,sciencedirect,a primer on the present state and future prospects for machine learning and artificial intelligence applications in cardiology,https://api.elsevier.com/content/abstract/scopus_id/85122266625,"The artificial intelligence (AI) revolution is well underway, including in the medical field, and has dramatically transformed our lives. An understanding of the basics of AI applications, their development, and challenges to their clinical implementation is important for clinicians to fully appreciate the possibilities of AI. Such a foundation would ensure that clinicians have a good grasp and realistic expectations for AI in medicine and prevent discrepancies between the promised and real-world impact. When quantifying the track record for AI applications in cardiology, we found that a substantial number of AI systems are never deployed in clinical practice, although there certainly are many success stories. Successful implementations shared the following: they came from clinical areas where large amount of training data was available; were deployable into a single diagnostic modality; prediction models generally had high performance in external validation; and most were developed as part of collaborations with medical device manufacturers who had substantial experience with implementation of new clinical technology. When looking into the current processes used for developing AI-based systems, we suggest that expanding the analytic framework to address potential deployment and implementation issues at project outset will improve the rate of successful implementation, and will be a necessary next step for AI to achieve its full potential in cardiovascular medicine.",health
10.1016/j.compbiomed.2021.105144,preprocessed,Computers in Biology and Medicine,scopus,2022-02-01,sciencedirect,domain generalization on medical imaging classification using episodic training with task augmentation,https://api.elsevier.com/content/abstract/scopus_id/85121969937,"Medical imaging datasets usually exhibit domain shift due to the variations of scanner vendors, imaging protocols, etc. This raises the concern about the generalization capacity of machine learning models. Domain generalization (DG), which aims to learn a model from multiple source domains such that it can be directly generalized to unseen test domains, seems particularly promising to medical imaging community. To address DG, recent model-agnostic meta-learning (MAML) has been introduced, which transfers the knowledge from previous training tasks to facilitate the learning of novel testing tasks. However, in clinical practice, there are usually only a few annotated source domains available, which decreases the capacity of training task generation and thus increases the risk of overfitting to training tasks in the paradigm. In this paper, we propose a novel DG scheme of episodic training with task augmentation on medical imaging classification. Based on meta-learning, we develop the paradigm of episodic training to construct the knowledge transfer from episodic training-task simulation to the real testing task of DG. Motivated by the limited number of source domains in real-world medical deployment, we consider the unique task-level overfitting and we propose task augmentation to enhance the variety during training task generation to alleviate it. With the established learning framework, we further exploit a novel meta-objective to regularize the deep embedding of training domains. To validate the effectiveness of the proposed method, we perform experiments on histopathological images and abdominal CT images.",health
10.1016/j.scs.2021.103559,preprocessed,Sustainable Cities and Society,scopus,2022-02-01,sciencedirect,assessment of sustainable development objectives in smart labs: technology and sustainability at the service of society,https://api.elsevier.com/content/abstract/scopus_id/85120052266,"Sustainable development is the working basis of engineering research and cities are becoming increasingly flexible, inclusive and intelligent. In this context, there is a need for environments that emulate real-life spaces in which cutting-edge technologies can be implemented for subsequent deployment in society. Smart Labs or Living Labs are spaces for innovation, research and experimentation that integrate systems, devices and methodologies focused on people and their environments. The technologies studied and developed in such labs can then be deployed in human spaces to provide intelligence, comfort, health and sustainability. Health and wellness, energy and environment, artificial intelligence, big data and digital rights are some of the disciplines being studied. At the same time, the UN 2030 Agenda provides a comprehensive framework to promote human well-being through the Sustainable Development Goals. In this work, an evaluation model of its indicators in smart environments is performed through a mixed review methodology. The objective of this work is the analysis and implementation of the SDGs in Smart Labs through a literature review and a case study of UJAmI, the smart laboratory of the University of Jaén. The results provide quantitative and qualitative data on the present and future of the smart devices implemented in the UJAmI lab, providing a roadmap for future developments.",health
10.1016/j.jiac.2021.10.027,preprocessed,Journal of Infection and Chemotherapy,scopus,2022-02-01,sciencedirect,a study of quality assessment in sars-cov-2 pathogen nucleic acid amplification tests performance; from the results of external quality assessment survey of clinical laboratories in the tokyo metropolitan government external quality assessment program in 2020,https://api.elsevier.com/content/abstract/scopus_id/85119258737,"Introduction
                  The Tokyo Metropolitan Government (TMG) conducted an external quality assessment (EQA) survey of pathogen nucleic acid amplification tests (NAATs) as a TMG EQA program for SARS-CoV-2 for clinical laboratories in Tokyo.
               
                  Methods
                  We diluted and prepared a standard product manufactured by Company A to about 2,500 copies/mL to make a positive control and distribute it with a negative control. The participants reported the use of the NAATs methods for SARS-CoV-2, the name of the real-time RT-PCR kit, the name of the detection device, the target gene(s), nucleic acid extraction kit, Threshold Cycle value in the case of RT-PCR and the Threshold time value and Differential calculation value in the case of Loop-Mediated Isothermal Amplification (LAMP) method.
               
                  Results
                  As a result, 17 laboratories using fully automated equipment and 34 laboratories using the RT-PCR method reported generally appropriate results in this EQA survey. On the other hand, among the laboratories that adopted the LAMP method, there were a plurality of laboratories that judged positive samples to be negative.
               
                  Conclusion
                  The false negative result is considered to be due to the fact that the amount of virus genome contained in the quality control reagent used this time was below the detection limit of the LAMP method combined with the rapid extraction reagent for influenza virus. On the other hand, false positive results are considered to be due to the non-specific reaction of the NAATs. The EQA program must be continued for the proper implementation of the pathogen NAATs.",health
10.1016/j.ress.2021.108119,preprocessed,Reliability Engineering and System Safety,scopus,2022-02-01,sciencedirect,prognostics and health management (phm): where are we and where do we (need to) go in theory and practice,https://api.elsevier.com/content/abstract/scopus_id/85117331443,"We are performing the digital transition of industry, living the 4th industrial revolution, building a new World in which the digital, physical and human dimensions are interrelated in complex socio-cyber-physical systems. For the sustainability of these transformations, knowledge, information and data must be integrated within model-based and data-driven approaches of Prognostics and Health Management (PHM) for the assessment and prediction of structures, systems and components (SSCs) evolutions and process behaviors, so as to allow anticipating failures and avoiding accidents, thus, aiming at improved safe and reliable design, operation and maintenance.
                  There is already a plethora of methods available for many potential applications and more are being developed: yet, there are still a number of critical problems which impede full deployment of PHM and its benefits in practice. In this respect, this paper does not aim at providing a survey of existing works for an introduction to PHM nor at providing new tools or methods for its further development; rather, it aims at pointing out main challenges and directions of advancements, for full deployment of condition-based and predictive maintenance in practice.",health
10.1016/j.future.2021.08.030,preprocessed,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,a wearable-based posture recognition system with ai-assisted approach for healthcare iot,https://api.elsevier.com/content/abstract/scopus_id/85115908462,"Human posture recognition is a challenging task in the medical healthcare industry, when pursuing intelligence, accuracy, security, privacy, and efficiency, etc. Currently, the main posture recognition methods are captured-behaviors-based visual image analysis and wearable devices-based signal analysis. However, these methods suffer from issues such as high misjudgment rate, high-cost and low-efficiency. To address these issues, we propose a collaborative AI-IoT-based solution (namely, WMHPR) that embeds with advanced AI-assisted approach. In WMHPR, we propose the multi-posture recognition (MPR), an offline algorithm is implemented on wearable hardware, to identify posture based on multi-dimensions data. Meanwhile, an AI-based algorithm running on the cloud server (online), named Cascade-AdaBoosting-CART (CACT), is proposed to further enhance the reliability and accuracy of MPR. We recruit 20 volunteers for real-life experiments to evaluate the effectiveness, and the results show our solution is significantly outstanding in terms of accuracy and reliability while comparing with other typical algorithms.",health
10.1016/j.future.2021.09.010,preprocessed,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,xsru-iomt: explainable simple recurrent units for threat detection in internet of medical things networks,https://api.elsevier.com/content/abstract/scopus_id/85115376405,"The Internet of Medical Things (IoMT) is increasingly replacing the traditional healthcare systems. However, less focus has been paid to their security against cyber-threats in the implementation of the IoMT and its networks. One of the key reasons can be the challenging task of optimizing typical security solutions to the IoMT networks. And despite the rising admiration of machine learning and deep learning methods in the cyber-security domain (e.g., a threat detection system), most of these methods are acknowledged as a black-box model. The explainable AI (XAI) has become progressively vital to understand the employed learning models to improve trust level and empower security experts to interpret the prediction decisions. The authors propose a highly efficient model named XSRU-IoMT, for effective and timely detection of sophisticated attack vectors in IoMT networks. The proposed model is developed using novel bidirectional simple recurrent units (SRU) using the phenomenon of skip connections to eradicate the vanishing gradient problem and achieve a fast training process in recurrent networks. We also explore the concepts of XAI to improve trust level by providing explanations of the predictive decisions and enabling humans and security experts to understand the causal reasoning and underlying data evidence. The evaluation results on the ToN_IoT dataset demonstrate the effectiveness and superiority of the proposed XSRU-IoMT model as compared to the state-of-the-art compelling detection models, suggesting its usefulness as a viable deployment model in real-IoMT networks.",health
10.1016/j.aej.2021.06.024,preprocessed,Alexandria Engineering Journal,scopus,2022-02-01,sciencedirect,"automatic diagnosis of covid-19 disease using deep convolutional neural network with multi-feature channel from respiratory sound data: cough, voice, and breath",https://api.elsevier.com/content/abstract/scopus_id/85109458695,"The problem of respiratory sound classification has received good attention from the clinical scientists and medical researcher’s community in the last year to the diagnosis of COVID-19 disease. The Artificial Intelligence (AI) based models deployed into the real-world to identify the COVID-19 disease from human-generated sounds such as voice/speech, dry cough, and breath. The CNN (Convolutional Neural Network) is used to solve many real-world problems with Artificial Intelligence (AI) based machines. We have proposed and implemented a multi-channeled Deep Convolutional Neural Network (DCNN) for automatic diagnosis of COVID-19 disease from human respiratory sounds like a voice, dry cough, and breath, and it will give better accuracy and performance than previous models. We have applied multi-feature channels such as the data De-noising Auto Encoder (DAE) technique, GFCC (Gamma-tone Frequency Cepstral Coefficients), and IMFCC (Improved Multi-frequency Cepstral Coefficients) methods on augmented data to extract the deep features for the input of the CNN. The proposed approach improves system performance to the diagnosis of COVID-19 disease and provides better results on the COVID-19 respiratory sound dataset.",health
10.1016/j.vaccine.2021.12.014,preprocessed,Vaccine,scopus,2022-01-28,sciencedirect,humoral response to the sars-cov-2 bnt162b2 mrna vaccine: real-world data from a large cohort of healthcare workers,https://api.elsevier.com/content/abstract/scopus_id/85121675841,"Background
                  The SARS-CoV-2 pandemic was responsible for the death of millions of people around the world, which accelerated the study of vaccines. The BNT162b2 mRNA COVID-19 is a messenger RNA vaccine that encodes the spike protein of the virus. However, the duration of the protection conferred by this vaccine and factors associated with immune responses require validation in large cohorts.
               
                  Methods
                  Here, we present data of humoral immune response to vaccination in4264 healthcare workers, tested before (T0) and 15 and 90 days (T1 and T2, respectively) following vaccination.Peripheral blood was collected for immunological analysis using the Quant SARS-CoV-2 IgG II Chemiluminescent Microparticle Immunoassay (CMIA) to determine anti-spike IgG, receptor binding domain (RBD), S1 subunit of SARS-CoV-2.
               
                  Findings
                  At T0, 96·8% (n = 4129) of participants had IgG antibodies non-reactive to anti-SARS-CoV-2. Fifteen days after completing the vaccination, the IgG overall median titer was significantly elevated (21·7x103
                     AU/mL). Both for uni- and multivariate logistic regression analyses women presented higher antibody levels than men, independent of age. Titers were significantly altered among age groups, decreasing by each increase in 10-year of age. At 3 months after completing the vaccination, anti-SARS-CoV-2 IgG titers were 6·3-fold diminished.
                  This real-world post-vaccination data confirmed production of a frequent and elevated anti-SARS-CoV-2 IgG titers, associated with high protection rates. Females and younger participants had higher titer 15 days after vaccination, and despite the significant reduction from 15-to-90 days, those with higher pre-vaccination titers maintained higher levels throughout the remaining timepoints.
               
                  Interpretation
                  These findings support the need to track humoral immunity kinetics to uncover viral susceptibility and eventually implement re-vaccination, particularly in groups prone to lower humoral immune response.
               
                  Funding
                  No external funding was received to conduct this study.",health
10.1016/j.ijhydene.2022.01.145,preprocessed,International Journal of Hydrogen Energy,scopus,2022-01-01,sciencedirect,real-time data-driven fault diagnosis of proton exchange membrane fuel cell system based on binary encoding convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85124312531,"The performance of proton exchange Membrane fuel cell (PEMFC) fault diagnosis system plays an important role in normal operation of PEMFC. Therefore, a new fault diagnosis algorithm based on binary matrix encoding neural network called BinE-CNN is proposed. In BinE-CNN, high-dimensional features are extracted through binary encoding, and the feature maps are transferred to a convolutional neural network (CNN) to realize seven-category fault classification. For development of BinE-CNN, a PEMFC model is modeled to generate simulative datasets. Simulative test precision and Frames per second (FPS) of BinE-CNN have reached respectively 0.973 and 999.8 (better than support vector machines (SVM), long short-term memory neural network (LSTM), etc.). In experimental verification section, fault datasets are collected during bench test. After that, BinE-CNN is deployed on vehicle control unit (VCU) to verify its engineering value (real-time and precision). The result meet both requirements, with time cost of 96.15 ms and precision of 0.931.",health
10.1016/j.medin.2021.12.005,preprocessed,Medicina Intensiva,scopus,2022-01-01,sciencedirect,impact of aspergillus spp. isolation in the first 24 hours of admission in critically ill patients with severe influenza virus pneumonia,https://api.elsevier.com/content/abstract/scopus_id/85124136387,"Objective
                  To determine the incidence and impact of Aspergillus spp. isolation (AI) on ICU mortality in critically ill patients with severe influenza pneumonia during the first 24h of admission.
               
                  Design
                  Secondary analysis of an observational and prospective cohort study.
               
                  Setting
                  ICUs voluntary participating in the Spanish severe Influenza pneumonia registry, between June 2009 and June 2019.
               
                  Patients
                  Consecutive patients admitted to the ICU with diagnosis of severe influenza pneumonia, confirmed by real-time polymerase chain reaction.
               
                  Interventions
                  None.
               
                  Main variables of interest
                  Incidence of AI in respiratory samples. Demographic variables, comorbidities, need for mechanical ventilation and the presence of shock according at admission. Acute Physiology and Chronic Health Evaluation II (APACHE II) scale calculated on ICU admission.
               
                  Results
                  3702 patients were analyzed in this study. AI incidence was 1.13% (n
                     =42). Hematological malignancies (OR 4.39, 95% CI 1.92–10.04); HIV (OR 3.83, 95% CI 1.08–13.63), and other immunosuppression situations (OR 4.87, 95% CI 1.99–11.87) were factors independently associated with the presence of Aspergillus spp. The automatic CHAID decision tree showed that hematologic disease with an incidence of 3.3% was the most closely AI related variable. Hematological disease (OR 2.62 95% CI 1.95–3.51), immunosuppression (OR 2.05 95% CI 1.46–2.88) and AI (OR 3.24, 95% CI 1.60–6.53) were variables independently associated with ICU mortality.
               
                  Conclusions
                  Empirical antifungal treatment in our population may only be justified in immunocompromised patients. In moderate-high risk cases, active search for Aspergillus spp. should be implemented.",health
10.1016/j.fmre.2021.12.005,preprocessed,Fundamental Research,scopus,2022-01-01,sciencedirect,ai-aided on-chip nucleic acid assay for smart diagnosis of infectious disease,https://api.elsevier.com/content/abstract/scopus_id/85122505591,"Global pandemics such as COVID-19 have resulted in significant global social and economic disruption. Although polymerase chain reaction (PCR) is recommended as the standard test for identifying the SARS-CoV-2, conventional assays are time-consuming. In parallel, although artificial intelligence (AI) has been employed to contain the disease, the implementation of AI in PCR analytics, which may enhance the cognition of diagnostics, is quite rare. The information that the amplification curve reveals can reflect the dynamics of reactions. Here, we present a novel AI-aided on-chip approach by integrating deep learning with microfluidic paper-based analytical devices (µPADs) to detect synthetic RNA templates of the SARS-CoV-2 ORF1ab gene. The µPADs feature a multilayer structure by which the devices are compatible with conventional PCR instruments. During analysis, real-time PCR data were synchronously fed to three unsupervised learning models with deep neural networks, including RNN, LSTM, and GRU. Of these, the GRU is found to be most effective and accurate. Based on the experimentally obtained datasets, qualitative forecasting can be made as early as 13 cycles, which significantly enhances the efficiency of the PCR tests by 67.5% (∼40 min). Also, an accurate prediction of the end-point value of PCR curves can be obtained by GRU around 20 cycles. To further improve PCR testing efficiency, we also propose AI-aided dynamic evaluation criteria for determining critical cycle numbers, which enables real-time quantitative analysis of PCR tests. The presented approach is the first to integrate AI for on-chip PCR data analysis. It is capable of forecasting the final output and the trend of qPCR in addition to the conventional end-point Cq calculation. It is also capable of fully exploring the dynamics and intrinsic features of each reaction. This work leverages methodologies from diverse disciplines to provide perspectives and insights beyond the scope of a single scientific field. It is universally applicable and can be extended to multiple areas of fundamental research.",health
10.1016/j.animal.2021.100432,preprocessed,Animal,scopus,2022-01-01,sciencedirect,a machine vision system to predict individual cow feed intake of different feeds in a cowshed,https://api.elsevier.com/content/abstract/scopus_id/85122307007,"Data on individual feed intake of dairy cows, an important variable for farm management, are currently unavailable in commercial dairies. A real-time machine vision system including models that are able to adapt to multiple types of feed was developed to predict individual feed intake of dairy cows. Using a Red-Green-Blue-Depth (RGBD) camera, images of feed piles of two different feed types (lactating cows' feed and heifers' feed) were acquired in a research dairy farm, for a range of feed weights under varied configurations and illuminations. Several models were developed to predict individual feed intake: two Transfer Learning (TL) models based on Convolutional Neural Networks (CNNs), one CNN model trained on both feed types, and one Multilayer Perceptron and Convolutional Neural Network model trained on both feed types, along with categorical data. We also implemented a statistical method to compare these four models using a Linear Mixed Model and a Generalised Linear Mixed Model, showing that all models are significantly different. The TL models performed best and were trained on both feeds with TL methods. These models achieved Mean Absolute Errors (MAEs) of 0.12 and 0.13 kg per meal with RMSE of 0.18 and 0.17 kg per meal for the two different feeds, when tested on varied data collected manually in a cowshed. Testing the model with actual cows’ meals data automatically collected by the system in the cowshed resulted in a MAE of 0.14 kg per meal and RMSE of 0.19 kg per meal. These results suggest the potential of measuring individual feed intake of dairy cows in a cowshed using RGBD cameras and Deep Learning models that can be applied and tuned to different types of feed.",health
10.1016/j.ebiom.2021.103774,preprocessed,eBioMedicine,scopus,2022-01-01,sciencedirect,accuracy and ease-of-use of seven point-of-care sars-cov-2 antigen-detecting tests: a multi-centre clinical evaluation,https://api.elsevier.com/content/abstract/scopus_id/85121647709,"Background
                  Antigen-detecting rapid diagnostic tests (Ag-RDTs) for SARS-CoV-2 are important diagnostic tools. We assessed clinical performance and ease-of-use of seven Ag-RDTs in a prospective, manufacturer-independent, multi-centre cross-sectional diagnostic accuracy study to inform global decision makers.
               
                  Methods
                  Unvaccinated participants suspected of a first SARS-CoV-2 infection were recruited at six sites (Germany, Brazil). Ag-RDTs were evaluated sequentially, with collection of paired swabs for routine reverse transcription polymerase chain reaction (RT-PCR) testing and Ag-RDT testing. Performance was compared to RT-PCR overall and in sub-group analyses (viral load, symptoms, symptoms duration). To understandusability a System Usability Scale (SUS) questionnaire and ease-of-use (EoU) assessment were performed.
               
                  Findings
                  7471 participants were included in the analysis. Sensitivities across Ag-RDTs ranged from 70·4%-90·1%, specificities were above 97·2% for all Ag-RDTs but one (93·1%).Ag-RDTs, Mologic, Bionote, Standard Q, showed diagnostic accuracy in line with WHO targets (> 80% sensitivity, > 97% specificity). All tests showed high sensitivity in the first three days after symptom onset (≥87·1%) and in individuals with viral loads≥ 6 log10SARS-CoV2 RNA copies/mL (≥ 88·7%). Usability varied, with Rapigen, Bionote and Standard Q reaching very good scores; 90, 88 and 84/100, respectively.
               
                  Interpretation
                  Variability in test performance is partially explained by variable viral loads in population evaluated over the course of the pandemic. All Ag-RDTs reach high sensitivity early in the disease and in individuals with high viral loads, supporting their role in identifying transmission relevant infections. For easy-to-use tests, performance shown will likely be maintained in routine implementation.
               
                  Funding
                  Ministry of Science, Research and Arts, State of Baden-Wuerttemberg, Germany, internal funds from Heidelberg University Hospital, University Hospital Charité − Universitätsmedizin Berlin, UK Department of International Development, WHO, Unitaid.",health
10.1016/j.measurement.2021.110491,preprocessed,Measurement: Journal of the International Measurement Confederation,scopus,2022-01-01,sciencedirect,chxcapsnet: deep capsule network with transfer learning for evaluating pneumonia in paediatric chest radiographs,https://api.elsevier.com/content/abstract/scopus_id/85121269671,"Pneumonia is the primary cause of death in children under the age of 5 years. Faster and more accurate laboratory testing aids in the prescription of appropriate treatment for children suspected of having pneumonia, lowering mortality. In this work, we implement a deep neural network model to efficiently evaluate pediatric pneumonia from chest radio graph images. Our network uses a combination of convolutional and capsule layers to capture abstract details as well as low level hidden features from the radio graphic images, allowing the model to generate more generic predictions. Furthermore, we employ transfer learning approach to extract spatial features from the raw input radio graph images, allowing the model to save resources while enhancing performance. The capsule layer weights of the network are updated using the dynamic routing algorithm. The proposed model is evaluated using benchmark pneumonia dataset Kermany et al. 2018, and the outcomes of our experimental studies indicate that the capsules employed in the network enhance the learning of disease level features that are essential in diagnosing pneumonia. According to our comparison studies, the proposed model with Convolution base from InceptionV3 attached with Capsule layers at the end surpasses several existing models by achieving an accuracy of 94.84%. The proposed model is superior in terms of various performance measures such as accuracy and recall, and is well suited to real-time pediatric pneumonia diagnosis, substituting manual chest radiography examination.",health
10.1016/s2589-7500(21)00211-9,preprocessed,The Lancet Digital Health,scopus,2022-01-01,sciencedirect,"deep learning-based classification of kidney transplant pathology: a retrospective, multicentre, proof-of-concept study",https://api.elsevier.com/content/abstract/scopus_id/85120858490,"Background
                  Histopathological assessment of transplant biopsies is currently the standard method to diagnose allograft rejection and can help guide patient management, but it is one of the most challenging areas of pathology, requiring considerable expertise, time, and effort. We aimed to analyse the utility of deep learning to preclassify histology of kidney allograft biopsies into three main broad categories (ie, normal, rejection, and other diseases) as a potential biopsy triage system focusing on transplant rejection.
               
                  Methods
                  We performed a retrospective, multicentre, proof-of-concept study using 5844 digital whole slide images of kidney allograft biopsies from 1948 patients. Kidney allograft biopsy samples were identified by a database search in the Departments of Pathology of the Amsterdam UMC, Amsterdam, Netherlands (1130 patients) and the University Medical Center Utrecht, Utrecht, Netherlands (717 patients). 101 consecutive kidney transplant biopsies were identified in the archive of the Institute of Pathology, RWTH Aachen University Hospital, Aachen, Germany. Convolutional neural networks (CNNs) were trained to classify allograft biopsies as normal, rejection, or other diseases. Three times cross-validation (1847 patients) and deployment on an external real-world cohort (101 patients) were used for validation. Area under the receiver operating characteristic curve (AUROC) was used as the main performance metric (the primary endpoint to assess CNN performance).
               
                  Findings
                  Serial CNNs, first classifying kidney allograft biopsies as normal (AUROC 0·87 [ten times bootstrapped CI 0·85–0·88]) and disease (0·87 [0·86–0·88]), followed by a second CNN classifying biopsies classified as disease into rejection (0·75 [0·73–0·76]) and other diseases (0·75 [0·72–0·77]), showed similar AUROC in cross-validation and deployment on independent real-world data (first CNN normal AUROC 0·83 [0·80–0·85], disease 0·83 [0·73–0·91]; second CNN rejection 0·61 [0·51–0·70], other diseases 0·61 [0·50–0·74]). A single CNN classifying biopsies as normal, rejection, or other diseases showed similar performance in cross-validation (normal AUROC 0·80 [0·73–0·84], rejection 0·76 [0·66–0·80], other diseases 0·50 [0·36–0·57]) and generalised well for normal and rejection classes in the real-world data. Visualisation techniques highlighted rejection-relevant areas of biopsies in the tubulointerstitium.
               
                  Interpretation
                  This study showed that deep learning-based classification of transplant biopsies could support pathological diagnostics of kidney allograft rejection.
               
                  Funding
                  European Research Council; German Research Foundation; German Federal Ministries of Education and Research, Health, and Economic Affairs and Energy; Dutch Kidney Foundation; Human(e) AI Research Priority Area of the University of Amsterdam; and Max-Eder Programme of German Cancer Aid.",health
10.1016/j.ergon.2021.103234,preprocessed,International Journal of Industrial Ergonomics,scopus,2022-01-01,sciencedirect,industrial intelligence in the care of workers’ mental health: a review of status and challenges,https://api.elsevier.com/content/abstract/scopus_id/85120173556,"Mental health is a current concern because people worldwide have been committed to disorders that impair lives as a whole, affecting emotional states, behaviors, and body responses. These disorders decrease worker's productivity, impact industries economically, and cause serious psycho-physical conditions. However, technological advances have leveraged the industry to a novel phase where digitalization and automation provide a new reality. Hence, this industrial transformation may contribute to assists human beings in the workplace with a focus on mental health. This article presents a systematic literature review to investigate studies regarding technologies employed in the care of worker's mental health and the industrial role in this scenario. Three general, three focused, and three descriptive questions highlight the academic progress of industrial concern on mental health, implemented systems and cases, and research challenges. As a result, the review discussed 31 studies, extracted from an initial corpus of 25269, ranging from January 2010 to November 2020. The studies approached stress as the most frequent mental issue in the industry and Support Vector Machine (SVM) as the most used machine learning algorithm, where biomarkers presented the primary data extractors to deal with this theme. Moreover, information fusion methods improved the accuracy of specific cases. However, a growing interest in mental health care has emerged only in recent years, and several challenges require efforts before applying systems in real industrial environments.",health
10.1016/j.suscom.2021.100622,preprocessed,Sustainable Computing: Informatics and Systems,scopus,2022-01-01,sciencedirect,internet of things for sustaining a smart and secure healthcare system,https://api.elsevier.com/content/abstract/scopus_id/85119703892,"The thyroid is a key endocrine gland in the human body that regulates several bodily processes, including protein synthesis, energy consumption, and the body’s reaction to other hormones. Segmentation and volume regeneration of the thyroid is particularly important for identifying thyroid-related diseases since the majority of these problems result in a change in the thyroid’s shape and scale over time. There is an urgent need for research on the disease’s origins and spread. The Internet of Things, cloud computing, and artificial intelligence all provide real-time processing for a variety of applications in the healthcare sector. In healthcare and biomedicine applications, machine learning algorithms are increasingly being utilized to make critical choices. Thyroid patients urgently need a robust and latency-sensitive Quality of Service framework. This paper aims to integrate fog computing and artificial intelligence with smart health to provide a dependable platform for thyroid infection early detection. To identify thyroid patients, a novel ensemble-based classifier is proposed. The thyroid dataset is obtained from the UCI library and the simulation is carried out utilizing Python programming. To increase the framework’s security, encryption and decryption methods are suggested. The suggested framework’s performance is assessed in terms of latency, network use, RAM utilization, and energy consumption. On the other side, the suggested classifier’s accuracy, precision, specificity, sensitivity and F1 score are all assessed. The result demonstrates that the suggested framework and classifier perform consistently better than conventional frameworks and classifiers.",health
10.1016/j.bspc.2021.103123,preprocessed,Biomedical Signal Processing and Control,scopus,2022-01-01,sciencedirect,real-time application based cnn architecture for automatic usct bone image segmentation,https://api.elsevier.com/content/abstract/scopus_id/85114454344,"Artificial Intelligence (AI) in medical image analysis has achieved excellent success in automatic diagnosis in the same way as clinician, especially in the ultrasound field. In this work, we develop a new segmentation application based on various Convolutional Neural Network (CNN) models for Ultrasonic Computed Tomographic (USCT) images. To evaluate the proposed segmentation system, we use different state-of-the-art models for better segmentation performances to train and test the suggested system. We ensure in this work a USCT data augmentation technique based on the Haar wavelet transform and the improved k-means algorithms. Thus, we offer a free dataset for USCT researchers. Moreover, the proposed CNN system is trained and tested using the networks of Adadelta and Adam optimizers. The whole system is implemented on a CPU and a GPU for complexity analysis. High segmentation accuracy has been achieved using the Adadelta optimizer, reaching 99.24%, 99.19%, 99.13% and 99.10% for VGG-Segnet, VGG-Unet, Fully CNN (FCN)-8 and FCN-32 models, respectively. To obtain better results, we use the Adam optimizer to train and test different architectures, and we obtain more competitive results attaining 99.55%, 99.31%, 99.35% and 99.45% for VGG-Segnet, VGG-Unet, FCN-8 and FCN-32, respectively. The achieved results outperform the state of the art in terms of accuracy and time speed up. Moreover, our proposed CNN segmentation confirms the low computational complexity of the system. In addition, our system proves to be a good candidate for medical real-time applications thanks to its implementation on the GPU.",health
10.1109/sii52469.2022.9708896,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,integration of a reconfigurable robotic workcell for assembly operations in automotive industry,https://ieeexplore.ieee.org/document/9708896/,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",industry
10.1109/ccnc49033.2022.9700522,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,a deep reinforcement learning-based resource management scheme for sdn-mec-supported xr applications,https://ieeexplore.ieee.org/document/9700522/,"The Multi-Access Edge Computing (MEC) paradigm provides a promising solution for efficient computing services at edge nodes, such as base stations (BS), access points (AP), etc. By offloading highly intensive computational tasks to MEC servers, critical benefits in terms of reducing energy consumption at mobile devices and lowering processing latency can be achieved to support high Quality of Service (QoS) to many applications. Among the services which would benefit from MEC deployments are eXtended Reality (XR) applications which are receiving increasing attention from both academia and industry. XR applications have high resource requirements, mostly in terms of network bandwidth, computation and storage. Often these resources are not available in classic network architectures and especially not when XR applications are run by mobile devices. This paper leverages the concepts of Software Defined Networking (SDN) and Network Function Virtualization (NFV) to propose an innovative resource management scheme considering heterogeneous QoS requirements at the MEC server level. The resource assignment is formulated by employing a Deep Reinforcement Learning (DRL) technique to support high quality of XR services. The simulation results show how our proposed solution outperforms other state-of-the-art resource management-based schemes.",industry
10.1109/jiot.2021.3079440,preprocessed,IEEE Internet of Things Journal,IEEE,2015-01-15 20:22:00,ieeexplore,deep-learning-enabled automatic optical inspection for module-level defects in lcd,https://ieeexplore.ieee.org/document/9429707/,"Liquid crystal display (LCD) defects detection on module level is increasingly important for flat-panel displays (FPD) industry to increase the production capacity via machine vision technology. However, it is an overwhelmingly challenging issue due to various difficulties. This article discloses a practical automatic optical inspection (AOI) system consisting of hardware structure and software algorithm to detect module-level defects. The AOI system is the core component to build a distributed integrated inspection system with the help of the Internet of Things (IoT). Starting from the analysis of the challenges encountered in module-level defects inspection, a delicate photograph scheme is proposed to reveal different kinds of defects. In order to robustly work on the module-level defects detection with complex situations, a novel framework based on YOLOV3 detection unit is proposed in this article, including the preprocessing module, detection module, defects definition module, and interferences elimination module. To the best of our knowledge, this is the first work that designs a practical AOI system for module-level defects detection. In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on the manufacturing lines. The evaluation of the detection performance of the AOI system in comparison with a manual scheme indicates that the proposed system is practical for module-level defects detection. Currently, the proposed system has been deployed in a real-world LCD manufacturing line from a major player in the world.",industry
10.1109/tcyb.2020.2964011,preprocessed,IEEE Transactions on Cybernetics,IEEE,2022-01-01 00:00:00,ieeexplore,hierarchical granular computing-based model and its reinforcement structural learning for construction of long-term prediction intervals,https://ieeexplore.ieee.org/document/8972350/,"As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.",industry
10.1109/access.2021.3138990,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"microgrid digital twins: concepts, applications, and future trends",https://ieeexplore.ieee.org/document/9663369/,"Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the <italic>digital twinning</italic> concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs’ operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed.",industry
10.1109/tii.2021.3086149,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,toward a web-based digital twin thermal power plant,https://ieeexplore.ieee.org/document/9446630/,"As a crucial part of cyber-physical systems, a digital twin can process data, visualize processes, and send commands to the control system, which can be used for the research on thermal power plants that are vital for providing energy for manufacturing and industry, and also daily consumptions. This article introduces the methodologies and techniques toward a web-based digital twin thermal power plant. To implement a web-based digital twin thermal power plant, the architecture, modeling, control algorithm, rule model, and physical-digital twin control are explored. The potential functionalities of the web-based digital twin including real-time monitoring, visualization and interactions, and provided services for physical thermal plants and universities are also presented. A case study has been provided to illustrate the web-based digital twin power plant. The research in this article can provide potential solutions for web-based digital twin research and education.",industry
10.1109/tii.2021.3093388,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-04-01 00:00:00,ieeexplore,an adaptive deep learning framework for fast recognition of integrated circuit markings,https://ieeexplore.ieee.org/document/9468418/,"Fast recognition of integrated circuit (IC) markings is an essential but challenging task in electronic device manufacturing lines. This article develops an adaptive deep learning framework to facilitate the fast marking recognition of IC chips. The proposed framework contains four deep learning components, namely, chip segmentation, orientation correction, character extraction, and character recognition. The four components utilize different convolutional neural network structures to guarantee excellent adaptivity to a wide range of IC types and mitigate the influence of the low-quality chip images. In particular, the character extraction model is comprised of two improved label generation strategies and a proposed border correction method, so as to accommodate tiny scale chips and compactly printed markings. Experiments from the chip image dataset of a real laptop manufacturing line reached a recognition Precision of 91.73% and the Recall of 92.93%. The results demonstrate the superiority of the proposed framework to the state-of-the-art models and the effectiveness of handling a great diversity of chips with different scales, shapes, text fonts, marking colors, and layouts.",industry
10.1109/tii.2021.3131355,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,dynamic network slicing orchestration for remote adaptation and configuration in industrial iot,https://ieeexplore.ieee.org/document/9629333/,"As an emerging and prospective paradigm, the industrial Internet of Things (IIoT) enable intelligent manufacturing through the interconnection and interaction of industrial production elements. The traditional approach that transmits data in a single physical network is undesirable because such a scheme cannot meet the network requirements of different industrial applications. To address this problem, in this article, we propose a network slicing orchestration system for remote adaptation and configuration in smart factories. We exploit software-defined networking and network functions virtualization to slice the physical network into multiple virtual networks. Different applications can use a dedicated network that meets its requirements with limited network resources with this scheme. To optimize network resource allocation and adapt to the dynamic network environments, we propose two heuristic algorithms with the assistance of artificial intelligence and the theoretical analysis of the network slicing system. We conduct numerical simulations to learn the performance of the proposed algorithms. Our experimental results show the effectiveness and efficiency of our proposed algorithms when multiple network services are concurrently running in the IIoT. Finally, we use a case study to verify the feasibility of the proposed network slicing orchestration system on a real smart manufacturing testbed.",industry
10.1109/tii.2021.3128972,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-05-01 00:00:00,ieeexplore,guest editorial: security and privacy of federated learning solutions for industrial iot applications,https://ieeexplore.ieee.org/document/9619939/,"The Industrial Internet of Things (IoT) typically consists of several thousands of heterogeneous devices, such as sensors, actuators, access points, machinery, end-users' handheld equipment, and supply chain. In such an industrial environment, a multitude of data is generated from massive IoT devices, e.g., sensors for monitoring the environment, reading temperature, and gauging pressure. Most of the data are from delay-sensitive and computation-intensive applications, such as real-time manufacturing and automated diagnostics, which require big data analytics with low latency. Machine learning (ML) has been witnessed as an efficient solution for big data analytics. The majority of such ML algorithms are centralized methods, meaning that they first gather data from different users for use as a training dataset, which is placed on the ML server, and then build a model to classify the new data samples by applying the ML algorithms to this training dataset. However, the access to these datasets in the centralized ML methods raises concerns about data privacy for users. Federated learning (FL) was designed to protect data privacy to address a part of these issues. In FL, each participant uses a global training model without uploading their private data to a third-party server. Compared with the conventional ML, FL can preserve data security, especially in terms of participant data during the learning process. In particular, FL can also help in updating server-side data for the global model, and the participant is not required to provide their data. However, in FL, individual computing units may show abnormal actions, such as faulty software, hardware invasions, unreliable communication channels, and malicious samples deliberately crafting the model. To mitigate these challenges, we require robust policies to control the learning phases in FL. Motivated by the abovementioned issues, this special section solicits original research and practical contributions that advance the security and privacy of the FL solutions for industrial IoT applications as follows.",industry
10.1109/access.2022.3140595,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,integrating artificial intelligence internet of things and 5g for next-generation smartgrid: a survey of trends challenges and prospect,https://ieeexplore.ieee.org/document/9672084/,"Smartgrid is a paradigm that was introduced into the conventional electricity network to enhance the way generation, transmission, and distribution networks interrelate. It involves the use of Information and Communication Technology (ICT) and other solution in fault and intrusion detection, mere monitoring of energy generation, transmission, and distribution. However, on one hand, the actual and earlier smartgrid, do not integrate more advanced features such as automatic decision making, security, scalability, self-healing and awareness, real-time monitoring, cross-layer compatibility, etc. On the other hand, the emergence of the digitalization of the communication infrastructure to support the economic sector which among them are energy generation and distribution grid with Artificial Intelligence (AI) and large-scale Machine to Machine (M2M) communication. With the future Massive Internet of Things (MIoT) as one of the pillars of 5G/6G network factory, it is the enabler to support the next generation smart grid by providing the needed platform that integrates, in addition to the communication infrastructure, the AI and IoT support, providing a multitenant system. This paper aim at presenting a comprehensive review of next smart grid research trends and technological background, discuss a futuristic next-generation smart grid driven by artificial intelligence (AI) and leverage by IoT and 5G. In addition, it discusses the challenges of next-generation smart-grids as it relate to the integration of AI, IoT and 5G for better smart grid architecture. Also, proffers possible solutions to some of the challenges and standards to support this novel trend. A corresponding future work will dwell on the implementation of the discussed integration of AI, IoT and 5G for next-generation smart grid, using Matlab, NS2/NS3, Open-daylight and Mininet as soft tools and compare with related literature.",industry
10.1109/tii.2021.3077865,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,a data stream cleaning system using edge intelligence for smart city industrial environments,https://ieeexplore.ieee.org/document/9424956/,"Cities are becoming smarter because of recent advances in artificial intelligence and the Internet of Things. However, heterogeneous data source in smart cities are continuously producing low-quality data, and ever-growing applications have greater real-time requirements. Therefore, this article proposes a data stream cleaning system (named DSCS) using edge intelligence to utilize the advantages of cloud servers and edge devices. The DSCS in edge nodes consists of a dynamic protocol interpreter, a structure parser, and a cleaning model activator. Meanwhile, a cloud server, which has pools of protocol and structured programs and cleaning models, supports the edge nodes to adapt massive heterogeneous data sources. To validate the proposed data cleaning system, we applied it to two scenarios: monitoring the injection molding machines, and base stations. The DSCS can have a stable processing time when the number of accessed edge devices is increased, as well as a good cleaning effect.",industry
10.1109/access.2022.3145236,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,colonization by algorithms in the fourth industrial revolution,https://ieeexplore.ieee.org/document/9690490/,"Data gathering and information processing have evolved to where it is almost unfathomable how much exists in digital form today. The generation thereof also no longer involves an explicit instruction from human to machine but can happen in real-time without human intervention. Artificial intelligence, machine learning, and cognitive computing are being utilized to mine data from a variety of sources. One such (profitable) source is human beings. Digital algorithms are designed to harness the power of technology to gather information. There has always been a sense of secrecy regarding some information (classified, top secret, confidential, etc.) but the Fourth Industrial Revolution has created the means to gather extremely large amounts of data, unknown to its sources. Anthropological value systems should become a fundamental foundation of digital algorithms. Such an approach could prevent software from exploiting its sources, especially minorities. Value systems together with ethics are guided by people’s culture. In ethically aligned algorithm design, value systems and digital technologies intersect and govern how algorithms are developed, the way data is engaged, and further the discipline of digital humanities.",industry
10.1109/tii.2021.3081417,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,early classification of industrial alarm floods based on semisupervised learning,https://ieeexplore.ieee.org/document/9435070/,"Early classification of ongoing alarm floods in industrial monitoring systems is crucial to provide a safe and efficient operation. It can provide online decision support for plant operators to take timely action, without waiting for the end of an alarm flood. In this article, a data-driven approach is proposed to address the early classification problem with unlabeled historical data. To prioritize earlier activated alarms and take advantage of the triggering time information of alarms, a vector representation called exponentially attenuated component (EAC) is used to represent alarm floods. This makes alarm sequences fit for different powerful machine learning algorithms, which can be easily implemented online with acceptable computational complexities. A method based on the time information of unlabeled historical alarm floods is formulated to determine the attenuation coefficient for EAC representation. With the Gaussian mixture model, an efficient semisupervised approach is proposed to provide an early classification of alarm floods using unlabeled historical data. It includes two phases: offline clustering and online classification, where the clustering step is automated in terms of choosing the optimal number of clusters by applying an efficient cluster validity index. The efficiency of the proposed method is validated by the Tennessee Eastman process benchmark and a real industrial dataset.",industry
10.1109/tii.2021.3130279,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,imitation learning based heavy-hitter scheduling scheme in software-defined industrial networks,https://ieeexplore.ieee.org/document/9626609/,"To realize flexible networking and on-demand topology reconstructing, software-defined industrial networks (SDINs) are increasingly embracing the flat structure. Similar to software defined networks (SDN), SDIN suffers from low traffic scheduling efficiency caused by large and imbalanced flows, known as the heavy hitters problem. Due to such heavy hitters, industrial networks may fail to satisfy application’s QoS requirements, which results in more severe damages. To improve flow scheduling efficiency under heavy hitters, this article introduces a novel imitation learning-based flow scheduling (ILFS) method. ILFS utilizes P4-based In-band Network Telemetry (INT) to collect fine-grained, real-time traffic data from SDIN’s data plane. In the control plane, it integrates the Generative Adversarial Imitation Learning (GAIL) model with a soft actor critic to preserve the experiences of flow, thereby better scheduling large flows. Our experiments thoroughly compare ILFS’s performance with several state-of-the-art traffic scheduling strategies. The results indicate that ILFS successfully controls the link bandwidth the utilization between 10<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and 80<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and significantly improves the average network throughput and link utilization rate.",industry
10.1109/tii.2021.3124848,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,qos and privacy-aware routing for 5g-enabled industrial internet of things: a federated reinforcement learning approach,https://ieeexplore.ieee.org/document/9601174/,"The development and maturity of the fifth-generation (5G) wireless communication technology provides the industrial Internet of Things (IIoT) with ultra-reliable and low-latency communications and massive machine-type communications, and forms a novel IIoT architecture, 5G-IIoT. However, massive data transfer between interconnecting industrial devices also brings new challenges for the 5G-IIoT routing process in terms of latency, load balancing, and data privacy, which affect the development of 5G-IIoT applications. Moreover, the existing research works on IIoT routing mostly focus on the latency and the reliability of the routing, disregarding the privacy security in the routing process. To solve these problems, in this article, we propose a quality of service (QoS) and data privacy-aware routing protocol, named QoSPR, for 5G-IIoT. Specifically, we improve the community detection algorithm info-map to divide the routing area into optimal subdomains, based on which the deep reinforcement learning algorithm is applied to build the gateway deployment model for latency reduction and load-balancing improvement. To eliminate areal differences, while considering the privacy preservation of the routing data, the federated reinforcement learning is applied to obtain the universal gateway deployment model. Then, based on the gateway deployment, the QoS and data privacy-aware routing is accomplished by establishing communications along the load-balancing routes of the minimum latencies. The validation experiment is conducted on real datasets. The experiment results show that as a data privacy-aware routing protocol, the QoSPR can significantly reduce both average latency and maximum latency, while maintaining excellent load balancing in 5G-IIoT.",industry
10.1109/tii.2021.3077005,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,verifiable data mining against malicious adversaries in industrial internet of things,https://ieeexplore.ieee.org/document/9422191/,"With the large-scaled data generated from various interconnected machines and networks, Industrial Internet of Things (IIoT) provides unprecedented opportunities for facilitating data mining for industrial applications. The current IIoT architecture tends to adopt cloud computing for further timely mining IIoT data, however, the openness of security-critical IIoT becomes challenging in terms of unbearable privacy issues. Most existing privacy-preserving data mining (PPDM) techniques are designed to resist honest-but-curious adversaries (i.e., cloud servers and data users). Due to the complexity and openness in IIoT, PPDM is significantly difficult with the presence of malicious adversaries in IIoT who may incur incorrect learned models and inference results. To solve the aforementioned issues, we propose a framework to extend existing PPDM to guard linear regression against malicious behaviors (hereafter referred to as GuardLR). To prevent dishonest computations of cloud servers and inconsistent inputs of data users, we first design a privacy-preserving verifiable learning scheme for linear regression, which guarantees the correctness of learning. In this article, to avoid malicious clouds from returning incorrect inference results, we design a privacy-preserving prediction scheme with lightweight verification. Our formal security analysis shows that GuardLR achieves privacy, completeness, and soundness. Empirical experiments using real-world datasets also demonstrate that GuardLR has high computational efficiency and accuracy.",industry
10.1109/tnse.2021.3075428,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,ai-assisted energy-efficient and intelligent routing for reconfigurable wireless networks,https://ieeexplore.ieee.org/document/9416866/,"Intelligent network management for reconfigurable wireless networks such as 5G and beyond applications is crucial for many industrial applications, and has been the subject of ongoing research. This paper proposes an Artificial Intelligence(AI)-Assisted energy-efficient and intelligent routing, based on both energy efficiency prioritization and AI theory, in order to meet the exacting demands particularly in a real-world scenario. Specifically, to achieve network intelligence and quality of service (QoS), we use the AI theory to enhance routing adaptivity for intelligent network management in reconfigurable wireless networks. The software-defined networking idea is used to achieve this goal from a network-level perspective. To facilitate self-awareness, self-study, self-decision making, and self-configuration, we construct a mathematical model to convert the energy-efficient and intelligent routing problem into a multi-constraint optimal problem. Then an AI-assisted intelligent routing algorithm is designed to dynamically and adaptively change link weighs, which allows us to achieve optimal energy efficiency. Findings from our simulation suggest the potential of our proposed approach.",industry
10.1109/tnse.2021.3055835,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,cloud versus edge deployment strategies of real-time face recognition inference,https://ieeexplore.ieee.org/document/9350171/,"Choosing the appropriate deployment strategy for any Deep Learning (DL) project in a production environment has always been the most challenging problem for industrial practitioners. There are several conflicting constraints and controversial approaches when it comes to deployment. Among these problems, the deployment on cloud versus the deployment on edge represents a common dilemma. In a nutshell, each approach provides benefits where the other would have limitations. This paper presents a real-world case study on deploying a face recognition application using MTCNN detector and FaceNet recognizer. We report the challenges faced to decide on the best deployment strategy. We propose three inference architectures for the deployment, including cloud-based, edge-based, and hybrid. Furthermore, we evaluate the performance of face recognition inference on different cloud-based and edge-based GPU platforms. We consider different models of Jetson boards for the edge (Nano, TX2, Xavier NX, Xavier AGX) and various GPUs for the cloud (GTX 1080, RTX 2080Ti, RTX 2070, and RTX 8000). We also investigate the effect of deep learning model optimization using TensorRT and TFLite compared to a standard Tensorflow GPU model, and the effect of input resolution. We provide a benchmarking study for all these devices in terms of frames per second, execution times, energy and memory usages. After conducting a total of 294 experiments, the results demonstrate that the TensorRT optimization provides the fastest execution on all cloud and edge devices, at the expense of significantly larger energy consumption (up to +40% and +35% for edge and cloud devices, respectively, compared to Tensorflow). Whereas TFLite is the most efficient framework in terms of memory and power consumption, while providing significantly less (-4% to -62%) processing acceleration than TensorRT. <italic>Practitioners Note:</italic> The study reported in this paper presents the real-challenges that we faced during our development and deployment of a face-recognition application both on the edge and on the cloud, and the solutions we have developed to solve these problems. The code, results, and interactive analytic dashboards of this paper will be put public upon publication.",industry
10.1109/tii.2021.3075464,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-04-01 00:00:00,ieeexplore,dnnoff: offloading dnn-based intelligent iot applications in mobile edge computing,https://ieeexplore.ieee.org/document/9416166/,"A deep neural network (DNN) has become increasingly popular in industrial Internet of Things scenarios. Due to high demands on computational capability, it is hard for DNN-based applications to directly run on intelligent end devices with limited resources. Computation offloading technology offers a feasible solution by offloading some computation-intensive tasks to the cloud or edges. Supporting such capability is not easy due to two aspects: <italic>Adaptability:</italic> offloading should dynamically occur among computation nodes. <italic>Effectiveness:</italic> it needs to be determined which parts are worth offloading. This article proposes a novel approach, called DNNOff. For a given DNN-based application, DNNOff first rewrites the source code to implement a special program structure supporting on-demand offloading and, at runtime, automatically determines the offloading scheme. We evaluated DNNOff on a real-world intelligent application, with three DNN models. Our results show that, compared with other approaches, DNNOff saves response time by 12.4–66.6% on average.",industry
10.1109/access.2022.3149050,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,design and implementation of traffic generation model and spectrum requirement calculator for private 5g network,https://ieeexplore.ieee.org/document/9703352/,"This paper proposes a neural 5G traffic generation model and a methodology for calculating the spectrum requirements of private 5G networks to provide various industrial communication services. To accurately calculate the spectral requirements, it is necessary to analyze the actual data volume and traffic type of industrial cases. However, because there is currently no suitable traffic model to test loads in private 5G networks, we have developed a generative adversarial network (GAN)-based traffic generator that can generate realistic traffic by learning actual traffic traces collected by mobile network operators. In addition, in the case of industrial applications, probability-based traffic models were used in parallel as there were not enough real data to be learned. The proposed 5G traffic generation model is combined with the proposed 5G spectrum calculation methodology, enabling more accurate spectrum requirements calculation through traffic simulation similar to a real-life environment. In this paper, the spectrum requirements are calculated differently according to two types of duplexing, namely frequency division duplexing (FDD) and time division duplexing (TDD). As a guide for companies aiming to provide advanced wireless connectivity for a wide variety of vertical industries using 5G networks, eight use cases defined in the 5G Alliance for Connected Industries and Automation (ACIA) white paper were simulated. The spectrum requirements were calculated under various simulation conditions considering varying traffic loads, deployment scenarios, and duplexing types. Various simulation results confirmed that a bandwidth of at least 22.0 MHz to a maximum of 397.8 MHz is required depending on the deployment scenario.",industry
10.1109/tmech.2021.3065522,preprocessed,IEEE/ASME Transactions on Mechatronics,IEEE,2022-02-01 00:00:00,ieeexplore,federated transfer learning for intelligent fault diagnostics using deep adversarial networks with data privacy,https://ieeexplore.ieee.org/document/9376674/,"Intelligent data-driven machinery fault diagnosis methods have been popularly developed in the past years. While fairly high diagnosis accuracies have been obtained, large amounts of labeled training data are mostly required, which are difficult to collect in practice. The promising collaborative model training solution with multiple users poses high demands on data privacy due to conflict of interests. Furthermore, in the real industries, the data from different users can be usually collected from different machine operating conditions. The domain shift phenomenon and data privacy concern make the joint model training scheme quite challenging. To address this issue, a federated transfer learning method for fault diagnosis is proposed in this article. Different models can be used by different users to enhance data privacy. A federal initialization stage is introduced to keep similar data structures in distributed feature extractions, and a federated communication stage is further implemented using deep adversarial learning. A prediction consistency scheme is also adopted to increase model robustness. Experiments on two real-world datasets suggest the proposed federated transfer learning method is promising for real industrial applications.",industry
10.1109/tie.2021.3057030,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-02-01 00:00:00,ieeexplore,kdnet-rul: a knowledge distillation framework to compress deep neural networks for machine remaining useful life prediction,https://ieeexplore.ieee.org/document/9351733/,"Machine remaining useful life (RUL) prediction is vital in improving the reliability of industrial systems and reducing maintenance cost. Recently, long short-term memory (LSTM) based algorithms have achieved state-of-the-art performance for RUL prediction due to their strong capability of modeling sequential sensory data. In many cases, the RUL prediction algorithms are required to be deployed on edge devices to support real-time decision making, reduce the data communication cost, and preserve the data privacy. However, the powerful LSTM-based methods which have high complexity cannot be deployed to edge devices with limited computational power and memory. To solve this problem, we propose a knowledge distillation framework, entitled KDnet-RUL, to compress a complex LSTM-based method for RUL prediction. Specifically, it includes a generative adversarial network based knowledge distillation (GAN-KD) for disparate architecture knowledge transfer, a learning-during-teaching based knowledge distillation (LDT-KD) for identical architecture knowledge transfer, and a sequential distillation upon LDT-KD for complicated datasets. We leverage simple and complicated datasets to verify the effectiveness of the proposed KDnet-RUL. The results demonstrate that the proposed method significantly outperforms state-of-the-art KD methods. The compressed model with 12.8 times less weights and 46.2 times less total float point operations even achieves a comparable performance with the complex LSTM model for RUL prediction.",industry
10.1109/tpds.2021.3104255,preprocessed,IEEE Transactions on Parallel and Distributed Systems,IEEE,2022-06-01 00:00:00,ieeexplore,taskflow: a lightweight parallel and heterogeneous task graph computing system,https://ieeexplore.ieee.org/document/9511796/,"Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.",industry
10.26599/tst.2020.9010055,preprocessed,Tsinghua Science and Technology,TUP,2022-04-01 00:00:00,ieeexplore,underground pipeline surveillance with an algorithm based on statistical time-frequency acoustic features,https://ieeexplore.ieee.org/document/9552663/,"Underground pipeline networks suffer from severe damage by earth-moving devices due to rapid urbanization. Thus, designing a round-the-clock intelligent surveillance system has become crucial and urgent. In this study, we develop an acoustic signal-based excavation device recognition system for underground pipeline protection. The front-end hardware system is equipped with an acoustic sensor array, an Analog-to-Digital Converter (ADC) module (ADS1274), and an industrial processor Advanced RISC Machine (ARM) cortex-A8 for signal collection and algorithm implementation. Then, a novel Statistical Time-Frequency acoustic Feature (STFF) is proposed, and a fast Extreme Learning Machine (ELM) is adopted as the classifier. Experiments on real recorded data show that the proposed STFF achieves better discriminative capability than the conventional acoustic cepstrum features. In addition, the surveillance platform is applicable for encountering big data owing to the fast learning speed of ELM.",industry
10.1007/978-3-030-42462-6_123,preprocessed,The Palgrave Handbook of Climate Resilient Societies,Springer,2021-01-01 00:00:00,springer,water 4.0: enhancing climate resilience,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-42462-6_123,"For this chapter, water 4.0 is defined as the industry 4.0 concept applied to the water sector. As industry 4.0 reflects the fourth industrial revolution , water 4.0 reflects the fourth water revolution . Based on the literature review and case studies, this chapter examines a proposition that water 4.0 will increase not only the sector’s economic effectiveness but also sustainability including climate resilience. Relevant technologies include digital twins , visualization, wireless monitoring sensors, industrial internet of things (IoT/IIoT), cloud computing, and predictive or prescriptive analytics but also blockchain , drones, and cybersecurity. For water 4.0 becoming a reality, water utility companies need not only collect more data but also to have proper analytical tools in place to convert data into information supporting optimal decisions. The current tools should preferably be replaced by machine learning algorithms that are nonlinear, nonstationary, and dynamic and thus aligned closely with the real world. It has been suggested in this chapter that such disruptive technologies be introduced through an ISO 55001-based asset management system (AMS). ISO 19650 series supplements ISO 55001 and contains additional requirements for the AMS development by focusing particularly on asset information. For this purpose, the series provides assistance with big data and digital twins . Two approaches are applicable to the implementation of water 4.0 through AMS: adaptability and more traditional continuous improvement with the former considered in this chapter as preferred but requires a sufficient level of asset management maturity. Therefore, it might be prudent that every organization sets their own water 4.0 -related standards and objectives in their own AMS and considers the preferred level of adaptability . Adaptability is arguably required for water 4.0 with adaptation bringing the greatest value .",industry
http://arxiv.org/abs/2202.10075v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"icsml: industrial control systems machine learning inference framework
  natively executing on iec 61131-3 languages",http://arxiv.org/abs/2202.10075v1,"Industrial Control Systems (ICS) have played a catalytic role in enabling the
4th Industrial Revolution. ICS devices like Programmable Logic Controllers
(PLCs), automate, monitor and control critical processes in industrial, energy
and commercial environments. The convergence of traditional Operational
Technology (OT) with Information Technology (IT) has opened a new and unique
threat landscape. This has inspired defense research that focuses heavily on
Machine Learning (ML) based anomaly detection methods that run on external IT
hardware which means an increase in costs and the further expansion of the
threat landscape. To remove this requirement, we introduce the ICS Machine
Learning inference framework (ICSML) which enables the execution of ML models
natively on the PLC. ICSML is implemented in IEC 61131-3 code and works around
the limitations imposed by the domain-specific languages, providing a complete
set of components for the creation of fully fledged ML models in a way similar
to established ML frameworks. We then demonstrate a complete end-to-end
methodology for creating ICS ML models using an external framework for training
and ICSML for the PLC implementation. To evaluate our contributions we run a
series of benchmarks studying memory and performance and compare our solution
to the TFLite inference framework. Finally, to demonstrate the abilities of
ICSML and to verify its non-intrusive nature, we develop and evaluate a case
study of a real defense for process aware attacks against a Multi Stage Flash
(MSF) desalination plant.",industry
http://arxiv.org/abs/2202.09549v1,preprocessed,arxiv,arxiv,2022-02-19 00:00:00,arxiv,"learning to detect slip with barometric tactile sensors and a temporal
  convolutional neural network",http://arxiv.org/abs/2202.09549v1,"The ability to perceive object slip via tactile feedback enables humans to
accomplish complex manipulation tasks including maintaining a stable grasp.
Despite the utility of tactile information for many applications, tactile
sensors have yet to be widely deployed in industrial robotics settings; part of
the challenge lies in identifying slip and other events from the tactile data
stream. In this paper, we present a learning-based method to detect slip using
barometric tactile sensors. These sensors have many desirable properties
including high durability and reliability, and are built from inexpensive,
off-the-shelf components. We train a temporal convolution neural network to
detect slip, achieving high detection accuracies while displaying robustness to
the speed and direction of the slip motion. Further, we test our detector on
two manipulation tasks involving a variety of common objects and demonstrate
successful generalization to real-world scenarios not seen during training. We
argue that barometric tactile sensing technology, combined with data-driven
learning, is suitable for many manipulation tasks such as slip compensation.",industry
http://arxiv.org/abs/2202.09113v1,preprocessed,arxiv,arxiv,2022-02-18 00:00:00,arxiv,how to manage tiny machine learning at scale: an industrial perspective,http://arxiv.org/abs/2202.09113v1,"Tiny machine learning (TinyML) has gained widespread popularity where machine
learning (ML) is democratized on ubiquitous microcontrollers, processing sensor
data everywhere in real-time. To manage TinyML in the industry, where mass
deployment happens, we consider the hardware and software constraints, ranging
from available onboard sensors and memory size to ML-model architectures and
runtime platforms. However, Internet of Things (IoT) devices are typically
tailored to specific tasks and are subject to heterogeneity and limited
resources. Moreover, TinyML models have been developed with different
structures and are often distributed without a clear understanding of their
working principles, leading to a fragmented ecosystem. Considering these
challenges, we propose a framework using Semantic Web technologies to enable
the joint management of TinyML models and IoT devices at scale, from modeling
information to discovering possible combinations and benchmarking, and
eventually facilitate TinyML component exchange and reuse. We present an
ontology (semantic schema) for neural network models aligned with the World
Wide Web Consortium (W3C) Thing Description, which semantically describes IoT
devices. Furthermore, a Knowledge Graph of 23 publicly available ML models and
six IoT devices were used to demonstrate our concept in three case studies, and
we shared the code and examples to enhance reproducibility:
https://github.com/Haoyu-R/How-to-Manage-TinyML-at-Scale",industry
http://arxiv.org/abs/2202.08897v1,preprocessed,arxiv,arxiv,2022-02-17 00:00:00,arxiv,"implementing spiking neural networks on neuromorphic architectures: a
  review",http://arxiv.org/abs/2202.08897v1,"Recently, both industry and academia have proposed several different
neuromorphic systems to execute machine learning applications that are designed
using Spiking Neural Networks (SNNs). With the growing complexity on design and
technology fronts, programming such systems to admit and execute a machine
learning application is becoming increasingly challenging. Additionally,
neuromorphic systems are required to guarantee real-time performance, consume
lower energy, and provide tolerance to logic and memory failures. Consequently,
there is a clear need for system software frameworks that can implement machine
learning applications on current and emerging neuromorphic systems, and
simultaneously address performance, energy, and reliability. Here, we provide a
comprehensive overview of such frameworks proposed for both, platform-based
design and hardware-software co-design. We highlight challenges and
opportunities that the future holds in the area of system software technology
for neuromorphic computing.",industry
http://arxiv.org/abs/2202.06149v1,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,"automatic issue classifier: a transfer learning framework for
  classifying issue reports",http://arxiv.org/abs/2202.06149v1,"Issue tracking systems are used in the software industry for the facilitation
of maintenance activities that keep the software robust and up to date with
ever-changing industry requirements. Usually, users report issues that can be
categorized into different labels such as bug reports, enhancement requests,
and questions related to the software. Most of the issue tracking systems make
the labelling of these issue reports optional for the issue submitter, which
leads to a large number of unlabeled issue reports. In this paper, we present a
state-of-the-art method to classify the issue reports into their respective
categories i.e. bug, enhancement, and question. This is a challenging task
because of the common use of informal language in the issue reports. Existing
studies use traditional natural language processing approaches adopting
key-word based features, which fail to incorporate the contextual relationship
between words and therefore result in a high rate of false positives and false
negatives. Moreover, previous works utilize a uni-label approach to classify
the issue reports however, in reality, an issue-submitter can tag one issue
report with more than one label at a time. This paper presents our approach to
classify the issue reports in a multi-label setting. We use an off-the-shelf
neural network called RoBERTa and fine-tune it to classify the issue reports.
We validate our approach on issue reports belonging to numerous industrial
projects from GitHub. We were able to achieve promising F-1 scores of 81%, 74%,
and 80% for bug reports, enhancements, and questions, respectively. We also
develop an industry tool called Automatic Issue Classifier (AIC), which
automatically assigns labels to newly reported issues on GitHub repositories
with high accuracy.",industry
http://arxiv.org/abs/2202.04834v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"geometric digital twinning of industrial facilities: retrieval of
  industrial shapes",http://arxiv.org/abs/2202.04834v1,"This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",industry
http://arxiv.org/abs/2202.03028v1,preprocessed,arxiv,arxiv,2022-02-07 00:00:00,arxiv,quark: a framework for quantum computing application benchmarking,http://arxiv.org/abs/2202.03028v1,"Quantum computing (QC) is anticipated to provide a speedup over classical HPC
approaches for specific problems in optimization, simulation, and machine
learning. With the advances in quantum computing toward practical applications,
the need to analyze and compare different quantum solutions increases. While
different low-level benchmarks for QC exist, these benchmarks do not provide
sufficient insights into real-world application-level performance. We propose
an application-centric benchmark method and the QUantum computing Application
benchmaRK (QUARK) framework to foster the investigation and creation of
application benchmarks for QC. This paper establishes three significant
contributions: (1) it makes a case for application-level benchmarks and
provides an in-depth ""pen and paper"" benchmark formulation of two reference
problems: robot path and vehicle option optimization from the industrial
domain; (2) it proposes the open-source QUARK framework for designing,
implementing, executing, and analyzing benchmarks; (3) it provides multiple
reference implementations for these two reference problems based on different
known, and where needed, extended, classical and quantum algorithmic approaches
and analyzes their performance on different types of infrastructures.",industry
http://arxiv.org/abs/2202.02813v2,preprocessed,arxiv,arxiv,2022-02-06 00:00:00,arxiv,a coding framework and benchmark towards compressed video understanding,http://arxiv.org/abs/2202.02813v2,"Most video understanding methods are learned on high-quality videos. However,
in real-world scenarios, the videos are first compressed before the
transportation and then decompressed for understanding. The decompressed videos
may have lost the critical information to the downstream tasks. To address this
issue, we propose the first coding framework for compressed video
understanding, where another learnable analytic bitstream is simultaneously
transported with the original video bitstream. With the dedicatedly designed
self-supervised optimization target and dynamic network architectures, this new
stream largely boosts the downstream tasks yet with a small bit cost. By only
one-time training, our framework can be deployed for multiple downstream tasks.
Our framework also enjoys the best of both two worlds, (1) high efficiency of
industrial video codec and (2) flexible coding capability of neural networks
(NNs). Finally, we build a rigorous benchmark for compressed video
understanding on three popular tasks over seven large-scale datasets and four
different compression levels. The proposed Understanding oriented Video Coding
framework UVC consistently demonstrates significantly stronger performances
than the baseline industrial codec.",industry
http://arxiv.org/abs/2201.12170v3,preprocessed,arxiv,arxiv,2022-01-28 00:00:00,arxiv,"unsupervised single-shot depth estimation using perceptual
  reconstruction",http://arxiv.org/abs/2201.12170v3,"Real-time estimation of actual object depth is a module that is essential to
performing various autonomous system tasks such as 3D reconstruction, scene
understanding and condition assessment of machinery parts. During the last
decade of machine learning, extensive deployment of deep learning methods to
computer vision tasks has yielded approaches that succeed in achieving
realistic depth synthesis out of a simple RGB modality. While most of these
models are based on paired depth data or availability of video sequences and
stereo images, methods for single-view depth synthesis in a fully unsupervised
setting have hardly been explored. This study presents the most recent advances
in the field of generative neural networks, leveraging them to perform fully
unsupervised single-shot depth synthesis. Two generators for RGB-to-depth and
depth-to-RGB transfer are implemented and simultaneously optimized using the
Wasserstein-1 distance and a novel perceptual reconstruction term. To ensure
that the proposed method is plausible, we comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database and the SURREAL dataset that records body depth. The success observed
in this study suggests the great potential for unsupervised single-shot depth
estimation in real-world applications.",industry
http://arxiv.org/abs/2201.06735v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,ai augmented digital metal component,http://arxiv.org/abs/2201.06735v1,"The aim of this work is to propose a new paradigm that imparts intelligence
to metal parts with the fusion of metal additive manufacturing and artificial
intelligence (AI). Our digital metal part classifies the status with real time
data processing with convolutional neural network (CNN). The training data for
the CNN is collected from a strain gauge embedded in metal parts by laser
powder bed fusion process. We implement this approach using additive
manufacturing, demonstrate a self-cognitive metal part recognizing partial
screw loosening, malfunctioning, and external impacting object. The results
indicate that metal part can recognize subtle change of multiple fixation state
under repetitive compression with 89.1% accuracy with test sets. The proposed
strategy showed promising potential in contributing to the hyper-connectivity
for next generation of digital metal based mechanical systems",industry
http://arxiv.org/abs/2201.06616v2,preprocessed,arxiv,arxiv,2022-01-17 00:00:00,arxiv,improving the quality control of seismic data through active learning,http://arxiv.org/abs/2201.06616v2,"In image denoising problems, the increasing density of available images makes
an exhaustive visual inspection impossible and therefore automated methods
based on machine-learning must be deployed for this purpose. This is
particulary the case in seismic signal processing. Engineers/geophysicists have
to deal with millions of seismic time series. Finding the sub-surface
properties useful for the oil industry may take up to a year and is very costly
in terms of computing/human resources. In particular, the data must go through
different steps of noise attenuation. Each denoise step is then ideally
followed by a quality control (QC) stage performed by means of human expertise.
To learn a quality control classifier in a supervised manner, labeled training
data must be available, but collecting the labels from human experts is
extremely time-consuming. We therefore propose a novel active learning
methodology to sequentially select the most relevant data, which are then given
back to a human expert for labeling. Beyond the application in geophysics, the
technique we promote in this paper, based on estimates of the local error and
its uncertainty, is generic. Its performance is supported by strong empirical
evidence, as illustrated by the numerical experiments presented in this
article, where it is compared to alternative active learning strategies both on
synthetic and real seismic datasets.",industry
http://arxiv.org/abs/2201.06599v1,preprocessed,arxiv,arxiv,2022-01-17 00:00:00,arxiv,"who supervises the supervisor? model monitoring in production using deep
  feature embeddings with applications to workpiece inspection",http://arxiv.org/abs/2201.06599v1,"The automation of condition monitoring and workpiece inspection plays an
essential role in maintaining high quality as well as high throughput of the
manufacturing process. To this end, the recent rise of developments in machine
learning has lead to vast improvements in the area of autonomous process
supervision. However, the more complex and powerful these models become, the
less transparent and explainable they generally are as well. One of the main
challenges is the monitoring of live deployments of these machine learning
systems and raising alerts when encountering events that might impact model
performance. In particular, supervised classifiers are typically build under
the assumption of stationarity in the underlying data distribution. For
example, a visual inspection system trained on a set of material surface
defects generally does not adapt or even recognize gradual changes in the data
distribution - an issue known as ""data drift"" - such as the emergence of new
types of surface defects. This, in turn, may lead to detrimental
mispredictions, e.g. samples from new defect classes being classified as
non-defective. To this end, it is desirable to provide real-time tracking of a
classifier's performance to inform about the putative onset of additional error
classes and the necessity for manual intervention with respect to classifier
re-training. Here, we propose an unsupervised framework that acts on top of a
supervised classification system, thereby harnessing its internal deep feature
representations as a proxy to track changes in the data distribution during
deployment and, hence, to anticipate classifier performance degradation.",industry
http://arxiv.org/abs/2201.04263v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,the human factor in ai safety,http://arxiv.org/abs/2201.04263v1,"AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.",industry
http://arxiv.org/abs/2201.02028v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,"a light in the dark: deep learning practices for industrial computer
  vision",http://arxiv.org/abs/2201.02028v1,"In recent years, large pre-trained deep neural networks (DNNs) have
revolutionized the field of computer vision (CV). Although these DNNs have been
shown to be very well suited for general image recognition tasks, application
in industry is often precluded for three reasons: 1) large pre-trained DNNs are
built on hundreds of millions of parameters, making deployment on many devices
impossible, 2) the underlying dataset for pre-training consists of general
objects, while industrial cases often consist of very specific objects, such as
structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal
issues for companies. As a remedy, we study neural networks for CV that we
train from scratch. For this purpose, we use a real-world case from a solar
wafer manufacturer. We find that our neural networks achieve similar
performances as pre-trained DNNs, even though they consist of far fewer
parameters and do not rely on third-party datasets.",industry
10.1016/j.compag.2022.106688,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,implementation of a decision support system for prediction of the total soluble solids of industrial tomato using machine learning models,https://api.elsevier.com/content/abstract/scopus_id/85122635918,"Tomato is the second most important vegetable in the world, both in terms of production and consumption. Especially for the cultivation of industrial tomato, harvest is conducted when the total soluble solids, a major quality characteristic, are as high as possible. Advancements in technology have made Decision Support Systems simpler and more applicable in an everyday basis. Data Analysis, combined with Machine Learning algorithms are considered the future of sustainable agriculture, allowing farmers to be advised about the best possible decisions for their cultivation. Farmers need to adopt this kind of technology in order to be able to know when the quality of tomatoes is at its peak, in order to gather their product from the field. The implementation of a Decision Support System to predict the total soluble solids was conducted,based on data from previous years, including quality data (pH, Bostwick, L, a/b, Mean Weight, °Brix), the type of hybrid used, weather data and soil data from the fields. Data derived from fields in 6 different regions in the northwestern Peloponnese, Greece over 6 cultivation periods, created a dataset of 33 different inputs. Thirteen different algorithms were put into evaluation in order to find the best one in terms of speed and efficiency. In this research, we developed a Decision Support System using the K-nearest algorithm, which proved to be the best for our dataset. The predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. This DSS which is using real time weather data as an input is expected to be a valuable tool for the farmers.",industry
10.1016/j.autcon.2021.104088,preprocessed,Automation in Construction,scopus,2022-02-01,sciencedirect,vision-based high-precision intelligent monitoring for shield tail clearance,https://api.elsevier.com/content/abstract/scopus_id/85120874971,"Real-time shield tail clearance measurement and monitoring is a key task during shield tunneling construction. The shield tail clearance measurement and monitoring technology development is still in its infancy, the current methods are mainly designed manually based on intuition. In order to fill the gap between the requirement of shield tail clearance measurement and monitoring and the limitations of the current methods, this paper systematically studies the existing mechanisms related to shield tail clearance measurement and monitoring, and develops a high-precision intelligent monitoring system for shield tail clearance. The proposed monitoring system includes four components: 1) two types of shield tail clearance calculation models, 2) the integrated hardware of the monitoring system which is composed of a data acquisition unit, a signal transmission unit and a control unit, 3) the region of interest (ROI) extraction method based on deep neural network, and the image processing algorithms for image enhancement and feature extraction, 4) the custom-developed software built on mature integrated development environment (IDE). After the calculation model of shield tail clearance is established, the system uses monitoring devices equipped with industrial cameras to obtain the on-site image, and then applies image processing technologies along with deep learning approach to extract the key features, which are brought into the model to calculate the values of shield tail clearance, finally displays these values and simulates the current tunneling attitude of the shield machine in real time. The experimental results show that the system proposed in this paper achieves the goal of high precision measuring and real-time monitoring of the shield tail clearance.",industry
10.1016/j.jisa.2021.103046,preprocessed,Journal of Information Security and Applications,scopus,2022-02-01,sciencedirect,aicrit: a unified framework for real-time anomaly detection in water treatment plants,https://api.elsevier.com/content/abstract/scopus_id/85119422439,"Industrial Control Systems (ICS) in public infrastructure, such as water treatment and distribution plants, have become a target of sophisticated cyber-attacks. Given the ever-present insider and other threats in such systems, there is a need to deploy mechanisms for defense and incidence response beyond the traditional. In this work we present AICrit that operates over the physical constraints and domain norms for accurate and timely detection of process anomalies. AICrit learns system-wide normal behavior using design knowledge and machine learning algorithms to recognize abnormal or irregular behavioral patterns resulting due to process anomalies. AICrit was implemented and evaluated in SWaT by launching several real-time stealthy and coordinated attacks. Experimental results attest to the effectiveness of AICrit in the timely detection of process anomalies with a low occurrence of false alarms. The underlying methodology used in the design of AICrit is generic and applicable to other ICS in various domains such as power, energy, and transportation.",industry
10.1016/j.apenergy.2021.118127,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,data-driven control of room temperature and bidirectional ev charging using deep reinforcement learning: simulations and experiments,https://api.elsevier.com/content/abstract/scopus_id/85118721393,"The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.",industry
10.1016/j.ssci.2021.105529,preprocessed,Safety Science,scopus,2022-02-01,sciencedirect,a novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry
10.1016/j.eswa.2021.116045,preprocessed,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,posimnet-r: an immunologic resilient approach to position routers in industrial wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/85117584055,"Industry 4.0 has increased the interest in employing Industrial Wireless Sensor Network (IWSN) technologies in industrial automation. The advantages range from ease of installation and maintenance to reduced deployment time and infrastructure costs. However, industrial automation has critical requirements regarding network infrastructure, such as reliability and failure tolerance. Therefore, it is imperative to have an adequate placement of sensor and router nodes, to obtain a network with multiple paths, allowing the data to reach management systems within a reasonable time, even in the event of failures. The placement of router nodes has to consider latency, network lifespan, connectivity, and failure tolerance aspects in a possibly hostile environment, with classified areas and obstacles such as silos, tanks and buildings. We present a new approach, called POSIMNET-R, to place IWSN routing nodes in an industrial configuration, which circumvents forbidden areas and obstacles, based on Artificial Immunological Networks. The resulting network offers low failure rates and path redundancy criteria. The results have shown that POSIMNET-R was capable of providing a reliable network with multiple paths and resilience of the used routers equal to 81.50% in the basic case study and 73.66% in the real case scenario.",industry
10.1016/j.comcom.2021.10.036,preprocessed,Computer Communications,scopus,2022-01-15,sciencedirect,lstm-mfcn: a time series classifier based on multi-scale spatial–temporal features,https://api.elsevier.com/content/abstract/scopus_id/85119299619,"Time series classification (TSC) task attracts huge interests, since they correspond to the real-world problems in a wide variety of fields, such as industry monitoring. Deep learning methods, especially CNN and FCN, shows competitive performance in TSC task by their virtue of good adaption for raw time series and self-adapting extraction of features. Then various variants of CNN are proposed so as to make further breakthrough by the better perception to characteristics of data. Among them, LSTM-FCN and GRU-FCN who learn spatial and temporal features simultaneously are the most remarkable ones, achieving state of the art results. Therefore, inspired by their success and in consideration of the discriminative features implied in time series are diverse in size, a multimodal network LSTM-MFCN composed of multi-scale FCN (MFCN) and LSTM are proposed in this work. The gate-based network LSTM naturally fits to various terms time dependencies, and FCN with multi-scale sets of filters are capable to perceive spatial features of different range from time series curves. Besides, dilation convolution is deployed to build multi-scale receptive fields in larger level without increasing the parameters to be trained. The full perception of large multi-scale spatial–temporal features lead LSTM-MFCN to possess comprehensive and thorough grasp to time series, thus achieve even better accuracies. Finally, two representative architectures are presented specifically and their experiments on UCR datasets reveals the effectiveness and superiority of proposed LSTM-MFCN.",industry
10.1016/j.energy.2021.122359,preprocessed,Energy,scopus,2022-01-15,sciencedirect,fuzzy inference system application for oil-water flow patterns identification,https://api.elsevier.com/content/abstract/scopus_id/85117714992,"Prediction of oil-water two-phase flow pattern provides an effective solution for reducing oil production costs. In this research, the fuzzy inference system (FIS) is utilized to predict fluid flow patterns and establish a new adaptable prediction model. This paper takes No. 10 industrial white oil and tap water as the research objects to simulate fluids, and analyzes the changes of the pipeline angle, the total flow of oil-water two-phase flow and the convective pattern of water cut. A data set containing 60 samples was used to create the model, and the Mamdani fuzzy model was established using MATLAB software. The results show that compared with the BP neural network algorithm, the model set forth in the present paper has higher accuracy and reliability, and can achieve real-time monitoring and effectively reduce errors, especially in the case of decision-making. In addition, the fuzzy model is demonstrated that in the entire production logging process of non-vertical wells, the use of a fuzzy inference system to predict fluid flow patterns can greatly save production costs while ensuring the safe operation of production equipment.",industry
10.1016/j.aca.2021.339411,preprocessed,Analytica Chimica Acta,scopus,2022-01-01,sciencedirect,a video processing and machine vision-based automatic analyzer to determine sequentially total suspended and settleable solids in wastewater,https://api.elsevier.com/content/abstract/scopus_id/85123884378,"The monitoring of total suspended (TSS) and settleable (SetS) solids in wastewater is essential to maintain the quality parameters for aquatic biota because they can transport pollutants and block light penetration. Determining them by their respective reference methods, however, is laborious, expensive, and time consuming. To overcome this, we developed a new analytical instrument called Solids in Wastewater's Machine Vision-based Automatic Analyzer (SWAMVA), which is equiped with an automatic sampler and a software for real-time digital movie capture to quantify sequentially the TSS and SetS contents in wastewater samples. The machine vision algorithm (MVA) coupled with the Red color plane (derived from color histograms in the Red-Green-Blue (RGB) system) showed the best prediction results with R2 of 0.988 and 0.964, and relative error of prediction (REP) of 6.133 and 9.115% for TSS and SetS, respectively. The constructed models were validated by Analysis of Variance (ANOVA), and the accuracy and precision of the predictions by the t- and F-tests, respectively, at a 0.05 significance level. The elliptical joint confidence region (EJCR) test confirmed the accuracy, while the coefficient of variation (CV) of 6.529 and 10.908% confirmed the good precisions, respectively. Compared with the reference method (Standard Methods For the Examination of Water and Wastewater), the proposed method reduced the analysis volume from 1.5 L to just 15 mL and the analysis time from 12 h to 24 s per sample. Therefore, SWAMVA can be considered an important alternative to the determination of TSS and SetS in wastewater as an automatic, fast, and low-cost analytical tool, following the principles of Green Chemistry and exploiting Industry 4.0 features such as intelligent processing, miniaturization, and machine vision.",industry
10.1016/j.jmsy.2022.01.010,preprocessed,Journal of Manufacturing Systems,scopus,2022-01-01,sciencedirect,"towards edge computing in intelligent manufacturing: past, present and future",https://api.elsevier.com/content/abstract/scopus_id/85123859503,"Industry 4.0 (I4.0) is the fourth industrial revolution and a synonym for intelligent manufacturing. It drives the convergence of several cutting-edge technologies to provoke autonomous, fully integrated, collaborated, highly automated, and customized industries. Edge Computing (EC), a highly distributed framework, emerged a couple of years ago and embraced the industry to leverage the benefit of low latency and near real-time performance. It brings computation and storage in the close proximity of end devices and reduces the cloud overhead. In addition to improved operational efficiency, storage, and latency, EC further reduces the cost, improves productivity with higher quality maintenance and customer satisfaction. At the digital-to-digital stage of the Physical-Digital-Physical (PDP) loop, adapting EC can furnish tremendous benefits and further accelerate the next stages of the loop. This survey identifies the past and present works oriented towards Intelligent Manufacturing integrated with the EC platform and categorizes the research based on architecture, intelligence platform, edge objectives, and application. Herein, the authors have incorporated; (1) The progress in I4.0 following the PDP loop; (2) The discussion on EC in I4.0 and their Research Trend; (3) Methods to bring intelligence to the edge. To the best of our knowledge, it is the first review article that focuses on the applications and objectives of EC in Intelligent Manufacturing. It also outlines the optimum solutions to bring intelligence to the edge by overcoming the resource and complexity-bound with accuracy and latency constraints for the decision-making processes. Future directions include the less explored research areas, challenges in edge deployment in industries, and the integration of trending technologies such as Blockchain, Software Defined Networking, and 5 G with EC to excite the EC researchers. A few collaborative edge scenarios are discussed for the promotion and application of EC in I4.0. Nevertheless, efficient edge deployments face many challenges since studies are still limited to conceptual levels or design steps, and future orientation to application strategies for Smart Manufacturing is required.",industry
10.1016/j.cie.2021.107824,preprocessed,Computers and Industrial Engineering,scopus,2022-01-01,sciencedirect,new perspectives and results for smart operators in industry 4.0: a human-centered approach,https://api.elsevier.com/content/abstract/scopus_id/85122422552,"Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators.
                  To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept.
                  Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework.",industry
10.1016/j.softx.2021.100956,preprocessed,SoftwareX,scopus,2022-01-01,sciencedirect,tx2_fcnn_node: an open-source ros compatible tool for monocular depth reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85121968187,"We present tx2_fcnn_node – a Robot Operating System (ROS) compatible tool that is aimed at seamless integration of various monocular depth reconstruction neural networks to the robotic software based on ROS (which is a de-facto standard in the area of robotics). Our tool simplifies the process of deploying, evaluating, and comparing depth reconstruction neural networks both on real robots and in simulation. We complement our software with a set of the precompiled neural networks which can be used off the shelf, with some of them being able to demonstrate near real-time performance when running onboard compact embedded platforms, e.g. Nvidia Jetson TX2, that are often used nowadays both in academia and industry.",industry
10.1016/j.suscom.2021.100650,preprocessed,Sustainable Computing: Informatics and Systems,scopus,2022-01-01,sciencedirect,a conceptual framework for the implementation of industry 4.0 in legal informatics,https://api.elsevier.com/content/abstract/scopus_id/85121918847,"The growing number of applications of Industry 4.0 in the field of legal informatics offers huge opportunities for data scientists and academic researchers. The term Industry 4.0 is defined as “the fourth industrial revolution that connects embedded systems to Cyber-Physical-Systems”. It emphasizes the end-to-end digitalization of all physical resources and integrating digital environments with the value chain organizations. Industry 4.0 comprises a variety of technologies such as Cyber-Physical-Systems, Big Data, Internet of Things, Artificial Intelligence, Cloud Computing and Cybersecurity. It has been found that the implementation of these technologies may be useful in achieving the objective of legal informatics. It can help lawmakers to align jurisprudence by providing modern computational technologies to improve and advance the traditional legal justice system. Further, the integration of legal informatics with Industry 4.0 will be strengthening the legal justice system by providing decision-making support, data transparency, real-time monitoring, cost-effective solution, and triple-bottom-line performance in the future. Therefore, the article aims to determine the implementation patterns of Industry 4.0 technologies in legislative institutions and administrations. The study proposes a conceptual framework that integrates Industry 4.0 with legal informatics. The findings show that implementing Industry 4.0 technologies such as Artificial Intelligence, Big Data, and Cloud Computing plays a vital role for legal firms that are currently in the nascent stage of development.",industry
10.1016/j.compag.2021.106635,preprocessed,Computers and Electronics in Agriculture,scopus,2022-01-01,sciencedirect,intelligent iot-multiagent precision irrigation approach for improving water use efficiency in irrigation systems at farm and district scales,https://api.elsevier.com/content/abstract/scopus_id/85121511874,"The fourth industrial revolution in agriculture seeks the automation of traditional practices, using modern smart technologies. Advances in electronics, computation and the internet of things are integrated for improving field inputs management. The aim of this paper is to present the design and implementation of an intelligent IoT-multiagent precision irrigation approach for improving water use efficiency in irrigation systems. The study site was the large-scale irrigation and drainage district of Chicamocha and Firavitoba (Usochicamocha) located in Boyacá - Colombia, where water is distributed from the Chicamocha riverbed. In the proposed system, irrigation is supervised and controlled in each field by an intelligent irrigation agent that autonomously prescribes and applies water amounts with agronomical criteria. The methodology was applied with real (cyber-physical) and virtual (simulated) intelligent agents and was extended to eleven pump stations that supply water to 5911 fields. Using a MQTT protocol, hundreds of irrigation intelligent agents report water prescriptions and crop characteristics to a master agent in each pump station, who creates a regional irrigation map to manage georeferenced field information and performs negotiation of water resources between agents according to supply availability. Field maps and intelligent irrigation agents can be visualized using devices with internet access. Results demonstrated that irrigation amounts were correctly applied on the fields, thus improving the water use efficiency. This technology is a novel support to decision-making in water resources management applications at field and district scales.",industry
10.1016/j.enconman.2021.115030,preprocessed,Energy Conversion and Management,scopus,2022-01-01,sciencedirect,deep reinforcement learning based energy management strategy of fuel cell hybrid railway vehicles considering fuel cell aging,https://api.elsevier.com/content/abstract/scopus_id/85119905389,"In the rail transportation industry, growing energy and environmental awareness requires the use of alternatives to combustion engines. These include hybrid electrically driven railway vehicles powered by fuel cells and batteries. The cost of hydrogen consumption and the lifetime of fuel cells are currently the main challenges that need to be addressed before widespread deployment of fuel cell railway vehicles can be realized. With this in mind, this work focuses on the energy management system with emphasis on optimizing the energy distribution to reduce the overall operational cost. The presented energy management strategy (EMS) aims at minimizing hydrogen consumption and fuel cell aging costs while achieving a favorable balance between battery charging and discharging. In order to take fuel cell aging into account in energy management and mitigate fuel cell aging trough power distribution, an online fuel cell aging estimation model based on four operation modes is introduced and applied. Moreover, the advanced deep reinforcement learning method Twin Delayed Deep Deterministic Policy Gradient (TD3) is used to obtain a promising EMS. To improve the adaptability of the strategy, a stochastic training environment, which is based on real measured speed profiles considering passenger numbers is used for training. Assuming different environmental and passenger transport volumes, the results confirm that the proposed TD3-EMS achieves battery charge-sustaining at low hydrogen consumption while slowing down fuel cell degradation.",industry
10.1016/j.ijpe.2021.108339,preprocessed,International Journal of Production Economics,scopus,2022-01-01,sciencedirect,age-based preventive maintenance with multiple printing options,https://api.elsevier.com/content/abstract/scopus_id/85118549755,"In today's economic context, production systems must be readily available and machinery downtime kept to a minimum. Maintenance and spare parts inventory management play a vital role in achieving these goals, and preventive maintenance has increasingly been considered in maintenance policies. Additive manufacturing (AM) has recently been combined with preventive maintenance, and thus represents an emerging research direction. However, few studies have as yet been conducted in this research stream, and we intend to fill this gap. Our study makes three main contributions. First, we address the main limitations of two current models (i.e., assuming that no failure occurs during the replenishment lead time of the spare parts). Second, we propose a new maintenance policy that considers two printing options with different levels of reliability and unitary purchase costs. Third, we develop a decision support system (DSS) to assist managers in deciding whether to implement a preventive maintenance policy that includes AM or conventional manufacturing (CM) parts. We take an interdisciplinary approach to conducting a parametrical analysis where we consider real data on the reliability of CM and AM parts, in addition to the impact of post-processing operations and optimization routines. We find that AM-based preventive maintenance policies are favored when the MTTF and the backorder costs are low and when the failure and maintenance costs are high. These findings have been incorporated into the DSS, which provides thresholds for every parameter to guide practitioners in choosing between AM and CM parts for preventive maintenance, without requiring time-expensive calculations.",industry
10.1016/j.cose.2021.102500,preprocessed,Computers and Security,scopus,2022-01-01,sciencedirect,antiviruses under the microscope: a hands-on perspective,https://api.elsevier.com/content/abstract/scopus_id/85118529412,"AntiViruses (AVs) are the main defense line against attacks for most users and much research has been done about them, especially proposing new detection procedures that work in academic prototypes. However, as most current and commercial AVs are closed-source solutions, in practice, little is known about their real internals: information such as what is a typical AV database size, the detection methods effectively used in each operation mode, and how often on average the AVs are updated are still unknown. This prevents research work from meeting the industrial practices more thoroughly. To fill this gap, in this work, we systematize the knowledge about AVs. To do so, we first surveyed the literature and identified existing knowledge gaps in AV internals’ working. Further, we bridged these gaps by analyzing popular (Windows, Linux, and Android) AV solutions to check their operations in practice. Our methodology encompassed multiple techniques, from tracing to fuzzing. We detail current AV’s architecture, including their multiple components, such as browser extensions and injected libraries, regarding their implementation, monitoring features, and self-protection capabilities. We discovered, for instance, a great disparity in the set of API functions hooked by the distinct AV’s libraries, which might have a significant impact in the viability of academically-proposed detection models (e.g., machine learning-based ones).",industry
10.1016/j.compind.2021.103556,preprocessed,Computers in Industry,scopus,2022-01-01,sciencedirect,c-ports: a proposal for a comprehensive standardization and implementation plan of digital services offered by the “port of the future”,https://api.elsevier.com/content/abstract/scopus_id/85118477493,"In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called “Port of the Future”. How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel & Marine Navigation, (ii) e-Freight & (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C-Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed.",industry
10.1016/j.dss.2021.113653,preprocessed,Decision Support Systems,scopus,2022-01-01,sciencedirect,ai-based industrial full-service offerings: a model for payment structure selection considering predictive power,https://api.elsevier.com/content/abstract/scopus_id/85114151068,"Artificial Intelligence and servitization reshape the way that manufacturing companies derive value. Aiming to sustain competitive advantage and intensify customer loyalty, full-service providers offer the use of their products as a service to achieve continuous revenues. For this purpose, companies implement AI classification algorithms to enable high levels of service at controllable costs. However, traditional asset sellers who become service providers require previously atypical payment structures, as classic payment methods involving a one-time fee for production costs and profit margins are unsuitable. In addition, a low predictive power of the implemented classification algorithm can lead to misclassifications, which diminish the achievable level of service and the intended net present value of the resultant service. While previous works focus solely on the costs of such misclassifications, our decision model highlights implications for payment structures, service levels, and – ultimately – the net present value of such data-driven service offerings. Our research suggests that predictive power can be a major factor in selecting a suitable payment structure and the overall design of service level agreements. Therefore, we compare common payment structures for data-driven services and investigate their relationship to predictive power. We develop our model using a design science methodology and iteratively evaluate our results using a four-step approach that includes interviews with industry experts and the application of our model to a real-world use case. In summary, our research extends the existing knowledge of servitization and data-driven services in the manufacturing industry through a quantitative decision model.",industry
10.1109/ccnc49033.2022.9700515,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,cave-vr and unity game engine for visualizing city scale 3d meshes,https://ieeexplore.ieee.org/document/9700515/,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",smart cities
10.1109/ccnc49033.2022.9700676,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,qos-aware priority-based task offloading for deep learning services at the edge,https://ieeexplore.ieee.org/document/9700676/,"Emerging Edge Computing (EC) technology has shown promise for many delay-sensitive Deep Learning (DL) based applications of smart cities in terms of improved Quality-of-Service (QoS). EC requires judicious decisions which jointly consider the limited capacity of the edge servers and provided QoS of DL-dependent services. In a smart city environment, tasks may have varying priorities in terms of when and how to serve them; thus, priorities of the tasks have to be considered when making resource management decisions. In this paper, we focus on finding optimal offloading decisions in a three-tier user-edge-cloud architecture while considering different priority classes for the DL-based services and making a trade-off between a task’s completion time and the provided accuracy by the DL-based service. We cast the optimization problem as an Integer Linear Program (ILP) where the objective is to maximize a function called gain of system (GoS) defined based on provided QoS and priority of the tasks. We prove the problem is NP-hard. We then propose an efficient offloading algorithm, called PGUS, that is shown to achieve near-optimal results in terms of the provided GoS. Finally, we compare our proposed algorithm, PGUS, with heuristics and a state-of-the-art algorithm, called GUS, using both numerical analysis and real-world implementation. Our results show that PGUS outperforms GUS by a factor of 45% in average in terms of serving the top 25% higher priority classes of the tasks while still keeping the overall percentage of the dropped tasks minimal and the overall gain of system maximized.",smart cities
10.1109/jiot.2021.3100068,preprocessed,IEEE Internet of Things Journal,IEEE,2022-03-01 00:00:00,ieeexplore,astcn: an attentive spatial–temporal convolutional network for flow prediction,https://ieeexplore.ieee.org/document/9511315/,"Flow prediction attracts intensive research interests, since it can offer essential support to many crucial problems in public safety and smart city, e.g., epidemic spread prediction and medical resource allocation optimization. Among all the models in flow prediction, deep learning models (e.g., convolutional neural networks, recurrent neural networks, and graph neural networks) are popular and outperform other statistics and machine learning models, since they can learn intrinsic structures and extract features from spatial–temporal (ST) data. However, most of them set strict temporal periods in the prediction or separate the interaction between spatial and temporal correlations. Therefore, the prediction accuracy is affected. To overcome the difficulties, we propose a flow prediction network attentive spatial–temporal convolutional network (ASTCN), which can effectively handle large-scale flow data and learn complex features. In ASTCN, we leverage an attention mechanism to overcome the previous problem of strict temporal periods, and can effectively fuse ST data with multiple factors from different time-series sources. Furthermore, we propose a causal 3-D convolutional layer based on temporal convolutional networks (TCNs). It can simultaneously extract both spatial and temporal features to improve the prediction accuracy. We comprehensively conducted our experiments based on real-world data sets. Experimental results show that ASTCN outperforms the state-of-the-art methods by at least 3.78% in root mean square error. Therefore, ASTCN is a potential solution to other large-scale ST problems.",smart cities
10.1109/jiot.2021.3097768,preprocessed,IEEE Internet of Things Journal,IEEE,2022-03-01 00:00:00,ieeexplore,user-aware and flexible proactive caching using lstm and ensemble learning in iot-mec networks,https://ieeexplore.ieee.org/document/9488291/,"To meet the stringent demands of emerging Internet-of-Things (IoT) applications, such as smart home, smart city, and virtual reality in 5G/6G IoT networks, edge content caching for mobile/multiaccess edge computing (MEC) has been identified as a promising approach to improve the quality of services in terms of latency and energy consumption. However, the limitations of cache capacity make it difficult to develop an effective common caching framework that satisfies diverse user preferences. In this article, we propose a new content caching strategy that maximizes the cache hit ratio through flexible prediction in dynamically changing network and user environments. It is based on a hierarchical deep learning architecture: long short-term memory (LSTM)-based local learning and ensemble-based meta-learning. First, as a local learning model, we employ an LSTM method with seasonal-trend decomposition using loess (STL)-based preprocessing. It identifies the attributes for demand prediction on the contents in various demographic user groups. Second, as a metalearning model, we employ a regression-based ensemble learning method, which uses an online convex optimization framework and exhibits sublinear “regret” performance. It orchestrates the obtained multiple demographic user preferences into a unified caching strategy in real time. Extensive experiments were conducted on the popular MovieLens data sets. It was shown that the proposed control provides up to a 30% higher cache hit ratio than conventional representative algorithms and a near-optimal cache hit ratio within approximately 9% of the optimal caching scheme with perfect prior knowledge of content popularity. The proposed learning and caching control can be implemented as a core function of the 5G/6G standard’s network data analytic function (NWDAF) module.",smart cities
10.1109/tnse.2021.3050781,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,vfchain: enabling verifiable and auditable federated learning via blockchain systems,https://ieeexplore.ieee.org/document/9321132/,"Advanced artificial intelligence techniques, such as federated learning, has been applied to broad areas, e.g., image classification, speech recognition, smart city, and healthcare. Despite intensive research on federated learning, existing schemes are vulnerable to attacks and can hardly meet the security requirements for real-world applications. The problem of designing a secure federated learning framework to ensure the correctness of training procedure has not been sufficiently studied and remains open. In this paper, we propose VFChain, a verifiable and auditable federated learning framework based on the blockchain system. First, to provide the verifiability, a committee is selected through the blockchain to collectively aggregate models and record verifiable proofs in the blockchain. Then, to provide the auditability, a novel authenticated data structure is proposed for blockchain to improve the search efficiency of verifiable proofs and support a secure rotation of committee. Finally, to further improve the search efficiency, an optimization scheme is proposed to support multiple-model learning tasks. We implement VFChain and conduct extensive experiments by utilizing the popular deep learning models over the public real-world dataset. The evaluation results demonstrate the effectiveness of our proposed VFChain system.",smart cities
10.1109/tkde.2020.2985954,preprocessed,IEEE Transactions on Knowledge and Data Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,incorporating multi-source urban data for personalized and context-aware multi-modal transportation recommendation,https://ieeexplore.ieee.org/document/9063461/,"Transportation recommendation is one important map service in navigation applications. Previous transportation recommendation solutions fail to deliver satisfactory user experience because their recommendations only consider routes in one transportation mode (uni-modal, e.g., taxi, bus, cycle) and largely overlook situational context. In this work, we propose <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq1-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>, a multi-task deep learning based recommendation system that offers multi-modal transportation planning and is adaptive to various situational context (e.g., nearby point-of-interest (POI) distribution and weather). We leverage the availability of existing routing engines and big urban data, and design a novel two-level framework that integrates uni-modal and multi-modal (e.g., taxi-bus, bus-cycle) routes as well as heterogeneous urban data for intelligent multi-modal transportation recommendation. In addition to urban context features constructed from multi-source urban data, we learn the latent representations of users, origin-destination (OD) pairs and transportation modes based on user implicit feedbacks, which captures the collaborative transportation mode preferences of users and OD pairs. Moreover, we propose two models to recommend the proper route among various uni-modal and multi-modal transportation routes: (1) a light-weight gradient boosting decision tree (GBDT) based recommendation model; and (2) a multi-task wide and deep learning (MTWDL) based recommendation model. We also optimize the framework to support real-time, large-scale route query and recommendation. We deploy <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq2-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> on Baidu Maps,<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref><fn id=""fn1""><label>1.</label><p><uri>https://maps.baidu.com/</uri>.</p> </fn> one of the world's largest map services. Real-world urban-scale experiments demonstrate the effectiveness and efficiency of our proposed system. Since its deployment in August 2018, <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq3-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> has answered over a hundred million route recommendation queries made by over ten million distinct users. The GBDT based model and MTWDL based model achieve 82.8 and 96.6 percent relative improvement of user click ratio, respectively.",smart cities
10.1109/scset55041.2022.00053,preprocessed,2022 International Seminar on Computer Science and Engineering Technology (SCSET),IEEE,2022-01-09 00:00:00,ieeexplore,research on leak location method of water supply network based on deep neural network model,https://ieeexplore.ieee.org/document/9700905/,"The water supply network is one of the important infrastructure in urban construction. It has strong theoretical and practical significance to realize the real-time monitoring and leak location of the water supply network. In this paper, based on the similarity of water supply network node pressure, fuzzy C-means clustering algorithm is used to realize the selection of finite monitoring points. On this basis, a depth neural network model is constructed according to the pressure changes of the monitoring points before and after the leakage of the water supply network, so as to locate the leakage points. In the experimental part, hydraulics simulation was conducted by using EPANETH pipe network adjustment software according to the layout structure of water supply network, and the pressure of all nodes was obtained. A deep neural network model was established by Keras in Tensorflow framework. After model training and testing, the training error was controlled within the effective range of 5 %. Finally, the model is applied to the actual leakage problem of underground water supply network in Langxi County of Xuancheng City, and the accurate location of the leakage point is realized. The experiment proves the feasibility and accuracy of the method proposed in this paper.",smart cities
10.1109/tgrs.2021.3056624,preprocessed,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,attention-based multiscale residual adaptation network for cross-scene classification,https://ieeexplore.ieee.org/document/9377566/,"In recent years, classification has obtained ever-rising attention and has been applied to many areas in the field of remote sensing, including land use, forest monitoring, urban planning, and vegetation management. Due to the lack of labeled data and the poor generalization ability of supervised models, cross-scene classification is proposed for better utilization of the existing knowledge. Existing adaptation methods for cross-scene classification only consider the marginal distribution, while the conditional distribution is equally important in real applications. In addition, approaches based on deep learning align the distribution of features extracted from a single-scale structure, leading to the loss of information. To overcome the above drawbacks, an Attention-based Multiscale Residual Adaptation Network (AMRAN) is proposed for cross-scene classification tasks. In the proposed AMRAN, both the marginal and conditional distributions are taken into consideration for more comprehensive alignment. Besides, the attention mechanism and the multiscale strategy are used to extract more robust features and more complete information, respectively. Experimental results between four existing scene classification data sets demonstrate that AMRAN has a significant improvement compared with the state-of-the-art deep adaptation methods.",smart cities
10.1109/access.2021.3137031,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,autonomous detection and deterrence of pigeons on buildings by drones,https://ieeexplore.ieee.org/document/9656717/,"Pigeons may transmit diseases to humans and cause damages to buildings, monuments, and other infrastructure. Therefore, several control strategies have been developed, but they have been found to be either ineffective or harmful to animals and often depend on human operation. This study proposes a system capable of autonomously detecting and deterring pigeons on building roofs using a drone. The presence and position of pigeons were detected in real time by a neural network using images taken by a video camera located on the roof. Moreover, a drone was utilized to deter the animals. Field experiments were conducted in a real-world urban setting to assess the proposed system by comparing the number of animals and their stay durations for over five days against the 21-day-trial experiment without the drone. During the five days of experiments, the drone was automatically deployed 55 times and was significantly effective in reducing the number of birds and their stay durations without causing any harm to them. In conclusion, this study has proven the effectiveness of this system in deterring birds, and this approach can be seen as a fully autonomous alternative to the already existing methods.",smart cities
10.1109/lgrs.2020.3030839,preprocessed,IEEE Geoscience and Remote Sensing Letters,IEEE,2000-01-01 00:00:00,ieeexplore,hybrid attention networks for flow and pressure forecasting in water distribution systems,https://ieeexplore.ieee.org/document/9241394/,"Multivariate geo-sensory time series prediction is challenging because of the complex spatial and temporal correlations. In urban water distribution systems (WDSs), numerous spatial-correlated sensors have been deployed to continuously collect hydraulic data. Forecasts of the monitored flow and pressure time series are of vital importance for operational decision making, alerts, and anomaly detection. To address this issue, we proposed a hybrid dual-stage spatial–temporal attention-based recurrent neural networks (hDS-RNN). Our model consists of two stages: a spatial attention-based encoder and a temporal attention-based decoder. Specifically, a hybrid spatial attention mechanism that employs inputs along the temporal and spatial axes is proposed. Experiments on a real-world data set are conducted, which demonstrate that our model outperformed seven baseline models in flow and pressure predictions in WDS.",smart cities
10.1109/jstars.2022.3142898,preprocessed,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,wh-mavs: a novel dataset and deep learning benchmark for multiple land use and land cover applications,https://ieeexplore.ieee.org/document/9681304/,"Over the past decade, many excellent data sharing efforts have enriched the remote sensing scene classification (SC) methods. These datasets have achieved great success in complex high-level semantic information interpretation. However, most existing datasets are collected from standard and ungeoreferenced image patches for algorithm training and evaluation. These datasets do not fit for practical applications and cannot be directly applied in further geographical study. Accordingly, we provide a large range high-resolution SC dataset with multiple time phases, called “<bold>W</bold>u<bold>h</bold>an <bold>M</bold>ulti<bold>a</bold>pplication <bold>V</bold>HR <bold>S</bold>cene classification dataset (WH-MAVS).” It facilitates the study of SC and scene change detection (SCD) algorithms. Moreover, it can also be directly employed to perform a variety of real-life land use application tasks. To the best of our knowledge, this is the first free, publicly available, georeferenced, and annotated dataset to cover almost an entire megacity. The WH-MAVS was collected and annotated from Google Earth imagery with the same spatial resolution and uniform nonoverlapping patch size, covering the central area of Wuhan, China. The total number of scene samples is 47 137, which belong to 14 classes with 23 567 labeled patches for each time phase in 2014 and 2016, respectively. The geographic coordinates of all samples in both time phases exhibit one-to-one correspondence with 23 202 unchanged image patches of scene categories and 365 changed ones. The distribution of the number of samples in each class is highly imbalanced; moreover, there are large intraclass differences and indistinguishable interclass variances. These characteristics are closer to the real land use/land cover application tasks and introduce further challenges to the related algorithm research. In addition, we conducted benchmark experiments on SC and SCD based on the WH-MAVS dataset with widely used deep learning models. DenseNet169 was found to achieve the best performance. The overall accuracies are 91.07% and 92.09%, respectively, in the 2014 and 2016 validation sets of WH-MAVS. Furthermore, SCD obtained by DenseNet169 has a binary change detection accuracy of 89.56% and a multiple (from–to) change detection accuracy of 86.70%. Over and above the research value of the algorithm, it is also proven to have practical applications in fields such as urban planning, landscape pattern analysis, and urban dynamic monitoring and analysis.",smart cities
10.1109/ccnc49033.2022.9700579,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,demo: an experimental environment based on mini-pcs for federated learning research,https://ieeexplore.ieee.org/document/9700579/,"There is a growing research interest in Federated Learning (FL), a promising approach for data privacy preservation and proximity of training to the network edge, where data is generated. Resource consumption for Machine Learning (ML) training and inference is important for edge nodes, but most of the proposed protocols and algorithms for FL are evaluated by simulations. In this demo paper, we present an environment based on distributed mini-PCs to enable experimental study of FL protocols and algorithms. We have installed low-capacity mini-PCs within a wireless city-level mesh network and deployed container-based FL components on these nodes. We show the deployed FL clients and server at different nodes in the city and demonstrate how an FL experiment can be set and run in a real environment.",smart cities
10.1109/wacv51458.2022.00308,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,multi-branch neural networks for video anomaly detection in adverse lighting and weather conditions,https://ieeexplore.ieee.org/document/9706717/,"Automated anomaly detection in surveillance videos has attracted much interest as it provides a scalable alternative to manual monitoring. Most existing approaches achieve good performance on clean benchmark datasets recorded in well-controlled environments. However, detecting anomalies is much more challenging in the real world. Adverse weather conditions like rain or changing brightness levels cause a significant shift in the input data distribution, which in turn can lead to the detector model incorrectly reporting high anomaly scores. Additionally, surveillance cameras are usually deployed in evolving environments such as a city street of which the appearance changes over time because of seasonal changes or roadworks. The anomaly detection model will need to be updated periodically to deal with these issues. In this paper, we introduce a multi-branch model that is equipped with a trainable preprocessing step and multiple identical branches for detecting anomalies during day and night as well as in sunny and rainy conditions. We experimentally validate our approach on a distorted version of the Avenue dataset and provide qualitative results on real-world surveillance camera data. Experimental results show that our method outperforms the existing methods in terms of detection accuracy while being faster and more robust on scenes with varying visibility.",smart cities
10.1109/tii.2021.3091597,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,optimal sizing and efficient routing of electric vehicles for a vehicle-on-demand system,https://ieeexplore.ieee.org/document/9462468/,"Due to the steep rise in global population, urbanization, and industrialization, most of the cities in the world today are witnessing increased carbon footprints and reduced per capita space. In such a scenario, vehicle sharing and carpooling systems, specifically with electric vehicles (EV), can significantly help due to the reduced cost of ownership, maintenance, and parking space. In this article, we study the challenging problem of optimal sizing and efficient routing for an electric vehicle-on-demand system. Users demand EVs at the pooling stations at different time instances with individual deadlines to reach the destinations. The objective is to fulfill all the demands respecting the deadlines with minimum investment, which essentially translates to minimizing the total number of EVs. We define the problem formally using mixed-integer linear programming formulation and propose a set of intelligent and efficient heuristic algorithms to solve it efficiently. The proposed algorithms’ performances are tested and validated in a simulated environment on a reasonable size city network with many EV demands. The results obtained show that the proposed heuristic algorithms are competent by reducing 200–360 EVs per day on a network of 282 charging ports, indicating their scalability to be implemented in real-world scenarios.",smart cities
10.1109/tits.2020.3029537,preprocessed,IEEE Transactions on Intelligent Transportation Systems,IEEE,2022-02-01 00:00:00,ieeexplore,spatial positioning token (sptoken) for smart mobility,https://ieeexplore.ieee.org/document/9238413/,"We introduce a permissioned distributed ledger technology (DLT) design for crowdsourced smart mobility applications. This architecture is based on a directed acyclic graph architecture (similar to the IOTA tangle) and uses both Proof-of-Work and Proof-of-Position mechanisms to provide protection against spam attacks and malevolent actors. In addition to enabling individuals to retain ownership of their data and to monetize it, the architecture is also suitable for distributed privacy-preserving machine learning algorithms, is lightweight, and can be implemented in simple internet-of-things (IoT) devices. To demonstrate its efficacy, we apply this framework to reinforcement learning settings where a third party is interested in acquiring information from agents. In particular, one may be interested in sampling an unknown vehicular traffic flow in a city, using a DLT-type architecture and without perturbing the density, with the idea of realizing a set of virtual tokens as surrogates of real vehicles to explore geographical areas of interest. These tokens, whose authenticated position determines write access to the ledger, are thus used to emulate the probing actions of commanded (real) vehicles on a given planned route by “jumping” from a passing-by vehicle to another to complete the planned trajectory. Consequently, the environment stays unaffected (i.e., the autonomy of participating vehicles is not influenced by the algorithm), regardless of the number of emitted tokens. The design of such a DLT architecture is presented, and numerical results from large-scale simulations are provided to validate the proposed approach.",smart cities
10.1109/tits.2020.3015542,preprocessed,IEEE Transactions on Intelligent Transportation Systems,IEEE,2022-02-01 00:00:00,ieeexplore,taxi demand prediction using parallel multi-task learning model,https://ieeexplore.ieee.org/document/9172100/,"Accurate and real-time taxi demand prediction can help managers pre-allocate taxi resources in cities, which assists drivers quickly finding passengers and reduce passengers’ waiting time. Most of the existing studies focus on mining spatial-temporal characteristics of taxi demand distributions, while lacking in modeling the correlations between taxi pick-up demand and the drop-off demand from the perspective of multi-task learning. In this article, we propose a multi-task learning model containing three parallel LSTM layers to co-predict taxi pick-up and drop-off demands, and compare the performance of single demand prediction methodology and that of two demands’ co-prediction methodology. Experimental results on real-world datasets demonstrate that the pick-up demand and the drop-off demand do depend on each other, and the effectiveness of the proposed co-prediction methods.",smart cities
10.1109/tii.2021.3071771,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,a utility-based subcontract method for sensing task in mobile crowd sensing,https://ieeexplore.ieee.org/document/9399247/,"In mobile crowd sensing, the mobile terminal integrates a variety of widely distributed sensing devices and communication ports. Sensing devices and communication ports can collect and share all kinds of perception data. However, inherent contradictions exist among perceived ability, communication port, and moving rule while collecting real-time and accurate sensing information. This article mainly focused on recruited and selected mobile nodes and assigned sensing tasks to improve the quality of sensing information. The optimization of the implementation stage of the sensing task is beyond the scope of this study. This article proposes a utility-based sensing task decomposition and subcontract algorithm, which is a method of sensing data acquisition that establishes direct collaboration between mobile nodes. A mobility model based on Markov chain is established to forecast the spatial distribution of sensing nodes. A utility function is designed to estimate the sensing task execution capacity of sensing nodes based on spatial distribution and tempo-spatial coverage of the collected data. The sensing tasks are then decomposed and subcontracted to neighboring nodes according to the utilities of the neighboring nodes to the decomposed sensing tasks. This method improves the quality of sensing data in terms of sensing data coverage and finished ratio of sensing task.",smart cities
10.1109/access.2022.3146728,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,assistive devices analysis for visually impaired persons: a review on taxonomy,https://ieeexplore.ieee.org/document/9693966/,"Visually impaired persons (VIPs) comprise a significant portion of the population and they are present in all corners of the world. In recent times, the technology proved its presence in every domain of life and innovative devices are assisting humans in all fields especially, artificial intelligence has dominated and outperformed the rest of the trades. VIPs need assistance in performing daily life tasks like object/obstacle detection and recognition, navigation, and mobility, particularly in indoor and outdoor environments. Moreover, the protection and safety of these people are of prime concern. Several devices and applications have been developed for the assistance of VIPs. Firstly, these devices take input from the surrounding environment through different sensors e.g. infrared radiation, ultrasonic, imagery sensor, etc. In the second stage, state of the art machine learning techniques process these signals and extract useful information. Finally, feedback is provided to the user through auditory and/or vibratory means. It is observed that most of the existing devices are constrained in their abilities. The paper presents a comprehensive comparative analysis of the state-of-the-art assistive devices for VIPs. These techniques are categorized based on their functionality and working principles. The main attributes, challenges, and limitations of these techniques have also been highlighted. Moreover, a score-based quantitative analysis of these devices is performed to highlight their feature enrichment capability for each category. It may help to select an appropriate device for a particular scenario.",smart cities
10.1109/tnse.2021.3072911,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,prioritized content determination and dissemination using reinforcement learning in dtns,https://ieeexplore.ieee.org/document/9403937/,"In a battlefield, several groups of soldiers are deployed with different mission goals by the command and control center (CC). To continue the missions appropriately and get a better understanding of the situation, the soldiers, as well as the CC, need to collect information of interest generated in different battle zones. However, due to the damaged network infrastructure in the hostile areas, it is a challenge to determine the topics of interest associated with the events and missions, and efficiently forward the associated content to the CC. Hence, the devices of the soldiers (nodes) generate, store and forward content hop by hop using a Delay Tolerant Network (DTN). While forwarding content, nodes avoid congestion so that meaningful content related to prioritized mission goals can be disseminated. In this dynamic surrounding, any sudden but important event-related content should also be sent to the CC with the help of intermediate nodes regardless of their own mission interests. We design a scheme to forward contents generated by the nodes to the CC using Reinforcement Learning (RL) while maximizing the number of interesting data in the respective nodes' buffer, and avoiding congestion. In this forwarding process, we focus on identifying the trending topics/keywords among changing missions and their related data at the node level, and the changes of interest of the nodes based on their mobility and connectivity patterns. Experiments are conducted using real datasets and ONE simulator to show the effectiveness of Reinforcement Learning (RL) on the prioritized content dissemination in a DTN.",smart cities
10.1109/ojits.2021.3139393,preprocessed,IEEE Open Journal of Intelligent Transportation Systems,IEEE,2000-01-01 00:00:00,ieeexplore,napc: a neural algorithm for automated passenger counting in public transport on a privacy-friendly dataset,https://ieeexplore.ieee.org/document/9665722/,"Real-time load information in public transport is of high importance for both passengers and service providers. Neural algorithms have shown a high performance on various object counting tasks and play a continually growing methodological role in developing automated passenger counting systems. However, the publication of public-space video footage is often contradicted by legal and ethical considerations to protect the passengers’ privacy. This work proposes an end-to-end Long Short-Term Memory network with a problem-adapted cost function that learned to count boarding and alighting passengers on a publicly available, comprehensive dataset of approx.13,000 manually annotated low-resolution 3D LiDAR video recordings (depth information only) from the doorways of a regional train. These depth recordings do not allow the identification of single individuals. For each door opening phase, the trained models predict the correct passenger count (ranging from 0 to 67) in approx.96% of boarding and alighting, respectively. Repeated training with different training and validation sets confirms the independence of this result from a specific test set.",smart cities
10.1109/jsen.2021.3132460,preprocessed,IEEE Sensors Journal,IEEE,2001-02-01 20:22:00,ieeexplore,automatic rail component detection based on attnconv-net,https://ieeexplore.ieee.org/document/9634063/,"The automatic detection of major rail components using railway images is beneficial to ensure the rail transport safety. In this paper, we propose an attention-powered deep convolutional network (AttnConv-net) to detect multiple rail components including the rail, clips, and bolts. The proposed method consists of a deep convolutional neural network (DCNN) as the backbone, cascading attention blocks (CAB), and two feed forward networks (FFN). Two types of positional embedding are applied to enrich information in latent features extracted from the backbone. Based on processed latent features, the CAB aims to learn the local context of rail components including their categories and component boundaries. Final categories and bounding boxes are generated via two FFN implemented in parallel. To enhance the detection of small components, various data augmentation methods are employed in training process. The effectiveness of the proposed AttnConv-net is validated with one real dataset and another synthesized dataset. Compared with classic convolutional neural network based methods, our proposed method simplifies the detection pipeline by eliminating the need of prior- and post-processing, which offers a new speed-quality solution to enable faster and more accurate image-based rail component detections.",smart cities
http://arxiv.org/abs/2202.08982v1,preprocessed,arxiv,arxiv,2022-02-18 00:00:00,arxiv,"pgcn: progressive graph convolutional networks for spatial-temporal
  traffic forecasting",http://arxiv.org/abs/2202.08982v1,"The complex spatial-temporal correlations in transportation networks make the
traffic forecasting problem challenging. Since transportation system inherently
possesses graph structures, much research efforts have been put with graph
neural networks. Recently, constructing adaptive graphs to the data has shown
promising results over the models relying on a single static graph structure.
However, the graph adaptations are applied during the training phases, and do
not reflect the data used during the testing phases. Such shortcomings can be
problematic especially in traffic forecasting since the traffic data often
suffers from the unexpected changes and irregularities in the time series. In
this study, we propose a novel traffic forecasting framework called Progressive
Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by
progressively adapting to input data during the training and the testing
phases. Specifically, we implemented the model to construct progressive
adjacency matrices by learning trend similarities among graph nodes. Then, the
model is combined with the dilated causal convolution and gated activation unit
to extract temporal features. With residual and skip connections, PGCN performs
the traffic prediction. When applied to four real-world traffic datasets of
diverse geometric nature, the proposed model achieves state-of-the-art
performance with consistency in all datasets. We conclude that the ability of
PGCN to progressively adapt to input data enables the model to generalize in
different study sites with robustness.",smart cities
http://arxiv.org/abs/2202.07147v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,"graph meta-reinforcement learning for transferable autonomous
  mobility-on-demand",http://arxiv.org/abs/2202.07147v1,"Autonomous Mobility-on-Demand (AMoD) systems represent an attractive
alternative to existing transportation paradigms, currently challenged by
urbanization and increasing travel needs. By centrally controlling a fleet of
self-driving vehicles, these systems provide mobility service to customers and
are currently starting to be deployed in a number of cities around the world.
Current learning-based approaches for controlling AMoD systems are limited to
the single-city scenario, whereby the service operator is allowed to take an
unlimited amount of operational decisions within the same transportation
system. However, real-world system operators can hardly afford to fully
re-train AMoD controllers for every city they operate in, as this could result
in a high number of poor-quality decisions during training, making the
single-city strategy a potentially impractical solution. To address these
limitations, we propose to formalize the multi-city AMoD problem through the
lens of meta-reinforcement learning (meta-RL) and devise an actor-critic
algorithm based on recurrent graph neural networks. In our approach, AMoD
controllers are explicitly trained such that a small amount of experience
within a new city will produce good system performance. Empirically, we show
how control policies learned through meta-RL are able to achieve near-optimal
performance on unseen cities by learning rapidly adaptable policies, thus
making them more robust not only to novel environments, but also to
distribution shifts common in real-world operations, such as special events,
unexpected congestion, and dynamic pricing schemes.",smart cities
http://arxiv.org/abs/2202.06639v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"on the complexity of object detection on real-world public
  transportation images for social distancing measurement",http://arxiv.org/abs/2202.06639v1,"Social distancing in public spaces has become an essential aspect in helping
to reduce the impact of the COVID-19 pandemic. Exploiting recent advances in
machine learning, there have been many studies in the literature implementing
social distancing via object detection through the use of surveillance cameras
in public spaces. However, to date, there has been no study of social distance
measurement on public transport. The public transport setting has some unique
challenges, including some low-resolution images and camera locations that can
lead to the partial occlusion of passengers, which make it challenging to
perform accurate detection. Thus, in this paper, we investigate the challenges
of performing accurate social distance measurement on public transportation. We
benchmark several state-of-the-art object detection algorithms using real-world
footage taken from the London Underground and bus network. The work highlights
the complexity of performing social distancing measurement on images from
current public transportation onboard cameras. Further, exploiting domain
knowledge of expected passenger behaviour, we attempt to improve the quality of
the detections using various strategies and show improvement over using vanilla
object detection alone.",smart cities
http://arxiv.org/abs/2202.06608v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"unscene: toward unsupervised scenario extraction for automated driving
  systems from urban naturalistic road traffic data",http://arxiv.org/abs/2202.06608v1,"Scenario-based testing is a promising approach to solve the challenge of
proving the safe behavior of vehicles equipped with automated driving systems
(ADS). Since an infinite number of concrete scenarios can theoretically occur
in real-world road traffic, the extraction of relevant scenarios that are
sensitive regarding the safety-related behavior of ADS-equipped vehicles is a
key aspect for the successful verification and validation of these systems.
Therefore, this paper provides a method for extracting multimodal urban traffic
scenarios from naturalistic road traffic data in an unsupervised manner for
minimizing the amount of (potentially biased) prior expert knowledge needed.
Rather than an (expensive) rule-based assignment by extracting concrete
scenarios into predefined functional scenarios, the presented method deploys an
unsupervised machine learning pipeline. It includes principal feature analysis,
feature extraction with so-called scenario grids, dimensionality reduction by
principal component analysis, scenario clustering as well as cluster
validation. The approach allows exploring the unknown natures of the data and
interpreting them as scenarios that experts could not have anticipated. The
method is demonstrated and evaluated for naturalistic road traffic data at
urban intersections from the inD and the Silicon Valley dataset. The findings
encourage the use of this type of data as well as unsupervised machine learning
approaches as important pillar for a systematic construction of a relevant
scenario database with sufficient coverage for testing ADS.",smart cities
http://arxiv.org/abs/2202.05334v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"learning the pedestrian-vehicle interaction for pedestrian trajectory
  prediction",http://arxiv.org/abs/2202.05334v1,"In this paper, we study the interaction between pedestrians and vehicles and
propose a novel neural network structure called the Pedestrian-Vehicle
Interaction (PVI) extractor for learning the pedestrian-vehicle interaction. We
implement the proposed PVI extractor on both sequential approaches (long
short-term memory (LSTM) models) and non-sequential approaches (convolutional
models). We use the Waymo Open Dataset that contains real-world urban traffic
scenes with both pedestrian and vehicle annotations. For the LSTM-based models,
our proposed model is compared with Social-LSTM and Social-GAN, and using our
proposed PVI extractor reduces the average displacement error (ADE) and the
final displacement error (FDE) by 7.46% and 5.24%, respectively. For the
convolutional-based models, our proposed model is compared with Social-STGCNN
and Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and
FDE by 2.10% and 1.27%, respectively. The results show that the
pedestrian-vehicle interaction influences pedestrian behavior, and the models
using the proposed PVI extractor can capture the interaction between
pedestrians and vehicles, and thereby outperform the compared methods.",smart cities
http://arxiv.org/abs/2202.05118v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"reinforcement learning in the wild: scalable rl dispatching algorithm
  deployed in ridehailing marketplace",http://arxiv.org/abs/2202.05118v1,"In this study, a real-time dispatching algorithm based on reinforcement
learning is proposed and for the first time, is deployed in large scale.
Current dispatching methods in ridehailing platforms are dominantly based on
myopic or rule-based non-myopic approaches. Reinforcement learning enables
dispatching policies that are informed of historical data and able to employ
the learned information to optimize returns of expected future trajectories.
Previous studies in this field yielded promising results, yet have left room
for further improvements in terms of performance gain, self-dependency,
transferability, and scalable deployment mechanisms. The present study proposes
a standalone RL-based dispatching solution that is equipped with multiple
mechanisms to ensure robust and efficient on-policy learning and inference
while being adaptable for full-scale deployment. A new form of value updating
based on temporal difference is proposed that is more adapted to the inherent
uncertainty of the problem. For the driver-order assignment, a customized
utility function is proposed that when tuned based on the statistics of the
market, results in remarkable performance improvement and interpretability. In
addition, for reducing the risk of cancellation after drivers' assignment, an
adaptive graph pruning strategy based on the multi-arm bandit problem is
introduced. The method is evaluated using offline simulation with real data and
yields notable performance improvement. In addition, the algorithm is deployed
online in multiple cities under DiDi's operation for A/B testing and is
launched in one of the major international markets as the primary mode of
dispatch. The deployed algorithm shows over 1.3% improvement in total driver
income from A/B testing. In addition, by causal inference analysis, as much as
5.3% improvement in major performance metrics is detected after full-scale
deployment.",smart cities
http://arxiv.org/abs/2202.04628v2,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,"reinforcement learning with sparse rewards using guidance from offline
  demonstration",http://arxiv.org/abs/2202.04628v2,"A major challenge in real-world reinforcement learning (RL) is the sparsity
of reward feedback. Often, what is available is an intuitive but sparse reward
function that only indicates whether the task is completed partially or fully.
However, the lack of carefully designed, fine grain feedback implies that most
existing RL algorithms fail to learn an acceptable policy in a reasonable time
frame. This is because of the large number of exploration actions that the
policy has to perform before it gets any useful feedback that it can learn
from. In this work, we address this challenging problem by developing an
algorithm that exploits the offline demonstration data generated by a
sub-optimal behavior policy for faster and efficient online RL in such sparse
reward settings. The proposed algorithm, which we call the Learning Online with
Guidance Offline (LOGO) algorithm, merges a policy improvement step with an
additional policy guidance step by using the offline demonstration data. The
key idea is that by obtaining guidance from - not imitating - the offline data,
LOGO orients its policy in the manner of the sub-optimal policy, while yet
being able to learn beyond and approach optimality. We provide a theoretical
analysis of our algorithm, and provide a lower bound on the performance
improvement in each learning episode. We also extend our algorithm to the even
more challenging incomplete observation setting, where the demonstration data
contains only a censored version of the true state observation. We demonstrate
the superior performance of our algorithm over state-of-the-art approaches on a
number of benchmark environments with sparse rewards and censored state.
Further, we demonstrate the value of our approach via implementing LOGO on a
mobile robot for trajectory tracking and obstacle avoidance, where it shows
excellent performance.",smart cities
http://arxiv.org/abs/2202.03917v1,preprocessed,arxiv,arxiv,2022-02-08 00:00:00,arxiv,edge-based fever screening system over private 5g,http://arxiv.org/abs/2202.03917v1,"Edge computing and 5G have made it possible to perform analytics closer to
the source of data and achieve super-low latency response times, which is not
possible with centralized cloud deployment. In this paper, we present a novel
fever-screening system, which uses edge machine learning techniques and
leverages private 5G to accurately identify and screen individuals with fever
in real-time. Particularly, we present deep-learning based novel techniques for
fusion and alignment of cross-spectral visual and thermal data streams at the
edge. Our novel Cross-Spectral Generative Adversarial Network (CS-GAN)
synthesizes visual images that have the key, representative object level
features required to uniquely associate objects across visual and thermal
spectrum. Two key features of CS-GAN are a novel, feature-preserving loss
function that results in high-quality pairing of corresponding cross-spectral
objects, and dual bottleneck residual layers with skip connections (a new,
network enhancement) to not only accelerate real-time inference, but to also
speed up convergence during model training at the edge. To the best of our
knowledge, this is the first technique that leverages 5G networks and limited
edge resources to enable real-time feature-level association of objects in
visual and thermal streams (30 ms per full HD frame on an Intel Core i7-8650
4-core, 1.9GHz mobile processor). To the best of our knowledge, this is also
the first system to achieve real-time operation, which has enabled fever
screening of employees and guests in arenas, theme parks, airports and other
critical facilities. By leveraging edge computing and 5G, our fever screening
system is able to achieve 98.5% accuracy and is able to process about 5X more
people when compared to a centralized cloud deployment.",smart cities
http://arxiv.org/abs/2202.02653v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,"millisecond speed deep learning based proton dose calculation with monte
  carlo accuracy",http://arxiv.org/abs/2202.02653v1,"Next generation online and real-time adaptive radiotherapy workflows require
precise particle transport simulations in sub-second times, which is unfeasible
with current analytical pencil beam algorithms (PBA) or stochastic Monte Carlo
(MC) methods. We present a data-driven millisecond speed dose calculation
algorithm (DoTA) accurately predicting the dose deposited by mono-energetic
proton pencil beams for arbitrary energies and patient geometries. Given the
forward-scattering nature of protons, we frame 3D particle transport as
modeling a sequence of 2D geometries in the beam's eye view. DoTA combines
convolutional neural networks extracting spatial features (e.g., tissue and
density contrasts) with a transformer self-attention backbone that routes
information between the sequence of geometry slices and a vector representing
the beam's energy, and is trained to predict low noise MC simulations of proton
beamlets using 80,000 different head and neck, lung, and prostate geometries.
Predicting beamlet doses in 5 ms with a very high gamma pass rate of 99.37%
(1%, 3 mm) compared to the ground truth MC calculations, DoTA significantly
improves upon analytical pencil beam algorithms both in precision and speed.
Offering MC accuracy 100 times faster than PBAs for pencil beams, our model
calculates full treatment plan doses in 10 to 15 s depending on the number of
beamlets, achieving a 99.70% (2%, 2 mm) gamma pass rate across 9 test patients.
Outperforming all previous analytical pencil beam and deep learning based
approaches, DoTA represents a new state of the art in data-driven dose
calculation and can directly compete with the speed of even commercial GPU MC
approaches. Providing the sub-second speed required for adaptive treatments,
straightforward implementations could offer similar benefits to other steps of
the radiotherapy workflow or other modalities such as helium or carbon
treatments.",smart cities
http://arxiv.org/abs/2202.01862v1,preprocessed,arxiv,arxiv,2022-02-03 00:00:00,arxiv,practical imitation learning in the real world via task consistency loss,http://arxiv.org/abs/2202.01862v1,"Recent work in visual end-to-end learning for robotics has shown the promise
of imitation learning across a variety of tasks. Such approaches are expensive
both because they require large amounts of real world training demonstrations
and because identifying the best model to deploy in the real world requires
time-consuming real-world evaluations. These challenges can be mitigated by
simulation: by supplementing real world data with simulated demonstrations and
using simulated evaluations to identify high performing policies. However, this
introduces the well-known ""reality gap"" problem, where simulator inaccuracies
decorrelate performance in simulation from that of reality. In this paper, we
build on top of prior work in GAN-based domain adaptation and introduce the
notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages
sim and real alignment both at the feature and action-prediction levels. We
demonstrate the effectiveness of our approach by teaching a mobile manipulator
to autonomously approach a door, turn the handle to open the door, and enter
the room. The policy performs control from RGB and depth images and generalizes
to doors not encountered in training data. We achieve 80% success across ten
seen and unseen scenes using only ~16.2 hours of teleoperated demonstrations in
sim and real. To the best of our knowledge, this is the first work to tackle
latched door opening from a purely end-to-end learning approach, where the task
of navigation and manipulation are jointly modeled by a single neural network.",smart cities
http://arxiv.org/abs/2201.09419v1,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,"automated machine learning for secure key rate in discrete-modulated
  continuous-variable quantum key distribution",http://arxiv.org/abs/2201.09419v1,"Continuous-variable quantum key distribution (CV QKD) with discrete
modulation has attracted increasing attention due to its experimental
simplicity, lower-cost implementation and compatibility with classical optical
communication. Correspondingly, some novel numerical methods have been proposed
to analyze the security of these protocols against collective attacks, which
promotes key rates over one hundred kilometers of fiber distance. However,
numerical methods are limited by their calculation time and resource
consumption, for which they cannot play more roles on mobile platforms in
quantum networks. To improve this issue, a neural network model predicting key
rates in nearly real time has been proposed previously. Here, we go further and
show a neural network model combined with Bayesian optimization. This model
automatically designs the best architecture of neural network computing key
rates in real time. We demonstrate our model with two variants of CV QKD
protocols with quaternary modulation. The results show high reliability with
secure probability as high as $99.15\%-99.59\%$, considerable tightness and
high efficiency with speedup of approximately $10^7$ in both cases. This
inspiring model enables the real-time computation of unstructured quantum key
distribution protocols' key rate more automatically and efficiently, which has
met the growing needs of implementing QKD protocols on moving platforms.",smart cities
http://arxiv.org/abs/2201.05858v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"smart parking space detection under hazy conditions using convolutional
  neural networks: a novel approach",http://arxiv.org/abs/2201.05858v1,"Limited urban parking space combined with urbanization has necessitated the
development of smart parking systems that can communicate the availability of
parking slots to the end users. Towards this, various deep learning based
solutions using convolutional neural networks have been proposed for parking
space occupation detection. Though these approaches are robust to partial
obstructions and lighting conditions, their performance is found to degrade in
the presence of haze conditions. Looking in this direction, this paper
investigates the use of dehazing networks that improves the performance of
parking space occupancy classifier under hazy conditions. Additionally,
training procedures are proposed for dehazing networks to maximize the
performance of the system on both hazy and non-hazy conditions. The proposed
system is deployable as part of existing smart parking systems where limited
number of cameras are used to monitor hundreds of parking spaces. To validate
our approach, we have developed a custom hazy parking system dataset from
real-world task-driven test set of RESIDE-\b{eta} dataset. The proposed
approach is tested against existing state-of-the-art parking space detectors on
CNRPark-EXT and hazy parking system datasets. Experimental results indicate
that there is a significant accuracy improvement of the proposed approach on
the hazy parking system dataset.",smart cities
http://arxiv.org/abs/2201.05024v2,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"real-time gpu-accelerated machine learning based multiuser detection for
  5g and beyond",http://arxiv.org/abs/2201.05024v2,"Adaptive partial linear beamforming meets the need of 5G and future 6G
applications for high flexibility and adaptability. Choosing an appropriate
tradeoff between conflicting goals opens the recently proposed multiuser (MU)
detection method. Due to their high spatial resolution, nonlinear beamforming
filters can significantly outperform linear approaches in stationary scenarios
with massive connectivity. However, a dramatic decrease in performance can be
expected in high mobility scenarios because they are very susceptible to
changes in the wireless channel. The robustness of linear filters is required,
considering these changes. One way to respond appropriately is to use online
machine learning algorithms. The theory of algorithms based on the adaptive
projected subgradient method (APSM) is rich, and they promise accurate tracking
capabilities in dynamic wireless environments. However, one of the main
challenges comes from the real-time implementation of these algorithms, which
involve projections on time-varying closed convex sets. While the projection
operations are relatively simple, their vast number poses a challenge in
ultralow latency (ULL) applications where latency constraints must be satisfied
in every radio frame. Taking non-orthogonal multiple access (NOMA) systems as
an example, this paper explores the acceleration of APSM-based algorithms
through massive parallelization. The result is a GPU-accelerated real-time
implementation of an orthogonal frequency-division multiplexing (OFDM)-based
transceiver that enables detection latency of less than one millisecond and
therefore complies with the requirements of 5G and beyond. To meet the
stringent physical layer latency requirements, careful co-design of hardware
and software is essential, especially in virtualized wireless systems with
hardware accelerators.",smart cities
http://arxiv.org/abs/2201.04349v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,video intelligence as a component of a global security system,http://arxiv.org/abs/2201.04349v1,"This paper describes the evolution of our research from video analytics to a
global security system with focus on the video surveillance component. Indeed
video surveillance has evolved from a commodity security tool up to the most
efficient way of tracking perpetrators when terrorism hits our modern urban
centers. As number of cameras soars, one could expect the system to leverage
the huge amount of data carried through the video streams to provide fast
access to video evidences, actionable intelligence for monitoring real-time
events and enabling predictive capacities to assist operators in their
surveillance tasks. This research explores a hybrid platform for video
intelligence capture, automated data extraction, supervised Machine Learning
for intelligently assisted urban video surveillance; Extension to other
components of a global security system are discussed. Applying Knowledge
Management principles in this research helps with deep problem understanding
and facilitates the implementation of efficient information and experience
sharing decision support systems providing assistance to people on the field as
well as in operations centers. The originality of this work is also the
creation of ""common"" human-machine and machine to machine language and a
security ontology.",smart cities
http://arxiv.org/abs/2201.03808v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,mobilefaceswap: a lightweight framework for video face swapping,http://arxiv.org/abs/2201.03808v1,"Advanced face swapping methods have achieved appealing results. However, most
of these methods have many parameters and computations, which makes it
challenging to apply them in real-time applications or deploy them on edge
devices like mobile phones. In this work, we propose a lightweight
Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by
dynamically adjusting the model parameters according to the identity
information. In particular, we design an efficient Identity Injection Module
(IIM) by introducing two dynamic neural network techniques, including the
weights prediction and weights modulation. Once the IDN is updated, it can be
applied to swap faces given any target image or video. The presented IDN
contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it
capable for real-time video face swapping on mobile phones. In addition, we
introduce a knowledge distillation-based method for stable training, and a loss
reweighting module is employed to obtain better synthesized results. Finally,
our method achieves comparable results with the teacher models and other
state-of-the-art methods.",smart cities
10.1016/j.rse.2021.112809,preprocessed,Remote Sensing of Environment,scopus,2022-02-01,sciencedirect,methanet – an ai-driven approach to quantifying methane point-source emission from high-resolution 2-d plume imagery,https://api.elsevier.com/content/abstract/scopus_id/85120521703,"Methane is one of the most important anthropogenic greenhouse gases with a significant impact on the Earth's radiation budget and tropospheric background ozone. Despite a well-constrained global budget, quantification of local and regional methane emissions has proven challenging. Recent advancements in airborne remote sensing instruments such as from the next-generation Airborne Visible/Infrared Imaging Spectrometer (AVIRIS-NG) provide 2-D observations of CH4 plume column enhancements at an unprecedented resolution of 1–5 m over large geographic areas. Quantifying an emission rate from observed plumes is a critical step for understanding local emission distributions and prioritizing mitigation efforts. However, there exists no method that can predict emission rates from detected plumes in real-time without ancillary data reliably. In order to predict methane point-source emissions directly from high resolution 2-D plume images without relying on other local measurements such as background wind speeds, we trained a convolutional neural network model called MethaNet. The training data was derived from large eddy simulations of methane plumes and realistic measurement noise over agricultural, desert and urban environments. Our model has a mean absolute percentage error for predicting unseen plumes under 17%, a significant improvement from previous methods that require wind information. Using MethaNet, a validation against a natural gas controlled-release experiment agrees to within the precision error estimate. Our results support the basis for the applicability of using deep learning techniques to quantify CH4 point sources in an automated manner over large geographical areas, not only for present and future airborne field campaigns but also for upcoming space-based observations in this decade.",smart cities
10.1016/j.cageo.2021.105010,preprocessed,Computers and Geosciences,scopus,2022-02-01,sciencedirect,geospatialvr: a web-based virtual reality framework for collaborative environmental simulations,https://api.elsevier.com/content/abstract/scopus_id/85120435814,"This research introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be served on any web platform and accessed via desktop and mobile devices and virtual reality headsets. The framework can generate realistic simulations of desired locations entailing the terrain, elevation model, infrastructures, dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, occupancy, traffic, weather). These layers enable in-situ visualization of useful data to aid public, scientists, officials, and decision-makers in acquiring a bird's eye view of the current, historical, or forecasted condition of a community. The framework incorporates multiuser support to allow different stakeholders to remotely work on the same VR environment and observe other users' actions and 3D positions via avatars in real-time, and thus, presenting the potential to be utilized as a virtual incident command center or a meeting room. GeospatialVR's purpose is to enhance existing web-based cyberinfrastructure systems with the integration of immersive geospatial capabilities to assist the development of next-generation information and decision support systems powered by virtual reality. Finally, several case studies have been developed for flooding, wildfire, transportation, and public safety.",smart cities
10.1016/j.apenergy.2021.118136,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,an online energy management system for ac/dc residential microgrids supported by non-intrusive load monitoring,https://api.elsevier.com/content/abstract/scopus_id/85119066285,"Traditional electric energy systems are experiencing a major revolution and the main drivers of this revolution are green transition and digitalization. In this paper, an advanced system-level EMS is proposed for residential AC/DC microgrids (MGs) by taking advantage of the innovations offered by digitalization. The proposed EMS supports green transition as it is designed for an MG that includes renewable energy sources (RESs), batteries, and electric vehicles. In addition, the electricity consumption behaviors of residential users have been automatically extracted to create a more flexible MG. Deep learning-supported Non-intrusive load monitoring (NILM) algorithm is deployed to analyze and disaggregate the aggregated consumption signal of each household in the MG. A two-level EMS is designed that coordinates both households and MG components using optimization, forecasting, and NILM modules. The proposed system-level EMS has been tested in a laboratory environment in real-time. Experiments are performed considering different optimization periods and the effectiveness of the proposed EMS has been shown for different optimization horizons. Compared to a peak shaving strategy as a benchmark, the proposed EMS for 24-hour horizon provides a 12.36% reduction in the residential MG daily operation cost.",smart cities
10.1016/j.snb.2021.130958,preprocessed,Sensors and Actuators B: Chemical,scopus,2022-01-15,sciencedirect,from air quality sensors to sensor networks: things we need to learn,https://api.elsevier.com/content/abstract/scopus_id/85118500250,"As a potential complement to traditional regulatory instruments, low-cost air quality sensors (LCAQS) can be deployed in dense monitoring networks to provide timely and comprehensive snapshots of pollutant concentrations and their spatial and temporal variability at various scales with relatively less cost and labor. However, a lack of practical guidance and a limited understanding of sensor data quality hinder the widespread application of this emerging technology. We leveraged air quality data collected from state and local monitoring agencies in metropolitan areas of the United States to evaluate how low-cost sensors could be deployed across the U.S. We found that ozone, as a secondary pollutant, is more homogeneous than other pollutants at various scales. PM2.5, CO, and NO2 displayed homogeneities that varied by city, making it challenging to design a uniform network that was suitable across geographies. Our low-cost sensor data in New York City indicated that PM2.5 sensors track well with light-scattering reference methods, particularly at low concentrations. The same phenomenon was also found after thoroughly evaluating sensor evaluation reports from the Air Quality Sensor Performance Evaluation Center (AQ-SPEC). Furthermore, LCAQS data collected during wildfire episodes in Portland, OR show that a real-time (i.e. in situ) machine learning calibration process is a promising approach to address the data quality challenges persisting in LCAQS applications. Our research highlights the urgency and importance of practical guidance for deploying LCAQS.",smart cities
10.1016/j.jsr.2021.12.010,preprocessed,Journal of Safety Research,scopus,2022-01-01,sciencedirect,learning to interpret novel ehmi: the effect of vehicle kinematics and ehmi familiarity on pedestrian’ crossing behavior,https://api.elsevier.com/content/abstract/scopus_id/85123375400,"Introduction: In current urban traffic, pedestrians attempting to cross the road at un-signalized locations are thought to mostly use implicit communication, such as deceleration cues, to interpret a vehicle’s intention to yield. There is less reliance on explicit driver- or vehicle-based messages, such as hand/head movements, or flashing lights/beeping horns. With the impending deployment of Automated Vehicles (AV), especially those at SAE Level 4 and 5, where the driver is no longer in control of the vehicle, there has been a surge in interest in the value of new forms of communication for AVs, for example, via different types of external Human Machine Interfaces (eHMIs). However, there is still much to be understood about how quickly a novel eHMI affects pedestrian crossing decisions, and whether it provides any additional aid, above and beyond implicit/kinematic information from the vehicle. The aim of this between-participant study, funded by the H2020 interACT project, was to investigate how the combination of kinematic information from a vehicle (e.g., Speed and Deceleration), and eHMI designs, play a role in assisting the crossing decision of pedestrians in a cave-based pedestrian simulator. Method: Using an existing, well-recognized, message for yielding (Flashing Headlights - FH) as a benchmark, this study also investigated how quickly a novel eHMI (Slow Pulsing Light Band – SPLB) was learned. To investigate the effect of eHMI visibility on crossing decisions, the distance at which each eHMI was perceivable was also measured. Results: Results showed that, compared to SPLB, the FH led to earlier crossings during vehicle deceleration, especially at lower approaching speeds, and smaller time gaps. However, although FH was visible earlier than SPLB, this visibility does not appear to be the only reason for earlier crossings, with message familiarity thought to play a role. Participants were found to learn the meaning conveyed by FH relatively quickly, crossing around 1 second earlier in its presence (compared to the no eHMI condition), across the three blocks of trials. On the other hand, it took participants at least one block of 12 trials for the new SPLB signal to affect crossing, which only accelerated crossing initiations by around 200 ms, compared to the no eHMI condition. The role of comprehension, long-term exposure, and familiarity of novel messages in this context is therefore important, if AVs are to provide safe, trustworthy communication messages, which will enhance traffic flow and efficiency.",smart cities
10.1016/j.isprsjprs.2021.10.015,preprocessed,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2022-01-01,sciencedirect,changemask: deep multi-task encoder-transformer-decoder architecture for semantic change detection,https://api.elsevier.com/content/abstract/scopus_id/85119995073,"Multi-temporal high spatial resolution earth observation makes it possible to detect complex urban land surface changes, which is a significant and challenging task in remote sensing communities. Previous works mainly focus on binary change detection (BCD) based on modern technologies, e.g., deep fully convolutional network (FCN), whereas the deep network architecture for semantic change detection (SCD) is insufficiently explored in current literature. In this paper, we propose a deep multi-task encoder-transformer-decoder architecture (ChangeMask) designed by exploring two important inductive biases: sematic-change causal relationship and temporal symmetry. ChangeMask decouples the SCD into a temporal-wise semantic segmentation and a BCD, and then integrates these two tasks into a general encoder-transformer-decoder framework. In the encoder part, we design a semantic-aware encoder to model the semantic-change causal relationship. This encoder is only used to learn semantic representation and then learn change representation from semantic representation via a later transformer module. In this way, change representation can constrain semantic representation during training, which introduces a regularization to reduce the risk of overfitting. To learn a robust change representation from semantic representation, we propose a temporal-symmetric transformer (TST) to guarantee temporal symmetry for change representation and keep it discriminative. Based on the above semantic representation and change representation, we adopt simple multi-task decoders to output semantic change map. Benefiting from the differentiable building blocks, ChangeMask can be trained by a multi-task loss function, which significantly simplifies the whole pipeline of applying ChangeMask. The comprehensive experimental results on two large-scale SCD datasets confirm the effectiveness and superiority of ChangeMask in SCD. Besides, to demonstrate the potential value in real-world applications, e.g., automatic urban analysis and decision-making, we deploy the ChangeMask to map a large geographic area covering 30 km2 with 300 million pixels. Code will be made available.",smart cities
10.1016/j.scs.2021.103364,preprocessed,Sustainable Cities and Society,scopus,2022-01-01,sciencedirect,blockchain-enabled secure framework for energy-efficient smart parking in sustainable city environment,https://api.elsevier.com/content/abstract/scopus_id/85117732239,"In the smart city environment, parking vehicle management is an essential requirement for citizens in the current situation because every city is rapidly growing as a crowded environment. Specific planning, operation, and thinking can address this problem with the Internet of things (IoT) and Information Communication Technologies (ICT). Existing research provides various solutions and methods for parking systems in the smart city. However, smart parking has many challenges, such as centralization, communication bandwidth, energy efficiency, integrity, security, and privacy. Inspired by Blockchain and AI technology, we propose a Blockchain-enabled Secure Framework for Energy-Efficient Smart Parking in Sustainable City Environment. The transport layer implements the ECC algorithm to encrypt and decrypt the parking zones data for secure communication. The RSU-based-Blockchain network offers authentication and verification of data at the security layer in a distributed manner. Virtualization technology is used for data storage and provides an energy-efficient environment using virtual machines. With Deep LSTM networks, we analyze the parking zone's data and offer the best parking space to the drivers with the best location and timing. We evaluate the proposed architecture using quantitative, qualitative analysis and provide the driver's best parking space.",smart cities
10.1016/j.apenergy.2021.117853,preprocessed,Applied Energy,scopus,2022-01-01,sciencedirect,transferable representation modelling for real-time energy management of the plug-in hybrid vehicle based on k-fold fuzzy learning and gaussian process regression,https://api.elsevier.com/content/abstract/scopus_id/85114985028,"Electric vehicles, including plug-in hybrids, are important for achieving net-zero emission and will dominate road transportation in the future. Energy management, which optimizes the onboard energy usage, is a critical functionality of electric vehicles. It is usually developed following the model-based routine, which is conventionally costly and time-consuming and is hard to meet the increasing market competition in the digital era. To reduce the development workload for the energy management controller, this paper studies an innovative transfer learning routine. A new transferable representation control model is proposed by incorporating two promising artificial intelligence technologies, adaptive neural fuzzy inference system and Gaussian process regression, where the former applies k-fold cross valudation to build a neural fuzzy system for real-time implementation of offline optimization result, and the later connects the neural fuzzy system with a ‘deeper’ architecture to transfer the offline optimization knowledge learnt at source domain to new target domains. By introducing a concept of control utility that evaluates vehicle energy efficiency with a penalty on usage of battery energy, experimental evaluations based on the hardware-in-the-loop testing platform are conducted. Competitive real-time control ultility values (as much as 90% of offline benchmarking results) can be achieved by the proposed control method. They are over 27% higher than that achieved by the neural-network-based model.",smart cities
10.1109/jsac.2021.3118405,preprocessed,IEEE Journal on Selected Areas in Communications,IEEE,2022-02-01 00:00:00,ieeexplore,"learning-based prediction, rendering and transmission for interactive virtual reality in ris-assisted terahertz networks",https://ieeexplore.ieee.org/document/9565222/,"The quality of experience (QoE) requirements of wireless virtual reality (VR) can only be satisfied with high data rate, high reliability, and low VR interaction latency. This high data rate over short transmission distances may be achieved via the abundant bandwidth in the terahertz (THz) band. However, THz waves experience severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with programmable reflecting elements. Meanwhile, the low VR interaction latency can be achieved with the mobile edge computing (MEC) network architecture due to its computation capabilities. Motivated by these considerations, in this paper, we propose an MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by taking into account the uplink viewpoint prediction and position transmission, the MEC rendering, and the downlink transmission. We propose two methods, which are referred to as centralized online gated recurrent unit (GRU) and distributed federated averaging (FedAvg), to predict the viewpoints of the VR users. In the uplink, an algorithm that integrates online long-short term memory (LSTM) and convolutional neural networks (CNN) is deployed to predict the locations and the line-of-sight and non-line-of-sight statuses of the VR users over time. In the downlink, we develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS under latency constraints. Simulation results show that our proposed learning architecture achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.",multimedia
10.1109/tnsre.2022.3147772,preprocessed,IEEE Transactions on Neural Systems and Rehabilitation Engineering,IEEE,2000-01-01 00:00:00,ieeexplore,improving automatic control of upper-limb prosthesis wrists using gaze-centered eye tracking and deep learning,https://ieeexplore.ieee.org/document/9698069/,"Many upper-limb prostheses lack proper wrist rotation functionality, leading to users performing poor compensatory strategies, leading to overuse or abandonment. In this study, we investigate the validity of creating and implementing a data-driven predictive control strategy in object grasping tasks performed in virtual reality. We propose the idea of using gaze-centered vision to predict the wrist rotations of a user and implement a user study to investigate the impact of using this predictive control. We demonstrate that using this vision-based predictive system leads to a decrease in compensatory movement in the shoulder, as well as task completion time. We discuss the cases in which the virtual prosthesis with the predictive model implemented did and did not make a physical improvement in various arm movements. We also discuss the cognitive value in implementing such predictive control strategies into prosthetic controllers. We find that gaze-centered vision provides information about the intent of the user when performing object reaching and that the performance of prosthetic hands improves greatly when wrist prediction is implemented. Lastly, we address the limitations of this study in the context of both the study itself as well as any future physical implementations.",multimedia
10.1109/tmrb.2021.3129113,preprocessed,IEEE Transactions on Medical Robotics and Bionics,IEEE,2022-02-01 00:00:00,ieeexplore,learning a generic olfactory search strategy from silk moths by deep inverse reinforcement learning,https://ieeexplore.ieee.org/document/9619462/,"Despite their simple nervous systems, insects efficiently search for and find sources of odorants. Hence, it is necessary to model and implement such behavior in artificial agents (robots), to enable them to detect dangerous substances such as drugs, gas leaks, and explosives. Previous studies have approached behavioral modeling with either statistical or machine-learning methods. In this study, we determined the behavior trajectories of male silk moths using a virtual reality (VR) system. We then modeled these trajectories as a Markov decision process (MDP) and employed inverse reinforcement learning (IRL) to learn their reward function. Furthermore, we estimated the optimal policy from the learned reward function. We then conducted olfactory search simulations and determined that the IRL-based policy could locate odor sources with a high success rate. This was also investigated under environmental conditions different from those faced by real moths on the VR system. The obtained results indicate that IRL can generically represent olfactory search strategies that are adaptable to various environments.",multimedia
10.1109/wacvw54805.2022.00069,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),IEEE,2022-01-08 00:00:00,ieeexplore,aa3dnet: attention augmented real time 3d object detection,https://ieeexplore.ieee.org/document/9707544/,"In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Bird’s Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at &gt;30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",multimedia
10.1109/wacv51458.2022.00380,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,fast-clocs: fast camera-lidar object candidates fusion for 3d object detection,https://ieeexplore.ieee.org/document/9706631/,"When compared to single modality approaches, fusion-based object detection methods often require more complex models to integrate heterogeneous sensor data, and use more GPU memory and computational resources. This is particularly true for camera-LiDAR based multimodal fusion, which may require three separate deep-learning networks and/or processing pipelines that are designated for the visual data, LiDAR data, and for some form of a fusion framework. In this paper, we propose Fast Camera-LiDAR Object Candidates (Fast-CLOCs) fusion network that can run high-accuracy fusion-based 3D object detection in near real-time. Fast-CLOCs operates on the output candidates before Non-Maximum Suppression (NMS) of any 3D detector, and adds a lightweight 3D detector-cued 2D image detector (3D-Q-2D) to extract visual features from the image domain to improve 3D detections significantly. The 3D detection candidates are shared with the proposed 3D-Q-2D image detector as proposals to reduce the network complexity drastically. The superior experimental results of our Fast-CLOCs on the challenging KITTI and nuScenes datasets illustrate that our Fast-CLOCs outperforms state-of-the-art fusion-based 3D object detection approaches. We will release the code upon publication.",multimedia
10.1109/wacv51458.2022.00037,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,occlusion resistant network for 3d face reconstruction,https://ieeexplore.ieee.org/document/9706716/,"3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of 0.77 against 5.84 of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of 6.67 in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.",multimedia
10.1109/lra.2022.3142439,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,anytime 3d object reconstruction using multi-modal variational autoencoder,https://ieeexplore.ieee.org/document/9681277/,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository<sup>1</sup>.",multimedia
10.1109/access.2022.3140332,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,spatial-importance-based computation scheme for real-time object detection from 3d sensor data,https://ieeexplore.ieee.org/document/9668921/,"Three-dimensional (3D) sensor networks using multiple light-detection-and-ranging (LIDAR) sensors are good for smart monitoring of spots, such as intersections, with high potential risk of road-traffic accidents. The image sensors must share the strictly limited computation capacity of an edge computer. To have the computation speeds required from real-time applications, the system must have a short computation delay while maintaining the quality of the output, e.g., the accuracy of the object detection. This paper proposes a spatial-importance-based computation scheme that can be implemented on an edge computer of image-sensor networks composed of 3D sensors. The scheme considers regions where objects exist as more likely to be ones of higher spatial importance. It processes point-cloud data from each region according to the spatial importance of that region. By prioritizing regions with high spatial importance, it shortens the computation delay involved in the object detection. A point-cloud dataset obtained by a moving car equipped with a LIDAR unit was used to numerically evaluate the proposed scheme. The results indicate that the scheme shortens the delay in object detection.",multimedia
10.1109/lsp.2022.3144074,preprocessed,IEEE Signal Processing Letters,IEEE,2000-01-01 00:00:00,ieeexplore,rethinking lightweight: multiple angle strategy for efficient video action recognition,https://ieeexplore.ieee.org/document/9684992/,"Video action recognition task involves modeling spatiotemporal information, and efficiency is critical to capture spatiotemporal dependencies in the video. Most existing models rely on optical flow information to capture the dynamic visual tempos between consecutive video frames. Although impressive performance can be achieved by combining optical flow with RGB, the time-consuming nature of optical flow computation cannot be ignored. Moreover, 3D CNN has successfully modeled spatiotemporal information, yet the enormous computational volume is unsuitable for real-time action recognition. In this letter, we propose a novel lightweight video feature extraction strategy that achieves better recognition performance with lower FLOPs. In particular, we perform convolution on the video cube from three orthogonal angles to learn its appearance and motion features. Compared with the computational volume of 3D CNN, our proposed method is more economical and thus meets the lightweight requirements. Extensive experimental results on public Something Something-V1<inline-formula><tex-math notation=""LaTeX"">$\&amp;$</tex-math></inline-formula>V2 and Diving48 datasets show our approach achieves the state-of-the-art performance.",multimedia
10.1109/lra.2022.3147337,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,sim2air - synthetic aerial dataset for uav monitoring,https://ieeexplore.ieee.org/document/9699390/,"In this letter, we propose a novel approach to generate a synthetic aerial dataset for application in UAV monitoring. We propose to accentuate shape-based object representation by applying texture randomization. A diverse dataset with photorealism in all parameters such as shape, pose, lighting, scale, viewpoint, etc. except for atypical textures is created in a 3D modelling software Blender. Our approach specifically targets two conditions in aerial images where texture of objects is difficult to detect, namely challenging illumination and objects occupying only a small portion of the image. Experimental evaluation of YOLO and Faster R-CNN detectors trained on synthetic data with randomized textures confirmed our approach by increasing the mAP value (17 and 3.7 percentage points for YOLO; 20 and 1.1 percentage points for Faster R-CNN) on two test datasets of real images, both containing UAV-to-UAV images with motion blur. Testing on different domains, we conclude that the more the generalisation ability is put to the test, the more apparent are the advantages of the shape-based representation.",multimedia
10.1109/lra.2021.3116700,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments,https://ieeexplore.ieee.org/document/9555228/,"Obstacle avoidance for robotic manipulators can be challenging when they operate in unstructured environments. This problem is probed with the sim-to-real (sim2real) deep reinforcement learning, such that a moving policy of the robotic arm is learnt in a simulator and then adapted to the real world. However, the problem of sim2real adaptation is notoriously difficult. To this end, this work proposes (1) a unified representation of obstacles and targets to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model combining the unified representation with the deep reinforcement learning control module that can be trained by interacting with the environment. Such a representation is agnostic to the shape and appearance of the underlying objects, which simplifies and unifies the scene representation in both simulated and real worlds. We implement this idea with a vision-based actor-critic framework by devising a bounding box predictor module. The predictor estimates the 3D bounding boxes of obstacles and targets from the RGB-D input. The features extracted by the predictor are fed into the policy network, and all the modules are jointly trained. This makes the policy learn object-aware scene representation, which leads to a data-efficient learning of the obstacle avoidance policy. Our experiments in simulated environment and the real-world show that the end-to-end model of the unified representation achieves better sim2real adaption and scene generalization than state-of-the-art techniques.",multimedia
10.1109/jiot.2021.3089080,preprocessed,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,audio-visual autoencoding for privacy-preserving video streaming,https://ieeexplore.ieee.org/document/9453730/,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",multimedia
10.1109/wacv51458.2022.00135,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,detecting tear gas canisters with limited training data,https://ieeexplore.ieee.org/document/9706699/,"Human rights investigations often entail triaging large volumes of open source images and video in order to find moments that are relevant to a given investigation and warrant further inspection. Searching for instances of tear gas usage online manually is laborious and time-consuming. In this paper, we study various object detection models for their potential use in the discovery and identification of tear gas canisters for human rights monitors. CNN based object detection typically requires large volumes of training data, and prior to our work, an appropriate dataset of tear gas canisters did not exist. We benchmark methods for training object detectors using limited labelled data: we fine-tune different object detection models on the limited labelled data and compare performance to a few shot detector and augmentation strategies using synthetic data. We provide a dataset for evaluating and training tear gas canister detectors and indicate how such detectors can be deployed in real-world contexts for investigating human rights violations. Our experiments show that various techniques can improve results, including fine-tuning state of the art detectors, using few shot detectors, and including synthetic data as part of the training set.",multimedia
10.1109/comsnets53615.2022.9668364,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,open-air off-street vehicle parking management system using deep neural networks: a case study,https://ieeexplore.ieee.org/document/9668364/,"Smart parking solution aims to output real-time parking occupancy information. It helps to reduce parking bay search time, traffic, fuel consumption, and thereby vehicular emissions with increased road safety. A computer vision-based solution using camera video data is most reliable and rational since it allows monitoring the entire open-air parking area at once. A real-time parking solution (cloud-based, server processing, or onboard processing) helps bring the occupancy information to the end-user. It comes with many challenges such as viewing angles, lighting conditions, model optimization, reducing inference time, and many more real-world challenges. Hence, this paper presents a case study on real-time open-air off-street intelligent parking management using a deep neural network. Also, most of the earlier research works focus on day-time data and do not discuss the night data. So, in this work, we perform experiments on realtime 24-hour data from an input camera video source mounted to monitor parking at IIT Hyderabad (IITH) parking lot. Our experiments demonstrate the real-world challenges and can help improve parking performance, deployment at IITH, and relevant parking systems in general.",multimedia
10.1109/comsnets53615.2022.9668498,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,tele-driving an electric vehicle over a private lte network,https://ieeexplore.ieee.org/document/9668498/,"We demonstrate tele-driving operation for an electric vehicle capable of stopping itself in case of system failure over a captive LTE network deployed in a university campus. Our electronically controlled vehicle is driven remotely by an operator from a control room which receives the multi-camera real-time video feed from the vehicle over this network. Our primary contribution includes the responsive emergency braking mechanism for the vehicle, modular vehicle design based on CAN bus, low latency LTE MAC scheduler design, and modifications to popular video tool, FFMPEG to support low latency real time video streaming. Our demonstration shows complete integration of the different components, i.e., the vehicle, the LTE network and the remote driving application. Another salient feature of our system is the O-RAN compliant RAN awareness module and KPI (Key Performance Indicator) application which enables real-time network performance monitoring.",multimedia
10.1109/access.2022.3147519,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a deep learning-based approach for inappropriate content detection and classification of youtube videos,https://ieeexplore.ieee.org/document/9696242/,"The exponential growth of videos on YouTube has attracted billions of viewers among which the majority belongs to a young demographic. Malicious uploaders also find this platform as an opportunity to spread upsetting visual content, such as using animated cartoon videos to share inappropriate content with children. Therefore, an automatic real-time video content filtering mechanism is highly suggested to be integrated into social media platforms. In this study, a novel deep learning-based architecture is proposed for the detection and classification of inappropriate content in videos. For this, the proposed framework employs an ImageNet pre-trained convolutional neural network (CNN) model known as EfficientNet-B7 to extract video descriptors, which are then fed to bidirectional long short-term memory (BiLSTM) network to learn effective video representations and perform multiclass video classification. An attention mechanism is also integrated after BiLSTM to apply attention probability distribution in the network. These models are evaluated on a manually annotated dataset of 111,156 cartoon clips collected from YouTube videos. Experimental results demonstrated that EfficientNet-BiLSTM (accuracy = 95.66%) performs better than attention mechanism-based EfficientNet-BiLSTM (accuracy = 95.30%) framework. Secondly, the traditional machine learning classifiers perform relatively poor than deep learning classifiers. Overall, the architecture of EfficientNet and BiLSTM with 128 hidden units yielded state-of-the-art performance (f1 score = 0.9267). Furthermore, the performance comparison against existing state-of-the-art approaches verified that BiLSTM on top of CNN captures better contextual information of video descriptors in network architecture, and hence achieved better results in child inappropriate video content detection and classification.",multimedia
10.1109/tmm.2021.3050086,preprocessed,IEEE Transactions on Multimedia,IEEE,2000-01-01 00:00:00,ieeexplore,improving robustness of dash against unpredictable network variations,https://ieeexplore.ieee.org/document/9317786/,"Most video players use adaptive bitrate (ABR) algorithms to provide good quality-of-experience (QoE) in dynamic network conditions. To deal with the adaptation challenges, many ABR algorithms select bitrate by optimizing a defined QoE function. Within the framework, various algorithms mainly differ in how the optimization problem is solved, including prediction-based approaches and learn-based approaches. However, these algorithms suffer from limited performance in the current popular mobile streaming which has limited resources and rapidly changing link rates. Existing machine-learning approaches face deployment difficulties on mobile devices, and prediction-based approaches that rely on throughput prediction experience large buffer occupancy variations in cellular networks, resulting in rebuffering frequently. To provide a robust and lightweight ABR algorithm for mobile streaming, this work improves the robustness of prediction-based scheme against unpredictable network variations and develops RBC (Robust Bitrate Controller) algorithm. Rather than optimizing QoE over the entire buffer capacity, RBC creates buffer margins to absorb the impact of throughput jitters and solves QoE maximization on the narrowed buffer range. The amount of buffer margin is dynamically adjusted based on the real-time throughput fluctuation to ensure sufficient de-jitter space. For online lightweight deployment, RBC provides a closed-form solution of the desired bitrate with small computation complexity by using adaptive control approach. Trace-driven experiments and real-world tests show that RBC effectively reduces the playback freezing and gains an improvement in overall QoE.",multimedia
10.1109/tcsvt.2021.3066675,preprocessed,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,2022-02-01 00:00:00,ieeexplore,spatio-temporal online matrix factorization for multi-scale moving objects detection,https://ieeexplore.ieee.org/document/9380454/,"Detecting moving objects from the video sequences has been treated as a challenging computer vision task, since the problems of dynamic background, multi-scale moving objects and various noise interference impact the corresponding feasibility and efficiency. In this paper, a novel spatio-temporal online matrix factorization (STOMF) method is proposed to detect multi-scale moving objects under dynamic background. To accommodate a wide range of the real noise distractions, we apply a specific mixture of exponential power (MoEP) distributions to the framework of low-rank matrix factorization (LRMF). For the optimization of solution algorithm, a temporal difference motion prior (TDMP) model is proposed, which estimates the motion matrix and calculates the weight matrix. Moreover, a partial spatial motion information (PSMI) post-processing method is further designed to implement multi-scale objects extraction in varieties of complex dynamic scenes, which utilizes partial background and motion information. The superiority of the STOMF method is validated by massive experiments on practical datasets, as compared with state-of-the-art moving objects detection approaches.",multimedia
10.1109/iccece54139.2022.9712814,preprocessed,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),IEEE,2022-01-16 00:00:00,ieeexplore,a lightweight sar image recognition algorithm based on deep convolutional neural network,https://ieeexplore.ieee.org/document/9712814/,"Synthetic Aperture Radar (SAR) can provide large-scale, all-time, all-weather imaging and therefore plays an important role in both military reconnaissance and battlefield perception. In recent years, due to the outstanding performance of deep convolutional neural networks (CNNs) on image recognition, the approaches that apply CNNs to SAR target recognition has attracted widespread attention among domestic and foreign scholars. The CNNs can achieve high accuracy, yet they often contain huge number of parameters and occupy too much memory to be deployed on devices with memory constraint. In this work, we build light-weight SAR image recognition models by performing iterative pruning and retraining on three representative architectures. The algorithm can lead to a 50% reduction in the number of parameters, while still obtaining a high accuracy of 98.5%. Our algorithm and results provide a potential direction for building light-weight SAR image model that can be deployed for real-world applications.",multimedia
10.1109/tim.2021.3132332,preprocessed,IEEE Transactions on Instrumentation and Measurement,IEEE,2000-01-01 00:00:00,ieeexplore,finger vein recognition algorithm based on lightweight deep convolutional neural network,https://ieeexplore.ieee.org/document/9633979/,"Even though the deep neural networks have strong feature representation capability and high recognition accuracy in finger vein recognition, the deep models are computationally intensive and poor in timeliness. To address these issues, this article proposes a lightweight algorithm for finger vein image recognition and matching. The proposed algorithm uses a lightweight convolutional model in the backbone network and employs a triplet loss function to train the model, which not only improves the matching accuracy, but also satisfies the real-time matching requirements. In addition, the Mini-region of interest (RoI) and finger vein pattern feature extraction also effectively solve the problems of large amounts of calculation and background noise. Moreover, the present model recognizes new categories based on the feature vector space constructed by the finger vein recognition system, so that new categories can be recognized without retraining the model. The results show that the finger vein recognition and matching algorithm proposed in this article achieves 99.3% and 99.6% in recognition accuracy and 14.2 and 16.5 ms in matching time for the dataset Shandong University Machine Learning and Applications Laboratory-Homologous Multimodal Biometric Traits (SDUMLA-HMT) and Peking University Finger Vein Dataset (PKU-FVD), respectively. These metrics show that our approach is time-saving and more effective than previous algorithms. Compared with the state-of-the-art finger vein recognition algorithm, the proposed algorithm improves 1.45% in recognition accuracy while saving 45.7% in recognition time.",multimedia
10.1109/taslp.2021.3129994,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,soundstream: an end-to-end neural audio codec,https://ieeexplore.ieee.org/document/9625818/,"We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.",multimedia
10.1109/access.2021.3139537,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"automatic adaptation of open educational resources: an approach from a multilevel methodology based on students’ preferences, educational special needs, artificial intelligence and accessibility metadata",https://ieeexplore.ieee.org/document/9669174/,"The need for adaptive e-learning environments that respond to learning variability is now a fundamental requirement in education, as it helps to ensure that students learn and pass their courses within a set time frame. Although guidelines, techniques and methods have been established in recent years to contribute to the development of accessible and adaptable e-learning environments that promote digital inclusion, their implementation is challenging due to the lack of knowledge of an adequate way to do it and because it is considered more of a technological competence for scholars in the area. In this context, automated support for adapting material that responds to the correct use of accessibility metadata not only provides a way to improve the description of adapted educational resources, but also facilitates their search according to the needs and preferences of students, particularly those with disabilities. In this article, we carry out a multilevel methodological proposal for the automatic adaptation of open educational resources, in order to provide a tool that contributes to the accessibility and correct use of their metadata in e-learning environments. A research is conducted with students with disabilities to establish their real needs and preferences, highlighting the need to strengthen the adequate description and coherent alternative text in images, the correct subtitling in videos and the conversion of audio to text, data that are relevant to our proposal. The research conducted aims to contribute with an automated support tool in the generation of accessible educational resources that are correctly labeled for search and reuse. This research also aims to support researchers in artificial intelligence applications to address challenges and opportunities in the field of virtual education, in addition to providing an overview that could help those who generate educational resources and maintain their interest in making them accessible.",multimedia
10.1109/access.2022.3140901,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a two-stage deep neuroevolutionary technique for self-adaptive speech enhancement,https://ieeexplore.ieee.org/document/9672141/,"This paper presents a novel self-adaptive approach for speech enhancement in the context of highly nonstationary noise. A two-stage deep neuroevolutionary technique for speech enhancement is proposed. The first stage is composed of a deep neural network (DNN) method for speech enhancement. Two DNN methods were tested at this stage, namely, both a deep complex convolution recurrent network (DCCRN) and a residual long short-term memory neural network (ResLSTM). The ResLSTM method was combined with a minimum mean-square error method to perform a preliminary enhancement. The ResLSTM network is used as an <italic>a priori</italic> signal-to-noise ratio (SNR) estimator. The second stage implements a self-adaptive multiband spectral subtraction enhancement method using tuning optimization based on a genetic algorithm. The proposed two-stage technique is evaluated using objective measures of speech quality and intelligibility. The experiments are carried out using the NOIZEUS noisy speech corpus using conditions of real-world stationary, colored, and nonstationary noise sources at multiple SNR levels. These experiments demonstrate the advantage of building a cooperative approach using evolutionary and deep learning-based techniques that are capable of achieving robust speech enhancement in adverse conditions. Indeed, the experimental tests show that the proposed two-stage technique outperformed a baseline implementation using a state-of-the-art deep learning approach by an average 13% and 6% improvement for six noise conditions at a −5 dB and a 0 dB input SNR, respectively.",multimedia
10.1109/taslp.2021.3126947,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,end-to-end neural based modification of noisy speech for speech-in-noise intelligibility improvement,https://ieeexplore.ieee.org/document/9611022/,"Intelligibility of speech can be significantly reduced when it is presented in adverse near-end listening conditions, like background noise. Multiple approaches have been suggested to improve the perception of speech in such conditions. However, most of these approaches were designed to work with clean input speech. Therefore, they have serious limitations when deployed in real world applications like telephony and hearing aids, where noisy input speech is quite common. In this paper we present an end-to-end neural network approach for the above problem, which effectively reduces the input noise and improves the intelligibility for listeners in adverse conditions. To that end, a convolutional neural network topology with variable dilation factors is proposed and evaluated both in a causal and a non-causal configuration using raw speech as input. A Teacher-Student training strategy is employed, where the Teacher is a well-established speech-in-noise intelligibility enhancer based on spectral shaping followed by dynamic range compression (SSDRC). The evaluation is performed both objectively using the speech intelligibility in bits metric (SIIB), and subjectively on the Greek Harvard corpus. A noise robust multi-band version of SSDRC was used as a baseline. Compared with the baseline, at 0 dB input SNR, the suggested neural network system achieved about 380% and 230% relative SIIB improvements in fluctuating and stationary backgrounds, respectively. Subjectively, the suggested model increased listeners’ keyword correct rate in stationary noise from 25% to 60% at 0 dB input SNR, and from about 52% to 75% at 5 dB input SNR, compared with the baseline.",multimedia
10.1109/taslp.2021.3133190,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,vace-wpe: virtual acoustic channel expansion based on neural networks for weighted prediction error-based speech dereverberation,https://ieeexplore.ieee.org/document/9640471/,"Speech dereverberation is an important issue for many real-world speech processing applications. Among the techniques developed, the weighted prediction error (WPE) algorithm has been widely adopted and advanced over the last decade, which blindly cancels out the late reverberation component from the reverberant mixture of microphone signals. In this study, we extend the neural-network-based virtual acoustic channel expansion (VACE) framework for the WPE-based speech dereverberation, a variant of the WPE that we recently proposed to enable the use of dual-channel WPE algorithm in a single-microphone speech dereverberation scenario. Based on the previous study, some ablation studies are conducted regarding the constituents of the VACE-WPE in an offline processing scenario. These studies reveal the characteristics of the system, thereby simplifying the architecture and leading to the introduction of new strategies for training the neural network for the VACE. Experimental results demonstrate that VACE-WPE (our PyTorch implementation and pre-trained models are available from <uri>https://github.com/dreadbird06/vace_wpe</uri>) considerably outperforms its single-channel counterpart in simulated noisy reverberant environments in terms of objective speech quality and is superior to the single-channel WPE as well as several fully neural speech dereverberation methods when employed as the front-end for the far-field automatic speech recognizer.",multimedia
10.1007/978-3-030-85365-5_17,preprocessed,"Advances in Deep Learning, Artificial Intelligence and Robotics",Springer,2022-01-01 00:00:00,springer,robust model for rural education using deep learning and robotics,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85365-5_17,"Rural Education is important for the overall development of villages. National Achievement Survey (NAS) has surveyed and reported in many States of India consistent decline in the learning levels of students in mathematics, language and science from class III to class VIII studying in the government school system. Smart Villages is only possible if the literacy level and infrastructure improves considerably. This paper aims to perform a reality check of the situation by comparing different rural areas of various countries including India and study of related work done. The paper proposes a Robust Model for Rural Education by developing an intelligent humanoid robot using the Deep Learning approach, Human recognition, Object Recognition and Speech Recognition. The data set consists of Primary and Secondary Student data of around 10,000 Students (5 years) from 5 villages. The Proposed Model would be compared with existing models on the parameters of Learnability, Decision making, Flexibility and Cost-effectiveness. The implementation of this Model will help in decreasing the drop out rate, evaluate Students and give them a Learning platform based on their characteristics, increase adaptive and self paced learning. This Model can also be executed for Rural Adult Education and Skill building so that the Smart Village concept can become a reality.",multimedia
10.1007/s00530-021-00881-8,preprocessed,Multimedia Systems,Springer,2022-01-29 00:00:00,springer,an object detection-based few-shot learning approach for multimedia quality assessment,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00881-8,"A large portion of the global population generates various multimedia data such as texts, images, videos, etc. One of the most common categories which influences the public at large is visual multimedia content. Due to the different social media platforms (e.g., Whatsapp, Twitter, Facebook, Instagram, and YouTube), these materials are passed without censorship and national boundaries. Multimedia data containing any violent or vulgar objects could trigger public unrest, and thus, it is a serious threat to the law and order of the land. Children and teenagers use social media like never before in previous generations and create lots of multimedia data. It is important to assess the quality of multimedia content without any bias and prejudices. Although the mainstream social media platforms use different filters and moderation using human experts, it is impossible to verify the terabytes of uploaded images and videos. Thus, it is inevitable to automate the content assessment phase without incurring an increase in upload time. This study aims to prevent uploading or to tag an image/video with a reasonable percentage of a gun as content. In this paper, object detection architectures such as Faster RCNN, EfficientDet, and YOLOv5 have been used to demonstrate how these techniques can efficiently detect human faces and different types of guns in given multimedia data (images/videos). The models are tested on various test images and video clips. A comparative analysis has also been discussed based on mean average precision and frames per second metric. The YOLOv5 provides the best-performing results as high as 80.39% and 35.22% at $$\text{mAP}_{0.5}$$ mAP 0.5 and $$\text{mAP}_{[0.50:0.95]}$$ mAP [ 0.50 : 0.95 ] , respectively. A face recognition task requires thousands of samples and the usual deep learning models are data-driven. On the contrary, a few-shot learning approach has been implemented to recognize the detected faces categorizing the content as real or reel.",multimedia
10.1007/s00371-021-02347-4,preprocessed,The Visual Computer,Springer,2022-01-13 00:00:00,springer,a detailed analysis of image and video forgery detection techniques,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02347-4,"With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site’s or can be as malicious as to defame or hurt one’s reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one’s identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery.",multimedia
10.1007/978-3-030-92127-9_68,preprocessed,"11th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions and Artificial Intelligence - ICSCCW-2021",Springer,2022-01-01 00:00:00,springer,application of digital twin theory for improvement of natural gas treatment unit,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92127-9_68,"This paper describes fundamental principles of Digital Twins theory and provides exact investigation results in application of Digital Twins theory in upstream branch of oil and gas industry, namely based on example of natural gas treatment plant’s performance increase. As one of key process units of natural gas treatment which allows to implement most powerful functions of digital twin gas sweetening unit with set of membranes is considered as object of investigation. On the base of membrane technology manipulated variables are defined as inputs to digital twin model. Some theoretical results as well as real references of model’s engine calculations are reflected in the paper. Details of technical dashboards to visualize calculated results of running model based on manipulated variables are presented including monthly key performance indicators report dashboard, process flow diagram dashboard and high-level management dashboard. Paper also demonstrates data flow between digital twin model and real process unit and also inside digital twin model.",multimedia
10.1007/978-3-030-93564-1_38,preprocessed,7th International Conference on Advancements of Medicine and Health Care through Technology,Springer,2022-01-01 00:00:00,springer,programing a robotic ambulance with virtual reality,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93564-1_38,"By applying artificial intelligence and virtual reality, this study presents results and challenges for robotic ambulances. Programming vehicle dynamics and testing protocol is intended to support task of developing an autonomous ambulance and engineering efforts. Testing parameters are controlled with automated driver. Using software will diminish human input in driving. Actual values of robotic ambulance in testing are displacement, speed, and acceleration. Ambulance’s velocity in testing on a virtual track with corners and straight lines is an important kinematic parameter that influences safety of transport and some program sections. Accelerations are also important to be programmed. Objective of this paper is to highlight sequences of programming robotic ambulance using virtual reality and artificial intelligence. Results are consisting in testing scenarios, ambulance automated driving program on virtual track, refined program code, solutions for challenges.",multimedia
10.1007/978-3-030-95405-5_9,preprocessed,Advanced Data Mining and Applications,Springer,2022-01-01 00:00:00,springer,smart online exam proctoring assist for cheating detection,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95405-5_9,"Online exams are the most preferred mode of exams in online learning environment. This mode of exam has been even more prevalent and a necessity in the event of a forced closure of face-to-face teaching such as the recent Covid-19 pandemic. Naturally, conducting online exams poses much greater challenge to preserving academic integrity compared to conducting on-site face-to-face exams. As there is no human proctor for policing the examinee on site, the chances of cheating are high. Various online exam proctoring tools are being used by educational institutes worldwide, which offer different solutions to reduce the chances of cheating. The most common technique followed by these tools is recording of video and audio of the examinee during the whole duration of exam. These videos can be analyzed later by human examiner to detect possible cheating case. However, viewing hours of exam videos for each student can be impractical for a large class and thus detecting cheating would be next to impossible. Although some AI-based tools are being used by some proctoring software to raise flags, they are not always very useful. In this paper we propose a cheating detection technique that analyzes an exam video to extract four types of event data, which are then fed to a pre-trained classification model for detecting cheating activity. We formulate the cheating detection problem as a multivariate time-series classification problem by transforming each video into a multivariate time-series representing the time-varying event data extracted from each frame of the video. We have developed a real dataset of cheating videos and conduct extensive experiments with varying video lengths, different deep learning and traditional machine learning models and feature sets, achieving prediction accuracy as high as 97.7%.",multimedia
10.1007/978-981-16-5689-7_12,preprocessed,Advances in Data and Information Sciences,Springer,2022-01-01 00:00:00,springer,applications of high dimensional neural networks: a survey,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5689-7_12,"The evolution of artificial neural networks has always been inspired by enormous power of human brain. This survey can be an eye-opener for researchers as its diverse applications of HDNNs in present scenario shows an intelligent way to mimic human brain without creating a complex neuronal architecture having large number of layers. HDNN’s urgency is evident because in science many quantities are measured not by single values for each of them but a group of values defines 1 single unit as: signal has two values: amplitude and phase. The deficiency in present literature on the questions allied with HDNN application looks like sluggish down research focal point and growth in the area. Hence, there exists a call for state-of-the-art addressing high-dimensional problems in neural networks. The study equips readers with a lucid acquaintance of the existing and novel inclination in HDNN replicas. A lot of applications of HDNNs in various disciplines like: healthcare, climate, security, speech recognition, computer vision, music signal processing, production, stock, science, etc. are covered here to confirm advancement in HDNNs. Study divulges that HDNNs is prevalently known as: CVNN, QVNN, 3D VVNN, and OVNN. This paper also reveals that HDNNs have outperformed real valued neural networks in terms of resource utilization, training data set requirement, and accuracy of results. To see a comparative picture of significance and possible implementation of different HDNNs few charts are provided. A motivational message and suggestions for future researches in this area of High-Dimensionality will conclude this paper.",multimedia
http://arxiv.org/abs/2202.08227v1,preprocessed,arxiv,arxiv,2022-02-16 00:00:00,arxiv,ditto: building digital twins of articulated objects from interaction,http://arxiv.org/abs/2202.08227v1,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",multimedia
http://arxiv.org/abs/2202.06483v2,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,bifsmn: binary neural network for keyword spotting,http://arxiv.org/abs/2202.06483v2,"The deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications. However, computational resources for these
networks are significantly constrained since they usually run on-call on edge
devices. In this paper, we present BiFSMN, an accurate and extreme-efficient
binary neural network for KWS. We first construct a High-frequency Enhancement
Distillation scheme for the binarization-aware training, which emphasizes the
high-frequency information from the full-precision network's representation
that is more crucial for the optimization of the binarized network. Then, to
allow the instant and adaptive accuracy-efficiency trade-offs at runtime, we
also propose a Thinnable Binarization Architecture to further liberate the
acceleration potential of the binarized network from the topology perspective.
Moreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8
devices which fully utilizes registers and increases instruction throughput to
push the limit of deployment efficiency. Extensive experiments show that BiFSMN
outperforms existing binarization methods by convincing margins on various
datasets and is even comparable with the full-precision counterpart (e.g., less
than 3% drop on Speech Commands V1-12). We highlight that benefiting from the
thinnable architecture and the optimized 1-bit implementation, BiFSMN can
achieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge
hardware.",multimedia
http://arxiv.org/abs/2202.05940v1,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,automatic curriculum generation for learning adaptation in networking,http://arxiv.org/abs/2202.05940v1,"As deep reinforcement learning (RL) showcases its strengths in networking and
systems, its pitfalls also come to the public's attention--when trained to
handle a wide range of network workloads and previously unseen deployment
environments, RL policies often manifest suboptimal performance and poor
generalizability.
  To tackle these problems, we present Genet, a new training framework for
learning better RL-based network adaptation algorithms. Genet is built on the
concept of curriculum learning, which has proved effective against similar
issues in other domains where RL is extensively employed. At a high level,
curriculum learning gradually presents more difficult environments to the
training, rather than choosing them randomly, so that the current RL model can
make meaningful progress in training. However, applying curriculum learning in
networking is challenging because it remains unknown how to measure the
""difficulty"" of a network environment.
  Instead of relying on handcrafted heuristics to determine the environment's
difficulty level, our insight is to utilize traditional rule-based (non-RL)
baselines: If the current RL model performs significantly worse in a network
environment than the baselines, then the model's potential to improve when
further trained in this environment is substantial. Therefore, Genet
automatically searches for the environments where the current model falls
significantly behind a traditional baseline scheme and iteratively promotes
these environments as the training progresses. Through evaluating Genet on
three use cases--adaptive video streaming, congestion control, and load
balancing, we show that Genet produces RL policies which outperform both
regularly trained RL policies and traditional baselines in each context, not
only under synthetic workloads but also in real environments.",multimedia
http://arxiv.org/abs/2202.05811v1,preprocessed,arxiv,arxiv,2022-02-11 00:00:00,arxiv,overhead image factors for underwater sonar-based slam,http://arxiv.org/abs/2202.05811v1,"Simultaneous localization and mapping (SLAM) is a critical capability for any
autonomous underwater vehicle (AUV). However, robust, accurate state estimation
is still a work in progress when using low-cost sensors. We propose enhancing a
typical low-cost sensor package using widely available and often free prior
information; overhead imagery. Given an AUV's sonar image and a partially
overlapping, globally-referenced overhead image, we propose using a
convolutional neural network (CNN) to generate a synthetic overhead image
predicting the above-surface appearance of the sonar image contents. We then
use this synthetic overhead image to register our observations to the provided
global overhead image. Once registered, the transformation is introduced as a
factor into a pose SLAM factor graph. We use a state-of-the-art simulation
environment to perform validation over a series of benchmark trajectories and
quantitatively show the improved accuracy of robot state estimation using the
proposed approach. We also show qualitative outcomes from a real AUV field
deployment. Video attachment: https://youtu.be/_uWljtp58ks",multimedia
http://arxiv.org/abs/2202.04971v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"asrpu: a programmable accelerator for low-power automatic speech
  recognition",http://arxiv.org/abs/2202.04971v1,"The outstanding accuracy achieved by modern Automatic Speech Recognition
(ASR) systems is enabling them to quickly become a mainstream technology. ASR
is essential for many applications, such as speech-based assistants, dictation
systems and real-time language translation. However, highly accurate ASR
systems are computationally expensive, requiring on the order of billions of
arithmetic operations to decode each second of audio, which conflicts with a
growing interest in deploying ASR on edge devices. On these devices, hardware
acceleration is key for achieving acceptable performance. However, ASR is a
rich and fast-changing field, and thus, any overly specialized hardware
accelerator may quickly become obsolete.
  In this paper, we tackle those challenges by proposing ASRPU, a programmable
accelerator for on-edge ASR. ASRPU contains a pool of general-purpose cores
that execute small pieces of parallel code. Each of these programs computes one
part of the overall decoder (e.g. a layer in a neural network). The accelerator
automates some carefully chosen parts of the decoder to simplify the
programming without sacrificing generality. We provide an analysis of a modern
ASR system implemented on ASRPU and show that this architecture can achieve
real-time decoding with a very low power budget.",multimedia
http://arxiv.org/abs/2202.02656v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,a survey of top-down approaches for human pose estimation,http://arxiv.org/abs/2202.02656v1,"Human pose estimation in two-dimensional images videos has been a hot topic
in the computer vision problem recently due to its vast benefits and potential
applications for improving human life, such as behaviors recognition, motion
capture and augmented reality, training robots, and movement tracking. Many
state-of-the-art methods implemented with Deep Learning have addressed several
challenges and brought tremendous remarkable results in the field of human pose
estimation. Approaches are classified into two kinds: the two-step framework
(top-down approach) and the part-based framework (bottom-up approach). While
the two-step framework first incorporates a person detector and then estimates
the pose within each box independently, detecting all body parts in the image
and associating parts belonging to distinct persons is conducted in the
part-based framework. This paper aims to provide newcomers with an extensive
review of deep learning methods-based 2D images for recognizing the pose of
people, which only focuses on top-down approaches since 2016. The discussion
through this paper presents significant detectors and estimators depending on
mathematical background, the challenges and limitations, benchmark datasets,
evaluation metrics, and comparison between methods.",multimedia
http://arxiv.org/abs/2201.10910v1,preprocessed,arxiv,arxiv,2022-01-26 00:00:00,arxiv,"a bayesian based deep unrolling algorithm for single-photon lidar
  systems",http://arxiv.org/abs/2201.10910v1,"Deploying 3D single-photon Lidar imaging in real world applications faces
multiple challenges including imaging in high noise environments. Several
algorithms have been proposed to address these issues based on statistical or
learning-based frameworks. Statistical methods provide rich information about
the inferred parameters but are limited by the assumed model correlation
structures, while deep learning methods show state-of-the-art performance but
limited inference guarantees, preventing their extended use in critical
applications. This paper unrolls a statistical Bayesian algorithm into a new
deep learning architecture for robust image reconstruction from single-photon
Lidar data, i.e., the algorithm's iterative steps are converted into neural
network layers. The resulting algorithm benefits from the advantages of both
statistical and learning based frameworks, providing best estimates with
improved network interpretability. Compared to existing learning-based
solutions, the proposed architecture requires a reduced number of trainable
parameters, is more robust to noise and mismodelling effects, and provides
richer information about the estimates including uncertainty measures. Results
on synthetic and real data show competitive results regarding the quality of
the inference and computational complexity when compared to state-of-the-art
algorithms.",multimedia
http://arxiv.org/abs/2201.10369v1,preprocessed,arxiv,arxiv,2022-01-25 00:00:00,arxiv,winograd convolution for deep neural networks: efficient point selection,http://arxiv.org/abs/2201.10369v1,"Convolutional neural networks (CNNs) have dramatically improved the accuracy
of tasks such as object recognition, image segmentation and interactive speech
systems. CNNs require large amounts of computing resources because
ofcomputationally intensive convolution layers. Fast convolution algorithms
such as Winograd convolution can greatly reduce the computational cost of these
layers at a cost of poor numeric properties, such that greater savings in
computation exponentially increase floating point errors.
  A defining feature of each Winograd convolution algorithm is a set of
real-value points where polynomials are sampled. The choice of points impacts
the numeric accuracy of the algorithm, but the optimal set of points for small
convolutions remains unknown. Existing work considers only small integers and
simple fractions as candidate points. In this work, we propose a novel approach
to point selection using points of the form {-1/c , -c, c, 1/c } using the full
range of real-valued numbers for c. We show that groups of this form cause
cancellations in the Winograd transform matrices that reduce numeric error. We
find empirically that the error for different values of c forms a rough curve
across the range of real-value numbers helping to localize the values of c that
reduce error and that lower errors can be achieved with non-obvious real-valued
evaluation points instead of integers or simple fractions. We study a range of
sizes for small convolutions and achieve reduction in error ranging from 2% to
around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in
cases when we select a subset of our proposed points which will always lead to
a lower error. Finally we implement a complete Winograd convolution layer and
use it to run deep convolution neural networks on real datasets and show that
our proposed points reduce error, ranging from 22% to 63%.",multimedia
http://arxiv.org/abs/2201.09550v1,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,crowd tracking and monitoring middleware via map-reduce,http://arxiv.org/abs/2201.09550v1,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques.",multimedia
http://arxiv.org/abs/2201.10947v1,preprocessed,arxiv,arxiv,2022-01-22 00:00:00,arxiv,"enabling deep learning on edge devices through filter pruning and
  knowledge transfer",http://arxiv.org/abs/2201.10947v1,"Deep learning models have introduced various intelligent applications to edge
devices, such as image classification, speech recognition, and augmented
reality. There is an increasing need of training such models on the devices in
order to deliver personalized, responsive, and private learning. To address
this need, this paper presents a new solution for deploying and training
state-of-the-art models on the resource-constrained devices. First, the paper
proposes a novel filter-pruning-based model compression method to create
lightweight trainable models from large models trained in the cloud, without
much loss of accuracy. Second, it proposes a novel knowledge transfer method to
enable the on-device model to update incrementally in real time or near real
time using incremental learning on new data and enable the on-device model to
learn the unseen categories with the help of the in-cloud model in an
unsupervised fashion. The results show that 1) our model compression method can
remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy
of over 90% on CIFAR-10; 2) our knowledge transfer method enables the
compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good
accuracy on old categories; 3) it allows the compressed models to converge
within real time (three to six minutes) on the edge for incremental learning
tasks; 4) it enables the model to classify unseen categories of data (78.92%
Top-1 accuracy) that it is never trained with.",multimedia
http://arxiv.org/abs/2201.08619v1,preprocessed,arxiv,arxiv,2022-01-21 00:00:00,arxiv,"dangerous cloaking: natural trigger based backdoor attacks on object
  detectors in the physical world",http://arxiv.org/abs/2201.08619v1,"Deep learning models have been shown to be vulnerable to recent backdoor
attacks. A backdoored model behaves normally for inputs containing no
attacker-secretly-chosen trigger and maliciously for inputs with the trigger.
To date, backdoor attacks and countermeasures mainly focus on image
classification tasks. And most of them are implemented in the digital world
with digital triggers. Besides the classification tasks, object detection
systems are also considered as one of the basic foundations of computer vision
tasks. However, there is no investigation and understanding of the backdoor
vulnerability of the object detector, even in the digital world with digital
triggers. For the first time, this work demonstrates that existing object
detectors are inherently susceptible to physical backdoor attacks. We use a
natural T-shirt bought from a market as a trigger to enable the cloaking
effect--the person bounding-box disappears in front of the object detector. We
show that such a backdoor can be implanted from two exploitable attack
scenarios into the object detector, which is outsourced or fine-tuned through a
pretrained model. We have extensively evaluated three popular object detection
algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building
upon 19 videos shot in real-world scenes, we confirm that the backdoor attack
is robust against various factors: movement, distance, angle, non-rigid
deformation, and lighting. Specifically, the attack success rate (ASR) in most
videos is 100% or close to it, while the clean data accuracy of the backdoored
model is the same as its clean counterpart. The latter implies that it is
infeasible to detect the backdoor behavior merely through a validation set. The
averaged ASR still remains sufficiently high to be 78% in the transfer learning
attack scenarios evaluated on CenterNet. See the demo video on
https://youtu.be/Q3HOF4OobbY.",multimedia
http://arxiv.org/abs/2201.08197v1,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,"enhancement or super-resolution: learning-based adaptive video streaming
  with client-side video processing",http://arxiv.org/abs/2201.08197v1,"The rapid development of multimedia and communication technology has resulted
in an urgent need for high-quality video streaming. However, robust video
streaming under fluctuating network conditions and heterogeneous client
computing capabilities remains a challenge. In this paper, we consider an
enhancement-enabled video streaming network under a time-varying wireless
network and limited computation capacity. ""Enhancement"" means that the client
can improve the quality of the downloaded video segments via image processing
modules. We aim to design a joint bitrate adaptation and client-side
enhancement algorithm toward maximizing the quality of experience (QoE). We
formulate the problem as a Markov decision process (MDP) and propose a deep
reinforcement learning (DRL)-based framework, named ENAVS. As video streaming
quality is mainly affected by video compression, we demonstrate that the video
enhancement algorithm outperforms the super-resolution algorithm in terms of
signal-to-noise ratio and frames per second, suggesting a better solution for
client processing in video streaming. Ultimately, we implement ENAVS and
demonstrate extensive testbed results under real-world bandwidth traces and
videos. The simulation shows that ENAVS is capable of delivering 5%-14% more
QoE under the same bandwidth and computing power conditions as conventional ABR
streaming.",multimedia
http://arxiv.org/abs/2201.08102v2,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,safe deep rl in 3d environments using human feedback,http://arxiv.org/abs/2201.08102v2,"Agents should avoid unsafe behaviour during both training and deployment.
This typically requires a simulator and a procedural specification of unsafe
behaviour. Unfortunately, a simulator is not always available, and procedurally
specifying constraints can be difficult or impossible for many real-world
tasks. A recently introduced technique, ReQueST, aims to solve this problem by
learning a neural simulator of the environment from safe human trajectories,
then using the learned simulator to efficiently learn a reward model from human
feedback. However, it is yet unknown whether this approach is feasible in
complex 3D environments with feedback obtained from real humans - whether
sufficient pixel-based neural simulator quality can be achieved, and whether
the human data requirements are viable in terms of both quantity and quality.
In this paper we answer this question in the affirmative, using ReQueST to
train an agent to perform a 3D first-person object collection task using data
entirely from human contractors. We show that the resulting agent exhibits an
order of magnitude reduction in unsafe behaviour compared to standard
reinforcement learning.",multimedia
http://arxiv.org/abs/2201.07312v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,model-driven cluster resource management for ai workloads in edge clouds,http://arxiv.org/abs/2201.07312v1,"Since emerging edge applications such as Internet of Things (IoT) analytics
and augmented reality have tight latency constraints, hardware AI accelerators
have been recently proposed to speed up deep neural network (DNN) inference run
by these applications. Resource-constrained edge servers and accelerators tend
to be multiplexed across multiple IoT applications, introducing the potential
for performance interference between latency-sensitive workloads. In this
paper, we design analytic models to capture the performance of DNN inference
workloads on shared edge accelerators, such as GPU and edgeTPU, under different
multiplexing and concurrency behaviors. After validating our models using
extensive experiments, we use them to design various cluster resource
management algorithms to intelligently manage multiple applications on edge
accelerators while respecting their latency constraints. We implement a
prototype of our system in Kubernetes and show that our system can host 2.3X
more DNN applications in heterogeneous multi-tenant edge clusters with no
latency violations when compared to traditional knapsack hosting algorithms.",multimedia
http://arxiv.org/abs/2201.07232v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,"real-time x-ray phase-contrast imaging using spinnet -- a speckle-based
  phase-contrast imaging neural network",http://arxiv.org/abs/2201.07232v1,"X-ray phase-contrast imaging has become indispensable for visualizing samples
with low absorption contrast. In this regard, speckle-based techniques have
shown significant advantages in spatial resolution, phase sensitivity, and
implementation flexibility compared with traditional methods. However, their
computational cost has hindered their wider adoption. By exploiting the power
of deep learning, we developed a new speckle-based phase-contrast imaging
neural network (SPINNet) that boosts the phase retrieval speed by at least two
orders of magnitude compared to existing methods. To achieve this performance,
we combined SPINNet with a novel coded-mask-based technique, an enhanced
version of the speckle-based method. Using this scheme, we demonstrate a
simultaneous reconstruction of absorption and phase images on the order of 100
ms, where a traditional correlation-based analysis would take several minutes
even with a cluster. In addition to significant improvement in speed, our
experimental results show that the imaging resolution and phase retrieval
quality of SPINNet outperform existing single-shot speckle-based methods.
Furthermore, we successfully demonstrate its application in 3D X-ray
phase-contrast tomography. Our result shows that SPINNet could enable many
applications requiring high-resolution and fast data acquisition and
processing, such as in-situ and in-operando 2D and 3D phase-contrast imaging
and real-time at-wavelength metrology and wavefront sensing.",multimedia
http://arxiv.org/abs/2201.10978v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,machine learning for food review and recommendation,http://arxiv.org/abs/2201.10978v1,"Food reviews and recommendations have always been important for online food
service websites. However, reviewing and recommending food is not simple as it
is likely to be overwhelmed by disparate contexts and meanings. In this paper,
we use different deep learning approaches to address the problems of sentiment
analysis, automatic review tag generation, and retrieval of food reviews. We
propose to develop a web-based food review system at Nanyang Technological
University (NTU) named NTU Food Hunter, which incorporates different deep
learning approaches that help users with food selection. First, we implement
the BERT and LSTM deep learning models into the system for sentiment analysis
of food reviews. Then, we develop a Part-of-Speech (POS) algorithm to
automatically identify and extract adjective-noun pairs from the review content
for review tag generation based on POS tagging and dependency parsing. Finally,
we also train a RankNet model for the re-ranking of the retrieval results to
improve the accuracy in our Solr-based food reviews search system. The
experimental results show that our proposed deep learning approaches are
promising for the applications of real-world problems.",multimedia
http://arxiv.org/abs/2201.06912v1,preprocessed,arxiv,arxiv,2022-01-14 00:00:00,arxiv,digital twin: from concept to practice,http://arxiv.org/abs/2201.06912v1,"Recent technological developments and advances in Artificial Intelligence
(AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT),
virtually making it possible to introduce automation into all aspects of work
processes. Given these possibilities that DT can offer, practitioners are
facing increasingly difficult decisions regarding what capabilities to select
while deploying a DT in practice. The lack of research in this field has not
helped either. It has resulted in the rebranding and reuse of emerging
technological capabilities like prediction, simulation, AI, and Machine
Learning (ML) as necessary constituents of DT. Inappropriate selection of
capabilities in a DT can result in missed opportunities, strategic
misalignments, inflated expectations, and risk of it being rejected as just
hype by the practitioners. To alleviate this challenge, this paper proposes the
digitalization framework, designed and developed by following a Design Science
Research (DSR) methodology over a period of 18 months. The framework can help
practitioners select an appropriate level of sophistication in a DT by weighing
the pros and cons for each level, deciding evaluation criteria for the digital
twin system, and assessing the implications of the selected DT on the
organizational processes and strategies, and value creation. Three real-life
case studies illustrate the application and usefulness of the framework.",multimedia
http://arxiv.org/abs/2201.05184v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"achieving ai-enabled robust end-to-end quality of experience over radio
  access networks",http://arxiv.org/abs/2201.05184v1,"Emerging applications such as Augmented Reality, the Internet of Vehicles and
Remote Surgery require both computing and networking functions working in
harmony. The End-to-end (E2E) quality of experience (QoE) for these
applications depends on the synchronous allocation of networking and computing
resources. However, the relationship between the resources and the E2E QoE
outcomes is typically stochastic and non-linear. In order to make efficient
resource allocation decisions, it is essential to model these relationships.
This article presents a novel machine-learning based approach to learn these
relationships and concurrently orchestrate both resources for this purpose. The
machine learning models further help make robust allocation decisions regarding
stochastic variations and simplify robust optimization to a conventional
constrained optimization. When resources are insufficient to accommodate all
application requirements, our framework supports executing some of the
applications with minimal degradation (graceful degradation) of E2E QoE. We
also show how we can implement the learning and optimization methods in a
distributed fashion by the Software-Defined Network (SDN) and Kubernetes
technologies. Our results show that deep learning-based modelling achieves E2E
QoE with approximately 99.8\% accuracy, and our robust joint-optimization
technique allocates resources efficiently when compared to existing
differential services alternatives.",multimedia
http://arxiv.org/abs/2201.04833v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"snapshotnet: self-supervised feature learning for point cloud data
  segmentation using minimal labeled data",http://arxiv.org/abs/2201.04833v1,"Manually annotating complex scene point cloud datasets is both costly and
error-prone. To reduce the reliance on labeled data, a new model called
SnapshotNet is proposed as a self-supervised feature learning approach, which
directly works on the unlabeled point cloud data of a complex 3D scene. The
SnapshotNet pipeline includes three stages. In the snapshot capturing stage,
snapshots, which are defined as local collections of points, are sampled from
the point cloud scene. A snapshot could be a view of a local 3D scan directly
captured from the real scene, or a virtual view of such from a large 3D point
cloud dataset. Snapshots could also be sampled at different sampling rates or
fields of view (FOVs), thus multi-FOV snapshots, to capture scale information
from the scene. In the feature learning stage, a new pre-text task called
multi-FOV contrasting is proposed to recognize whether two snapshots are from
the same object or not, within the same FOV or across different FOVs. Snapshots
go through two self-supervised learning steps: the contrastive learning step
with both part and scale contrasting, followed by a snapshot clustering step to
extract higher level semantic features. Then a weakly-supervised segmentation
stage is implemented by first training a standard SVM classifier on the learned
features with a small fraction of labeled snapshots. The trained SVM is used to
predict labels for input snapshots and predicted labels are converted into
point-wise label assignments for semantic segmentation of the entire scene
using a voting procedure. The experiments are conducted on the Semantic3D
dataset and the results have shown that the proposed method is capable of
learning effective features from snapshots of complex scene data without any
labels. Moreover, the proposed method has shown advantages when comparing to
the SOA method on weakly-supervised point cloud semantic segmentation.",multimedia
http://arxiv.org/abs/2201.04195v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,matching-based service offloading for compute-less driven iot networks,http://arxiv.org/abs/2201.04195v1,"With the advent of the Internet of Things (IoT) and 5G networks, edge
computing is offering new opportunities for business model and use cases
innovations. Service providers can now virtualize the cloud beyond the data
center to meet the latency, data sovereignty, reliability, and interoperability
requirements. Yet, many new applications (e.g., augmented reality, virtual
reality, artificial intelligence) are computation-intensive and
delay-sensitivity. These applications are invoked heavily with similar inputs
that could lead to the same output. Compute-less networks aim to implement a
network with a minimum amount of computation and communication. This can be
realized by offloading prevalent services to the edge and thus minimizing
communication in the core network and eliminating redundant computations using
the computation reuse concept. In this paper, we present matching-based
services offloading schemes for compute-less IoT networks. We adopt the
matching theory to match service offloading to the appropriate edge server(s).
Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme
that aims to offload the most invoked and highly reusable services to the
appropriate edge servers. We further extend WHISTLE to provide horizontal
one-to-many computation reuse sharing among edge servers which leads to
bouncing less computation back to the cloud. We evaluate the efficiency and
effectiveness of WHISTLE with a real-world dataset. The obtained findings show
that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the
computation up to 77%, and decrease the communication up to 71%. Theoretical
analyses also prove the stability of the designed schemes.",multimedia
http://arxiv.org/abs/2201.04014v2,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,captcha attack: turning captchas against humanity,http://arxiv.org/abs/2201.04014v2,"Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
  In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.",multimedia
http://arxiv.org/abs/2201.03804v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,"ci-avsr: a cantonese audio-visual speech dataset for in-car command
  recognition",http://arxiv.org/abs/2201.03804v1,"With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",multimedia
http://arxiv.org/abs/2201.03335v2,preprocessed,arxiv,arxiv,2022-01-10 00:00:00,arxiv,"deepke: a deep learning based knowledge extraction toolkit for knowledge
  base population",http://arxiv.org/abs/2201.03335v2,"We present a new open-source and extensible knowledge extraction toolkit,
called DeepKE (Deep learning based Knowledge Extraction), supporting standard
fully supervised, low-resource few-shot and document-level scenarios. DeepKE
implements various information extraction tasks, including named entity
recognition, relation extraction and attribute extraction. With a unified
framework, DeepKE allows developers and researchers to customize datasets and
models to extract information from unstructured texts according to their
requirements. Specifically, DeepKE not only provides various functional modules
and model implementation for different tasks and scenarios but also organizes
all components by consistent frameworks to maintain sufficient modularity and
extensibility. Besides, we present an online platform in
http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has
been equipped with Google Colab tutorials and comprehensive documents for
beginners. We release the source code at https://github.com/zjunlp/DeepKE, with
a demo video.",multimedia
http://arxiv.org/abs/2201.02503v1,preprocessed,arxiv,arxiv,2022-01-07 00:00:00,arxiv,"a review of deep learning techniques for markerless human motion on
  synthetic datasets",http://arxiv.org/abs/2201.02503v1,"Markerless motion capture has become an active field of research in computer
vision in recent years. Its extensive applications are known in a great variety
of fields, including computer animation, human motion analysis, biomedical
research, virtual reality, and sports science. Estimating human posture has
recently gained increasing attention in the computer vision community, but due
to the depth of uncertainty and the lack of the synthetic datasets, it is a
challenging task. Various approaches have recently been proposed to solve this
problem, many of which are based on deep learning. They are primarily focused
on improving the performance of existing benchmarks with significant advances,
especially 2D images. Based on powerful deep learning techniques and recently
collected real-world datasets, we explored a model that can predict the
skeleton of an animation based solely on 2D images. Frames generated from
different real-world datasets with synthesized poses using different body
shapes from simple to complex. The implementation process uses DeepLabCut on
its own dataset to perform many necessary steps, then use the input frames to
train the model. The output is an animated skeleton for human movement. The
composite dataset and other results are the ""ground truth"" of the deep model.",multimedia
http://arxiv.org/abs/2201.02279v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,de-rendering 3d objects in the wild,http://arxiv.org/abs/2201.02279v1,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.",multimedia
http://arxiv.org/abs/2201.01144v2,preprocessed,arxiv,arxiv,2022-01-04 00:00:00,arxiv,digital twin network: opportunities and challenges,http://arxiv.org/abs/2201.01144v2,"The proliferation of emergent network applications (e.g., AR/VR, telesurgery,
real-time communications) is increasing the difficulty of managing modern
communication networks. These applications typically have stringent
requirements (e.g., ultra-low deterministic latency), making it more difficult
for network operators to manage their network resources efficiently. In this
article, we propose the Digital Twin Network (DTN) as a key enabler for
efficient network management in modern networks. We describe the general
architecture of the DTN and argue that recent trends in Machine Learning (ML)
enable building a DTN that efficiently and accurately mimics real-world
networks. In addition, we explore the main ML technologies that enable
developing the components of the DTN architecture. Finally, we describe the
open challenges that the research community has to address in the upcoming
years in order to enable the deployment of the DTN in real-world scenarios.",multimedia
http://arxiv.org/abs/2201.00768v1,preprocessed,arxiv,arxiv,2022-01-03 00:00:00,arxiv,"robust natural language processing: recent advances, challenges, and
  future directions",http://arxiv.org/abs/2201.00768v1,"Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.",multimedia
http://arxiv.org/abs/2201.00309v1,preprocessed,arxiv,arxiv,2022-01-02 00:00:00,arxiv,"optimizing machine learning inference queries with correlative proxy
  models",http://arxiv.org/abs/2201.00309v1,"We consider accelerating machine learning (ML) inference queries on
unstructured datasets. Expensive operators such as feature extractors and
classifiers are deployed as user-defined functions(UDFs), which are not
penetrable with classic query optimization techniques such as predicate
push-down. Recent optimization schemes (e.g., Probabilistic Predicates or PP)
assume independence among the query predicates, build a proxy model for each
predicate offline, and rewrite a new query by injecting these cheap proxy
models in the front of the expensive ML UDFs. In such a manner, unlikely inputs
that do not satisfy query predicates are filtered early to bypass the ML UDFs.
We show that enforcing the independence assumption in this context may result
in sub-optimal plans. In this paper, we propose CORE, a query optimizer that
better exploits the predicate correlations and accelerates ML inference
queries. Our solution builds the proxy models online for a new query and
leverages a branch-and-bound search process to reduce the building costs.
Results on three real-world text, image and video datasets show that CORE
improves the query throughput by up to 63% compared to PP and up to 80%
compared to running the queries as it is.",multimedia
10.1016/j.enconman.2022.115217,preprocessed,Energy Conversion and Management,scopus,2022-02-15,sciencedirect,robopv: an integrated software package for autonomous aerial monitoring of large scale pv plants,https://api.elsevier.com/content/abstract/scopus_id/85122793867,"In this paper, a novel software package, called RoboPV, is introduced for autonomous aerial monitoring of PV plants. RoboPV automatically performs aerial monitoring of PV plants, from optimal trajectory planning to image processing and pattern recognition for real-time fault detection and analysis. RoboPV consists of four integrated components: boundary area detection, path planning, dynamic processing, and fault detection. To design an optimal flight path, aerial images of PV plants, which have been collected from experimental flights, are given as inputs to a developed encoder-decoder deep learning architecture to extract boundary points of PV plants automatically. Then, a novel path planning algorithm is conducted by RoboPV to design an optimal flight path with full coverage of whole regions of the PV plant. Aerial images are analysed in real-time during the flight by a high precise neural network trained for automatic fault detection. In this study, several decision-making and maneuver algorithms were developed for various real-world flight conditions to improve the performance of RoboPV during an autonomous aerial inspection. RoboPV is a modular processing library that can be installed on any micro-computer processor with a low computational power. Moreover, supporting the MAVLink communication protocol enables RoboPV to connect with an intelligent Pixhawk flight autopilot and navigate a wide range of multi-rotors. To demonstrate the performance of RoboPV, a six degrees of freedom dynamic model of a multi-rotor is developed in a SIMULINK environment with a defined aerial monitoring mission on three different real megawatt-scale PV plants. The results prove that RoboPV can execute the autonomous aerial inspection with an overall accuracy of 93% for large-scale PV plants.",multimedia
10.1016/j.ymssp.2021.108284,preprocessed,Mechanical Systems and Signal Processing,scopus,2022-02-15,sciencedirect,real-time model calibration with deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85112506465,"The real-time, and accurate inference of model parameters is of great importance in many scientific and engineering disciplines that use computational models (such as a digital twin) for the analysis and prediction of complex physical processes. However, fast and accurate inference for processes of complex systems cannot easily be achieved in real-time with state-of-the-art methods under noisy real-world conditions with the requirement of a real-time response. The primary reason is that the inference of model parameters with traditional techniques based on optimization or sampling often suffers from computational and statistical challenges, resulting in a trade-off between accuracy and deployment time. In this paper, we propose a novel framework for inference of model parameters based on reinforcement learning. The proposed methodology is demonstrated and evaluated on two different physics-based models of turbofan engines. The experimental results demonstrate that the proposed methodology outperforms all other tested methods in terms of speed and robustness, with high inference accuracy.",multimedia
10.1016/j.vrih.2022.01.004,preprocessed,Virtual Reality and Intelligent Hardware,scopus,2022-02-01,sciencedirect,virtual-reality-based digital twin of office spaces with social distance measurement feature,https://api.elsevier.com/content/abstract/scopus_id/85124517698,"Background
                  Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
               
                  Methods
                  In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
               
                  Results
                  We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
               
                  Conclusions
                  A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.",multimedia
10.1016/j.pmcj.2022.101540,preprocessed,Pervasive and Mobile Computing,scopus,2022-02-01,sciencedirect,lwtool: a data processing toolkit for building a real-time pressure mapping smart textile software system,https://api.elsevier.com/content/abstract/scopus_id/85123030397,"Pressure mapping smart textile is a new type of sensing modality that transforms the pressure distribution over surfaces into digital ”image” and ”video”, that has rich application scenarios in Human Activity Recognition (HAR), because all human activities are linked with force change over certain surfaces. To speed up its application exploration, we propose a toolkit named LwTool for the data processing, including: (a) a feature library, including 1830 ready-to-use temporal and spatial features, (b) a hierarchical feature selection framework that automatically picks out the best features for a new application from the feature library. As real-time processing capability is important for instant user feedback, we emphasize not only on good recognition result but also on reducing time cost when selecting features. Our library and algorithms are validated on Smart-Toy and Smart-Bedsheet applications, an 89.7% accuracy for Smart-Toy and an 83.8% accuracy for Smart-Bedsheet can be achieved (10-fold cross-validation) using our feature library. Adopting the feature selection algorithm, the processing speed is increased by more than 3 times while maintaining high accuracy for both two applications. We believe our method could be a general and powerful toolkit in building real-time recognition software systems for pressure mapping smart textile.",multimedia
10.1016/j.cviu.2021.103339,preprocessed,Computer Vision and Image Understanding,scopus,2022-02-01,sciencedirect,snapshotnet: self-supervised feature learning for point cloud data segmentation using minimal labeled data,https://api.elsevier.com/content/abstract/scopus_id/85122523188,"Manually annotating complex scene point cloud datasets is both costly and error-prone. To reduce the reliance on labeled data, a new model called SnapshotNet is proposed as a self-supervised feature learning approach, which directly works on the unlabeled point cloud data of a complex 3D scene. The SnapshotNet pipeline includes three stages. In the snapshot capturing stage, snapshots, which are defined as local collections of points, are sampled from the point cloud scene. A snapshot could be a view of a local 3D scan directly captured from the real scene, or a virtual view of such from a large 3D point cloud dataset. Snapshots could also be sampled at different sampling rates or fields of view (FOVs), thus multi-FOV snapshots, to capture scale information from the scene. In the feature learning stage, a new pre-text task called multi-FOV contrasting is proposed to recognize whether two snapshots are from the same object or not, within the same FOV or across different FOVs. Snapshots go through two self-supervised learning steps: the contrastive learning step with both part contrasting and scale contrasting, followed by a snapshot clustering step to extract higher level semantic features. Then a weakly-supervised segmentation stage is implemented by first training a standard SVM classifier on the learned features with a small fraction of labeled snapshots. Then trained SVM is further used to predict labels for input snapshots and predicted labels are converted into point-wise label assignments for semantic segmentation of the entire scene using a voting procedure. The experiments are conducted on the Semantic3D dataset and the results have shown that the proposed method is capable of learning effective features from snapshots of complex scene data without any labels. Moreover, the proposed weakly-supervised method has shown advantages when comparing to the state of the art method on weakly-supervised point cloud semantic segmentation.",multimedia
10.1016/j.compag.2021.106644,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,ric-net: a plant disease classification model based on the fusion of inception and residual structure and embedded attention mechanism,https://api.elsevier.com/content/abstract/scopus_id/85121931762,"In this paper, we proposed a convolutional neural network based on Inception and residual structure with an embedded modified convolutional block attention module (CBAM), aiming to improve the classification of plant leaf diseases. Corn, potatoes and tomatoes are the most cultivated grains in southern China. The leaves of the three crops are very fragile and sensitive and are susceptible to leaf diseases, such as leaf blight of corn, late blight of potato and mosaic virus of tomato. These diseases cannot be identified at early stages. Therefore, an efficient solution is proposed by deep learning techniques to detect the disease categories of crops, which can effectively prevent the spread of diseases and ensure the normal growth of plants. In this experiment, our model achieved an overall accuracy of 99.55% for the identification of the three diseases of corn, potato and tomato. In addition, we tested the three plants individually. The classification accuracy of our model on corn, potato and tomato was 98.44%, 99.43% and 95.20%, respectively. We have also developed a web-based real-time plant disease classification system and deployed our model. The system had good performance in time and accuracy evaluation metrics. The results of this study showed that our model had fewer parameters, shorter training time, and higher recognition accuracy compared to existing image classification models.",multimedia
10.1016/j.cose.2021.102539,preprocessed,Computers and Security,scopus,2022-02-01,sciencedirect,deep face fuzzy vault: implementation and performance,https://api.elsevier.com/content/abstract/scopus_id/85119176199,"Biometric technologies, especially face recognition, have become an essential part of identity management systems worldwide. In deployments of biometrics, secure storage of biometric information is necessary in order to protect the users’ privacy. In this context, biometric cryptosystems are designed to meet key requirements of biometric information protection enabling a privacy-preserving storage and comparison of biometric data, e.g. feature vectors extracted from facial images. Until now, biometric cryptosystems have hardly been applied to state-of-the-art biometric recognition systems utilizing deep convolutional neural networks.
                  This work investigates the application of a well-known biometric cryptosystem, i.e. the improved fuzzy vault scheme, to facial feature vectors extracted through deep convolutional neural networks. To this end, a feature transformation method is introduced which maps fixed-length real-valued deep feature vectors to integer-valued feature sets. As part of said feature transformation, a detailed analysis of different feature quantisation and binarisation techniques is conducted. At key binding, obtained feature sets are locked in an unlinkable improved fuzzy vault. For key retrieval, the efficiency of different polynomial reconstruction techniques is investigated. The proposed feature transformation method and template protection scheme are agnostic of the biometric characteristic and, thus, can be applied to virtually any biometric features computed by a deep neural network. In experiments, an unlinkable improved deep face fuzzy vault-based template protection scheme is constructed employing features extracted with a state-of-the-art deep convolutional neural network trained with the additive angular margin loss (ArcFace). For the best configuration, a false non-match rate below 1% at a false match rate of 0.01%, is achieved in cross-database experiments on the FERET and FRGCv2 face databases. On average, a security level of up to approximately 28 bits is obtained. This work presents an effective face-based fuzzy vault scheme providing privacy protection of facial reference data as well as digital key derivation from face.",multimedia
10.1016/j.eswa.2021.116073,preprocessed,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,efficient machine learning approach for volunteer eye-blink detection in real-time using webcam,https://api.elsevier.com/content/abstract/scopus_id/85117617175,"The progressive diminishment of motor capacities due to Amyotrophic Lateral Sclerosis (ALS) causes a severe communication deficit. The development of Alternative Communication software aids ALS patients in overcoming communication issues and the detection of communication signals plays a big role in this task. In this paper, volunteer eye-blinking is proposed as human–computer interaction signal and an intelligent Computer Vision detector was built for handling the captured data in real-time using a generic webcam. The eye-blink detection was treated as an extension of the eye-state classification, and the base pipeline used is delineated as follows: face detection, face alignment, region-of-interest (ROI) extraction, and eye-state classification. Furthermore, this pipeline was complemented with auxiliary models: a rotation compensator, a ROIs evaluator, and a moving average filter. Two new datasets were created: the Youtube Eye-state Classification (YEC) dataset, built from the AVSpeech dataset by extracting face images; and the Autonomus Blink Dataset (ABD), built completely as a result of the present work. The YEC allowed training the eye-classification task; ABD was specifically idealized taking into consideration volunteer eye-blinking detection. The proposed models, a Convolutional Neural Network (CNN) and a Support Vector Machine (SVM), were trained by the YEC dataset and performance evaluation experiments for both models were conducted across different databases: CeW, ZJU, Eyeblink, Talking Face (public datasets) and ABD. The impact of the proposed auxiliary models was evaluated and the CNN and SVM models were compared for the eye-state classification task. Promising results were obtained: 97.44% accuracy for the eye-state classification task on the CeW dataset and 92.63% F1-Score for the eye-blink detection task on the ABD dataset.",multimedia
10.1016/j.apacoust.2021.108439,preprocessed,Applied Acoustics,scopus,2022-01-15,sciencedirect,"camnet: a controllable acoustic model for efficient, expressive, high-quality text-to-speech",https://api.elsevier.com/content/abstract/scopus_id/85116891718,"Spoken language is becoming one of the key components of human–machine interaction, both to send information to the machine – e.g. voice control – and to receive from it – e.g. virtual assistants. In this scenario, text-to-speech (TTS) models have become an essential artificial intelligence capacity. Even though this interaction can be based on neutral style speech, generating speech with different styles, pitches and speaking rates may improve user experience. With this in view, this paper presents CAMNet, a controllable acoustic model for efficient, expressive, high-quality TTS. CAMNet is based on deep convolutional TTS (DCTTS), a state-of-art acoustic model which is efficient and produces neutral speech. DCTTS was first adapted to generate Bark cepstrum acoustic features in order to integrate well with the LPCNet (linear prediction coefficient) neural vocoder and to remove the reduction factor which demanded the presence of an upsampling network before the vocoder – i.e. the CAMNet output can be directly fed into LPCNet. Next, style transfer functionality was added by means of a novel characterisation of the prosodic information from the Bark cepstrum acoustic features and a new approach to inject this information into the convolutional layers. Finally, controllability is provided via a variational auto-encoder module which creates a smoothed disentangled latent space which allows interpolation and extrapolation of reference styles as well as independent and simultaneous control of two generative factors: pitch and speaking rate. Moreover, this controllability is implemented using a simple offset-based approach. To sum up, CAMNet is an efficient acoustic model which provides a simple but consistent controllability on coarse-grained expression, pitch and speaking rate while still providing high-quality synthesised speech.",multimedia
10.1016/j.buildenv.2021.108532,preprocessed,Building and Environment,scopus,2022-01-01,sciencedirect,personal thermal comfort models using digital twins: preference prediction with bim-extracted spatial–temporal proximity data from build2vec,https://api.elsevier.com/content/abstract/scopus_id/85119963452,"Conventional thermal preference prediction in buildings has limitations due to the difficulty in capturing all environmental and personal factors. New model features can improve the ability of a machine learning model to classify a person’s thermal preference. The spatial context of a building can provide information to models about the windows, walls, heating and cooling sources, air diffusers, and other factors that create micro-environments that influence thermal comfort. Due to spatial heterogeneity, it is impractical to position sensors at a high enough resolution to capture all conditions. This research aims to build upon an existing vector-based spatial model, called Build2Vec, for predicting spatial–temporal occupants’ indoor environmental preferences. Build2Vec utilizes the spatial data from the Building Information Model (BIM) and indoor localization in a real-world setting. This framework uses longitudinal intensive thermal comfort subjective feedback from smart watch-based ecological momentary assessments (EMA). The aggregation of these data is combined into a graph network structure (i.e., objects and relations) and used as input for a classification model to predict occupant thermal preference. The results of a test implementation show 14%–28% accuracy improvement over a set of baselines that use conventional thermal preference prediction input variables.",multimedia
10.1016/j.jobe.2021.103571,preprocessed,Journal of Building Engineering,scopus,2022-01-01,sciencedirect,an integrated building energy performance evaluation method: from parametric modeling to ga-nn based energy consumption prediction modeling,https://api.elsevier.com/content/abstract/scopus_id/85119285403,"Building energy performance evaluation, as an important process in a sustainable building design, has important consequences for global energy conservation and environmental protection. The traditional methods to perform this evaluation are usually time-consuming and computationally complex, and have high requirements for designers’ professional knowledge on architectural physics and software operation skills. To solve these problems and provide rapid, user-friendly, and more accurate prediction results, this study presents an efficient building energy performance evaluation method which integrates building information modeling, energy simulation, and energy consumption prediction together. This method follows a three-stage research framework: Stage 1 proposes a rapid 3D building energy modeling process according to the parameterized setting, Stage 2 generates numerous simulation results automatically by EnergyPlus, and Stage 3 develops the user-friendly building energy consumption prediction model with the help of the Genetic Algorithm-Neural Network (GA-NN) and provides the energy performance level of the building design after the prediction. A case study is carried out to present the overall process and verify the accuracy of the proposed three-stage building energy performance evaluation method. This study contributes to the improvement of both the extensive dataset establishment and the operational efficiency of building energy consumption prediction. It can provide designers with a real-time, user-friendly, and reliable building energy consumption prediction tool and an energy performance assessment basis in the design phase of construction projects.",multimedia
10.1016/j.eswa.2021.115973,preprocessed,Expert Systems with Applications,scopus,2022-01-01,sciencedirect,deep correlation mining for multi-task image clustering,https://api.elsevier.com/content/abstract/scopus_id/85116928779,"Multi-task clustering (MTC) aims to enhance the performance of each individual task by leveraging the correlation information among them. Existing MTC algorithms usually first extract the feature representations of each task and then learn the relationships among multiple tasks for clustering. However, the multi-task correlations are not embedded into the feature learning in existing MTC. In addition, many real applications, such as image clustering, always perform visual feature extraction and clustering assignment separately, which often results in local optimal clustering resolutions. In this study, an end-to-end MTC framework, named Deep correlation mining for Multi-Task image Clustering (DMTC), is proposed to explore multi-task correlations and conduct image clustering simultaneously. Specifically, DMTC consists of two sub-networks: a between-task network (B-net) and a within-task network (W-net), which learn the correlations among multiple tasks and the relationships in each individual task, respectively, based on a deep convolutional network. To optimize B-net, an optimization procedure is proposed as follows: (1) DMTC builds a pseudo-graph to discover similar samples among tasks and obtain the positive pairs of possible related tasks. (2) A discriminator is designed to calculate the mutual information between the deep and shallow representations of related tasks, which can estimate the relatedness between each pair of related tasks. After that, the trained parameters in B-net are transferred to the within-task networks (W-net) as their initialized parameters, in which the above optimization procedure is performed again to obtain the final cluster partition by end-to-end training. Experimental results on NUS-Wide, Caltech-256, Cifar-100 and Pascal VOC demonstrate that our proposed DMTC method
                        1
                     
                     
                        1
                        The source code is available in https://github.com/Xiaoqiang-Yan/DMTC.
                     compares favorably to the state-of-the-art methods.",multimedia
10.1016/j.autcon.2021.103996,preprocessed,Automation in Construction,scopus,2022-01-01,sciencedirect,implementation experiments on convolutional neural network training using synthetic images for 3d pose estimation of an excavator on real images,https://api.elsevier.com/content/abstract/scopus_id/85116888055,"Remote and descriptive visualization of spatio-temporal information of excavator activities may increase awareness about jobsite hazards and operational performance in earthwork operations. One of the emerging approaches to collect this information is to extract the 3D pose of an excavator from the video frames using a convolutional neural network (CNN). However, this method requires labeling the training datasets, which are difficult to prepare because of conditions unsuitable for installing the motion capture sensors. This study investigates the performance of a CNN for estimating the 3D pose when trained on a synthetic dataset. In particular, a kinematic constraint is proposed to update the model parameters efficiently during training. The results show that the proposed method estimated the 3D poses of a real excavator with an average pose error of 9.63°. Hence, the proposed data augmentation method could help address the training data issues and improves the learning of real data complexity.",multimedia
10.1016/j.sigpro.2021.108317,preprocessed,Signal Processing,scopus,2022-01-01,sciencedirect,selective fixed-filter active noise control based on convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85114690529,"Active noise control (ANC) technology is increasingly ubiquitous in wearable audio devices, or hearables. Owing to its low computational complexity, high robustness, and exemplary performance in dealing with dynamic noise, the fixed-coefficient control filter strategy plays a central role in portable ANC implementation. Unlike its traditional adaptive counterpart, the fixed-filter strategy is unable to attain optimal noise reduction for different types of noise. Hence, we propose a selective fixed-filter ANC method based on a simplified two-dimensional convolution neural network (2D CNN), which is implemented on a co-processor (e.g., in a mobile phone), to derive the most suitable control filter for different noise types. To further reduce classification complexity, we designed a lightweight one-dimensional CNN (1D CNN), which can directly classify noise types in time domain. A numerical simulation based on measured paths in headphones demonstrates the proposed algorithm’s efficacy in attenuating real-world non-stationary noise over conventional adaptive algorithms.",multimedia
10.1016/j.csl.2021.101275,preprocessed,Computer Speech and Language,scopus,2022-01-01,sciencedirect,feature learning for efficient asr-free keyword spotting in low-resource languages,https://api.elsevier.com/content/abstract/scopus_id/85113158279,"We consider feature learning for a computationally efficient method of keyword spotting that can be applied in severely under-resourced settings. The objective is to support humanitarian relief programmes by the United Nations (UN) in parts of Africa in which almost no language resources are available. To allow a keyword spotting system to be rapidly developed in such a language, we rely on a small and easily-compiled set of isolated keywords. Using the isolated keywords as templates, we apply dynamic time warping (DTW) to a much larger corpus of in-domain but untranscribed speech. The resulting DTW alignment scores are used to train a convolutional neural network (CNN) which is orders of magnitude more computationally efficient than DTW and therefore suitable for real-time application. We optimise this ASR-free neural network keyword spotting procedure by identifying acoustic features that provide robust performance in this almost zero-resource setting. First, we consider the benefits of incorporating information from well-resourced but unrelated languages by incorporating a multilingual bottleneck feature (BNF) extractor. Next, we consider using features extracted from an autoencoder (AE) trained on in-domain but untranscribed data. Finally, we consider features obtained from a correspondence autoencoder (CAE) which is initialised with the AE and subsequently fine-tuned on the small set of in-domain labelled data. Experiments in South African English and Luganda, a low-resource language, demonstrate that, on their own, both the BNF and CAE features can achieve a 5% relative performance improvement over baseline MFCCs. However, by using BNFs as input to the CAE, even better performance is achieved, resulting in a more than 27% relative improvement over MFCCs in ROC area-under-the-curve (AUC) and more than twice as many top-10 retrievals. We also show that, using these features, the CNN-DTW keyword spotter performs almost as well as the DTW keyword spotter while comfortably outperforming a baseline CNN trained only on the keyword templates. We conclude that a CNN-DTW keyword spotter using BNF-derived CAE features represents a computationally efficient approach with very competitive performance that is suited to rapid deployment in a severely under-resourced scenario.",multimedia
10.1016/j.petrol.2021.109332,preprocessed,Journal of Petroleum Science and Engineering,scopus,2022-01-01,sciencedirect,end-to-end neural network approach to 3d reservoir simulation and adaptation,https://api.elsevier.com/content/abstract/scopus_id/85112357560,"Reservoir simulation and adaptation (also known as history matching) are typically considered as separate problems. While a set of models are aimed at the solution of the forward simulation problem assuming all initial geological parameters are known, the other set of models adjust geological parameters under the fixed forward simulation model to fit production data. This results in many difficulties for both reservoir engineers and developers of new efficient computation schemes. We present a unified approach to reservoir simulation and adaptation problems. A single neural network model allows a forward pass from initial geological parameters of the 3D reservoir model through dynamic state variables to well’s production rates and backward gradient propagation to any model inputs and variables. The model fitting and geological parameters adaptation both become the optimization problem over specific parts of the same neural network model. Standard gradient-based optimization schemes can be used to find the optimal solution. Using real-world oilfield model and historical production rates we demonstrate that the suggested approach allows reservoir simulation and history matching with a benefit of several orders of magnitude simulation speed-up. Finally, to propagate this research we open-source a Python-based framework DeepField that allows standard processing of reservoir models and reproducing the approach presented in this paper.",multimedia
10.1016/j.patcog.2021.108205,preprocessed,Pattern Recognition,scopus,2022-01-01,sciencedirect,tracking more than 100 arbitrary objects at 25 fps through deep learning,https://api.elsevier.com/content/abstract/scopus_id/85111593606,"Most video analytics applications rely on object detectors to localize objects in frames. However, when real-time is a requirement, running the detector at all the frames is usually not possible. This is somewhat circumvented by instantiating visual object trackers between detector calls, but this does not scale with the number of objects. To tackle this problem, we present SiamMT, a new deep learning multiple visual object tracking solution that applies single-object tracking principles to multiple arbitrary objects in real-time. To achieve this, SiamMT reuses feature computations, implements a novel crop-and-resize operator, and defines a new and efficient pairwise similarity operator. SiamMT naturally scales up to several dozens of targets, reaching 25 fps with 122 simultaneous objects for VGA videos, or up to 100 simultaneous objects in HD720 video. SiamMT has been validated on five large real-time benchmarks, achieving leading performance against current state-of-the-art trackers.",multimedia
10.1109/access.2022.3147955,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,how to boost the performance of recommender systems by social trust? studying the challenges and proposing a solution,https://ieeexplore.ieee.org/document/9698036/,"With the increasing number of items in electronic retailers, news websites, etc., finding interesting items concerning the taste of users is becoming more challenging. Recommender Systems (RS) are a well-known solution to this issue. Collaborative filtering (CF) is a widely accepted and popular technique to implement an RS. However, cold-start and data sparsity problems reduce the performance of CF methods. One promising solution for these issues is to use the social trust information. However, how to properly use social trust information is a hot and still open question. In this paper, we propose a similarity measure and a simple link prediction method to address this question and employ them in trust-aware matrix factorization. Especially, our proposed similarity measure is asymmetric to consider the nature of social relationships. Also, to have a more accurate similarity estimation, we have considered both the user’s historical ratings and trust relations, and we have determined the weight of each source. Finally, we have used the item-based model and the level of interest a user’s trustee have for an item to improve the performance of the proposed method for sparse datasets. We conduct extensive performance evaluations in terms of rate prediction and interesting items found. Experimental results on three real-world datasets demonstrate the effectiveness of the proposed method, especially in terms of Mean Absolute Error.",science
10.1109/ccnc49033.2022.9700613,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,first experimental results on real-time cleaning activity monitoring system,https://ieeexplore.ieee.org/document/9700613/,"The COVID-19 pandemic has presented social challenges to establish the new normal lifestyle in our daily lives. The goal of this paper is to enable easy and low-cost monitoring of cleaning activity to keep a clean environment for preventing infection. Although human activity recognition has been a hot research topic in pervasive computing, existing schemes have not been optimized for monitoring cleaning activities. To address this issue, this paper provides an initial concept and preliminary experimental results of cleaning activity recognition using accelerometer data and RFID tags. In the proposed scheme, machine learning technologies and short range wireless communication are employed for recognizing the time and place of wiping as an example of cleaning activities, because it is an important activity for shared places to avoid infection. This paper reports the evaluation results on the recognition accuracy using the proof-of-concept (PoC) implementation to clarify the required sampling rate and time-window size for further experiments. Also, a real-time feedback system is implemented to provide the monitoring results for users. The proposed scheme contributes for efficient monitoring of cleaning activities for creating the new normal era.",science
10.1109/wacv51458.2022.00391,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,unveiling real-life effects of online photo sharing,https://ieeexplore.ieee.org/document/9706741/,"Social networks give free access to their services in exchange for the right to exploit their users’ data. Data sharing is done in an initial context which is chosen by the users. However, data are used by social networks and third parties in different contexts which are often not transparent. In order to unveil such usages, we propose an approach which focuses on the effects of data sharing in impactful real-life situations. Focus is put on visual content because of its strong influence in shaping online user profiles. The approach relies on three components: (1) a set of visual objects with associated situation impact ratings obtained by crowdsourcing, (2) a corresponding set of object detectors for mining users’ photos and (3) a ground truth dataset made of 500 visual user profiles which are manually rated per situation. These components are combined in LERV UP, a method which learns to rate visual user profiles in each situation. LERV UP exploits a new image descriptor which aggregates object ratings and object detections at user level and an attention mechanism which boosts highly-rated objects to prevent them from being overwhelmed by low-rated ones. Performance is evaluated per situation by measuring the correlation between the automatic ranking of profile ratings and a manual ground truth. Results indicate that LERV UP is effective since a strong correlation of the two rankings is obtained. A practical implementation of the approach in a mobile app which raises user awareness about shared data usage is also discussed.",science
10.1109/access.2021.3137636,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a deep learning-based framework for phishing website detection,https://ieeexplore.ieee.org/document/9661323/,"Phishing attackers spread phishing links through e-mail, text messages, and social media platforms. They use social engineering skills to trick users into visiting phishing websites and entering crucial personal information. In the end, the stolen personal information is used to defraud the trust of regular websites or financial institutions to obtain illegal benefits. With the development and applications of machine learning technology, many machine learning-based solutions for detecting phishing have been proposed. Some solutions are based on the features extracted by rules, and some of the features need to rely on third-party services, which will cause instability and time-consuming issues in the prediction service. In this paper, we propose a deep learning-based framework for detecting phishing websites. We have implemented the framework as a browser plug-in capable of determining whether there is a phishing risk in real-time when the user visits a web page and gives a warning message. The real-time prediction service combines multiple strategies to improve accuracy, reduce false alarm rates, and reduce calculation time, including whitelist filtering, blacklist interception, and machine learning (ML) prediction. In the ML prediction module, we compared multiple machine learning models using several datasets. From the experimental results, the RNN-GRU model obtained the highest accuracy of 99.18%, demonstrating the feasibility of the proposed solution.",science
10.1109/access.2021.3140175,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"a metaverse: taxonomy, components, applications, and open challenges",https://ieeexplore.ieee.org/document/9667507/,"Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to connectivity with reality using virtual currency. The integration of enhanced social activities and neural-net methods requires a new definition of Metaverse suitable for the present, different from the previous Metaverse. This paper divides the concepts and essential techniques necessary for realizing the Metaverse into three components (i.e., hardware, software, and contents) and three approaches (i.e., user interaction, implementation, and application) rather than marketing or hardware approach to conduct a comprehensive analysis. Furthermore, we describe essential methods based on three components and techniques to Metaverse’s representative Ready Player One, Roblox, and Facebook research in the domain of films, games, and studies. Finally, we summarize the limitations and directions for implementing the immersive Metaverse as social influences, constraints, and open challenges.",science
10.1109/tc.2021.3057082,preprocessed,IEEE Transactions on Computers,IEEE,2022-03-01 00:00:00,ieeexplore,<sc>lime</sc>: low-cost and incremental learning for dynamic heterogeneous information networks,https://ieeexplore.ieee.org/document/9353275/,"Understanding the interconnected relationships of large-scale information networks like social, scholar and Internet of Things networks is vital for tasks like recommendation and fraud detection. The vast majority of the real-world networks are inherently heterogeneous and dynamic, containing many different types of nodes and edges and can change drastically over time. The dynamicity and heterogeneity make it extremely challenging to reason about the network structure. Unfortunately, existing approaches are inadequate in modeling real-life dynamical networks as they either have strong assumption of a given stochastic process or fail to capture the heterogeneity of network structure, and they all require extensive computational resources. We introduce <sc>Lime</sc>, a better approach for modeling dynamic and heterogeneous information networks. <sc>Lime</sc> is designed to extract high-quality network representation with significantly lower memory resources and computational time over the state-of-the-arts. Unlike prior work that uses a vector to encode each network node, we exploit the semantic relationships among network nodes to encode multiple nodes with similar semantics in shared vectors. By using many fewer node vectors, our approach significantly reduces the required memory space for encoding large-scale networks. To effectively trade information sharing for reduced memory footprint, we employ the recursive neural network (RsNN) with carefully designed optimization strategies to explore the node semantics in a novel cuboid space. We then go further by showing, for the first time, how an effective incremental learning approach can be developed – with the help of RsNN, our cuboid structure, and a set of novel optimization techniques – to allow a learning framework to quickly and efficiently adapt to a constantly evolving network. We evaluate <sc>Lime</sc> by applying it to three representative network-based tasks, node classification, node clustering and anomaly detection, performing on three large-scale datasets. We compare <sc>Lime</sc> against eleven prior state-of-the-art approaches for learning network representation. Our extensive experiments demonstrate that <sc>Lime</sc> not only reduces the memory footprint by over 80 percent and the processing time over 2x when learning network representation but also delivers comparable performance for downstream processing tasks. We show that our incremental learning method can boost the learning time by up to 20x without compromising the quality of the learned network representation.",science
10.1109/tifs.2021.3131026,preprocessed,IEEE Transactions on Information Forensics and Security,IEEE,2000-01-01 00:00:00,ieeexplore,poligraph: intrusion-tolerant and distributed fake news detection system,https://ieeexplore.ieee.org/document/9627681/,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally (<inline-formula> <tex-math notation=""LaTeX"">${4\%}$ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">${7\%}$ </tex-math></inline-formula>) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach.",science
10.1109/tii.2021.3117861,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-05-01 00:00:00,ieeexplore,toward fairness-aware time-sensitive asynchronous federated learning for critical energy infrastructure,https://ieeexplore.ieee.org/document/9560090/,"Critical energy infrastructure (CEI) systems are vital to underpin the national economy and social development, but vulnerable to cyber attack and data privacy leakage when distributed machine learning technologies are deployed on them. Although federated learning (FL) has promoted distributed collaborative learning while keeping natural compliance with the privacy protection, it is tremendously difficult to schedule edge nodes of CEI collaboratively when asynchronous FL tasks are applied in CEI system, since the CEI system must make an irrevocable immediate decision on whether to hire a participant who arrives and departs dynamically without knowing future information. In this article, we tackle this issue by designing fairness-aware and time-sensitive task allocation mechanisms in asynchronous FL for CEI. First, we design an optimal multidimensional contract to guarantee the reliability, honesty, and fairness, and maximize the learning accuracy for the fixed deadline scenario. Second, we design a multimetric participant recruitment mechanism to control time consumption for the limited budget scenario, prove that the problem of optimizing this mechanism is NP-hard, and propose an <inline-formula><tex-math notation=""LaTeX"">$e$</tex-math></inline-formula>-approximation algorithm accordingly. Finally, extensive experiments using both real-world data and simulated data further demonstrate the effectiveness and efficiency of our proposed mechanisms compared to the state-of-the-art approaches.",science
10.1109/tgrs.2021.3087186,preprocessed,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,3-d gabor convolutional neural network for hyperspectral image classification,https://ieeexplore.ieee.org/document/9460777/,"Due to the detailed spectral information through hundreds of narrow spectral bands provided by hyperspectral image (HSI) data, it can be employed to accurately classify diverse materials of interest, which is one of the core applications of hyperspectral remote sensing technology. In recent years, with the rapid development of deep learning, convolutional neural networks (CNNs) have been successfully applied in many fields, including HSI classification. However, the random gradient descent-based parameter updating scheme is too general and leading to the inefficiency of CNN models. Moreover, the high dimensionality and limited training samples of HSI data also exacerbate the overfitting problem. To tackle these issues, in this article, a novel deep network with multilayer and multibranch architecture, named 3-D Gabor CNN (3DG-CNN), is proposed for HSI classification. More precisely, since the predefined 3-D Gabor filters in multiple scales and orientations could well characterize the internal spatial–spectral structure of HSI data from various perspectives, the 3-D Gabor-modulated kernels (3-D GMKs) are employed to replace the random initialization kernels. Moreover, the specially designed multibranch architecture enables the network to better integrating the scalable property of 3-D Gabor filters; thus, the representative ability and robustness of the extracted features can be greatly improved. Alternatively, the number of network parameters is substantially reduced due to the incorporation of 3-D Gabor modulation, relieving the training complexity and also alleviating the training process from overfitting. Experimental results on four real HSI datasets (including two newly released ones in the literature) have demonstrated that the proposed 3DG-CNN model can achieve better performance than several widely used machine-learning-based and deep-learning-based approaches. For the sake of reproducibility, the codes of the proposed 3DG-CNN model are available at <uri>http://jiasen.tech/papers/</uri>.",science
10.1109/tcbb.2021.3082915,preprocessed,IEEE/ACM Transactions on Computational Biology and Bioinformatics,IEEE,2022-02-01 00:00:00,ieeexplore,graphplas: refined classification of plasmid sequences using assembly graphs,https://ieeexplore.ieee.org/document/9439922/,"Plasmids are extra-chromosomal genetic materials with important markers that affect the function and behaviour of the microorganisms supporting their environmental adaptations. Hence the identification and recovery of such plasmid sequences from assemblies is a crucial task in metagenomics analysis. In the past, machine learning approaches have been developed to separate chromosomes and plasmids. However, there is always a compromise between precision and recall in the existing classification approaches. The similarity of compositions between chromosomes and their plasmids makes it difficult to separate plasmids and chromosomes with high accuracy. However, high confidence classifications are accurate with a significant compromise of recall, and vice versa. Hence, the requirement exists to have more sophisticated approaches to separate plasmids and chromosomes accurately while retaining an acceptable trade-off between precision and recall. We present GraphPlas, a novel approach for plasmid recovery using coverage, composition and assembly graph topology. We evaluated GraphPlas on simulated and real short read assemblies with varying compositions of plasmids and chromosomes. Our experiments show that GraphPlas is able to significantly improve accuracy in detecting plasmid and chromosomal contigs on top of popular state-of-the-art plasmid detection tools. <p>The source code is freely available at: <uri>https://github.com/anuradhawick/GraphPlas</uri>.</p>",science
10.1109/lra.2022.3140793,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,vision-based self-adaptive gripping in a trimodal robotic sorting end-effector,https://ieeexplore.ieee.org/document/9672720/,"Recyclable waste management, which includes sorting as a key process, is a crucial component of maintaining a sustainable ecosystem. The use of robots in sorting could significantly facilitate the production of secondary raw materials from waste in the sense of a recycling economy. However, due to the complex and heterogeneous types of the recyclable items, the conventional robotic gripping end-effectors, which typically come with a fixed structure, are unlikely to hold onto the full range of items to enable separation and recycling. To this end, a trimodal adaptive end-effector is proposed that can be integrated with robotic manipulators to improve their gripping versatility. The end-effector can deploy effective modes of gripping to different objects in response to their size and porosity via gripping mechanisms based on Nano Polyurethane (PU) adhesive gels, pumpless vacuum suction, and radially deployable claws. While the end-effector's mechanical design allows the three gripping modes to be deployed independently or in conjunction with one another, this work aims at deploying modes that are effective for gripping onto the recyclable item. In order to decide on the suitable modes of gripping a real-time vision system is designed to measure the size and porosity of the recyclable items and advise on a suitable combination of gripping modes to be deployed. Integrated current sensors provide an indication of successful gripping and releasing of the recyclable items. The results of the experiments confirmed the ability of our vision-based approach in identifying suitable gripping modes in real-time, the deployment of the relevant mechanisms and successful gripping onto a maximum of 84.8% (single-mode), 90.9% (dual-mode) and 96.9% (triple-mode) of a specified set of recyclable items.",science
http://arxiv.org/abs/2202.10335v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,explainability in machine learning: a pedagogical perspective,http://arxiv.org/abs/2202.10335v1,"Given the importance of integrating of explainability into machine learning,
at present, there are a lack of pedagogical resources exploring this.
Specifically, we have found a need for resources in explaining how one can
teach the advantages of explainability in machine learning. Often pedagogical
approaches in the field of machine learning focus on getting students prepared
to apply various models in the real world setting, but much less attention is
given to teaching students the various techniques one could employ to explain a
model's decision-making process. Furthermore, explainability can benefit from a
narrative structure that aids one in understanding which techniques are
governed by which questions about the data.
  We provide a pedagogical perspective on how to structure the learning process
to better impart knowledge to students and researchers in machine learning,
when and how to implement various explainability techniques as well as how to
interpret the results. We discuss a system of teaching explainability in
machine learning, by exploring the advantages and disadvantages of various
opaque and transparent machine learning models, as well as when to utilize
specific explainability techniques and the various frameworks used to structure
the tools for explainability. Among discussing concrete assignments, we will
also discuss ways to structure potential assignments to best help students
learn to use explainability as a tool alongside any given machine learning
application.
  Data science professionals completing the course will have a birds-eye view
of a rapidly developing area and will be confident to deploy machine learning
more widely. A preliminary analysis on the effectiveness of a recently
delivered course following the structure presented here is included as evidence
supporting our pedagogical approach.",science
http://arxiv.org/abs/2202.10144v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"inferring network structure with unobservable nodes from time series
  data",http://arxiv.org/abs/2202.10144v1,"Network structures play important roles in social, technological and
biological systems. However, the observable nodes and connections in real cases
are often incomplete or unavailable due to measurement errors, private
protection issues, or other problems. Therefore, inferring the complete network
structure is useful for understanding human interactions and complex dynamics.
The existing studies have not fully solved the problem of inferring network
structure with partial information about connections or nodes. In this paper,
we tackle the problem by utilizing time-series data generated by network
dynamics. We regard the network inference problem based on dynamical time
series data as a problem of minimizing errors for predicting states of
observable nodes and proposed a novel data-driven deep learning model called
Gumbel-softmax Inference for Network (GIN) to solve the problem under
incomplete information. The GIN framework includes three modules: a dynamics
learner, a network generator, and an initial state generator to infer the
unobservable parts of the network. We implement experiments on artificial and
empirical social networks with discrete and continuous dynamics. The
experiments show that our method can infer the unknown parts of the structure
and the initial states of the observable nodes with up to 90\% accuracy. The
accuracy declines linearly with the increase of the fractions of unobservable
nodes. Our framework may have wide applications where the network structure is
hard to obtain and the time series data is rich.",science
http://arxiv.org/abs/2202.08450v1,preprocessed,arxiv,arxiv,2022-02-17 00:00:00,arxiv,"design-bench: benchmarks for data-driven offline model-based
  optimization",http://arxiv.org/abs/2202.08450v1,"Black-box model-based optimization (MBO) problems, where the goal is to find
a design input that maximizes an unknown objective function, are ubiquitous in
a wide range of domains, such as the design of proteins, DNA sequences,
aircraft, and robots. Solving model-based optimization problems typically
requires actively querying the unknown objective function on design proposals,
which means physically building the candidate molecule, aircraft, or robot,
testing it, and storing the result. This process can be expensive and time
consuming, and one might instead prefer to optimize for the best design using
only the data one already has. This setting -- called offline MBO -- poses
substantial and different algorithmic challenges than more commonly studied
online techniques. A number of recent works have demonstrated success with
offline MBO for high-dimensional optimization problems using high-capacity deep
neural networks. However, the lack of standardized benchmarks in this emerging
field is making progress difficult to track. To address this, we present
Design-Bench, a benchmark for offline MBO with a unified evaluation protocol
and reference implementations of recent methods. Our benchmark includes a suite
of diverse and realistic tasks derived from real-world optimization problems in
biology, materials science, and robotics that present distinct challenges for
offline MBO. Our benchmark and reference implementations are released at
github.com/rail-berkeley/design-bench and
github.com/rail-berkeley/design-baselines.",science
http://arxiv.org/abs/2202.07785v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,predictability and surprise in large generative models,http://arxiv.org/abs/2202.07785v1,"Large-scale pre-training has recently emerged as a technique for creating
capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG,
Gopher, and many others. In this paper, we highlight a counterintuitive
property of such models and discuss the policy implications of this property.
Namely, these generative models have an unusual combination of predictable loss
on a broad training distribution (as embodied in their ""scaling laws""), and
unpredictable specific capabilities, inputs, and outputs. We believe that the
high-level predictability and appearance of useful capabilities drives rapid
development of such models, while the unpredictable qualities make it difficult
to anticipate the consequences of model deployment. We go through examples of
how this combination can lead to socially harmful behavior with examples from
the literature and real world observations, and we also perform two novel
experiments to illustrate our point about harms from unpredictability.
Furthermore, we analyze how these conflicting properties combine to give model
developers various motivations for deploying these models, and challenges that
can hinder deployment. We conclude with a list of possible interventions the AI
community may take to increase the chance of these models having a beneficial
impact. We intend this paper to be useful to policymakers who want to
understand and regulate AI systems, technologists who care about the potential
policy impact of their work, and academics who want to analyze, critique, and
potentially develop large generative models.",science
http://arxiv.org/abs/2202.07475v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"a real-time system for detecting landslide reports on social media using
  artificial intelligence",http://arxiv.org/abs/2202.07475v1,"This paper presents an online system that leverages social media data in real
time to identify landslide-related information automatically using
state-of-the-art artificial intelligence techniques. The designed system can
(i) reduce the information overload by eliminating duplicate and irrelevant
content, (ii) identify landslide images, (iii) infer geolocation of the images,
and (iv) categorize the user type (organization or person) of the account
sharing the information. The system was deployed in February 2020 online at
https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter
data stream and has been running continuously since then to provide
time-critical information to partners such as British Geological Survey and
European Mediterranean Seismological Centre. We trust this system can both
contribute to harvesting of global landslide data for further research and
support global landslide maps to facilitate emergency response and decision
making.",science
http://arxiv.org/abs/2202.00617v1,preprocessed,arxiv,arxiv,2022-02-01 00:00:00,arxiv,"a general, evolution-inspired reward function for social robotics",http://arxiv.org/abs/2202.00617v1,"The field of social robotics will likely need to depart from a paradigm of
designed behaviours and imitation learning and adopt modern reinforcement
learning (RL) methods to enable robots to interact fluidly and efficaciously
with humans. In this paper, we present the Social Reward Function as a
mechanism to provide (1) a real-time, dense reward function necessary for the
deployment of RL agents in social robotics, and (2) a standardised objective
metric for comparing the efficacy of different social robots. The Social Reward
Function is designed to closely mimic those genetically endowed social
perception capabilities of humans in an effort to provide a simple, stable and
culture-agnostic reward function. Presently, datasets used in social robotics
are either small or significantly out-of-domain with respect to social
robotics. The use of the Social Reward Function will allow larger in-domain
datasets to be collected close to the behaviour policy of social robots, which
will allow both further improvements to reward functions and to the behaviour
policies of social robots. We believe this will be the key enabler to
developing efficacious social robots in the future.",science
http://arxiv.org/abs/2201.08475v1,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,gengnn: a generic fpga framework for graph neural network acceleration,http://arxiv.org/abs/2201.08475v1,"Graph neural networks (GNNs) have recently exploded in popularity thanks to
their broad applicability to ubiquitous graph-related problems such as quantum
chemistry, drug discovery, and high energy physics. However, meeting demand for
novel GNN models and fast inference simultaneously is challenging because of
the gap between the difficulty in developing efficient FPGA accelerators and
the rapid pace of creation of new GNN models. Prior art focuses on the
acceleration of specific classes of GNNs but lacks the generality to work
across existing models or to extend to new and emerging GNN models. In this
work, we propose a generic GNN acceleration framework using High-Level
Synthesis (HLS), named GenGNN, with two-fold goals. First, we aim to deliver
ultra-fast GNN inference without any graph pre-processing for real-time
requirements. Second, we aim to support a diverse set of GNN models with the
extensibility to flexibly adapt to new models. The framework features an
optimized message-passing structure applicable to all models, combined with a
rich library of model-specific components. We verify our implementation
on-board on the Xilinx Alveo U50 FPGA and observe a speed-up of up to 25x
against CPU (6226R) baseline and 13x against GPU (A6000) baseline. Our HLS code
will be open-source on GitHub upon acceptance.",science
http://arxiv.org/abs/2201.03413v1,preprocessed,arxiv,arxiv,2022-01-10 00:00:00,arxiv,systems challenges for trustworthy embodied systems,http://arxiv.org/abs/2201.03413v1,"A new generation of increasingly autonomous and self-learning systems, which
we call embodied systems, is about to be developed. When deploying these
systems into a real-life context we face various engineering challenges, as it
is crucial to coordinate the behavior of embodied systems in a beneficial
manner, ensure their compatibility with our human-centered social values, and
design verifiably safe and reliable human-machine interaction. We are arguing
that raditional systems engineering is coming to a climacteric from embedded to
embodied systems, and with assuring the trustworthiness of dynamic federations
of situationally aware, intent-driven, explorative, ever-evolving, largely
non-predictable, and increasingly autonomous embodied systems in uncertain,
complex, and unpredictable real-world contexts. We are also identifying a
number of urgent systems challenges for trustworthy embodied systems, including
robust and human-centric AI, cognitive architectures, uncertainty
quantification, trustworthy self-integration, and continual analysis and
assurance.",science
http://arxiv.org/abs/2201.03550v1,preprocessed,arxiv,arxiv,2022-01-09 00:00:00,arxiv,"machine learning enabling high-throughput and remote operations at
  large-scale user facilities",http://arxiv.org/abs/2201.03550v1,"Imaging, scattering, and spectroscopy are fundamental in understanding and
discovering new functional materials. Contemporary innovations in automation
and experimental techniques have led to these measurements being performed much
faster and with higher resolution, thus producing vast amounts of data for
analysis. These innovations are particularly pronounced at user facilities and
synchrotron light sources. Machine learning (ML) methods are regularly
developed to process and interpret large datasets in real-time with
measurements. However, there remain conceptual barriers to entry for the
facility general user community, whom often lack expertise in ML, and technical
barriers for deploying ML models. Herein, we demonstrate a variety of
archetypal ML models for on-the-fly analysis at multiple beamlines at the
National Synchrotron Light Source II (NSLS-II). We describe these examples
instructively, with a focus on integrating the models into existing
experimental workflows, such that the reader can easily include their own ML
techniques into experiments at NSLS-II or facilities with a common
infrastructure. The framework presented here shows how with little effort,
diverse ML models operate in conjunction with feedback loops via integration
into the existing Bluesky Suite for experimental orchestration and data
management.",science
http://arxiv.org/abs/2201.02734v1,preprocessed,arxiv,arxiv,2022-01-02 00:00:00,arxiv,building human-like communicative intelligence: a grounded perspective,http://arxiv.org/abs/2201.02734v1,"Modern Artificial Intelligence (AI) systems excel at diverse tasks, from
image classification to strategy games, even outperforming humans in many of
these domains. After making astounding progress in language learning in the
recent decade, AI systems, however, seem to approach the ceiling that does not
reflect important aspects of human communicative capacities. Unlike human
learners, communicative AI systems often fail to systematically generalize to
new data, suffer from sample inefficiency, fail to capture common-sense
semantic knowledge, and do not translate to real-world communicative
situations. Cognitive Science offers several insights on how AI could move
forward from this point. This paper aims to: (1) suggest that the dominant
cognitively-inspired AI directions, based on nativist and symbolic paradigms,
lack necessary substantiation and concreteness to guide progress in modern AI,
and (2) articulate an alternative, ""grounded"", perspective on AI advancement,
inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research.
I review results on 4E research lines in Cognitive Science to distinguish the
main aspects of naturalistic learning conditions that play causal roles for
human language development. I then use this analysis to propose a list of
concrete, implementable components for building ""grounded"" linguistic
intelligence. These components include embodying machines in a
perception-action cycle, equipping agents with active exploration mechanisms so
they can build their own curriculum, allowing agents to gradually develop motor
abilities to promote piecemeal language development, and endowing the agents
with adaptive feedback from their physical and social environment. I hope that
these ideas can direct AI research towards building machines that develop
human-like language abilities through their experiences with the world.",science
10.1016/j.engstruct.2021.113824,preprocessed,Engineering Structures,scopus,2022-02-15,sciencedirect,"explainable machine learning using real, synthetic and augmented fire tests to predict fire resistance and spalling of rc columns",https://api.elsevier.com/content/abstract/scopus_id/85122261194,"This paper presents the development of systematic machine learning (ML) approach to enable explainable and rapid assessment of fire resistance and fire-induced spalling of reinforced concrete (RC) columns. The developed approach comprises an ensemble of three novel ML algorithms namely; random forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL). These algorithms are trained to account for a wide collection of geometric characteristics and material properties, as well as loading conditions to examine fire performance of normal and high strength RC columns by analyzing a comprehensive database of fire tests comprising of over 494 observations. The developed ensemble is also capable of presenting quantifiable insights to ML predictions; thus, breaking free from the notion of “black-box” ML and establishing a solid step towards transparent and explainable ML. Most importantly, this work tackles the scarcity of available fire tests by proposing new techniques to leverage the use of real, synthetic, and augmented fire test observations. The developed ML ensemble has been calibrated and validated for standard and design fire exposures and one-, two-, three- and four-sided fire exposures thus; covering a wide range of practical scenarios present during fire incidents. When fully deployed, the developed ensemble can analyze over 5,000 RC columns in under 60 s; thus, providing an attractive solution for researchers and practitioners. The presented approach can also be easily extended for evaluating fire resistance and spalling of other structural members under varying fire scenarios and loading conditions and hence paves the way to modernize the state of this research area and practice.",science
10.1016/j.comcom.2021.11.011,preprocessed,Computer Communications,scopus,2022-02-01,sciencedirect,ran energy efficiency and failure rate through ann traffic predictions processing,https://api.elsevier.com/content/abstract/scopus_id/85120657977,"In this paper, we focus on the application of ML tools to resource management in a portion of a Radio Access Network (RAN) and, in particular, to Base Station (BS) activation and deactivation, aiming at reducing energy consumption while providing enough capacity to satisfy the variable traffic demand generated by end users. In order to properly decide on BS (de)activation, traffic predictions are needed, and Artificial Neural Networks (ANN) are used for this purpose. Since critical BS (de)activation decisions are not taken in proximity of minima and maxima of the traffic patterns, high accuracy in the traffic estimation is not required at those times, but only close to the times when a decision is taken. This calls for careful processing of the ANN traffic predictions to increase the probability of correct decision. Numerical performance results in terms of energy saving and traffic lost due to incorrect BS deactivations are obtained by simulating algorithms for traffic predictions processing, using real traffic as input. Results suggest that good performance trade-offs can be achieved even in presence of non-negligible traffic prediction errors, if these forecasts are properly processed. The impact of forecast processing for dynamic resource allocation on the BS failure rate is also investigated. Results reveal that conservative approaches better prevent BSs from hardware failure. Nevertheless, the deployment of newer devices, designed for fast dynamic networks, allows the adoption of approaches which frequently activate and deactivate BSs, thus achieving higher energy saving.",science
10.1016/j.simpat.2021.102446,preprocessed,Simulation Modelling Practice and Theory,scopus,2022-02-01,sciencedirect,modelling argumentation in short text: a case of social media debate,https://api.elsevier.com/content/abstract/scopus_id/85120649661,"The technological leaps of artificial intelligence (AI) and the rise of machine learning have triggered significant progress in a plethora of natural language processing (NLP) and natural language understanding tasks. One of these tasks is argumentation mining which has received significant interest in recent years and is regarded as a key domain for future decision-making systems, behaviour modelling, and natural language understanding problems. Until recently, natural language modelling tasks, such as computational argumentation schemes, were often tested in controlled environments, such as persuasive essays, reducing unexpected behaviours that could occur in real-life settings, like a public debate on social media. Additionally, the growing demand for enhancing the trust and the explainability of the AI services has dictated the design and adoption of modelling schemes to increase the confidence in the outcomes of the AI solutions. This paper attempts to explore modelling argumentation in short text and proposes a novel framework for argumentation detection under the name Abstract Framework for Argumentation Detection (AFAD). Moreover, different proof-of-concept implementations are provided to examine the applicability of the proposed framework to very short text developing a rule-based mechanism and compare the results with data-driven solutions. Eventually, a combination of the deployed methods is applied increasing the correct predictions in the minority class on an imbalanced dataset. The findings suggest that the modelling process provides solid grounds for technical research while the hybrid solutions have the potential to be applied to a wide range of NLP-related tasks offering a deeper understanding of human language and reasoning.",science
10.1016/j.apenergy.2021.118085,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,ship energy management system development and experimental evaluation utilizing marine loading cycles based on machine learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85120648001,"In order to develop energy management systems for hybrid ship propulsion plants that are truly optimal and robust, it is important that the test conditions in experimental facilities are as close as possible to real world applications. In this context, a framework for the design and experimental evaluation of power-split control systems for ship propulsion is proposed. Using machine learning, data from ship operation are processed and 20 loading patterns are recognized; representative templates are extracted to be used as marine loading cycles in the energy management system development and testing. A ship propulsion model with wave disturbance is utilized to simulate realistic loading scenarios on the experimental facility. A predictive energy management system is presented, that controls the diesel engine and the electric motor/generator based on a strategy that defines the trade-off between fuel consumption and NOx emissions minimization. In addition the propeller load characteristics that are estimated and a speed predictor are utilized to aid the optimization within the 10 s prediction time window. A parametric simulation study is performed for the trade-off evaluation between fuel consumption and NOx emissions reduction potential of the control scheme. Finally, utilizing an extracted loading cycle, the energy management system is experimentally implemented and tested in real-time operation, where it has to cope with environmental disturbance rejection and follow the desired speed profile while performing the power-split control in respect to the fuel to NOx weighting strategy. Based on the experimental results in a hybrid diesel–electric marine powertrain with a 260 kW diesel engine and a 90 kW electric machine, fuel consumption and NOx emissions reduction by 6% and 8.5% respectively, were achieved over the tested profile. In this framework, the capabilities of the energy management system in realistic operation conditions can be exploited and evaluated.",science
10.1016/j.jwpe.2021.102452,preprocessed,Journal of Water Process Engineering,scopus,2022-02-01,sciencedirect,polyamine-modified polyacrylonitrile fibers for efficient removal of u(vi) from real fluorine-contained low-level radioactive wastewater,https://api.elsevier.com/content/abstract/scopus_id/85119972768,"It is of great significance to develop an adsorbent with high adsorption capacity and excellent resistance to anion and cation interference toward the removal of U(VI). Herein, a novel polyamine-modified polyacrylonitrile-based fiber (PANPA) has been synthesized through hydrothermal method, which can validly remove U(VI) from solution. Combined with mesoscopic, spectral characterization and simulation method, the removal behavior and mechanism of U(VI) from high fluorine uranium-containing wastewater by PANPA are systematically investigated. The results show that, based on the strong coordination principle of polyamine group and UO2
                     2+, PANPA can selectively remove U(VI) from wastewater. In addition, the q
                     
                        max
                      of 459.27 mg g−1 was more than that of many other adsorbent materials. More importantly, PANPA is not affected by high concentration of F−, and exhibits higher distribution coefficient (559,900 mL g−1) and removal efficiency (99.5%) to U(VI) than other coexisting ions in real wastewater. Furthermore, the column experiment was also implemented to remove U(VI). The results indicate that PANPA is a promising material to effectively remove U(VI) from real wastewater produced during the fabrication of nuclear fuel elements.",science
10.1016/j.saa.2021.120347,preprocessed,Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy,scopus,2022-01-15,sciencedirect,rapid discrimination of curcuma longa and curcuma xanthorrhiza using direct analysis in real time mass spectrometry and near infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115004546,"This study describes a newly developed method for the fast and straightforward differentiation of two turmeric species using Direct Analysis in Real Time mass spectrometry and miniaturized Near Infrared spectroscopy. Multivariate analyses (PCA and LDA) were performed on the mass spectrometric data, thus creating a powerful model for the discrimination of Curcumalonga and Curcumaxanthorrhiza. Cross-validation of the model revealed correctness-scores of 100% with 20-fold as well as leave-one-out validation techniques. To further estimate the models prediction power, seven retail samples of turmeric powder were analyzed and assorted to a species. Looking for a fast, non-invasive, cost-efficient and laboratory independent method, miniaturized NIR spectrometers offer an alternative for quality control of turmeric species. However, different technologies implemented to compensate for their small size, lead to different applicability of these spectrometers. Therefore, we investigated the three handheld spectrometers microPHAZIR, MicroNIR 2200 and MicroNIR 1700ES for their application in spice analysis in hyphenation to PCA, LDA and ANN methods used for the discriminant analysis. While microPHAZIR proved to be the most valuable device for differentiating C.longa and C.xanthorrhiza, MicroNIR 1700ES offered the worst results. These findings are interpreted on the basis of a quantum chemical simulation of the NIR spectrum of curcumin as the representative constituent. It was found that the information accessible to MicroNIR 1700ES that is relevant to the analyzed constituents is located in the spectral region prone to interferences with the matrix, likely limiting the performance of this spectrometer in this analytical scenario.",science
10.1016/j.dss.2021.113665,preprocessed,Decision Support Systems,scopus,2022-01-01,sciencedirect,a constraint programming model for making recommendations in personal process management: a design science research approach,https://api.elsevier.com/content/abstract/scopus_id/85115634506,"Decision-making in everyday life has an essential role in effectively completing personal tasks and processes. The complexity of these processes and the resulting cognitive load of managing them may vary significantly. To decrease the cognitive load created by such decision-making efforts and to obtain better outcomes, recommendation systems carry significant potential. In order to investigate the benefits provided by decision support systems (DSS) in personal process management (PPM), we first build a constraint programming (CP) model and a prototype context-aware-mobile application employing this CP model. Then, we evaluate the application and the model via two exemplary real-world scenarios. The scenarios form the core of the experiments conducted with 50 participants. We compare the participants’ planning performances with and without the PPM system with quantitative metrics such as planning times and scenario objective values. In addition, System Usability Scale (SUS) questionnaires and open-ended questions provide qualitative evaluation results. Throughout the study, we apply the Design Science Research methodology to rigorously conduct research activities by proof of concept, proof of use, and proof of value. The empirical results clearly show that our proposed model for PPM is effective, and the developed prototype solution generates positive participant comments as well as a high SUS score. Overall, the prototype PPM system with CP implementation leads to better planning in less time in the planning phase, and it lets the user do fast replanning in the execution phase, which is invaluable in dynamically changing situations such as daily activities.",science
10.1016/j.postharvbio.2021.111741,preprocessed,Postharvest Biology and Technology,scopus,2022-01-01,sciencedirect,multi-output 1-dimensional convolutional neural networks for simultaneous prediction of different traits of fruit based on near-infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115232057,"In spectral data predictive modelling of fresh fruit, often the models are calibrated to predict multiple responses. A common method to deal with such a multi-response predictive modelling is the partial least-squares (PLS2) regression. Recently, deep learning (DL) has shown to outperform partial least-squares (PLS) approaches for single fruit traits prediction. The DL can also be adapted to perform multi-response modelling. This study presents an implementation of DL modelling for multi-response prediction for spectral data of fresh fruit. To show this, a real NIR data set related to SSC and MC measurements in pear fruit was used. Since DL models perform better with larger data sets, a data augmentation procedure was performed prior to data modelling. Furthermore, a comparative study was also performed between two of the most used DL architectures for spectral analysis, their multi-output and single-output variants and a classic baseline model using PLS2. A key point to note that all the DL modelling presented in this study is performed using novel automated optimisation tools such as Bayesian optimisation and Hyperband. The results showed that DL models can be easily adapted by changing the output of the fully connected layers to perform multi-response modelling. In comparison to the PLS2, the multi-response DL model showed ∼13 % lower root mean squared error (RMSE), showing the ease and superiority of handling multi-response by DL models for spectral calibration.",science
10.1109/tie.2021.3090707,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-06-01 00:00:00,ieeexplore,active object detection based on a novel deep q-learning network and long-term learning strategy for the service robot,https://ieeexplore.ieee.org/document/9464751/,"This article focuses on active object detection (AOD), one of the greatest challenges in the robotics field. A novel deep-Q-learning-network-based approach is proposed to utilize more useful status information for enhancing the training efficiency and testing accuracy of AOD by adding the cropped target object (TGOJ) from the current state as a new input. Different from the existing researches, a novel reward function, combing the area factor and distance factor of the bounding box, is designed to make the robot not only get closer to the TGOJ but also obtain a better observation viewpoint. Moreover, to overcome the differences between the training dataset and new environments as well as improving the adaptation of the AOD model, a reward-based long-term learning strategy including a novel training strategy is presented. The comparable experiments and the ablation study have been implemented in an AOD dataset, proving that our method owns better performance and efficiency than the comparable methods. Meanwhile, the experiments in the real-world scenario with a robot indicate the validity of the proposed method.",robotics
10.1109/lra.2022.3146515,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,kineverse: a symbolic articulation model framework for model-agnostic mobile manipulation,https://ieeexplore.ieee.org/document/9695204/,"Service robots in the future need to execute abstract instructions such as “fetch the milk from the fridge”. To translate such instructions into actionable plans, robots require in-depth background knowledge. With regards to interactions with doors and drawers, robots require articulation models that they can use for state estimation and motion planning. Existing frameworks model articulated connections as abstract concepts such as <italic>prismatic</italic>, or <italic>revolute</italic>, but do not provide a parameterized model of these connections for computation. In this letter, we introduce a novel framework that uses symbolic mathematical expressions to model articulated structures – robots and objects alike – in a unified and extensible manner. We provide a theoretical description of this framework, and the operations that are supported by its models, and introduce an architecture to exchange our models in robotic applications, making them as flexible as any other environmental observation. To demonstrate the utility of our approach, we employ our practical implementation <italic>Kineverse</italic> for solving common robotics tasks from state estimation and mobile manipulation, and use it further in real-world mobile robot manipulation.",robotics
10.1109/lra.2022.3146945,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,"tacto: a fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors",https://ieeexplore.ieee.org/document/9697425/,"Simulators perform an important role in prototyping, debugging, and benchmarking new advances in robotics and learning for control. Although many physics engines exist, some aspects of the real world are harder than others to simulate. One of the aspects that have so far eluded accurate simulation is touch sensing. To address this gap, we present TACTO – a fast, flexible, and open-source simulator for vision-based tactile sensors. This simulator allows to render realistic high-resolution touch readings at hundreds of frames per second, and can be easily configured to simulate different vision-based tactile sensors, including DIGIT and OmniTact. In this letter, we detail the principles that drove the implementation of TACTO and how they are reflected in its architecture. We demonstrate TACTO on a perceptual task, by learning to predict grasp stability using touch from 1 million grasps, and on a marble manipulation control task. Moreover, we provide a proof-of-concept that TACTO can be successfully used for Sim2Real applications. We believe that TACTO is a step towards the widespread adoption of touch sensing in robotic applications, and to enable machine learning practitioners interested in multi-modal learning and control.",robotics
10.1109/lra.2021.3123374,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,uncertainty for identifying open-set errors in visual object detection,https://ieeexplore.ieee.org/document/9591346/,"Deployed into an open world, object detectors are prone to open-set errors, false positive detections of object classes not present in the training dataset.We propose GMM-Det, a real-time method for extracting epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified by their low log-probability under all Gaussian Mixture Models. We test two common detector architectures, Faster R-CNN and RetinaNet, across three varied datasets spanning robotics and computer vision. Our results show that GMM-Det consistently outperforms existing uncertainty techniques for identifying and rejecting open-set detections, especially at the low-error-rate operating point required for safety-critical applications. GMM-Det maintains object detection performance, and introduces only minimal computational overhead. We also introduce a methodology for converting existing object detection datasets into specific <italic>open-set</italic> datasets to evaluate open-set performance in object detection.",robotics
10.1109/tro.2021.3084374,preprocessed,IEEE Transactions on Robotics,IEEE,2022-02-01 00:00:00,ieeexplore,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,https://ieeexplore.ieee.org/document/9453856/,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",robotics
10.1109/tfuzz.2020.3033141,preprocessed,IEEE Transactions on Fuzzy Systems,IEEE,2022-01-01 00:00:00,ieeexplore,fuzzy double deep q-network-based gait pattern controller for humanoid robots,https://ieeexplore.ieee.org/document/9237162/,"In this article, the adaptive-network-based fuzzy inference system (ANFIS) is combined with the double deep <italic>Q</italic>-network (DDQN) to realize a fuzzy DDQN (FDDQN) such that a humanoid robot can generate a linear inverted pendulum model-based gait pattern in real time. The FDDQN not only allows the humanoid robot to correct the gait pattern instantly but also improves its stability. The proposed scheme is designed and implemented in a toddler-sized humanoid robot called Louis. First, four pressure sensors are installed on the bottom of the sole and one inertial measurement unit is set up on the trunk of the robot. A wireless communication chip is employed to transfer the data to a computer to determine the required parameters for the robot. Next, a control system based on the Linux operating system is developed. The values of the center of pressure and acceleration obtained with the ANFIS are adopted to train the DDQN. The proposed neural network comprises four layers, and the model is cautiously selected to avoid overfitting. The proposed scheme is verified using a robot simulator and then real-time-tested on Louis. The experimental results indicate that the FDDQN can provide the robot timely feedback during walking as well as helps it in adjusting the gait pattern independently. The balancing of the robot through effective dynamic feedback is similar to the balancing ability of an infant learning to walk.",robotics
10.1109/sii52469.2022.9708826,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,evaluation of variable impedance- and hybrid force/motioncontrollers for learning force tracking skills,https://ieeexplore.ieee.org/document/9708826/,"For robots to perform real-world force interaction tasks with human level dexterity, it is crucial to develop adaptable and compliant force controllers. Learning techniques, especially reinforcement learning, provide a platform to develop adaptable controllers for complex robotic tasks. This paper presents an evaluation of two prominent force control methods, variable impedance control and hybrid force-motion control in a robot learning framework. The controllers are evaluated on a Franka Emika Panda robotic manipulator for a robotic interaction task demanding force and motion tracking using a model-based reinforcement learning algorithm, PILCO. Utilizing the learning framework to find the optimal controller parameters has significantly improved the performance of the controllers. The implementation of the controllers integrated with the robot learning framework is available on https://github.com/martihmy/Compliant_control.",robotics
10.1109/lra.2022.3142433,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,enabling low-cost full surface tactile skin for human robot interaction,https://ieeexplore.ieee.org/document/9681158/,"Realizing full coverage, low-maintenance, and low-cost tactile skin is a <italic>de facto</italic> design dream since the invention of robots. It ensures safety and enables collaborative work protocols for human robot interactions (HRI). The on-robot tactile capability is realized by deploying an array of external sensors or inferring from proprioceptive information that comes with the robot, such as motor torque. However, these methods may be cumbersome, introduce extra management cost, expensive, lack real-world robustness, or require special robot designs. In this letter, we present <italic>SonicSkin</italic>, a low-cost ($2) and easy to deploy system that localizes the on-robot human touch and estimates the touch pressure without actually attaching sensors at potential touch locations. The system requires only a single pair of piezoelectric transducers (<italic>i.e.</italic> one transmitter and one receiver) attached on the target robot and turns the robot itself into a versatile sensor. We present a set of novel algorithms to progressively address the unique challenges posed by our system design. We put together an end-to-end <italic>SonicSkin</italic> system on a Jaco robot arm that runs in real-time, and conducted an extensive real-world study including 57019 actual evaluation datapoints under various challenging conditions from 12 human subjects. <italic>SonicSkin</italic> achieves less than 2 cm localization error for 96.4% of touches, with more than 96.7% cross-correlation similarity between the predicted touch pressure and the ground truth touch pressure.",robotics
10.1109/access.2022.3145969,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,fastmde: a fast cnn architecture for monocular depth estimation at high resolution,https://ieeexplore.ieee.org/document/9690863/,"A depth map helps robots and autonomous vehicles (AVs) visualize the three-dimensional world to navigate and localize neighboring obstacles. However, it is difficult to develop a deep learning model that can estimate the depth map from a single image in real-time. This study proposes a fast monocular depth estimation model named <italic>FastMDE</italic> by optimizing the deep convolutional neural network according to the encoder-decoder architecture. The decoder needs to obtain partial and semantic feature maps from the encoding phase to improve the depth estimation accuracy. Therefore, we designed FastMDE with two effective strategies. The first one involved redesigning the skip connection with the features of the squeeze-excitation module to obtain partial and semantic feature maps of the encoding phase. The second strategy involved redesigning the decoder by using the fusion dense block to permit the usage of high-resolution features that were learned earlier in the network before upsampling. The proposed FastMDE model utilizes only 4.1 M parameters, which is much lesser than the parameters utilized by state-of-art models. Thus, FastDME has a higher accuracy and lower latency than previous models. This study also demonstrates that MDE can leverage deep neural networks in real-time (i.e., 30 fps) with the Linux embedded board Nvidia Jetson Xavier NX. The model can facilitate the development and applications with superior performances and easy deployment on an embedded platform.",robotics
10.1109/lra.2021.3129136,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,ocrtoc: a cloud-based competition and benchmark for robotic grasping and manipulation,https://ieeexplore.ieee.org/document/9619915/,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is <uri>www.ocrtoc.org</uri>, and the OCRTOC software package is available at <uri>https://github.com/OCRTOC/OCRTOC_software_package</uri>.",robotics
10.1109/lra.2022.3143289,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,visuotactile 6d pose estimation of an in-hand object using vision and tactile sensor data,https://ieeexplore.ieee.org/document/9682507/,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",robotics
10.1109/sii52469.2022.9708882,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,reinforcement learning based hierarchical control for path tracking of a wheeled bipedal robot with sim-to-real framework,https://ieeexplore.ieee.org/document/9708882/,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",robotics
10.1109/lra.2022.3146900,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,open simulation environment for learning and practice of robot-assisted surgical suturing,https://ieeexplore.ieee.org/document/9697399/,"Automation has the potential to improve the standard of care but is difficult to realize due to perceptual challenges, especially in soft-tissue surgery. Machine learning can provide solutions, but typically requires large amounts of training data, which is time-consuming to collect. Even with shared platforms, hardware differences can prevent effective sharing of data between institutions. This letter proposes a standardized simulation platform for training and testing algorithms to control surgical robotic systems, which is built upon an open-source simulator, the Asynchronous Multi-Body Framework (AMBF), to enable quick prototyping of different scenes. An illustrative example of a suturing task on a phantom is presented and has formed the basis of a challenge, released to the community. The top-level contribution is the open-source release of a dynamic simulation environment that enables realistic suturing on a phantom, but supporting contributions include its extendable architectural design and a series of algorithmic optimizations to achieve real-time control and collision detection, realistic behavior of the needle and suture, and generation of multi-modal ground-truth data, including labeled depth data. These capabilities enable simulation-based surgical training and support research in machine learning for surgical scene perception and autonomous action.",robotics
10.1109/lra.2022.3145971,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,focus on impact: indoor exploration with intrinsic motivation,https://ieeexplore.ieee.org/document/9691914/,"Exploration of indoor environments has recently experienced a significant interest, also thanks to the introduction of deep neural agents built in a hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on simulated environments. Current state-of-the-art methods employ a dense extrinsic reward that requires the complete a priori knowledge of the layout of the training environment to learn an effective exploration policy. However, such information is expensive to gather in terms of time and resources. In this work, we propose to train the model with a purely intrinsic reward signal to guide exploration, which is based on the impact of the robot’s actions on its internal representation of the environment. So far, impact-based rewards have been employed for simple tasks and in procedurally generated synthetic environments with countable states. Since the number of states observable by the agent in realistic indoor environments is non-countable, we include a neural-based density model and replace the traditional count-based regularization with an estimated pseudo-count of previously visited states. The proposed exploration approach outperforms DRL-based competitors relying on intrinsic rewards and surpasses the agents trained with a dense extrinsic reward computed with the environment layouts. We also show that a robot equipped with the proposed approach seamlessly adapts to point-goal navigation and real-world deployment.",robotics
10.1109/lra.2022.3141150,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,reve-ce: remote embodied visual referring expression in continuous environment,https://ieeexplore.ieee.org/document/9674225/,"Ithas always been a great challenge for the robot to navigate in the visual world following natural language instructions. Recently, several tasks such as the Vision-and-Language Navigation (VLN) and Remote Embodied Visual Referring Expression in Real Indoor Environments (REVERIE) are proposed trying to solve this challenge. And the most significant difference between VLN and REVERIE tasks is that REVERIE uses a higher guidance level instruction. However, the navigation process of REVERIE is implemented in a discrete environment, which is unrealistic in real world scenarios. To make the REVERIE task more consistent with the real physical world, we develop a new task of Remote Embodied Visual Referring Expression in Continuous Environment, namely REVE-CE, in which the agent executes a much longer sequence of low-level actions given language instructions. Furthermore, we propose a multi-branch cross modal attention (MBCMA) framework to solve the proposed REVE-CE task. Extensive experiments are conducted demonstrating that the proposed framework greatly outperforms the state-of-the-art VLN baselines and a new benchmark for the proposed REVE-CE task is built.",robotics
http://arxiv.org/abs/2202.08004v1,preprocessed,arxiv,arxiv,2022-02-16 00:00:00,arxiv,deep koopman operator with control for nonlinear systems,http://arxiv.org/abs/2202.08004v1,"Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. We then design an auxiliary control network to encode
the nonlinear state-dependent control term to model the nonlinearity in control
input. For linear control, this encoded term is considered the new control
variable instead, ensuring the linearity of the embedding space. Then we deploy
Linear Quadratic Regulator (LQR) on the linear embedding space to derive the
optimal control policy and decode the actual control input from the control
net. Experimental results demonstrate that our approach outperforms other
existing methods, reducing the prediction error by order-of-magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and 7 Dof robotic manipulator.",robotics
http://arxiv.org/abs/2202.07064v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"towards hardware implementation of wta for cpg-based control of a
  spiking robotic arm",http://arxiv.org/abs/2202.07064v1,"Biological nervous systems typically perform the control of numerous degrees
of freedom for example in animal limbs. Neuromorphic engineers study these
systems by emulating them in hardware for a deeper understanding and its
possible application to solve complex problems in engineering and robotics.
Central-Pattern-Generators (CPGs) are part of neuro-controllers, typically used
at their last steps to produce rhythmic patterns for limbs movement. Different
patterns and gaits typically compete through winner-take-all (WTA) circuits to
produce the right movements. In this work we present a WTA circuit implemented
in a Spiking-Neural-Network (SNN) processor to produce such patterns for
controlling a robotic arm in real-time. The robot uses spike-based
proportional-integrativederivative (SPID) controllers to keep a commanded joint
position from the winner population of neurons of the WTA circuit. Experiments
demonstrate the feasibility of robotic control with spiking circuits following
brain-inspiration.",robotics
http://arxiv.org/abs/2202.06003v2,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,robust learning from observation with model misspecification,http://arxiv.org/abs/2202.06003v2,"Imitation learning (IL) is a popular paradigm for training policies in
robotic systems when specifying the reward function is difficult. However,
despite the success of IL algorithms, they impose the somewhat unrealistic
requirement that the expert demonstrations must come from the same domain in
which a new imitator policy is to be learned. We consider a practical setting,
where (i) state-only expert demonstrations from the real (deployment)
environment are given to the learner, (ii) the imitation learner policy is
trained in a simulation (training) environment whose transition dynamics is
slightly different from the real environment, and (iii) the learner does not
have any access to the real environment during the training phase beyond the
batch of demonstrations given. Most of the current IL methods, such as
generative adversarial imitation learning and its state-only variants, fail to
imitate the optimal expert behavior under the above setting. By leveraging
insights from the Robust reinforcement learning (RL) literature and building on
recent adversarial imitation approaches, we propose a robust IL algorithm to
learn policies that can effectively transfer to the real environment without
fine-tuning. Furthermore, we empirically demonstrate on continuous-control
benchmarks that our method outperforms the state-of-the-art state-only IL
method in terms of the zero-shot transfer performance in the real environment
and robust performance under different testing conditions.",robotics
http://arxiv.org/abs/2201.09857v2,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,"stops: short-term-based volatility-controlled policy search and its
  global convergence",http://arxiv.org/abs/2201.09857v2,"It remains challenging to deploy existing risk-averse approaches to
real-world applications. The reasons are multi-fold, including the lack of
global optimality guarantee and the necessity of learning from long-term
consecutive trajectories. Long-term consecutive trajectories are prone to
involving visiting hazardous states, which is a major concern in the
risk-averse setting. This paper proposes \textbf{\ul{S}}hort-\textbf{\ul{T}}erm
V\textbf{\ul{O}}latility-controlled \textbf{\ul{P}}olicy \textbf{\ul{S}}earch
(STOPS), a novel algorithm that solves risk-averse problems by learning from
short-term trajectories instead of long-term trajectories. Short-term
trajectories are more flexible to generate, and can avoid the danger of
hazardous state visitations. By using an actor-critic scheme with an
overparameterized two-layer neural network, our algorithm finds a globally
optimal policy at a sublinear rate with proximal policy optimization and
natural policy gradient, with effectiveness comparable to the state-of-the-art
convergence rate of risk-neutral policy-search methods. The algorithm is
evaluated on challenging Mujoco robot simulation tasks under the mean-variance
evaluation metric. Both theoretical analysis and experimental results
demonstrate a state-of-the-art level of STOPS' performance among existing
risk-averse policy search methods.",robotics
http://arxiv.org/abs/2201.05753v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"parameter identification and motion control for articulated rigid body
  robots using differentiable position-based dynamics",http://arxiv.org/abs/2201.05753v1,"Simulation modeling of robots, objects, and environments is the backbone for
all model-based control and learning. It is leveraged broadly across dynamic
programming and model-predictive control, as well as data generation for
imitation, transfer, and reinforcement learning. In addition to fidelity, key
features of models in these control and learning contexts are speed, stability,
and native differentiability. However, many popular simulation platforms for
robotics today lack at least one of the features above. More recently,
position-based dynamics (PBD) has become a very popular simulation tool for
modeling complex scenes of rigid and non-rigid object interactions, due to its
speed and stability, and is starting to gain significant interest in robotics
for its potential use in model-based control and learning. Thus, in this paper,
we present a mathematical formulation for coupling position-based dynamics
(PBD) simulation and optimal robot design, model-based motion control and
system identification. Our framework breaks down PBD definitions and
derivations for various types of joint-based articulated rigid bodies. We
present a back-propagation method with automatic differentiation, which can
integrate both positional and angular geometric constraints. Our framework can
critically provide the native gradient information and perform gradient-based
optimization tasks. We also propose articulated joint model representations and
simulation workflow for our differentiable framework. We demonstrate the
capability of the framework in efficient optimal robot design, accurate
trajectory torque estimation and supporting spring stiffness estimation, where
we achieve minor errors. We also implement impedance control in real robots to
demonstrate the potential of our differentiable framework in human-in-the-loop
applications.",robotics
http://arxiv.org/abs/2201.01369v1,preprocessed,arxiv,arxiv,2022-01-04 00:00:00,arxiv,"using simulation optimization to improve zero-shot policy transfer of
  quadrotors",http://arxiv.org/abs/2201.01369v1,"In this work, we show that it is possible to train low-level control policies
with reinforcement learning entirely in simulation and, then, deploy them on a
quadrotor robot without using real-world data to fine-tune. To render zero-shot
policy transfers feasible, we apply simulation optimization to narrow the
reality gap. Our neural network-based policies use only onboard sensor data and
run entirely on the embedded drone hardware. In extensive real-world
experiments, we compare three different control structures ranging from
low-level pulse-width-modulated motor commands to high-level attitude control
based on nested proportional-integral-derivative controllers. Our experiments
show that low-level controllers trained with reinforcement learning require a
more accurate simulation than higher-level control policies.",robotics
10.1016/j.artint.2021.103637,preprocessed,Artificial Intelligence,scopus,2022-02-01,sciencedirect,online perceptual learning and natural language acquisition for autonomous robots,https://api.elsevier.com/content/abstract/scopus_id/85120333069,"In this work, the problem of bootstrapping knowledge in language and vision for autonomous robots is addressed through novel techniques in grammar induction and word grounding to the perceptual world. In particular, we demonstrate a system, called OLAV, which is able, for the first time, to (1) learn to form discrete concepts from sensory data; (2) ground language (n-grams) to these concepts; (3) induce a grammar for the language being used to describe the perceptual world; and moreover to do all this incrementally, without storing all previous data. The learning is achieved in a loosely-supervised manner from raw linguistic and visual data. Moreover, the learnt model is transparent, rather than a black-box model and is thus open to human inspection. The visual data is collected using three different robotic platforms deployed in real-world and simulated environments and equipped with different sensing modalities, while the linguistic data is collected using online crowdsourcing tools and volunteers. The analysis performed on these robots demonstrates the effectiveness of the framework in learning visual concepts, language groundings and grammatical structure in these three online settings.",robotics
10.1016/j.automatica.2021.110007,preprocessed,Automatica,scopus,2022-01-01,sciencedirect,an analytic layer-wise deep learning framework with applications to robotics,https://api.elsevier.com/content/abstract/scopus_id/85118989490,"Deep learning (DL) has achieved great success in many applications, but it has been less well analyzed from the theoretical perspective. The unexplainable success of black-box DL models has raised questions among scientists and promoted the emergence of the field of explainable artificial intelligence (XAI). In robotics, it is particularly important to deploy DL algorithms in a predictable and stable manner as robots are active agents that need to interact safely with the physical world. This paper presents an analytic deep learning framework for fully connected neural networks, which can be applied for both regression problems and classification problems. Examples for regression and classification problems include online robot control and robot vision. We present two layer-wise learning algorithms such that the convergence of the learning systems can be analyzed. Firstly, an inverse layer-wise learning algorithm for multilayer networks with convergence analysis for each layer is presented to understand the problems of layer-wise deep learning. Secondly, a forward progressive learning algorithm where the deep networks are built progressively by using single hidden layer networks is developed to achieve better accuracy. It is shown that the progressive learning method can be used for fine-tuning of weights from convergence point of view. The effectiveness of the proposed framework is illustrated based on classical benchmark recognition tasks using the MNIST and CIFAR-10 datasets and the results show a good balance between performance and explainability. The proposed method is subsequently applied for online learning of robot kinematics and experimental results on kinematic control of UR5e robot with unknown model are presented.",robotics
10.1109/ccnc49033.2022.9700725,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,mitigating location-based attacks using predication models in vehicular ad-hoc networks,https://ieeexplore.ieee.org/document/9700725/,"The modern world is constantly in a state of technological revolution. Everyday new technological ideas, inventions, and threats emerge. With modern computer software and hardware advancements, we have the emergence of the Internet of Things (IoT). In conjunction, modern car companies have a push from public demand for a fully-autonomous car. To accomplish autonomy, small, and secure Vehicular Ad-Hoc Networks (VANETs) it is necessary to ensure that the systems that rely on connected vehicle data is reliable and accurate. In the event there is a malicious actor manipulating the data through replica and injection attacks or there is a hardware failure yielding inaccurate location information, it is necessary to explore efficient methods for predicting connected vehicles locations such that these systems, which rely on accurate information are not impacted. This study analyzes multiple clustering and prediction models to discover how effectively a multi-layered machine learning approach is able to meet the real-time requirement of future generation smart cities.",autonomous vehicle
10.1109/ccnc49033.2022.9700636,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,balancing latency and accuracy on deep video analytics at the edge,https://ieeexplore.ieee.org/document/9700636/,"Real-time deep video analytic at the edge is an enabling technology for emerging applications, such as vulnerable road user detection for autonomous driving, which requires highly accurate results of model inference within a low latency. In this paper, we investigate the accuracy-latency trade-off in the design and implementation of real-time deep video analytic at the edge. Without loss of generality, we select the widely used YOLO-based object detection and WebRTC-based video streaming for case study. Here, the latency consists of both networking latency caused by video streaming and the processing latency for video encoding/decoding and model inference. We conduct extensive measurements to figure out how the dynamically changing settings of video streaming affect the achieved latency, the quality of video, and further the accuracy of model inference. Based on the findings, we propose a mechanism for adapting video streaming settings (i.e. bitrate, resolution) online to optimize the accuracy of video analytic within latency constraints. The mechanism has proved, through a simulated setup, to be efficient in searching the optimal settings.",autonomous vehicle
10.1109/iccece54139.2022.9712792,preprocessed,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),IEEE,2022-01-16 00:00:00,ieeexplore,design of deep learning based autonomous driving control algorithm,https://ieeexplore.ieee.org/document/9712792/,"In recent years, with the continuous development of the field of artificial intelligence, autonomous driving technology has gained widespread attention. In order to meet the purpose of changing driving behavior and completing driving tasks in real time without human intervention. In this paper, we study the design and implementation process of end-to-end autonomous driving algorithms based on computer vision and deep learning, and explain the elements of algorithm design from a theoretical perspective. The method of continuous steering angle prediction for autonomous driving based on convolutional neural network is proposed, as well as the method of network pre-training and overfitting prevention to improve the training effect and generalization ability. The difference with the traditional end-to-end control methods is that the traditional methods study the problem abstractly as a classification problem, describing the motion in terms of direction with a coarser granularity. The method proposed in this paper treats it as a regression problem, describing the motion in terms of steering angles, which provides a more accurate description of the motion and is more adaptive.",autonomous vehicle
10.1109/wacv51458.2022.00206,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,plugging self-supervised monocular depth into unsupervised domain adaptation for semantic segmentation,https://ieeexplore.ieee.org/document/9707096/,"Although recent semantic segmentation methods have made remarkable progress, they still rely on large amounts of annotated training data, which are often infeasible to collect in the autonomous driving scenario. Previous works usually tackle this issue with Unsupervised Domain Adaptation (UDA), which entails training a network on synthetic images and applying the model to real ones while minimizing the discrepancy between the two domains. Yet, these techniques do not consider additional information that may be obtained from other tasks. Differently, we propose to exploit self-supervised monocular depth estimation to improve UDA for semantic segmentation. On one hand, we deploy depth to realize a plug-in component which can inject complementary geometric cues into any existing UDA method. We further rely on depth to generate a large and varied set of samples to Self-Train the final model. Our whole proposal allows for achieving state-of-the-art performance (58.8 mIoU) in the GTA5 → CS benchmark. Code is available at https://github.com/CVLAB-Unibo/d4-dbst.",autonomous vehicle
10.1109/jas.2021.1003907,preprocessed,IEEE/CAA Journal of Automatica Sinica,IEEE,2022-02-01 00:00:00,ieeexplore,domain-invariant similarity activation map contrastive learning for retrieval-based long-term visual localization,https://ieeexplore.ieee.org/document/9358457/,"Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.",autonomous vehicle
10.1007/s11042-021-11437-3,preprocessed,Multimedia Tools and Applications,Springer,2022-01-01 00:00:00,springer,deep reinforcement learning based control for autonomous vehicles in carla,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11437-3,"Nowadays, Artificial Intelligence (AI) is growing by leaps and bounds in almost all fields of technology, and Autonomous Vehicles (AV) research is one more of them. This paper proposes the using of algorithms based on Deep Learning (DL) in the control layer of an autonomous vehicle. More specifically, Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) are implemented in order to compare results between them. The aim of this work is to obtain a trained model, applying a DRL algorithm, able of sending control commands to the vehicle to navigate properly and efficiently following a determined route. In addition, for each of the algorithms, several agents are presented as a solution, so that each of these agents uses different data sources to achieve the vehicle control commands. For this purpose, an open-source simulator such as CARLA is used, providing to the system with the ability to perform a multitude of tests without any risk into an hyper-realistic urban simulation environment, something that is unthinkable in the real world. The results obtained show that both DQN and DDPG reach the goal, but DDPG obtains a better performance. DDPG perfoms trajectories very similar to classic controller as LQR. In both cases RMSE is lower than 0.1m following trajectories with a range 180-700m. To conclude, some conclusions and future works are commented.",autonomous vehicle
http://arxiv.org/abs/2202.04224v1,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,intelligent autonomous intersection management,http://arxiv.org/abs/2202.04224v1,"Connected Autonomous Vehicles will make autonomous intersection management a
reality replacing traditional traffic signal control. Autonomous intersection
management requires time and speed adjustment of vehicles arriving at an
intersection for collision-free passing through the intersection. Due to its
computational complexity, this problem has been studied only when vehicle
arrival times towards the vicinity of the intersection are known beforehand,
which limits the applicability of these solutions for real-time deployment. To
solve the real-time autonomous traffic intersection management problem, we
propose a reinforcement learning (RL) based multiagent architecture and a novel
RL algorithm coined multi-discount Q-learning. In multi-discount Q-learning, we
introduce a simple yet effective way to solve a Markov Decision Process by
preserving both short-term and long-term goals, which is crucial for
collision-free speed control. Our empirical results show that our RL-based
multiagent solution can achieve near-optimal performance efficiently when
minimizing the travel time through an intersection.",autonomous vehicle
http://arxiv.org/abs/2202.02352v1,preprocessed,arxiv,arxiv,2022-02-04 00:00:00,arxiv,"learning interpretable, high-performing policies for continuous control
  problems",http://arxiv.org/abs/2202.02352v1,"Gradient-based approaches in reinforcement learning (RL) have achieved
tremendous success in learning policies for continuous control problems. While
the performance of these approaches warrants real-world adoption in domains,
such as in autonomous driving and robotics, these policies lack
interpretability, limiting deployability in safety-critical and
legally-regulated domains. Such domains require interpretable and verifiable
control policies that maintain high performance. We propose Interpretable
Continuous Control Trees (ICCTs), a tree-based model that can be optimized via
modern, gradient-based, RL approaches to produce high-performing, interpretable
policies. The key to our approach is a procedure for allowing direct
optimization in a sparse decision-tree-like representation. We validate ICCTs
against baselines across six domains, showing that ICCTs are capable of
learning interpretable policy representations that parity or outperform
baselines by up to 33$\%$ in autonomous driving scenarios while achieving a
$300$x-$600$x reduction in the number of policy parameters against deep
learning baselines.",autonomous vehicle
http://arxiv.org/abs/2202.00091v1,preprocessed,arxiv,arxiv,2022-01-31 00:00:00,arxiv,"query efficient decision based sparse attacks against black-box deep
  learning models",http://arxiv.org/abs/2202.00091v1,"Despite our best efforts, deep learning models remain highly vulnerable to
even tiny adversarial perturbations applied to the inputs. The ability to
extract information from solely the output of a machine learning model to craft
adversarial perturbations to black-box models is a practical threat against
real-world systems, such as autonomous cars or machine learning models exposed
as a service (MLaaS). Of particular interest are sparse attacks. The
realization of sparse attacks in black-box models demonstrates that machine
learning models are more vulnerable than we believe. Because these attacks aim
to minimize the number of perturbed pixels measured by l_0 norm-required to
mislead a model by solely observing the decision (the predicted label) returned
to a model query; the so-called decision-based attack setting. But, such an
attack leads to an NP-hard optimization problem. We develop an evolution-based
algorithm-SparseEvo-for the problem and evaluate against both convolutional
deep neural networks and vision transformers. Notably, vision transformers are
yet to be investigated under a decision-based attack setting. SparseEvo
requires significantly fewer model queries than the state-of-the-art sparse
attack Pointwise for both untargeted and targeted attacks. The attack
algorithm, although conceptually simple, is also competitive with only a
limited query budget against the state-of-the-art gradient-based whitebox
attacks in standard computer vision tasks such as ImageNet. Importantly, the
query efficient SparseEvo, along with decision-based attacks, in general, raise
new questions regarding the safety of deployed systems and poses new directions
to study and understand the robustness of machine learning models.",autonomous vehicle
http://arxiv.org/abs/2201.05797v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"finding label and model errors in perception data with learned
  observation assertions",http://arxiv.org/abs/2201.05797v1,"ML is being deployed in complex, real-world scenarios where errors have
impactful consequences. In these systems, thorough testing of the ML pipelines
is critical. A key component in ML deployment pipelines is the curation of
labeled training data. Common practice in the ML literature assumes that labels
are the ground truth. However, in our experience in a large autonomous vehicle
development center, we have found that vendors can often provide erroneous
labels, which can lead to downstream safety risks in trained models.
  To address these issues, we propose a new abstraction, learned observation
assertions, and implement it in a system called Fixy. Fixy leverages existing
organizational resources, such as existing (possibly noisy) labeled datasets or
previously trained ML models, to learn a probabilistic model for finding errors
in human- or model-generated labels. Given user-provided features and these
existing resources, Fixy learns feature distributions that specify likely and
unlikely values (e.g., that a speed of 30mph is likely but 300mph is unlikely).
It then uses these feature distributions to score labels for potential errors.
We show that FIxy can automatically rank potential errors in real datasets with
up to 2$\times$ higher precision compared to recent work on model assertions
and standard techniques such as uncertainty sampling.",autonomous vehicle
http://arxiv.org/abs/2201.05518v1,preprocessed,arxiv,arxiv,2022-01-14 00:00:00,arxiv,ugv-uav object geolocation in unstructured environments,http://arxiv.org/abs/2201.05518v1,"A robotic system of multiple unmanned ground vehicles (UGVs) and unmanned
aerial vehicles (UAVs) has the potential for advancing autonomous object
geolocation performance. Much research has focused on algorithmic improvements
on individual components, such as navigation, motion planning, and perception.
In this paper, we present a UGV-UAV object detection and geolocation system,
which performs perception, navigation, and planning autonomously in real scale
in unstructured environment. We designed novel sensor pods equipped with
multispectral (visible, near-infrared, thermal), high resolution (181.6 Mega
Pixels), stereo (near-infrared pair), wide field of view (192 degree HFOV)
array. We developed a novel on-board software-hardware architecture to process
the high volume sensor data in real-time, and we built a custom AI subsystem
composed of detection, tracking, navigation, and planning for autonomous
objects geolocation in real-time.
  This research is the first real scale demonstration of such high speed data
processing capability. Our novel modular sensor pod can boost relevant computer
vision and machine learning research. Our novel hardware-software architecture
is a solid foundation for system-level and component-level research. Our system
is validated through data-driven offline tests as well as a series of field
tests in unstructured environments. We present quantitative results as well as
discussions on key robotic system level challenges which manifest when we build
and test the system. This system is the first step toward a UGV-UAV cooperative
reconnaissance system in the future.",autonomous vehicle
10.1016/j.jag.2021.102652,preprocessed,International Journal of Applied Earth Observation and Geoinformation,scopus,2022-02-01,sciencedirect,developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big uav imagery data,https://api.elsevier.com/content/abstract/scopus_id/85122505895,"The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.",autonomous vehicle
10.1016/j.compag.2021.106574,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,perennial ryegrass biomass retrieval through multispectral uav data,https://api.elsevier.com/content/abstract/scopus_id/85122407603,"Frequent biomass measurement is a key activity for optimal perennial ryegrass (Lolium perenne) management in intensive forage-based dairy operations. Due to the necessary high-frequency (i.e., weekly or monthly) pasture monitoring and continuous trend of larger dairy farms, such activity is perceived as an operational bottleneck. Consequently, substantial effort is directed to the development of accurate and automated technological solutions for biomass assessment. The popularization of unmanned aerial vehicles (UAVs) combined with multispectral cameras should allow for an optimal observational system able to deploy machine learning algorithms for near real-time biomass dry-matter (DM) mapping. For successful operation, these systems should deliver radiometrically accurate orthomosaics and robust models able to generalize across different periods. Nevertheless, the accuracy of radiometric calibration and generalization ability of these models is seldom evaluated. Also, such pipelines should require minimum processing power and allow for fast deployment. This study has established a two-year experiment comparing reflectance measurements between a handheld spectrometer and a commercial multispectral UAV camera. Different algorithms based on regression-tree architecture were contrasted regarding accuracy, speed, and model size. Model performances were validated, providing error-metrics for baseline accuracy and temporal validation. The results have shown that the standard procedure for multispectral imagery radiometric calibration is sub-optimal, requiring further post-processing and presenting low correlation with handheld measurements across spectral bands and dates. Nevertheless, after post-calibration, the use of spectral imagery has presented better baseline error than the point-based sensors, respectively displaying an average of 397.3 and 464.2 kg DM/ha when employed alongside the best performing algorithm (i.e., Cubist). When trained and validated across different years, model performance was largely reduced and deemed unfit for operational purposes. The Cubist/M5 family of algorithms have exhibited advantageous characteristics such as compact model structure, allowing for a higher level of model interpretability, while displaying a smaller size and faster deployment than the Random Forest, Boosted, and Bagged Regression Trees algorithms.",autonomous vehicle
10.1016/j.aap.2021.106473,preprocessed,Accident Analysis and Prevention,scopus,2022-02-01,sciencedirect,mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors,https://api.elsevier.com/content/abstract/scopus_id/85118989110,"Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.",autonomous vehicle
10.1016/j.inffus.2021.09.004,preprocessed,Information Fusion,scopus,2022-02-01,sciencedirect,multimodal earth observation data fusion: graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",autonomous vehicle
10.1016/j.isatra.2022.01.014,preprocessed,ISA Transactions,scopus,2022-01-01,sciencedirect,"intelligent framework for automated failure prediction, detection, and classification of mission critical autonomous flights",https://api.elsevier.com/content/abstract/scopus_id/85123893673,"Autonomous flights are the major industry contributors towards next-generation developments in pervasive and ubiquitous computing. Modern aerial vehicles are designed to receive actuator commands from the primary autopilot software as input to regulate their servos for adjusting control surfaces. Due to real-time interaction with the actual physical environment, there exists a high risk of control surface failures for engine, rudder, elevators, and ailerons etc. If not anticipated and then timely controlled, failures occurring during the flight can have severe and cataclysmic consequences, which may result in mid-air collision or ultimate crash. Humongous amount of sensory data being generated throughout mission-critical flights, makes it an ideal candidate for applying advanced data-driven machine learning techniques to identify intelligent insights related to failures for instant recovery from emergencies. In this paper, we present a novel framework based on machine learning techniques for failure prediction, detection, and classification for autonomous aerial vehicles. The proposed framework utilizes long short-term memory recurrent neural network architecture to analyze time series data and has been applied at the AirLab Failure and Anomaly flight dataset, which is a comprehensive publicly available dataset of various fault types in fixed-wing autonomous aerial vehicles’ control surfaces. The proposed framework is able to predict failure with an average accuracy of 93% and the average time-to-predict a failure is 19 s before the actual occurrence of the failure, which is 10 s better than current state-of-the-art. Failure detection accuracy is 100% and average detection time is 0.74 s after happening of failure, which is 1.28 s better than current state-of-the-art. Failure classification accuracy of proposed framework is 100%. The performance analysis shows the strength of the proposed methodology to be used as a real-time failure prediction and a pseudo-real-time failure detection along with a failure classification framework for eventual deployment with actual mission-critical autonomous flights.",autonomous vehicle
10.1016/j.trc.2021.103499,preprocessed,Transportation Research Part C: Emerging Technologies,scopus,2022-01-01,sciencedirect,do autonomous vehicles drive like humans? a turing approach and an application to sae automation level 2 cars,https://api.elsevier.com/content/abstract/scopus_id/85120490088,"Fully automated vehicles (AVs) are set to become a reality in future decades and changes are to be expected in user perceptions and behavior. While AV acceptability has been widely studied, changes in human drivers’ behavior and in passengers’ reactions have received less attention. It is not yet possible to ascertain the risk of driver behavioral changes such as overreaction, and the corresponding safety problems, in mixed traffic with partially AVs. Nor has there been proper investigation of the potential unease of car occupants trained for human control, when exposed to automatic maneuvers. The conjecture proposed in this paper is that automation Level 2 vehicles do not induce potentially adverse effects in traditional vehicle drivers’ behavior or in occupants’ reactions, provided that they are indistinguishable from human-driven vehicles. To this end, the paper proposes a Turing approach to test the “humanity” of automation Level 2 vehicles. The proposed test was applied to the results of an experimental campaign carried out in Italy: 546 car passengers were interviewed on board Level 2 cars in which they could not see the driver. They were asked whether a specific driving action (braking, accelerating, lane keeping) had been performed by the human driver or by the automatic on-board software under different traffic conditions (congestion and speed). Estimation results show that in most cases the interviewees were unable to distinguish the Artificial Intelligence (AI) from the human driver by observing random responses with a 95% significance level (proportion of success statistically equal to 50%). However, in the case of moderate braking and lane keeping at >100 km/h and in high traffic congestion, respondents recognized AI control from the human driver above pure chance, with 62–69% correct response rates. These findings, if confirmed in other case studies, could significantly impact on AVs acceptability, also contributing to their design as well as to long-debated ethical questions. AI driving software could be designed and tested for “humanity”, as long as safety is guaranteed, and autonomous cars could be allowed to circulate as long as they cannot be distinguished from human-driven vehicles in recurrent driving conditions.",autonomous vehicle
10.1016/j.dsp.2021.103290,preprocessed,Digital Signal Processing: A Review Journal,scopus,2022-01-01,sciencedirect,deep residual learning-based cognitive model for detection and classification of transmitted signal patterns in 5g smart city networks,https://api.elsevier.com/content/abstract/scopus_id/85118634214,"Primary user (PU) signal detection or classification is a critical component of cognitive radio (CR) related wireless communication applications. In CR, the PU detection methods are mostly based on statistical models, and their detection performance heavily relies on the accuracy of assumed models. In this paper, we design a novel detector, dubbed as PU-Net, that dynamically learns the PU activity patterns in a cognitive 5G smart city, where a network of unmanned aerial vehicles (UAVs) is deployed as flying base stations to serve the Internet-of-Things (IoT) users. Unlike the traditional schemes, the PU-Net is free from signal-noise model assumptions and is leveraged through deep residual learning integrated with atrous spatial pyramid pooling (ASPP) to sense the PU's transmitted signal patterns in the network. The PU-Net detects and classifies the active and idle PU states by exploiting the multilevel spatial-temporal features in the signal and noise frames. The proposed model is trained using locally synthesized Rayleigh channel-impaired data with large variability of modulated signals and different noise floor regimes. Additionally, the PU-Net model is blind-tested and evaluated on real-world over-the-air signals and with variable-length frames and varying channel effects at secondary users (SUs). With extensive experiments, it is shown that PU-Net outperforms other benchmark detectors, obtaining an accuracy of 0.9974, with 0.9978 recall and 0.9970 precision in detecting and classifying the PU transmitted signal patterns. Correspondingly, the proposed PU-Net can be adopted for IoT/UAV-assisted communication systems in optimizing spectrum efficiency and resolving the coexistence issues in 5G and beyond networks.",autonomous vehicle
10.1016/j.engappai.2021.104514,preprocessed,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,instance-based defense against adversarial attacks in deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",autonomous vehicle
10.1016/j.inffus.2021.07.004,preprocessed,Information Fusion,scopus,2022-01-01,sciencedirect,saccadefork: a lightweight multi-sensor fusion-based target detector,https://api.elsevier.com/content/abstract/scopus_id/85112374720,"Commercialization of self-driving applications requires precision and reliability of the perception system due to the highly dynamic and complex road environment. Early perception systems either rely on the camera or on LiDAR for moving obstacle detection. With the development of vehicular sensors and deep learning technologies, the multi-view and sensor fusion based convolutional neural network (CNN) model for detection tasks has become a popular research area. In this paper, we present a novel multi-sensor fusion-based CNN model–SaccadeFork–that integrates the image and upsampled LiDAR point clouds as the input. SaccadeFork includes two modules: (1) a lightweight backbone that consists of hourglass convolution feature extraction module and a parallel dilation convolution module for adaptation of the system to different target sizes; (2) an anchor-based detection head. The model also considers deployment of resource-limited edge devices in the vehicle. Two refinement strategies, i.e., Mixup and Swish activation function are also adopted to improve the model. Comparison with a series of latest models on public dataset of KITTI shows that SaccadeFork can achieve the optimal detection accuracy on vehicles and pedestrians under different scenarios. The final model is also deployed and tested on a local dataset collected based on edge devices and low-cost sensor solutions, and the results show that the model can achieve real-time efficiency and high detection accuracy.",autonomous vehicle
10.1109/jiot.2021.3096637,preprocessed,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,an integrated framework for health state monitoring in a smart factory employing iot and big data techniques,https://ieeexplore.ieee.org/document/9481251/,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",health
10.1109/tie.2021.3065616,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-03-01 00:00:00,ieeexplore,health management of dry-type transformer based on broad learning system,https://ieeexplore.ieee.org/document/9380956/,"This article presents a novel health management method of the dry-type transformer to diagnose the early unhealthy behavior and evaluate the transformer's health condition by health score. The health condition diagnosis implemented by a proposed dynamic-weighted-feed-back broad learning system (BLS) (DW-FB-BLS) method, which helps to determine the BLS network structure effectively, and adjusts the weight of features in the online application to avoid reduction of accuracy caused by concept drift. Then, a rational score rule is set to evaluate the health condition of the dry-type transformer by health score, which allows intuitive presentation and preservation of transformer's health condition over a long period. Finally, the effectiveness and validity of the proposed method are verified based on the real field data of dry-type transformer. Satisfactory results for unhealthy behavior diagnosis and health evaluation are obtained, it shows that health management of this article can reflect the real health condition of dry-type transformer appropriately.",health
10.1109/comsnets53615.2022.9668420,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,ml-based device-agnostic human activity detection with wifi sniffer traffic,https://ieeexplore.ieee.org/document/9668420/,"Human Activity Detection plays a pivotal role in smoothly managing the health care for the elderly and those with chronic health conditions in smart home environments. Even though there are several technological advancements made in this area, solutions like smartwatches are costly to afford and the solutions that rely on sensors and cameras suffer from privacy concerns. While wireless channel state information-based approaches can address some of these limitations, these approaches either require special hardware to be deployed or modifications at the WiFi access point. In this paper, we propose to detect human activities in a device-agnostic manner by leveraging passively captured passively captured WiFi MAC-layer traffic with the help of a sniffer. In that way, elderly people and those who suffer from chronic health conditions do not need to put any sensors on their body. This approach is not only cost-effective, but it is also easy to deploy without requiring any changes at the WiFi access point or installing special sensors in the environment. We train and test six off-the-shelf machine learning models on 15+ hours worth of WiFi MAC-layer traffic collected in a home environment. We present a proof-of-concept system prototype that employs this approach. We are able to detect six activities - (a) Walking vs Sitting, (b) Sleeping vs Awake, and (c) Using Phone vs Not Using Phone in three different real-world scenarios. Our evaluation reveals that WiFi MAC-layer traffic has special signatures to detect human activities and we are able to achieve 92.49 % detection accuracy in the best case.",health
10.1109/access.2021.3114590,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,correlation-aware sport training evaluation for players with trust based on mahalanobis distance,https://ieeexplore.ieee.org/document/9543705/,"With the widely-adopted idea of health and longevity, sports have been becoming one of the most popular entertainment ways of the public. For the majority of sports, players need to know about their concrete physical conditions in a real time manner so as to pursue a good sport score or ranking in a competition or a race. Generally, we can achieve the above goal through analyzing and evaluating the daily training scores of each player. However, there are often multiple physical trainings for players and various correlations are existent among them, which significantly decrease the fairness and trust of player training score evaluation and ranking since traditional multi-dimensional data integration solutions are often based on a strong hypothesis, i.e., the involved multiple dimensions are independent with each other. In view of this shortcoming, we introduce the Mahalanobis Distance into the multi-dimensional player training score evaluation and further propose a <underline>c</underline>orrelation-aware <underline>p</underline>layer training score <underline>e</underline>valuation method with trust (abbreviated as CPE<sub>MD</sub>) based on <underline>M</underline>ahalanobis <underline>D</underline>istance. As Mahalanobis Distance can eliminate the hidden linear correlations among the involved multiple dimensions, we can guarantee the fairness and trust of Mahalanobis Distance-based player training score evaluation and ranking results. At last, we use a case study to show the feasibility of CPE<sub>MD</sub> in this paper.",health
10.1109/tie.2021.3068681,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-03-01 00:00:00,ieeexplore,degradation estimation and prediction of electronic packages using data-driven approach,https://ieeexplore.ieee.org/document/9390342/,"Recent trends in automotive electronics such as automated driving will increase the number and complexity of electronics used in safety-relevant applications. Applications in logistics or ridesharing will require a specific year of service rather than the conventional mileage usage. Reliable operations of the electronic systems must be assured at all times, regardless of the usage condition. A more dynamic and on-demand way of assuring the system availability will have to be developed. This article proposes a thermomechanical stress-based prognostics method as a potential solution. The goal is achieved by several novel advancements. On the experimental front, a key microelectronics package is developed to directly apply the prognostics and health management concept using a piezoresistive silicon-based stress sensor. Additional hardware for safe and secure data transmission and data processing is also developed, which is critically required for recording <italic>in situ</italic> and real-time data. On the data management front, proper data-driven approaches have to be identified to handle the unique dataset from the stress sensor employed in this study. The approaches effectively handle the massive amount of data that reveals the important information and automation of the prognostic process and thus to be able to detect, classify, locate, and predict the failure. The statistical techniques for diagnostics and the machine learning algorithms for health assessment and prognostics are also determined to implement the approaches in a simple, fast, but accurate way within the capacity of limited computing power. The proposed prognostics approach is implemented with actual microelectronics packages subjected to harsh accelerated testing conditions. The results corroborate the validity of the proposed prognostics approach.",health
10.1109/jbhi.2021.3119325,preprocessed,IEEE Journal of Biomedical and Health Informatics,IEEE,2022-01-01 00:00:00,ieeexplore,mix-and-interpolate: a training strategy to deal with source-biased medical data,https://ieeexplore.ieee.org/document/9568732/,"Till March 31st, 2021, the coronavirus disease 2019 (COVID-19) had reportedly infected more than 127 million people and caused over 2.5 million deaths worldwide. Timely diagnosis of COVID-19 is crucial for management of individual patients as well as containment of the highly contagious disease. Having realized the clinical value of non-contrast chest computed tomography (CT) for diagnosis of COVID-19, deep learning (DL) based automated methods have been proposed to aid the radiologists in reading the huge quantities of CT exams as a result of the pandemic. In this work, we address an overlooked problem for training deep convolutional neural networks for COVID-19 classification using real-world multi-source data, namely, the <italic>data source bias</italic> problem. The data source bias problem refers to the situation in which certain sources of data comprise only a single class of data, and training with such source-biased data may make the DL models learn to distinguish data sources instead of COVID-19. To overcome this problem, we propose MIx-aNd-Interpolate (MINI), a conceptually simple, easy-to-implement, efficient yet effective training strategy. The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performance upon the baseline (which does not deal with the source bias), and 2) MINI is superior to competing methods in terms of the extent of improvement.",health
10.1109/tsmc.2020.3018102,preprocessed,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2022-02-01 00:00:00,ieeexplore,novel fast and automatic condition monitoring strategy based on small amount of labeled data,https://ieeexplore.ieee.org/document/9194082/,"Signal-based automatic condition monitoring techniques are one of the effective ways to ensure the operational safety of modern industrial systems. Currently, the main challenge is to increase their effectiveness under noisy environment and with limited labeled data. In this article, a novel fast and automatic signal-based fault diagnosis procedure that does not require any kinds of machine learning algorithms is proposed. This procedure is based on the measurement of the similarity in the frequency domain between the collected data and a small amount of labeled reference signals (LRSs). The LRSs are obtained under various known operation conditions using fast Fourier transform (FFT) algorithm. The proposed approach is suitable for a real-time implementation and capable of successfully overcoming the challenge of condition monitoring of rotating machines under noisy environment and with limited labeled data. The merits, fastness, and also robustness against noise of the proposed technique are demonstrated experimentally through different practical applications, as well as compared to state-of-the-art procedures on a database of vibration signals measured under a variety of machine health and working conditions.",health
10.1109/jbhi.2021.3088750,preprocessed,IEEE Journal of Biomedical and Health Informatics,IEEE,2022-01-01 00:00:00,ieeexplore,stroke risk prediction with hybrid deep transfer learning framework,https://ieeexplore.ieee.org/document/9453166/,"Stroke has become a leading cause of death and long-term disability in the world with no effective treatment. Deep learning-based approaches have the potential to outperform existing stroke risk prediction models, but they rely on large well-labeled data. Due to the strict privacy protection policy in health-care systems, stroke data is usually distributed among different hospitals in small pieces. In addition, the positive and negative instances of such data are extremely imbalanced. Transfer learning can solve small data issue by exploiting the knowledge of a correlated domain, especially when multiple source of data are available. In this work, we propose a novel Hybrid Deep Transfer Learning-based Stroke Risk Prediction (HDTL-SRP) scheme to exploit the knowledge structure from multiple correlated sources (i.e., external stroke data, chronic diseases data, such as hypertension and diabetes). The proposed framework has been extensively tested in synthetic and real-world scenarios, and it outperforms the state-of-the-art stroke risk prediction models. It also shows the potential of real-world deployment among multiple hospitals aided with 5 G/B5G infrastructures.",health
10.1109/access.2022.3141913,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,decentralized federated learning for healthcare networks: a case study on tumor segmentation,https://ieeexplore.ieee.org/document/9676574/,"Smart healthcare relies on artificial intelligence (AI) functions for learning and analysis of patient data. Since large and diverse datasets for training of Machine Learning (ML) models can rarely be found in individual medical centers, classical centralized AI requires moving privacy-sensitive data from medical institutions to data centers that process the fused information. Training on data centers thus requires higher communication resource/energy demands while violating privacy. This is considered today as a significant bottleneck in pursuing scientific collaboration across trans-national clinical medical research centers. Recently, federated learning (FL) has emerged as a distributed AI approach that enables the cooperative training of ML models, without the need of sharing patient data. This paper dives into the analysis of different FL methods and proposes a real-time distributed networking framework based on the Message Queuing Telemetry Transport (MQTT) protocol. In particular, we design a number of solutions for ML over networks, based on FL tools relying on a parameter server (PS) and fully decentralized paradigms driven by consensus methods. The proposed approach is validated in the context of brain tumor segmentation, using a modified version of the popular U-NET model with representative clinical datasets obtained from the daily clinical workflow. The FL process is implemented on multiple physically separated machines located in different countries and communicating over the Internet. The real-time test-bed is used to obtain measurements of training accuracy vs. latency trade-offs, and to highlight key operational conditions that affect the performance in real deployments.",health
10.1109/tii.2021.3093905,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,"modeling, detecting, and mitigating threats against industrial healthcare systems: a combined software defined networking and reinforcement learning approach",https://ieeexplore.ieee.org/document/9470933/,"The rise of the Internet of Medical Things introduces the healthcare ecosystem in a new digital era with multiple benefits, such as remote medical assistance, real-time monitoring, and pervasive control. However, despite the valuable healthcare services, this progression raises significant cybersecurity and privacy concerns. In this article, we focus our attention on the IEC 60 870-5-104 protocol, which is widely adopted in industrial healthcare systems. First, we investigate and assess the severity of the IEC 60 870-5-104 cyberattacks by providing a quantitative threat model, which relies on Attack Defence Trees and Common Vulnerability Scoring System v3.1. Next, we introduce an intrusion detection and prevention system (IDPS), which is capable of discriminating and mitigating automatically the IEC 60 870-5-104 cyberattacks. The proposed IDPS takes full advantage of the machine learning (ML) and software defined networking (SDN) technologies. ML is used to detect the IEC 60 870-5-104 cyberattacks, utilizing 1) Transmission Control Protocol/Internet Protocol network flow statistics and 2) IEC 60 870-5-104 payload flow statistics. On the other side, the automated mitigation is transformed into a multiarmed bandit problem, which is solved through a reinforcement learning method called Thomson sampling and SDN. The evaluation analysis demonstrates the efficiency of the proposed IDPS in terms of intrusion detection accuracy and automated mitigation performance. The detection accuracy and the F1 score of the proposed IDPS reach 0.831 and 0.8258, respectively, while the mitigation accuracy is calculated at 0.923.",health
10.1007/978-3-030-64573-1_164,preprocessed,Artificial Intelligence in Medicine,Springer,2022-01-01 00:00:00,springer,aim in endoscopy procedures,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-64573-1_164,"Artificial intelligence (AI) is revolutionizing the way medicine is practiced. In this context, the application of AI algorithms in endoscopy is gaining increasing attention so that modern endoscopy is moving towards more and more assisted/automatic solutions. Several approaches have been carried out in order to improve accuracy in diagnosis and surgical procedures. In this chapter, a general overview of the main contributions in the field is surveyed. Four main categories of applications were identified, namely, (i) detection and diagnosis during endoscopic procedure, (ii) informative frame selection, (iii) mosaicking and surface reconstruction, (iv) augmented reality systems for intraoperative assistance and surgeon training. Discussions on future research directions and implementation in clinical practice are provided.",health
10.1007/978-3-030-80928-7_10,preprocessed,Machine Learning for Critical Internet of Medical Things,Springer,2022-01-01 00:00:00,springer,aiiomt: iomt-based system-enabled artificial intelligence for enhanced smart healthcare systems,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80928-7_10,"The healthcare system has been on the frontline in recent years, researchers have tried to find solutions to different illnesses and sickness by applying various modern methods. But the major difference among them is that in recent years, other powerful new tools have emerged, which could be used as an instrument in the healthcare system and keeping it within reasonable limits. One of those technological tools is the Internet of Medical Things (IoMT) and Artificial intelligence (AI). Recently, AI enabled with IoMT-based systems is causing a paradigm shift in the healthcare zone, and the applicability might yield profit especially in diagnosis, prediction, and treatment of different diseases outbreak. The application of AI enabled with IoT-based systems in the healthcare system can be expediting the diagnoses and monitoring of disease and minimizes the burden of medical processes. Therefore, this chapter reviews the applicability of AiIoMT-based system in healthcare systems and the research challenges in deployment of AiIoMT system. The chapter also proposed an AiIoMT-based framework for diagnosis and monitoring of patients in real time. The model was tested using cytology image dataset and evaluated based on accuracy, sensitivity, specificity, F-score, and precision. The findings show a greater diagnosis accuracy of 99.5%, which shows that the AI model is a promising algorithm for the diagnosis of diseases in an IoMT-based system. The diagnosis, prediction, treatment, screening, and medication in the healthcare system have significantly improved with the continuing expansion in the methods having seriously reduced human intervention in medical practice.",health
http://arxiv.org/abs/2202.10336v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,artificial intelligence for the metaverse: a survey,http://arxiv.org/abs/2202.10336v1,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.",health
http://arxiv.org/abs/2202.04361v2,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,"molecular-scale integration of multi-modal sensing and neuromorphic
  computing with organic electrochemical transistors",http://arxiv.org/abs/2202.04361v2,"Abstract: Bionic learning with fused sensing, memory and processing functions
outperforms artificial neural networks running on silicon chips in terms of
efficiency and footprint. However, digital hardware implementation of bionic
learning suffers from device heterogeneity in sensors and processing cores,
which incurs large hardware, energy and time overheads. Here, we present a
universal solution to simultaneously perform multi-modal sensing, memory and
processing using organic electrochemical transistors with designed architecture
and tailored channel morphology, selective ion injection into the
crystalline/amorphous regions. The resultant device work as either a volatile
receptor that shows multi-modal sensing, or a non-volatile synapse that
features record-high 10-bit analog states, low switching stochasticity and good
retention without the integration of any extra devices. Homogeneous integration
of such devices enables bionic learning functions such as conditioned reflex
and real-time cardiac disease diagnose via reservoir computing, illustrating
the promise for future smart edge health informatics.",health
http://arxiv.org/abs/2202.02559v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,"digital twin of wireless systems: overview, taxonomy, challenges, and
  opportunities",http://arxiv.org/abs/2202.02559v1,"Future wireless services must be focused on improving the quality of life by
enabling various applications, such as extended reality, brain-computer
interaction, and healthcare. These applications have diverse performance
requirements (e.g., user-defined quality of experience metrics, latency, and
reliability) that are challenging to be fulfilled by existing wireless systems.
To meet the diverse requirements of the emerging applications, the concept of a
digital twin has been recently proposed. A digital twin uses a virtual
representation along with security-related technologies (e.g., blockchain),
communication technologies (e.g., 6G), computing technologies (e.g., edge
computing), and machine learning, so as to enable the smart applications. In
this tutorial, we present a comprehensive overview on digital twins for
wireless systems. First, we present an overview of fundamental concepts (i.e.,
design aspects, high-level architecture, and frameworks) of digital twin of
wireless systems. Second, a comprehensive taxonomy is devised for both
different aspects. These aspects are twins for wireless and wireless for twins.
For the twins for wireless aspect, we consider parameters, such as twin objects
design, prototyping, deployment trends, physical devices design, interface
design, incentive mechanism, twins isolation, and decoupling. On the other
hand, for wireless for twins, parameters such as, twin objects access aspects,
security and privacy, and air interface design are considered. Finally, open
research challenges and opportunities are presented along with causes and
possible solutions.",health
http://arxiv.org/abs/2202.01176v1,preprocessed,arxiv,arxiv,2022-02-02 00:00:00,arxiv,epidemic dreams: dreaming about health during the covid-19 pandemic,http://arxiv.org/abs/2202.01176v1,"The continuity hypothesis of dreams suggests that the content of dreams is
continuous with the dreamer's waking experiences. Given the unprecedented
nature of the experiences during COVID-19, we studied the continuity hypothesis
in the context of the pandemic. We implemented a deep-learning algorithm that
can extract mentions of medical conditions from text and applied it to two
datasets collected during the pandemic: 2,888 dream reports (dreaming life
experiences), and 57M tweets mentioning the pandemic (waking life experiences).
The health expressions common to both sets were typical COVID-19 symptoms
(e.g., cough, fever, and anxiety), suggesting that dreams reflected people's
real-world experiences. The health expressions that distinguished the two sets
reflected differences in thought processes: expressions in waking life
reflected a linear and logical thought process and, as such, described
realistic symptoms or related disorders (e.g., nasal pain, SARS, H1N1); those
in dreaming life reflected a thought process closer to the visual and emotional
spheres and, as such, described either conditions unrelated to the virus (e.g.,
maggots, deformities, snakebites), or conditions of surreal nature (e.g., teeth
falling out, body crumbling into sand). Our results confirm that dream reports
represent an understudied yet valuable source of people's health experiences in
the real world.",health
http://arxiv.org/abs/2202.01034v1,preprocessed,arxiv,arxiv,2022-02-02 00:00:00,arxiv,"maintaining fairness across distribution shift: do we have viable
  solutions for real-world applications?",http://arxiv.org/abs/2202.01034v1,"Fairness and robustness are often considered as orthogonal dimensions when
evaluating machine learning models. However, recent work has revealed
interactions between fairness and robustness, showing that fairness properties
are not necessarily maintained under distribution shift. In healthcare
settings, this can result in e.g. a model that performs fairly according to a
selected metric in ""hospital A"" showing unfairness when deployed in ""hospital
B"". While a nascent field has emerged to develop provable fair and robust
models, it typically relies on strong assumptions about the shift, limiting its
impact for real-world applications. In this work, we explore the settings in
which recently proposed mitigation strategies are applicable by referring to a
causal framing. Using examples of predictive models in dermatology and
electronic health records, we show that real-world applications are complex and
often invalidate the assumptions of such methods. Our work hence highlights
technical, practical, and engineering gaps that prevent the development of
robustly fair machine learning models for real-world applications. Finally, we
discuss potential remedies at each step of the machine learning pipeline.",health
http://arxiv.org/abs/2201.07711v1,preprocessed,arxiv,arxiv,2022-01-19 00:00:00,arxiv,enhancing the security & privacy of wearable brain-computer interfaces,http://arxiv.org/abs/2201.07711v1,"Brain computing interfaces (BCI) are used in a plethora of
safety/privacy-critical applications, ranging from healthcare to smart
communication and control. Wearable BCI setups typically involve a head-mounted
sensor connected to a mobile device, combined with ML-based data processing.
Consequently, they are susceptible to a multiplicity of attacks across the
hardware, software, and networking stacks used that can leak users' brainwave
data or at worst relinquish control of BCI-assisted devices to remote
attackers. In this paper, we: (i) analyse the whole-system security and privacy
threats to existing wearable BCI products from an operating system and
adversarial machine learning perspective; and (ii) introduce Argus, the first
information flow control system for wearable BCI applications that mitigates
these attacks. Argus' domain-specific design leads to a lightweight
implementation on Linux ARM platforms suitable for existing BCI use-cases. Our
proof of concept attacks on real-world BCI devices (Muse, NeuroSky, and
OpenBCI) led us to discover more than 300 vulnerabilities across the stacks of
six major attack vectors. Our evaluation shows Argus is highly effective in
tracking sensitive dataflows and restricting these attacks with an acceptable
memory and performance overhead (<15%).",health
http://arxiv.org/abs/2201.07888v1,preprocessed,arxiv,arxiv,2022-01-16 00:00:00,arxiv,"adaptive energy management for self-sustainable wearables in mobile
  health",http://arxiv.org/abs/2201.07888v1,"Wearable devices that integrate multiple sensors, processors, and
communication technologies have the potential to transform mobile health for
remote monitoring of health parameters. However, the small form factor of the
wearable devices limits the battery size and operating lifetime. As a result,
the devices require frequent recharging, which has limited their widespread
adoption. Energy harvesting has emerged as an effective method towards
sustainable operation of wearable devices. Unfortunately, energy harvesting
alone is not sufficient to fulfill the energy requirements of wearable devices.
This paper studies the novel problem of adaptive energy management towards the
goal of self-sustainable wearables by using harvested energy to supplement the
battery energy and to reduce manual recharging by users. To solve this problem,
we propose a principled algorithm referred as AdaEM. There are two key ideas
behind AdaEM. First, it uses machine learning (ML) methods to learn predictive
models of user activity and energy usage patterns. These models allow us to
estimate the potential of energy harvesting in a day as a function of the user
activities. Second, it reasons about the uncertainty in predictions and
estimations from the ML models to optimize the energy management decisions
using a dynamic robust optimization (DyRO) formulation. We propose a
light-weight solution for DyRO to meet the practical needs of deployment. We
validate the AdaEM approach on a wearable device prototype consisting of solar
and motion energy harvesting using real-world data of user activities.
Experiments show that AdaEM achieves solutions that are within 5% of the
optimal with less than 0.005% execution time and energy overhead.",health
http://arxiv.org/abs/2201.05115v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,functional anomaly detection: a benchmark study,http://arxiv.org/abs/2201.05115v1,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",health
http://arxiv.org/abs/2202.00478v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,"neurahealthnlp: an automated screening pipeline to detect undiagnosed
  cognitive impairment in electronic health records with deep learning and
  natural language processing",http://arxiv.org/abs/2202.00478v1,"Dementia related cognitive impairment (CI) affects over 55 million people
worldwide and is growing rapidly at the rate of one new case every 3 seconds.
With a recurring failure of clinical trials, early diagnosis is crucial, but
75% of dementia cases go undiagnosed globally with up to 90% in
low-and-middle-income countries. Current diagnostic methods are notoriously
complex, involving manual review of medical notes, numerous cognitive tests,
expensive brain scans or spinal fluid tests. Information relevant to CI is
often found in the electronic health records (EHRs) and can provide vital clues
for early diagnosis, but a manual review by experts is tedious and error prone.
This project develops a novel state-of-the-art automated screening pipeline for
scalable and high-speed discovery of undetected CI in EHRs. To understand the
linguistic context from complex language structures in EHR, a database of 8,656
sequences was constructed to train attention-based deep learning natural
language processing model to classify sequences. A patient level prediction
model based on logistic regression was developed using the sequence level
classifier. The deep learning system achieved 93% accuracy and AUC = 0.98 to
identify patients who had no earlier diagnosis, dementia-related diagnosis
code, or dementia-related medications in their EHR. These patients would have
otherwise gone undetected or detected too late. The EHR screening pipeline was
deployed in NeuraHealthNLP, a web application for automated and real-time CI
screening by simply uploading EHRs in a browser. NeuraHealthNLP is cheaper,
faster, more accessible, and outperforms current clinical methods including
text-based analytics and machine learning approaches. It makes early diagnosis
viable in regions with scarce health care services but accessible internet or
cellular services.",health
http://arxiv.org/abs/2201.04967v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,"adherence forecasting for guided internet-delivered cognitive behavioral
  therapy: a minimally data-sensitive approach",http://arxiv.org/abs/2201.04967v1,"Internet-delivered psychological treatments (IDPT) are seen as an effective
and scalable pathway to improving the accessibility of mental healthcare.
Within this context, treatment adherence is an especially relevant challenge to
address due to the reduced interaction between healthcare professionals and
patients, compared to more traditional interventions. In parallel, there are
increasing regulations when using peoples' personal data, especially in the
digital sphere. In such regulations, data minimization is often a core tenant
such as within the General Data Protection Regulation (GDPR). Consequently,
this work proposes a deep-learning approach to perform automatic adherence
forecasting, while only relying on minimally sensitive login/logout data. This
approach was tested on a dataset containing 342 patients undergoing guided
internet-delivered cognitive behavioral therapy (G-ICBT) treatment. The
proposed Self-Attention Network achieved over 70% average balanced accuracy,
when only 1/3 of the treatment duration had elapsed. As such, this study
demonstrates that automatic adherence forecasting for G-ICBT, is achievable
using only minimally sensitive data, thus facilitating the implementation of
such tools within real-world IDPT platforms.",health
http://arxiv.org/abs/2201.01943v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,"machine learning: algorithms, models, and applications",http://arxiv.org/abs/2201.01943v1,"Recent times are witnessing rapid development in machine learning algorithm
systems, especially in reinforcement learning, natural language processing,
computer and robot vision, image processing, speech, and emotional processing
and understanding. In tune with the increasing importance and relevance of
machine learning models, algorithms, and their applications, and with the
emergence of more innovative uses cases of deep learning and artificial
intelligence, the current volume presents a few innovative research works and
their applications in real world, such as stock trading, medical and healthcare
systems, and software automation. The chapters in the book illustrate how
machine learning and deep learning algorithms and models are designed,
optimized, and deployed. The volume will be useful for advanced graduate and
doctoral students, researchers, faculty members of universities, practicing
data scientists and data engineers, professionals, and consultants working on
the broad areas of machine learning, deep learning, and artificial
intelligence.",health
10.1016/j.physc.2021.1354007,preprocessed,Physica C: Superconductivity and its Applications,scopus,2022-02-15,sciencedirect,optical fibre based quench detection in hts applications using machine learning classifiers,https://api.elsevier.com/content/abstract/scopus_id/85122309535,"A Mach-Zehnder Interferometer (MZI) based optical fibre sensing technique, developed and patented by EPFL, is an efficient and economical way to detect hotspots in High Temperature Superconductor (HTS) applications. Due to the MZI sensitivity being a composite of strain sensitive and temperature sensitive contributions, the MZI gives an instantaneous response to a quench (within 10 ms), because of the quick strain transfer to the optical fibre. However, the MZI output signal can also manifest the environmental noise caused by mechanical vibrations, bubbling in the cryostat and temperature variations, along with the response to the quench. This presents the problems of false alarms and indiscernible response to a quench. Discrete wavelet transform (DWT) has been proven to be a useful tool for feature extraction in different fields requiring signal categorization and hence holds the potential to enable quench recognition in the MZI output. This paper proposes an effective approach of performing DWT based feature extraction on experimental data and subsequently using the extracted features for the MZI response classification using two machine learning based classification techniques: k-nearest neighbours (KNN) and Artificial Neural Network (ANN). For this manuscript, experiments were performed using MZI for quench detection in an HTS tape. Feature extraction was then implemented on these experimental measurements using discrete wavelet coefficients extracted at different decomposition levels from the MZI output; these features were then used to train the KNN and ANN models for identifying quench in the MZI signal. This method could be a valuable supplement to the MZI technique by enabling the development of a real time application that can process the MZI output data as well as eliminate the occurrences of false alarms; thereby facilitating reliable quench detection. With this development, the MZI technique would become an even more attractive solution for the health monitoring of HTS applications.",health
10.1016/j.comnet.2021.108661,preprocessed,Computer Networks,scopus,2022-02-11,sciencedirect,evaluating federated learning for intrusion detection in internet of things: review and challenges,https://api.elsevier.com/content/abstract/scopus_id/85121903251,"The application of Machine Learning (ML) techniques to the well-known intrusion detection systems (IDS) is key to cope with increasingly sophisticated cybersecurity attacks through an effective and efficient detection process. In the context of the Internet of Things (IoT), most ML-enabled IDS approaches use centralized approaches where IoT devices share their data with data centers for further analysis. To mitigate privacy concerns associated with centralized approaches, in recent years the use of Federated Learning (FL) has attracted a significant interest in different sectors, including healthcare and transport systems. However, the development of FL-enabled IDS for IoT is in its infancy, and still requires research efforts from various areas, in order to identify the main challenges for the deployment in real-world scenarios. In this direction, our work evaluates a FL-enabled IDS approach based on a multiclass classifier considering different data distributions for the detection of different attacks in an IoT scenario. In particular, we use three different settings that are obtained by partitioning the recent ToN_IoT dataset according to IoT devices’ IP address and types of attack. Furthermore, we evaluate the impact of different aggregation functions according to such setting by using the recent IBMFL framework as FL implementation. Additionally, we identify a set of challenges and future directions based on the existing literature and the analysis of our evaluation results.",health
10.1016/j.ces.2021.117205,preprocessed,Chemical Engineering Science,scopus,2022-02-02,sciencedirect,"developments of leak detection, diagnostics, and prediction algorithms in multiphase flows",https://api.elsevier.com/content/abstract/scopus_id/85118896502,"Leak detection, diagnostics, and prediction constitute a crucial phase of the flow assurance risk management process for onshore and offshore pipelines. There are a variety of techniques and algorithms that can be deployed to address each aspect. To date, most review papers have concentrated on steady-state and single-phase flow conditions. The goal of the current review is therefore to carry out a thorough analysis of the available leak detection and diagnosis methods by focusing on (i) multiphase flow and transient flow conditions, (ii) model-based and data-driven techniques, (iii) prediction tools, and (iv) performance measures. Detailed assessment of leak detection methods based on accuracy, complexity, data requirement, and cost of installation are discussed. Data-driven techniques are utterly dependent on qualitative and quantitative data available from pipeline systems. Contrastingly data-driven techniques, model-based techniques require less data to achieve leak detection, provided that a nearly accurate base model is available. Different methodologies and technologies can be combined in order to produce the best detection and diagnosis outputs. In many cases, statistical analysis was combined with the Real Time Transient Method (RTTM), which helped to minimize false alarms. The material in this review can be used as a robust guide for the design of diagnostic systems and further research.",health
10.1016/j.cjca.2021.11.009,preprocessed,Canadian Journal of Cardiology,scopus,2022-02-01,sciencedirect,a primer on the present state and future prospects for machine learning and artificial intelligence applications in cardiology,https://api.elsevier.com/content/abstract/scopus_id/85122266625,"The artificial intelligence (AI) revolution is well underway, including in the medical field, and has dramatically transformed our lives. An understanding of the basics of AI applications, their development, and challenges to their clinical implementation is important for clinicians to fully appreciate the possibilities of AI. Such a foundation would ensure that clinicians have a good grasp and realistic expectations for AI in medicine and prevent discrepancies between the promised and real-world impact. When quantifying the track record for AI applications in cardiology, we found that a substantial number of AI systems are never deployed in clinical practice, although there certainly are many success stories. Successful implementations shared the following: they came from clinical areas where large amount of training data was available; were deployable into a single diagnostic modality; prediction models generally had high performance in external validation; and most were developed as part of collaborations with medical device manufacturers who had substantial experience with implementation of new clinical technology. When looking into the current processes used for developing AI-based systems, we suggest that expanding the analytic framework to address potential deployment and implementation issues at project outset will improve the rate of successful implementation, and will be a necessary next step for AI to achieve its full potential in cardiovascular medicine.",health
10.1016/j.compbiomed.2021.105144,preprocessed,Computers in Biology and Medicine,scopus,2022-02-01,sciencedirect,domain generalization on medical imaging classification using episodic training with task augmentation,https://api.elsevier.com/content/abstract/scopus_id/85121969937,"Medical imaging datasets usually exhibit domain shift due to the variations of scanner vendors, imaging protocols, etc. This raises the concern about the generalization capacity of machine learning models. Domain generalization (DG), which aims to learn a model from multiple source domains such that it can be directly generalized to unseen test domains, seems particularly promising to medical imaging community. To address DG, recent model-agnostic meta-learning (MAML) has been introduced, which transfers the knowledge from previous training tasks to facilitate the learning of novel testing tasks. However, in clinical practice, there are usually only a few annotated source domains available, which decreases the capacity of training task generation and thus increases the risk of overfitting to training tasks in the paradigm. In this paper, we propose a novel DG scheme of episodic training with task augmentation on medical imaging classification. Based on meta-learning, we develop the paradigm of episodic training to construct the knowledge transfer from episodic training-task simulation to the real testing task of DG. Motivated by the limited number of source domains in real-world medical deployment, we consider the unique task-level overfitting and we propose task augmentation to enhance the variety during training task generation to alleviate it. With the established learning framework, we further exploit a novel meta-objective to regularize the deep embedding of training domains. To validate the effectiveness of the proposed method, we perform experiments on histopathological images and abdominal CT images.",health
10.1016/j.scs.2021.103559,preprocessed,Sustainable Cities and Society,scopus,2022-02-01,sciencedirect,assessment of sustainable development objectives in smart labs: technology and sustainability at the service of society,https://api.elsevier.com/content/abstract/scopus_id/85120052266,"Sustainable development is the working basis of engineering research and cities are becoming increasingly flexible, inclusive and intelligent. In this context, there is a need for environments that emulate real-life spaces in which cutting-edge technologies can be implemented for subsequent deployment in society. Smart Labs or Living Labs are spaces for innovation, research and experimentation that integrate systems, devices and methodologies focused on people and their environments. The technologies studied and developed in such labs can then be deployed in human spaces to provide intelligence, comfort, health and sustainability. Health and wellness, energy and environment, artificial intelligence, big data and digital rights are some of the disciplines being studied. At the same time, the UN 2030 Agenda provides a comprehensive framework to promote human well-being through the Sustainable Development Goals. In this work, an evaluation model of its indicators in smart environments is performed through a mixed review methodology. The objective of this work is the analysis and implementation of the SDGs in Smart Labs through a literature review and a case study of UJAmI, the smart laboratory of the University of Jaén. The results provide quantitative and qualitative data on the present and future of the smart devices implemented in the UJAmI lab, providing a roadmap for future developments.",health
10.1016/j.jiac.2021.10.027,preprocessed,Journal of Infection and Chemotherapy,scopus,2022-02-01,sciencedirect,a study of quality assessment in sars-cov-2 pathogen nucleic acid amplification tests performance; from the results of external quality assessment survey of clinical laboratories in the tokyo metropolitan government external quality assessment program in 2020,https://api.elsevier.com/content/abstract/scopus_id/85119258737,"Introduction
                  The Tokyo Metropolitan Government (TMG) conducted an external quality assessment (EQA) survey of pathogen nucleic acid amplification tests (NAATs) as a TMG EQA program for SARS-CoV-2 for clinical laboratories in Tokyo.
               
                  Methods
                  We diluted and prepared a standard product manufactured by Company A to about 2,500 copies/mL to make a positive control and distribute it with a negative control. The participants reported the use of the NAATs methods for SARS-CoV-2, the name of the real-time RT-PCR kit, the name of the detection device, the target gene(s), nucleic acid extraction kit, Threshold Cycle value in the case of RT-PCR and the Threshold time value and Differential calculation value in the case of Loop-Mediated Isothermal Amplification (LAMP) method.
               
                  Results
                  As a result, 17 laboratories using fully automated equipment and 34 laboratories using the RT-PCR method reported generally appropriate results in this EQA survey. On the other hand, among the laboratories that adopted the LAMP method, there were a plurality of laboratories that judged positive samples to be negative.
               
                  Conclusion
                  The false negative result is considered to be due to the fact that the amount of virus genome contained in the quality control reagent used this time was below the detection limit of the LAMP method combined with the rapid extraction reagent for influenza virus. On the other hand, false positive results are considered to be due to the non-specific reaction of the NAATs. The EQA program must be continued for the proper implementation of the pathogen NAATs.",health
10.1016/j.ress.2021.108119,preprocessed,Reliability Engineering and System Safety,scopus,2022-02-01,sciencedirect,prognostics and health management (phm): where are we and where do we (need to) go in theory and practice,https://api.elsevier.com/content/abstract/scopus_id/85117331443,"We are performing the digital transition of industry, living the 4th industrial revolution, building a new World in which the digital, physical and human dimensions are interrelated in complex socio-cyber-physical systems. For the sustainability of these transformations, knowledge, information and data must be integrated within model-based and data-driven approaches of Prognostics and Health Management (PHM) for the assessment and prediction of structures, systems and components (SSCs) evolutions and process behaviors, so as to allow anticipating failures and avoiding accidents, thus, aiming at improved safe and reliable design, operation and maintenance.
                  There is already a plethora of methods available for many potential applications and more are being developed: yet, there are still a number of critical problems which impede full deployment of PHM and its benefits in practice. In this respect, this paper does not aim at providing a survey of existing works for an introduction to PHM nor at providing new tools or methods for its further development; rather, it aims at pointing out main challenges and directions of advancements, for full deployment of condition-based and predictive maintenance in practice.",health
10.1016/j.future.2021.08.030,preprocessed,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,a wearable-based posture recognition system with ai-assisted approach for healthcare iot,https://api.elsevier.com/content/abstract/scopus_id/85115908462,"Human posture recognition is a challenging task in the medical healthcare industry, when pursuing intelligence, accuracy, security, privacy, and efficiency, etc. Currently, the main posture recognition methods are captured-behaviors-based visual image analysis and wearable devices-based signal analysis. However, these methods suffer from issues such as high misjudgment rate, high-cost and low-efficiency. To address these issues, we propose a collaborative AI-IoT-based solution (namely, WMHPR) that embeds with advanced AI-assisted approach. In WMHPR, we propose the multi-posture recognition (MPR), an offline algorithm is implemented on wearable hardware, to identify posture based on multi-dimensions data. Meanwhile, an AI-based algorithm running on the cloud server (online), named Cascade-AdaBoosting-CART (CACT), is proposed to further enhance the reliability and accuracy of MPR. We recruit 20 volunteers for real-life experiments to evaluate the effectiveness, and the results show our solution is significantly outstanding in terms of accuracy and reliability while comparing with other typical algorithms.",health
10.1016/j.future.2021.09.010,preprocessed,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,xsru-iomt: explainable simple recurrent units for threat detection in internet of medical things networks,https://api.elsevier.com/content/abstract/scopus_id/85115376405,"The Internet of Medical Things (IoMT) is increasingly replacing the traditional healthcare systems. However, less focus has been paid to their security against cyber-threats in the implementation of the IoMT and its networks. One of the key reasons can be the challenging task of optimizing typical security solutions to the IoMT networks. And despite the rising admiration of machine learning and deep learning methods in the cyber-security domain (e.g., a threat detection system), most of these methods are acknowledged as a black-box model. The explainable AI (XAI) has become progressively vital to understand the employed learning models to improve trust level and empower security experts to interpret the prediction decisions. The authors propose a highly efficient model named XSRU-IoMT, for effective and timely detection of sophisticated attack vectors in IoMT networks. The proposed model is developed using novel bidirectional simple recurrent units (SRU) using the phenomenon of skip connections to eradicate the vanishing gradient problem and achieve a fast training process in recurrent networks. We also explore the concepts of XAI to improve trust level by providing explanations of the predictive decisions and enabling humans and security experts to understand the causal reasoning and underlying data evidence. The evaluation results on the ToN_IoT dataset demonstrate the effectiveness and superiority of the proposed XSRU-IoMT model as compared to the state-of-the-art compelling detection models, suggesting its usefulness as a viable deployment model in real-IoMT networks.",health
10.1016/j.aej.2021.06.024,preprocessed,Alexandria Engineering Journal,scopus,2022-02-01,sciencedirect,"automatic diagnosis of covid-19 disease using deep convolutional neural network with multi-feature channel from respiratory sound data: cough, voice, and breath",https://api.elsevier.com/content/abstract/scopus_id/85109458695,"The problem of respiratory sound classification has received good attention from the clinical scientists and medical researcher’s community in the last year to the diagnosis of COVID-19 disease. The Artificial Intelligence (AI) based models deployed into the real-world to identify the COVID-19 disease from human-generated sounds such as voice/speech, dry cough, and breath. The CNN (Convolutional Neural Network) is used to solve many real-world problems with Artificial Intelligence (AI) based machines. We have proposed and implemented a multi-channeled Deep Convolutional Neural Network (DCNN) for automatic diagnosis of COVID-19 disease from human respiratory sounds like a voice, dry cough, and breath, and it will give better accuracy and performance than previous models. We have applied multi-feature channels such as the data De-noising Auto Encoder (DAE) technique, GFCC (Gamma-tone Frequency Cepstral Coefficients), and IMFCC (Improved Multi-frequency Cepstral Coefficients) methods on augmented data to extract the deep features for the input of the CNN. The proposed approach improves system performance to the diagnosis of COVID-19 disease and provides better results on the COVID-19 respiratory sound dataset.",health
10.1016/j.vaccine.2021.12.014,preprocessed,Vaccine,scopus,2022-01-28,sciencedirect,humoral response to the sars-cov-2 bnt162b2 mrna vaccine: real-world data from a large cohort of healthcare workers,https://api.elsevier.com/content/abstract/scopus_id/85121675841,"Background
                  The SARS-CoV-2 pandemic was responsible for the death of millions of people around the world, which accelerated the study of vaccines. The BNT162b2 mRNA COVID-19 is a messenger RNA vaccine that encodes the spike protein of the virus. However, the duration of the protection conferred by this vaccine and factors associated with immune responses require validation in large cohorts.
               
                  Methods
                  Here, we present data of humoral immune response to vaccination in4264 healthcare workers, tested before (T0) and 15 and 90 days (T1 and T2, respectively) following vaccination.Peripheral blood was collected for immunological analysis using the Quant SARS-CoV-2 IgG II Chemiluminescent Microparticle Immunoassay (CMIA) to determine anti-spike IgG, receptor binding domain (RBD), S1 subunit of SARS-CoV-2.
               
                  Findings
                  At T0, 96·8% (n = 4129) of participants had IgG antibodies non-reactive to anti-SARS-CoV-2. Fifteen days after completing the vaccination, the IgG overall median titer was significantly elevated (21·7x103
                     AU/mL). Both for uni- and multivariate logistic regression analyses women presented higher antibody levels than men, independent of age. Titers were significantly altered among age groups, decreasing by each increase in 10-year of age. At 3 months after completing the vaccination, anti-SARS-CoV-2 IgG titers were 6·3-fold diminished.
                  This real-world post-vaccination data confirmed production of a frequent and elevated anti-SARS-CoV-2 IgG titers, associated with high protection rates. Females and younger participants had higher titer 15 days after vaccination, and despite the significant reduction from 15-to-90 days, those with higher pre-vaccination titers maintained higher levels throughout the remaining timepoints.
               
                  Interpretation
                  These findings support the need to track humoral immunity kinetics to uncover viral susceptibility and eventually implement re-vaccination, particularly in groups prone to lower humoral immune response.
               
                  Funding
                  No external funding was received to conduct this study.",health
10.1016/j.ijhydene.2022.01.145,preprocessed,International Journal of Hydrogen Energy,scopus,2022-01-01,sciencedirect,real-time data-driven fault diagnosis of proton exchange membrane fuel cell system based on binary encoding convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85124312531,"The performance of proton exchange Membrane fuel cell (PEMFC) fault diagnosis system plays an important role in normal operation of PEMFC. Therefore, a new fault diagnosis algorithm based on binary matrix encoding neural network called BinE-CNN is proposed. In BinE-CNN, high-dimensional features are extracted through binary encoding, and the feature maps are transferred to a convolutional neural network (CNN) to realize seven-category fault classification. For development of BinE-CNN, a PEMFC model is modeled to generate simulative datasets. Simulative test precision and Frames per second (FPS) of BinE-CNN have reached respectively 0.973 and 999.8 (better than support vector machines (SVM), long short-term memory neural network (LSTM), etc.). In experimental verification section, fault datasets are collected during bench test. After that, BinE-CNN is deployed on vehicle control unit (VCU) to verify its engineering value (real-time and precision). The result meet both requirements, with time cost of 96.15 ms and precision of 0.931.",health
10.1016/j.medin.2021.12.005,preprocessed,Medicina Intensiva,scopus,2022-01-01,sciencedirect,impact of aspergillus spp. isolation in the first 24 hours of admission in critically ill patients with severe influenza virus pneumonia,https://api.elsevier.com/content/abstract/scopus_id/85124136387,"Objective
                  To determine the incidence and impact of Aspergillus spp. isolation (AI) on ICU mortality in critically ill patients with severe influenza pneumonia during the first 24h of admission.
               
                  Design
                  Secondary analysis of an observational and prospective cohort study.
               
                  Setting
                  ICUs voluntary participating in the Spanish severe Influenza pneumonia registry, between June 2009 and June 2019.
               
                  Patients
                  Consecutive patients admitted to the ICU with diagnosis of severe influenza pneumonia, confirmed by real-time polymerase chain reaction.
               
                  Interventions
                  None.
               
                  Main variables of interest
                  Incidence of AI in respiratory samples. Demographic variables, comorbidities, need for mechanical ventilation and the presence of shock according at admission. Acute Physiology and Chronic Health Evaluation II (APACHE II) scale calculated on ICU admission.
               
                  Results
                  3702 patients were analyzed in this study. AI incidence was 1.13% (n
                     =42). Hematological malignancies (OR 4.39, 95% CI 1.92–10.04); HIV (OR 3.83, 95% CI 1.08–13.63), and other immunosuppression situations (OR 4.87, 95% CI 1.99–11.87) were factors independently associated with the presence of Aspergillus spp. The automatic CHAID decision tree showed that hematologic disease with an incidence of 3.3% was the most closely AI related variable. Hematological disease (OR 2.62 95% CI 1.95–3.51), immunosuppression (OR 2.05 95% CI 1.46–2.88) and AI (OR 3.24, 95% CI 1.60–6.53) were variables independently associated with ICU mortality.
               
                  Conclusions
                  Empirical antifungal treatment in our population may only be justified in immunocompromised patients. In moderate-high risk cases, active search for Aspergillus spp. should be implemented.",health
10.1016/j.fmre.2021.12.005,preprocessed,Fundamental Research,scopus,2022-01-01,sciencedirect,ai-aided on-chip nucleic acid assay for smart diagnosis of infectious disease,https://api.elsevier.com/content/abstract/scopus_id/85122505591,"Global pandemics such as COVID-19 have resulted in significant global social and economic disruption. Although polymerase chain reaction (PCR) is recommended as the standard test for identifying the SARS-CoV-2, conventional assays are time-consuming. In parallel, although artificial intelligence (AI) has been employed to contain the disease, the implementation of AI in PCR analytics, which may enhance the cognition of diagnostics, is quite rare. The information that the amplification curve reveals can reflect the dynamics of reactions. Here, we present a novel AI-aided on-chip approach by integrating deep learning with microfluidic paper-based analytical devices (µPADs) to detect synthetic RNA templates of the SARS-CoV-2 ORF1ab gene. The µPADs feature a multilayer structure by which the devices are compatible with conventional PCR instruments. During analysis, real-time PCR data were synchronously fed to three unsupervised learning models with deep neural networks, including RNN, LSTM, and GRU. Of these, the GRU is found to be most effective and accurate. Based on the experimentally obtained datasets, qualitative forecasting can be made as early as 13 cycles, which significantly enhances the efficiency of the PCR tests by 67.5% (∼40 min). Also, an accurate prediction of the end-point value of PCR curves can be obtained by GRU around 20 cycles. To further improve PCR testing efficiency, we also propose AI-aided dynamic evaluation criteria for determining critical cycle numbers, which enables real-time quantitative analysis of PCR tests. The presented approach is the first to integrate AI for on-chip PCR data analysis. It is capable of forecasting the final output and the trend of qPCR in addition to the conventional end-point Cq calculation. It is also capable of fully exploring the dynamics and intrinsic features of each reaction. This work leverages methodologies from diverse disciplines to provide perspectives and insights beyond the scope of a single scientific field. It is universally applicable and can be extended to multiple areas of fundamental research.",health
10.1016/j.animal.2021.100432,preprocessed,Animal,scopus,2022-01-01,sciencedirect,a machine vision system to predict individual cow feed intake of different feeds in a cowshed,https://api.elsevier.com/content/abstract/scopus_id/85122307007,"Data on individual feed intake of dairy cows, an important variable for farm management, are currently unavailable in commercial dairies. A real-time machine vision system including models that are able to adapt to multiple types of feed was developed to predict individual feed intake of dairy cows. Using a Red-Green-Blue-Depth (RGBD) camera, images of feed piles of two different feed types (lactating cows' feed and heifers' feed) were acquired in a research dairy farm, for a range of feed weights under varied configurations and illuminations. Several models were developed to predict individual feed intake: two Transfer Learning (TL) models based on Convolutional Neural Networks (CNNs), one CNN model trained on both feed types, and one Multilayer Perceptron and Convolutional Neural Network model trained on both feed types, along with categorical data. We also implemented a statistical method to compare these four models using a Linear Mixed Model and a Generalised Linear Mixed Model, showing that all models are significantly different. The TL models performed best and were trained on both feeds with TL methods. These models achieved Mean Absolute Errors (MAEs) of 0.12 and 0.13 kg per meal with RMSE of 0.18 and 0.17 kg per meal for the two different feeds, when tested on varied data collected manually in a cowshed. Testing the model with actual cows’ meals data automatically collected by the system in the cowshed resulted in a MAE of 0.14 kg per meal and RMSE of 0.19 kg per meal. These results suggest the potential of measuring individual feed intake of dairy cows in a cowshed using RGBD cameras and Deep Learning models that can be applied and tuned to different types of feed.",health
10.1016/j.ebiom.2021.103774,preprocessed,eBioMedicine,scopus,2022-01-01,sciencedirect,accuracy and ease-of-use of seven point-of-care sars-cov-2 antigen-detecting tests: a multi-centre clinical evaluation,https://api.elsevier.com/content/abstract/scopus_id/85121647709,"Background
                  Antigen-detecting rapid diagnostic tests (Ag-RDTs) for SARS-CoV-2 are important diagnostic tools. We assessed clinical performance and ease-of-use of seven Ag-RDTs in a prospective, manufacturer-independent, multi-centre cross-sectional diagnostic accuracy study to inform global decision makers.
               
                  Methods
                  Unvaccinated participants suspected of a first SARS-CoV-2 infection were recruited at six sites (Germany, Brazil). Ag-RDTs were evaluated sequentially, with collection of paired swabs for routine reverse transcription polymerase chain reaction (RT-PCR) testing and Ag-RDT testing. Performance was compared to RT-PCR overall and in sub-group analyses (viral load, symptoms, symptoms duration). To understandusability a System Usability Scale (SUS) questionnaire and ease-of-use (EoU) assessment were performed.
               
                  Findings
                  7471 participants were included in the analysis. Sensitivities across Ag-RDTs ranged from 70·4%-90·1%, specificities were above 97·2% for all Ag-RDTs but one (93·1%).Ag-RDTs, Mologic, Bionote, Standard Q, showed diagnostic accuracy in line with WHO targets (> 80% sensitivity, > 97% specificity). All tests showed high sensitivity in the first three days after symptom onset (≥87·1%) and in individuals with viral loads≥ 6 log10SARS-CoV2 RNA copies/mL (≥ 88·7%). Usability varied, with Rapigen, Bionote and Standard Q reaching very good scores; 90, 88 and 84/100, respectively.
               
                  Interpretation
                  Variability in test performance is partially explained by variable viral loads in population evaluated over the course of the pandemic. All Ag-RDTs reach high sensitivity early in the disease and in individuals with high viral loads, supporting their role in identifying transmission relevant infections. For easy-to-use tests, performance shown will likely be maintained in routine implementation.
               
                  Funding
                  Ministry of Science, Research and Arts, State of Baden-Wuerttemberg, Germany, internal funds from Heidelberg University Hospital, University Hospital Charité − Universitätsmedizin Berlin, UK Department of International Development, WHO, Unitaid.",health
10.1016/j.measurement.2021.110491,preprocessed,Measurement: Journal of the International Measurement Confederation,scopus,2022-01-01,sciencedirect,chxcapsnet: deep capsule network with transfer learning for evaluating pneumonia in paediatric chest radiographs,https://api.elsevier.com/content/abstract/scopus_id/85121269671,"Pneumonia is the primary cause of death in children under the age of 5 years. Faster and more accurate laboratory testing aids in the prescription of appropriate treatment for children suspected of having pneumonia, lowering mortality. In this work, we implement a deep neural network model to efficiently evaluate pediatric pneumonia from chest radio graph images. Our network uses a combination of convolutional and capsule layers to capture abstract details as well as low level hidden features from the radio graphic images, allowing the model to generate more generic predictions. Furthermore, we employ transfer learning approach to extract spatial features from the raw input radio graph images, allowing the model to save resources while enhancing performance. The capsule layer weights of the network are updated using the dynamic routing algorithm. The proposed model is evaluated using benchmark pneumonia dataset Kermany et al. 2018, and the outcomes of our experimental studies indicate that the capsules employed in the network enhance the learning of disease level features that are essential in diagnosing pneumonia. According to our comparison studies, the proposed model with Convolution base from InceptionV3 attached with Capsule layers at the end surpasses several existing models by achieving an accuracy of 94.84%. The proposed model is superior in terms of various performance measures such as accuracy and recall, and is well suited to real-time pediatric pneumonia diagnosis, substituting manual chest radiography examination.",health
10.1016/s2589-7500(21)00211-9,preprocessed,The Lancet Digital Health,scopus,2022-01-01,sciencedirect,"deep learning-based classification of kidney transplant pathology: a retrospective, multicentre, proof-of-concept study",https://api.elsevier.com/content/abstract/scopus_id/85120858490,"Background
                  Histopathological assessment of transplant biopsies is currently the standard method to diagnose allograft rejection and can help guide patient management, but it is one of the most challenging areas of pathology, requiring considerable expertise, time, and effort. We aimed to analyse the utility of deep learning to preclassify histology of kidney allograft biopsies into three main broad categories (ie, normal, rejection, and other diseases) as a potential biopsy triage system focusing on transplant rejection.
               
                  Methods
                  We performed a retrospective, multicentre, proof-of-concept study using 5844 digital whole slide images of kidney allograft biopsies from 1948 patients. Kidney allograft biopsy samples were identified by a database search in the Departments of Pathology of the Amsterdam UMC, Amsterdam, Netherlands (1130 patients) and the University Medical Center Utrecht, Utrecht, Netherlands (717 patients). 101 consecutive kidney transplant biopsies were identified in the archive of the Institute of Pathology, RWTH Aachen University Hospital, Aachen, Germany. Convolutional neural networks (CNNs) were trained to classify allograft biopsies as normal, rejection, or other diseases. Three times cross-validation (1847 patients) and deployment on an external real-world cohort (101 patients) were used for validation. Area under the receiver operating characteristic curve (AUROC) was used as the main performance metric (the primary endpoint to assess CNN performance).
               
                  Findings
                  Serial CNNs, first classifying kidney allograft biopsies as normal (AUROC 0·87 [ten times bootstrapped CI 0·85–0·88]) and disease (0·87 [0·86–0·88]), followed by a second CNN classifying biopsies classified as disease into rejection (0·75 [0·73–0·76]) and other diseases (0·75 [0·72–0·77]), showed similar AUROC in cross-validation and deployment on independent real-world data (first CNN normal AUROC 0·83 [0·80–0·85], disease 0·83 [0·73–0·91]; second CNN rejection 0·61 [0·51–0·70], other diseases 0·61 [0·50–0·74]). A single CNN classifying biopsies as normal, rejection, or other diseases showed similar performance in cross-validation (normal AUROC 0·80 [0·73–0·84], rejection 0·76 [0·66–0·80], other diseases 0·50 [0·36–0·57]) and generalised well for normal and rejection classes in the real-world data. Visualisation techniques highlighted rejection-relevant areas of biopsies in the tubulointerstitium.
               
                  Interpretation
                  This study showed that deep learning-based classification of transplant biopsies could support pathological diagnostics of kidney allograft rejection.
               
                  Funding
                  European Research Council; German Research Foundation; German Federal Ministries of Education and Research, Health, and Economic Affairs and Energy; Dutch Kidney Foundation; Human(e) AI Research Priority Area of the University of Amsterdam; and Max-Eder Programme of German Cancer Aid.",health
10.1016/j.ergon.2021.103234,preprocessed,International Journal of Industrial Ergonomics,scopus,2022-01-01,sciencedirect,industrial intelligence in the care of workers’ mental health: a review of status and challenges,https://api.elsevier.com/content/abstract/scopus_id/85120173556,"Mental health is a current concern because people worldwide have been committed to disorders that impair lives as a whole, affecting emotional states, behaviors, and body responses. These disorders decrease worker's productivity, impact industries economically, and cause serious psycho-physical conditions. However, technological advances have leveraged the industry to a novel phase where digitalization and automation provide a new reality. Hence, this industrial transformation may contribute to assists human beings in the workplace with a focus on mental health. This article presents a systematic literature review to investigate studies regarding technologies employed in the care of worker's mental health and the industrial role in this scenario. Three general, three focused, and three descriptive questions highlight the academic progress of industrial concern on mental health, implemented systems and cases, and research challenges. As a result, the review discussed 31 studies, extracted from an initial corpus of 25269, ranging from January 2010 to November 2020. The studies approached stress as the most frequent mental issue in the industry and Support Vector Machine (SVM) as the most used machine learning algorithm, where biomarkers presented the primary data extractors to deal with this theme. Moreover, information fusion methods improved the accuracy of specific cases. However, a growing interest in mental health care has emerged only in recent years, and several challenges require efforts before applying systems in real industrial environments.",health
10.1016/j.suscom.2021.100622,preprocessed,Sustainable Computing: Informatics and Systems,scopus,2022-01-01,sciencedirect,internet of things for sustaining a smart and secure healthcare system,https://api.elsevier.com/content/abstract/scopus_id/85119703892,"The thyroid is a key endocrine gland in the human body that regulates several bodily processes, including protein synthesis, energy consumption, and the body’s reaction to other hormones. Segmentation and volume regeneration of the thyroid is particularly important for identifying thyroid-related diseases since the majority of these problems result in a change in the thyroid’s shape and scale over time. There is an urgent need for research on the disease’s origins and spread. The Internet of Things, cloud computing, and artificial intelligence all provide real-time processing for a variety of applications in the healthcare sector. In healthcare and biomedicine applications, machine learning algorithms are increasingly being utilized to make critical choices. Thyroid patients urgently need a robust and latency-sensitive Quality of Service framework. This paper aims to integrate fog computing and artificial intelligence with smart health to provide a dependable platform for thyroid infection early detection. To identify thyroid patients, a novel ensemble-based classifier is proposed. The thyroid dataset is obtained from the UCI library and the simulation is carried out utilizing Python programming. To increase the framework’s security, encryption and decryption methods are suggested. The suggested framework’s performance is assessed in terms of latency, network use, RAM utilization, and energy consumption. On the other side, the suggested classifier’s accuracy, precision, specificity, sensitivity and F1 score are all assessed. The result demonstrates that the suggested framework and classifier perform consistently better than conventional frameworks and classifiers.",health
10.1016/j.bspc.2021.103123,preprocessed,Biomedical Signal Processing and Control,scopus,2022-01-01,sciencedirect,real-time application based cnn architecture for automatic usct bone image segmentation,https://api.elsevier.com/content/abstract/scopus_id/85114454344,"Artificial Intelligence (AI) in medical image analysis has achieved excellent success in automatic diagnosis in the same way as clinician, especially in the ultrasound field. In this work, we develop a new segmentation application based on various Convolutional Neural Network (CNN) models for Ultrasonic Computed Tomographic (USCT) images. To evaluate the proposed segmentation system, we use different state-of-the-art models for better segmentation performances to train and test the suggested system. We ensure in this work a USCT data augmentation technique based on the Haar wavelet transform and the improved k-means algorithms. Thus, we offer a free dataset for USCT researchers. Moreover, the proposed CNN system is trained and tested using the networks of Adadelta and Adam optimizers. The whole system is implemented on a CPU and a GPU for complexity analysis. High segmentation accuracy has been achieved using the Adadelta optimizer, reaching 99.24%, 99.19%, 99.13% and 99.10% for VGG-Segnet, VGG-Unet, Fully CNN (FCN)-8 and FCN-32 models, respectively. To obtain better results, we use the Adam optimizer to train and test different architectures, and we obtain more competitive results attaining 99.55%, 99.31%, 99.35% and 99.45% for VGG-Segnet, VGG-Unet, FCN-8 and FCN-32, respectively. The achieved results outperform the state of the art in terms of accuracy and time speed up. Moreover, our proposed CNN segmentation confirms the low computational complexity of the system. In addition, our system proves to be a good candidate for medical real-time applications thanks to its implementation on the GPU.",health
10.1109/sii52469.2022.9708896,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,integration of a reconfigurable robotic workcell for assembly operations in automotive industry,https://ieeexplore.ieee.org/document/9708896/,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",industry
10.1109/ccnc49033.2022.9700522,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,a deep reinforcement learning-based resource management scheme for sdn-mec-supported xr applications,https://ieeexplore.ieee.org/document/9700522/,"The Multi-Access Edge Computing (MEC) paradigm provides a promising solution for efficient computing services at edge nodes, such as base stations (BS), access points (AP), etc. By offloading highly intensive computational tasks to MEC servers, critical benefits in terms of reducing energy consumption at mobile devices and lowering processing latency can be achieved to support high Quality of Service (QoS) to many applications. Among the services which would benefit from MEC deployments are eXtended Reality (XR) applications which are receiving increasing attention from both academia and industry. XR applications have high resource requirements, mostly in terms of network bandwidth, computation and storage. Often these resources are not available in classic network architectures and especially not when XR applications are run by mobile devices. This paper leverages the concepts of Software Defined Networking (SDN) and Network Function Virtualization (NFV) to propose an innovative resource management scheme considering heterogeneous QoS requirements at the MEC server level. The resource assignment is formulated by employing a Deep Reinforcement Learning (DRL) technique to support high quality of XR services. The simulation results show how our proposed solution outperforms other state-of-the-art resource management-based schemes.",industry
10.1109/jiot.2021.3079440,preprocessed,IEEE Internet of Things Journal,IEEE,2015-01-15 20:22:00,ieeexplore,deep-learning-enabled automatic optical inspection for module-level defects in lcd,https://ieeexplore.ieee.org/document/9429707/,"Liquid crystal display (LCD) defects detection on module level is increasingly important for flat-panel displays (FPD) industry to increase the production capacity via machine vision technology. However, it is an overwhelmingly challenging issue due to various difficulties. This article discloses a practical automatic optical inspection (AOI) system consisting of hardware structure and software algorithm to detect module-level defects. The AOI system is the core component to build a distributed integrated inspection system with the help of the Internet of Things (IoT). Starting from the analysis of the challenges encountered in module-level defects inspection, a delicate photograph scheme is proposed to reveal different kinds of defects. In order to robustly work on the module-level defects detection with complex situations, a novel framework based on YOLOV3 detection unit is proposed in this article, including the preprocessing module, detection module, defects definition module, and interferences elimination module. To the best of our knowledge, this is the first work that designs a practical AOI system for module-level defects detection. In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on the manufacturing lines. The evaluation of the detection performance of the AOI system in comparison with a manual scheme indicates that the proposed system is practical for module-level defects detection. Currently, the proposed system has been deployed in a real-world LCD manufacturing line from a major player in the world.",industry
10.1109/tcyb.2020.2964011,preprocessed,IEEE Transactions on Cybernetics,IEEE,2022-01-01 00:00:00,ieeexplore,hierarchical granular computing-based model and its reinforcement structural learning for construction of long-term prediction intervals,https://ieeexplore.ieee.org/document/8972350/,"As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.",industry
10.1109/access.2021.3138990,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"microgrid digital twins: concepts, applications, and future trends",https://ieeexplore.ieee.org/document/9663369/,"Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the <italic>digital twinning</italic> concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs’ operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed.",industry
10.1109/tii.2021.3086149,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,toward a web-based digital twin thermal power plant,https://ieeexplore.ieee.org/document/9446630/,"As a crucial part of cyber-physical systems, a digital twin can process data, visualize processes, and send commands to the control system, which can be used for the research on thermal power plants that are vital for providing energy for manufacturing and industry, and also daily consumptions. This article introduces the methodologies and techniques toward a web-based digital twin thermal power plant. To implement a web-based digital twin thermal power plant, the architecture, modeling, control algorithm, rule model, and physical-digital twin control are explored. The potential functionalities of the web-based digital twin including real-time monitoring, visualization and interactions, and provided services for physical thermal plants and universities are also presented. A case study has been provided to illustrate the web-based digital twin power plant. The research in this article can provide potential solutions for web-based digital twin research and education.",industry
10.1109/tii.2021.3093388,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-04-01 00:00:00,ieeexplore,an adaptive deep learning framework for fast recognition of integrated circuit markings,https://ieeexplore.ieee.org/document/9468418/,"Fast recognition of integrated circuit (IC) markings is an essential but challenging task in electronic device manufacturing lines. This article develops an adaptive deep learning framework to facilitate the fast marking recognition of IC chips. The proposed framework contains four deep learning components, namely, chip segmentation, orientation correction, character extraction, and character recognition. The four components utilize different convolutional neural network structures to guarantee excellent adaptivity to a wide range of IC types and mitigate the influence of the low-quality chip images. In particular, the character extraction model is comprised of two improved label generation strategies and a proposed border correction method, so as to accommodate tiny scale chips and compactly printed markings. Experiments from the chip image dataset of a real laptop manufacturing line reached a recognition Precision of 91.73% and the Recall of 92.93%. The results demonstrate the superiority of the proposed framework to the state-of-the-art models and the effectiveness of handling a great diversity of chips with different scales, shapes, text fonts, marking colors, and layouts.",industry
10.1109/tii.2021.3131355,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,dynamic network slicing orchestration for remote adaptation and configuration in industrial iot,https://ieeexplore.ieee.org/document/9629333/,"As an emerging and prospective paradigm, the industrial Internet of Things (IIoT) enable intelligent manufacturing through the interconnection and interaction of industrial production elements. The traditional approach that transmits data in a single physical network is undesirable because such a scheme cannot meet the network requirements of different industrial applications. To address this problem, in this article, we propose a network slicing orchestration system for remote adaptation and configuration in smart factories. We exploit software-defined networking and network functions virtualization to slice the physical network into multiple virtual networks. Different applications can use a dedicated network that meets its requirements with limited network resources with this scheme. To optimize network resource allocation and adapt to the dynamic network environments, we propose two heuristic algorithms with the assistance of artificial intelligence and the theoretical analysis of the network slicing system. We conduct numerical simulations to learn the performance of the proposed algorithms. Our experimental results show the effectiveness and efficiency of our proposed algorithms when multiple network services are concurrently running in the IIoT. Finally, we use a case study to verify the feasibility of the proposed network slicing orchestration system on a real smart manufacturing testbed.",industry
10.1109/tii.2021.3128972,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-05-01 00:00:00,ieeexplore,guest editorial: security and privacy of federated learning solutions for industrial iot applications,https://ieeexplore.ieee.org/document/9619939/,"The Industrial Internet of Things (IoT) typically consists of several thousands of heterogeneous devices, such as sensors, actuators, access points, machinery, end-users' handheld equipment, and supply chain. In such an industrial environment, a multitude of data is generated from massive IoT devices, e.g., sensors for monitoring the environment, reading temperature, and gauging pressure. Most of the data are from delay-sensitive and computation-intensive applications, such as real-time manufacturing and automated diagnostics, which require big data analytics with low latency. Machine learning (ML) has been witnessed as an efficient solution for big data analytics. The majority of such ML algorithms are centralized methods, meaning that they first gather data from different users for use as a training dataset, which is placed on the ML server, and then build a model to classify the new data samples by applying the ML algorithms to this training dataset. However, the access to these datasets in the centralized ML methods raises concerns about data privacy for users. Federated learning (FL) was designed to protect data privacy to address a part of these issues. In FL, each participant uses a global training model without uploading their private data to a third-party server. Compared with the conventional ML, FL can preserve data security, especially in terms of participant data during the learning process. In particular, FL can also help in updating server-side data for the global model, and the participant is not required to provide their data. However, in FL, individual computing units may show abnormal actions, such as faulty software, hardware invasions, unreliable communication channels, and malicious samples deliberately crafting the model. To mitigate these challenges, we require robust policies to control the learning phases in FL. Motivated by the abovementioned issues, this special section solicits original research and practical contributions that advance the security and privacy of the FL solutions for industrial IoT applications as follows.",industry
10.1109/access.2022.3140595,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,integrating artificial intelligence internet of things and 5g for next-generation smartgrid: a survey of trends challenges and prospect,https://ieeexplore.ieee.org/document/9672084/,"Smartgrid is a paradigm that was introduced into the conventional electricity network to enhance the way generation, transmission, and distribution networks interrelate. It involves the use of Information and Communication Technology (ICT) and other solution in fault and intrusion detection, mere monitoring of energy generation, transmission, and distribution. However, on one hand, the actual and earlier smartgrid, do not integrate more advanced features such as automatic decision making, security, scalability, self-healing and awareness, real-time monitoring, cross-layer compatibility, etc. On the other hand, the emergence of the digitalization of the communication infrastructure to support the economic sector which among them are energy generation and distribution grid with Artificial Intelligence (AI) and large-scale Machine to Machine (M2M) communication. With the future Massive Internet of Things (MIoT) as one of the pillars of 5G/6G network factory, it is the enabler to support the next generation smart grid by providing the needed platform that integrates, in addition to the communication infrastructure, the AI and IoT support, providing a multitenant system. This paper aim at presenting a comprehensive review of next smart grid research trends and technological background, discuss a futuristic next-generation smart grid driven by artificial intelligence (AI) and leverage by IoT and 5G. In addition, it discusses the challenges of next-generation smart-grids as it relate to the integration of AI, IoT and 5G for better smart grid architecture. Also, proffers possible solutions to some of the challenges and standards to support this novel trend. A corresponding future work will dwell on the implementation of the discussed integration of AI, IoT and 5G for next-generation smart grid, using Matlab, NS2/NS3, Open-daylight and Mininet as soft tools and compare with related literature.",industry
10.1109/tii.2021.3077865,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,a data stream cleaning system using edge intelligence for smart city industrial environments,https://ieeexplore.ieee.org/document/9424956/,"Cities are becoming smarter because of recent advances in artificial intelligence and the Internet of Things. However, heterogeneous data source in smart cities are continuously producing low-quality data, and ever-growing applications have greater real-time requirements. Therefore, this article proposes a data stream cleaning system (named DSCS) using edge intelligence to utilize the advantages of cloud servers and edge devices. The DSCS in edge nodes consists of a dynamic protocol interpreter, a structure parser, and a cleaning model activator. Meanwhile, a cloud server, which has pools of protocol and structured programs and cleaning models, supports the edge nodes to adapt massive heterogeneous data sources. To validate the proposed data cleaning system, we applied it to two scenarios: monitoring the injection molding machines, and base stations. The DSCS can have a stable processing time when the number of accessed edge devices is increased, as well as a good cleaning effect.",industry
10.1109/access.2022.3145236,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,colonization by algorithms in the fourth industrial revolution,https://ieeexplore.ieee.org/document/9690490/,"Data gathering and information processing have evolved to where it is almost unfathomable how much exists in digital form today. The generation thereof also no longer involves an explicit instruction from human to machine but can happen in real-time without human intervention. Artificial intelligence, machine learning, and cognitive computing are being utilized to mine data from a variety of sources. One such (profitable) source is human beings. Digital algorithms are designed to harness the power of technology to gather information. There has always been a sense of secrecy regarding some information (classified, top secret, confidential, etc.) but the Fourth Industrial Revolution has created the means to gather extremely large amounts of data, unknown to its sources. Anthropological value systems should become a fundamental foundation of digital algorithms. Such an approach could prevent software from exploiting its sources, especially minorities. Value systems together with ethics are guided by people’s culture. In ethically aligned algorithm design, value systems and digital technologies intersect and govern how algorithms are developed, the way data is engaged, and further the discipline of digital humanities.",industry
10.1109/tii.2021.3081417,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,early classification of industrial alarm floods based on semisupervised learning,https://ieeexplore.ieee.org/document/9435070/,"Early classification of ongoing alarm floods in industrial monitoring systems is crucial to provide a safe and efficient operation. It can provide online decision support for plant operators to take timely action, without waiting for the end of an alarm flood. In this article, a data-driven approach is proposed to address the early classification problem with unlabeled historical data. To prioritize earlier activated alarms and take advantage of the triggering time information of alarms, a vector representation called exponentially attenuated component (EAC) is used to represent alarm floods. This makes alarm sequences fit for different powerful machine learning algorithms, which can be easily implemented online with acceptable computational complexities. A method based on the time information of unlabeled historical alarm floods is formulated to determine the attenuation coefficient for EAC representation. With the Gaussian mixture model, an efficient semisupervised approach is proposed to provide an early classification of alarm floods using unlabeled historical data. It includes two phases: offline clustering and online classification, where the clustering step is automated in terms of choosing the optimal number of clusters by applying an efficient cluster validity index. The efficiency of the proposed method is validated by the Tennessee Eastman process benchmark and a real industrial dataset.",industry
10.1109/tii.2021.3130279,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,imitation learning based heavy-hitter scheduling scheme in software-defined industrial networks,https://ieeexplore.ieee.org/document/9626609/,"To realize flexible networking and on-demand topology reconstructing, software-defined industrial networks (SDINs) are increasingly embracing the flat structure. Similar to software defined networks (SDN), SDIN suffers from low traffic scheduling efficiency caused by large and imbalanced flows, known as the heavy hitters problem. Due to such heavy hitters, industrial networks may fail to satisfy application’s QoS requirements, which results in more severe damages. To improve flow scheduling efficiency under heavy hitters, this article introduces a novel imitation learning-based flow scheduling (ILFS) method. ILFS utilizes P4-based In-band Network Telemetry (INT) to collect fine-grained, real-time traffic data from SDIN’s data plane. In the control plane, it integrates the Generative Adversarial Imitation Learning (GAIL) model with a soft actor critic to preserve the experiences of flow, thereby better scheduling large flows. Our experiments thoroughly compare ILFS’s performance with several state-of-the-art traffic scheduling strategies. The results indicate that ILFS successfully controls the link bandwidth the utilization between 10<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and 80<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and significantly improves the average network throughput and link utilization rate.",industry
10.1109/tii.2021.3124848,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,qos and privacy-aware routing for 5g-enabled industrial internet of things: a federated reinforcement learning approach,https://ieeexplore.ieee.org/document/9601174/,"The development and maturity of the fifth-generation (5G) wireless communication technology provides the industrial Internet of Things (IIoT) with ultra-reliable and low-latency communications and massive machine-type communications, and forms a novel IIoT architecture, 5G-IIoT. However, massive data transfer between interconnecting industrial devices also brings new challenges for the 5G-IIoT routing process in terms of latency, load balancing, and data privacy, which affect the development of 5G-IIoT applications. Moreover, the existing research works on IIoT routing mostly focus on the latency and the reliability of the routing, disregarding the privacy security in the routing process. To solve these problems, in this article, we propose a quality of service (QoS) and data privacy-aware routing protocol, named QoSPR, for 5G-IIoT. Specifically, we improve the community detection algorithm info-map to divide the routing area into optimal subdomains, based on which the deep reinforcement learning algorithm is applied to build the gateway deployment model for latency reduction and load-balancing improvement. To eliminate areal differences, while considering the privacy preservation of the routing data, the federated reinforcement learning is applied to obtain the universal gateway deployment model. Then, based on the gateway deployment, the QoS and data privacy-aware routing is accomplished by establishing communications along the load-balancing routes of the minimum latencies. The validation experiment is conducted on real datasets. The experiment results show that as a data privacy-aware routing protocol, the QoSPR can significantly reduce both average latency and maximum latency, while maintaining excellent load balancing in 5G-IIoT.",industry
10.1109/tii.2021.3077005,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,verifiable data mining against malicious adversaries in industrial internet of things,https://ieeexplore.ieee.org/document/9422191/,"With the large-scaled data generated from various interconnected machines and networks, Industrial Internet of Things (IIoT) provides unprecedented opportunities for facilitating data mining for industrial applications. The current IIoT architecture tends to adopt cloud computing for further timely mining IIoT data, however, the openness of security-critical IIoT becomes challenging in terms of unbearable privacy issues. Most existing privacy-preserving data mining (PPDM) techniques are designed to resist honest-but-curious adversaries (i.e., cloud servers and data users). Due to the complexity and openness in IIoT, PPDM is significantly difficult with the presence of malicious adversaries in IIoT who may incur incorrect learned models and inference results. To solve the aforementioned issues, we propose a framework to extend existing PPDM to guard linear regression against malicious behaviors (hereafter referred to as GuardLR). To prevent dishonest computations of cloud servers and inconsistent inputs of data users, we first design a privacy-preserving verifiable learning scheme for linear regression, which guarantees the correctness of learning. In this article, to avoid malicious clouds from returning incorrect inference results, we design a privacy-preserving prediction scheme with lightweight verification. Our formal security analysis shows that GuardLR achieves privacy, completeness, and soundness. Empirical experiments using real-world datasets also demonstrate that GuardLR has high computational efficiency and accuracy.",industry
10.1109/tnse.2021.3075428,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,ai-assisted energy-efficient and intelligent routing for reconfigurable wireless networks,https://ieeexplore.ieee.org/document/9416866/,"Intelligent network management for reconfigurable wireless networks such as 5G and beyond applications is crucial for many industrial applications, and has been the subject of ongoing research. This paper proposes an Artificial Intelligence(AI)-Assisted energy-efficient and intelligent routing, based on both energy efficiency prioritization and AI theory, in order to meet the exacting demands particularly in a real-world scenario. Specifically, to achieve network intelligence and quality of service (QoS), we use the AI theory to enhance routing adaptivity for intelligent network management in reconfigurable wireless networks. The software-defined networking idea is used to achieve this goal from a network-level perspective. To facilitate self-awareness, self-study, self-decision making, and self-configuration, we construct a mathematical model to convert the energy-efficient and intelligent routing problem into a multi-constraint optimal problem. Then an AI-assisted intelligent routing algorithm is designed to dynamically and adaptively change link weighs, which allows us to achieve optimal energy efficiency. Findings from our simulation suggest the potential of our proposed approach.",industry
10.1109/tnse.2021.3055835,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,cloud versus edge deployment strategies of real-time face recognition inference,https://ieeexplore.ieee.org/document/9350171/,"Choosing the appropriate deployment strategy for any Deep Learning (DL) project in a production environment has always been the most challenging problem for industrial practitioners. There are several conflicting constraints and controversial approaches when it comes to deployment. Among these problems, the deployment on cloud versus the deployment on edge represents a common dilemma. In a nutshell, each approach provides benefits where the other would have limitations. This paper presents a real-world case study on deploying a face recognition application using MTCNN detector and FaceNet recognizer. We report the challenges faced to decide on the best deployment strategy. We propose three inference architectures for the deployment, including cloud-based, edge-based, and hybrid. Furthermore, we evaluate the performance of face recognition inference on different cloud-based and edge-based GPU platforms. We consider different models of Jetson boards for the edge (Nano, TX2, Xavier NX, Xavier AGX) and various GPUs for the cloud (GTX 1080, RTX 2080Ti, RTX 2070, and RTX 8000). We also investigate the effect of deep learning model optimization using TensorRT and TFLite compared to a standard Tensorflow GPU model, and the effect of input resolution. We provide a benchmarking study for all these devices in terms of frames per second, execution times, energy and memory usages. After conducting a total of 294 experiments, the results demonstrate that the TensorRT optimization provides the fastest execution on all cloud and edge devices, at the expense of significantly larger energy consumption (up to +40% and +35% for edge and cloud devices, respectively, compared to Tensorflow). Whereas TFLite is the most efficient framework in terms of memory and power consumption, while providing significantly less (-4% to -62%) processing acceleration than TensorRT. <italic>Practitioners Note:</italic> The study reported in this paper presents the real-challenges that we faced during our development and deployment of a face-recognition application both on the edge and on the cloud, and the solutions we have developed to solve these problems. The code, results, and interactive analytic dashboards of this paper will be put public upon publication.",industry
10.1109/tii.2021.3075464,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-04-01 00:00:00,ieeexplore,dnnoff: offloading dnn-based intelligent iot applications in mobile edge computing,https://ieeexplore.ieee.org/document/9416166/,"A deep neural network (DNN) has become increasingly popular in industrial Internet of Things scenarios. Due to high demands on computational capability, it is hard for DNN-based applications to directly run on intelligent end devices with limited resources. Computation offloading technology offers a feasible solution by offloading some computation-intensive tasks to the cloud or edges. Supporting such capability is not easy due to two aspects: <italic>Adaptability:</italic> offloading should dynamically occur among computation nodes. <italic>Effectiveness:</italic> it needs to be determined which parts are worth offloading. This article proposes a novel approach, called DNNOff. For a given DNN-based application, DNNOff first rewrites the source code to implement a special program structure supporting on-demand offloading and, at runtime, automatically determines the offloading scheme. We evaluated DNNOff on a real-world intelligent application, with three DNN models. Our results show that, compared with other approaches, DNNOff saves response time by 12.4–66.6% on average.",industry
10.1109/access.2022.3149050,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,design and implementation of traffic generation model and spectrum requirement calculator for private 5g network,https://ieeexplore.ieee.org/document/9703352/,"This paper proposes a neural 5G traffic generation model and a methodology for calculating the spectrum requirements of private 5G networks to provide various industrial communication services. To accurately calculate the spectral requirements, it is necessary to analyze the actual data volume and traffic type of industrial cases. However, because there is currently no suitable traffic model to test loads in private 5G networks, we have developed a generative adversarial network (GAN)-based traffic generator that can generate realistic traffic by learning actual traffic traces collected by mobile network operators. In addition, in the case of industrial applications, probability-based traffic models were used in parallel as there were not enough real data to be learned. The proposed 5G traffic generation model is combined with the proposed 5G spectrum calculation methodology, enabling more accurate spectrum requirements calculation through traffic simulation similar to a real-life environment. In this paper, the spectrum requirements are calculated differently according to two types of duplexing, namely frequency division duplexing (FDD) and time division duplexing (TDD). As a guide for companies aiming to provide advanced wireless connectivity for a wide variety of vertical industries using 5G networks, eight use cases defined in the 5G Alliance for Connected Industries and Automation (ACIA) white paper were simulated. The spectrum requirements were calculated under various simulation conditions considering varying traffic loads, deployment scenarios, and duplexing types. Various simulation results confirmed that a bandwidth of at least 22.0 MHz to a maximum of 397.8 MHz is required depending on the deployment scenario.",industry
10.1109/tmech.2021.3065522,preprocessed,IEEE/ASME Transactions on Mechatronics,IEEE,2022-02-01 00:00:00,ieeexplore,federated transfer learning for intelligent fault diagnostics using deep adversarial networks with data privacy,https://ieeexplore.ieee.org/document/9376674/,"Intelligent data-driven machinery fault diagnosis methods have been popularly developed in the past years. While fairly high diagnosis accuracies have been obtained, large amounts of labeled training data are mostly required, which are difficult to collect in practice. The promising collaborative model training solution with multiple users poses high demands on data privacy due to conflict of interests. Furthermore, in the real industries, the data from different users can be usually collected from different machine operating conditions. The domain shift phenomenon and data privacy concern make the joint model training scheme quite challenging. To address this issue, a federated transfer learning method for fault diagnosis is proposed in this article. Different models can be used by different users to enhance data privacy. A federal initialization stage is introduced to keep similar data structures in distributed feature extractions, and a federated communication stage is further implemented using deep adversarial learning. A prediction consistency scheme is also adopted to increase model robustness. Experiments on two real-world datasets suggest the proposed federated transfer learning method is promising for real industrial applications.",industry
10.1109/tie.2021.3057030,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-02-01 00:00:00,ieeexplore,kdnet-rul: a knowledge distillation framework to compress deep neural networks for machine remaining useful life prediction,https://ieeexplore.ieee.org/document/9351733/,"Machine remaining useful life (RUL) prediction is vital in improving the reliability of industrial systems and reducing maintenance cost. Recently, long short-term memory (LSTM) based algorithms have achieved state-of-the-art performance for RUL prediction due to their strong capability of modeling sequential sensory data. In many cases, the RUL prediction algorithms are required to be deployed on edge devices to support real-time decision making, reduce the data communication cost, and preserve the data privacy. However, the powerful LSTM-based methods which have high complexity cannot be deployed to edge devices with limited computational power and memory. To solve this problem, we propose a knowledge distillation framework, entitled KDnet-RUL, to compress a complex LSTM-based method for RUL prediction. Specifically, it includes a generative adversarial network based knowledge distillation (GAN-KD) for disparate architecture knowledge transfer, a learning-during-teaching based knowledge distillation (LDT-KD) for identical architecture knowledge transfer, and a sequential distillation upon LDT-KD for complicated datasets. We leverage simple and complicated datasets to verify the effectiveness of the proposed KDnet-RUL. The results demonstrate that the proposed method significantly outperforms state-of-the-art KD methods. The compressed model with 12.8 times less weights and 46.2 times less total float point operations even achieves a comparable performance with the complex LSTM model for RUL prediction.",industry
10.1109/tpds.2021.3104255,preprocessed,IEEE Transactions on Parallel and Distributed Systems,IEEE,2022-06-01 00:00:00,ieeexplore,taskflow: a lightweight parallel and heterogeneous task graph computing system,https://ieeexplore.ieee.org/document/9511796/,"Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.",industry
10.26599/tst.2020.9010055,preprocessed,Tsinghua Science and Technology,TUP,2022-04-01 00:00:00,ieeexplore,underground pipeline surveillance with an algorithm based on statistical time-frequency acoustic features,https://ieeexplore.ieee.org/document/9552663/,"Underground pipeline networks suffer from severe damage by earth-moving devices due to rapid urbanization. Thus, designing a round-the-clock intelligent surveillance system has become crucial and urgent. In this study, we develop an acoustic signal-based excavation device recognition system for underground pipeline protection. The front-end hardware system is equipped with an acoustic sensor array, an Analog-to-Digital Converter (ADC) module (ADS1274), and an industrial processor Advanced RISC Machine (ARM) cortex-A8 for signal collection and algorithm implementation. Then, a novel Statistical Time-Frequency acoustic Feature (STFF) is proposed, and a fast Extreme Learning Machine (ELM) is adopted as the classifier. Experiments on real recorded data show that the proposed STFF achieves better discriminative capability than the conventional acoustic cepstrum features. In addition, the surveillance platform is applicable for encountering big data owing to the fast learning speed of ELM.",industry
10.1007/978-3-030-42462-6_123,preprocessed,The Palgrave Handbook of Climate Resilient Societies,Springer,2021-01-01 00:00:00,springer,water 4.0: enhancing climate resilience,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-42462-6_123,"For this chapter, water 4.0 is defined as the industry 4.0 concept applied to the water sector. As industry 4.0 reflects the fourth industrial revolution , water 4.0 reflects the fourth water revolution . Based on the literature review and case studies, this chapter examines a proposition that water 4.0 will increase not only the sector’s economic effectiveness but also sustainability including climate resilience. Relevant technologies include digital twins , visualization, wireless monitoring sensors, industrial internet of things (IoT/IIoT), cloud computing, and predictive or prescriptive analytics but also blockchain , drones, and cybersecurity. For water 4.0 becoming a reality, water utility companies need not only collect more data but also to have proper analytical tools in place to convert data into information supporting optimal decisions. The current tools should preferably be replaced by machine learning algorithms that are nonlinear, nonstationary, and dynamic and thus aligned closely with the real world. It has been suggested in this chapter that such disruptive technologies be introduced through an ISO 55001-based asset management system (AMS). ISO 19650 series supplements ISO 55001 and contains additional requirements for the AMS development by focusing particularly on asset information. For this purpose, the series provides assistance with big data and digital twins . Two approaches are applicable to the implementation of water 4.0 through AMS: adaptability and more traditional continuous improvement with the former considered in this chapter as preferred but requires a sufficient level of asset management maturity. Therefore, it might be prudent that every organization sets their own water 4.0 -related standards and objectives in their own AMS and considers the preferred level of adaptability . Adaptability is arguably required for water 4.0 with adaptation bringing the greatest value .",industry
http://arxiv.org/abs/2202.10075v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"icsml: industrial control systems machine learning inference framework
  natively executing on iec 61131-3 languages",http://arxiv.org/abs/2202.10075v1,"Industrial Control Systems (ICS) have played a catalytic role in enabling the
4th Industrial Revolution. ICS devices like Programmable Logic Controllers
(PLCs), automate, monitor and control critical processes in industrial, energy
and commercial environments. The convergence of traditional Operational
Technology (OT) with Information Technology (IT) has opened a new and unique
threat landscape. This has inspired defense research that focuses heavily on
Machine Learning (ML) based anomaly detection methods that run on external IT
hardware which means an increase in costs and the further expansion of the
threat landscape. To remove this requirement, we introduce the ICS Machine
Learning inference framework (ICSML) which enables the execution of ML models
natively on the PLC. ICSML is implemented in IEC 61131-3 code and works around
the limitations imposed by the domain-specific languages, providing a complete
set of components for the creation of fully fledged ML models in a way similar
to established ML frameworks. We then demonstrate a complete end-to-end
methodology for creating ICS ML models using an external framework for training
and ICSML for the PLC implementation. To evaluate our contributions we run a
series of benchmarks studying memory and performance and compare our solution
to the TFLite inference framework. Finally, to demonstrate the abilities of
ICSML and to verify its non-intrusive nature, we develop and evaluate a case
study of a real defense for process aware attacks against a Multi Stage Flash
(MSF) desalination plant.",industry
http://arxiv.org/abs/2202.09549v1,preprocessed,arxiv,arxiv,2022-02-19 00:00:00,arxiv,"learning to detect slip with barometric tactile sensors and a temporal
  convolutional neural network",http://arxiv.org/abs/2202.09549v1,"The ability to perceive object slip via tactile feedback enables humans to
accomplish complex manipulation tasks including maintaining a stable grasp.
Despite the utility of tactile information for many applications, tactile
sensors have yet to be widely deployed in industrial robotics settings; part of
the challenge lies in identifying slip and other events from the tactile data
stream. In this paper, we present a learning-based method to detect slip using
barometric tactile sensors. These sensors have many desirable properties
including high durability and reliability, and are built from inexpensive,
off-the-shelf components. We train a temporal convolution neural network to
detect slip, achieving high detection accuracies while displaying robustness to
the speed and direction of the slip motion. Further, we test our detector on
two manipulation tasks involving a variety of common objects and demonstrate
successful generalization to real-world scenarios not seen during training. We
argue that barometric tactile sensing technology, combined with data-driven
learning, is suitable for many manipulation tasks such as slip compensation.",industry
http://arxiv.org/abs/2202.09113v1,preprocessed,arxiv,arxiv,2022-02-18 00:00:00,arxiv,how to manage tiny machine learning at scale: an industrial perspective,http://arxiv.org/abs/2202.09113v1,"Tiny machine learning (TinyML) has gained widespread popularity where machine
learning (ML) is democratized on ubiquitous microcontrollers, processing sensor
data everywhere in real-time. To manage TinyML in the industry, where mass
deployment happens, we consider the hardware and software constraints, ranging
from available onboard sensors and memory size to ML-model architectures and
runtime platforms. However, Internet of Things (IoT) devices are typically
tailored to specific tasks and are subject to heterogeneity and limited
resources. Moreover, TinyML models have been developed with different
structures and are often distributed without a clear understanding of their
working principles, leading to a fragmented ecosystem. Considering these
challenges, we propose a framework using Semantic Web technologies to enable
the joint management of TinyML models and IoT devices at scale, from modeling
information to discovering possible combinations and benchmarking, and
eventually facilitate TinyML component exchange and reuse. We present an
ontology (semantic schema) for neural network models aligned with the World
Wide Web Consortium (W3C) Thing Description, which semantically describes IoT
devices. Furthermore, a Knowledge Graph of 23 publicly available ML models and
six IoT devices were used to demonstrate our concept in three case studies, and
we shared the code and examples to enhance reproducibility:
https://github.com/Haoyu-R/How-to-Manage-TinyML-at-Scale",industry
http://arxiv.org/abs/2202.08897v1,preprocessed,arxiv,arxiv,2022-02-17 00:00:00,arxiv,"implementing spiking neural networks on neuromorphic architectures: a
  review",http://arxiv.org/abs/2202.08897v1,"Recently, both industry and academia have proposed several different
neuromorphic systems to execute machine learning applications that are designed
using Spiking Neural Networks (SNNs). With the growing complexity on design and
technology fronts, programming such systems to admit and execute a machine
learning application is becoming increasingly challenging. Additionally,
neuromorphic systems are required to guarantee real-time performance, consume
lower energy, and provide tolerance to logic and memory failures. Consequently,
there is a clear need for system software frameworks that can implement machine
learning applications on current and emerging neuromorphic systems, and
simultaneously address performance, energy, and reliability. Here, we provide a
comprehensive overview of such frameworks proposed for both, platform-based
design and hardware-software co-design. We highlight challenges and
opportunities that the future holds in the area of system software technology
for neuromorphic computing.",industry
http://arxiv.org/abs/2202.06149v1,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,"automatic issue classifier: a transfer learning framework for
  classifying issue reports",http://arxiv.org/abs/2202.06149v1,"Issue tracking systems are used in the software industry for the facilitation
of maintenance activities that keep the software robust and up to date with
ever-changing industry requirements. Usually, users report issues that can be
categorized into different labels such as bug reports, enhancement requests,
and questions related to the software. Most of the issue tracking systems make
the labelling of these issue reports optional for the issue submitter, which
leads to a large number of unlabeled issue reports. In this paper, we present a
state-of-the-art method to classify the issue reports into their respective
categories i.e. bug, enhancement, and question. This is a challenging task
because of the common use of informal language in the issue reports. Existing
studies use traditional natural language processing approaches adopting
key-word based features, which fail to incorporate the contextual relationship
between words and therefore result in a high rate of false positives and false
negatives. Moreover, previous works utilize a uni-label approach to classify
the issue reports however, in reality, an issue-submitter can tag one issue
report with more than one label at a time. This paper presents our approach to
classify the issue reports in a multi-label setting. We use an off-the-shelf
neural network called RoBERTa and fine-tune it to classify the issue reports.
We validate our approach on issue reports belonging to numerous industrial
projects from GitHub. We were able to achieve promising F-1 scores of 81%, 74%,
and 80% for bug reports, enhancements, and questions, respectively. We also
develop an industry tool called Automatic Issue Classifier (AIC), which
automatically assigns labels to newly reported issues on GitHub repositories
with high accuracy.",industry
http://arxiv.org/abs/2202.04834v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"geometric digital twinning of industrial facilities: retrieval of
  industrial shapes",http://arxiv.org/abs/2202.04834v1,"This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",industry
http://arxiv.org/abs/2202.03028v1,preprocessed,arxiv,arxiv,2022-02-07 00:00:00,arxiv,quark: a framework for quantum computing application benchmarking,http://arxiv.org/abs/2202.03028v1,"Quantum computing (QC) is anticipated to provide a speedup over classical HPC
approaches for specific problems in optimization, simulation, and machine
learning. With the advances in quantum computing toward practical applications,
the need to analyze and compare different quantum solutions increases. While
different low-level benchmarks for QC exist, these benchmarks do not provide
sufficient insights into real-world application-level performance. We propose
an application-centric benchmark method and the QUantum computing Application
benchmaRK (QUARK) framework to foster the investigation and creation of
application benchmarks for QC. This paper establishes three significant
contributions: (1) it makes a case for application-level benchmarks and
provides an in-depth ""pen and paper"" benchmark formulation of two reference
problems: robot path and vehicle option optimization from the industrial
domain; (2) it proposes the open-source QUARK framework for designing,
implementing, executing, and analyzing benchmarks; (3) it provides multiple
reference implementations for these two reference problems based on different
known, and where needed, extended, classical and quantum algorithmic approaches
and analyzes their performance on different types of infrastructures.",industry
http://arxiv.org/abs/2202.02813v2,preprocessed,arxiv,arxiv,2022-02-06 00:00:00,arxiv,a coding framework and benchmark towards compressed video understanding,http://arxiv.org/abs/2202.02813v2,"Most video understanding methods are learned on high-quality videos. However,
in real-world scenarios, the videos are first compressed before the
transportation and then decompressed for understanding. The decompressed videos
may have lost the critical information to the downstream tasks. To address this
issue, we propose the first coding framework for compressed video
understanding, where another learnable analytic bitstream is simultaneously
transported with the original video bitstream. With the dedicatedly designed
self-supervised optimization target and dynamic network architectures, this new
stream largely boosts the downstream tasks yet with a small bit cost. By only
one-time training, our framework can be deployed for multiple downstream tasks.
Our framework also enjoys the best of both two worlds, (1) high efficiency of
industrial video codec and (2) flexible coding capability of neural networks
(NNs). Finally, we build a rigorous benchmark for compressed video
understanding on three popular tasks over seven large-scale datasets and four
different compression levels. The proposed Understanding oriented Video Coding
framework UVC consistently demonstrates significantly stronger performances
than the baseline industrial codec.",industry
http://arxiv.org/abs/2201.12170v3,preprocessed,arxiv,arxiv,2022-01-28 00:00:00,arxiv,"unsupervised single-shot depth estimation using perceptual
  reconstruction",http://arxiv.org/abs/2201.12170v3,"Real-time estimation of actual object depth is a module that is essential to
performing various autonomous system tasks such as 3D reconstruction, scene
understanding and condition assessment of machinery parts. During the last
decade of machine learning, extensive deployment of deep learning methods to
computer vision tasks has yielded approaches that succeed in achieving
realistic depth synthesis out of a simple RGB modality. While most of these
models are based on paired depth data or availability of video sequences and
stereo images, methods for single-view depth synthesis in a fully unsupervised
setting have hardly been explored. This study presents the most recent advances
in the field of generative neural networks, leveraging them to perform fully
unsupervised single-shot depth synthesis. Two generators for RGB-to-depth and
depth-to-RGB transfer are implemented and simultaneously optimized using the
Wasserstein-1 distance and a novel perceptual reconstruction term. To ensure
that the proposed method is plausible, we comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database and the SURREAL dataset that records body depth. The success observed
in this study suggests the great potential for unsupervised single-shot depth
estimation in real-world applications.",industry
http://arxiv.org/abs/2201.06735v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,ai augmented digital metal component,http://arxiv.org/abs/2201.06735v1,"The aim of this work is to propose a new paradigm that imparts intelligence
to metal parts with the fusion of metal additive manufacturing and artificial
intelligence (AI). Our digital metal part classifies the status with real time
data processing with convolutional neural network (CNN). The training data for
the CNN is collected from a strain gauge embedded in metal parts by laser
powder bed fusion process. We implement this approach using additive
manufacturing, demonstrate a self-cognitive metal part recognizing partial
screw loosening, malfunctioning, and external impacting object. The results
indicate that metal part can recognize subtle change of multiple fixation state
under repetitive compression with 89.1% accuracy with test sets. The proposed
strategy showed promising potential in contributing to the hyper-connectivity
for next generation of digital metal based mechanical systems",industry
http://arxiv.org/abs/2201.06616v2,preprocessed,arxiv,arxiv,2022-01-17 00:00:00,arxiv,improving the quality control of seismic data through active learning,http://arxiv.org/abs/2201.06616v2,"In image denoising problems, the increasing density of available images makes
an exhaustive visual inspection impossible and therefore automated methods
based on machine-learning must be deployed for this purpose. This is
particulary the case in seismic signal processing. Engineers/geophysicists have
to deal with millions of seismic time series. Finding the sub-surface
properties useful for the oil industry may take up to a year and is very costly
in terms of computing/human resources. In particular, the data must go through
different steps of noise attenuation. Each denoise step is then ideally
followed by a quality control (QC) stage performed by means of human expertise.
To learn a quality control classifier in a supervised manner, labeled training
data must be available, but collecting the labels from human experts is
extremely time-consuming. We therefore propose a novel active learning
methodology to sequentially select the most relevant data, which are then given
back to a human expert for labeling. Beyond the application in geophysics, the
technique we promote in this paper, based on estimates of the local error and
its uncertainty, is generic. Its performance is supported by strong empirical
evidence, as illustrated by the numerical experiments presented in this
article, where it is compared to alternative active learning strategies both on
synthetic and real seismic datasets.",industry
http://arxiv.org/abs/2201.06599v1,preprocessed,arxiv,arxiv,2022-01-17 00:00:00,arxiv,"who supervises the supervisor? model monitoring in production using deep
  feature embeddings with applications to workpiece inspection",http://arxiv.org/abs/2201.06599v1,"The automation of condition monitoring and workpiece inspection plays an
essential role in maintaining high quality as well as high throughput of the
manufacturing process. To this end, the recent rise of developments in machine
learning has lead to vast improvements in the area of autonomous process
supervision. However, the more complex and powerful these models become, the
less transparent and explainable they generally are as well. One of the main
challenges is the monitoring of live deployments of these machine learning
systems and raising alerts when encountering events that might impact model
performance. In particular, supervised classifiers are typically build under
the assumption of stationarity in the underlying data distribution. For
example, a visual inspection system trained on a set of material surface
defects generally does not adapt or even recognize gradual changes in the data
distribution - an issue known as ""data drift"" - such as the emergence of new
types of surface defects. This, in turn, may lead to detrimental
mispredictions, e.g. samples from new defect classes being classified as
non-defective. To this end, it is desirable to provide real-time tracking of a
classifier's performance to inform about the putative onset of additional error
classes and the necessity for manual intervention with respect to classifier
re-training. Here, we propose an unsupervised framework that acts on top of a
supervised classification system, thereby harnessing its internal deep feature
representations as a proxy to track changes in the data distribution during
deployment and, hence, to anticipate classifier performance degradation.",industry
http://arxiv.org/abs/2201.04263v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,the human factor in ai safety,http://arxiv.org/abs/2201.04263v1,"AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.",industry
http://arxiv.org/abs/2201.02028v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,"a light in the dark: deep learning practices for industrial computer
  vision",http://arxiv.org/abs/2201.02028v1,"In recent years, large pre-trained deep neural networks (DNNs) have
revolutionized the field of computer vision (CV). Although these DNNs have been
shown to be very well suited for general image recognition tasks, application
in industry is often precluded for three reasons: 1) large pre-trained DNNs are
built on hundreds of millions of parameters, making deployment on many devices
impossible, 2) the underlying dataset for pre-training consists of general
objects, while industrial cases often consist of very specific objects, such as
structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal
issues for companies. As a remedy, we study neural networks for CV that we
train from scratch. For this purpose, we use a real-world case from a solar
wafer manufacturer. We find that our neural networks achieve similar
performances as pre-trained DNNs, even though they consist of far fewer
parameters and do not rely on third-party datasets.",industry
10.1016/j.compag.2022.106688,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,implementation of a decision support system for prediction of the total soluble solids of industrial tomato using machine learning models,https://api.elsevier.com/content/abstract/scopus_id/85122635918,"Tomato is the second most important vegetable in the world, both in terms of production and consumption. Especially for the cultivation of industrial tomato, harvest is conducted when the total soluble solids, a major quality characteristic, are as high as possible. Advancements in technology have made Decision Support Systems simpler and more applicable in an everyday basis. Data Analysis, combined with Machine Learning algorithms are considered the future of sustainable agriculture, allowing farmers to be advised about the best possible decisions for their cultivation. Farmers need to adopt this kind of technology in order to be able to know when the quality of tomatoes is at its peak, in order to gather their product from the field. The implementation of a Decision Support System to predict the total soluble solids was conducted,based on data from previous years, including quality data (pH, Bostwick, L, a/b, Mean Weight, °Brix), the type of hybrid used, weather data and soil data from the fields. Data derived from fields in 6 different regions in the northwestern Peloponnese, Greece over 6 cultivation periods, created a dataset of 33 different inputs. Thirteen different algorithms were put into evaluation in order to find the best one in terms of speed and efficiency. In this research, we developed a Decision Support System using the K-nearest algorithm, which proved to be the best for our dataset. The predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. This DSS which is using real time weather data as an input is expected to be a valuable tool for the farmers.",industry
10.1016/j.autcon.2021.104088,preprocessed,Automation in Construction,scopus,2022-02-01,sciencedirect,vision-based high-precision intelligent monitoring for shield tail clearance,https://api.elsevier.com/content/abstract/scopus_id/85120874971,"Real-time shield tail clearance measurement and monitoring is a key task during shield tunneling construction. The shield tail clearance measurement and monitoring technology development is still in its infancy, the current methods are mainly designed manually based on intuition. In order to fill the gap between the requirement of shield tail clearance measurement and monitoring and the limitations of the current methods, this paper systematically studies the existing mechanisms related to shield tail clearance measurement and monitoring, and develops a high-precision intelligent monitoring system for shield tail clearance. The proposed monitoring system includes four components: 1) two types of shield tail clearance calculation models, 2) the integrated hardware of the monitoring system which is composed of a data acquisition unit, a signal transmission unit and a control unit, 3) the region of interest (ROI) extraction method based on deep neural network, and the image processing algorithms for image enhancement and feature extraction, 4) the custom-developed software built on mature integrated development environment (IDE). After the calculation model of shield tail clearance is established, the system uses monitoring devices equipped with industrial cameras to obtain the on-site image, and then applies image processing technologies along with deep learning approach to extract the key features, which are brought into the model to calculate the values of shield tail clearance, finally displays these values and simulates the current tunneling attitude of the shield machine in real time. The experimental results show that the system proposed in this paper achieves the goal of high precision measuring and real-time monitoring of the shield tail clearance.",industry
10.1016/j.jisa.2021.103046,preprocessed,Journal of Information Security and Applications,scopus,2022-02-01,sciencedirect,aicrit: a unified framework for real-time anomaly detection in water treatment plants,https://api.elsevier.com/content/abstract/scopus_id/85119422439,"Industrial Control Systems (ICS) in public infrastructure, such as water treatment and distribution plants, have become a target of sophisticated cyber-attacks. Given the ever-present insider and other threats in such systems, there is a need to deploy mechanisms for defense and incidence response beyond the traditional. In this work we present AICrit that operates over the physical constraints and domain norms for accurate and timely detection of process anomalies. AICrit learns system-wide normal behavior using design knowledge and machine learning algorithms to recognize abnormal or irregular behavioral patterns resulting due to process anomalies. AICrit was implemented and evaluated in SWaT by launching several real-time stealthy and coordinated attacks. Experimental results attest to the effectiveness of AICrit in the timely detection of process anomalies with a low occurrence of false alarms. The underlying methodology used in the design of AICrit is generic and applicable to other ICS in various domains such as power, energy, and transportation.",industry
10.1016/j.apenergy.2021.118127,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,data-driven control of room temperature and bidirectional ev charging using deep reinforcement learning: simulations and experiments,https://api.elsevier.com/content/abstract/scopus_id/85118721393,"The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.",industry
10.1016/j.ssci.2021.105529,preprocessed,Safety Science,scopus,2022-02-01,sciencedirect,a novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry
10.1016/j.eswa.2021.116045,preprocessed,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,posimnet-r: an immunologic resilient approach to position routers in industrial wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/85117584055,"Industry 4.0 has increased the interest in employing Industrial Wireless Sensor Network (IWSN) technologies in industrial automation. The advantages range from ease of installation and maintenance to reduced deployment time and infrastructure costs. However, industrial automation has critical requirements regarding network infrastructure, such as reliability and failure tolerance. Therefore, it is imperative to have an adequate placement of sensor and router nodes, to obtain a network with multiple paths, allowing the data to reach management systems within a reasonable time, even in the event of failures. The placement of router nodes has to consider latency, network lifespan, connectivity, and failure tolerance aspects in a possibly hostile environment, with classified areas and obstacles such as silos, tanks and buildings. We present a new approach, called POSIMNET-R, to place IWSN routing nodes in an industrial configuration, which circumvents forbidden areas and obstacles, based on Artificial Immunological Networks. The resulting network offers low failure rates and path redundancy criteria. The results have shown that POSIMNET-R was capable of providing a reliable network with multiple paths and resilience of the used routers equal to 81.50% in the basic case study and 73.66% in the real case scenario.",industry
10.1016/j.comcom.2021.10.036,preprocessed,Computer Communications,scopus,2022-01-15,sciencedirect,lstm-mfcn: a time series classifier based on multi-scale spatial–temporal features,https://api.elsevier.com/content/abstract/scopus_id/85119299619,"Time series classification (TSC) task attracts huge interests, since they correspond to the real-world problems in a wide variety of fields, such as industry monitoring. Deep learning methods, especially CNN and FCN, shows competitive performance in TSC task by their virtue of good adaption for raw time series and self-adapting extraction of features. Then various variants of CNN are proposed so as to make further breakthrough by the better perception to characteristics of data. Among them, LSTM-FCN and GRU-FCN who learn spatial and temporal features simultaneously are the most remarkable ones, achieving state of the art results. Therefore, inspired by their success and in consideration of the discriminative features implied in time series are diverse in size, a multimodal network LSTM-MFCN composed of multi-scale FCN (MFCN) and LSTM are proposed in this work. The gate-based network LSTM naturally fits to various terms time dependencies, and FCN with multi-scale sets of filters are capable to perceive spatial features of different range from time series curves. Besides, dilation convolution is deployed to build multi-scale receptive fields in larger level without increasing the parameters to be trained. The full perception of large multi-scale spatial–temporal features lead LSTM-MFCN to possess comprehensive and thorough grasp to time series, thus achieve even better accuracies. Finally, two representative architectures are presented specifically and their experiments on UCR datasets reveals the effectiveness and superiority of proposed LSTM-MFCN.",industry
10.1016/j.energy.2021.122359,preprocessed,Energy,scopus,2022-01-15,sciencedirect,fuzzy inference system application for oil-water flow patterns identification,https://api.elsevier.com/content/abstract/scopus_id/85117714992,"Prediction of oil-water two-phase flow pattern provides an effective solution for reducing oil production costs. In this research, the fuzzy inference system (FIS) is utilized to predict fluid flow patterns and establish a new adaptable prediction model. This paper takes No. 10 industrial white oil and tap water as the research objects to simulate fluids, and analyzes the changes of the pipeline angle, the total flow of oil-water two-phase flow and the convective pattern of water cut. A data set containing 60 samples was used to create the model, and the Mamdani fuzzy model was established using MATLAB software. The results show that compared with the BP neural network algorithm, the model set forth in the present paper has higher accuracy and reliability, and can achieve real-time monitoring and effectively reduce errors, especially in the case of decision-making. In addition, the fuzzy model is demonstrated that in the entire production logging process of non-vertical wells, the use of a fuzzy inference system to predict fluid flow patterns can greatly save production costs while ensuring the safe operation of production equipment.",industry
10.1016/j.aca.2021.339411,preprocessed,Analytica Chimica Acta,scopus,2022-01-01,sciencedirect,a video processing and machine vision-based automatic analyzer to determine sequentially total suspended and settleable solids in wastewater,https://api.elsevier.com/content/abstract/scopus_id/85123884378,"The monitoring of total suspended (TSS) and settleable (SetS) solids in wastewater is essential to maintain the quality parameters for aquatic biota because they can transport pollutants and block light penetration. Determining them by their respective reference methods, however, is laborious, expensive, and time consuming. To overcome this, we developed a new analytical instrument called Solids in Wastewater's Machine Vision-based Automatic Analyzer (SWAMVA), which is equiped with an automatic sampler and a software for real-time digital movie capture to quantify sequentially the TSS and SetS contents in wastewater samples. The machine vision algorithm (MVA) coupled with the Red color plane (derived from color histograms in the Red-Green-Blue (RGB) system) showed the best prediction results with R2 of 0.988 and 0.964, and relative error of prediction (REP) of 6.133 and 9.115% for TSS and SetS, respectively. The constructed models were validated by Analysis of Variance (ANOVA), and the accuracy and precision of the predictions by the t- and F-tests, respectively, at a 0.05 significance level. The elliptical joint confidence region (EJCR) test confirmed the accuracy, while the coefficient of variation (CV) of 6.529 and 10.908% confirmed the good precisions, respectively. Compared with the reference method (Standard Methods For the Examination of Water and Wastewater), the proposed method reduced the analysis volume from 1.5 L to just 15 mL and the analysis time from 12 h to 24 s per sample. Therefore, SWAMVA can be considered an important alternative to the determination of TSS and SetS in wastewater as an automatic, fast, and low-cost analytical tool, following the principles of Green Chemistry and exploiting Industry 4.0 features such as intelligent processing, miniaturization, and machine vision.",industry
10.1016/j.jmsy.2022.01.010,preprocessed,Journal of Manufacturing Systems,scopus,2022-01-01,sciencedirect,"towards edge computing in intelligent manufacturing: past, present and future",https://api.elsevier.com/content/abstract/scopus_id/85123859503,"Industry 4.0 (I4.0) is the fourth industrial revolution and a synonym for intelligent manufacturing. It drives the convergence of several cutting-edge technologies to provoke autonomous, fully integrated, collaborated, highly automated, and customized industries. Edge Computing (EC), a highly distributed framework, emerged a couple of years ago and embraced the industry to leverage the benefit of low latency and near real-time performance. It brings computation and storage in the close proximity of end devices and reduces the cloud overhead. In addition to improved operational efficiency, storage, and latency, EC further reduces the cost, improves productivity with higher quality maintenance and customer satisfaction. At the digital-to-digital stage of the Physical-Digital-Physical (PDP) loop, adapting EC can furnish tremendous benefits and further accelerate the next stages of the loop. This survey identifies the past and present works oriented towards Intelligent Manufacturing integrated with the EC platform and categorizes the research based on architecture, intelligence platform, edge objectives, and application. Herein, the authors have incorporated; (1) The progress in I4.0 following the PDP loop; (2) The discussion on EC in I4.0 and their Research Trend; (3) Methods to bring intelligence to the edge. To the best of our knowledge, it is the first review article that focuses on the applications and objectives of EC in Intelligent Manufacturing. It also outlines the optimum solutions to bring intelligence to the edge by overcoming the resource and complexity-bound with accuracy and latency constraints for the decision-making processes. Future directions include the less explored research areas, challenges in edge deployment in industries, and the integration of trending technologies such as Blockchain, Software Defined Networking, and 5 G with EC to excite the EC researchers. A few collaborative edge scenarios are discussed for the promotion and application of EC in I4.0. Nevertheless, efficient edge deployments face many challenges since studies are still limited to conceptual levels or design steps, and future orientation to application strategies for Smart Manufacturing is required.",industry
10.1016/j.cie.2021.107824,preprocessed,Computers and Industrial Engineering,scopus,2022-01-01,sciencedirect,new perspectives and results for smart operators in industry 4.0: a human-centered approach,https://api.elsevier.com/content/abstract/scopus_id/85122422552,"Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators.
                  To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept.
                  Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework.",industry
10.1016/j.softx.2021.100956,preprocessed,SoftwareX,scopus,2022-01-01,sciencedirect,tx2_fcnn_node: an open-source ros compatible tool for monocular depth reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85121968187,"We present tx2_fcnn_node – a Robot Operating System (ROS) compatible tool that is aimed at seamless integration of various monocular depth reconstruction neural networks to the robotic software based on ROS (which is a de-facto standard in the area of robotics). Our tool simplifies the process of deploying, evaluating, and comparing depth reconstruction neural networks both on real robots and in simulation. We complement our software with a set of the precompiled neural networks which can be used off the shelf, with some of them being able to demonstrate near real-time performance when running onboard compact embedded platforms, e.g. Nvidia Jetson TX2, that are often used nowadays both in academia and industry.",industry
10.1016/j.suscom.2021.100650,preprocessed,Sustainable Computing: Informatics and Systems,scopus,2022-01-01,sciencedirect,a conceptual framework for the implementation of industry 4.0 in legal informatics,https://api.elsevier.com/content/abstract/scopus_id/85121918847,"The growing number of applications of Industry 4.0 in the field of legal informatics offers huge opportunities for data scientists and academic researchers. The term Industry 4.0 is defined as “the fourth industrial revolution that connects embedded systems to Cyber-Physical-Systems”. It emphasizes the end-to-end digitalization of all physical resources and integrating digital environments with the value chain organizations. Industry 4.0 comprises a variety of technologies such as Cyber-Physical-Systems, Big Data, Internet of Things, Artificial Intelligence, Cloud Computing and Cybersecurity. It has been found that the implementation of these technologies may be useful in achieving the objective of legal informatics. It can help lawmakers to align jurisprudence by providing modern computational technologies to improve and advance the traditional legal justice system. Further, the integration of legal informatics with Industry 4.0 will be strengthening the legal justice system by providing decision-making support, data transparency, real-time monitoring, cost-effective solution, and triple-bottom-line performance in the future. Therefore, the article aims to determine the implementation patterns of Industry 4.0 technologies in legislative institutions and administrations. The study proposes a conceptual framework that integrates Industry 4.0 with legal informatics. The findings show that implementing Industry 4.0 technologies such as Artificial Intelligence, Big Data, and Cloud Computing plays a vital role for legal firms that are currently in the nascent stage of development.",industry
10.1016/j.compag.2021.106635,preprocessed,Computers and Electronics in Agriculture,scopus,2022-01-01,sciencedirect,intelligent iot-multiagent precision irrigation approach for improving water use efficiency in irrigation systems at farm and district scales,https://api.elsevier.com/content/abstract/scopus_id/85121511874,"The fourth industrial revolution in agriculture seeks the automation of traditional practices, using modern smart technologies. Advances in electronics, computation and the internet of things are integrated for improving field inputs management. The aim of this paper is to present the design and implementation of an intelligent IoT-multiagent precision irrigation approach for improving water use efficiency in irrigation systems. The study site was the large-scale irrigation and drainage district of Chicamocha and Firavitoba (Usochicamocha) located in Boyacá - Colombia, where water is distributed from the Chicamocha riverbed. In the proposed system, irrigation is supervised and controlled in each field by an intelligent irrigation agent that autonomously prescribes and applies water amounts with agronomical criteria. The methodology was applied with real (cyber-physical) and virtual (simulated) intelligent agents and was extended to eleven pump stations that supply water to 5911 fields. Using a MQTT protocol, hundreds of irrigation intelligent agents report water prescriptions and crop characteristics to a master agent in each pump station, who creates a regional irrigation map to manage georeferenced field information and performs negotiation of water resources between agents according to supply availability. Field maps and intelligent irrigation agents can be visualized using devices with internet access. Results demonstrated that irrigation amounts were correctly applied on the fields, thus improving the water use efficiency. This technology is a novel support to decision-making in water resources management applications at field and district scales.",industry
10.1016/j.enconman.2021.115030,preprocessed,Energy Conversion and Management,scopus,2022-01-01,sciencedirect,deep reinforcement learning based energy management strategy of fuel cell hybrid railway vehicles considering fuel cell aging,https://api.elsevier.com/content/abstract/scopus_id/85119905389,"In the rail transportation industry, growing energy and environmental awareness requires the use of alternatives to combustion engines. These include hybrid electrically driven railway vehicles powered by fuel cells and batteries. The cost of hydrogen consumption and the lifetime of fuel cells are currently the main challenges that need to be addressed before widespread deployment of fuel cell railway vehicles can be realized. With this in mind, this work focuses on the energy management system with emphasis on optimizing the energy distribution to reduce the overall operational cost. The presented energy management strategy (EMS) aims at minimizing hydrogen consumption and fuel cell aging costs while achieving a favorable balance between battery charging and discharging. In order to take fuel cell aging into account in energy management and mitigate fuel cell aging trough power distribution, an online fuel cell aging estimation model based on four operation modes is introduced and applied. Moreover, the advanced deep reinforcement learning method Twin Delayed Deep Deterministic Policy Gradient (TD3) is used to obtain a promising EMS. To improve the adaptability of the strategy, a stochastic training environment, which is based on real measured speed profiles considering passenger numbers is used for training. Assuming different environmental and passenger transport volumes, the results confirm that the proposed TD3-EMS achieves battery charge-sustaining at low hydrogen consumption while slowing down fuel cell degradation.",industry
10.1016/j.ijpe.2021.108339,preprocessed,International Journal of Production Economics,scopus,2022-01-01,sciencedirect,age-based preventive maintenance with multiple printing options,https://api.elsevier.com/content/abstract/scopus_id/85118549755,"In today's economic context, production systems must be readily available and machinery downtime kept to a minimum. Maintenance and spare parts inventory management play a vital role in achieving these goals, and preventive maintenance has increasingly been considered in maintenance policies. Additive manufacturing (AM) has recently been combined with preventive maintenance, and thus represents an emerging research direction. However, few studies have as yet been conducted in this research stream, and we intend to fill this gap. Our study makes three main contributions. First, we address the main limitations of two current models (i.e., assuming that no failure occurs during the replenishment lead time of the spare parts). Second, we propose a new maintenance policy that considers two printing options with different levels of reliability and unitary purchase costs. Third, we develop a decision support system (DSS) to assist managers in deciding whether to implement a preventive maintenance policy that includes AM or conventional manufacturing (CM) parts. We take an interdisciplinary approach to conducting a parametrical analysis where we consider real data on the reliability of CM and AM parts, in addition to the impact of post-processing operations and optimization routines. We find that AM-based preventive maintenance policies are favored when the MTTF and the backorder costs are low and when the failure and maintenance costs are high. These findings have been incorporated into the DSS, which provides thresholds for every parameter to guide practitioners in choosing between AM and CM parts for preventive maintenance, without requiring time-expensive calculations.",industry
10.1016/j.cose.2021.102500,preprocessed,Computers and Security,scopus,2022-01-01,sciencedirect,antiviruses under the microscope: a hands-on perspective,https://api.elsevier.com/content/abstract/scopus_id/85118529412,"AntiViruses (AVs) are the main defense line against attacks for most users and much research has been done about them, especially proposing new detection procedures that work in academic prototypes. However, as most current and commercial AVs are closed-source solutions, in practice, little is known about their real internals: information such as what is a typical AV database size, the detection methods effectively used in each operation mode, and how often on average the AVs are updated are still unknown. This prevents research work from meeting the industrial practices more thoroughly. To fill this gap, in this work, we systematize the knowledge about AVs. To do so, we first surveyed the literature and identified existing knowledge gaps in AV internals’ working. Further, we bridged these gaps by analyzing popular (Windows, Linux, and Android) AV solutions to check their operations in practice. Our methodology encompassed multiple techniques, from tracing to fuzzing. We detail current AV’s architecture, including their multiple components, such as browser extensions and injected libraries, regarding their implementation, monitoring features, and self-protection capabilities. We discovered, for instance, a great disparity in the set of API functions hooked by the distinct AV’s libraries, which might have a significant impact in the viability of academically-proposed detection models (e.g., machine learning-based ones).",industry
10.1016/j.compind.2021.103556,preprocessed,Computers in Industry,scopus,2022-01-01,sciencedirect,c-ports: a proposal for a comprehensive standardization and implementation plan of digital services offered by the “port of the future”,https://api.elsevier.com/content/abstract/scopus_id/85118477493,"In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called “Port of the Future”. How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel & Marine Navigation, (ii) e-Freight & (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C-Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed.",industry
10.1016/j.dss.2021.113653,preprocessed,Decision Support Systems,scopus,2022-01-01,sciencedirect,ai-based industrial full-service offerings: a model for payment structure selection considering predictive power,https://api.elsevier.com/content/abstract/scopus_id/85114151068,"Artificial Intelligence and servitization reshape the way that manufacturing companies derive value. Aiming to sustain competitive advantage and intensify customer loyalty, full-service providers offer the use of their products as a service to achieve continuous revenues. For this purpose, companies implement AI classification algorithms to enable high levels of service at controllable costs. However, traditional asset sellers who become service providers require previously atypical payment structures, as classic payment methods involving a one-time fee for production costs and profit margins are unsuitable. In addition, a low predictive power of the implemented classification algorithm can lead to misclassifications, which diminish the achievable level of service and the intended net present value of the resultant service. While previous works focus solely on the costs of such misclassifications, our decision model highlights implications for payment structures, service levels, and – ultimately – the net present value of such data-driven service offerings. Our research suggests that predictive power can be a major factor in selecting a suitable payment structure and the overall design of service level agreements. Therefore, we compare common payment structures for data-driven services and investigate their relationship to predictive power. We develop our model using a design science methodology and iteratively evaluate our results using a four-step approach that includes interviews with industry experts and the application of our model to a real-world use case. In summary, our research extends the existing knowledge of servitization and data-driven services in the manufacturing industry through a quantitative decision model.",industry
10.1109/ccnc49033.2022.9700515,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,cave-vr and unity game engine for visualizing city scale 3d meshes,https://ieeexplore.ieee.org/document/9700515/,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",smart cities
10.1109/ccnc49033.2022.9700676,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,qos-aware priority-based task offloading for deep learning services at the edge,https://ieeexplore.ieee.org/document/9700676/,"Emerging Edge Computing (EC) technology has shown promise for many delay-sensitive Deep Learning (DL) based applications of smart cities in terms of improved Quality-of-Service (QoS). EC requires judicious decisions which jointly consider the limited capacity of the edge servers and provided QoS of DL-dependent services. In a smart city environment, tasks may have varying priorities in terms of when and how to serve them; thus, priorities of the tasks have to be considered when making resource management decisions. In this paper, we focus on finding optimal offloading decisions in a three-tier user-edge-cloud architecture while considering different priority classes for the DL-based services and making a trade-off between a task’s completion time and the provided accuracy by the DL-based service. We cast the optimization problem as an Integer Linear Program (ILP) where the objective is to maximize a function called gain of system (GoS) defined based on provided QoS and priority of the tasks. We prove the problem is NP-hard. We then propose an efficient offloading algorithm, called PGUS, that is shown to achieve near-optimal results in terms of the provided GoS. Finally, we compare our proposed algorithm, PGUS, with heuristics and a state-of-the-art algorithm, called GUS, using both numerical analysis and real-world implementation. Our results show that PGUS outperforms GUS by a factor of 45% in average in terms of serving the top 25% higher priority classes of the tasks while still keeping the overall percentage of the dropped tasks minimal and the overall gain of system maximized.",smart cities
10.1109/jiot.2021.3100068,preprocessed,IEEE Internet of Things Journal,IEEE,2022-03-01 00:00:00,ieeexplore,astcn: an attentive spatial–temporal convolutional network for flow prediction,https://ieeexplore.ieee.org/document/9511315/,"Flow prediction attracts intensive research interests, since it can offer essential support to many crucial problems in public safety and smart city, e.g., epidemic spread prediction and medical resource allocation optimization. Among all the models in flow prediction, deep learning models (e.g., convolutional neural networks, recurrent neural networks, and graph neural networks) are popular and outperform other statistics and machine learning models, since they can learn intrinsic structures and extract features from spatial–temporal (ST) data. However, most of them set strict temporal periods in the prediction or separate the interaction between spatial and temporal correlations. Therefore, the prediction accuracy is affected. To overcome the difficulties, we propose a flow prediction network attentive spatial–temporal convolutional network (ASTCN), which can effectively handle large-scale flow data and learn complex features. In ASTCN, we leverage an attention mechanism to overcome the previous problem of strict temporal periods, and can effectively fuse ST data with multiple factors from different time-series sources. Furthermore, we propose a causal 3-D convolutional layer based on temporal convolutional networks (TCNs). It can simultaneously extract both spatial and temporal features to improve the prediction accuracy. We comprehensively conducted our experiments based on real-world data sets. Experimental results show that ASTCN outperforms the state-of-the-art methods by at least 3.78% in root mean square error. Therefore, ASTCN is a potential solution to other large-scale ST problems.",smart cities
10.1109/jiot.2021.3097768,preprocessed,IEEE Internet of Things Journal,IEEE,2022-03-01 00:00:00,ieeexplore,user-aware and flexible proactive caching using lstm and ensemble learning in iot-mec networks,https://ieeexplore.ieee.org/document/9488291/,"To meet the stringent demands of emerging Internet-of-Things (IoT) applications, such as smart home, smart city, and virtual reality in 5G/6G IoT networks, edge content caching for mobile/multiaccess edge computing (MEC) has been identified as a promising approach to improve the quality of services in terms of latency and energy consumption. However, the limitations of cache capacity make it difficult to develop an effective common caching framework that satisfies diverse user preferences. In this article, we propose a new content caching strategy that maximizes the cache hit ratio through flexible prediction in dynamically changing network and user environments. It is based on a hierarchical deep learning architecture: long short-term memory (LSTM)-based local learning and ensemble-based meta-learning. First, as a local learning model, we employ an LSTM method with seasonal-trend decomposition using loess (STL)-based preprocessing. It identifies the attributes for demand prediction on the contents in various demographic user groups. Second, as a metalearning model, we employ a regression-based ensemble learning method, which uses an online convex optimization framework and exhibits sublinear “regret” performance. It orchestrates the obtained multiple demographic user preferences into a unified caching strategy in real time. Extensive experiments were conducted on the popular MovieLens data sets. It was shown that the proposed control provides up to a 30% higher cache hit ratio than conventional representative algorithms and a near-optimal cache hit ratio within approximately 9% of the optimal caching scheme with perfect prior knowledge of content popularity. The proposed learning and caching control can be implemented as a core function of the 5G/6G standard’s network data analytic function (NWDAF) module.",smart cities
10.1109/tnse.2021.3050781,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,vfchain: enabling verifiable and auditable federated learning via blockchain systems,https://ieeexplore.ieee.org/document/9321132/,"Advanced artificial intelligence techniques, such as federated learning, has been applied to broad areas, e.g., image classification, speech recognition, smart city, and healthcare. Despite intensive research on federated learning, existing schemes are vulnerable to attacks and can hardly meet the security requirements for real-world applications. The problem of designing a secure federated learning framework to ensure the correctness of training procedure has not been sufficiently studied and remains open. In this paper, we propose VFChain, a verifiable and auditable federated learning framework based on the blockchain system. First, to provide the verifiability, a committee is selected through the blockchain to collectively aggregate models and record verifiable proofs in the blockchain. Then, to provide the auditability, a novel authenticated data structure is proposed for blockchain to improve the search efficiency of verifiable proofs and support a secure rotation of committee. Finally, to further improve the search efficiency, an optimization scheme is proposed to support multiple-model learning tasks. We implement VFChain and conduct extensive experiments by utilizing the popular deep learning models over the public real-world dataset. The evaluation results demonstrate the effectiveness of our proposed VFChain system.",smart cities
10.1109/tkde.2020.2985954,preprocessed,IEEE Transactions on Knowledge and Data Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,incorporating multi-source urban data for personalized and context-aware multi-modal transportation recommendation,https://ieeexplore.ieee.org/document/9063461/,"Transportation recommendation is one important map service in navigation applications. Previous transportation recommendation solutions fail to deliver satisfactory user experience because their recommendations only consider routes in one transportation mode (uni-modal, e.g., taxi, bus, cycle) and largely overlook situational context. In this work, we propose <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq1-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>, a multi-task deep learning based recommendation system that offers multi-modal transportation planning and is adaptive to various situational context (e.g., nearby point-of-interest (POI) distribution and weather). We leverage the availability of existing routing engines and big urban data, and design a novel two-level framework that integrates uni-modal and multi-modal (e.g., taxi-bus, bus-cycle) routes as well as heterogeneous urban data for intelligent multi-modal transportation recommendation. In addition to urban context features constructed from multi-source urban data, we learn the latent representations of users, origin-destination (OD) pairs and transportation modes based on user implicit feedbacks, which captures the collaborative transportation mode preferences of users and OD pairs. Moreover, we propose two models to recommend the proper route among various uni-modal and multi-modal transportation routes: (1) a light-weight gradient boosting decision tree (GBDT) based recommendation model; and (2) a multi-task wide and deep learning (MTWDL) based recommendation model. We also optimize the framework to support real-time, large-scale route query and recommendation. We deploy <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq2-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> on Baidu Maps,<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref><fn id=""fn1""><label>1.</label><p><uri>https://maps.baidu.com/</uri>.</p> </fn> one of the world's largest map services. Real-world urban-scale experiments demonstrate the effectiveness and efficiency of our proposed system. Since its deployment in August 2018, <inline-formula><tex-math notation=""LaTeX"">$\mathsf {Hydra}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi mathvariant=""sans-serif"">Hydra</mml:mi></mml:math><inline-graphic xlink:href=""liu-ieq3-2985954.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> has answered over a hundred million route recommendation queries made by over ten million distinct users. The GBDT based model and MTWDL based model achieve 82.8 and 96.6 percent relative improvement of user click ratio, respectively.",smart cities
10.1109/scset55041.2022.00053,preprocessed,2022 International Seminar on Computer Science and Engineering Technology (SCSET),IEEE,2022-01-09 00:00:00,ieeexplore,research on leak location method of water supply network based on deep neural network model,https://ieeexplore.ieee.org/document/9700905/,"The water supply network is one of the important infrastructure in urban construction. It has strong theoretical and practical significance to realize the real-time monitoring and leak location of the water supply network. In this paper, based on the similarity of water supply network node pressure, fuzzy C-means clustering algorithm is used to realize the selection of finite monitoring points. On this basis, a depth neural network model is constructed according to the pressure changes of the monitoring points before and after the leakage of the water supply network, so as to locate the leakage points. In the experimental part, hydraulics simulation was conducted by using EPANETH pipe network adjustment software according to the layout structure of water supply network, and the pressure of all nodes was obtained. A deep neural network model was established by Keras in Tensorflow framework. After model training and testing, the training error was controlled within the effective range of 5 %. Finally, the model is applied to the actual leakage problem of underground water supply network in Langxi County of Xuancheng City, and the accurate location of the leakage point is realized. The experiment proves the feasibility and accuracy of the method proposed in this paper.",smart cities
10.1109/tgrs.2021.3056624,preprocessed,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,attention-based multiscale residual adaptation network for cross-scene classification,https://ieeexplore.ieee.org/document/9377566/,"In recent years, classification has obtained ever-rising attention and has been applied to many areas in the field of remote sensing, including land use, forest monitoring, urban planning, and vegetation management. Due to the lack of labeled data and the poor generalization ability of supervised models, cross-scene classification is proposed for better utilization of the existing knowledge. Existing adaptation methods for cross-scene classification only consider the marginal distribution, while the conditional distribution is equally important in real applications. In addition, approaches based on deep learning align the distribution of features extracted from a single-scale structure, leading to the loss of information. To overcome the above drawbacks, an Attention-based Multiscale Residual Adaptation Network (AMRAN) is proposed for cross-scene classification tasks. In the proposed AMRAN, both the marginal and conditional distributions are taken into consideration for more comprehensive alignment. Besides, the attention mechanism and the multiscale strategy are used to extract more robust features and more complete information, respectively. Experimental results between four existing scene classification data sets demonstrate that AMRAN has a significant improvement compared with the state-of-the-art deep adaptation methods.",smart cities
10.1109/access.2021.3137031,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,autonomous detection and deterrence of pigeons on buildings by drones,https://ieeexplore.ieee.org/document/9656717/,"Pigeons may transmit diseases to humans and cause damages to buildings, monuments, and other infrastructure. Therefore, several control strategies have been developed, but they have been found to be either ineffective or harmful to animals and often depend on human operation. This study proposes a system capable of autonomously detecting and deterring pigeons on building roofs using a drone. The presence and position of pigeons were detected in real time by a neural network using images taken by a video camera located on the roof. Moreover, a drone was utilized to deter the animals. Field experiments were conducted in a real-world urban setting to assess the proposed system by comparing the number of animals and their stay durations for over five days against the 21-day-trial experiment without the drone. During the five days of experiments, the drone was automatically deployed 55 times and was significantly effective in reducing the number of birds and their stay durations without causing any harm to them. In conclusion, this study has proven the effectiveness of this system in deterring birds, and this approach can be seen as a fully autonomous alternative to the already existing methods.",smart cities
10.1109/lgrs.2020.3030839,preprocessed,IEEE Geoscience and Remote Sensing Letters,IEEE,2000-01-01 00:00:00,ieeexplore,hybrid attention networks for flow and pressure forecasting in water distribution systems,https://ieeexplore.ieee.org/document/9241394/,"Multivariate geo-sensory time series prediction is challenging because of the complex spatial and temporal correlations. In urban water distribution systems (WDSs), numerous spatial-correlated sensors have been deployed to continuously collect hydraulic data. Forecasts of the monitored flow and pressure time series are of vital importance for operational decision making, alerts, and anomaly detection. To address this issue, we proposed a hybrid dual-stage spatial–temporal attention-based recurrent neural networks (hDS-RNN). Our model consists of two stages: a spatial attention-based encoder and a temporal attention-based decoder. Specifically, a hybrid spatial attention mechanism that employs inputs along the temporal and spatial axes is proposed. Experiments on a real-world data set are conducted, which demonstrate that our model outperformed seven baseline models in flow and pressure predictions in WDS.",smart cities
10.1109/jstars.2022.3142898,preprocessed,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,wh-mavs: a novel dataset and deep learning benchmark for multiple land use and land cover applications,https://ieeexplore.ieee.org/document/9681304/,"Over the past decade, many excellent data sharing efforts have enriched the remote sensing scene classification (SC) methods. These datasets have achieved great success in complex high-level semantic information interpretation. However, most existing datasets are collected from standard and ungeoreferenced image patches for algorithm training and evaluation. These datasets do not fit for practical applications and cannot be directly applied in further geographical study. Accordingly, we provide a large range high-resolution SC dataset with multiple time phases, called “<bold>W</bold>u<bold>h</bold>an <bold>M</bold>ulti<bold>a</bold>pplication <bold>V</bold>HR <bold>S</bold>cene classification dataset (WH-MAVS).” It facilitates the study of SC and scene change detection (SCD) algorithms. Moreover, it can also be directly employed to perform a variety of real-life land use application tasks. To the best of our knowledge, this is the first free, publicly available, georeferenced, and annotated dataset to cover almost an entire megacity. The WH-MAVS was collected and annotated from Google Earth imagery with the same spatial resolution and uniform nonoverlapping patch size, covering the central area of Wuhan, China. The total number of scene samples is 47 137, which belong to 14 classes with 23 567 labeled patches for each time phase in 2014 and 2016, respectively. The geographic coordinates of all samples in both time phases exhibit one-to-one correspondence with 23 202 unchanged image patches of scene categories and 365 changed ones. The distribution of the number of samples in each class is highly imbalanced; moreover, there are large intraclass differences and indistinguishable interclass variances. These characteristics are closer to the real land use/land cover application tasks and introduce further challenges to the related algorithm research. In addition, we conducted benchmark experiments on SC and SCD based on the WH-MAVS dataset with widely used deep learning models. DenseNet169 was found to achieve the best performance. The overall accuracies are 91.07% and 92.09%, respectively, in the 2014 and 2016 validation sets of WH-MAVS. Furthermore, SCD obtained by DenseNet169 has a binary change detection accuracy of 89.56% and a multiple (from–to) change detection accuracy of 86.70%. Over and above the research value of the algorithm, it is also proven to have practical applications in fields such as urban planning, landscape pattern analysis, and urban dynamic monitoring and analysis.",smart cities
10.1109/ccnc49033.2022.9700579,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,demo: an experimental environment based on mini-pcs for federated learning research,https://ieeexplore.ieee.org/document/9700579/,"There is a growing research interest in Federated Learning (FL), a promising approach for data privacy preservation and proximity of training to the network edge, where data is generated. Resource consumption for Machine Learning (ML) training and inference is important for edge nodes, but most of the proposed protocols and algorithms for FL are evaluated by simulations. In this demo paper, we present an environment based on distributed mini-PCs to enable experimental study of FL protocols and algorithms. We have installed low-capacity mini-PCs within a wireless city-level mesh network and deployed container-based FL components on these nodes. We show the deployed FL clients and server at different nodes in the city and demonstrate how an FL experiment can be set and run in a real environment.",smart cities
10.1109/wacv51458.2022.00308,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,multi-branch neural networks for video anomaly detection in adverse lighting and weather conditions,https://ieeexplore.ieee.org/document/9706717/,"Automated anomaly detection in surveillance videos has attracted much interest as it provides a scalable alternative to manual monitoring. Most existing approaches achieve good performance on clean benchmark datasets recorded in well-controlled environments. However, detecting anomalies is much more challenging in the real world. Adverse weather conditions like rain or changing brightness levels cause a significant shift in the input data distribution, which in turn can lead to the detector model incorrectly reporting high anomaly scores. Additionally, surveillance cameras are usually deployed in evolving environments such as a city street of which the appearance changes over time because of seasonal changes or roadworks. The anomaly detection model will need to be updated periodically to deal with these issues. In this paper, we introduce a multi-branch model that is equipped with a trainable preprocessing step and multiple identical branches for detecting anomalies during day and night as well as in sunny and rainy conditions. We experimentally validate our approach on a distorted version of the Avenue dataset and provide qualitative results on real-world surveillance camera data. Experimental results show that our method outperforms the existing methods in terms of detection accuracy while being faster and more robust on scenes with varying visibility.",smart cities
10.1109/tii.2021.3091597,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,optimal sizing and efficient routing of electric vehicles for a vehicle-on-demand system,https://ieeexplore.ieee.org/document/9462468/,"Due to the steep rise in global population, urbanization, and industrialization, most of the cities in the world today are witnessing increased carbon footprints and reduced per capita space. In such a scenario, vehicle sharing and carpooling systems, specifically with electric vehicles (EV), can significantly help due to the reduced cost of ownership, maintenance, and parking space. In this article, we study the challenging problem of optimal sizing and efficient routing for an electric vehicle-on-demand system. Users demand EVs at the pooling stations at different time instances with individual deadlines to reach the destinations. The objective is to fulfill all the demands respecting the deadlines with minimum investment, which essentially translates to minimizing the total number of EVs. We define the problem formally using mixed-integer linear programming formulation and propose a set of intelligent and efficient heuristic algorithms to solve it efficiently. The proposed algorithms’ performances are tested and validated in a simulated environment on a reasonable size city network with many EV demands. The results obtained show that the proposed heuristic algorithms are competent by reducing 200–360 EVs per day on a network of 282 charging ports, indicating their scalability to be implemented in real-world scenarios.",smart cities
10.1109/tits.2020.3029537,preprocessed,IEEE Transactions on Intelligent Transportation Systems,IEEE,2022-02-01 00:00:00,ieeexplore,spatial positioning token (sptoken) for smart mobility,https://ieeexplore.ieee.org/document/9238413/,"We introduce a permissioned distributed ledger technology (DLT) design for crowdsourced smart mobility applications. This architecture is based on a directed acyclic graph architecture (similar to the IOTA tangle) and uses both Proof-of-Work and Proof-of-Position mechanisms to provide protection against spam attacks and malevolent actors. In addition to enabling individuals to retain ownership of their data and to monetize it, the architecture is also suitable for distributed privacy-preserving machine learning algorithms, is lightweight, and can be implemented in simple internet-of-things (IoT) devices. To demonstrate its efficacy, we apply this framework to reinforcement learning settings where a third party is interested in acquiring information from agents. In particular, one may be interested in sampling an unknown vehicular traffic flow in a city, using a DLT-type architecture and without perturbing the density, with the idea of realizing a set of virtual tokens as surrogates of real vehicles to explore geographical areas of interest. These tokens, whose authenticated position determines write access to the ledger, are thus used to emulate the probing actions of commanded (real) vehicles on a given planned route by “jumping” from a passing-by vehicle to another to complete the planned trajectory. Consequently, the environment stays unaffected (i.e., the autonomy of participating vehicles is not influenced by the algorithm), regardless of the number of emitted tokens. The design of such a DLT architecture is presented, and numerical results from large-scale simulations are provided to validate the proposed approach.",smart cities
10.1109/tits.2020.3015542,preprocessed,IEEE Transactions on Intelligent Transportation Systems,IEEE,2022-02-01 00:00:00,ieeexplore,taxi demand prediction using parallel multi-task learning model,https://ieeexplore.ieee.org/document/9172100/,"Accurate and real-time taxi demand prediction can help managers pre-allocate taxi resources in cities, which assists drivers quickly finding passengers and reduce passengers’ waiting time. Most of the existing studies focus on mining spatial-temporal characteristics of taxi demand distributions, while lacking in modeling the correlations between taxi pick-up demand and the drop-off demand from the perspective of multi-task learning. In this article, we propose a multi-task learning model containing three parallel LSTM layers to co-predict taxi pick-up and drop-off demands, and compare the performance of single demand prediction methodology and that of two demands’ co-prediction methodology. Experimental results on real-world datasets demonstrate that the pick-up demand and the drop-off demand do depend on each other, and the effectiveness of the proposed co-prediction methods.",smart cities
10.1109/tii.2021.3071771,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-02-01 00:00:00,ieeexplore,a utility-based subcontract method for sensing task in mobile crowd sensing,https://ieeexplore.ieee.org/document/9399247/,"In mobile crowd sensing, the mobile terminal integrates a variety of widely distributed sensing devices and communication ports. Sensing devices and communication ports can collect and share all kinds of perception data. However, inherent contradictions exist among perceived ability, communication port, and moving rule while collecting real-time and accurate sensing information. This article mainly focused on recruited and selected mobile nodes and assigned sensing tasks to improve the quality of sensing information. The optimization of the implementation stage of the sensing task is beyond the scope of this study. This article proposes a utility-based sensing task decomposition and subcontract algorithm, which is a method of sensing data acquisition that establishes direct collaboration between mobile nodes. A mobility model based on Markov chain is established to forecast the spatial distribution of sensing nodes. A utility function is designed to estimate the sensing task execution capacity of sensing nodes based on spatial distribution and tempo-spatial coverage of the collected data. The sensing tasks are then decomposed and subcontracted to neighboring nodes according to the utilities of the neighboring nodes to the decomposed sensing tasks. This method improves the quality of sensing data in terms of sensing data coverage and finished ratio of sensing task.",smart cities
10.1109/access.2022.3146728,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,assistive devices analysis for visually impaired persons: a review on taxonomy,https://ieeexplore.ieee.org/document/9693966/,"Visually impaired persons (VIPs) comprise a significant portion of the population and they are present in all corners of the world. In recent times, the technology proved its presence in every domain of life and innovative devices are assisting humans in all fields especially, artificial intelligence has dominated and outperformed the rest of the trades. VIPs need assistance in performing daily life tasks like object/obstacle detection and recognition, navigation, and mobility, particularly in indoor and outdoor environments. Moreover, the protection and safety of these people are of prime concern. Several devices and applications have been developed for the assistance of VIPs. Firstly, these devices take input from the surrounding environment through different sensors e.g. infrared radiation, ultrasonic, imagery sensor, etc. In the second stage, state of the art machine learning techniques process these signals and extract useful information. Finally, feedback is provided to the user through auditory and/or vibratory means. It is observed that most of the existing devices are constrained in their abilities. The paper presents a comprehensive comparative analysis of the state-of-the-art assistive devices for VIPs. These techniques are categorized based on their functionality and working principles. The main attributes, challenges, and limitations of these techniques have also been highlighted. Moreover, a score-based quantitative analysis of these devices is performed to highlight their feature enrichment capability for each category. It may help to select an appropriate device for a particular scenario.",smart cities
10.1109/tnse.2021.3072911,preprocessed,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,prioritized content determination and dissemination using reinforcement learning in dtns,https://ieeexplore.ieee.org/document/9403937/,"In a battlefield, several groups of soldiers are deployed with different mission goals by the command and control center (CC). To continue the missions appropriately and get a better understanding of the situation, the soldiers, as well as the CC, need to collect information of interest generated in different battle zones. However, due to the damaged network infrastructure in the hostile areas, it is a challenge to determine the topics of interest associated with the events and missions, and efficiently forward the associated content to the CC. Hence, the devices of the soldiers (nodes) generate, store and forward content hop by hop using a Delay Tolerant Network (DTN). While forwarding content, nodes avoid congestion so that meaningful content related to prioritized mission goals can be disseminated. In this dynamic surrounding, any sudden but important event-related content should also be sent to the CC with the help of intermediate nodes regardless of their own mission interests. We design a scheme to forward contents generated by the nodes to the CC using Reinforcement Learning (RL) while maximizing the number of interesting data in the respective nodes' buffer, and avoiding congestion. In this forwarding process, we focus on identifying the trending topics/keywords among changing missions and their related data at the node level, and the changes of interest of the nodes based on their mobility and connectivity patterns. Experiments are conducted using real datasets and ONE simulator to show the effectiveness of Reinforcement Learning (RL) on the prioritized content dissemination in a DTN.",smart cities
10.1109/ojits.2021.3139393,preprocessed,IEEE Open Journal of Intelligent Transportation Systems,IEEE,2000-01-01 00:00:00,ieeexplore,napc: a neural algorithm for automated passenger counting in public transport on a privacy-friendly dataset,https://ieeexplore.ieee.org/document/9665722/,"Real-time load information in public transport is of high importance for both passengers and service providers. Neural algorithms have shown a high performance on various object counting tasks and play a continually growing methodological role in developing automated passenger counting systems. However, the publication of public-space video footage is often contradicted by legal and ethical considerations to protect the passengers’ privacy. This work proposes an end-to-end Long Short-Term Memory network with a problem-adapted cost function that learned to count boarding and alighting passengers on a publicly available, comprehensive dataset of approx.13,000 manually annotated low-resolution 3D LiDAR video recordings (depth information only) from the doorways of a regional train. These depth recordings do not allow the identification of single individuals. For each door opening phase, the trained models predict the correct passenger count (ranging from 0 to 67) in approx.96% of boarding and alighting, respectively. Repeated training with different training and validation sets confirms the independence of this result from a specific test set.",smart cities
10.1109/jsen.2021.3132460,preprocessed,IEEE Sensors Journal,IEEE,2001-02-01 20:22:00,ieeexplore,automatic rail component detection based on attnconv-net,https://ieeexplore.ieee.org/document/9634063/,"The automatic detection of major rail components using railway images is beneficial to ensure the rail transport safety. In this paper, we propose an attention-powered deep convolutional network (AttnConv-net) to detect multiple rail components including the rail, clips, and bolts. The proposed method consists of a deep convolutional neural network (DCNN) as the backbone, cascading attention blocks (CAB), and two feed forward networks (FFN). Two types of positional embedding are applied to enrich information in latent features extracted from the backbone. Based on processed latent features, the CAB aims to learn the local context of rail components including their categories and component boundaries. Final categories and bounding boxes are generated via two FFN implemented in parallel. To enhance the detection of small components, various data augmentation methods are employed in training process. The effectiveness of the proposed AttnConv-net is validated with one real dataset and another synthesized dataset. Compared with classic convolutional neural network based methods, our proposed method simplifies the detection pipeline by eliminating the need of prior- and post-processing, which offers a new speed-quality solution to enable faster and more accurate image-based rail component detections.",smart cities
http://arxiv.org/abs/2202.08982v1,preprocessed,arxiv,arxiv,2022-02-18 00:00:00,arxiv,"pgcn: progressive graph convolutional networks for spatial-temporal
  traffic forecasting",http://arxiv.org/abs/2202.08982v1,"The complex spatial-temporal correlations in transportation networks make the
traffic forecasting problem challenging. Since transportation system inherently
possesses graph structures, much research efforts have been put with graph
neural networks. Recently, constructing adaptive graphs to the data has shown
promising results over the models relying on a single static graph structure.
However, the graph adaptations are applied during the training phases, and do
not reflect the data used during the testing phases. Such shortcomings can be
problematic especially in traffic forecasting since the traffic data often
suffers from the unexpected changes and irregularities in the time series. In
this study, we propose a novel traffic forecasting framework called Progressive
Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by
progressively adapting to input data during the training and the testing
phases. Specifically, we implemented the model to construct progressive
adjacency matrices by learning trend similarities among graph nodes. Then, the
model is combined with the dilated causal convolution and gated activation unit
to extract temporal features. With residual and skip connections, PGCN performs
the traffic prediction. When applied to four real-world traffic datasets of
diverse geometric nature, the proposed model achieves state-of-the-art
performance with consistency in all datasets. We conclude that the ability of
PGCN to progressively adapt to input data enables the model to generalize in
different study sites with robustness.",smart cities
http://arxiv.org/abs/2202.07147v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,"graph meta-reinforcement learning for transferable autonomous
  mobility-on-demand",http://arxiv.org/abs/2202.07147v1,"Autonomous Mobility-on-Demand (AMoD) systems represent an attractive
alternative to existing transportation paradigms, currently challenged by
urbanization and increasing travel needs. By centrally controlling a fleet of
self-driving vehicles, these systems provide mobility service to customers and
are currently starting to be deployed in a number of cities around the world.
Current learning-based approaches for controlling AMoD systems are limited to
the single-city scenario, whereby the service operator is allowed to take an
unlimited amount of operational decisions within the same transportation
system. However, real-world system operators can hardly afford to fully
re-train AMoD controllers for every city they operate in, as this could result
in a high number of poor-quality decisions during training, making the
single-city strategy a potentially impractical solution. To address these
limitations, we propose to formalize the multi-city AMoD problem through the
lens of meta-reinforcement learning (meta-RL) and devise an actor-critic
algorithm based on recurrent graph neural networks. In our approach, AMoD
controllers are explicitly trained such that a small amount of experience
within a new city will produce good system performance. Empirically, we show
how control policies learned through meta-RL are able to achieve near-optimal
performance on unseen cities by learning rapidly adaptable policies, thus
making them more robust not only to novel environments, but also to
distribution shifts common in real-world operations, such as special events,
unexpected congestion, and dynamic pricing schemes.",smart cities
http://arxiv.org/abs/2202.06639v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"on the complexity of object detection on real-world public
  transportation images for social distancing measurement",http://arxiv.org/abs/2202.06639v1,"Social distancing in public spaces has become an essential aspect in helping
to reduce the impact of the COVID-19 pandemic. Exploiting recent advances in
machine learning, there have been many studies in the literature implementing
social distancing via object detection through the use of surveillance cameras
in public spaces. However, to date, there has been no study of social distance
measurement on public transport. The public transport setting has some unique
challenges, including some low-resolution images and camera locations that can
lead to the partial occlusion of passengers, which make it challenging to
perform accurate detection. Thus, in this paper, we investigate the challenges
of performing accurate social distance measurement on public transportation. We
benchmark several state-of-the-art object detection algorithms using real-world
footage taken from the London Underground and bus network. The work highlights
the complexity of performing social distancing measurement on images from
current public transportation onboard cameras. Further, exploiting domain
knowledge of expected passenger behaviour, we attempt to improve the quality of
the detections using various strategies and show improvement over using vanilla
object detection alone.",smart cities
http://arxiv.org/abs/2202.06608v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"unscene: toward unsupervised scenario extraction for automated driving
  systems from urban naturalistic road traffic data",http://arxiv.org/abs/2202.06608v1,"Scenario-based testing is a promising approach to solve the challenge of
proving the safe behavior of vehicles equipped with automated driving systems
(ADS). Since an infinite number of concrete scenarios can theoretically occur
in real-world road traffic, the extraction of relevant scenarios that are
sensitive regarding the safety-related behavior of ADS-equipped vehicles is a
key aspect for the successful verification and validation of these systems.
Therefore, this paper provides a method for extracting multimodal urban traffic
scenarios from naturalistic road traffic data in an unsupervised manner for
minimizing the amount of (potentially biased) prior expert knowledge needed.
Rather than an (expensive) rule-based assignment by extracting concrete
scenarios into predefined functional scenarios, the presented method deploys an
unsupervised machine learning pipeline. It includes principal feature analysis,
feature extraction with so-called scenario grids, dimensionality reduction by
principal component analysis, scenario clustering as well as cluster
validation. The approach allows exploring the unknown natures of the data and
interpreting them as scenarios that experts could not have anticipated. The
method is demonstrated and evaluated for naturalistic road traffic data at
urban intersections from the inD and the Silicon Valley dataset. The findings
encourage the use of this type of data as well as unsupervised machine learning
approaches as important pillar for a systematic construction of a relevant
scenario database with sufficient coverage for testing ADS.",smart cities
http://arxiv.org/abs/2202.05334v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"learning the pedestrian-vehicle interaction for pedestrian trajectory
  prediction",http://arxiv.org/abs/2202.05334v1,"In this paper, we study the interaction between pedestrians and vehicles and
propose a novel neural network structure called the Pedestrian-Vehicle
Interaction (PVI) extractor for learning the pedestrian-vehicle interaction. We
implement the proposed PVI extractor on both sequential approaches (long
short-term memory (LSTM) models) and non-sequential approaches (convolutional
models). We use the Waymo Open Dataset that contains real-world urban traffic
scenes with both pedestrian and vehicle annotations. For the LSTM-based models,
our proposed model is compared with Social-LSTM and Social-GAN, and using our
proposed PVI extractor reduces the average displacement error (ADE) and the
final displacement error (FDE) by 7.46% and 5.24%, respectively. For the
convolutional-based models, our proposed model is compared with Social-STGCNN
and Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and
FDE by 2.10% and 1.27%, respectively. The results show that the
pedestrian-vehicle interaction influences pedestrian behavior, and the models
using the proposed PVI extractor can capture the interaction between
pedestrians and vehicles, and thereby outperform the compared methods.",smart cities
http://arxiv.org/abs/2202.05118v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"reinforcement learning in the wild: scalable rl dispatching algorithm
  deployed in ridehailing marketplace",http://arxiv.org/abs/2202.05118v1,"In this study, a real-time dispatching algorithm based on reinforcement
learning is proposed and for the first time, is deployed in large scale.
Current dispatching methods in ridehailing platforms are dominantly based on
myopic or rule-based non-myopic approaches. Reinforcement learning enables
dispatching policies that are informed of historical data and able to employ
the learned information to optimize returns of expected future trajectories.
Previous studies in this field yielded promising results, yet have left room
for further improvements in terms of performance gain, self-dependency,
transferability, and scalable deployment mechanisms. The present study proposes
a standalone RL-based dispatching solution that is equipped with multiple
mechanisms to ensure robust and efficient on-policy learning and inference
while being adaptable for full-scale deployment. A new form of value updating
based on temporal difference is proposed that is more adapted to the inherent
uncertainty of the problem. For the driver-order assignment, a customized
utility function is proposed that when tuned based on the statistics of the
market, results in remarkable performance improvement and interpretability. In
addition, for reducing the risk of cancellation after drivers' assignment, an
adaptive graph pruning strategy based on the multi-arm bandit problem is
introduced. The method is evaluated using offline simulation with real data and
yields notable performance improvement. In addition, the algorithm is deployed
online in multiple cities under DiDi's operation for A/B testing and is
launched in one of the major international markets as the primary mode of
dispatch. The deployed algorithm shows over 1.3% improvement in total driver
income from A/B testing. In addition, by causal inference analysis, as much as
5.3% improvement in major performance metrics is detected after full-scale
deployment.",smart cities
http://arxiv.org/abs/2202.04628v2,preprocessed,arxiv,arxiv,2022-02-09 00:00:00,arxiv,"reinforcement learning with sparse rewards using guidance from offline
  demonstration",http://arxiv.org/abs/2202.04628v2,"A major challenge in real-world reinforcement learning (RL) is the sparsity
of reward feedback. Often, what is available is an intuitive but sparse reward
function that only indicates whether the task is completed partially or fully.
However, the lack of carefully designed, fine grain feedback implies that most
existing RL algorithms fail to learn an acceptable policy in a reasonable time
frame. This is because of the large number of exploration actions that the
policy has to perform before it gets any useful feedback that it can learn
from. In this work, we address this challenging problem by developing an
algorithm that exploits the offline demonstration data generated by a
sub-optimal behavior policy for faster and efficient online RL in such sparse
reward settings. The proposed algorithm, which we call the Learning Online with
Guidance Offline (LOGO) algorithm, merges a policy improvement step with an
additional policy guidance step by using the offline demonstration data. The
key idea is that by obtaining guidance from - not imitating - the offline data,
LOGO orients its policy in the manner of the sub-optimal policy, while yet
being able to learn beyond and approach optimality. We provide a theoretical
analysis of our algorithm, and provide a lower bound on the performance
improvement in each learning episode. We also extend our algorithm to the even
more challenging incomplete observation setting, where the demonstration data
contains only a censored version of the true state observation. We demonstrate
the superior performance of our algorithm over state-of-the-art approaches on a
number of benchmark environments with sparse rewards and censored state.
Further, we demonstrate the value of our approach via implementing LOGO on a
mobile robot for trajectory tracking and obstacle avoidance, where it shows
excellent performance.",smart cities
http://arxiv.org/abs/2202.03917v1,preprocessed,arxiv,arxiv,2022-02-08 00:00:00,arxiv,edge-based fever screening system over private 5g,http://arxiv.org/abs/2202.03917v1,"Edge computing and 5G have made it possible to perform analytics closer to
the source of data and achieve super-low latency response times, which is not
possible with centralized cloud deployment. In this paper, we present a novel
fever-screening system, which uses edge machine learning techniques and
leverages private 5G to accurately identify and screen individuals with fever
in real-time. Particularly, we present deep-learning based novel techniques for
fusion and alignment of cross-spectral visual and thermal data streams at the
edge. Our novel Cross-Spectral Generative Adversarial Network (CS-GAN)
synthesizes visual images that have the key, representative object level
features required to uniquely associate objects across visual and thermal
spectrum. Two key features of CS-GAN are a novel, feature-preserving loss
function that results in high-quality pairing of corresponding cross-spectral
objects, and dual bottleneck residual layers with skip connections (a new,
network enhancement) to not only accelerate real-time inference, but to also
speed up convergence during model training at the edge. To the best of our
knowledge, this is the first technique that leverages 5G networks and limited
edge resources to enable real-time feature-level association of objects in
visual and thermal streams (30 ms per full HD frame on an Intel Core i7-8650
4-core, 1.9GHz mobile processor). To the best of our knowledge, this is also
the first system to achieve real-time operation, which has enabled fever
screening of employees and guests in arenas, theme parks, airports and other
critical facilities. By leveraging edge computing and 5G, our fever screening
system is able to achieve 98.5% accuracy and is able to process about 5X more
people when compared to a centralized cloud deployment.",smart cities
http://arxiv.org/abs/2202.02653v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,"millisecond speed deep learning based proton dose calculation with monte
  carlo accuracy",http://arxiv.org/abs/2202.02653v1,"Next generation online and real-time adaptive radiotherapy workflows require
precise particle transport simulations in sub-second times, which is unfeasible
with current analytical pencil beam algorithms (PBA) or stochastic Monte Carlo
(MC) methods. We present a data-driven millisecond speed dose calculation
algorithm (DoTA) accurately predicting the dose deposited by mono-energetic
proton pencil beams for arbitrary energies and patient geometries. Given the
forward-scattering nature of protons, we frame 3D particle transport as
modeling a sequence of 2D geometries in the beam's eye view. DoTA combines
convolutional neural networks extracting spatial features (e.g., tissue and
density contrasts) with a transformer self-attention backbone that routes
information between the sequence of geometry slices and a vector representing
the beam's energy, and is trained to predict low noise MC simulations of proton
beamlets using 80,000 different head and neck, lung, and prostate geometries.
Predicting beamlet doses in 5 ms with a very high gamma pass rate of 99.37%
(1%, 3 mm) compared to the ground truth MC calculations, DoTA significantly
improves upon analytical pencil beam algorithms both in precision and speed.
Offering MC accuracy 100 times faster than PBAs for pencil beams, our model
calculates full treatment plan doses in 10 to 15 s depending on the number of
beamlets, achieving a 99.70% (2%, 2 mm) gamma pass rate across 9 test patients.
Outperforming all previous analytical pencil beam and deep learning based
approaches, DoTA represents a new state of the art in data-driven dose
calculation and can directly compete with the speed of even commercial GPU MC
approaches. Providing the sub-second speed required for adaptive treatments,
straightforward implementations could offer similar benefits to other steps of
the radiotherapy workflow or other modalities such as helium or carbon
treatments.",smart cities
http://arxiv.org/abs/2202.01862v1,preprocessed,arxiv,arxiv,2022-02-03 00:00:00,arxiv,practical imitation learning in the real world via task consistency loss,http://arxiv.org/abs/2202.01862v1,"Recent work in visual end-to-end learning for robotics has shown the promise
of imitation learning across a variety of tasks. Such approaches are expensive
both because they require large amounts of real world training demonstrations
and because identifying the best model to deploy in the real world requires
time-consuming real-world evaluations. These challenges can be mitigated by
simulation: by supplementing real world data with simulated demonstrations and
using simulated evaluations to identify high performing policies. However, this
introduces the well-known ""reality gap"" problem, where simulator inaccuracies
decorrelate performance in simulation from that of reality. In this paper, we
build on top of prior work in GAN-based domain adaptation and introduce the
notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages
sim and real alignment both at the feature and action-prediction levels. We
demonstrate the effectiveness of our approach by teaching a mobile manipulator
to autonomously approach a door, turn the handle to open the door, and enter
the room. The policy performs control from RGB and depth images and generalizes
to doors not encountered in training data. We achieve 80% success across ten
seen and unseen scenes using only ~16.2 hours of teleoperated demonstrations in
sim and real. To the best of our knowledge, this is the first work to tackle
latched door opening from a purely end-to-end learning approach, where the task
of navigation and manipulation are jointly modeled by a single neural network.",smart cities
http://arxiv.org/abs/2201.09419v1,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,"automated machine learning for secure key rate in discrete-modulated
  continuous-variable quantum key distribution",http://arxiv.org/abs/2201.09419v1,"Continuous-variable quantum key distribution (CV QKD) with discrete
modulation has attracted increasing attention due to its experimental
simplicity, lower-cost implementation and compatibility with classical optical
communication. Correspondingly, some novel numerical methods have been proposed
to analyze the security of these protocols against collective attacks, which
promotes key rates over one hundred kilometers of fiber distance. However,
numerical methods are limited by their calculation time and resource
consumption, for which they cannot play more roles on mobile platforms in
quantum networks. To improve this issue, a neural network model predicting key
rates in nearly real time has been proposed previously. Here, we go further and
show a neural network model combined with Bayesian optimization. This model
automatically designs the best architecture of neural network computing key
rates in real time. We demonstrate our model with two variants of CV QKD
protocols with quaternary modulation. The results show high reliability with
secure probability as high as $99.15\%-99.59\%$, considerable tightness and
high efficiency with speedup of approximately $10^7$ in both cases. This
inspiring model enables the real-time computation of unstructured quantum key
distribution protocols' key rate more automatically and efficiently, which has
met the growing needs of implementing QKD protocols on moving platforms.",smart cities
http://arxiv.org/abs/2201.05858v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"smart parking space detection under hazy conditions using convolutional
  neural networks: a novel approach",http://arxiv.org/abs/2201.05858v1,"Limited urban parking space combined with urbanization has necessitated the
development of smart parking systems that can communicate the availability of
parking slots to the end users. Towards this, various deep learning based
solutions using convolutional neural networks have been proposed for parking
space occupation detection. Though these approaches are robust to partial
obstructions and lighting conditions, their performance is found to degrade in
the presence of haze conditions. Looking in this direction, this paper
investigates the use of dehazing networks that improves the performance of
parking space occupancy classifier under hazy conditions. Additionally,
training procedures are proposed for dehazing networks to maximize the
performance of the system on both hazy and non-hazy conditions. The proposed
system is deployable as part of existing smart parking systems where limited
number of cameras are used to monitor hundreds of parking spaces. To validate
our approach, we have developed a custom hazy parking system dataset from
real-world task-driven test set of RESIDE-\b{eta} dataset. The proposed
approach is tested against existing state-of-the-art parking space detectors on
CNRPark-EXT and hazy parking system datasets. Experimental results indicate
that there is a significant accuracy improvement of the proposed approach on
the hazy parking system dataset.",smart cities
http://arxiv.org/abs/2201.05024v2,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"real-time gpu-accelerated machine learning based multiuser detection for
  5g and beyond",http://arxiv.org/abs/2201.05024v2,"Adaptive partial linear beamforming meets the need of 5G and future 6G
applications for high flexibility and adaptability. Choosing an appropriate
tradeoff between conflicting goals opens the recently proposed multiuser (MU)
detection method. Due to their high spatial resolution, nonlinear beamforming
filters can significantly outperform linear approaches in stationary scenarios
with massive connectivity. However, a dramatic decrease in performance can be
expected in high mobility scenarios because they are very susceptible to
changes in the wireless channel. The robustness of linear filters is required,
considering these changes. One way to respond appropriately is to use online
machine learning algorithms. The theory of algorithms based on the adaptive
projected subgradient method (APSM) is rich, and they promise accurate tracking
capabilities in dynamic wireless environments. However, one of the main
challenges comes from the real-time implementation of these algorithms, which
involve projections on time-varying closed convex sets. While the projection
operations are relatively simple, their vast number poses a challenge in
ultralow latency (ULL) applications where latency constraints must be satisfied
in every radio frame. Taking non-orthogonal multiple access (NOMA) systems as
an example, this paper explores the acceleration of APSM-based algorithms
through massive parallelization. The result is a GPU-accelerated real-time
implementation of an orthogonal frequency-division multiplexing (OFDM)-based
transceiver that enables detection latency of less than one millisecond and
therefore complies with the requirements of 5G and beyond. To meet the
stringent physical layer latency requirements, careful co-design of hardware
and software is essential, especially in virtualized wireless systems with
hardware accelerators.",smart cities
http://arxiv.org/abs/2201.04349v1,preprocessed,arxiv,arxiv,2022-01-12 00:00:00,arxiv,video intelligence as a component of a global security system,http://arxiv.org/abs/2201.04349v1,"This paper describes the evolution of our research from video analytics to a
global security system with focus on the video surveillance component. Indeed
video surveillance has evolved from a commodity security tool up to the most
efficient way of tracking perpetrators when terrorism hits our modern urban
centers. As number of cameras soars, one could expect the system to leverage
the huge amount of data carried through the video streams to provide fast
access to video evidences, actionable intelligence for monitoring real-time
events and enabling predictive capacities to assist operators in their
surveillance tasks. This research explores a hybrid platform for video
intelligence capture, automated data extraction, supervised Machine Learning
for intelligently assisted urban video surveillance; Extension to other
components of a global security system are discussed. Applying Knowledge
Management principles in this research helps with deep problem understanding
and facilitates the implementation of efficient information and experience
sharing decision support systems providing assistance to people on the field as
well as in operations centers. The originality of this work is also the
creation of ""common"" human-machine and machine to machine language and a
security ontology.",smart cities
http://arxiv.org/abs/2201.03808v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,mobilefaceswap: a lightweight framework for video face swapping,http://arxiv.org/abs/2201.03808v1,"Advanced face swapping methods have achieved appealing results. However, most
of these methods have many parameters and computations, which makes it
challenging to apply them in real-time applications or deploy them on edge
devices like mobile phones. In this work, we propose a lightweight
Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by
dynamically adjusting the model parameters according to the identity
information. In particular, we design an efficient Identity Injection Module
(IIM) by introducing two dynamic neural network techniques, including the
weights prediction and weights modulation. Once the IDN is updated, it can be
applied to swap faces given any target image or video. The presented IDN
contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it
capable for real-time video face swapping on mobile phones. In addition, we
introduce a knowledge distillation-based method for stable training, and a loss
reweighting module is employed to obtain better synthesized results. Finally,
our method achieves comparable results with the teacher models and other
state-of-the-art methods.",smart cities
10.1016/j.rse.2021.112809,preprocessed,Remote Sensing of Environment,scopus,2022-02-01,sciencedirect,methanet – an ai-driven approach to quantifying methane point-source emission from high-resolution 2-d plume imagery,https://api.elsevier.com/content/abstract/scopus_id/85120521703,"Methane is one of the most important anthropogenic greenhouse gases with a significant impact on the Earth's radiation budget and tropospheric background ozone. Despite a well-constrained global budget, quantification of local and regional methane emissions has proven challenging. Recent advancements in airborne remote sensing instruments such as from the next-generation Airborne Visible/Infrared Imaging Spectrometer (AVIRIS-NG) provide 2-D observations of CH4 plume column enhancements at an unprecedented resolution of 1–5 m over large geographic areas. Quantifying an emission rate from observed plumes is a critical step for understanding local emission distributions and prioritizing mitigation efforts. However, there exists no method that can predict emission rates from detected plumes in real-time without ancillary data reliably. In order to predict methane point-source emissions directly from high resolution 2-D plume images without relying on other local measurements such as background wind speeds, we trained a convolutional neural network model called MethaNet. The training data was derived from large eddy simulations of methane plumes and realistic measurement noise over agricultural, desert and urban environments. Our model has a mean absolute percentage error for predicting unseen plumes under 17%, a significant improvement from previous methods that require wind information. Using MethaNet, a validation against a natural gas controlled-release experiment agrees to within the precision error estimate. Our results support the basis for the applicability of using deep learning techniques to quantify CH4 point sources in an automated manner over large geographical areas, not only for present and future airborne field campaigns but also for upcoming space-based observations in this decade.",smart cities
10.1016/j.cageo.2021.105010,preprocessed,Computers and Geosciences,scopus,2022-02-01,sciencedirect,geospatialvr: a web-based virtual reality framework for collaborative environmental simulations,https://api.elsevier.com/content/abstract/scopus_id/85120435814,"This research introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be served on any web platform and accessed via desktop and mobile devices and virtual reality headsets. The framework can generate realistic simulations of desired locations entailing the terrain, elevation model, infrastructures, dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, occupancy, traffic, weather). These layers enable in-situ visualization of useful data to aid public, scientists, officials, and decision-makers in acquiring a bird's eye view of the current, historical, or forecasted condition of a community. The framework incorporates multiuser support to allow different stakeholders to remotely work on the same VR environment and observe other users' actions and 3D positions via avatars in real-time, and thus, presenting the potential to be utilized as a virtual incident command center or a meeting room. GeospatialVR's purpose is to enhance existing web-based cyberinfrastructure systems with the integration of immersive geospatial capabilities to assist the development of next-generation information and decision support systems powered by virtual reality. Finally, several case studies have been developed for flooding, wildfire, transportation, and public safety.",smart cities
10.1016/j.apenergy.2021.118136,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,an online energy management system for ac/dc residential microgrids supported by non-intrusive load monitoring,https://api.elsevier.com/content/abstract/scopus_id/85119066285,"Traditional electric energy systems are experiencing a major revolution and the main drivers of this revolution are green transition and digitalization. In this paper, an advanced system-level EMS is proposed for residential AC/DC microgrids (MGs) by taking advantage of the innovations offered by digitalization. The proposed EMS supports green transition as it is designed for an MG that includes renewable energy sources (RESs), batteries, and electric vehicles. In addition, the electricity consumption behaviors of residential users have been automatically extracted to create a more flexible MG. Deep learning-supported Non-intrusive load monitoring (NILM) algorithm is deployed to analyze and disaggregate the aggregated consumption signal of each household in the MG. A two-level EMS is designed that coordinates both households and MG components using optimization, forecasting, and NILM modules. The proposed system-level EMS has been tested in a laboratory environment in real-time. Experiments are performed considering different optimization periods and the effectiveness of the proposed EMS has been shown for different optimization horizons. Compared to a peak shaving strategy as a benchmark, the proposed EMS for 24-hour horizon provides a 12.36% reduction in the residential MG daily operation cost.",smart cities
10.1016/j.snb.2021.130958,preprocessed,Sensors and Actuators B: Chemical,scopus,2022-01-15,sciencedirect,from air quality sensors to sensor networks: things we need to learn,https://api.elsevier.com/content/abstract/scopus_id/85118500250,"As a potential complement to traditional regulatory instruments, low-cost air quality sensors (LCAQS) can be deployed in dense monitoring networks to provide timely and comprehensive snapshots of pollutant concentrations and their spatial and temporal variability at various scales with relatively less cost and labor. However, a lack of practical guidance and a limited understanding of sensor data quality hinder the widespread application of this emerging technology. We leveraged air quality data collected from state and local monitoring agencies in metropolitan areas of the United States to evaluate how low-cost sensors could be deployed across the U.S. We found that ozone, as a secondary pollutant, is more homogeneous than other pollutants at various scales. PM2.5, CO, and NO2 displayed homogeneities that varied by city, making it challenging to design a uniform network that was suitable across geographies. Our low-cost sensor data in New York City indicated that PM2.5 sensors track well with light-scattering reference methods, particularly at low concentrations. The same phenomenon was also found after thoroughly evaluating sensor evaluation reports from the Air Quality Sensor Performance Evaluation Center (AQ-SPEC). Furthermore, LCAQS data collected during wildfire episodes in Portland, OR show that a real-time (i.e. in situ) machine learning calibration process is a promising approach to address the data quality challenges persisting in LCAQS applications. Our research highlights the urgency and importance of practical guidance for deploying LCAQS.",smart cities
10.1016/j.jsr.2021.12.010,preprocessed,Journal of Safety Research,scopus,2022-01-01,sciencedirect,learning to interpret novel ehmi: the effect of vehicle kinematics and ehmi familiarity on pedestrian’ crossing behavior,https://api.elsevier.com/content/abstract/scopus_id/85123375400,"Introduction: In current urban traffic, pedestrians attempting to cross the road at un-signalized locations are thought to mostly use implicit communication, such as deceleration cues, to interpret a vehicle’s intention to yield. There is less reliance on explicit driver- or vehicle-based messages, such as hand/head movements, or flashing lights/beeping horns. With the impending deployment of Automated Vehicles (AV), especially those at SAE Level 4 and 5, where the driver is no longer in control of the vehicle, there has been a surge in interest in the value of new forms of communication for AVs, for example, via different types of external Human Machine Interfaces (eHMIs). However, there is still much to be understood about how quickly a novel eHMI affects pedestrian crossing decisions, and whether it provides any additional aid, above and beyond implicit/kinematic information from the vehicle. The aim of this between-participant study, funded by the H2020 interACT project, was to investigate how the combination of kinematic information from a vehicle (e.g., Speed and Deceleration), and eHMI designs, play a role in assisting the crossing decision of pedestrians in a cave-based pedestrian simulator. Method: Using an existing, well-recognized, message for yielding (Flashing Headlights - FH) as a benchmark, this study also investigated how quickly a novel eHMI (Slow Pulsing Light Band – SPLB) was learned. To investigate the effect of eHMI visibility on crossing decisions, the distance at which each eHMI was perceivable was also measured. Results: Results showed that, compared to SPLB, the FH led to earlier crossings during vehicle deceleration, especially at lower approaching speeds, and smaller time gaps. However, although FH was visible earlier than SPLB, this visibility does not appear to be the only reason for earlier crossings, with message familiarity thought to play a role. Participants were found to learn the meaning conveyed by FH relatively quickly, crossing around 1 second earlier in its presence (compared to the no eHMI condition), across the three blocks of trials. On the other hand, it took participants at least one block of 12 trials for the new SPLB signal to affect crossing, which only accelerated crossing initiations by around 200 ms, compared to the no eHMI condition. The role of comprehension, long-term exposure, and familiarity of novel messages in this context is therefore important, if AVs are to provide safe, trustworthy communication messages, which will enhance traffic flow and efficiency.",smart cities
10.1016/j.isprsjprs.2021.10.015,preprocessed,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2022-01-01,sciencedirect,changemask: deep multi-task encoder-transformer-decoder architecture for semantic change detection,https://api.elsevier.com/content/abstract/scopus_id/85119995073,"Multi-temporal high spatial resolution earth observation makes it possible to detect complex urban land surface changes, which is a significant and challenging task in remote sensing communities. Previous works mainly focus on binary change detection (BCD) based on modern technologies, e.g., deep fully convolutional network (FCN), whereas the deep network architecture for semantic change detection (SCD) is insufficiently explored in current literature. In this paper, we propose a deep multi-task encoder-transformer-decoder architecture (ChangeMask) designed by exploring two important inductive biases: sematic-change causal relationship and temporal symmetry. ChangeMask decouples the SCD into a temporal-wise semantic segmentation and a BCD, and then integrates these two tasks into a general encoder-transformer-decoder framework. In the encoder part, we design a semantic-aware encoder to model the semantic-change causal relationship. This encoder is only used to learn semantic representation and then learn change representation from semantic representation via a later transformer module. In this way, change representation can constrain semantic representation during training, which introduces a regularization to reduce the risk of overfitting. To learn a robust change representation from semantic representation, we propose a temporal-symmetric transformer (TST) to guarantee temporal symmetry for change representation and keep it discriminative. Based on the above semantic representation and change representation, we adopt simple multi-task decoders to output semantic change map. Benefiting from the differentiable building blocks, ChangeMask can be trained by a multi-task loss function, which significantly simplifies the whole pipeline of applying ChangeMask. The comprehensive experimental results on two large-scale SCD datasets confirm the effectiveness and superiority of ChangeMask in SCD. Besides, to demonstrate the potential value in real-world applications, e.g., automatic urban analysis and decision-making, we deploy the ChangeMask to map a large geographic area covering 30 km2 with 300 million pixels. Code will be made available.",smart cities
10.1016/j.scs.2021.103364,preprocessed,Sustainable Cities and Society,scopus,2022-01-01,sciencedirect,blockchain-enabled secure framework for energy-efficient smart parking in sustainable city environment,https://api.elsevier.com/content/abstract/scopus_id/85117732239,"In the smart city environment, parking vehicle management is an essential requirement for citizens in the current situation because every city is rapidly growing as a crowded environment. Specific planning, operation, and thinking can address this problem with the Internet of things (IoT) and Information Communication Technologies (ICT). Existing research provides various solutions and methods for parking systems in the smart city. However, smart parking has many challenges, such as centralization, communication bandwidth, energy efficiency, integrity, security, and privacy. Inspired by Blockchain and AI technology, we propose a Blockchain-enabled Secure Framework for Energy-Efficient Smart Parking in Sustainable City Environment. The transport layer implements the ECC algorithm to encrypt and decrypt the parking zones data for secure communication. The RSU-based-Blockchain network offers authentication and verification of data at the security layer in a distributed manner. Virtualization technology is used for data storage and provides an energy-efficient environment using virtual machines. With Deep LSTM networks, we analyze the parking zone's data and offer the best parking space to the drivers with the best location and timing. We evaluate the proposed architecture using quantitative, qualitative analysis and provide the driver's best parking space.",smart cities
10.1016/j.apenergy.2021.117853,preprocessed,Applied Energy,scopus,2022-01-01,sciencedirect,transferable representation modelling for real-time energy management of the plug-in hybrid vehicle based on k-fold fuzzy learning and gaussian process regression,https://api.elsevier.com/content/abstract/scopus_id/85114985028,"Electric vehicles, including plug-in hybrids, are important for achieving net-zero emission and will dominate road transportation in the future. Energy management, which optimizes the onboard energy usage, is a critical functionality of electric vehicles. It is usually developed following the model-based routine, which is conventionally costly and time-consuming and is hard to meet the increasing market competition in the digital era. To reduce the development workload for the energy management controller, this paper studies an innovative transfer learning routine. A new transferable representation control model is proposed by incorporating two promising artificial intelligence technologies, adaptive neural fuzzy inference system and Gaussian process regression, where the former applies k-fold cross valudation to build a neural fuzzy system for real-time implementation of offline optimization result, and the later connects the neural fuzzy system with a ‘deeper’ architecture to transfer the offline optimization knowledge learnt at source domain to new target domains. By introducing a concept of control utility that evaluates vehicle energy efficiency with a penalty on usage of battery energy, experimental evaluations based on the hardware-in-the-loop testing platform are conducted. Competitive real-time control ultility values (as much as 90% of offline benchmarking results) can be achieved by the proposed control method. They are over 27% higher than that achieved by the neural-network-based model.",smart cities
10.1109/jsac.2021.3118405,preprocessed,IEEE Journal on Selected Areas in Communications,IEEE,2022-02-01 00:00:00,ieeexplore,"learning-based prediction, rendering and transmission for interactive virtual reality in ris-assisted terahertz networks",https://ieeexplore.ieee.org/document/9565222/,"The quality of experience (QoE) requirements of wireless virtual reality (VR) can only be satisfied with high data rate, high reliability, and low VR interaction latency. This high data rate over short transmission distances may be achieved via the abundant bandwidth in the terahertz (THz) band. However, THz waves experience severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with programmable reflecting elements. Meanwhile, the low VR interaction latency can be achieved with the mobile edge computing (MEC) network architecture due to its computation capabilities. Motivated by these considerations, in this paper, we propose an MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by taking into account the uplink viewpoint prediction and position transmission, the MEC rendering, and the downlink transmission. We propose two methods, which are referred to as centralized online gated recurrent unit (GRU) and distributed federated averaging (FedAvg), to predict the viewpoints of the VR users. In the uplink, an algorithm that integrates online long-short term memory (LSTM) and convolutional neural networks (CNN) is deployed to predict the locations and the line-of-sight and non-line-of-sight statuses of the VR users over time. In the downlink, we develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS under latency constraints. Simulation results show that our proposed learning architecture achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.",multimedia
10.1109/tnsre.2022.3147772,preprocessed,IEEE Transactions on Neural Systems and Rehabilitation Engineering,IEEE,2000-01-01 00:00:00,ieeexplore,improving automatic control of upper-limb prosthesis wrists using gaze-centered eye tracking and deep learning,https://ieeexplore.ieee.org/document/9698069/,"Many upper-limb prostheses lack proper wrist rotation functionality, leading to users performing poor compensatory strategies, leading to overuse or abandonment. In this study, we investigate the validity of creating and implementing a data-driven predictive control strategy in object grasping tasks performed in virtual reality. We propose the idea of using gaze-centered vision to predict the wrist rotations of a user and implement a user study to investigate the impact of using this predictive control. We demonstrate that using this vision-based predictive system leads to a decrease in compensatory movement in the shoulder, as well as task completion time. We discuss the cases in which the virtual prosthesis with the predictive model implemented did and did not make a physical improvement in various arm movements. We also discuss the cognitive value in implementing such predictive control strategies into prosthetic controllers. We find that gaze-centered vision provides information about the intent of the user when performing object reaching and that the performance of prosthetic hands improves greatly when wrist prediction is implemented. Lastly, we address the limitations of this study in the context of both the study itself as well as any future physical implementations.",multimedia
10.1109/tmrb.2021.3129113,preprocessed,IEEE Transactions on Medical Robotics and Bionics,IEEE,2022-02-01 00:00:00,ieeexplore,learning a generic olfactory search strategy from silk moths by deep inverse reinforcement learning,https://ieeexplore.ieee.org/document/9619462/,"Despite their simple nervous systems, insects efficiently search for and find sources of odorants. Hence, it is necessary to model and implement such behavior in artificial agents (robots), to enable them to detect dangerous substances such as drugs, gas leaks, and explosives. Previous studies have approached behavioral modeling with either statistical or machine-learning methods. In this study, we determined the behavior trajectories of male silk moths using a virtual reality (VR) system. We then modeled these trajectories as a Markov decision process (MDP) and employed inverse reinforcement learning (IRL) to learn their reward function. Furthermore, we estimated the optimal policy from the learned reward function. We then conducted olfactory search simulations and determined that the IRL-based policy could locate odor sources with a high success rate. This was also investigated under environmental conditions different from those faced by real moths on the VR system. The obtained results indicate that IRL can generically represent olfactory search strategies that are adaptable to various environments.",multimedia
10.1109/wacvw54805.2022.00069,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),IEEE,2022-01-08 00:00:00,ieeexplore,aa3dnet: attention augmented real time 3d object detection,https://ieeexplore.ieee.org/document/9707544/,"In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Bird’s Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at &gt;30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",multimedia
10.1109/wacv51458.2022.00380,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,fast-clocs: fast camera-lidar object candidates fusion for 3d object detection,https://ieeexplore.ieee.org/document/9706631/,"When compared to single modality approaches, fusion-based object detection methods often require more complex models to integrate heterogeneous sensor data, and use more GPU memory and computational resources. This is particularly true for camera-LiDAR based multimodal fusion, which may require three separate deep-learning networks and/or processing pipelines that are designated for the visual data, LiDAR data, and for some form of a fusion framework. In this paper, we propose Fast Camera-LiDAR Object Candidates (Fast-CLOCs) fusion network that can run high-accuracy fusion-based 3D object detection in near real-time. Fast-CLOCs operates on the output candidates before Non-Maximum Suppression (NMS) of any 3D detector, and adds a lightweight 3D detector-cued 2D image detector (3D-Q-2D) to extract visual features from the image domain to improve 3D detections significantly. The 3D detection candidates are shared with the proposed 3D-Q-2D image detector as proposals to reduce the network complexity drastically. The superior experimental results of our Fast-CLOCs on the challenging KITTI and nuScenes datasets illustrate that our Fast-CLOCs outperforms state-of-the-art fusion-based 3D object detection approaches. We will release the code upon publication.",multimedia
10.1109/wacv51458.2022.00037,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,occlusion resistant network for 3d face reconstruction,https://ieeexplore.ieee.org/document/9706716/,"3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of 0.77 against 5.84 of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of 6.67 in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.",multimedia
10.1109/lra.2022.3142439,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,anytime 3d object reconstruction using multi-modal variational autoencoder,https://ieeexplore.ieee.org/document/9681277/,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository<sup>1</sup>.",multimedia
10.1109/access.2022.3140332,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,spatial-importance-based computation scheme for real-time object detection from 3d sensor data,https://ieeexplore.ieee.org/document/9668921/,"Three-dimensional (3D) sensor networks using multiple light-detection-and-ranging (LIDAR) sensors are good for smart monitoring of spots, such as intersections, with high potential risk of road-traffic accidents. The image sensors must share the strictly limited computation capacity of an edge computer. To have the computation speeds required from real-time applications, the system must have a short computation delay while maintaining the quality of the output, e.g., the accuracy of the object detection. This paper proposes a spatial-importance-based computation scheme that can be implemented on an edge computer of image-sensor networks composed of 3D sensors. The scheme considers regions where objects exist as more likely to be ones of higher spatial importance. It processes point-cloud data from each region according to the spatial importance of that region. By prioritizing regions with high spatial importance, it shortens the computation delay involved in the object detection. A point-cloud dataset obtained by a moving car equipped with a LIDAR unit was used to numerically evaluate the proposed scheme. The results indicate that the scheme shortens the delay in object detection.",multimedia
10.1109/lsp.2022.3144074,preprocessed,IEEE Signal Processing Letters,IEEE,2000-01-01 00:00:00,ieeexplore,rethinking lightweight: multiple angle strategy for efficient video action recognition,https://ieeexplore.ieee.org/document/9684992/,"Video action recognition task involves modeling spatiotemporal information, and efficiency is critical to capture spatiotemporal dependencies in the video. Most existing models rely on optical flow information to capture the dynamic visual tempos between consecutive video frames. Although impressive performance can be achieved by combining optical flow with RGB, the time-consuming nature of optical flow computation cannot be ignored. Moreover, 3D CNN has successfully modeled spatiotemporal information, yet the enormous computational volume is unsuitable for real-time action recognition. In this letter, we propose a novel lightweight video feature extraction strategy that achieves better recognition performance with lower FLOPs. In particular, we perform convolution on the video cube from three orthogonal angles to learn its appearance and motion features. Compared with the computational volume of 3D CNN, our proposed method is more economical and thus meets the lightweight requirements. Extensive experimental results on public Something Something-V1<inline-formula><tex-math notation=""LaTeX"">$\&amp;$</tex-math></inline-formula>V2 and Diving48 datasets show our approach achieves the state-of-the-art performance.",multimedia
10.1109/lra.2022.3147337,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,sim2air - synthetic aerial dataset for uav monitoring,https://ieeexplore.ieee.org/document/9699390/,"In this letter, we propose a novel approach to generate a synthetic aerial dataset for application in UAV monitoring. We propose to accentuate shape-based object representation by applying texture randomization. A diverse dataset with photorealism in all parameters such as shape, pose, lighting, scale, viewpoint, etc. except for atypical textures is created in a 3D modelling software Blender. Our approach specifically targets two conditions in aerial images where texture of objects is difficult to detect, namely challenging illumination and objects occupying only a small portion of the image. Experimental evaluation of YOLO and Faster R-CNN detectors trained on synthetic data with randomized textures confirmed our approach by increasing the mAP value (17 and 3.7 percentage points for YOLO; 20 and 1.1 percentage points for Faster R-CNN) on two test datasets of real images, both containing UAV-to-UAV images with motion blur. Testing on different domains, we conclude that the more the generalisation ability is put to the test, the more apparent are the advantages of the shape-based representation.",multimedia
10.1109/lra.2021.3116700,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments,https://ieeexplore.ieee.org/document/9555228/,"Obstacle avoidance for robotic manipulators can be challenging when they operate in unstructured environments. This problem is probed with the sim-to-real (sim2real) deep reinforcement learning, such that a moving policy of the robotic arm is learnt in a simulator and then adapted to the real world. However, the problem of sim2real adaptation is notoriously difficult. To this end, this work proposes (1) a unified representation of obstacles and targets to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model combining the unified representation with the deep reinforcement learning control module that can be trained by interacting with the environment. Such a representation is agnostic to the shape and appearance of the underlying objects, which simplifies and unifies the scene representation in both simulated and real worlds. We implement this idea with a vision-based actor-critic framework by devising a bounding box predictor module. The predictor estimates the 3D bounding boxes of obstacles and targets from the RGB-D input. The features extracted by the predictor are fed into the policy network, and all the modules are jointly trained. This makes the policy learn object-aware scene representation, which leads to a data-efficient learning of the obstacle avoidance policy. Our experiments in simulated environment and the real-world show that the end-to-end model of the unified representation achieves better sim2real adaption and scene generalization than state-of-the-art techniques.",multimedia
10.1109/jiot.2021.3089080,preprocessed,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,audio-visual autoencoding for privacy-preserving video streaming,https://ieeexplore.ieee.org/document/9453730/,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",multimedia
10.1109/wacv51458.2022.00135,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,detecting tear gas canisters with limited training data,https://ieeexplore.ieee.org/document/9706699/,"Human rights investigations often entail triaging large volumes of open source images and video in order to find moments that are relevant to a given investigation and warrant further inspection. Searching for instances of tear gas usage online manually is laborious and time-consuming. In this paper, we study various object detection models for their potential use in the discovery and identification of tear gas canisters for human rights monitors. CNN based object detection typically requires large volumes of training data, and prior to our work, an appropriate dataset of tear gas canisters did not exist. We benchmark methods for training object detectors using limited labelled data: we fine-tune different object detection models on the limited labelled data and compare performance to a few shot detector and augmentation strategies using synthetic data. We provide a dataset for evaluating and training tear gas canister detectors and indicate how such detectors can be deployed in real-world contexts for investigating human rights violations. Our experiments show that various techniques can improve results, including fine-tuning state of the art detectors, using few shot detectors, and including synthetic data as part of the training set.",multimedia
10.1109/comsnets53615.2022.9668364,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,open-air off-street vehicle parking management system using deep neural networks: a case study,https://ieeexplore.ieee.org/document/9668364/,"Smart parking solution aims to output real-time parking occupancy information. It helps to reduce parking bay search time, traffic, fuel consumption, and thereby vehicular emissions with increased road safety. A computer vision-based solution using camera video data is most reliable and rational since it allows monitoring the entire open-air parking area at once. A real-time parking solution (cloud-based, server processing, or onboard processing) helps bring the occupancy information to the end-user. It comes with many challenges such as viewing angles, lighting conditions, model optimization, reducing inference time, and many more real-world challenges. Hence, this paper presents a case study on real-time open-air off-street intelligent parking management using a deep neural network. Also, most of the earlier research works focus on day-time data and do not discuss the night data. So, in this work, we perform experiments on realtime 24-hour data from an input camera video source mounted to monitor parking at IIT Hyderabad (IITH) parking lot. Our experiments demonstrate the real-world challenges and can help improve parking performance, deployment at IITH, and relevant parking systems in general.",multimedia
10.1109/comsnets53615.2022.9668498,preprocessed,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,tele-driving an electric vehicle over a private lte network,https://ieeexplore.ieee.org/document/9668498/,"We demonstrate tele-driving operation for an electric vehicle capable of stopping itself in case of system failure over a captive LTE network deployed in a university campus. Our electronically controlled vehicle is driven remotely by an operator from a control room which receives the multi-camera real-time video feed from the vehicle over this network. Our primary contribution includes the responsive emergency braking mechanism for the vehicle, modular vehicle design based on CAN bus, low latency LTE MAC scheduler design, and modifications to popular video tool, FFMPEG to support low latency real time video streaming. Our demonstration shows complete integration of the different components, i.e., the vehicle, the LTE network and the remote driving application. Another salient feature of our system is the O-RAN compliant RAN awareness module and KPI (Key Performance Indicator) application which enables real-time network performance monitoring.",multimedia
10.1109/access.2022.3147519,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a deep learning-based approach for inappropriate content detection and classification of youtube videos,https://ieeexplore.ieee.org/document/9696242/,"The exponential growth of videos on YouTube has attracted billions of viewers among which the majority belongs to a young demographic. Malicious uploaders also find this platform as an opportunity to spread upsetting visual content, such as using animated cartoon videos to share inappropriate content with children. Therefore, an automatic real-time video content filtering mechanism is highly suggested to be integrated into social media platforms. In this study, a novel deep learning-based architecture is proposed for the detection and classification of inappropriate content in videos. For this, the proposed framework employs an ImageNet pre-trained convolutional neural network (CNN) model known as EfficientNet-B7 to extract video descriptors, which are then fed to bidirectional long short-term memory (BiLSTM) network to learn effective video representations and perform multiclass video classification. An attention mechanism is also integrated after BiLSTM to apply attention probability distribution in the network. These models are evaluated on a manually annotated dataset of 111,156 cartoon clips collected from YouTube videos. Experimental results demonstrated that EfficientNet-BiLSTM (accuracy = 95.66%) performs better than attention mechanism-based EfficientNet-BiLSTM (accuracy = 95.30%) framework. Secondly, the traditional machine learning classifiers perform relatively poor than deep learning classifiers. Overall, the architecture of EfficientNet and BiLSTM with 128 hidden units yielded state-of-the-art performance (f1 score = 0.9267). Furthermore, the performance comparison against existing state-of-the-art approaches verified that BiLSTM on top of CNN captures better contextual information of video descriptors in network architecture, and hence achieved better results in child inappropriate video content detection and classification.",multimedia
10.1109/tmm.2021.3050086,preprocessed,IEEE Transactions on Multimedia,IEEE,2000-01-01 00:00:00,ieeexplore,improving robustness of dash against unpredictable network variations,https://ieeexplore.ieee.org/document/9317786/,"Most video players use adaptive bitrate (ABR) algorithms to provide good quality-of-experience (QoE) in dynamic network conditions. To deal with the adaptation challenges, many ABR algorithms select bitrate by optimizing a defined QoE function. Within the framework, various algorithms mainly differ in how the optimization problem is solved, including prediction-based approaches and learn-based approaches. However, these algorithms suffer from limited performance in the current popular mobile streaming which has limited resources and rapidly changing link rates. Existing machine-learning approaches face deployment difficulties on mobile devices, and prediction-based approaches that rely on throughput prediction experience large buffer occupancy variations in cellular networks, resulting in rebuffering frequently. To provide a robust and lightweight ABR algorithm for mobile streaming, this work improves the robustness of prediction-based scheme against unpredictable network variations and develops RBC (Robust Bitrate Controller) algorithm. Rather than optimizing QoE over the entire buffer capacity, RBC creates buffer margins to absorb the impact of throughput jitters and solves QoE maximization on the narrowed buffer range. The amount of buffer margin is dynamically adjusted based on the real-time throughput fluctuation to ensure sufficient de-jitter space. For online lightweight deployment, RBC provides a closed-form solution of the desired bitrate with small computation complexity by using adaptive control approach. Trace-driven experiments and real-world tests show that RBC effectively reduces the playback freezing and gains an improvement in overall QoE.",multimedia
10.1109/tcsvt.2021.3066675,preprocessed,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,2022-02-01 00:00:00,ieeexplore,spatio-temporal online matrix factorization for multi-scale moving objects detection,https://ieeexplore.ieee.org/document/9380454/,"Detecting moving objects from the video sequences has been treated as a challenging computer vision task, since the problems of dynamic background, multi-scale moving objects and various noise interference impact the corresponding feasibility and efficiency. In this paper, a novel spatio-temporal online matrix factorization (STOMF) method is proposed to detect multi-scale moving objects under dynamic background. To accommodate a wide range of the real noise distractions, we apply a specific mixture of exponential power (MoEP) distributions to the framework of low-rank matrix factorization (LRMF). For the optimization of solution algorithm, a temporal difference motion prior (TDMP) model is proposed, which estimates the motion matrix and calculates the weight matrix. Moreover, a partial spatial motion information (PSMI) post-processing method is further designed to implement multi-scale objects extraction in varieties of complex dynamic scenes, which utilizes partial background and motion information. The superiority of the STOMF method is validated by massive experiments on practical datasets, as compared with state-of-the-art moving objects detection approaches.",multimedia
10.1109/iccece54139.2022.9712814,preprocessed,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),IEEE,2022-01-16 00:00:00,ieeexplore,a lightweight sar image recognition algorithm based on deep convolutional neural network,https://ieeexplore.ieee.org/document/9712814/,"Synthetic Aperture Radar (SAR) can provide large-scale, all-time, all-weather imaging and therefore plays an important role in both military reconnaissance and battlefield perception. In recent years, due to the outstanding performance of deep convolutional neural networks (CNNs) on image recognition, the approaches that apply CNNs to SAR target recognition has attracted widespread attention among domestic and foreign scholars. The CNNs can achieve high accuracy, yet they often contain huge number of parameters and occupy too much memory to be deployed on devices with memory constraint. In this work, we build light-weight SAR image recognition models by performing iterative pruning and retraining on three representative architectures. The algorithm can lead to a 50% reduction in the number of parameters, while still obtaining a high accuracy of 98.5%. Our algorithm and results provide a potential direction for building light-weight SAR image model that can be deployed for real-world applications.",multimedia
10.1109/tim.2021.3132332,preprocessed,IEEE Transactions on Instrumentation and Measurement,IEEE,2000-01-01 00:00:00,ieeexplore,finger vein recognition algorithm based on lightweight deep convolutional neural network,https://ieeexplore.ieee.org/document/9633979/,"Even though the deep neural networks have strong feature representation capability and high recognition accuracy in finger vein recognition, the deep models are computationally intensive and poor in timeliness. To address these issues, this article proposes a lightweight algorithm for finger vein image recognition and matching. The proposed algorithm uses a lightweight convolutional model in the backbone network and employs a triplet loss function to train the model, which not only improves the matching accuracy, but also satisfies the real-time matching requirements. In addition, the Mini-region of interest (RoI) and finger vein pattern feature extraction also effectively solve the problems of large amounts of calculation and background noise. Moreover, the present model recognizes new categories based on the feature vector space constructed by the finger vein recognition system, so that new categories can be recognized without retraining the model. The results show that the finger vein recognition and matching algorithm proposed in this article achieves 99.3% and 99.6% in recognition accuracy and 14.2 and 16.5 ms in matching time for the dataset Shandong University Machine Learning and Applications Laboratory-Homologous Multimodal Biometric Traits (SDUMLA-HMT) and Peking University Finger Vein Dataset (PKU-FVD), respectively. These metrics show that our approach is time-saving and more effective than previous algorithms. Compared with the state-of-the-art finger vein recognition algorithm, the proposed algorithm improves 1.45% in recognition accuracy while saving 45.7% in recognition time.",multimedia
10.1109/taslp.2021.3129994,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,soundstream: an end-to-end neural audio codec,https://ieeexplore.ieee.org/document/9625818/,"We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.",multimedia
10.1109/access.2021.3139537,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"automatic adaptation of open educational resources: an approach from a multilevel methodology based on students’ preferences, educational special needs, artificial intelligence and accessibility metadata",https://ieeexplore.ieee.org/document/9669174/,"The need for adaptive e-learning environments that respond to learning variability is now a fundamental requirement in education, as it helps to ensure that students learn and pass their courses within a set time frame. Although guidelines, techniques and methods have been established in recent years to contribute to the development of accessible and adaptable e-learning environments that promote digital inclusion, their implementation is challenging due to the lack of knowledge of an adequate way to do it and because it is considered more of a technological competence for scholars in the area. In this context, automated support for adapting material that responds to the correct use of accessibility metadata not only provides a way to improve the description of adapted educational resources, but also facilitates their search according to the needs and preferences of students, particularly those with disabilities. In this article, we carry out a multilevel methodological proposal for the automatic adaptation of open educational resources, in order to provide a tool that contributes to the accessibility and correct use of their metadata in e-learning environments. A research is conducted with students with disabilities to establish their real needs and preferences, highlighting the need to strengthen the adequate description and coherent alternative text in images, the correct subtitling in videos and the conversion of audio to text, data that are relevant to our proposal. The research conducted aims to contribute with an automated support tool in the generation of accessible educational resources that are correctly labeled for search and reuse. This research also aims to support researchers in artificial intelligence applications to address challenges and opportunities in the field of virtual education, in addition to providing an overview that could help those who generate educational resources and maintain their interest in making them accessible.",multimedia
10.1109/access.2022.3140901,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a two-stage deep neuroevolutionary technique for self-adaptive speech enhancement,https://ieeexplore.ieee.org/document/9672141/,"This paper presents a novel self-adaptive approach for speech enhancement in the context of highly nonstationary noise. A two-stage deep neuroevolutionary technique for speech enhancement is proposed. The first stage is composed of a deep neural network (DNN) method for speech enhancement. Two DNN methods were tested at this stage, namely, both a deep complex convolution recurrent network (DCCRN) and a residual long short-term memory neural network (ResLSTM). The ResLSTM method was combined with a minimum mean-square error method to perform a preliminary enhancement. The ResLSTM network is used as an <italic>a priori</italic> signal-to-noise ratio (SNR) estimator. The second stage implements a self-adaptive multiband spectral subtraction enhancement method using tuning optimization based on a genetic algorithm. The proposed two-stage technique is evaluated using objective measures of speech quality and intelligibility. The experiments are carried out using the NOIZEUS noisy speech corpus using conditions of real-world stationary, colored, and nonstationary noise sources at multiple SNR levels. These experiments demonstrate the advantage of building a cooperative approach using evolutionary and deep learning-based techniques that are capable of achieving robust speech enhancement in adverse conditions. Indeed, the experimental tests show that the proposed two-stage technique outperformed a baseline implementation using a state-of-the-art deep learning approach by an average 13% and 6% improvement for six noise conditions at a −5 dB and a 0 dB input SNR, respectively.",multimedia
10.1109/taslp.2021.3126947,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,end-to-end neural based modification of noisy speech for speech-in-noise intelligibility improvement,https://ieeexplore.ieee.org/document/9611022/,"Intelligibility of speech can be significantly reduced when it is presented in adverse near-end listening conditions, like background noise. Multiple approaches have been suggested to improve the perception of speech in such conditions. However, most of these approaches were designed to work with clean input speech. Therefore, they have serious limitations when deployed in real world applications like telephony and hearing aids, where noisy input speech is quite common. In this paper we present an end-to-end neural network approach for the above problem, which effectively reduces the input noise and improves the intelligibility for listeners in adverse conditions. To that end, a convolutional neural network topology with variable dilation factors is proposed and evaluated both in a causal and a non-causal configuration using raw speech as input. A Teacher-Student training strategy is employed, where the Teacher is a well-established speech-in-noise intelligibility enhancer based on spectral shaping followed by dynamic range compression (SSDRC). The evaluation is performed both objectively using the speech intelligibility in bits metric (SIIB), and subjectively on the Greek Harvard corpus. A noise robust multi-band version of SSDRC was used as a baseline. Compared with the baseline, at 0 dB input SNR, the suggested neural network system achieved about 380% and 230% relative SIIB improvements in fluctuating and stationary backgrounds, respectively. Subjectively, the suggested model increased listeners’ keyword correct rate in stationary noise from 25% to 60% at 0 dB input SNR, and from about 52% to 75% at 5 dB input SNR, compared with the baseline.",multimedia
10.1109/taslp.2021.3133190,preprocessed,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,vace-wpe: virtual acoustic channel expansion based on neural networks for weighted prediction error-based speech dereverberation,https://ieeexplore.ieee.org/document/9640471/,"Speech dereverberation is an important issue for many real-world speech processing applications. Among the techniques developed, the weighted prediction error (WPE) algorithm has been widely adopted and advanced over the last decade, which blindly cancels out the late reverberation component from the reverberant mixture of microphone signals. In this study, we extend the neural-network-based virtual acoustic channel expansion (VACE) framework for the WPE-based speech dereverberation, a variant of the WPE that we recently proposed to enable the use of dual-channel WPE algorithm in a single-microphone speech dereverberation scenario. Based on the previous study, some ablation studies are conducted regarding the constituents of the VACE-WPE in an offline processing scenario. These studies reveal the characteristics of the system, thereby simplifying the architecture and leading to the introduction of new strategies for training the neural network for the VACE. Experimental results demonstrate that VACE-WPE (our PyTorch implementation and pre-trained models are available from <uri>https://github.com/dreadbird06/vace_wpe</uri>) considerably outperforms its single-channel counterpart in simulated noisy reverberant environments in terms of objective speech quality and is superior to the single-channel WPE as well as several fully neural speech dereverberation methods when employed as the front-end for the far-field automatic speech recognizer.",multimedia
10.1007/978-3-030-85365-5_17,preprocessed,"Advances in Deep Learning, Artificial Intelligence and Robotics",Springer,2022-01-01 00:00:00,springer,robust model for rural education using deep learning and robotics,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85365-5_17,"Rural Education is important for the overall development of villages. National Achievement Survey (NAS) has surveyed and reported in many States of India consistent decline in the learning levels of students in mathematics, language and science from class III to class VIII studying in the government school system. Smart Villages is only possible if the literacy level and infrastructure improves considerably. This paper aims to perform a reality check of the situation by comparing different rural areas of various countries including India and study of related work done. The paper proposes a Robust Model for Rural Education by developing an intelligent humanoid robot using the Deep Learning approach, Human recognition, Object Recognition and Speech Recognition. The data set consists of Primary and Secondary Student data of around 10,000 Students (5 years) from 5 villages. The Proposed Model would be compared with existing models on the parameters of Learnability, Decision making, Flexibility and Cost-effectiveness. The implementation of this Model will help in decreasing the drop out rate, evaluate Students and give them a Learning platform based on their characteristics, increase adaptive and self paced learning. This Model can also be executed for Rural Adult Education and Skill building so that the Smart Village concept can become a reality.",multimedia
10.1007/s00530-021-00881-8,preprocessed,Multimedia Systems,Springer,2022-01-29 00:00:00,springer,an object detection-based few-shot learning approach for multimedia quality assessment,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00881-8,"A large portion of the global population generates various multimedia data such as texts, images, videos, etc. One of the most common categories which influences the public at large is visual multimedia content. Due to the different social media platforms (e.g., Whatsapp, Twitter, Facebook, Instagram, and YouTube), these materials are passed without censorship and national boundaries. Multimedia data containing any violent or vulgar objects could trigger public unrest, and thus, it is a serious threat to the law and order of the land. Children and teenagers use social media like never before in previous generations and create lots of multimedia data. It is important to assess the quality of multimedia content without any bias and prejudices. Although the mainstream social media platforms use different filters and moderation using human experts, it is impossible to verify the terabytes of uploaded images and videos. Thus, it is inevitable to automate the content assessment phase without incurring an increase in upload time. This study aims to prevent uploading or to tag an image/video with a reasonable percentage of a gun as content. In this paper, object detection architectures such as Faster RCNN, EfficientDet, and YOLOv5 have been used to demonstrate how these techniques can efficiently detect human faces and different types of guns in given multimedia data (images/videos). The models are tested on various test images and video clips. A comparative analysis has also been discussed based on mean average precision and frames per second metric. The YOLOv5 provides the best-performing results as high as 80.39% and 35.22% at $$\text{mAP}_{0.5}$$ mAP 0.5 and $$\text{mAP}_{[0.50:0.95]}$$ mAP [ 0.50 : 0.95 ] , respectively. A face recognition task requires thousands of samples and the usual deep learning models are data-driven. On the contrary, a few-shot learning approach has been implemented to recognize the detected faces categorizing the content as real or reel.",multimedia
10.1007/s00371-021-02347-4,preprocessed,The Visual Computer,Springer,2022-01-13 00:00:00,springer,a detailed analysis of image and video forgery detection techniques,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02347-4,"With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site’s or can be as malicious as to defame or hurt one’s reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one’s identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery.",multimedia
10.1007/978-3-030-92127-9_68,preprocessed,"11th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions and Artificial Intelligence - ICSCCW-2021",Springer,2022-01-01 00:00:00,springer,application of digital twin theory for improvement of natural gas treatment unit,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92127-9_68,"This paper describes fundamental principles of Digital Twins theory and provides exact investigation results in application of Digital Twins theory in upstream branch of oil and gas industry, namely based on example of natural gas treatment plant’s performance increase. As one of key process units of natural gas treatment which allows to implement most powerful functions of digital twin gas sweetening unit with set of membranes is considered as object of investigation. On the base of membrane technology manipulated variables are defined as inputs to digital twin model. Some theoretical results as well as real references of model’s engine calculations are reflected in the paper. Details of technical dashboards to visualize calculated results of running model based on manipulated variables are presented including monthly key performance indicators report dashboard, process flow diagram dashboard and high-level management dashboard. Paper also demonstrates data flow between digital twin model and real process unit and also inside digital twin model.",multimedia
10.1007/978-3-030-93564-1_38,preprocessed,7th International Conference on Advancements of Medicine and Health Care through Technology,Springer,2022-01-01 00:00:00,springer,programing a robotic ambulance with virtual reality,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93564-1_38,"By applying artificial intelligence and virtual reality, this study presents results and challenges for robotic ambulances. Programming vehicle dynamics and testing protocol is intended to support task of developing an autonomous ambulance and engineering efforts. Testing parameters are controlled with automated driver. Using software will diminish human input in driving. Actual values of robotic ambulance in testing are displacement, speed, and acceleration. Ambulance’s velocity in testing on a virtual track with corners and straight lines is an important kinematic parameter that influences safety of transport and some program sections. Accelerations are also important to be programmed. Objective of this paper is to highlight sequences of programming robotic ambulance using virtual reality and artificial intelligence. Results are consisting in testing scenarios, ambulance automated driving program on virtual track, refined program code, solutions for challenges.",multimedia
10.1007/978-3-030-95405-5_9,preprocessed,Advanced Data Mining and Applications,Springer,2022-01-01 00:00:00,springer,smart online exam proctoring assist for cheating detection,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95405-5_9,"Online exams are the most preferred mode of exams in online learning environment. This mode of exam has been even more prevalent and a necessity in the event of a forced closure of face-to-face teaching such as the recent Covid-19 pandemic. Naturally, conducting online exams poses much greater challenge to preserving academic integrity compared to conducting on-site face-to-face exams. As there is no human proctor for policing the examinee on site, the chances of cheating are high. Various online exam proctoring tools are being used by educational institutes worldwide, which offer different solutions to reduce the chances of cheating. The most common technique followed by these tools is recording of video and audio of the examinee during the whole duration of exam. These videos can be analyzed later by human examiner to detect possible cheating case. However, viewing hours of exam videos for each student can be impractical for a large class and thus detecting cheating would be next to impossible. Although some AI-based tools are being used by some proctoring software to raise flags, they are not always very useful. In this paper we propose a cheating detection technique that analyzes an exam video to extract four types of event data, which are then fed to a pre-trained classification model for detecting cheating activity. We formulate the cheating detection problem as a multivariate time-series classification problem by transforming each video into a multivariate time-series representing the time-varying event data extracted from each frame of the video. We have developed a real dataset of cheating videos and conduct extensive experiments with varying video lengths, different deep learning and traditional machine learning models and feature sets, achieving prediction accuracy as high as 97.7%.",multimedia
10.1007/978-981-16-5689-7_12,preprocessed,Advances in Data and Information Sciences,Springer,2022-01-01 00:00:00,springer,applications of high dimensional neural networks: a survey,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5689-7_12,"The evolution of artificial neural networks has always been inspired by enormous power of human brain. This survey can be an eye-opener for researchers as its diverse applications of HDNNs in present scenario shows an intelligent way to mimic human brain without creating a complex neuronal architecture having large number of layers. HDNN’s urgency is evident because in science many quantities are measured not by single values for each of them but a group of values defines 1 single unit as: signal has two values: amplitude and phase. The deficiency in present literature on the questions allied with HDNN application looks like sluggish down research focal point and growth in the area. Hence, there exists a call for state-of-the-art addressing high-dimensional problems in neural networks. The study equips readers with a lucid acquaintance of the existing and novel inclination in HDNN replicas. A lot of applications of HDNNs in various disciplines like: healthcare, climate, security, speech recognition, computer vision, music signal processing, production, stock, science, etc. are covered here to confirm advancement in HDNNs. Study divulges that HDNNs is prevalently known as: CVNN, QVNN, 3D VVNN, and OVNN. This paper also reveals that HDNNs have outperformed real valued neural networks in terms of resource utilization, training data set requirement, and accuracy of results. To see a comparative picture of significance and possible implementation of different HDNNs few charts are provided. A motivational message and suggestions for future researches in this area of High-Dimensionality will conclude this paper.",multimedia
http://arxiv.org/abs/2202.08227v1,preprocessed,arxiv,arxiv,2022-02-16 00:00:00,arxiv,ditto: building digital twins of articulated objects from interaction,http://arxiv.org/abs/2202.08227v1,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",multimedia
http://arxiv.org/abs/2202.06483v2,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,bifsmn: binary neural network for keyword spotting,http://arxiv.org/abs/2202.06483v2,"The deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications. However, computational resources for these
networks are significantly constrained since they usually run on-call on edge
devices. In this paper, we present BiFSMN, an accurate and extreme-efficient
binary neural network for KWS. We first construct a High-frequency Enhancement
Distillation scheme for the binarization-aware training, which emphasizes the
high-frequency information from the full-precision network's representation
that is more crucial for the optimization of the binarized network. Then, to
allow the instant and adaptive accuracy-efficiency trade-offs at runtime, we
also propose a Thinnable Binarization Architecture to further liberate the
acceleration potential of the binarized network from the topology perspective.
Moreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8
devices which fully utilizes registers and increases instruction throughput to
push the limit of deployment efficiency. Extensive experiments show that BiFSMN
outperforms existing binarization methods by convincing margins on various
datasets and is even comparable with the full-precision counterpart (e.g., less
than 3% drop on Speech Commands V1-12). We highlight that benefiting from the
thinnable architecture and the optimized 1-bit implementation, BiFSMN can
achieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge
hardware.",multimedia
http://arxiv.org/abs/2202.05940v1,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,automatic curriculum generation for learning adaptation in networking,http://arxiv.org/abs/2202.05940v1,"As deep reinforcement learning (RL) showcases its strengths in networking and
systems, its pitfalls also come to the public's attention--when trained to
handle a wide range of network workloads and previously unseen deployment
environments, RL policies often manifest suboptimal performance and poor
generalizability.
  To tackle these problems, we present Genet, a new training framework for
learning better RL-based network adaptation algorithms. Genet is built on the
concept of curriculum learning, which has proved effective against similar
issues in other domains where RL is extensively employed. At a high level,
curriculum learning gradually presents more difficult environments to the
training, rather than choosing them randomly, so that the current RL model can
make meaningful progress in training. However, applying curriculum learning in
networking is challenging because it remains unknown how to measure the
""difficulty"" of a network environment.
  Instead of relying on handcrafted heuristics to determine the environment's
difficulty level, our insight is to utilize traditional rule-based (non-RL)
baselines: If the current RL model performs significantly worse in a network
environment than the baselines, then the model's potential to improve when
further trained in this environment is substantial. Therefore, Genet
automatically searches for the environments where the current model falls
significantly behind a traditional baseline scheme and iteratively promotes
these environments as the training progresses. Through evaluating Genet on
three use cases--adaptive video streaming, congestion control, and load
balancing, we show that Genet produces RL policies which outperform both
regularly trained RL policies and traditional baselines in each context, not
only under synthetic workloads but also in real environments.",multimedia
http://arxiv.org/abs/2202.05811v1,preprocessed,arxiv,arxiv,2022-02-11 00:00:00,arxiv,overhead image factors for underwater sonar-based slam,http://arxiv.org/abs/2202.05811v1,"Simultaneous localization and mapping (SLAM) is a critical capability for any
autonomous underwater vehicle (AUV). However, robust, accurate state estimation
is still a work in progress when using low-cost sensors. We propose enhancing a
typical low-cost sensor package using widely available and often free prior
information; overhead imagery. Given an AUV's sonar image and a partially
overlapping, globally-referenced overhead image, we propose using a
convolutional neural network (CNN) to generate a synthetic overhead image
predicting the above-surface appearance of the sonar image contents. We then
use this synthetic overhead image to register our observations to the provided
global overhead image. Once registered, the transformation is introduced as a
factor into a pose SLAM factor graph. We use a state-of-the-art simulation
environment to perform validation over a series of benchmark trajectories and
quantitatively show the improved accuracy of robot state estimation using the
proposed approach. We also show qualitative outcomes from a real AUV field
deployment. Video attachment: https://youtu.be/_uWljtp58ks",multimedia
http://arxiv.org/abs/2202.04971v1,preprocessed,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"asrpu: a programmable accelerator for low-power automatic speech
  recognition",http://arxiv.org/abs/2202.04971v1,"The outstanding accuracy achieved by modern Automatic Speech Recognition
(ASR) systems is enabling them to quickly become a mainstream technology. ASR
is essential for many applications, such as speech-based assistants, dictation
systems and real-time language translation. However, highly accurate ASR
systems are computationally expensive, requiring on the order of billions of
arithmetic operations to decode each second of audio, which conflicts with a
growing interest in deploying ASR on edge devices. On these devices, hardware
acceleration is key for achieving acceptable performance. However, ASR is a
rich and fast-changing field, and thus, any overly specialized hardware
accelerator may quickly become obsolete.
  In this paper, we tackle those challenges by proposing ASRPU, a programmable
accelerator for on-edge ASR. ASRPU contains a pool of general-purpose cores
that execute small pieces of parallel code. Each of these programs computes one
part of the overall decoder (e.g. a layer in a neural network). The accelerator
automates some carefully chosen parts of the decoder to simplify the
programming without sacrificing generality. We provide an analysis of a modern
ASR system implemented on ASRPU and show that this architecture can achieve
real-time decoding with a very low power budget.",multimedia
http://arxiv.org/abs/2202.02656v1,preprocessed,arxiv,arxiv,2022-02-05 00:00:00,arxiv,a survey of top-down approaches for human pose estimation,http://arxiv.org/abs/2202.02656v1,"Human pose estimation in two-dimensional images videos has been a hot topic
in the computer vision problem recently due to its vast benefits and potential
applications for improving human life, such as behaviors recognition, motion
capture and augmented reality, training robots, and movement tracking. Many
state-of-the-art methods implemented with Deep Learning have addressed several
challenges and brought tremendous remarkable results in the field of human pose
estimation. Approaches are classified into two kinds: the two-step framework
(top-down approach) and the part-based framework (bottom-up approach). While
the two-step framework first incorporates a person detector and then estimates
the pose within each box independently, detecting all body parts in the image
and associating parts belonging to distinct persons is conducted in the
part-based framework. This paper aims to provide newcomers with an extensive
review of deep learning methods-based 2D images for recognizing the pose of
people, which only focuses on top-down approaches since 2016. The discussion
through this paper presents significant detectors and estimators depending on
mathematical background, the challenges and limitations, benchmark datasets,
evaluation metrics, and comparison between methods.",multimedia
http://arxiv.org/abs/2201.10910v1,preprocessed,arxiv,arxiv,2022-01-26 00:00:00,arxiv,"a bayesian based deep unrolling algorithm for single-photon lidar
  systems",http://arxiv.org/abs/2201.10910v1,"Deploying 3D single-photon Lidar imaging in real world applications faces
multiple challenges including imaging in high noise environments. Several
algorithms have been proposed to address these issues based on statistical or
learning-based frameworks. Statistical methods provide rich information about
the inferred parameters but are limited by the assumed model correlation
structures, while deep learning methods show state-of-the-art performance but
limited inference guarantees, preventing their extended use in critical
applications. This paper unrolls a statistical Bayesian algorithm into a new
deep learning architecture for robust image reconstruction from single-photon
Lidar data, i.e., the algorithm's iterative steps are converted into neural
network layers. The resulting algorithm benefits from the advantages of both
statistical and learning based frameworks, providing best estimates with
improved network interpretability. Compared to existing learning-based
solutions, the proposed architecture requires a reduced number of trainable
parameters, is more robust to noise and mismodelling effects, and provides
richer information about the estimates including uncertainty measures. Results
on synthetic and real data show competitive results regarding the quality of
the inference and computational complexity when compared to state-of-the-art
algorithms.",multimedia
http://arxiv.org/abs/2201.10369v1,preprocessed,arxiv,arxiv,2022-01-25 00:00:00,arxiv,winograd convolution for deep neural networks: efficient point selection,http://arxiv.org/abs/2201.10369v1,"Convolutional neural networks (CNNs) have dramatically improved the accuracy
of tasks such as object recognition, image segmentation and interactive speech
systems. CNNs require large amounts of computing resources because
ofcomputationally intensive convolution layers. Fast convolution algorithms
such as Winograd convolution can greatly reduce the computational cost of these
layers at a cost of poor numeric properties, such that greater savings in
computation exponentially increase floating point errors.
  A defining feature of each Winograd convolution algorithm is a set of
real-value points where polynomials are sampled. The choice of points impacts
the numeric accuracy of the algorithm, but the optimal set of points for small
convolutions remains unknown. Existing work considers only small integers and
simple fractions as candidate points. In this work, we propose a novel approach
to point selection using points of the form {-1/c , -c, c, 1/c } using the full
range of real-valued numbers for c. We show that groups of this form cause
cancellations in the Winograd transform matrices that reduce numeric error. We
find empirically that the error for different values of c forms a rough curve
across the range of real-value numbers helping to localize the values of c that
reduce error and that lower errors can be achieved with non-obvious real-valued
evaluation points instead of integers or simple fractions. We study a range of
sizes for small convolutions and achieve reduction in error ranging from 2% to
around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in
cases when we select a subset of our proposed points which will always lead to
a lower error. Finally we implement a complete Winograd convolution layer and
use it to run deep convolution neural networks on real datasets and show that
our proposed points reduce error, ranging from 22% to 63%.",multimedia
http://arxiv.org/abs/2201.09550v1,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,crowd tracking and monitoring middleware via map-reduce,http://arxiv.org/abs/2201.09550v1,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques.",multimedia
http://arxiv.org/abs/2201.10947v1,preprocessed,arxiv,arxiv,2022-01-22 00:00:00,arxiv,"enabling deep learning on edge devices through filter pruning and
  knowledge transfer",http://arxiv.org/abs/2201.10947v1,"Deep learning models have introduced various intelligent applications to edge
devices, such as image classification, speech recognition, and augmented
reality. There is an increasing need of training such models on the devices in
order to deliver personalized, responsive, and private learning. To address
this need, this paper presents a new solution for deploying and training
state-of-the-art models on the resource-constrained devices. First, the paper
proposes a novel filter-pruning-based model compression method to create
lightweight trainable models from large models trained in the cloud, without
much loss of accuracy. Second, it proposes a novel knowledge transfer method to
enable the on-device model to update incrementally in real time or near real
time using incremental learning on new data and enable the on-device model to
learn the unseen categories with the help of the in-cloud model in an
unsupervised fashion. The results show that 1) our model compression method can
remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy
of over 90% on CIFAR-10; 2) our knowledge transfer method enables the
compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good
accuracy on old categories; 3) it allows the compressed models to converge
within real time (three to six minutes) on the edge for incremental learning
tasks; 4) it enables the model to classify unseen categories of data (78.92%
Top-1 accuracy) that it is never trained with.",multimedia
http://arxiv.org/abs/2201.08619v1,preprocessed,arxiv,arxiv,2022-01-21 00:00:00,arxiv,"dangerous cloaking: natural trigger based backdoor attacks on object
  detectors in the physical world",http://arxiv.org/abs/2201.08619v1,"Deep learning models have been shown to be vulnerable to recent backdoor
attacks. A backdoored model behaves normally for inputs containing no
attacker-secretly-chosen trigger and maliciously for inputs with the trigger.
To date, backdoor attacks and countermeasures mainly focus on image
classification tasks. And most of them are implemented in the digital world
with digital triggers. Besides the classification tasks, object detection
systems are also considered as one of the basic foundations of computer vision
tasks. However, there is no investigation and understanding of the backdoor
vulnerability of the object detector, even in the digital world with digital
triggers. For the first time, this work demonstrates that existing object
detectors are inherently susceptible to physical backdoor attacks. We use a
natural T-shirt bought from a market as a trigger to enable the cloaking
effect--the person bounding-box disappears in front of the object detector. We
show that such a backdoor can be implanted from two exploitable attack
scenarios into the object detector, which is outsourced or fine-tuned through a
pretrained model. We have extensively evaluated three popular object detection
algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building
upon 19 videos shot in real-world scenes, we confirm that the backdoor attack
is robust against various factors: movement, distance, angle, non-rigid
deformation, and lighting. Specifically, the attack success rate (ASR) in most
videos is 100% or close to it, while the clean data accuracy of the backdoored
model is the same as its clean counterpart. The latter implies that it is
infeasible to detect the backdoor behavior merely through a validation set. The
averaged ASR still remains sufficiently high to be 78% in the transfer learning
attack scenarios evaluated on CenterNet. See the demo video on
https://youtu.be/Q3HOF4OobbY.",multimedia
http://arxiv.org/abs/2201.08197v1,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,"enhancement or super-resolution: learning-based adaptive video streaming
  with client-side video processing",http://arxiv.org/abs/2201.08197v1,"The rapid development of multimedia and communication technology has resulted
in an urgent need for high-quality video streaming. However, robust video
streaming under fluctuating network conditions and heterogeneous client
computing capabilities remains a challenge. In this paper, we consider an
enhancement-enabled video streaming network under a time-varying wireless
network and limited computation capacity. ""Enhancement"" means that the client
can improve the quality of the downloaded video segments via image processing
modules. We aim to design a joint bitrate adaptation and client-side
enhancement algorithm toward maximizing the quality of experience (QoE). We
formulate the problem as a Markov decision process (MDP) and propose a deep
reinforcement learning (DRL)-based framework, named ENAVS. As video streaming
quality is mainly affected by video compression, we demonstrate that the video
enhancement algorithm outperforms the super-resolution algorithm in terms of
signal-to-noise ratio and frames per second, suggesting a better solution for
client processing in video streaming. Ultimately, we implement ENAVS and
demonstrate extensive testbed results under real-world bandwidth traces and
videos. The simulation shows that ENAVS is capable of delivering 5%-14% more
QoE under the same bandwidth and computing power conditions as conventional ABR
streaming.",multimedia
http://arxiv.org/abs/2201.08102v2,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,safe deep rl in 3d environments using human feedback,http://arxiv.org/abs/2201.08102v2,"Agents should avoid unsafe behaviour during both training and deployment.
This typically requires a simulator and a procedural specification of unsafe
behaviour. Unfortunately, a simulator is not always available, and procedurally
specifying constraints can be difficult or impossible for many real-world
tasks. A recently introduced technique, ReQueST, aims to solve this problem by
learning a neural simulator of the environment from safe human trajectories,
then using the learned simulator to efficiently learn a reward model from human
feedback. However, it is yet unknown whether this approach is feasible in
complex 3D environments with feedback obtained from real humans - whether
sufficient pixel-based neural simulator quality can be achieved, and whether
the human data requirements are viable in terms of both quantity and quality.
In this paper we answer this question in the affirmative, using ReQueST to
train an agent to perform a 3D first-person object collection task using data
entirely from human contractors. We show that the resulting agent exhibits an
order of magnitude reduction in unsafe behaviour compared to standard
reinforcement learning.",multimedia
http://arxiv.org/abs/2201.07312v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,model-driven cluster resource management for ai workloads in edge clouds,http://arxiv.org/abs/2201.07312v1,"Since emerging edge applications such as Internet of Things (IoT) analytics
and augmented reality have tight latency constraints, hardware AI accelerators
have been recently proposed to speed up deep neural network (DNN) inference run
by these applications. Resource-constrained edge servers and accelerators tend
to be multiplexed across multiple IoT applications, introducing the potential
for performance interference between latency-sensitive workloads. In this
paper, we design analytic models to capture the performance of DNN inference
workloads on shared edge accelerators, such as GPU and edgeTPU, under different
multiplexing and concurrency behaviors. After validating our models using
extensive experiments, we use them to design various cluster resource
management algorithms to intelligently manage multiple applications on edge
accelerators while respecting their latency constraints. We implement a
prototype of our system in Kubernetes and show that our system can host 2.3X
more DNN applications in heterogeneous multi-tenant edge clusters with no
latency violations when compared to traditional knapsack hosting algorithms.",multimedia
http://arxiv.org/abs/2201.07232v1,preprocessed,arxiv,arxiv,2022-01-18 00:00:00,arxiv,"real-time x-ray phase-contrast imaging using spinnet -- a speckle-based
  phase-contrast imaging neural network",http://arxiv.org/abs/2201.07232v1,"X-ray phase-contrast imaging has become indispensable for visualizing samples
with low absorption contrast. In this regard, speckle-based techniques have
shown significant advantages in spatial resolution, phase sensitivity, and
implementation flexibility compared with traditional methods. However, their
computational cost has hindered their wider adoption. By exploiting the power
of deep learning, we developed a new speckle-based phase-contrast imaging
neural network (SPINNet) that boosts the phase retrieval speed by at least two
orders of magnitude compared to existing methods. To achieve this performance,
we combined SPINNet with a novel coded-mask-based technique, an enhanced
version of the speckle-based method. Using this scheme, we demonstrate a
simultaneous reconstruction of absorption and phase images on the order of 100
ms, where a traditional correlation-based analysis would take several minutes
even with a cluster. In addition to significant improvement in speed, our
experimental results show that the imaging resolution and phase retrieval
quality of SPINNet outperform existing single-shot speckle-based methods.
Furthermore, we successfully demonstrate its application in 3D X-ray
phase-contrast tomography. Our result shows that SPINNet could enable many
applications requiring high-resolution and fast data acquisition and
processing, such as in-situ and in-operando 2D and 3D phase-contrast imaging
and real-time at-wavelength metrology and wavefront sensing.",multimedia
http://arxiv.org/abs/2201.10978v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,machine learning for food review and recommendation,http://arxiv.org/abs/2201.10978v1,"Food reviews and recommendations have always been important for online food
service websites. However, reviewing and recommending food is not simple as it
is likely to be overwhelmed by disparate contexts and meanings. In this paper,
we use different deep learning approaches to address the problems of sentiment
analysis, automatic review tag generation, and retrieval of food reviews. We
propose to develop a web-based food review system at Nanyang Technological
University (NTU) named NTU Food Hunter, which incorporates different deep
learning approaches that help users with food selection. First, we implement
the BERT and LSTM deep learning models into the system for sentiment analysis
of food reviews. Then, we develop a Part-of-Speech (POS) algorithm to
automatically identify and extract adjective-noun pairs from the review content
for review tag generation based on POS tagging and dependency parsing. Finally,
we also train a RankNet model for the re-ranking of the retrieval results to
improve the accuracy in our Solr-based food reviews search system. The
experimental results show that our proposed deep learning approaches are
promising for the applications of real-world problems.",multimedia
http://arxiv.org/abs/2201.06912v1,preprocessed,arxiv,arxiv,2022-01-14 00:00:00,arxiv,digital twin: from concept to practice,http://arxiv.org/abs/2201.06912v1,"Recent technological developments and advances in Artificial Intelligence
(AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT),
virtually making it possible to introduce automation into all aspects of work
processes. Given these possibilities that DT can offer, practitioners are
facing increasingly difficult decisions regarding what capabilities to select
while deploying a DT in practice. The lack of research in this field has not
helped either. It has resulted in the rebranding and reuse of emerging
technological capabilities like prediction, simulation, AI, and Machine
Learning (ML) as necessary constituents of DT. Inappropriate selection of
capabilities in a DT can result in missed opportunities, strategic
misalignments, inflated expectations, and risk of it being rejected as just
hype by the practitioners. To alleviate this challenge, this paper proposes the
digitalization framework, designed and developed by following a Design Science
Research (DSR) methodology over a period of 18 months. The framework can help
practitioners select an appropriate level of sophistication in a DT by weighing
the pros and cons for each level, deciding evaluation criteria for the digital
twin system, and assessing the implications of the selected DT on the
organizational processes and strategies, and value creation. Three real-life
case studies illustrate the application and usefulness of the framework.",multimedia
http://arxiv.org/abs/2201.05184v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"achieving ai-enabled robust end-to-end quality of experience over radio
  access networks",http://arxiv.org/abs/2201.05184v1,"Emerging applications such as Augmented Reality, the Internet of Vehicles and
Remote Surgery require both computing and networking functions working in
harmony. The End-to-end (E2E) quality of experience (QoE) for these
applications depends on the synchronous allocation of networking and computing
resources. However, the relationship between the resources and the E2E QoE
outcomes is typically stochastic and non-linear. In order to make efficient
resource allocation decisions, it is essential to model these relationships.
This article presents a novel machine-learning based approach to learn these
relationships and concurrently orchestrate both resources for this purpose. The
machine learning models further help make robust allocation decisions regarding
stochastic variations and simplify robust optimization to a conventional
constrained optimization. When resources are insufficient to accommodate all
application requirements, our framework supports executing some of the
applications with minimal degradation (graceful degradation) of E2E QoE. We
also show how we can implement the learning and optimization methods in a
distributed fashion by the Software-Defined Network (SDN) and Kubernetes
technologies. Our results show that deep learning-based modelling achieves E2E
QoE with approximately 99.8\% accuracy, and our robust joint-optimization
technique allocates resources efficiently when compared to existing
differential services alternatives.",multimedia
http://arxiv.org/abs/2201.04833v1,preprocessed,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"snapshotnet: self-supervised feature learning for point cloud data
  segmentation using minimal labeled data",http://arxiv.org/abs/2201.04833v1,"Manually annotating complex scene point cloud datasets is both costly and
error-prone. To reduce the reliance on labeled data, a new model called
SnapshotNet is proposed as a self-supervised feature learning approach, which
directly works on the unlabeled point cloud data of a complex 3D scene. The
SnapshotNet pipeline includes three stages. In the snapshot capturing stage,
snapshots, which are defined as local collections of points, are sampled from
the point cloud scene. A snapshot could be a view of a local 3D scan directly
captured from the real scene, or a virtual view of such from a large 3D point
cloud dataset. Snapshots could also be sampled at different sampling rates or
fields of view (FOVs), thus multi-FOV snapshots, to capture scale information
from the scene. In the feature learning stage, a new pre-text task called
multi-FOV contrasting is proposed to recognize whether two snapshots are from
the same object or not, within the same FOV or across different FOVs. Snapshots
go through two self-supervised learning steps: the contrastive learning step
with both part and scale contrasting, followed by a snapshot clustering step to
extract higher level semantic features. Then a weakly-supervised segmentation
stage is implemented by first training a standard SVM classifier on the learned
features with a small fraction of labeled snapshots. The trained SVM is used to
predict labels for input snapshots and predicted labels are converted into
point-wise label assignments for semantic segmentation of the entire scene
using a voting procedure. The experiments are conducted on the Semantic3D
dataset and the results have shown that the proposed method is capable of
learning effective features from snapshots of complex scene data without any
labels. Moreover, the proposed method has shown advantages when comparing to
the SOA method on weakly-supervised point cloud semantic segmentation.",multimedia
http://arxiv.org/abs/2201.04195v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,matching-based service offloading for compute-less driven iot networks,http://arxiv.org/abs/2201.04195v1,"With the advent of the Internet of Things (IoT) and 5G networks, edge
computing is offering new opportunities for business model and use cases
innovations. Service providers can now virtualize the cloud beyond the data
center to meet the latency, data sovereignty, reliability, and interoperability
requirements. Yet, many new applications (e.g., augmented reality, virtual
reality, artificial intelligence) are computation-intensive and
delay-sensitivity. These applications are invoked heavily with similar inputs
that could lead to the same output. Compute-less networks aim to implement a
network with a minimum amount of computation and communication. This can be
realized by offloading prevalent services to the edge and thus minimizing
communication in the core network and eliminating redundant computations using
the computation reuse concept. In this paper, we present matching-based
services offloading schemes for compute-less IoT networks. We adopt the
matching theory to match service offloading to the appropriate edge server(s).
Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme
that aims to offload the most invoked and highly reusable services to the
appropriate edge servers. We further extend WHISTLE to provide horizontal
one-to-many computation reuse sharing among edge servers which leads to
bouncing less computation back to the cloud. We evaluate the efficiency and
effectiveness of WHISTLE with a real-world dataset. The obtained findings show
that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the
computation up to 77%, and decrease the communication up to 71%. Theoretical
analyses also prove the stability of the designed schemes.",multimedia
http://arxiv.org/abs/2201.04014v2,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,captcha attack: turning captchas against humanity,http://arxiv.org/abs/2201.04014v2,"Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
  In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.",multimedia
http://arxiv.org/abs/2201.03804v1,preprocessed,arxiv,arxiv,2022-01-11 00:00:00,arxiv,"ci-avsr: a cantonese audio-visual speech dataset for in-car command
  recognition",http://arxiv.org/abs/2201.03804v1,"With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",multimedia
http://arxiv.org/abs/2201.03335v2,preprocessed,arxiv,arxiv,2022-01-10 00:00:00,arxiv,"deepke: a deep learning based knowledge extraction toolkit for knowledge
  base population",http://arxiv.org/abs/2201.03335v2,"We present a new open-source and extensible knowledge extraction toolkit,
called DeepKE (Deep learning based Knowledge Extraction), supporting standard
fully supervised, low-resource few-shot and document-level scenarios. DeepKE
implements various information extraction tasks, including named entity
recognition, relation extraction and attribute extraction. With a unified
framework, DeepKE allows developers and researchers to customize datasets and
models to extract information from unstructured texts according to their
requirements. Specifically, DeepKE not only provides various functional modules
and model implementation for different tasks and scenarios but also organizes
all components by consistent frameworks to maintain sufficient modularity and
extensibility. Besides, we present an online platform in
http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has
been equipped with Google Colab tutorials and comprehensive documents for
beginners. We release the source code at https://github.com/zjunlp/DeepKE, with
a demo video.",multimedia
http://arxiv.org/abs/2201.02503v1,preprocessed,arxiv,arxiv,2022-01-07 00:00:00,arxiv,"a review of deep learning techniques for markerless human motion on
  synthetic datasets",http://arxiv.org/abs/2201.02503v1,"Markerless motion capture has become an active field of research in computer
vision in recent years. Its extensive applications are known in a great variety
of fields, including computer animation, human motion analysis, biomedical
research, virtual reality, and sports science. Estimating human posture has
recently gained increasing attention in the computer vision community, but due
to the depth of uncertainty and the lack of the synthetic datasets, it is a
challenging task. Various approaches have recently been proposed to solve this
problem, many of which are based on deep learning. They are primarily focused
on improving the performance of existing benchmarks with significant advances,
especially 2D images. Based on powerful deep learning techniques and recently
collected real-world datasets, we explored a model that can predict the
skeleton of an animation based solely on 2D images. Frames generated from
different real-world datasets with synthesized poses using different body
shapes from simple to complex. The implementation process uses DeepLabCut on
its own dataset to perform many necessary steps, then use the input frames to
train the model. The output is an animated skeleton for human movement. The
composite dataset and other results are the ""ground truth"" of the deep model.",multimedia
http://arxiv.org/abs/2201.02279v1,preprocessed,arxiv,arxiv,2022-01-06 00:00:00,arxiv,de-rendering 3d objects in the wild,http://arxiv.org/abs/2201.02279v1,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.",multimedia
http://arxiv.org/abs/2201.01144v2,preprocessed,arxiv,arxiv,2022-01-04 00:00:00,arxiv,digital twin network: opportunities and challenges,http://arxiv.org/abs/2201.01144v2,"The proliferation of emergent network applications (e.g., AR/VR, telesurgery,
real-time communications) is increasing the difficulty of managing modern
communication networks. These applications typically have stringent
requirements (e.g., ultra-low deterministic latency), making it more difficult
for network operators to manage their network resources efficiently. In this
article, we propose the Digital Twin Network (DTN) as a key enabler for
efficient network management in modern networks. We describe the general
architecture of the DTN and argue that recent trends in Machine Learning (ML)
enable building a DTN that efficiently and accurately mimics real-world
networks. In addition, we explore the main ML technologies that enable
developing the components of the DTN architecture. Finally, we describe the
open challenges that the research community has to address in the upcoming
years in order to enable the deployment of the DTN in real-world scenarios.",multimedia
http://arxiv.org/abs/2201.00768v1,preprocessed,arxiv,arxiv,2022-01-03 00:00:00,arxiv,"robust natural language processing: recent advances, challenges, and
  future directions",http://arxiv.org/abs/2201.00768v1,"Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.",multimedia
http://arxiv.org/abs/2201.00309v1,preprocessed,arxiv,arxiv,2022-01-02 00:00:00,arxiv,"optimizing machine learning inference queries with correlative proxy
  models",http://arxiv.org/abs/2201.00309v1,"We consider accelerating machine learning (ML) inference queries on
unstructured datasets. Expensive operators such as feature extractors and
classifiers are deployed as user-defined functions(UDFs), which are not
penetrable with classic query optimization techniques such as predicate
push-down. Recent optimization schemes (e.g., Probabilistic Predicates or PP)
assume independence among the query predicates, build a proxy model for each
predicate offline, and rewrite a new query by injecting these cheap proxy
models in the front of the expensive ML UDFs. In such a manner, unlikely inputs
that do not satisfy query predicates are filtered early to bypass the ML UDFs.
We show that enforcing the independence assumption in this context may result
in sub-optimal plans. In this paper, we propose CORE, a query optimizer that
better exploits the predicate correlations and accelerates ML inference
queries. Our solution builds the proxy models online for a new query and
leverages a branch-and-bound search process to reduce the building costs.
Results on three real-world text, image and video datasets show that CORE
improves the query throughput by up to 63% compared to PP and up to 80%
compared to running the queries as it is.",multimedia
10.1016/j.enconman.2022.115217,preprocessed,Energy Conversion and Management,scopus,2022-02-15,sciencedirect,robopv: an integrated software package for autonomous aerial monitoring of large scale pv plants,https://api.elsevier.com/content/abstract/scopus_id/85122793867,"In this paper, a novel software package, called RoboPV, is introduced for autonomous aerial monitoring of PV plants. RoboPV automatically performs aerial monitoring of PV plants, from optimal trajectory planning to image processing and pattern recognition for real-time fault detection and analysis. RoboPV consists of four integrated components: boundary area detection, path planning, dynamic processing, and fault detection. To design an optimal flight path, aerial images of PV plants, which have been collected from experimental flights, are given as inputs to a developed encoder-decoder deep learning architecture to extract boundary points of PV plants automatically. Then, a novel path planning algorithm is conducted by RoboPV to design an optimal flight path with full coverage of whole regions of the PV plant. Aerial images are analysed in real-time during the flight by a high precise neural network trained for automatic fault detection. In this study, several decision-making and maneuver algorithms were developed for various real-world flight conditions to improve the performance of RoboPV during an autonomous aerial inspection. RoboPV is a modular processing library that can be installed on any micro-computer processor with a low computational power. Moreover, supporting the MAVLink communication protocol enables RoboPV to connect with an intelligent Pixhawk flight autopilot and navigate a wide range of multi-rotors. To demonstrate the performance of RoboPV, a six degrees of freedom dynamic model of a multi-rotor is developed in a SIMULINK environment with a defined aerial monitoring mission on three different real megawatt-scale PV plants. The results prove that RoboPV can execute the autonomous aerial inspection with an overall accuracy of 93% for large-scale PV plants.",multimedia
10.1016/j.ymssp.2021.108284,preprocessed,Mechanical Systems and Signal Processing,scopus,2022-02-15,sciencedirect,real-time model calibration with deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85112506465,"The real-time, and accurate inference of model parameters is of great importance in many scientific and engineering disciplines that use computational models (such as a digital twin) for the analysis and prediction of complex physical processes. However, fast and accurate inference for processes of complex systems cannot easily be achieved in real-time with state-of-the-art methods under noisy real-world conditions with the requirement of a real-time response. The primary reason is that the inference of model parameters with traditional techniques based on optimization or sampling often suffers from computational and statistical challenges, resulting in a trade-off between accuracy and deployment time. In this paper, we propose a novel framework for inference of model parameters based on reinforcement learning. The proposed methodology is demonstrated and evaluated on two different physics-based models of turbofan engines. The experimental results demonstrate that the proposed methodology outperforms all other tested methods in terms of speed and robustness, with high inference accuracy.",multimedia
10.1016/j.vrih.2022.01.004,preprocessed,Virtual Reality and Intelligent Hardware,scopus,2022-02-01,sciencedirect,virtual-reality-based digital twin of office spaces with social distance measurement feature,https://api.elsevier.com/content/abstract/scopus_id/85124517698,"Background
                  Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
               
                  Methods
                  In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
               
                  Results
                  We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
               
                  Conclusions
                  A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.",multimedia
10.1016/j.pmcj.2022.101540,preprocessed,Pervasive and Mobile Computing,scopus,2022-02-01,sciencedirect,lwtool: a data processing toolkit for building a real-time pressure mapping smart textile software system,https://api.elsevier.com/content/abstract/scopus_id/85123030397,"Pressure mapping smart textile is a new type of sensing modality that transforms the pressure distribution over surfaces into digital ”image” and ”video”, that has rich application scenarios in Human Activity Recognition (HAR), because all human activities are linked with force change over certain surfaces. To speed up its application exploration, we propose a toolkit named LwTool for the data processing, including: (a) a feature library, including 1830 ready-to-use temporal and spatial features, (b) a hierarchical feature selection framework that automatically picks out the best features for a new application from the feature library. As real-time processing capability is important for instant user feedback, we emphasize not only on good recognition result but also on reducing time cost when selecting features. Our library and algorithms are validated on Smart-Toy and Smart-Bedsheet applications, an 89.7% accuracy for Smart-Toy and an 83.8% accuracy for Smart-Bedsheet can be achieved (10-fold cross-validation) using our feature library. Adopting the feature selection algorithm, the processing speed is increased by more than 3 times while maintaining high accuracy for both two applications. We believe our method could be a general and powerful toolkit in building real-time recognition software systems for pressure mapping smart textile.",multimedia
10.1016/j.cviu.2021.103339,preprocessed,Computer Vision and Image Understanding,scopus,2022-02-01,sciencedirect,snapshotnet: self-supervised feature learning for point cloud data segmentation using minimal labeled data,https://api.elsevier.com/content/abstract/scopus_id/85122523188,"Manually annotating complex scene point cloud datasets is both costly and error-prone. To reduce the reliance on labeled data, a new model called SnapshotNet is proposed as a self-supervised feature learning approach, which directly works on the unlabeled point cloud data of a complex 3D scene. The SnapshotNet pipeline includes three stages. In the snapshot capturing stage, snapshots, which are defined as local collections of points, are sampled from the point cloud scene. A snapshot could be a view of a local 3D scan directly captured from the real scene, or a virtual view of such from a large 3D point cloud dataset. Snapshots could also be sampled at different sampling rates or fields of view (FOVs), thus multi-FOV snapshots, to capture scale information from the scene. In the feature learning stage, a new pre-text task called multi-FOV contrasting is proposed to recognize whether two snapshots are from the same object or not, within the same FOV or across different FOVs. Snapshots go through two self-supervised learning steps: the contrastive learning step with both part contrasting and scale contrasting, followed by a snapshot clustering step to extract higher level semantic features. Then a weakly-supervised segmentation stage is implemented by first training a standard SVM classifier on the learned features with a small fraction of labeled snapshots. Then trained SVM is further used to predict labels for input snapshots and predicted labels are converted into point-wise label assignments for semantic segmentation of the entire scene using a voting procedure. The experiments are conducted on the Semantic3D dataset and the results have shown that the proposed method is capable of learning effective features from snapshots of complex scene data without any labels. Moreover, the proposed weakly-supervised method has shown advantages when comparing to the state of the art method on weakly-supervised point cloud semantic segmentation.",multimedia
10.1016/j.compag.2021.106644,preprocessed,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,ric-net: a plant disease classification model based on the fusion of inception and residual structure and embedded attention mechanism,https://api.elsevier.com/content/abstract/scopus_id/85121931762,"In this paper, we proposed a convolutional neural network based on Inception and residual structure with an embedded modified convolutional block attention module (CBAM), aiming to improve the classification of plant leaf diseases. Corn, potatoes and tomatoes are the most cultivated grains in southern China. The leaves of the three crops are very fragile and sensitive and are susceptible to leaf diseases, such as leaf blight of corn, late blight of potato and mosaic virus of tomato. These diseases cannot be identified at early stages. Therefore, an efficient solution is proposed by deep learning techniques to detect the disease categories of crops, which can effectively prevent the spread of diseases and ensure the normal growth of plants. In this experiment, our model achieved an overall accuracy of 99.55% for the identification of the three diseases of corn, potato and tomato. In addition, we tested the three plants individually. The classification accuracy of our model on corn, potato and tomato was 98.44%, 99.43% and 95.20%, respectively. We have also developed a web-based real-time plant disease classification system and deployed our model. The system had good performance in time and accuracy evaluation metrics. The results of this study showed that our model had fewer parameters, shorter training time, and higher recognition accuracy compared to existing image classification models.",multimedia
10.1016/j.cose.2021.102539,preprocessed,Computers and Security,scopus,2022-02-01,sciencedirect,deep face fuzzy vault: implementation and performance,https://api.elsevier.com/content/abstract/scopus_id/85119176199,"Biometric technologies, especially face recognition, have become an essential part of identity management systems worldwide. In deployments of biometrics, secure storage of biometric information is necessary in order to protect the users’ privacy. In this context, biometric cryptosystems are designed to meet key requirements of biometric information protection enabling a privacy-preserving storage and comparison of biometric data, e.g. feature vectors extracted from facial images. Until now, biometric cryptosystems have hardly been applied to state-of-the-art biometric recognition systems utilizing deep convolutional neural networks.
                  This work investigates the application of a well-known biometric cryptosystem, i.e. the improved fuzzy vault scheme, to facial feature vectors extracted through deep convolutional neural networks. To this end, a feature transformation method is introduced which maps fixed-length real-valued deep feature vectors to integer-valued feature sets. As part of said feature transformation, a detailed analysis of different feature quantisation and binarisation techniques is conducted. At key binding, obtained feature sets are locked in an unlinkable improved fuzzy vault. For key retrieval, the efficiency of different polynomial reconstruction techniques is investigated. The proposed feature transformation method and template protection scheme are agnostic of the biometric characteristic and, thus, can be applied to virtually any biometric features computed by a deep neural network. In experiments, an unlinkable improved deep face fuzzy vault-based template protection scheme is constructed employing features extracted with a state-of-the-art deep convolutional neural network trained with the additive angular margin loss (ArcFace). For the best configuration, a false non-match rate below 1% at a false match rate of 0.01%, is achieved in cross-database experiments on the FERET and FRGCv2 face databases. On average, a security level of up to approximately 28 bits is obtained. This work presents an effective face-based fuzzy vault scheme providing privacy protection of facial reference data as well as digital key derivation from face.",multimedia
10.1016/j.eswa.2021.116073,preprocessed,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,efficient machine learning approach for volunteer eye-blink detection in real-time using webcam,https://api.elsevier.com/content/abstract/scopus_id/85117617175,"The progressive diminishment of motor capacities due to Amyotrophic Lateral Sclerosis (ALS) causes a severe communication deficit. The development of Alternative Communication software aids ALS patients in overcoming communication issues and the detection of communication signals plays a big role in this task. In this paper, volunteer eye-blinking is proposed as human–computer interaction signal and an intelligent Computer Vision detector was built for handling the captured data in real-time using a generic webcam. The eye-blink detection was treated as an extension of the eye-state classification, and the base pipeline used is delineated as follows: face detection, face alignment, region-of-interest (ROI) extraction, and eye-state classification. Furthermore, this pipeline was complemented with auxiliary models: a rotation compensator, a ROIs evaluator, and a moving average filter. Two new datasets were created: the Youtube Eye-state Classification (YEC) dataset, built from the AVSpeech dataset by extracting face images; and the Autonomus Blink Dataset (ABD), built completely as a result of the present work. The YEC allowed training the eye-classification task; ABD was specifically idealized taking into consideration volunteer eye-blinking detection. The proposed models, a Convolutional Neural Network (CNN) and a Support Vector Machine (SVM), were trained by the YEC dataset and performance evaluation experiments for both models were conducted across different databases: CeW, ZJU, Eyeblink, Talking Face (public datasets) and ABD. The impact of the proposed auxiliary models was evaluated and the CNN and SVM models were compared for the eye-state classification task. Promising results were obtained: 97.44% accuracy for the eye-state classification task on the CeW dataset and 92.63% F1-Score for the eye-blink detection task on the ABD dataset.",multimedia
10.1016/j.apacoust.2021.108439,preprocessed,Applied Acoustics,scopus,2022-01-15,sciencedirect,"camnet: a controllable acoustic model for efficient, expressive, high-quality text-to-speech",https://api.elsevier.com/content/abstract/scopus_id/85116891718,"Spoken language is becoming one of the key components of human–machine interaction, both to send information to the machine – e.g. voice control – and to receive from it – e.g. virtual assistants. In this scenario, text-to-speech (TTS) models have become an essential artificial intelligence capacity. Even though this interaction can be based on neutral style speech, generating speech with different styles, pitches and speaking rates may improve user experience. With this in view, this paper presents CAMNet, a controllable acoustic model for efficient, expressive, high-quality TTS. CAMNet is based on deep convolutional TTS (DCTTS), a state-of-art acoustic model which is efficient and produces neutral speech. DCTTS was first adapted to generate Bark cepstrum acoustic features in order to integrate well with the LPCNet (linear prediction coefficient) neural vocoder and to remove the reduction factor which demanded the presence of an upsampling network before the vocoder – i.e. the CAMNet output can be directly fed into LPCNet. Next, style transfer functionality was added by means of a novel characterisation of the prosodic information from the Bark cepstrum acoustic features and a new approach to inject this information into the convolutional layers. Finally, controllability is provided via a variational auto-encoder module which creates a smoothed disentangled latent space which allows interpolation and extrapolation of reference styles as well as independent and simultaneous control of two generative factors: pitch and speaking rate. Moreover, this controllability is implemented using a simple offset-based approach. To sum up, CAMNet is an efficient acoustic model which provides a simple but consistent controllability on coarse-grained expression, pitch and speaking rate while still providing high-quality synthesised speech.",multimedia
10.1016/j.buildenv.2021.108532,preprocessed,Building and Environment,scopus,2022-01-01,sciencedirect,personal thermal comfort models using digital twins: preference prediction with bim-extracted spatial–temporal proximity data from build2vec,https://api.elsevier.com/content/abstract/scopus_id/85119963452,"Conventional thermal preference prediction in buildings has limitations due to the difficulty in capturing all environmental and personal factors. New model features can improve the ability of a machine learning model to classify a person’s thermal preference. The spatial context of a building can provide information to models about the windows, walls, heating and cooling sources, air diffusers, and other factors that create micro-environments that influence thermal comfort. Due to spatial heterogeneity, it is impractical to position sensors at a high enough resolution to capture all conditions. This research aims to build upon an existing vector-based spatial model, called Build2Vec, for predicting spatial–temporal occupants’ indoor environmental preferences. Build2Vec utilizes the spatial data from the Building Information Model (BIM) and indoor localization in a real-world setting. This framework uses longitudinal intensive thermal comfort subjective feedback from smart watch-based ecological momentary assessments (EMA). The aggregation of these data is combined into a graph network structure (i.e., objects and relations) and used as input for a classification model to predict occupant thermal preference. The results of a test implementation show 14%–28% accuracy improvement over a set of baselines that use conventional thermal preference prediction input variables.",multimedia
10.1016/j.jobe.2021.103571,preprocessed,Journal of Building Engineering,scopus,2022-01-01,sciencedirect,an integrated building energy performance evaluation method: from parametric modeling to ga-nn based energy consumption prediction modeling,https://api.elsevier.com/content/abstract/scopus_id/85119285403,"Building energy performance evaluation, as an important process in a sustainable building design, has important consequences for global energy conservation and environmental protection. The traditional methods to perform this evaluation are usually time-consuming and computationally complex, and have high requirements for designers’ professional knowledge on architectural physics and software operation skills. To solve these problems and provide rapid, user-friendly, and more accurate prediction results, this study presents an efficient building energy performance evaluation method which integrates building information modeling, energy simulation, and energy consumption prediction together. This method follows a three-stage research framework: Stage 1 proposes a rapid 3D building energy modeling process according to the parameterized setting, Stage 2 generates numerous simulation results automatically by EnergyPlus, and Stage 3 develops the user-friendly building energy consumption prediction model with the help of the Genetic Algorithm-Neural Network (GA-NN) and provides the energy performance level of the building design after the prediction. A case study is carried out to present the overall process and verify the accuracy of the proposed three-stage building energy performance evaluation method. This study contributes to the improvement of both the extensive dataset establishment and the operational efficiency of building energy consumption prediction. It can provide designers with a real-time, user-friendly, and reliable building energy consumption prediction tool and an energy performance assessment basis in the design phase of construction projects.",multimedia
10.1016/j.eswa.2021.115973,preprocessed,Expert Systems with Applications,scopus,2022-01-01,sciencedirect,deep correlation mining for multi-task image clustering,https://api.elsevier.com/content/abstract/scopus_id/85116928779,"Multi-task clustering (MTC) aims to enhance the performance of each individual task by leveraging the correlation information among them. Existing MTC algorithms usually first extract the feature representations of each task and then learn the relationships among multiple tasks for clustering. However, the multi-task correlations are not embedded into the feature learning in existing MTC. In addition, many real applications, such as image clustering, always perform visual feature extraction and clustering assignment separately, which often results in local optimal clustering resolutions. In this study, an end-to-end MTC framework, named Deep correlation mining for Multi-Task image Clustering (DMTC), is proposed to explore multi-task correlations and conduct image clustering simultaneously. Specifically, DMTC consists of two sub-networks: a between-task network (B-net) and a within-task network (W-net), which learn the correlations among multiple tasks and the relationships in each individual task, respectively, based on a deep convolutional network. To optimize B-net, an optimization procedure is proposed as follows: (1) DMTC builds a pseudo-graph to discover similar samples among tasks and obtain the positive pairs of possible related tasks. (2) A discriminator is designed to calculate the mutual information between the deep and shallow representations of related tasks, which can estimate the relatedness between each pair of related tasks. After that, the trained parameters in B-net are transferred to the within-task networks (W-net) as their initialized parameters, in which the above optimization procedure is performed again to obtain the final cluster partition by end-to-end training. Experimental results on NUS-Wide, Caltech-256, Cifar-100 and Pascal VOC demonstrate that our proposed DMTC method
                        1
                     
                     
                        1
                        The source code is available in https://github.com/Xiaoqiang-Yan/DMTC.
                     compares favorably to the state-of-the-art methods.",multimedia
10.1016/j.autcon.2021.103996,preprocessed,Automation in Construction,scopus,2022-01-01,sciencedirect,implementation experiments on convolutional neural network training using synthetic images for 3d pose estimation of an excavator on real images,https://api.elsevier.com/content/abstract/scopus_id/85116888055,"Remote and descriptive visualization of spatio-temporal information of excavator activities may increase awareness about jobsite hazards and operational performance in earthwork operations. One of the emerging approaches to collect this information is to extract the 3D pose of an excavator from the video frames using a convolutional neural network (CNN). However, this method requires labeling the training datasets, which are difficult to prepare because of conditions unsuitable for installing the motion capture sensors. This study investigates the performance of a CNN for estimating the 3D pose when trained on a synthetic dataset. In particular, a kinematic constraint is proposed to update the model parameters efficiently during training. The results show that the proposed method estimated the 3D poses of a real excavator with an average pose error of 9.63°. Hence, the proposed data augmentation method could help address the training data issues and improves the learning of real data complexity.",multimedia
10.1016/j.sigpro.2021.108317,preprocessed,Signal Processing,scopus,2022-01-01,sciencedirect,selective fixed-filter active noise control based on convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85114690529,"Active noise control (ANC) technology is increasingly ubiquitous in wearable audio devices, or hearables. Owing to its low computational complexity, high robustness, and exemplary performance in dealing with dynamic noise, the fixed-coefficient control filter strategy plays a central role in portable ANC implementation. Unlike its traditional adaptive counterpart, the fixed-filter strategy is unable to attain optimal noise reduction for different types of noise. Hence, we propose a selective fixed-filter ANC method based on a simplified two-dimensional convolution neural network (2D CNN), which is implemented on a co-processor (e.g., in a mobile phone), to derive the most suitable control filter for different noise types. To further reduce classification complexity, we designed a lightweight one-dimensional CNN (1D CNN), which can directly classify noise types in time domain. A numerical simulation based on measured paths in headphones demonstrates the proposed algorithm’s efficacy in attenuating real-world non-stationary noise over conventional adaptive algorithms.",multimedia
10.1016/j.csl.2021.101275,preprocessed,Computer Speech and Language,scopus,2022-01-01,sciencedirect,feature learning for efficient asr-free keyword spotting in low-resource languages,https://api.elsevier.com/content/abstract/scopus_id/85113158279,"We consider feature learning for a computationally efficient method of keyword spotting that can be applied in severely under-resourced settings. The objective is to support humanitarian relief programmes by the United Nations (UN) in parts of Africa in which almost no language resources are available. To allow a keyword spotting system to be rapidly developed in such a language, we rely on a small and easily-compiled set of isolated keywords. Using the isolated keywords as templates, we apply dynamic time warping (DTW) to a much larger corpus of in-domain but untranscribed speech. The resulting DTW alignment scores are used to train a convolutional neural network (CNN) which is orders of magnitude more computationally efficient than DTW and therefore suitable for real-time application. We optimise this ASR-free neural network keyword spotting procedure by identifying acoustic features that provide robust performance in this almost zero-resource setting. First, we consider the benefits of incorporating information from well-resourced but unrelated languages by incorporating a multilingual bottleneck feature (BNF) extractor. Next, we consider using features extracted from an autoencoder (AE) trained on in-domain but untranscribed data. Finally, we consider features obtained from a correspondence autoencoder (CAE) which is initialised with the AE and subsequently fine-tuned on the small set of in-domain labelled data. Experiments in South African English and Luganda, a low-resource language, demonstrate that, on their own, both the BNF and CAE features can achieve a 5% relative performance improvement over baseline MFCCs. However, by using BNFs as input to the CAE, even better performance is achieved, resulting in a more than 27% relative improvement over MFCCs in ROC area-under-the-curve (AUC) and more than twice as many top-10 retrievals. We also show that, using these features, the CNN-DTW keyword spotter performs almost as well as the DTW keyword spotter while comfortably outperforming a baseline CNN trained only on the keyword templates. We conclude that a CNN-DTW keyword spotter using BNF-derived CAE features represents a computationally efficient approach with very competitive performance that is suited to rapid deployment in a severely under-resourced scenario.",multimedia
10.1016/j.petrol.2021.109332,preprocessed,Journal of Petroleum Science and Engineering,scopus,2022-01-01,sciencedirect,end-to-end neural network approach to 3d reservoir simulation and adaptation,https://api.elsevier.com/content/abstract/scopus_id/85112357560,"Reservoir simulation and adaptation (also known as history matching) are typically considered as separate problems. While a set of models are aimed at the solution of the forward simulation problem assuming all initial geological parameters are known, the other set of models adjust geological parameters under the fixed forward simulation model to fit production data. This results in many difficulties for both reservoir engineers and developers of new efficient computation schemes. We present a unified approach to reservoir simulation and adaptation problems. A single neural network model allows a forward pass from initial geological parameters of the 3D reservoir model through dynamic state variables to well’s production rates and backward gradient propagation to any model inputs and variables. The model fitting and geological parameters adaptation both become the optimization problem over specific parts of the same neural network model. Standard gradient-based optimization schemes can be used to find the optimal solution. Using real-world oilfield model and historical production rates we demonstrate that the suggested approach allows reservoir simulation and history matching with a benefit of several orders of magnitude simulation speed-up. Finally, to propagate this research we open-source a Python-based framework DeepField that allows standard processing of reservoir models and reproducing the approach presented in this paper.",multimedia
10.1016/j.patcog.2021.108205,preprocessed,Pattern Recognition,scopus,2022-01-01,sciencedirect,tracking more than 100 arbitrary objects at 25 fps through deep learning,https://api.elsevier.com/content/abstract/scopus_id/85111593606,"Most video analytics applications rely on object detectors to localize objects in frames. However, when real-time is a requirement, running the detector at all the frames is usually not possible. This is somewhat circumvented by instantiating visual object trackers between detector calls, but this does not scale with the number of objects. To tackle this problem, we present SiamMT, a new deep learning multiple visual object tracking solution that applies single-object tracking principles to multiple arbitrary objects in real-time. To achieve this, SiamMT reuses feature computations, implements a novel crop-and-resize operator, and defines a new and efficient pairwise similarity operator. SiamMT naturally scales up to several dozens of targets, reaching 25 fps with 122 simultaneous objects for VGA videos, or up to 100 simultaneous objects in HD720 video. SiamMT has been validated on five large real-time benchmarks, achieving leading performance against current state-of-the-art trackers.",multimedia
10.1109/access.2022.3147955,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,how to boost the performance of recommender systems by social trust? studying the challenges and proposing a solution,https://ieeexplore.ieee.org/document/9698036/,"With the increasing number of items in electronic retailers, news websites, etc., finding interesting items concerning the taste of users is becoming more challenging. Recommender Systems (RS) are a well-known solution to this issue. Collaborative filtering (CF) is a widely accepted and popular technique to implement an RS. However, cold-start and data sparsity problems reduce the performance of CF methods. One promising solution for these issues is to use the social trust information. However, how to properly use social trust information is a hot and still open question. In this paper, we propose a similarity measure and a simple link prediction method to address this question and employ them in trust-aware matrix factorization. Especially, our proposed similarity measure is asymmetric to consider the nature of social relationships. Also, to have a more accurate similarity estimation, we have considered both the user’s historical ratings and trust relations, and we have determined the weight of each source. Finally, we have used the item-based model and the level of interest a user’s trustee have for an item to improve the performance of the proposed method for sparse datasets. We conduct extensive performance evaluations in terms of rate prediction and interesting items found. Experimental results on three real-world datasets demonstrate the effectiveness of the proposed method, especially in terms of Mean Absolute Error.",science
10.1109/ccnc49033.2022.9700613,preprocessed,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,first experimental results on real-time cleaning activity monitoring system,https://ieeexplore.ieee.org/document/9700613/,"The COVID-19 pandemic has presented social challenges to establish the new normal lifestyle in our daily lives. The goal of this paper is to enable easy and low-cost monitoring of cleaning activity to keep a clean environment for preventing infection. Although human activity recognition has been a hot research topic in pervasive computing, existing schemes have not been optimized for monitoring cleaning activities. To address this issue, this paper provides an initial concept and preliminary experimental results of cleaning activity recognition using accelerometer data and RFID tags. In the proposed scheme, machine learning technologies and short range wireless communication are employed for recognizing the time and place of wiping as an example of cleaning activities, because it is an important activity for shared places to avoid infection. This paper reports the evaluation results on the recognition accuracy using the proof-of-concept (PoC) implementation to clarify the required sampling rate and time-window size for further experiments. Also, a real-time feedback system is implemented to provide the monitoring results for users. The proposed scheme contributes for efficient monitoring of cleaning activities for creating the new normal era.",science
10.1109/wacv51458.2022.00391,preprocessed,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,unveiling real-life effects of online photo sharing,https://ieeexplore.ieee.org/document/9706741/,"Social networks give free access to their services in exchange for the right to exploit their users’ data. Data sharing is done in an initial context which is chosen by the users. However, data are used by social networks and third parties in different contexts which are often not transparent. In order to unveil such usages, we propose an approach which focuses on the effects of data sharing in impactful real-life situations. Focus is put on visual content because of its strong influence in shaping online user profiles. The approach relies on three components: (1) a set of visual objects with associated situation impact ratings obtained by crowdsourcing, (2) a corresponding set of object detectors for mining users’ photos and (3) a ground truth dataset made of 500 visual user profiles which are manually rated per situation. These components are combined in LERV UP, a method which learns to rate visual user profiles in each situation. LERV UP exploits a new image descriptor which aggregates object ratings and object detections at user level and an attention mechanism which boosts highly-rated objects to prevent them from being overwhelmed by low-rated ones. Performance is evaluated per situation by measuring the correlation between the automatic ranking of profile ratings and a manual ground truth. Results indicate that LERV UP is effective since a strong correlation of the two rankings is obtained. A practical implementation of the approach in a mobile app which raises user awareness about shared data usage is also discussed.",science
10.1109/access.2021.3137636,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,a deep learning-based framework for phishing website detection,https://ieeexplore.ieee.org/document/9661323/,"Phishing attackers spread phishing links through e-mail, text messages, and social media platforms. They use social engineering skills to trick users into visiting phishing websites and entering crucial personal information. In the end, the stolen personal information is used to defraud the trust of regular websites or financial institutions to obtain illegal benefits. With the development and applications of machine learning technology, many machine learning-based solutions for detecting phishing have been proposed. Some solutions are based on the features extracted by rules, and some of the features need to rely on third-party services, which will cause instability and time-consuming issues in the prediction service. In this paper, we propose a deep learning-based framework for detecting phishing websites. We have implemented the framework as a browser plug-in capable of determining whether there is a phishing risk in real-time when the user visits a web page and gives a warning message. The real-time prediction service combines multiple strategies to improve accuracy, reduce false alarm rates, and reduce calculation time, including whitelist filtering, blacklist interception, and machine learning (ML) prediction. In the ML prediction module, we compared multiple machine learning models using several datasets. From the experimental results, the RNN-GRU model obtained the highest accuracy of 99.18%, demonstrating the feasibility of the proposed solution.",science
10.1109/access.2021.3140175,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"a metaverse: taxonomy, components, applications, and open challenges",https://ieeexplore.ieee.org/document/9667507/,"Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to connectivity with reality using virtual currency. The integration of enhanced social activities and neural-net methods requires a new definition of Metaverse suitable for the present, different from the previous Metaverse. This paper divides the concepts and essential techniques necessary for realizing the Metaverse into three components (i.e., hardware, software, and contents) and three approaches (i.e., user interaction, implementation, and application) rather than marketing or hardware approach to conduct a comprehensive analysis. Furthermore, we describe essential methods based on three components and techniques to Metaverse’s representative Ready Player One, Roblox, and Facebook research in the domain of films, games, and studies. Finally, we summarize the limitations and directions for implementing the immersive Metaverse as social influences, constraints, and open challenges.",science
10.1109/tc.2021.3057082,preprocessed,IEEE Transactions on Computers,IEEE,2022-03-01 00:00:00,ieeexplore,<sc>lime</sc>: low-cost and incremental learning for dynamic heterogeneous information networks,https://ieeexplore.ieee.org/document/9353275/,"Understanding the interconnected relationships of large-scale information networks like social, scholar and Internet of Things networks is vital for tasks like recommendation and fraud detection. The vast majority of the real-world networks are inherently heterogeneous and dynamic, containing many different types of nodes and edges and can change drastically over time. The dynamicity and heterogeneity make it extremely challenging to reason about the network structure. Unfortunately, existing approaches are inadequate in modeling real-life dynamical networks as they either have strong assumption of a given stochastic process or fail to capture the heterogeneity of network structure, and they all require extensive computational resources. We introduce <sc>Lime</sc>, a better approach for modeling dynamic and heterogeneous information networks. <sc>Lime</sc> is designed to extract high-quality network representation with significantly lower memory resources and computational time over the state-of-the-arts. Unlike prior work that uses a vector to encode each network node, we exploit the semantic relationships among network nodes to encode multiple nodes with similar semantics in shared vectors. By using many fewer node vectors, our approach significantly reduces the required memory space for encoding large-scale networks. To effectively trade information sharing for reduced memory footprint, we employ the recursive neural network (RsNN) with carefully designed optimization strategies to explore the node semantics in a novel cuboid space. We then go further by showing, for the first time, how an effective incremental learning approach can be developed – with the help of RsNN, our cuboid structure, and a set of novel optimization techniques – to allow a learning framework to quickly and efficiently adapt to a constantly evolving network. We evaluate <sc>Lime</sc> by applying it to three representative network-based tasks, node classification, node clustering and anomaly detection, performing on three large-scale datasets. We compare <sc>Lime</sc> against eleven prior state-of-the-art approaches for learning network representation. Our extensive experiments demonstrate that <sc>Lime</sc> not only reduces the memory footprint by over 80 percent and the processing time over 2x when learning network representation but also delivers comparable performance for downstream processing tasks. We show that our incremental learning method can boost the learning time by up to 20x without compromising the quality of the learned network representation.",science
10.1109/tifs.2021.3131026,preprocessed,IEEE Transactions on Information Forensics and Security,IEEE,2000-01-01 00:00:00,ieeexplore,poligraph: intrusion-tolerant and distributed fake news detection system,https://ieeexplore.ieee.org/document/9627681/,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally (<inline-formula> <tex-math notation=""LaTeX"">${4\%}$ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">${7\%}$ </tex-math></inline-formula>) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach.",science
10.1109/tii.2021.3117861,preprocessed,IEEE Transactions on Industrial Informatics,IEEE,2022-05-01 00:00:00,ieeexplore,toward fairness-aware time-sensitive asynchronous federated learning for critical energy infrastructure,https://ieeexplore.ieee.org/document/9560090/,"Critical energy infrastructure (CEI) systems are vital to underpin the national economy and social development, but vulnerable to cyber attack and data privacy leakage when distributed machine learning technologies are deployed on them. Although federated learning (FL) has promoted distributed collaborative learning while keeping natural compliance with the privacy protection, it is tremendously difficult to schedule edge nodes of CEI collaboratively when asynchronous FL tasks are applied in CEI system, since the CEI system must make an irrevocable immediate decision on whether to hire a participant who arrives and departs dynamically without knowing future information. In this article, we tackle this issue by designing fairness-aware and time-sensitive task allocation mechanisms in asynchronous FL for CEI. First, we design an optimal multidimensional contract to guarantee the reliability, honesty, and fairness, and maximize the learning accuracy for the fixed deadline scenario. Second, we design a multimetric participant recruitment mechanism to control time consumption for the limited budget scenario, prove that the problem of optimizing this mechanism is NP-hard, and propose an <inline-formula><tex-math notation=""LaTeX"">$e$</tex-math></inline-formula>-approximation algorithm accordingly. Finally, extensive experiments using both real-world data and simulated data further demonstrate the effectiveness and efficiency of our proposed mechanisms compared to the state-of-the-art approaches.",science
10.1109/tgrs.2021.3087186,preprocessed,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2000-01-01 00:00:00,ieeexplore,3-d gabor convolutional neural network for hyperspectral image classification,https://ieeexplore.ieee.org/document/9460777/,"Due to the detailed spectral information through hundreds of narrow spectral bands provided by hyperspectral image (HSI) data, it can be employed to accurately classify diverse materials of interest, which is one of the core applications of hyperspectral remote sensing technology. In recent years, with the rapid development of deep learning, convolutional neural networks (CNNs) have been successfully applied in many fields, including HSI classification. However, the random gradient descent-based parameter updating scheme is too general and leading to the inefficiency of CNN models. Moreover, the high dimensionality and limited training samples of HSI data also exacerbate the overfitting problem. To tackle these issues, in this article, a novel deep network with multilayer and multibranch architecture, named 3-D Gabor CNN (3DG-CNN), is proposed for HSI classification. More precisely, since the predefined 3-D Gabor filters in multiple scales and orientations could well characterize the internal spatial–spectral structure of HSI data from various perspectives, the 3-D Gabor-modulated kernels (3-D GMKs) are employed to replace the random initialization kernels. Moreover, the specially designed multibranch architecture enables the network to better integrating the scalable property of 3-D Gabor filters; thus, the representative ability and robustness of the extracted features can be greatly improved. Alternatively, the number of network parameters is substantially reduced due to the incorporation of 3-D Gabor modulation, relieving the training complexity and also alleviating the training process from overfitting. Experimental results on four real HSI datasets (including two newly released ones in the literature) have demonstrated that the proposed 3DG-CNN model can achieve better performance than several widely used machine-learning-based and deep-learning-based approaches. For the sake of reproducibility, the codes of the proposed 3DG-CNN model are available at <uri>http://jiasen.tech/papers/</uri>.",science
10.1109/tcbb.2021.3082915,preprocessed,IEEE/ACM Transactions on Computational Biology and Bioinformatics,IEEE,2022-02-01 00:00:00,ieeexplore,graphplas: refined classification of plasmid sequences using assembly graphs,https://ieeexplore.ieee.org/document/9439922/,"Plasmids are extra-chromosomal genetic materials with important markers that affect the function and behaviour of the microorganisms supporting their environmental adaptations. Hence the identification and recovery of such plasmid sequences from assemblies is a crucial task in metagenomics analysis. In the past, machine learning approaches have been developed to separate chromosomes and plasmids. However, there is always a compromise between precision and recall in the existing classification approaches. The similarity of compositions between chromosomes and their plasmids makes it difficult to separate plasmids and chromosomes with high accuracy. However, high confidence classifications are accurate with a significant compromise of recall, and vice versa. Hence, the requirement exists to have more sophisticated approaches to separate plasmids and chromosomes accurately while retaining an acceptable trade-off between precision and recall. We present GraphPlas, a novel approach for plasmid recovery using coverage, composition and assembly graph topology. We evaluated GraphPlas on simulated and real short read assemblies with varying compositions of plasmids and chromosomes. Our experiments show that GraphPlas is able to significantly improve accuracy in detecting plasmid and chromosomal contigs on top of popular state-of-the-art plasmid detection tools. <p>The source code is freely available at: <uri>https://github.com/anuradhawick/GraphPlas</uri>.</p>",science
10.1109/lra.2022.3140793,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,vision-based self-adaptive gripping in a trimodal robotic sorting end-effector,https://ieeexplore.ieee.org/document/9672720/,"Recyclable waste management, which includes sorting as a key process, is a crucial component of maintaining a sustainable ecosystem. The use of robots in sorting could significantly facilitate the production of secondary raw materials from waste in the sense of a recycling economy. However, due to the complex and heterogeneous types of the recyclable items, the conventional robotic gripping end-effectors, which typically come with a fixed structure, are unlikely to hold onto the full range of items to enable separation and recycling. To this end, a trimodal adaptive end-effector is proposed that can be integrated with robotic manipulators to improve their gripping versatility. The end-effector can deploy effective modes of gripping to different objects in response to their size and porosity via gripping mechanisms based on Nano Polyurethane (PU) adhesive gels, pumpless vacuum suction, and radially deployable claws. While the end-effector's mechanical design allows the three gripping modes to be deployed independently or in conjunction with one another, this work aims at deploying modes that are effective for gripping onto the recyclable item. In order to decide on the suitable modes of gripping a real-time vision system is designed to measure the size and porosity of the recyclable items and advise on a suitable combination of gripping modes to be deployed. Integrated current sensors provide an indication of successful gripping and releasing of the recyclable items. The results of the experiments confirmed the ability of our vision-based approach in identifying suitable gripping modes in real-time, the deployment of the relevant mechanisms and successful gripping onto a maximum of 84.8% (single-mode), 90.9% (dual-mode) and 96.9% (triple-mode) of a specified set of recyclable items.",science
http://arxiv.org/abs/2202.10335v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,explainability in machine learning: a pedagogical perspective,http://arxiv.org/abs/2202.10335v1,"Given the importance of integrating of explainability into machine learning,
at present, there are a lack of pedagogical resources exploring this.
Specifically, we have found a need for resources in explaining how one can
teach the advantages of explainability in machine learning. Often pedagogical
approaches in the field of machine learning focus on getting students prepared
to apply various models in the real world setting, but much less attention is
given to teaching students the various techniques one could employ to explain a
model's decision-making process. Furthermore, explainability can benefit from a
narrative structure that aids one in understanding which techniques are
governed by which questions about the data.
  We provide a pedagogical perspective on how to structure the learning process
to better impart knowledge to students and researchers in machine learning,
when and how to implement various explainability techniques as well as how to
interpret the results. We discuss a system of teaching explainability in
machine learning, by exploring the advantages and disadvantages of various
opaque and transparent machine learning models, as well as when to utilize
specific explainability techniques and the various frameworks used to structure
the tools for explainability. Among discussing concrete assignments, we will
also discuss ways to structure potential assignments to best help students
learn to use explainability as a tool alongside any given machine learning
application.
  Data science professionals completing the course will have a birds-eye view
of a rapidly developing area and will be confident to deploy machine learning
more widely. A preliminary analysis on the effectiveness of a recently
delivered course following the structure presented here is included as evidence
supporting our pedagogical approach.",science
http://arxiv.org/abs/2202.10144v1,preprocessed,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"inferring network structure with unobservable nodes from time series
  data",http://arxiv.org/abs/2202.10144v1,"Network structures play important roles in social, technological and
biological systems. However, the observable nodes and connections in real cases
are often incomplete or unavailable due to measurement errors, private
protection issues, or other problems. Therefore, inferring the complete network
structure is useful for understanding human interactions and complex dynamics.
The existing studies have not fully solved the problem of inferring network
structure with partial information about connections or nodes. In this paper,
we tackle the problem by utilizing time-series data generated by network
dynamics. We regard the network inference problem based on dynamical time
series data as a problem of minimizing errors for predicting states of
observable nodes and proposed a novel data-driven deep learning model called
Gumbel-softmax Inference for Network (GIN) to solve the problem under
incomplete information. The GIN framework includes three modules: a dynamics
learner, a network generator, and an initial state generator to infer the
unobservable parts of the network. We implement experiments on artificial and
empirical social networks with discrete and continuous dynamics. The
experiments show that our method can infer the unknown parts of the structure
and the initial states of the observable nodes with up to 90\% accuracy. The
accuracy declines linearly with the increase of the fractions of unobservable
nodes. Our framework may have wide applications where the network structure is
hard to obtain and the time series data is rich.",science
http://arxiv.org/abs/2202.08450v1,preprocessed,arxiv,arxiv,2022-02-17 00:00:00,arxiv,"design-bench: benchmarks for data-driven offline model-based
  optimization",http://arxiv.org/abs/2202.08450v1,"Black-box model-based optimization (MBO) problems, where the goal is to find
a design input that maximizes an unknown objective function, are ubiquitous in
a wide range of domains, such as the design of proteins, DNA sequences,
aircraft, and robots. Solving model-based optimization problems typically
requires actively querying the unknown objective function on design proposals,
which means physically building the candidate molecule, aircraft, or robot,
testing it, and storing the result. This process can be expensive and time
consuming, and one might instead prefer to optimize for the best design using
only the data one already has. This setting -- called offline MBO -- poses
substantial and different algorithmic challenges than more commonly studied
online techniques. A number of recent works have demonstrated success with
offline MBO for high-dimensional optimization problems using high-capacity deep
neural networks. However, the lack of standardized benchmarks in this emerging
field is making progress difficult to track. To address this, we present
Design-Bench, a benchmark for offline MBO with a unified evaluation protocol
and reference implementations of recent methods. Our benchmark includes a suite
of diverse and realistic tasks derived from real-world optimization problems in
biology, materials science, and robotics that present distinct challenges for
offline MBO. Our benchmark and reference implementations are released at
github.com/rail-berkeley/design-bench and
github.com/rail-berkeley/design-baselines.",science
http://arxiv.org/abs/2202.07785v1,preprocessed,arxiv,arxiv,2022-02-15 00:00:00,arxiv,predictability and surprise in large generative models,http://arxiv.org/abs/2202.07785v1,"Large-scale pre-training has recently emerged as a technique for creating
capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG,
Gopher, and many others. In this paper, we highlight a counterintuitive
property of such models and discuss the policy implications of this property.
Namely, these generative models have an unusual combination of predictable loss
on a broad training distribution (as embodied in their ""scaling laws""), and
unpredictable specific capabilities, inputs, and outputs. We believe that the
high-level predictability and appearance of useful capabilities drives rapid
development of such models, while the unpredictable qualities make it difficult
to anticipate the consequences of model deployment. We go through examples of
how this combination can lead to socially harmful behavior with examples from
the literature and real world observations, and we also perform two novel
experiments to illustrate our point about harms from unpredictability.
Furthermore, we analyze how these conflicting properties combine to give model
developers various motivations for deploying these models, and challenges that
can hinder deployment. We conclude with a list of possible interventions the AI
community may take to increase the chance of these models having a beneficial
impact. We intend this paper to be useful to policymakers who want to
understand and regulate AI systems, technologists who care about the potential
policy impact of their work, and academics who want to analyze, critique, and
potentially develop large generative models.",science
http://arxiv.org/abs/2202.07475v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"a real-time system for detecting landslide reports on social media using
  artificial intelligence",http://arxiv.org/abs/2202.07475v1,"This paper presents an online system that leverages social media data in real
time to identify landslide-related information automatically using
state-of-the-art artificial intelligence techniques. The designed system can
(i) reduce the information overload by eliminating duplicate and irrelevant
content, (ii) identify landslide images, (iii) infer geolocation of the images,
and (iv) categorize the user type (organization or person) of the account
sharing the information. The system was deployed in February 2020 online at
https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter
data stream and has been running continuously since then to provide
time-critical information to partners such as British Geological Survey and
European Mediterranean Seismological Centre. We trust this system can both
contribute to harvesting of global landslide data for further research and
support global landslide maps to facilitate emergency response and decision
making.",science
http://arxiv.org/abs/2202.00617v1,preprocessed,arxiv,arxiv,2022-02-01 00:00:00,arxiv,"a general, evolution-inspired reward function for social robotics",http://arxiv.org/abs/2202.00617v1,"The field of social robotics will likely need to depart from a paradigm of
designed behaviours and imitation learning and adopt modern reinforcement
learning (RL) methods to enable robots to interact fluidly and efficaciously
with humans. In this paper, we present the Social Reward Function as a
mechanism to provide (1) a real-time, dense reward function necessary for the
deployment of RL agents in social robotics, and (2) a standardised objective
metric for comparing the efficacy of different social robots. The Social Reward
Function is designed to closely mimic those genetically endowed social
perception capabilities of humans in an effort to provide a simple, stable and
culture-agnostic reward function. Presently, datasets used in social robotics
are either small or significantly out-of-domain with respect to social
robotics. The use of the Social Reward Function will allow larger in-domain
datasets to be collected close to the behaviour policy of social robots, which
will allow both further improvements to reward functions and to the behaviour
policies of social robots. We believe this will be the key enabler to
developing efficacious social robots in the future.",science
http://arxiv.org/abs/2201.08475v1,preprocessed,arxiv,arxiv,2022-01-20 00:00:00,arxiv,gengnn: a generic fpga framework for graph neural network acceleration,http://arxiv.org/abs/2201.08475v1,"Graph neural networks (GNNs) have recently exploded in popularity thanks to
their broad applicability to ubiquitous graph-related problems such as quantum
chemistry, drug discovery, and high energy physics. However, meeting demand for
novel GNN models and fast inference simultaneously is challenging because of
the gap between the difficulty in developing efficient FPGA accelerators and
the rapid pace of creation of new GNN models. Prior art focuses on the
acceleration of specific classes of GNNs but lacks the generality to work
across existing models or to extend to new and emerging GNN models. In this
work, we propose a generic GNN acceleration framework using High-Level
Synthesis (HLS), named GenGNN, with two-fold goals. First, we aim to deliver
ultra-fast GNN inference without any graph pre-processing for real-time
requirements. Second, we aim to support a diverse set of GNN models with the
extensibility to flexibly adapt to new models. The framework features an
optimized message-passing structure applicable to all models, combined with a
rich library of model-specific components. We verify our implementation
on-board on the Xilinx Alveo U50 FPGA and observe a speed-up of up to 25x
against CPU (6226R) baseline and 13x against GPU (A6000) baseline. Our HLS code
will be open-source on GitHub upon acceptance.",science
http://arxiv.org/abs/2201.03413v1,preprocessed,arxiv,arxiv,2022-01-10 00:00:00,arxiv,systems challenges for trustworthy embodied systems,http://arxiv.org/abs/2201.03413v1,"A new generation of increasingly autonomous and self-learning systems, which
we call embodied systems, is about to be developed. When deploying these
systems into a real-life context we face various engineering challenges, as it
is crucial to coordinate the behavior of embodied systems in a beneficial
manner, ensure their compatibility with our human-centered social values, and
design verifiably safe and reliable human-machine interaction. We are arguing
that raditional systems engineering is coming to a climacteric from embedded to
embodied systems, and with assuring the trustworthiness of dynamic federations
of situationally aware, intent-driven, explorative, ever-evolving, largely
non-predictable, and increasingly autonomous embodied systems in uncertain,
complex, and unpredictable real-world contexts. We are also identifying a
number of urgent systems challenges for trustworthy embodied systems, including
robust and human-centric AI, cognitive architectures, uncertainty
quantification, trustworthy self-integration, and continual analysis and
assurance.",science
http://arxiv.org/abs/2201.03550v1,preprocessed,arxiv,arxiv,2022-01-09 00:00:00,arxiv,"machine learning enabling high-throughput and remote operations at
  large-scale user facilities",http://arxiv.org/abs/2201.03550v1,"Imaging, scattering, and spectroscopy are fundamental in understanding and
discovering new functional materials. Contemporary innovations in automation
and experimental techniques have led to these measurements being performed much
faster and with higher resolution, thus producing vast amounts of data for
analysis. These innovations are particularly pronounced at user facilities and
synchrotron light sources. Machine learning (ML) methods are regularly
developed to process and interpret large datasets in real-time with
measurements. However, there remain conceptual barriers to entry for the
facility general user community, whom often lack expertise in ML, and technical
barriers for deploying ML models. Herein, we demonstrate a variety of
archetypal ML models for on-the-fly analysis at multiple beamlines at the
National Synchrotron Light Source II (NSLS-II). We describe these examples
instructively, with a focus on integrating the models into existing
experimental workflows, such that the reader can easily include their own ML
techniques into experiments at NSLS-II or facilities with a common
infrastructure. The framework presented here shows how with little effort,
diverse ML models operate in conjunction with feedback loops via integration
into the existing Bluesky Suite for experimental orchestration and data
management.",science
http://arxiv.org/abs/2201.02734v1,preprocessed,arxiv,arxiv,2022-01-02 00:00:00,arxiv,building human-like communicative intelligence: a grounded perspective,http://arxiv.org/abs/2201.02734v1,"Modern Artificial Intelligence (AI) systems excel at diverse tasks, from
image classification to strategy games, even outperforming humans in many of
these domains. After making astounding progress in language learning in the
recent decade, AI systems, however, seem to approach the ceiling that does not
reflect important aspects of human communicative capacities. Unlike human
learners, communicative AI systems often fail to systematically generalize to
new data, suffer from sample inefficiency, fail to capture common-sense
semantic knowledge, and do not translate to real-world communicative
situations. Cognitive Science offers several insights on how AI could move
forward from this point. This paper aims to: (1) suggest that the dominant
cognitively-inspired AI directions, based on nativist and symbolic paradigms,
lack necessary substantiation and concreteness to guide progress in modern AI,
and (2) articulate an alternative, ""grounded"", perspective on AI advancement,
inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research.
I review results on 4E research lines in Cognitive Science to distinguish the
main aspects of naturalistic learning conditions that play causal roles for
human language development. I then use this analysis to propose a list of
concrete, implementable components for building ""grounded"" linguistic
intelligence. These components include embodying machines in a
perception-action cycle, equipping agents with active exploration mechanisms so
they can build their own curriculum, allowing agents to gradually develop motor
abilities to promote piecemeal language development, and endowing the agents
with adaptive feedback from their physical and social environment. I hope that
these ideas can direct AI research towards building machines that develop
human-like language abilities through their experiences with the world.",science
10.1016/j.engstruct.2021.113824,preprocessed,Engineering Structures,scopus,2022-02-15,sciencedirect,"explainable machine learning using real, synthetic and augmented fire tests to predict fire resistance and spalling of rc columns",https://api.elsevier.com/content/abstract/scopus_id/85122261194,"This paper presents the development of systematic machine learning (ML) approach to enable explainable and rapid assessment of fire resistance and fire-induced spalling of reinforced concrete (RC) columns. The developed approach comprises an ensemble of three novel ML algorithms namely; random forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL). These algorithms are trained to account for a wide collection of geometric characteristics and material properties, as well as loading conditions to examine fire performance of normal and high strength RC columns by analyzing a comprehensive database of fire tests comprising of over 494 observations. The developed ensemble is also capable of presenting quantifiable insights to ML predictions; thus, breaking free from the notion of “black-box” ML and establishing a solid step towards transparent and explainable ML. Most importantly, this work tackles the scarcity of available fire tests by proposing new techniques to leverage the use of real, synthetic, and augmented fire test observations. The developed ML ensemble has been calibrated and validated for standard and design fire exposures and one-, two-, three- and four-sided fire exposures thus; covering a wide range of practical scenarios present during fire incidents. When fully deployed, the developed ensemble can analyze over 5,000 RC columns in under 60 s; thus, providing an attractive solution for researchers and practitioners. The presented approach can also be easily extended for evaluating fire resistance and spalling of other structural members under varying fire scenarios and loading conditions and hence paves the way to modernize the state of this research area and practice.",science
10.1016/j.comcom.2021.11.011,preprocessed,Computer Communications,scopus,2022-02-01,sciencedirect,ran energy efficiency and failure rate through ann traffic predictions processing,https://api.elsevier.com/content/abstract/scopus_id/85120657977,"In this paper, we focus on the application of ML tools to resource management in a portion of a Radio Access Network (RAN) and, in particular, to Base Station (BS) activation and deactivation, aiming at reducing energy consumption while providing enough capacity to satisfy the variable traffic demand generated by end users. In order to properly decide on BS (de)activation, traffic predictions are needed, and Artificial Neural Networks (ANN) are used for this purpose. Since critical BS (de)activation decisions are not taken in proximity of minima and maxima of the traffic patterns, high accuracy in the traffic estimation is not required at those times, but only close to the times when a decision is taken. This calls for careful processing of the ANN traffic predictions to increase the probability of correct decision. Numerical performance results in terms of energy saving and traffic lost due to incorrect BS deactivations are obtained by simulating algorithms for traffic predictions processing, using real traffic as input. Results suggest that good performance trade-offs can be achieved even in presence of non-negligible traffic prediction errors, if these forecasts are properly processed. The impact of forecast processing for dynamic resource allocation on the BS failure rate is also investigated. Results reveal that conservative approaches better prevent BSs from hardware failure. Nevertheless, the deployment of newer devices, designed for fast dynamic networks, allows the adoption of approaches which frequently activate and deactivate BSs, thus achieving higher energy saving.",science
10.1016/j.simpat.2021.102446,preprocessed,Simulation Modelling Practice and Theory,scopus,2022-02-01,sciencedirect,modelling argumentation in short text: a case of social media debate,https://api.elsevier.com/content/abstract/scopus_id/85120649661,"The technological leaps of artificial intelligence (AI) and the rise of machine learning have triggered significant progress in a plethora of natural language processing (NLP) and natural language understanding tasks. One of these tasks is argumentation mining which has received significant interest in recent years and is regarded as a key domain for future decision-making systems, behaviour modelling, and natural language understanding problems. Until recently, natural language modelling tasks, such as computational argumentation schemes, were often tested in controlled environments, such as persuasive essays, reducing unexpected behaviours that could occur in real-life settings, like a public debate on social media. Additionally, the growing demand for enhancing the trust and the explainability of the AI services has dictated the design and adoption of modelling schemes to increase the confidence in the outcomes of the AI solutions. This paper attempts to explore modelling argumentation in short text and proposes a novel framework for argumentation detection under the name Abstract Framework for Argumentation Detection (AFAD). Moreover, different proof-of-concept implementations are provided to examine the applicability of the proposed framework to very short text developing a rule-based mechanism and compare the results with data-driven solutions. Eventually, a combination of the deployed methods is applied increasing the correct predictions in the minority class on an imbalanced dataset. The findings suggest that the modelling process provides solid grounds for technical research while the hybrid solutions have the potential to be applied to a wide range of NLP-related tasks offering a deeper understanding of human language and reasoning.",science
10.1016/j.apenergy.2021.118085,preprocessed,Applied Energy,scopus,2022-02-01,sciencedirect,ship energy management system development and experimental evaluation utilizing marine loading cycles based on machine learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85120648001,"In order to develop energy management systems for hybrid ship propulsion plants that are truly optimal and robust, it is important that the test conditions in experimental facilities are as close as possible to real world applications. In this context, a framework for the design and experimental evaluation of power-split control systems for ship propulsion is proposed. Using machine learning, data from ship operation are processed and 20 loading patterns are recognized; representative templates are extracted to be used as marine loading cycles in the energy management system development and testing. A ship propulsion model with wave disturbance is utilized to simulate realistic loading scenarios on the experimental facility. A predictive energy management system is presented, that controls the diesel engine and the electric motor/generator based on a strategy that defines the trade-off between fuel consumption and NOx emissions minimization. In addition the propeller load characteristics that are estimated and a speed predictor are utilized to aid the optimization within the 10 s prediction time window. A parametric simulation study is performed for the trade-off evaluation between fuel consumption and NOx emissions reduction potential of the control scheme. Finally, utilizing an extracted loading cycle, the energy management system is experimentally implemented and tested in real-time operation, where it has to cope with environmental disturbance rejection and follow the desired speed profile while performing the power-split control in respect to the fuel to NOx weighting strategy. Based on the experimental results in a hybrid diesel–electric marine powertrain with a 260 kW diesel engine and a 90 kW electric machine, fuel consumption and NOx emissions reduction by 6% and 8.5% respectively, were achieved over the tested profile. In this framework, the capabilities of the energy management system in realistic operation conditions can be exploited and evaluated.",science
10.1016/j.jwpe.2021.102452,preprocessed,Journal of Water Process Engineering,scopus,2022-02-01,sciencedirect,polyamine-modified polyacrylonitrile fibers for efficient removal of u(vi) from real fluorine-contained low-level radioactive wastewater,https://api.elsevier.com/content/abstract/scopus_id/85119972768,"It is of great significance to develop an adsorbent with high adsorption capacity and excellent resistance to anion and cation interference toward the removal of U(VI). Herein, a novel polyamine-modified polyacrylonitrile-based fiber (PANPA) has been synthesized through hydrothermal method, which can validly remove U(VI) from solution. Combined with mesoscopic, spectral characterization and simulation method, the removal behavior and mechanism of U(VI) from high fluorine uranium-containing wastewater by PANPA are systematically investigated. The results show that, based on the strong coordination principle of polyamine group and UO2
                     2+, PANPA can selectively remove U(VI) from wastewater. In addition, the q
                     
                        max
                      of 459.27 mg g−1 was more than that of many other adsorbent materials. More importantly, PANPA is not affected by high concentration of F−, and exhibits higher distribution coefficient (559,900 mL g−1) and removal efficiency (99.5%) to U(VI) than other coexisting ions in real wastewater. Furthermore, the column experiment was also implemented to remove U(VI). The results indicate that PANPA is a promising material to effectively remove U(VI) from real wastewater produced during the fabrication of nuclear fuel elements.",science
10.1016/j.saa.2021.120347,preprocessed,Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy,scopus,2022-01-15,sciencedirect,rapid discrimination of curcuma longa and curcuma xanthorrhiza using direct analysis in real time mass spectrometry and near infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115004546,"This study describes a newly developed method for the fast and straightforward differentiation of two turmeric species using Direct Analysis in Real Time mass spectrometry and miniaturized Near Infrared spectroscopy. Multivariate analyses (PCA and LDA) were performed on the mass spectrometric data, thus creating a powerful model for the discrimination of Curcumalonga and Curcumaxanthorrhiza. Cross-validation of the model revealed correctness-scores of 100% with 20-fold as well as leave-one-out validation techniques. To further estimate the models prediction power, seven retail samples of turmeric powder were analyzed and assorted to a species. Looking for a fast, non-invasive, cost-efficient and laboratory independent method, miniaturized NIR spectrometers offer an alternative for quality control of turmeric species. However, different technologies implemented to compensate for their small size, lead to different applicability of these spectrometers. Therefore, we investigated the three handheld spectrometers microPHAZIR, MicroNIR 2200 and MicroNIR 1700ES for their application in spice analysis in hyphenation to PCA, LDA and ANN methods used for the discriminant analysis. While microPHAZIR proved to be the most valuable device for differentiating C.longa and C.xanthorrhiza, MicroNIR 1700ES offered the worst results. These findings are interpreted on the basis of a quantum chemical simulation of the NIR spectrum of curcumin as the representative constituent. It was found that the information accessible to MicroNIR 1700ES that is relevant to the analyzed constituents is located in the spectral region prone to interferences with the matrix, likely limiting the performance of this spectrometer in this analytical scenario.",science
10.1016/j.dss.2021.113665,preprocessed,Decision Support Systems,scopus,2022-01-01,sciencedirect,a constraint programming model for making recommendations in personal process management: a design science research approach,https://api.elsevier.com/content/abstract/scopus_id/85115634506,"Decision-making in everyday life has an essential role in effectively completing personal tasks and processes. The complexity of these processes and the resulting cognitive load of managing them may vary significantly. To decrease the cognitive load created by such decision-making efforts and to obtain better outcomes, recommendation systems carry significant potential. In order to investigate the benefits provided by decision support systems (DSS) in personal process management (PPM), we first build a constraint programming (CP) model and a prototype context-aware-mobile application employing this CP model. Then, we evaluate the application and the model via two exemplary real-world scenarios. The scenarios form the core of the experiments conducted with 50 participants. We compare the participants’ planning performances with and without the PPM system with quantitative metrics such as planning times and scenario objective values. In addition, System Usability Scale (SUS) questionnaires and open-ended questions provide qualitative evaluation results. Throughout the study, we apply the Design Science Research methodology to rigorously conduct research activities by proof of concept, proof of use, and proof of value. The empirical results clearly show that our proposed model for PPM is effective, and the developed prototype solution generates positive participant comments as well as a high SUS score. Overall, the prototype PPM system with CP implementation leads to better planning in less time in the planning phase, and it lets the user do fast replanning in the execution phase, which is invaluable in dynamically changing situations such as daily activities.",science
10.1016/j.postharvbio.2021.111741,preprocessed,Postharvest Biology and Technology,scopus,2022-01-01,sciencedirect,multi-output 1-dimensional convolutional neural networks for simultaneous prediction of different traits of fruit based on near-infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115232057,"In spectral data predictive modelling of fresh fruit, often the models are calibrated to predict multiple responses. A common method to deal with such a multi-response predictive modelling is the partial least-squares (PLS2) regression. Recently, deep learning (DL) has shown to outperform partial least-squares (PLS) approaches for single fruit traits prediction. The DL can also be adapted to perform multi-response modelling. This study presents an implementation of DL modelling for multi-response prediction for spectral data of fresh fruit. To show this, a real NIR data set related to SSC and MC measurements in pear fruit was used. Since DL models perform better with larger data sets, a data augmentation procedure was performed prior to data modelling. Furthermore, a comparative study was also performed between two of the most used DL architectures for spectral analysis, their multi-output and single-output variants and a classic baseline model using PLS2. A key point to note that all the DL modelling presented in this study is performed using novel automated optimisation tools such as Bayesian optimisation and Hyperband. The results showed that DL models can be easily adapted by changing the output of the fully connected layers to perform multi-response modelling. In comparison to the PLS2, the multi-response DL model showed ∼13 % lower root mean squared error (RMSE), showing the ease and superiority of handling multi-response by DL models for spectral calibration.",science
10.1109/tie.2021.3090707,preprocessed,IEEE Transactions on Industrial Electronics,IEEE,2022-06-01 00:00:00,ieeexplore,active object detection based on a novel deep q-learning network and long-term learning strategy for the service robot,https://ieeexplore.ieee.org/document/9464751/,"This article focuses on active object detection (AOD), one of the greatest challenges in the robotics field. A novel deep-Q-learning-network-based approach is proposed to utilize more useful status information for enhancing the training efficiency and testing accuracy of AOD by adding the cropped target object (TGOJ) from the current state as a new input. Different from the existing researches, a novel reward function, combing the area factor and distance factor of the bounding box, is designed to make the robot not only get closer to the TGOJ but also obtain a better observation viewpoint. Moreover, to overcome the differences between the training dataset and new environments as well as improving the adaptation of the AOD model, a reward-based long-term learning strategy including a novel training strategy is presented. The comparable experiments and the ablation study have been implemented in an AOD dataset, proving that our method owns better performance and efficiency than the comparable methods. Meanwhile, the experiments in the real-world scenario with a robot indicate the validity of the proposed method.",robotics
10.1109/lra.2022.3146515,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,kineverse: a symbolic articulation model framework for model-agnostic mobile manipulation,https://ieeexplore.ieee.org/document/9695204/,"Service robots in the future need to execute abstract instructions such as “fetch the milk from the fridge”. To translate such instructions into actionable plans, robots require in-depth background knowledge. With regards to interactions with doors and drawers, robots require articulation models that they can use for state estimation and motion planning. Existing frameworks model articulated connections as abstract concepts such as <italic>prismatic</italic>, or <italic>revolute</italic>, but do not provide a parameterized model of these connections for computation. In this letter, we introduce a novel framework that uses symbolic mathematical expressions to model articulated structures – robots and objects alike – in a unified and extensible manner. We provide a theoretical description of this framework, and the operations that are supported by its models, and introduce an architecture to exchange our models in robotic applications, making them as flexible as any other environmental observation. To demonstrate the utility of our approach, we employ our practical implementation <italic>Kineverse</italic> for solving common robotics tasks from state estimation and mobile manipulation, and use it further in real-world mobile robot manipulation.",robotics
10.1109/lra.2022.3146945,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,"tacto: a fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors",https://ieeexplore.ieee.org/document/9697425/,"Simulators perform an important role in prototyping, debugging, and benchmarking new advances in robotics and learning for control. Although many physics engines exist, some aspects of the real world are harder than others to simulate. One of the aspects that have so far eluded accurate simulation is touch sensing. To address this gap, we present TACTO – a fast, flexible, and open-source simulator for vision-based tactile sensors. This simulator allows to render realistic high-resolution touch readings at hundreds of frames per second, and can be easily configured to simulate different vision-based tactile sensors, including DIGIT and OmniTact. In this letter, we detail the principles that drove the implementation of TACTO and how they are reflected in its architecture. We demonstrate TACTO on a perceptual task, by learning to predict grasp stability using touch from 1 million grasps, and on a marble manipulation control task. Moreover, we provide a proof-of-concept that TACTO can be successfully used for Sim2Real applications. We believe that TACTO is a step towards the widespread adoption of touch sensing in robotic applications, and to enable machine learning practitioners interested in multi-modal learning and control.",robotics
10.1109/lra.2021.3123374,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,uncertainty for identifying open-set errors in visual object detection,https://ieeexplore.ieee.org/document/9591346/,"Deployed into an open world, object detectors are prone to open-set errors, false positive detections of object classes not present in the training dataset.We propose GMM-Det, a real-time method for extracting epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified by their low log-probability under all Gaussian Mixture Models. We test two common detector architectures, Faster R-CNN and RetinaNet, across three varied datasets spanning robotics and computer vision. Our results show that GMM-Det consistently outperforms existing uncertainty techniques for identifying and rejecting open-set detections, especially at the low-error-rate operating point required for safety-critical applications. GMM-Det maintains object detection performance, and introduces only minimal computational overhead. We also introduce a methodology for converting existing object detection datasets into specific <italic>open-set</italic> datasets to evaluate open-set performance in object detection.",robotics
10.1109/tro.2021.3084374,preprocessed,IEEE Transactions on Robotics,IEEE,2022-02-01 00:00:00,ieeexplore,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,https://ieeexplore.ieee.org/document/9453856/,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",robotics
10.1109/tfuzz.2020.3033141,preprocessed,IEEE Transactions on Fuzzy Systems,IEEE,2022-01-01 00:00:00,ieeexplore,fuzzy double deep q-network-based gait pattern controller for humanoid robots,https://ieeexplore.ieee.org/document/9237162/,"In this article, the adaptive-network-based fuzzy inference system (ANFIS) is combined with the double deep <italic>Q</italic>-network (DDQN) to realize a fuzzy DDQN (FDDQN) such that a humanoid robot can generate a linear inverted pendulum model-based gait pattern in real time. The FDDQN not only allows the humanoid robot to correct the gait pattern instantly but also improves its stability. The proposed scheme is designed and implemented in a toddler-sized humanoid robot called Louis. First, four pressure sensors are installed on the bottom of the sole and one inertial measurement unit is set up on the trunk of the robot. A wireless communication chip is employed to transfer the data to a computer to determine the required parameters for the robot. Next, a control system based on the Linux operating system is developed. The values of the center of pressure and acceleration obtained with the ANFIS are adopted to train the DDQN. The proposed neural network comprises four layers, and the model is cautiously selected to avoid overfitting. The proposed scheme is verified using a robot simulator and then real-time-tested on Louis. The experimental results indicate that the FDDQN can provide the robot timely feedback during walking as well as helps it in adjusting the gait pattern independently. The balancing of the robot through effective dynamic feedback is similar to the balancing ability of an infant learning to walk.",robotics
10.1109/sii52469.2022.9708826,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,evaluation of variable impedance- and hybrid force/motioncontrollers for learning force tracking skills,https://ieeexplore.ieee.org/document/9708826/,"For robots to perform real-world force interaction tasks with human level dexterity, it is crucial to develop adaptable and compliant force controllers. Learning techniques, especially reinforcement learning, provide a platform to develop adaptable controllers for complex robotic tasks. This paper presents an evaluation of two prominent force control methods, variable impedance control and hybrid force-motion control in a robot learning framework. The controllers are evaluated on a Franka Emika Panda robotic manipulator for a robotic interaction task demanding force and motion tracking using a model-based reinforcement learning algorithm, PILCO. Utilizing the learning framework to find the optimal controller parameters has significantly improved the performance of the controllers. The implementation of the controllers integrated with the robot learning framework is available on https://github.com/martihmy/Compliant_control.",robotics
10.1109/lra.2022.3142433,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,enabling low-cost full surface tactile skin for human robot interaction,https://ieeexplore.ieee.org/document/9681158/,"Realizing full coverage, low-maintenance, and low-cost tactile skin is a <italic>de facto</italic> design dream since the invention of robots. It ensures safety and enables collaborative work protocols for human robot interactions (HRI). The on-robot tactile capability is realized by deploying an array of external sensors or inferring from proprioceptive information that comes with the robot, such as motor torque. However, these methods may be cumbersome, introduce extra management cost, expensive, lack real-world robustness, or require special robot designs. In this letter, we present <italic>SonicSkin</italic>, a low-cost ($2) and easy to deploy system that localizes the on-robot human touch and estimates the touch pressure without actually attaching sensors at potential touch locations. The system requires only a single pair of piezoelectric transducers (<italic>i.e.</italic> one transmitter and one receiver) attached on the target robot and turns the robot itself into a versatile sensor. We present a set of novel algorithms to progressively address the unique challenges posed by our system design. We put together an end-to-end <italic>SonicSkin</italic> system on a Jaco robot arm that runs in real-time, and conducted an extensive real-world study including 57019 actual evaluation datapoints under various challenging conditions from 12 human subjects. <italic>SonicSkin</italic> achieves less than 2 cm localization error for 96.4% of touches, with more than 96.7% cross-correlation similarity between the predicted touch pressure and the ground truth touch pressure.",robotics
10.1109/access.2022.3145969,preprocessed,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,fastmde: a fast cnn architecture for monocular depth estimation at high resolution,https://ieeexplore.ieee.org/document/9690863/,"A depth map helps robots and autonomous vehicles (AVs) visualize the three-dimensional world to navigate and localize neighboring obstacles. However, it is difficult to develop a deep learning model that can estimate the depth map from a single image in real-time. This study proposes a fast monocular depth estimation model named <italic>FastMDE</italic> by optimizing the deep convolutional neural network according to the encoder-decoder architecture. The decoder needs to obtain partial and semantic feature maps from the encoding phase to improve the depth estimation accuracy. Therefore, we designed FastMDE with two effective strategies. The first one involved redesigning the skip connection with the features of the squeeze-excitation module to obtain partial and semantic feature maps of the encoding phase. The second strategy involved redesigning the decoder by using the fusion dense block to permit the usage of high-resolution features that were learned earlier in the network before upsampling. The proposed FastMDE model utilizes only 4.1 M parameters, which is much lesser than the parameters utilized by state-of-art models. Thus, FastDME has a higher accuracy and lower latency than previous models. This study also demonstrates that MDE can leverage deep neural networks in real-time (i.e., 30 fps) with the Linux embedded board Nvidia Jetson Xavier NX. The model can facilitate the development and applications with superior performances and easy deployment on an embedded platform.",robotics
10.1109/lra.2021.3129136,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,ocrtoc: a cloud-based competition and benchmark for robotic grasping and manipulation,https://ieeexplore.ieee.org/document/9619915/,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is <uri>www.ocrtoc.org</uri>, and the OCRTOC software package is available at <uri>https://github.com/OCRTOC/OCRTOC_software_package</uri>.",robotics
10.1109/lra.2022.3143289,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,visuotactile 6d pose estimation of an in-hand object using vision and tactile sensor data,https://ieeexplore.ieee.org/document/9682507/,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",robotics
10.1109/sii52469.2022.9708882,preprocessed,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,reinforcement learning based hierarchical control for path tracking of a wheeled bipedal robot with sim-to-real framework,https://ieeexplore.ieee.org/document/9708882/,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",robotics
10.1109/lra.2022.3146900,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,open simulation environment for learning and practice of robot-assisted surgical suturing,https://ieeexplore.ieee.org/document/9697399/,"Automation has the potential to improve the standard of care but is difficult to realize due to perceptual challenges, especially in soft-tissue surgery. Machine learning can provide solutions, but typically requires large amounts of training data, which is time-consuming to collect. Even with shared platforms, hardware differences can prevent effective sharing of data between institutions. This letter proposes a standardized simulation platform for training and testing algorithms to control surgical robotic systems, which is built upon an open-source simulator, the Asynchronous Multi-Body Framework (AMBF), to enable quick prototyping of different scenes. An illustrative example of a suturing task on a phantom is presented and has formed the basis of a challenge, released to the community. The top-level contribution is the open-source release of a dynamic simulation environment that enables realistic suturing on a phantom, but supporting contributions include its extendable architectural design and a series of algorithmic optimizations to achieve real-time control and collision detection, realistic behavior of the needle and suture, and generation of multi-modal ground-truth data, including labeled depth data. These capabilities enable simulation-based surgical training and support research in machine learning for surgical scene perception and autonomous action.",robotics
10.1109/lra.2022.3145971,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,focus on impact: indoor exploration with intrinsic motivation,https://ieeexplore.ieee.org/document/9691914/,"Exploration of indoor environments has recently experienced a significant interest, also thanks to the introduction of deep neural agents built in a hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on simulated environments. Current state-of-the-art methods employ a dense extrinsic reward that requires the complete a priori knowledge of the layout of the training environment to learn an effective exploration policy. However, such information is expensive to gather in terms of time and resources. In this work, we propose to train the model with a purely intrinsic reward signal to guide exploration, which is based on the impact of the robot’s actions on its internal representation of the environment. So far, impact-based rewards have been employed for simple tasks and in procedurally generated synthetic environments with countable states. Since the number of states observable by the agent in realistic indoor environments is non-countable, we include a neural-based density model and replace the traditional count-based regularization with an estimated pseudo-count of previously visited states. The proposed exploration approach outperforms DRL-based competitors relying on intrinsic rewards and surpasses the agents trained with a dense extrinsic reward computed with the environment layouts. We also show that a robot equipped with the proposed approach seamlessly adapts to point-goal navigation and real-world deployment.",robotics
10.1109/lra.2022.3141150,preprocessed,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,reve-ce: remote embodied visual referring expression in continuous environment,https://ieeexplore.ieee.org/document/9674225/,"Ithas always been a great challenge for the robot to navigate in the visual world following natural language instructions. Recently, several tasks such as the Vision-and-Language Navigation (VLN) and Remote Embodied Visual Referring Expression in Real Indoor Environments (REVERIE) are proposed trying to solve this challenge. And the most significant difference between VLN and REVERIE tasks is that REVERIE uses a higher guidance level instruction. However, the navigation process of REVERIE is implemented in a discrete environment, which is unrealistic in real world scenarios. To make the REVERIE task more consistent with the real physical world, we develop a new task of Remote Embodied Visual Referring Expression in Continuous Environment, namely REVE-CE, in which the agent executes a much longer sequence of low-level actions given language instructions. Furthermore, we propose a multi-branch cross modal attention (MBCMA) framework to solve the proposed REVE-CE task. Extensive experiments are conducted demonstrating that the proposed framework greatly outperforms the state-of-the-art VLN baselines and a new benchmark for the proposed REVE-CE task is built.",robotics
http://arxiv.org/abs/2202.08004v1,preprocessed,arxiv,arxiv,2022-02-16 00:00:00,arxiv,deep koopman operator with control for nonlinear systems,http://arxiv.org/abs/2202.08004v1,"Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. We then design an auxiliary control network to encode
the nonlinear state-dependent control term to model the nonlinearity in control
input. For linear control, this encoded term is considered the new control
variable instead, ensuring the linearity of the embedding space. Then we deploy
Linear Quadratic Regulator (LQR) on the linear embedding space to derive the
optimal control policy and decode the actual control input from the control
net. Experimental results demonstrate that our approach outperforms other
existing methods, reducing the prediction error by order-of-magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and 7 Dof robotic manipulator.",robotics
http://arxiv.org/abs/2202.07064v1,preprocessed,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"towards hardware implementation of wta for cpg-based control of a
  spiking robotic arm",http://arxiv.org/abs/2202.07064v1,"Biological nervous systems typically perform the control of numerous degrees
of freedom for example in animal limbs. Neuromorphic engineers study these
systems by emulating them in hardware for a deeper understanding and its
possible application to solve complex problems in engineering and robotics.
Central-Pattern-Generators (CPGs) are part of neuro-controllers, typically used
at their last steps to produce rhythmic patterns for limbs movement. Different
patterns and gaits typically compete through winner-take-all (WTA) circuits to
produce the right movements. In this work we present a WTA circuit implemented
in a Spiking-Neural-Network (SNN) processor to produce such patterns for
controlling a robotic arm in real-time. The robot uses spike-based
proportional-integrativederivative (SPID) controllers to keep a commanded joint
position from the winner population of neurons of the WTA circuit. Experiments
demonstrate the feasibility of robotic control with spiking circuits following
brain-inspiration.",robotics
http://arxiv.org/abs/2202.06003v2,preprocessed,arxiv,arxiv,2022-02-12 00:00:00,arxiv,robust learning from observation with model misspecification,http://arxiv.org/abs/2202.06003v2,"Imitation learning (IL) is a popular paradigm for training policies in
robotic systems when specifying the reward function is difficult. However,
despite the success of IL algorithms, they impose the somewhat unrealistic
requirement that the expert demonstrations must come from the same domain in
which a new imitator policy is to be learned. We consider a practical setting,
where (i) state-only expert demonstrations from the real (deployment)
environment are given to the learner, (ii) the imitation learner policy is
trained in a simulation (training) environment whose transition dynamics is
slightly different from the real environment, and (iii) the learner does not
have any access to the real environment during the training phase beyond the
batch of demonstrations given. Most of the current IL methods, such as
generative adversarial imitation learning and its state-only variants, fail to
imitate the optimal expert behavior under the above setting. By leveraging
insights from the Robust reinforcement learning (RL) literature and building on
recent adversarial imitation approaches, we propose a robust IL algorithm to
learn policies that can effectively transfer to the real environment without
fine-tuning. Furthermore, we empirically demonstrate on continuous-control
benchmarks that our method outperforms the state-of-the-art state-only IL
method in terms of the zero-shot transfer performance in the real environment
and robust performance under different testing conditions.",robotics
http://arxiv.org/abs/2201.09857v2,preprocessed,arxiv,arxiv,2022-01-24 00:00:00,arxiv,"stops: short-term-based volatility-controlled policy search and its
  global convergence",http://arxiv.org/abs/2201.09857v2,"It remains challenging to deploy existing risk-averse approaches to
real-world applications. The reasons are multi-fold, including the lack of
global optimality guarantee and the necessity of learning from long-term
consecutive trajectories. Long-term consecutive trajectories are prone to
involving visiting hazardous states, which is a major concern in the
risk-averse setting. This paper proposes \textbf{\ul{S}}hort-\textbf{\ul{T}}erm
V\textbf{\ul{O}}latility-controlled \textbf{\ul{P}}olicy \textbf{\ul{S}}earch
(STOPS), a novel algorithm that solves risk-averse problems by learning from
short-term trajectories instead of long-term trajectories. Short-term
trajectories are more flexible to generate, and can avoid the danger of
hazardous state visitations. By using an actor-critic scheme with an
overparameterized two-layer neural network, our algorithm finds a globally
optimal policy at a sublinear rate with proximal policy optimization and
natural policy gradient, with effectiveness comparable to the state-of-the-art
convergence rate of risk-neutral policy-search methods. The algorithm is
evaluated on challenging Mujoco robot simulation tasks under the mean-variance
evaluation metric. Both theoretical analysis and experimental results
demonstrate a state-of-the-art level of STOPS' performance among existing
risk-averse policy search methods.",robotics
http://arxiv.org/abs/2201.05753v1,preprocessed,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"parameter identification and motion control for articulated rigid body
  robots using differentiable position-based dynamics",http://arxiv.org/abs/2201.05753v1,"Simulation modeling of robots, objects, and environments is the backbone for
all model-based control and learning. It is leveraged broadly across dynamic
programming and model-predictive control, as well as data generation for
imitation, transfer, and reinforcement learning. In addition to fidelity, key
features of models in these control and learning contexts are speed, stability,
and native differentiability. However, many popular simulation platforms for
robotics today lack at least one of the features above. More recently,
position-based dynamics (PBD) has become a very popular simulation tool for
modeling complex scenes of rigid and non-rigid object interactions, due to its
speed and stability, and is starting to gain significant interest in robotics
for its potential use in model-based control and learning. Thus, in this paper,
we present a mathematical formulation for coupling position-based dynamics
(PBD) simulation and optimal robot design, model-based motion control and
system identification. Our framework breaks down PBD definitions and
derivations for various types of joint-based articulated rigid bodies. We
present a back-propagation method with automatic differentiation, which can
integrate both positional and angular geometric constraints. Our framework can
critically provide the native gradient information and perform gradient-based
optimization tasks. We also propose articulated joint model representations and
simulation workflow for our differentiable framework. We demonstrate the
capability of the framework in efficient optimal robot design, accurate
trajectory torque estimation and supporting spring stiffness estimation, where
we achieve minor errors. We also implement impedance control in real robots to
demonstrate the potential of our differentiable framework in human-in-the-loop
applications.",robotics
http://arxiv.org/abs/2201.01369v1,preprocessed,arxiv,arxiv,2022-01-04 00:00:00,arxiv,"using simulation optimization to improve zero-shot policy transfer of
  quadrotors",http://arxiv.org/abs/2201.01369v1,"In this work, we show that it is possible to train low-level control policies
with reinforcement learning entirely in simulation and, then, deploy them on a
quadrotor robot without using real-world data to fine-tune. To render zero-shot
policy transfers feasible, we apply simulation optimization to narrow the
reality gap. Our neural network-based policies use only onboard sensor data and
run entirely on the embedded drone hardware. In extensive real-world
experiments, we compare three different control structures ranging from
low-level pulse-width-modulated motor commands to high-level attitude control
based on nested proportional-integral-derivative controllers. Our experiments
show that low-level controllers trained with reinforcement learning require a
more accurate simulation than higher-level control policies.",robotics
10.1016/j.artint.2021.103637,preprocessed,Artificial Intelligence,scopus,2022-02-01,sciencedirect,online perceptual learning and natural language acquisition for autonomous robots,https://api.elsevier.com/content/abstract/scopus_id/85120333069,"In this work, the problem of bootstrapping knowledge in language and vision for autonomous robots is addressed through novel techniques in grammar induction and word grounding to the perceptual world. In particular, we demonstrate a system, called OLAV, which is able, for the first time, to (1) learn to form discrete concepts from sensory data; (2) ground language (n-grams) to these concepts; (3) induce a grammar for the language being used to describe the perceptual world; and moreover to do all this incrementally, without storing all previous data. The learning is achieved in a loosely-supervised manner from raw linguistic and visual data. Moreover, the learnt model is transparent, rather than a black-box model and is thus open to human inspection. The visual data is collected using three different robotic platforms deployed in real-world and simulated environments and equipped with different sensing modalities, while the linguistic data is collected using online crowdsourcing tools and volunteers. The analysis performed on these robots demonstrates the effectiveness of the framework in learning visual concepts, language groundings and grammatical structure in these three online settings.",robotics
10.1016/j.automatica.2021.110007,preprocessed,Automatica,scopus,2022-01-01,sciencedirect,an analytic layer-wise deep learning framework with applications to robotics,https://api.elsevier.com/content/abstract/scopus_id/85118989490,"Deep learning (DL) has achieved great success in many applications, but it has been less well analyzed from the theoretical perspective. The unexplainable success of black-box DL models has raised questions among scientists and promoted the emergence of the field of explainable artificial intelligence (XAI). In robotics, it is particularly important to deploy DL algorithms in a predictable and stable manner as robots are active agents that need to interact safely with the physical world. This paper presents an analytic deep learning framework for fully connected neural networks, which can be applied for both regression problems and classification problems. Examples for regression and classification problems include online robot control and robot vision. We present two layer-wise learning algorithms such that the convergence of the learning systems can be analyzed. Firstly, an inverse layer-wise learning algorithm for multilayer networks with convergence analysis for each layer is presented to understand the problems of layer-wise deep learning. Secondly, a forward progressive learning algorithm where the deep networks are built progressively by using single hidden layer networks is developed to achieve better accuracy. It is shown that the progressive learning method can be used for fine-tuning of weights from convergence point of view. The effectiveness of the proposed framework is illustrated based on classical benchmark recognition tasks using the MNIST and CIFAR-10 datasets and the results show a good balance between performance and explainability. The proposed method is subsequently applied for online learning of robot kinematics and experimental results on kinematic control of UR5e robot with unknown model are presented.",robotics
