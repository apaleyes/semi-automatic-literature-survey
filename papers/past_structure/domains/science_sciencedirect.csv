id,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1016/j.resconrec.2021.106022,Journal,"Resources, Conservation and Recycling",scopus,2022-03-01,sciencedirect,Using computer vision to recognize composition of construction waste mixtures: A semantic segmentation approach,https://api.elsevier.com/content/abstract/scopus_id/85118570774,"Timely and accurate recognition of construction waste (CW) composition can provide yardstick information for its subsequent management (e.g., segregation, determining proper disposal destination). Increasingly, smart technologies such as computer vision (CV), robotics, and artificial intelligence (AI) are deployed to automate waste composition recognition. Existing studies focus on individual waste objects in well-controlled environments, but do not consider the complexity of the real-life scenarios. This research takes the challenges of the mixture and clutter nature of CW as a departure point and attempts to automate CW composition recognition by using CV technologies. Firstly, meticulous data collection, cleansing, and annotation efforts are made to create a high-quality CW dataset comprising 5,366 images. Then, a state-of-the-art CV semantic segmentation technique, DeepLabv3+, is introduced to develop a CW segmentation model. Finally, several training hyperparameters are tested via orthogonal experiments to calibrate the model performance. The proposed approach achieved a mean Intersection over Union (mIoU) of 0.56 in segmenting nine types of materials/objects with a time performance of 0.51 s per image. The approach was found to be robust to variation of illumination and vehicle types. The study contributes to the important problem of material composition recognition, formalizing a deep learning-based semantic segmentation approach for CW composition recognition in complex environments. It paves the way for better CW management, particularly in engaging robotics, in the future. The trained models are hosted on GitHub, based on which researchers can further finetune for their specific applications.",science
10.1016/j.ymssp.2021.108284,Journal,Mechanical Systems and Signal Processing,scopus,2022-02-15,sciencedirect,Real-time model calibration with deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85112506465,"The real-time, and accurate inference of model parameters is of great importance in many scientific and engineering disciplines that use computational models (such as a digital twin) for the analysis and prediction of complex physical processes. However, fast and accurate inference for processes of complex systems cannot easily be achieved in real-time with state-of-the-art methods under noisy real-world conditions with the requirement of a real-time response. The primary reason is that the inference of model parameters with traditional techniques based on optimization or sampling often suffers from computational and statistical challenges, resulting in a trade-off between accuracy and deployment time. In this paper, we propose a novel framework for inference of model parameters based on reinforcement learning. The proposed methodology is demonstrated and evaluated on two different physics-based models of turbofan engines. The experimental results demonstrate that the proposed methodology outperforms all other tested methods in terms of speed and robustness, with high inference accuracy.",science
10.1016/j.ces.2021.117205,Journal,Chemical Engineering Science,scopus,2022-02-02,sciencedirect,"Developments of leak detection, diagnostics, and prediction algorithms in multiphase flows",https://api.elsevier.com/content/abstract/scopus_id/85118896502,"Leak detection, diagnostics, and prediction constitute a crucial phase of the flow assurance risk management process for onshore and offshore pipelines. There are a variety of techniques and algorithms that can be deployed to address each aspect. To date, most review papers have concentrated on steady-state and single-phase flow conditions. The goal of the current review is therefore to carry out a thorough analysis of the available leak detection and diagnosis methods by focusing on (i) multiphase flow and transient flow conditions, (ii) model-based and data-driven techniques, (iii) prediction tools, and (iv) performance measures. Detailed assessment of leak detection methods based on accuracy, complexity, data requirement, and cost of installation are discussed. Data-driven techniques are utterly dependent on qualitative and quantitative data available from pipeline systems. Contrastingly data-driven techniques, model-based techniques require less data to achieve leak detection, provided that a nearly accurate base model is available. Different methodologies and technologies can be combined in order to produce the best detection and diagnosis outputs. In many cases, statistical analysis was combined with the Real Time Transient Method (RTTM), which helped to minimize false alarms. The material in this review can be used as a robust guide for the design of diagnostic systems and further research.",science
10.1016/j.saa.2021.120347,Journal,Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy,scopus,2022-01-15,sciencedirect,Rapid discrimination of Curcuma longa and Curcuma xanthorrhiza using Direct Analysis in Real Time Mass Spectrometry and Near Infrared Spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115004546,"This study describes a newly developed method for the fast and straightforward differentiation of two turmeric species using Direct Analysis in Real Time mass spectrometry and miniaturized Near Infrared spectroscopy. Multivariate analyses (PCA and LDA) were performed on the mass spectrometric data, thus creating a powerful model for the discrimination of Curcumalonga and Curcumaxanthorrhiza. Cross-validation of the model revealed correctness-scores of 100% with 20-fold as well as leave-one-out validation techniques. To further estimate the models prediction power, seven retail samples of turmeric powder were analyzed and assorted to a species. Looking for a fast, non-invasive, cost-efficient and laboratory independent method, miniaturized NIR spectrometers offer an alternative for quality control of turmeric species. However, different technologies implemented to compensate for their small size, lead to different applicability of these spectrometers. Therefore, we investigated the three handheld spectrometers microPHAZIR, MicroNIR 2200 and MicroNIR 1700ES for their application in spice analysis in hyphenation to PCA, LDA and ANN methods used for the discriminant analysis. While microPHAZIR proved to be the most valuable device for differentiating C.longa and C.xanthorrhiza, MicroNIR 1700ES offered the worst results. These findings are interpreted on the basis of a quantum chemical simulation of the NIR spectrum of curcumin as the representative constituent. It was found that the information accessible to MicroNIR 1700ES that is relevant to the analyzed constituents is located in the spectral region prone to interferences with the matrix, likely limiting the performance of this spectrometer in this analytical scenario.",science
10.1016/j.dss.2021.113665,Journal,Decision Support Systems,scopus,2022-01-01,sciencedirect,A constraint programming model for making recommendations in personal process management: A design science research approach,https://api.elsevier.com/content/abstract/scopus_id/85115634506,"Decision-making in everyday life has an essential role in effectively completing personal tasks and processes. The complexity of these processes and the resulting cognitive load of managing them may vary significantly. To decrease the cognitive load created by such decision-making efforts and to obtain better outcomes, recommendation systems carry significant potential. In order to investigate the benefits provided by decision support systems (DSS) in personal process management (PPM), we first build a constraint programming (CP) model and a prototype context-aware-mobile application employing this CP model. Then, we evaluate the application and the model via two exemplary real-world scenarios. The scenarios form the core of the experiments conducted with 50 participants. We compare the participants’ planning performances with and without the PPM system with quantitative metrics such as planning times and scenario objective values. In addition, System Usability Scale (SUS) questionnaires and open-ended questions provide qualitative evaluation results. Throughout the study, we apply the Design Science Research methodology to rigorously conduct research activities by proof of concept, proof of use, and proof of value. The empirical results clearly show that our proposed model for PPM is effective, and the developed prototype solution generates positive participant comments as well as a high SUS score. Overall, the prototype PPM system with CP implementation leads to better planning in less time in the planning phase, and it lets the user do fast replanning in the execution phase, which is invaluable in dynamically changing situations such as daily activities.",science
10.1016/j.postharvbio.2021.111741,Journal,Postharvest Biology and Technology,scopus,2022-01-01,sciencedirect,Multi-output 1-dimensional convolutional neural networks for simultaneous prediction of different traits of fruit based on near-infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115232057,"In spectral data predictive modelling of fresh fruit, often the models are calibrated to predict multiple responses. A common method to deal with such a multi-response predictive modelling is the partial least-squares (PLS2) regression. Recently, deep learning (DL) has shown to outperform partial least-squares (PLS) approaches for single fruit traits prediction. The DL can also be adapted to perform multi-response modelling. This study presents an implementation of DL modelling for multi-response prediction for spectral data of fresh fruit. To show this, a real NIR data set related to SSC and MC measurements in pear fruit was used. Since DL models perform better with larger data sets, a data augmentation procedure was performed prior to data modelling. Furthermore, a comparative study was also performed between two of the most used DL architectures for spectral analysis, their multi-output and single-output variants and a classic baseline model using PLS2. A key point to note that all the DL modelling presented in this study is performed using novel automated optimisation tools such as Bayesian optimisation and Hyperband. The results showed that DL models can be easily adapted by changing the output of the fully connected layers to perform multi-response modelling. In comparison to the PLS2, the multi-response DL model showed ∼13 % lower root mean squared error (RMSE), showing the ease and superiority of handling multi-response by DL models for spectral calibration.",science
10.1016/j.knosys.2021.107504,Journal,Knowledge-Based Systems,scopus,2021-12-05,sciencedirect,Diacritics generation and application in hate speech detection on Vietnamese social networks,https://api.elsevier.com/content/abstract/scopus_id/85115945453,"One of the challenging problems in text processing is diacritics generation where one needs to generate diacritic marks for non-accented text. With an ever increasing amount of informal text without accents such as short text messages, emails or blog posts on social media, a software system which is capable of generating diacritic marks accurately is very useful and necessary in many situations. This paper presents an approach to improve the accuracy of diacritics generation for Vietnamese text. We propose two novel deep learning models which leverage a plausible conceptual representation for the phonetic structure of Vietnamese syllables. Experimental results on real-world datasets show that our models achieve a significant improvement as compared to the state-of-the-art methods for diacritics generation. We also demonstrate that the proposed models can be applied efficiently to improve the accuracy of hate speech detection on Vietnamese social networks.",science
10.1016/j.jneumeth.2021.109371,Journal,Journal of Neuroscience Methods,scopus,2021-12-01,sciencedirect,Development of deep learning models for microglia analyses in brain tissue using DeePathology™ STUDIO,https://api.elsevier.com/content/abstract/scopus_id/85116054792,"Background
                  Interest in artificial intelligence-driven analysis of medical images has seen a steep increase in recent years. Thus, our paper aims to promote and facilitate the use of this state-of-the-art technology to fellow researchers and clinicians.
               
                  New method
                  We present custom deep learning models generated in DeePathology™ STUDIO without the need for background knowledge in deep learning and computer science underlined by practical suggestions.
               
                  Results
                  We describe the general workflow in this commercially available software and present three real-world examples how to detect microglia on IBA1-stained mouse brain sections including their differences, validation results and analysis of a sample slide.
               
                  Comparison with existing methods
                  Deep-learning assisted analysis of histological images is faster than classical analysis methods, and offers a wide variety of detection possibilities that are not available using methods based on staining intensity.
               
                  Conclusions
                  Reduced researcher bias, increased speed and extended possibilities make deep-learning assisted analysis of histological images superior to traditional analysis methods for histological images.",science
10.1016/j.cma.2021.114121,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2021-12-01,sciencedirect,A virtual model architecture for engineering structures with Twin Extended Support Vector Regression (T-X-SVR) method,https://api.elsevier.com/content/abstract/scopus_id/85114715295,"A machine learning aided virtual model architecture framework is presented. With great computational stability and preserved convexity features, the Twin Extended Support Vector Regression (T-X-SVR) method is adopted to imitate the underpinned and sophisticated constitutive relationship between the systematic inputs and outcomes in real-world applications. Various numerical simulations can be implemented on the virtual model with greatly reduced computational costs. The multi-type information from heterogeneous sources and multifarious engineering applications are supported by the proposed framework. For the virtual model aided numerical analysis with the multi-type information, fuzzy-valued probabilistic distributional characteristics of the bounds of the concerned structural responses are estimated. The capability of the modularity feature of the virtual model improves the operational flexibility which makes the implementation of such advance technique more user friendly. To demonstrate the applicability and computational efficiency of the proposed framework, three practical engineering stimulated problems (i.e., the mechanical dynamic system, multiphysics with heat transfer and gas flow interaction, and the expansion process of a biomedical stent with both material and geometry nonlinearities), involving multi-type information (i.e., random parameters and fuzzy sets), are fully investigated through the proposed virtual model architecture framework.",science
10.1016/j.talanta.2021.122780,Journal,Talanta,scopus,2021-12-01,sciencedirect,A real-world approach to identifying animal bones and Lower Pleistocene fossils by laser induced breakdown spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85111807121,"Archaeological sites often contain accumulations of remains derived from different independent events produced by different agents. Thus, in Palaeolithic sites, it is normal to find alternating occupations between humans and carnivores. The faunal assemblages at these sites usually include hundreds or thousands of bone fragments, which are very difficult to associate them to specific individuals since there are no currently available techniques able to do it in a straightforward and cost-effective way. In this work we present a methodology that allows us to characterise the anatomical remains of a bone accumulation and relate them all back to the specific individuals to which they belong. In order to provide a real world application, we have used a selection of animal bones from different individuals belonging to deer and sheep (fed in a controlled way using the same diet). On the other hand, fossilized faunal remains have also been analysed to verify if these fossilized bones keep some of the fingerprinting of the animal from which they come from. For this purpose, we have developed a protocol using Laser Induced Breakdown Spectroscopy (LIBS) together with Neural Networks (NN) implemented here to discriminate and reassemble deer and sheep bones from different individuals, which we subsequently applied for these proposes to fossilized material. To the best of our knowledge, this is the first time that this technique has been applied for individual fingerprinting to actuality and fossil samples. The elemental composition of bones provides enough information to get a correct discrimination of different individuals. The spectral correlation has exceeded 95 %. and all individuals were correctly classified to the individual from which they come from. There have been no instances of false positives or false negatives in our tests or applications.",science
10.1016/j.future.2021.06.033,Journal,Future Generation Computer Systems,scopus,2021-12-01,sciencedirect,Detecting malicious behavior in social platforms via hybrid knowledge- and data-driven systems,https://api.elsevier.com/content/abstract/scopus_id/85109435960,"Among the wide variety of malicious behavior commonly observed in modern social platforms, one of the most notorious is the diffusion of fake news, given its potential to influence the opinions of millions of people who can be voters, consumers, or simply citizens going about their daily lives. In this paper, we implement and carry out an empirical evaluation of a version of the recently-proposed NetDER architecture for hybrid AI decision-support systems with the capability of leveraging the availability of machine learning modules, logical reasoning about unknown objects, and forecasts based on diffusion processes. NetDER is a general architecture for reasoning about different kinds of malicious behavior such as dissemination of fake news, hate speech, and malware, detection of botnet operations, prevention of cyber attacks including those targeting software products or blockchain transactions, among others. Here, we focus on the case of fake news dissemination on social platforms by three different kinds of users: non-malicious, malicious, and botnet members. In particular, we focus on three tasks: (i) determining who is responsible for posting a fake news article, (ii) detecting malicious users, and (iii) detecting which users belong to a botnet designed to disseminate fake news. Given the difficulty of obtaining adequate data with ground truth, we also develop a testbed that combines real-world fake news datasets with synthetically generated networks of users and fully-detailed traces of their behavior throughout a series of time points. We designed our testbed to be customizable for different problem sizes and settings, and make its code publicly available to be used in similar evaluation efforts. Finally, we report on the results of a thorough experimental evaluation of three variants of our model and six environmental settings over the three tasks. Our results clearly show the effects that the quality of knowledge engineering tasks, the quality of the underlying machine learning classifier used to detect fake news, and the specific environmental conditions have on smart policing efforts in social platforms.",science
10.1016/j.livsci.2021.104700,Journal,Livestock Science,scopus,2021-11-01,sciencedirect,A review of deep learning algorithms for computer vision systems in livestock,https://api.elsevier.com/content/abstract/scopus_id/85118744270,"In livestock operations, systematically monitoring animal body weight, biometric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves massive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phenotypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and the respective deep learning algorithms implemented in Animal Science studies. Then, we reviewed the published research studies in Animal Science which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an improved predictive ability in livestock applications and a faster inference. However, only a few articles fully described the deep learning algorithms and their implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missing. We summarized peer-reviewed articles by computer vision tasks (image classification, object detection, and object segmentation), deep learning algorithms, animal species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions.",science
10.1016/j.conbuildmat.2021.124915,Journal,Construction and Building Materials,scopus,2021-11-01,sciencedirect,Prediction models for low-temperature creep compliance of asphalt mixtures containing reclaimed asphalt pavement (RAP),https://api.elsevier.com/content/abstract/scopus_id/85115205905,"The low-temperature creep compliances (D(t)) of asphalt mixture is one of the necessary parameters to predict the depth and amount of low-temperature cracks. Level 3 analysis in Mechanistic-Empirical Pavement Design Guide (MEPDG) software uses asphalt binder properties parameters and mixture volumetric properties to predict D(t) when the real laboratorial data is not available. However, some parameters in the model may not be routinely measured in Superpave system, which restricts the use of the prediction model. In addition, new additives and recycling materials such as reclaimed asphalt pavement (RAP) have been extensively used in recent years and shown to have significant effect on the low-temperature cracking resistance of asphalt mixture. However, the effects have not been considered in the existing D(t) prediction models. Hence, the objective of this study is to develop models with significantly high accuracy to predict the D(t) of asphalt mixtures containing RAP. A total of 1890 sets of data points were collected from three different research projects. A Pearson correlation analysis was carried out to select the input parameters which are most influential to D(t). Two prediction models (i.e., multiple linear regression and artificial neural network (ANN) models) were proposed. A comprehensive analysis on the prediction accuracy and reasonability of the proposed models was conducted. The results showed that the proposed models had much better prediction performance with high accuracy than the existing models. The comparisons between the proposed models with the existing models confirmed that it is necessary to take the new additives and recycling materials into account in developing D(t) prediction models.",science
10.1016/j.asoc.2021.107792,Journal,Applied Soft Computing,scopus,2021-11-01,sciencedirect,Sentiment classification using attention mechanism and bidirectional long short-term memory network,https://api.elsevier.com/content/abstract/scopus_id/85112745936,"We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.",science
10.1016/j.talanta.2021.122608,Journal,Talanta,scopus,2021-11-01,sciencedirect,"PIXE based, Machine-Learning (PIXEL) supported workflow for glass fragments classification",https://api.elsevier.com/content/abstract/scopus_id/85109431731,"This paper presents a structured workflow for glass fragment analysis based on a combination of Elemental Analysis using PIXE and Machine Learning tools, with the ultimate goal of standardizing and helping forensic efforts. The proposed workflow was implemented on glass fragments received from the Israeli DIFS (Israeli Police Force's Division of Identification and Forensic Sciences) that were collected from various vehicles, including glass fragments from different manufacturers and years of production. We demonstrate that this workflow can produce models with high (>80%) accuracy in identifying glass fragment's origins and provide a test-case demonstrating how the model can be applied in real-life forensic events. We provide a standard, reproducible methodology that can be used in many forensic domains beyond glass fragments, for example, Gun Shot Residue, flammable liquids, illegal substances, and more.",science
10.1016/j.jhlste.2020.100275,Journal,"Journal of Hospitality, Leisure, Sport and Tourism Education",scopus,2021-11-01,sciencedirect,Industry 4.0 technologies in tourism education: Nurturing students to think with technology,https://api.elsevier.com/content/abstract/scopus_id/85092173436,"The Industry 4.0 revolution is bringing major transformations in the tourism systems design suitable for technologically oriented consumers. Indeed, methods and technologies introduced by Big Data, Automation, Virtual and augmented reality, Robotics and ICT well fit with the Tourism 4.0 paradigm. However, tourism students are not yet trained on techniques, issues and methods related to the Industry 4.0 framework.
                  Hence, relying on a careful examination of the literature on tourism market trends linked to the offer of innovative technological services, we identified conceptual, methodological, technological and practical skills to be developed in an academic curriculum for Tourism Science students. Learning path were focused on: i) processes of data acquisition from social media, ii) data analysis using Machine Learning techniques and iii) data design into significant elements useful to implement communication systems in the tourism field.
               
                  Results
                  showed that the most of participants achieved a medium-high evaluation for the implementation of the communication systems, applying appropriately techniques and tools learned along the course. Furthermore, the high percentage of students satisfaction registered in relation to the course, revealed that students enjoyed this experience. Outcomes reflects the acquisition and the awareness of those skills that will enable students to be conscious protagonists of their role in tourism 4.0.",science
10.1016/j.knosys.2021.107380,Journal,Knowledge-Based Systems,scopus,2021-10-27,sciencedirect,Hierarchical Social Similarity-guided Model with Dual-mode Attention for session-based recommendation,https://api.elsevier.com/content/abstract/scopus_id/85113609671,"Session-based recommendation models users’ interests in sessions to make recommendations. Many previous studies have shown that users usually have similar interests to their friends, and are easily influenced by friends. However, these studies also tend to ignore the fact that users’ interests may merely be similar to certain friends’ interests in certain aspects. To address the above issues, we propose a novel Hierarchical Social Similarity-guided Model with Dual-mode Attention (HMDA) for Session-based Recommendation. Specifically, we first calculate the item-level similarity between users and their friends to select influential friends. We then compute the aspect-level similarity to explore the aspect difference between users’ interests and friends’ interests. Under the guidance of the item-level and aspect-level similarity, HMDA is capable of effectively and accurately aggregating the social influence exerted by friends on users, and further combining users’ individual interests to enhance recommendation performance. In addition, we design a dual-mode attention mechanism to capture the internal dependence and mutual dependence between the long-term and short-term interests of users. The proposed model is extensively evaluated on three real-world datasets. Experimental results demonstrate that our model outperforms the state-of-the-art baseline methods.",science
10.1016/j.neucom.2021.07.045,Journal,Neurocomputing,scopus,2021-10-21,sciencedirect,Pruning and quantization for deep neural network acceleration: A survey,https://api.elsevier.com/content/abstract/scopus_id/85112651139,"Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.",science
10.1016/j.engstruct.2021.112877,Journal,Engineering Structures,scopus,2021-10-15,sciencedirect,Prediction of fire resistance of concrete encased steel composite columns using artificial neural network,https://api.elsevier.com/content/abstract/scopus_id/85111330781,"Concrete encased steel (CES) columns, also known as steel reinforced concrete (SRC) composite columns, exhibit superior fire resistance due to concrete acting as a protection layer for the embedded steel section. While modern design codes have provided design guides for the fire resistance of CES columns, they are only applicable to those made of normal strength concrete. For high strength CES columns, advanced analysis is needed to capture the brittleness of high strength concrete at elevated temperature. In this paper, two methods, namely the artificial neural network (ANN) and the analytical equations, are proposed to predict the fire resistance of axially-loaded CES columns made of high strength concrete. To train the ANN, a finite difference model is developed to compute the temperature field in CES columns and it is used to establish a database containing 15,200 specimens. The cross-sectional dimensions and materials grades of the specimens are carefully selected to cover a wide range of values including those commonly adopted in real-life applications. The inputs of the ANN are identified through an extensive parametric analysis. The selected ANN consists of 7 inputs, 3 outputs and 2 hidden layers and achieves a high determination coefficient R2 value of 0.999. For practical implementation, analytical equations are also derived and achieve high R2 values above 0.953. The predictive power of the ANN and the analytical equations are examined against the observations obtained from actual fire tests, showing reasonable accuracy of prediction. Both methods are simple, of high accuracy and have implicitly accounted for temperature-dependent material degradation, and hence do not require input of temperature-dependent material properties and advanced analysis software.",science
10.1016/j.envadv.2021.100089,Journal,Environmental Advances,scopus,2021-10-01,sciencedirect,"A practical study of CITES wood species identification by untargeted DART/QTOF, GC/QTOF and LC/QTOF together with machine learning processes and statistical analysis",https://api.elsevier.com/content/abstract/scopus_id/85118730822,"Illegal logging and trafficking of endangered timber species has attracted the world's major organized crime groups, with associated deforestation and serious social damage. The inability of traditional methodologies and DNA analysis to readily perform wood identification to the species level for monitoring has stimulated research on chemotyping techniques. In this study, simple wood extraction of endangered rosewoods (Dalbergia spp), amenable to use in the field, produced colorful hues that were suggestive of wood species. A more definitive study was conducted to develop wood species identification procedures using high-resolution quadrupole time-of-flight (QTOF) mass spectrometers interfaced with liquid chromatography (LC), gas chromatography (GC), and Direct Analysis in Real Time (DART). The time consuming process of extracting “identifying” mass spectral ions for species identification, contentious due to their ubiquitous nature, was supplanted by application of machine learning processes. The unbiased software mining of raw data from multiple analytical batches, followed by statistical Random Forest analysis, enabled discrimination between both anatomically and chemotypically similar Dalbergia species. Statistical Principal Component Analysis (PCA) scatterplots with 95% confidence ellipses were visually compelling in showing a differential clustering of Dalbergia from other commonly traded and lookalike wood species. The information rich raw data from GC or LC analyses offered a corroborative, legally defensible, and widely available confirmatory tool in the identification of timber species.",science
10.1016/j.jmoldx.2021.07.011,Journal,Journal of Molecular Diagnostics,scopus,2021-10-01,sciencedirect,Temporary Regulatory Deviations and the Coronavirus Disease 2019 (COVID-19) PCR Labeling Update Study Indicate What Laboratory-Developed Test Regulation by the US Food and Drug Administration (FDA) Could Look Like,https://api.elsevier.com/content/abstract/scopus_id/85116010045,"The coronavirus disease 2019 (COVID-19) response necessitated innovations and a series of regulatory deviations that also affected laboratory-developed tests (LDTs). To examine real-world consequences and specify regulatory paradigm shifts, legislative proposals were aligned on a common timeline with Emergency Use Authorization (EUA) of LDTs and the US Food and Drug Administration (FDA)-orchestrated severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) labeling update study. The initial EUA adoption by LDT developers shows that the FDA can have oversight over LDTs. We used efficiency-corrected microcosting of our EUA PCR assay to estimate the national cost of the labeling update study to $0.3 to $1.4 million US dollars. Labeling update study performance data showed lower average detection limits in commercial in vitro diagnostic (IVD) assays versus LDTs (32,000 ± 75,000 versus 71,000 ± 147,000 nucleic acid amplification tests/mL; P = 0.04); however, comparison also shows that FDA review of IVD assays and LDTs did not prevent differences between initial and labeling update performance (IVD assay, P < 0.0001; LDT, P = 0.003). The regulatory shifts re-emphasized that both commercial tests and LDTs rely heavily on laboratory competence and procedures; however, lack of performance data on authorized tests, when clinically implemented, precludes assessment of the benefit related to regulatory review. Temporary regulatory deviations during the pandemic and regulatory science tools (ie, reference material) have generated valuable real-world evidence to inform pending legislation regarding LDT regulation.",science
10.1016/j.compbiomed.2021.104820,Journal,Computers in Biology and Medicine,scopus,2021-10-01,sciencedirect,UICPC: Centrality-based clustering for scRNA-seq data analysis without user input,https://api.elsevier.com/content/abstract/scopus_id/85114481229,"scRNA-seq data analysis enables new possibilities for identification of novel cells, specific characterization of known cells and study of cell heterogeneity. The performance of most clustering methods especially developed for scRNA-seq is greatly influenced by user input. We propose a centrality-clustering method named UICPC and compare its performance with 9 state-of-the-art clustering methods on 11 real-world scRNA-seq datasets to demonstrate its effectiveness and usefulness in discovering cell groups. Our method does not require user input. However, it requires settings of threshold, which are benchmarked after performing extensive experiments. We observe that most compared approaches show poor performance due to high heterogeneity and large dataset dimensions. However, UICPC shows excellent performance in terms of NMI, Purity and ARI, respectively. UICPC is available as an R package and can be downloaded by clicking the link https://sites.google.com/view/hussinchowdhury/software.",science
10.1016/j.micpro.2021.104318,Journal,Microprocessors and Microsystems,scopus,2021-10-01,sciencedirect,Investigating data representation for efficient and reliable Convolutional Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/85112801148,"Nowadays, Convolutional Neural Networks (CNNs) are widely used as prediction models in different fields, with intensive use in real-time safety-critical systems. Recent studies have demonstrated that hardware faults induced by an external perturbation or aging effects, may significantly impact the CNN inference, leading to prediction failures. Therefore, ensuring the reliability of CNN platforms is crucial, especially when deployed in critical applications. A lot of effort has been made to reduce the memory and energy footprint of CNNs, paving the way to the adoption of approximate computing techniques such as quantization, reduced precision, weight sharing, and pruning. Unfortunately, approximate computing reduces the intrinsic redundancy of CNNs making them more efficient but less resilient to hardware faults. The goal of this work is twofold. First, we assess the reliability of a CNN when reduced bit widths and two different data types (floating- and fixed-point) are used to represent the network parameters (i.e., synaptic weights). Second, we intend to investigate the best compromise between data type, bit-widths reduction, and reliability. The characterization is performed through a fault injection environment built on the darknet open-source framework and targets two CNNs: LeNet-5 and YOLO. Experimental results show that fixed-point data provide the best trade-off between memory footprint reduction and CNN resilience. In particular, for LeNet-5, we achieved a 4X memory footprint reduction at the cost of a slightly reduced reliability (0.45% of critical faults) without retraining the CNN.",science
10.1016/j.applthermaleng.2021.117375,Journal,Applied Thermal Engineering,scopus,2021-10-01,sciencedirect,A numerical modelling of a multi-layer LaFeCoSi Active magnetic regenerator by using Artificial Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/85111976602,"One of the main problems in the framework of magnetic refrigeration regards low adiabatic temperature changes that occur in the magnetocaloric materials, which limits the widespread application of this technology. Therefore, the major effort of researchers is focused on the development of multi-layer Active Magnetic Regenerators, which allows to enlarge the temperature span of a magnetic refrigerator. The use of numerical models can help to understand the feasibility of such application with less effort in comparison with the use of experimental facilities. One of the main challenges in designing a numerical model of a multi-layer Active Magnetic Regenerator is the effective incorporation of the magnetocaloric data of different magnetocaloric materials, which are fundamental to correctly optimize the configuration of such a device with the aim to improve its performance. These data are usually obtained experimentally from different measurements and their integration into the numerical model is challenging. Therefore, this work proposes a modified multi-layer Active Magnetic Regenerator numerical model based on Artificial Neural Networks to integrate the magnetocaloric properties of magnetocaloric materials, allowing an easier and a more reliable implementation of the real properties of magnetocaloric materials. The proposed model was tested simulating a four-layer and a seven-layer LaFeCoSi Active Magnetic Regenerator. The use of Artificial Neural Networks to integrate the magnetocaloric properties of magnetocaloric materials into the multi-layer Active Magnetic Regenerator allowed to improve the accuracy of the model in comparison with the commonly used technique (i.e., Curie temperature shift method) when compared to the experimental data. Indeed, the maximum error of the maximum temperature span with zero thermal load was reduced from about 13 K to 6.6 K, for the seven-layer configuration, and from about 4.1 K to 1.0 K, for the four-layer configuration. Furthermore, the new model allows to obtain more reliable simulated data about the effectiveness of each layer of the Active Magnetic Regenerator, providing a more useful tool to discuss about the optimization of its configuration. The results shows that Artificial Neural Networks can be successfully applied for integrating the magnetocaloric properties of magnetocaloric materials into a multi-layer Active Magnetic Regenerator numerical model, improving its performance. They represent an innovative way to address the problem of including magnetocaloric properties into numerical models, opening the way to other possible Machine Learning techniques as alternatives to the usual Curie temperature shifting method used in the literature to date.",science
10.1016/j.asoc.2021.107720,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,TDMatcher: A topic-based approach to task-developer matching with predictive intelligence for recommendation,https://api.elsevier.com/content/abstract/scopus_id/85111305283,"Artificial Intelligence is currently gripping the business world, which is the next step on the journey from Big Data to full automation. As crowdsourcing has been widely adopted by more enterprises and developers, the software crowdsourcing platform is able to collect enough data. Therefore, we introduce predictive intelligence to solve complex problems. This provides a bridge between software developers and enterprises: developers look for suitable tasks, whose aim is to gain revenues with respect to their interests and abilities; enterprises look for developers that are able to complete crowdsourcing tasks and/or solve hard problems. One main problem is the prediction challenge, i.e., how to perfectly predict the developers for the software crowdsourcing tasks and make appropriate recommendations. To solve the problem, this paper introduces predictive intelligence and proposes TDMatcher, which can effectively perform task-developer pairs prediction and recommendations for software crowdsourcing. First, we builds a unified model for tasks and developers such that they can be matched in the same domain space. Second, we quantitatively measures the matching degree between tasks and developers. Third, we randomly generates potential matchings between developers and crowdsourcing tasks and then employs an MCMC sampling approach to optimize the whole process. Highly matched task-developer pairs can be achieved in the sampling process. In order to solve the cold-start problem, we constructs a social network for each new developer, which indicates that the developer’s interests/abilities to be modeled We implemented TDMatcher and evaluated it against the state-of-the-art approaches on the real-world dataset. The experimental results clearly demonstrate the superiority of TDMatcher. We measured our proposed TDMatcher through the accuracy, diversity and Harmonic Mean of TDMatcher, and found that: (1) TDMatcher outperforms the state-of-the-arts by 15+% in the prediction accuracy and 30% in diversity; and (2) TDMatcher achieves a balance between accuracy and diversity. We believe that TDMatcher provides crowdsourcing platforms with much more capabilities in finding appropriate developers to complete crowdsourcing tasks or vice versa.",science
10.1016/j.chaos.2021.111246,Journal,"Chaos, Solitons and Fractals",scopus,2021-10-01,sciencedirect,To restrict or not to restrict? Use of artificial neural network to evaluate the effectiveness of mitigation policies: A case study of Turkey,https://api.elsevier.com/content/abstract/scopus_id/85111261415,"Outbreaks, epidemics or pandemics have increased over the last years, increasing the morbidity and mortality over large geographical areas, as well as causing financial crises and irreversible social changes. Coping with emerging infectious diseases such as Covid-19, different mitigation policies are developed by countries. However, the benefit of each mitigation policy is still not well-explored due to the considerable difference between implementations of policies in each country. The question is which policies play a significant role in controlling Covid-19 transmission. Developing two models used in Artificial Neural Network, this study investigates the impact of mitigation policies or strategies (a combination of policies) by considering different vaccination and mutation scenarios. The former model requires the prediction of reproduction number based on the number of cases reported in previous days; whereas, the latter model is constructed based on the number of people impacted by a mitigation policy or strategy. Although the first model yields more accurate results, it requires the use of historical data; hence, the passage of time during a critical period of fighting against Covid-19. The benefit of the second model is that it can be implemented more quickly by determining a coefficient for each policy or strategy based on the restricted population and/or limited mobility. Testing different scenarios through a real-world example from Turkey, we find mitigation policies or strategies play a significant role in controlling Covid-19; as well as vaccination and mutation scenarios. Our results suggest continuous and predetermined mitigation policies or strategies should be implemented to control the spread of infectious diseases in addition to a successful vaccination program.",science
10.1016/j.asoc.2021.107644,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,Production scheduling in industrial mining complexes with incoming new information using tree search and deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85109174667,"Industrial mining complexes have implemented digital technologies and advanced sensors to monitor and gather real-time data about their different operational aspects, starting from the supply of materials from the mineral deposits involved to the products provided to customers. However, technologies are not available to respond in real-time to the incoming new information to adapt the short-term production schedule of a mining complex. A short-term production schedule determines the daily/weekly/monthly sequence of extraction, the destination of materials and utilization of processing streams. This paper presents a novel self-learning artificial intelligence algorithm for mining complexes that learns, from its own experience, to adapt the short-term production scheduling decisions by responding to incoming new information. The algorithm plays the game of short-term production scheduling on its own using a Monte Carlo tree search to train a deep neural network agent that adapts the short-term production schedule with incoming new information. The deep neural network agent evaluates the short-term production scheduling decisions and, in parallel, performs searches using the Monte Carlo tree search to generate experiences. The experiences are then used to train the agent. The agent improves the strength of the tree search, which results in an even stronger self-play to generate better experiences. An application of the proposed algorithm at a real-world copper mining complex shows its exceptional performance to adapt the 13-week short-term production schedule almost in real-time. The adapted production schedule successfully meets the different production requirements and makes better use of the processing capabilities, while also increasing copper concentrate production by 7% and cash flows by 12% compared to the initial production schedule. A video of the proposed algorithm can be found at https://youtu.be/_gSbzxMc_W8.",science
10.1016/j.jobe.2021.102795,Journal,Journal of Building Engineering,scopus,2021-10-01,sciencedirect,A comparative evaluation of semi-active control algorithms for real-time seismic protection of buildings via magnetorheological fluid dampers,https://api.elsevier.com/content/abstract/scopus_id/85107753497,"Semi-active vibration control is considered a powerful method in reducing the dynamic responses of buildings by using additional smart damping devices. In this study, magnetorheological (MR) dampers have been proposed as one of the semi-active control devices to mitigate the structural vibrations and improve the seismic performance of the structures. The performance of the MR dampers strongly depends on implemented controllers. Hence, the main purpose of this paper is to evaluate the efficiency of several semi-active control algorithms related to MR dampers for seismic control of civil building structures. A 5-story test structure is manufactured, and an MR damper is installed between the ground and the first floor. The performance of the semi-active control approach is experimentally evaluated on a shaking table under historical earthquake records. A neural network-based modeling approach is adopted in the inverse MR damper model for the current control. Three different control algorithms, namely Proportional-Integral-Derivative (PID), Sliding Mode (SMC) and Energy-based controller (EBC), are applied to the system in real-time. The shaking tests are also carried out on the structures with different natural frequencies by increasing the number of stories without changing the geometry and material properties of the 5-story building model. The results indicate that the SMC controller is the most effective control algorithm among all controllers in reducing the base shear force by 51%.",science
10.1016/j.eswa.2021.115081,Journal,Expert Systems with Applications,scopus,2021-10-01,sciencedirect,Utilizing 3D joints data extracted through depth camera to train classifiers for identifying suicide bomber,https://api.elsevier.com/content/abstract/scopus_id/85105277236,"Safety and security of humans is an important concern in every aspect. With the advancement in engineering, sciences, and technology (unfortunately) new methods to harm humans have also been introduced. At the same time, scientists are paying attention to the security aspects by developing new software and hardware gadgets. In comparison to the system level security, the safety/security of human beings is more important. Suicide bombing is one such nuisance that is still an open challenge for the world to detect before it is triggered. This work deals with the identification of a suicide bomber using a 3D depth camera and machine learning techniques. This work utilizes the skeletal data provided by the 3D depth camera to identify a bomber wearing a suicide jacket. The prediction is based on real-time 3D posture data of the body joints obtained through the depth camera. Using a comprehensive experimental design, a dataset is created consisting of 20 joints information obtained from 120 participants. The dataset records this for each of the participants with and without wearing a suicide jacket. Experiments are performed with the suicide jacket bearing 10- to 20-kg weight. Simulations are performed using 3D spatial features of the participants' body in four ways: full body joints (20 joints), upper-half of the body (above the spine base of the skeleton), 20 joints with 15 frames, and 20 joints with 20 frames. It is observed that 15 to 20 frames are sufficient to identify a suspected suicide bomber. The proposed framework utilize four classifiers to identify vulnerability of a subject to be a suicide bomber. Results show that the proposed framework is capable of identifying a suicide bomber with an average accuracy of 92.30%.",science
10.1016/j.cofs.2021.03.014,Journal,Current Opinion in Food Science,scopus,2021-10-01,sciencedirect,Novel digital technologies implemented in sensory science and consumer perception,https://api.elsevier.com/content/abstract/scopus_id/85104656313,"New and emerging digital technologies have been implemented in sensory science, which minimize subjectivity and biases in data acquisition and interpretation compared to traditional methods. These technologies have enabled the incorporation of physiological and emotional responses of panelists elicited by food, beverage, and packaging stimuli through accurate and unbiased information from different sensor technologies. This review focused on recent advances of digital technologies used for sensory science, such as (i) software for sensory science, (ii) integration of biometrics to assess physiological and emotional responses of panelists, (iii) incorporation of virtual, augmented, and mixed reality, and (iv) sensor technology (electronic noses and tongues) for sensory analysis. Rapid data acquisition and results’ interpretation could open the way to automation and implementation of Artificial Intelligence that could revolutionize the food and beverage industries. It also presents a proposed framework for integrating and implementing digital technologies through the food chain from farm/manufacturing facilities to the palate.",science
10.1016/j.chroma.2021.462459,Journal,Journal of Chromatography A,scopus,2021-09-27,sciencedirect,Magnet integrated fabric phase sorptive extraction of selected endocrine disrupting chemicals from human urine followed by high-performance liquid chromatography – photodiode array analysis,https://api.elsevier.com/content/abstract/scopus_id/85112491924,"In current paper, a new advanced modification of fabric phase sorptive extraction is introduced for the first time. This advantageous configuration that integrates the stirring and extraction mechanism into a single sample preparation device was originated by equally considering the beneficial role of the increase of extraction kinetics and more specifically of diffusion on the extraction efficiency of the equilibrium based microextraction techniques and the need for integrating and unite processes for better promotion and implementation of the principles of Green Analytical Chemistry.
                  The resulted magnet integrated fabric phase sorptive extraction (MI-FPSE) device was the spearhead to develop a new analytical methodology for the determination of selected very common endocrine disrupting chemicals as model analytes in human urine by high-performance liquid chromatography-photodiode array analysis. More specifically, the sol-gel Carbowax 20 M coated on hydrophilic cellulose fabric substrate, MI-FPSE device was efficiently employed for the establishment of a new extraction protocol before the chromatographic determination. The sample preparation workflow was methodically optimized in terms of the elution solvent mixture, the volume of the sample, the extraction and the elution time, the stirring speed during the extraction, the ionic strength, and the pH of the sample matrix. The chromatographic separation was performed on a Spherisorb C18 column and a gradient elution program within 14 minutes. Mobile phase consisted of 0.05 ammonium acetate aqueous solution and acetonitrile. The method was validated towards linearity, sensitivity, selectivity, precision, accuracy, and stability. LOD and LOQ ranged between 1.05-1.80 and 3.5-6.0 ng/mL, while %RSD values were found lower than 9.0% in all cases. The method was efficiently applied to the bioanalysis of real samples. All the chosen EDCs were measured at high detection levels. The new MI-FPSE device has demonstrated its performance superiority as a magnet integrated stand-alone extraction device and could be considered as a significant improvement in the field of analytical/bioanalytical sample preparation.",science
10.1016/j.knosys.2021.107302,Journal,Knowledge-Based Systems,scopus,2021-09-27,sciencedirect,A novel deep quantile matrix completion model for top-N recommendation[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85111890353,"Matrix completion models have been receiving keen attention due to their wide applications in science and engineering. However, the majority of these models assumes a symmetric noise distribution in their completion processes and uses conditional mean to characterize data distribution in a data set, the assumption of which incurs noticeable bias toward outliers. Recognizing the fact that noise distribution tends to be asymmetric in the real-world, this paper proposes a novel Deep Quantile Matrix Completion model, abbreviated as DQMC, which aims to accurately capture noise distribution in a data set by modeling conditional quantile of the data set instead of its conditional mean as traditionally handled by many state-of-the-art methods. Implemented via a deep computing paradigm, the newly proposed model maps a data set from its input space to the latent spaces through a two-branched deep autoencoder network. Such a mapping can effectively capture complex information latent in the data set. The proposed model is empowered by two key designed elements, including: (1) its two-branched deep autoencoder network that provides a flexible computing pathway to attain completion results with a high quality; (2) the introduction of a quantile loss function in combination with the proposed deep network, leading to a new unsupervised learning algorithm for tackling the matrix completion tasks with a superior capability. Comparative experimental results consistently demonstrate the superiority of the proposed DQMC model in conducting the top-N recommendation tasks involving both explicit and implicit rating data sets with respect to a series of state-of-the-art recommendation algorithms.",science
10.1016/j.xpro.2021.100639,Journal,STAR Protocols,scopus,2021-09-17,sciencedirect,Timesias: A machine learning pipeline for predicting outcomes from time-series clinical records,https://api.elsevier.com/content/abstract/scopus_id/85108953840,"The prediction of outcomes is a critical part of the clinical surveillance for hospitalized patients. Here, we present Timesias, a machine learning pipeline which predicts outcomes from real-time sequential clinical data. The strategy implemented in Timesias is the first-place solution in the crowd-sourcing DII (discover, innovate, impact) National Data Science Challenge involving more than 100,000 patients, achieving 0.85 as evaluated by AUROC (area under receiver operator characteristic curve) in predicting the early onset of sepsis status. Timesias is freely available via PyPI and GitHub.
                  For complete details on the use and execution of this protocol, please refer to Guan et al. (2021).",science
10.1016/j.foodchem.2021.129710,Journal,Food Chemistry,scopus,2021-09-15,sciencedirect,Lateral flow immunoassay for the simultaneous detection of fipronil and its metabolites in food samples,https://api.elsevier.com/content/abstract/scopus_id/85103939922,"We developed a sensitive and rapid lateral flow immunochromatographic (LFI) assay for the simultaneous detection of fipronil and its metabolites in eggs and cucumbers using gold nanoparticle (GNP)-labeled monoclonal antibodies (mAbs). Anti-fipronil mAbs (1B6) were produced using two haptens and identified by heterologous indirect competitive enzyme-linked immunosorbent assay (icELISA) with half maximal inhibitory concentration (IC50) and limit of detection (LOD) values of 0.46 ± 0.07 and 0.05 ± 0.01 ng mL−1, respectively. The developed LFI strip showed high sensitivity and specificity in the detection of fipronil with cut-off and visual limit of detection (vLOD) values of 10 and 0.25 ng mL−1, respectively. Furthermore, the application of LFI in the detection of fipronil-spiked egg and cucumber samples was validated by liquid chromatography tandem mass spectrometry (LC-MS/MS). Our developed LFI assay is suitable for detection of fipronil and its metabolites in real samples.",science
10.1016/j.clsr.2021.105564,Journal,Computer Law and Security Review,scopus,2021-09-01,sciencedirect,Legal evaluation of the attacks caused by artificial intelligence-based lethal weapon systems within the context of Rome statute,https://api.elsevier.com/content/abstract/scopus_id/85112789421,"Artificial intelligence (AI) as of the level of development reached today has become a scientific reality that is subject to study in the fields of law, political science, and other social sciences besides computer and software engineering. AI systems which perform relatively simple tasks in the early stages of the development period are expected to become fully or largely autonomous in the near future. Thanks to this, AI which includes the concepts of machine learning, deep learning, and autonomy, has begun to play an important role in producing and using smart arms. However, questions about AI-Based Lethal Weapon Systems (AILWS) and attacks that can be carried out by such systems have not been fully answered under legal aspect. More particularly, it is a controversial issue who will be responsible for the actions that an AILWS has committed. In this article, we discussed whether AILWS can commit offense in the context of the Rome Statute, examined the applicable law regarding the responsibility of AILWS, and tried to assess whether these systems can be held responsible in the context of international law, crime of aggression, and individual responsibility. It is our finding that international legal rules including the Rome Statute can be applied regarding the responsibility for the act/crime of aggression caused by AILWS. However, no matter how advanced the cognitive capacity of an AI software, it will not be possible to resort to the personal responsibility of this kind of system since it has no legal personality at all. In such a case, responsibility will remain with the actors who design, produce, and use the system. Last but not least, since no AILWS software does have specific codes of conduct that can make legal and ethical reasonings for today, at the end of the study it was recommended that states and non-governmental organizations together with manifacturers should constitute the necessary ethical rules written in software programs to prevent these systems from unlawful acts and to develop mechanisms that would restrain AI from working outside human control.",science
10.1016/j.urology.2021.06.008,Journal,Urology,scopus,2021-09-01,sciencedirect,Robot-assisted Magnetic Resonance Imaging-ultrasound Fusion Transperineal Targeted Biopsy,https://api.elsevier.com/content/abstract/scopus_id/85110653934,"Objective
                  To demonstrate the key steps to perform robot-assisted magnetic resonance imaging-ultrasound fusion transperineal prostate biopsy.
               
                  Materials and methods
                  Men with suspicion of prostate cancer underwent 3-Tesla multi-parametric MRI and were assigned a Prostate Imaging Reporting and Data System v2 score (PI-RADS). The prostate outline and suspicious lesions were marked by our radiologist using our software to produce a 3-dimensional prostate MRI model. All biopsies were performed under general anaesthesia and the real-time transrectal ultrasound model is created and subsequently fused with the MRI model using non-rigid software fusion. Transperineal targeted and systematic biopsy were then performed under stereotactic guidance using our robot-assisted prostate biopsy platform. Our clinically significant prostate cancer (Grade group ≥2) detection rates were previously described.
                        1
                     
                  
               
                  Results
                  Out of the 433 patients who underwent targeted and systematic biopsy, clinically-significant cancer detection rate was 46% (85% for PI- RADS 5 vs 38% for PI-RADS 4 vs 16% for PI-RADS 3; P < .001). Our overall complication rate was 13%, out of which the majority were Clavien-Dindo I (99%). The most common complications encountered were urinary retention (10%) and significant gross hematuria requiring bladder irrigation (2%). A higher prostate volume was associated with greater odds of urinary retention (OR 1.4, 95% CI: 1.21-1.65, P < .001 for every 10 mL increase in prostate volume). There was only 1 reported case of mild urinary tract infection.
               
                  Conclusion
                  Robot-assisted transperineal prostate biopsy has established itself as a reliable and accurate method of prostate cancer detection with minimal morbidity.",science
10.1016/j.jstrokecerebrovasdis.2021.105962,Journal,Journal of Stroke and Cerebrovascular Diseases,scopus,2021-09-01,sciencedirect,StrokeWatch: An Instrument for Objective Standardized Real-Time Measurement of Door-to-Needle Times in Acute Ischemic Stroke Treatment,https://api.elsevier.com/content/abstract/scopus_id/85109982805,"Objectives
                  Monitoring critical time intervals in acute ischemic stroke treatment delivers metrics for quality of performance – the door-to-needle time being well-established. To resolve the conflict of self-reporting bias a “StrokeWatch” was designed – an instrument for objective standardized real-time measurement of procedural times.
               
                  Materials and methods
                  An observational, monocentric analysis of patients receiving intravenous thrombolysis for acute ischemic stroke between January 2018 and September 2019 was performed based on an ongoing investigator-initiated, prospective, and blinded endpoint registry. Patient data and treatment intervals before and after introduction of ""StrokeWatch"" were compared.
               
                  Results
                  “StrokeWatch” was designed as a mobile board equipped with three digital stopwatches tracking door-to-needle, door-to-groin, and door-to-recanalization intervals as well as a form for standardized documentation. 118 patients before introduction of “StrokeWatch” (subgroup A) and 53 patients after introduction of “StrokeWatch” (subgroup B) were compared. There were no significant differences in baseline characteristics, procedural times, or clinical outcome. A non-significant increase in patients with door-to-needle intervals of 60 min or faster (93.2 vs 98.1%, p = 0.243) and good functional outcome (mRS d90 ≤ 2, 47.5 vs 58.5%, p = 0.218) as well as a significant increase in reports of delayed arrival of intra-hospital patient transport service (0.8 vs 13.2%, p = 0.001) were observed in subgroup B.
               
                  Conclusions
                  The implementation of StrokeWatch for objective standardized real-time measurement of door-to-needle times is feasible in a real-life setting without negative impact on procedural times or outcome. It helped to reassure a high-quality treatment standard and reveal factors associated with procedural delays.",science
10.1016/j.robot.2021.103830,Journal,Robotics and Autonomous Systems,scopus,2021-09-01,sciencedirect,Visual recognition of gymnastic exercise sequences. Application to supervision and robot learning by demonstration,https://api.elsevier.com/content/abstract/scopus_id/85109177424,"This work presents a novel software architecture to autonomously identify and evaluate the gymnastic activity that people are carrying out. It is composed of three different interconnected layers. The first corresponds to a Multilayer Perceptron (MLP) trained from a set of angular magnitudes derived from the information provided by the OpenPose library. This library works frame by frame, so some postures may be incorrectly detected due to eventual occlusions. The MLP layer makes it possible to accurately identify the posture a person is performing. A second layer, based on a Hidden Markov Model (HMM) and the Viterbi algorithm, filters the incorrect spurious postures. Thus, the accuracy of the algorithm is improved, leading to a precise sequence of postures. A third layer identifies the current exercise and evaluates whether the person is doing it at a correct speed. This layer uses an innovative Modified Levenshtein Distance (MLD), which considers not only the number of operations to transform a given sequence, but also the nature of the elements participating in the comparison. The system works in real time with little delay, thus recognizing sequences of arbitrary length and providing continuous feedback on the exercises being performed. An experiment carried out consisted in reproducing the output of the second layer on an autonomous Pepper robot that can be used in environments where physical exercise is performed, such as a residence for the elderly or others. It has reproduced different exercises previously executed by an instructor so that people can copy the robot. The article analyzes the current situation of the automated gymnastic activities recognition, presents the architecture, the different experiments carried out and the results obtained. The integration of the three components (MLP, HMM and MLD) results in a robust system that has allowed us to improve the results of previous works.",science
10.1016/j.sysarc.2021.102183,Journal,Journal of Systems Architecture,scopus,2021-09-01,sciencedirect,Memory-efficient deep learning inference with incremental weight loading and data layout reorganization on edge systems,https://api.elsevier.com/content/abstract/scopus_id/85107073021,"Pattern recognition applications such as face recognition and agricultural product detection have drawn a rapid interest on Cyber–Physical–Social-Systems (CPSS). These CPSS applications rely on the deep neural networks (DNN) to conduct the image classification. However, traditional DNN inference models in the cloud could suffer from network delay fluctuations and privacy leakage problems. In this regard, current real-time CPSS applications are preferred to be deployed on edge-end embedded devices. Constrained by the computing power and memory limitations of edge devices, improving the memory management efficacy is the key to improving the quality of service for model inference. First, this study explored the incremental loading strategy of model weights for the model inference. Second, the memory space at runtime is optimized through data layout reorganization from the spatial dimension. In particular, the proposed schemes are orthogonal to existing models. Experimental results demonstrate that the proposed approach reduced the memory consumption by 61.05% without additional inference time overhead.",science
10.1016/S2589-7500(21)00086-8,Journal,The Lancet Digital Health,scopus,2021-08-01,sciencedirect,Application of Comprehensive Artificial intelligence Retinal Expert (CARE) system: a national real-world evidence study,https://api.elsevier.com/content/abstract/scopus_id/85111153013,"Background
                  Medical artificial intelligence (AI) has entered the clinical implementation phase, although real-world performance of deep-learning systems (DLSs) for screening fundus disease remains unsatisfactory. Our study aimed to train a clinically applicable DLS for fundus diseases using data derived from the real world, and externally test the model using fundus photographs collected prospectively from the settings in which the model would most likely be adopted.
               
                  Methods
                  In this national real-world evidence study, we trained a DLS, the Comprehensive AI Retinal Expert (CARE) system, to identify the 14 most common retinal abnormalities using 207 228 colour fundus photographs derived from 16 clinical settings with different disease distributions. CARE was internally validated using 21 867 photographs and externally tested using 18 136 photographs prospectively collected from 35 real-world settings across China where CARE might be adopted, including eight tertiary hospitals, six community hospitals, and 21 physical examination centres. The performance of CARE was further compared with that of 16 ophthalmologists and tested using datasets with non-Chinese ethnicities and previously unused camera types. This study was registered with ClinicalTrials.gov, NCT04213430, and is currently closed.
               
                  Findings
                  The area under the receiver operating characteristic curve (AUC) in the internal validation set was 0·955 (SD 0·046). AUC values in the external test set were 0·965 (0·035) in tertiary hospitals, 0·983 (0·031) in community hospitals, and 0·953 (0·042) in physical examination centres. The performance of CARE was similar to that of ophthalmologists. Large variations in sensitivity were observed among the ophthalmologists in different regions and with varying experience. The system retained strong identification performance when tested using the non-Chinese dataset (AUC 0·960, 95% CI 0·957–0·964 in referable diabetic retinopathy).
               
                  Interpretation
                  Our DLS (CARE) showed satisfactory performance for screening multiple retinal abnormalities in real-world settings using prospectively collected fundus photographs, and so could allow the system to be implemented and adopted for clinical care.
               
                  Funding
                  This study was funded by the National Key R&D Programme of China, the Science and Technology Planning Projects of Guangdong Province, the National Natural Science Foundation of China, the Natural Science Foundation of Guangdong Province, and the Fundamental Research Funds for the Central Universities.
               
                  Translation
                  For the Chinese translation of the abstract see Supplementary Materials section.",science
10.1016/j.jbi.2021.103848,Journal,Journal of Biomedical Informatics,scopus,2021-08-01,sciencedirect,Face mask detection using deep learning: An approach to reduce risk of Coronavirus spread,https://api.elsevier.com/content/abstract/scopus_id/85109043381,"Effective strategies to restrain COVID-19 pandemic need high attention to mitigate negatively impacted communal health and global economy, with the brim-full horizon yet to unfold. In the absence of effective antiviral and limited medical resources, many measures are recommended by WHO to control the infection rate and avoid exhausting the limited medical resources. Wearing a mask is among the non-pharmaceutical intervention measures that can be used to cut the primary source of SARS-CoV2 droplets expelled by an infected individual. Regardless of discourse on medical resources and diversities in masks, all countries are mandating coverings over the nose and mouth in public. To contribute towards communal health, this paper aims to devise a highly accurate and real-time technique that can efficiently detect non-mask faces in public and thus, enforcing to wear mask. The proposed technique is ensemble of one-stage and two-stage detectors to achieve low inference time and high accuracy. We start with ResNet50 as a baseline and applied the concept of transfer learning to fuse high-level semantic information in multiple feature maps. In addition, we also propose a bounding box transformation to improve localization performance during mask detection. The experiment is conducted with three popular baseline models viz. ResNet50, AlexNet and MobileNet. We explored the possibility of these models to plug-in with the proposed model so that highly accurate results can be achieved in less inference time. It is observed that the proposed technique achieves high accuracy (98.2%) when implemented with ResNet50. Besides, the proposed model generates 11.07% and 6.44% higher precision and recall in mask detection when compared to the recent public baseline model published as RetinaFaceMask detector. The outstanding performance of the proposed model is highly suitable for video surveillance devices.",science
10.1016/j.ijdrr.2021.102397,Journal,International Journal of Disaster Risk Reduction,scopus,2021-08-01,sciencedirect,Twitter chirps for Syrian people: Sentiment analysis of tweets related to Syria Chemical Attack,https://api.elsevier.com/content/abstract/scopus_id/85108873548,"Purpose
                  The sentiment analysis of tweets provides information about peoples’ attitudes and perceptions towards an event. The current study showcases the role of Twitter in a crisis by analyzing the nature of tweets and the sentiments expressed by the Twitter-sphere during and after the “Khan Shaykhun Syria Chemical Attack.""
               
                  Methodology
                  A total of 13,156 tweets posted in English on Twitter during the first 27 days of the attack were downloaded and considered for the study. The content analysis of the tweets was done manually, and accordingly, the sentiments of the tweets were highlighted through eight broader categories. Furthermore, to visualize the positive, negative, and neutral sentiments of the tweets, the Orange Data 
                     M
                     ining Software, a powerful toolkit for machine learning, data mining, and data visualization, was used. VOSviewer (a software tool used for creating maps based on network data and for visualizing and exploring the maps) was also used to visualize the word frequency of the tweets.
               
                  Findings
                  Twitter is primarily used for situational awareness and acts as an emotional, social support system by sharing sentiments. 35.71% of the tweets are associated with ""
                     sharing news and information"" , with just 2.12% ""
                     supporting the government"". People mostly retweet the tweets that “criticize the government,” with an average retweet count of 15.84, followed by the ones “evincing emotions” (12.21). However, tweets that “raise questions” (3.32) and “provide suggestions” (2.51) fail to gain the attention of too many tweeter users, thus having less impact. People mostly like the tweets that “
                     support 
                     government” and “evince emotions,” with such tweets on an average receiving 9.89 and 8.37 likes, respectively. Individuals post a large number of tweets (10,137; 77.05%), followed by news channels (1157; 8.79%) and organizations of varied nature (950; 7.22%). However, 912 (6.93%) tweets are posted by users of anonymous nature. Text and text with images form most tweets contributing to 8061 (61.27%) and 3137 (23.84%) of the total tweet count. However, none of the tweets contain video only, and just 3 (0.02%) tweets embed only images. Text-video and text-image formats are highly re-tweeted and liked. It is evident that 53.70% of the tweets (n = 7065) reflect negative sentiments, while 12.67% (n = 1667) emulate positive sentiments and, 33.63% (n = 4424) showcase a neutral perception about the attack. One can visualize the U.S.A. among the top tweeting countries with the highest percentage of positive sentiments, followed by Canada and Israel. Turkey outscores all the countries in terms of negative tweets, followed by Syria and U.K. However, in terms of neutral tweets, Germany ranks first, followed by Iran and Canada. The tweets pour mainly for the first few days, indicating the concern of users for the victims. Later on, a declining trend of tweets is witnessed. “idlib”, “Syria”, “Syria chemical attack,” and “Assad” are the leading words used more than a thousand times in the tweets.
               
                  Research implications
                  The current study adds to the growing body of knowledge to the existing literature on Twitter and its use to narrowcast situational awareness during crisis episodes. One of the implications of the study is that the news agencies highly exploit the sharing side of Twitter during disasters by communicating real-time and unique information, create situational awareness, and connect to the digital audience. Twitter acts as an emotional outlet that facilitates the mining of condensed varied reactions towards an event to frame disaster response strategies and provides a sociological understanding of social media use during the crisis by the victims and viewers.
               
                  Originality
                  The study represents the sentiments of the Twitter-sphere towards the “Syria Chemical Attack.""",science
10.1016/j.bbrc.2020.10.077,Journal,Biochemical and Biophysical Research Communications,scopus,2021-07-30,sciencedirect,"Life, death, and self: Fundamental questions of primitive cognition viewed through the lens of body plasticity and synthetic organisms",https://api.elsevier.com/content/abstract/scopus_id/85095615733,"Central to the study of cognition is being able to specify the Subject that is making decisions and owning memories and preferences. However, all real cognitive agents are made of parts (such as brains made of cells). The integration of many active subunits into a coherent Self appearing at a larger scale of organization is one of the fundamental questions of evolutionary cognitive science. Typical biological model systems, whether basal or advanced, have a static anatomical structure which obscures important aspects of the mind-body relationship. Recent advances in bioengineering now make it possible to assemble, disassemble, and recombine biological structures at the cell, organ, and whole organism levels. Regenerative biology and controlled chimerism reveal that studies of cognition in intact, “standard”, evolved animal bodies are just a narrow slice of a much bigger and as-yet largely unexplored reality: the incredible plasticity of dynamic morphogenesis of biological forms that house and support diverse types of cognition. The ability to produce living organisms in novel configurations makes clear that traditional concepts, such as body, organism, genetic lineage, death, and memory are not as well-defined as commonly thought, and need considerable revision to account for the possible spectrum of living entities. Here, I review fascinating examples of experimental biology illustrating that the boundaries demarcating somatic and cognitive Selves are fluid, providing an opportunity to sharpen inquiries about how evolution exploits physical forces for multi-scale cognition. Developmental (pre-neural) bioelectricity contributes a novel perspective on how the dynamic control of growth and form of the body evolved into sophisticated cognitive capabilities. Most importantly, the development of functional biobots – synthetic living machines with behavioral capacity – provides a roadmap for greatly expanding our understanding of the origin and capacities of cognition in all of its possible material implementations, especially those that emerge de novo, with no lengthy evolutionary history of matching behavioral programs to bodyplan. Viewing fundamental questions through the lens of new, constructed living forms will have diverse impacts, not only in basic evolutionary biology and cognitive science, but also in regenerative medicine of the brain and in artificial intelligence.",science
10.1016/j.chemolab.2021.104329,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2021-07-15,sciencedirect,A novel approach for water quality classification based on the integration of deep learning and feature extraction techniques,https://api.elsevier.com/content/abstract/scopus_id/85105292476,"Water quality monitoring plays a vital role in the protection of water resources, environmental management, and decision-making. Artificial intelligence (AI) based on machine learning techniques has been widely used to evaluate and classify water quality for the last two decades. However, traditional machine learning techniques face many limitations, the most important of which is the inability to apply these techniques with big data generated by smart water quality monitoring stations to improve the prediction. Real-time water quality monitoring with high accuracy and efficiency for intelligent water quality monitoring stations requires new and sophisticated techniques based on machine and deep learning techniques. For this purpose, we propose a novel approach based on the integration of deep learning and feature extraction techniques to improve water quality classification. In this paper, was chosen the Tilesdit dam in Bouira (Algeria) as a case study. Moreover, we implemented the advanced deep learning method - Long Short Term Memory Recurrent Neural Networks (LSTM RNNs) to construct an intelligent model for drinking water quality classification. Furthermore, principal component analysis (PCA), linear discriminant analysis (LDA) and independent component analysis (ICA) techniques were used for features extraction and data reduction from original features. Additionally, we used three methods of cross-validation and two methods of the out-of-sample test to estimate the performance of LSTM RNNs model. From the results we found that the integration of LSTM RNNs with LDA, and LSTM RNNs with ICA yields an accuracy of 99.72%, using Random-Holdout technique.",science
10.1016/j.procs.2021.06.077,Conference Proceeding,Procedia Computer Science,scopus,2021-07-01,sciencedirect,Digital transformation as a new paradigm of economic policy,https://api.elsevier.com/content/abstract/scopus_id/85112599973,"This study analyzes the conceptual provisions related to solving problems related to the introduction of digital technologies and the formation of a digital economy based on them, reveals the dynamics of digital transformation and its impact on business processes and the interaction of states, business and civil society in the context of modern economic policy. We reviewed the policy of the Russian state in terms of overcoming both the existing and potential economic consequences of the COVID-19 pandemic based on published expert assessments. Our results confirmed that overcoming the current turbulent state of the digital economy in Russia requires: firstly, the development of digital entrepreneurship or the digital sector as the “core” of the digital economy, where digital technologies are created; secondly, the removal of restrictions on the movement of resources caused by the COVID-19 pandemic, as a result of the consistent implementation of a coordinated strategy for the digitalization of the economy, based on global cooperation in the field of economic policy; third, the process of reproduction of the social product, where production - distribution – exchange - consumption interact, should take place at the level of world standards; fourth, to introduce the “digital style”in economic policy through building technological chains and diversified connections; fifth, to develop artificial intelligence, the essence of which is to“break” the matrix of habitual life in order to launch a large-scale virtual program of a new being of humanity, spurred by the COVID-19 pandemic.",science
10.1016/j.softx.2021.100773,Journal,SoftwareX,scopus,2021-07-01,sciencedirect,NetCausality: A time-delayed neural network tool for causality detection and analysis,https://api.elsevier.com/content/abstract/scopus_id/85111857161,"The analysis of causality between systems is still an important research activity, which finds application in several fields of science. The software presented is a new tool for causality detection and analysis between time series. The proposed technique is based on time-delayed neural networks (TDNN). The tool is developed in MATLAB and it comprises three main functions. The first one returns the total causality between two or more systems of equations. The second tool is used to find the “time horizon”, id est the time delay at which the influence between the systems occurs. The last function is a causality feature detection to determine the time intervals, in which the mutual coupling is sufficiently strong to have a real influence on the target.",science
10.1016/j.ebiom.2021.103465,Journal,EBioMedicine,scopus,2021-07-01,sciencedirect,A mass spectrometry-based targeted assay for detection of SARS-CoV-2 antigen from clinical specimens,https://api.elsevier.com/content/abstract/scopus_id/85109005451,"Background
                  The COVID-19 pandemic caused by severe acute respiratory syndrome-coronavirus 2 (SARS-CoV-2) has overwhelmed health systems worldwide and highlighted limitations of diagnostic testing. Several types of diagnostic tests including RT-PCR-based assays and antigen detection by lateral flow assays, each with their own strengths and weaknesses, have been developed and deployed in a short time.
               
                  Methods
                  Here, we describe an immunoaffinity purification approach followed a by high resolution mass spectrometry-based targeted qualitative assay capable of detecting SARS-CoV-2 viral antigen from nasopharyngeal swab samples. Based on our discovery experiments using purified virus, recombinant viral protein and nasopharyngeal swab samples from COVID-19 positive patients, nucleocapsid protein was selected as a target antigen. We then developed an automated antibody capture-based workflow coupled to targeted high-field asymmetric waveform ion mobility spectrometry (FAIMS) - parallel reaction monitoring (PRM) assay on an Orbitrap Exploris 480 mass spectrometer. An ensemble machine learning-based model for determining COVID-19 positive samples was developed using fragment ion intensities from the PRM data.
               
                  Findings
                  The optimized targeted assay, which was used to analyze 88 positive and 88 negative nasopharyngeal swab samples for validation, resulted in 98% (95% CI = 0.922–0.997) (86/88) sensitivity and 100% (95% CI = 0.958–1.000) (88/88) specificity using RT-PCR-based molecular testing as the reference method.
               
                  Interpretation
                  Our results demonstrate that direct detection of infectious agents from clinical samples by tandem mass spectrometry-based assays have potential to be deployed as diagnostic assays in clinical laboratories, which has hitherto been limited to analysis of pure microbial cultures.
               
                  Funding
                  This study was supported by DBT/Wellcome Trust India Alliance Margdarshi Fellowship grant IA/M/15/1/502023 awarded to AP and the generosity of Eric and Wendy Schmidt.",science
10.1016/j.jstrokecerebrovasdis.2021.105826,Journal,Journal of Stroke and Cerebrovascular Diseases,scopus,2021-07-01,sciencedirect,Automatic Acute Stroke Symptom Detection and Emergency Medical Systems Alerting by Mobile Health Technologies: A Review,https://api.elsevier.com/content/abstract/scopus_id/85107711467,"Objectives
                  To survey recent advances in acute stroke symptom automatic detection and Emergency Medical Systems (EMS) alerting by mobile health technologies.
               
                  Materials and methods
                  Narrative review
               
                  Results
                  Delayed activation of EMS for stroke symptoms by patients and witnesses deprives patients of rapid access to brain-saving therapies and occurs due to public unawareness of stroke features, cognitive and motor deficits produced by the stroke itself, and sleep onset. A promising emerging approach to overcoming the inherent biologic constraints of patient capacity to self-detect and respond to stroke symptoms is continuous monitoring by mobile health technologies with wireless sensors and artificial intelligence recognition systems. This review surveys 11 sensing technologies - accelerometers, gyroscopes, magnetometers, pressure sensors, touch screen and keyboard input detectors, artificial vision, and artificial hearing; and 10 consumer device form factors in which they are increasingly implemented: smartphones, smart speakers, smart watches and fitness bands, smart speakers/voice assistants, home health robots, smart clothing, smart beds, closed circuit television, smart rings, and desktop/laptop/tablet computers.
               
                  Conclusions
                  The increase in computing power, wearable sensors, and mobile connectivity have ushered in an array of mobile health technologies that can transform stroke detection and EMS activation. By continuously monitoring a diverse range of biometric parameters, commercially available devices provide the technologic capability to detect cardinal language, motor, gait, and sensory signs of stroke onset. Intensified translational research to convert the promise of these technologies to validated, accurate real-world deployments are an important next priority for stroke investigation.",science
10.1016/j.cmpb.2021.106130,Journal,Computer Methods and Programs in Biomedicine,scopus,2021-07-01,sciencedirect,"Chest x-ray automated triage: A semiologic approach designed for clinical implementation, exploiting different types of labels through a combination of four Deep Learning architectures",https://api.elsevier.com/content/abstract/scopus_id/85107617149,"Background and objectives
                  The multiple chest x-ray datasets released in the last years have ground-truth labels intended for different computer vision tasks, suggesting that performance in automated chest x-ray interpretation might improve by using a method that can exploit diverse types of annotations. This work presents a Deep Learning method based on the late fusion of different convolutional architectures, that allows training with heterogeneous data with a simple implementation, and evaluates its performance on independent test data. We focused on obtaining a clinically useful tool that could be successfully integrated into a hospital workflow.
               
                  Materials and methods
                  Based on expert opinion, we selected four target chest x-ray findings, namely lung opacities, fractures, pneumothorax and pleural effusion. For each finding we defined the most suitable type of ground-truth label, and built four training datasets combining images from public chest x-ray datasets and our institutional archive. We trained four different Deep Learning architectures and combined their outputs with a late fusion strategy, obtaining a unified tool. The performance was measured on two test datasets: an external openly-available dataset, and a retrospective institutional dataset, to estimate performance on the local population.
               
                  Results
                  The external and local test sets had 4376 and 1064 images, respectively, for which the model showed an area under the Receiver Operating Characteristics curve of 0.75 (95%CI: 0.74–0.76) and 0.87 (95%CI: 0.86–0.89) in the detection of abnormal chest x-rays. For the local population, a sensitivity of 86% (95%CI: 84–90), and a specificity of 88% (95%CI: 86–90) were obtained, with no significant differences between demographic subgroups. We present examples of heatmaps to show the accomplished level of interpretability, examining true and false positives.
               
                  Conclusion
                  This study presents a new approach for exploiting heterogeneous labels from different chest x-ray datasets, by choosing Deep Learning architectures according to the radiological characteristics of each pathological finding. We estimated the tool's performance on the local population, obtaining results comparable to state-of-the-art metrics. We believe this approach is closer to the actual reading process of chest x-rays by professionals, and therefore more likely to be successful in a real clinical setting.",science
10.1016/j.jchromb.2021.122760,Journal,Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences,scopus,2021-07-01,sciencedirect,Rapid exposure monitoring of six bisphenols and diethylstilbestrol in human urine using fabric phase sorptive extraction followed by high performance liquid chromatography – photodiode array analysis,https://api.elsevier.com/content/abstract/scopus_id/85106648971,"A novel fabric phase sorptive extraction protocol is developed for rapid exposure monitoring of six bisphenol analogues, including bisphenol A, bisphenol S, bisphenol F, bisphenol E, bisphenol B, bisphenol C, and diethylstilbestrol (DES) from human urine prior to high-performance liquid chromatography-photodiode array analysis.
                  FPSE sample pretreatment protocol ensures the harmonization of the proposed method with the principles of Green Analytical Chemistry (GAC). Among eighteen evaluated FPSE membranes, sol-gel poly (ethylene glycol) (PEG) coated cellulose FPSE membrane resulted in the most efficient extraction. This polar FPSE membrane effectively exploits a number of advantageous features inherent to FPSE including sponge-like porous architecture of the sol-gel sorbent coating, favorable surface chemistry, flexibility and built-in permeability of cellulose fabric substrate, high primary contact surface area for rapid sorbent-analyte interaction, expanded pH, solvent and thermal stability as well as reusability of the FPSE membrane.
                  Optimization was centered on the evaluation of critical parameters, namely the size of the FPSE membrane, the elution solvent mixture, the volume of the sample, the extraction time, the elution time, the kind of the external agitation mechanical stimulus, the ionic strength and the pH of the sample. The chromatographic separation was achieved on a Spherisorb C18 column and a gradient elution program with mobile phase consisted of 0.05 ammonium acetate solution and acetonitrile. The total analysis time was 17.4 min. The developed method was validated in terms of linearity, sensitivity, selectivity, precision, accuracy, stability, and ruggedness. The limits of detection and quantification varied from 0.26–0.62 ng/mL and 0.8–1.9 ng/mL, respectively. The relative recoveries were calculated between 90.6 and 108.8%, while the RSD values were <10% in all cases. The effectiveness of the proposed method was confirmed by its successful implementation in the bioanalysis of real urine samples.",science
10.1016/j.measurement.2021.109528,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2021-07-01,sciencedirect,A molecular sensing method integrated with support vector machines to characterize asphalt mixtures,https://api.elsevier.com/content/abstract/scopus_id/85105846445,"Modern and heterogeneous asphalt mixtures are usually produced using various kinds of modifiers such as rubber, polymer, and fiber. These materials are incorporated to improve sustainability and reduce the extent and severity of distresses such as rutting and low-temperature cracking. Currently, there is a lack of a robust real-time method for the identification of these additives in mixtures. In this research, a portable molecular sensing technology is integrated with machine learning (ML) to characterize asphalt binders modified with ground tire rubber (GTR) and asphalt mixtures containing different amounts of recycled materials. A database containing several near-infrared (NIR) spectra for binder and asphalt mixture samples are used to develop the ML-based detection models. The acceptable accuracy reported in this study implies that the proposed integrated NIR and ML approach can be used as a promising tool to differentiate and classify various types of asphalt binders and mixtures. This monitoring and data collection framework can contribute to improved sustainability via accelerating and optimizing the construction material detection and selection process throughout the pavement life. Furthermore, the expensive and cumbersome process of binder extraction and recovery could be avoided using the proposed method.",science
10.1016/j.compbiomed.2021.104450,Journal,Computers in Biology and Medicine,scopus,2021-07-01,sciencedirect,A comprehensive review and analysis of supervised-learning and soft computing techniques for stress diagnosis in humans,https://api.elsevier.com/content/abstract/scopus_id/85105598718,"Stress is the most prevailing and global psychological condition that inevitably disrupts the mood and behavior of individuals. Chronic stress may gravely affect the physical, mental, and social behavior of victims and consequently induce myriad critical human disorders. Herein, a review has been presented where supervised learning (SL) and soft computing (SC) techniques used in stress diagnosis have been meticulously investigated to highlight the contributions, strengths, and challenges faced in the implementation of these methods in stress diagnostic models. A three-tier review strategy comprising of manuscript selection, data synthesis, and data analysis was adopted. The issues in SL strategies and the potential possibility of using hybrid techniques in stress diagnosis have been intensively investigated. The strengths and weaknesses of different SL (Bayesian classifier, random forest, support vector machine, and nearest neighbours) and SC (fuzzy logic, nature-inspired, and deep learning) techniques have been presented to obtain clear insights into these optimization strategies. The effects of social, behavioral, and biological stresses have been highlighted. The psychological, biological, and behavioral responses to stress have also been briefly elucidated. The findings of the study confirmed that different types of data/signals (related to skin temperature, electro-dermal activity, blood circulation, heart rate, facial expressions, etc.) have been used in stress diagnosis. Moreover, there is a potential scope for using distinct nature-inspired computing techniques (Genetic Algorithm, Particle Swarm Optimization, Ant Colony Optimization, Whale Optimization Algorithm, Butterfly Optimization, Harris Hawks Optimizer, and Crow Search Algorithm) and deep learning techniques (Deep-Belief Network, Convolutional-Neural Network, and Recurrent-Neural Network) on multimodal data compiled using behavioral testing, electroencephalogram signals, finger temperature, respiration rate, pupil diameter, galvanic-skin-response, and blood pressure. Likewise, there is a wider scope to investigate the use of SL and SC techniques in stress diagnosis using distinct dimensions such as sentiment analysis, speech recognition, handwriting recognition, and facial expressions. Finally, a hybrid model based on distinct computational methods influenced by both SL and SC techniques, adaption, parameter tuning, and the use of chaos, levy, and Gaussian distribution may address exploration and exploitation issues. However, factors such as real-time data collection, bias, integrity, multi-dimensional data, and data privacy make it challenging to design precise and innovative stress diagnostic systems based on artificial intelligence.",science
10.1016/j.compbiomed.2021.104448,Journal,Computers in Biology and Medicine,scopus,2021-07-01,sciencedirect,High precision in microRNA prediction: A novel genome-wide approach with convolutional deep residual networks,https://api.elsevier.com/content/abstract/scopus_id/85105586824,"MicroRNAs (miRNAs) are small non-coding RNAs that have a key role in the regulation of gene expression. The importance of miRNAs is widely acknowledged by the community nowadays and computational methods are needed for the precise prediction of novel candidates to miRNA. This task can be done by searching homologous with sequence alignment tools, but results are restricted to sequences that are very similar to the known miRNA precursors (pre-miRNAs). Besides, a very important property of pre-miRNAs, their secondary structure, is not taken into account by these methods. To fill this gap, many machine learning approaches were proposed in the last years. However, the methods are generally tested in very controlled conditions. If these methods were used under real conditions, the false positives increase and the precisions fall quite below those published. This work provides a novel approach for dealing with the computational prediction of pre-miRNAs: a convolutional deep residual neural network (mirDNN). This model was tested with several genomes of animals and plants, the full-genomes, achieving a precision up to 5 times larger than other approaches at the same recall rates. Furthermore, a novel validation methodology was used to ensure that the performance reported in this study can be effectively achieved when using mirDNN in novel species. To provide fast an easy access to mirDNN, a web demo is available at http://sinc.unl.edu.ar/web-demo/mirdnn/. The demo can process FASTA files with multiple sequences to calculate the prediction scores and generates the nucleotide importance plots.
               
                  Full source code
                  
                     http://sourceforge.net/projects/sourcesinc/files/mirdnn and https://github.com/cyones/mirDNN.
               
                  Contact
                  
                     gstegmayer@sinc.unl.edu.ar.",science
10.1016/j.rser.2021.110969,Journal,Renewable and Sustainable Energy Reviews,scopus,2021-07-01,sciencedirect,Intelligent building control systems for thermal comfort and energy-efficiency: A systematic review of artificial intelligence-assisted techniques,https://api.elsevier.com/content/abstract/scopus_id/85103719088,"Building operations represent a significant percentage of the total primary energy consumed in most countries due to the proliferation of Heating, Ventilation and Air-Conditioning (HVAC) installations in response to the growing demand for improved thermal comfort. Reducing the associated energy consumption while maintaining comfortable conditions in buildings are conflicting objectives and represent a typical optimization problem that requires intelligent system design. Over the last decade, different methodologies based on the Artificial Intelligence (AI) techniques have been deployed to find the sweet spot between energy use in HVAC systems and suitable indoor comfort levels to the occupants. This paper performs a comprehensive and an in-depth systematic review of AI-based techniques used for building control systems by assessing the outputs of these techniques, and their implementations in the reviewed works, as well as investigating their abilities to improve the energy-efficiency, while maintaining thermal comfort conditions. This enables a holistic view of (1) the complexities of delivering thermal comfort to users inside buildings in an energy-efficient way, and (2) the associated bibliographic material to assist researchers and experts in the field in tackling such a challenge. Among the 20 AI tools developed for both energy consumption and comfort control, functions such as identification and recognition patterns, optimization, predictive control. Based on the findings of this work, the application of AI technology in building control is a promising area of research and still an ongoing, i.e., the performance of AI-based control is not yet completely satisfactory. This is mainly due in part to the fact that these algorithms usually need a large amount of high-quality real-world data, which is lacking in the building or, more precisely, the energy sector. Based on the current study, from 1993 to 2020, the application of AI techniques and personalized comfort models has enabled energy savings on average between 21.81 and 44.36%, and comfort improvement on average between 21.67 and 85.77%. Finally, this paper discusses the challenges faced in the use of AI for energy productivity and comfort improvement, and opens main future directions in relation with AI-based building control systems for human comfort and energy-efficiency management.",science
10.1016/j.ins.2021.01.013,Journal,Information Sciences,scopus,2021-07-01,sciencedirect,Attributed community search based on effective scoring function and elastic greedy method,https://api.elsevier.com/content/abstract/scopus_id/85101624086,"In recent years, with the proliferation of rich attribute information available for entities in real-world networks and the increasing demand for more personalized community searches, attributed community search (ACS), an upgraded version of the community search problem, has attracted great attention from the both academic and industry areas. Some algorithms have been proposed to solve this novel research problem. However, they have a deficiency in evaluating the quality of the attributed community structure, which may mislead them and discover less valuable structures. In this paper, we make up for this defect, and propose the SFEG algorithm to better solve the ACS problem. SFEG designs a more effective scoring function to measure the quality of the discovered attributed community structure, and presents an elastic greedy optimization method to quickly maximize the function value to determine the target community with a specific meaning. The extensive experiments conducted on the attributed graph datasets with ground-truth communities show that our algorithm significantly outperforms the state-of-the-art.",science
10.1016/j.tele.2021.101583,Journal,Telematics and Informatics,scopus,2021-07-01,sciencedirect,Improving evidence-based assessment of players using serious games,https://api.elsevier.com/content/abstract/scopus_id/85101178082,"Serious games are highly interactive systems which can therefore capture large amounts of player interaction data. This data can be analyzed to provide a deep insight into the effect of the game on its players. However, traditional techniques to assess players of serious games make little use of interaction data, relying instead on costly external questionnaires. We propose an evidence-based process to improve the assessment of players by using their interaction data. The process first combines player interaction data and traditional questionnaires to derive and refine game learning analytics variables, which can then be used to predict the effects of the game on its players. Once the game is validated, and suitable prediction models have been built, the prediction models can be used in large-scale deployments to assess players solely based on their interactions, without the need for external questionnaires. We briefly describe two case studies where this combination of traditional questionnaires and data mining techniques has been successfully applied. The evidence-based assessment process proposed radically simplifies the deployment and application of serious games in real class settings.",science
10.1016/j.chemolab.2021.104314,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2021-06-15,sciencedirect,A scalable approach for the efficient segmentation of hyperspectral images,https://api.elsevier.com/content/abstract/scopus_id/85105360467,"The number of applications of hyperspectral imaging (HSI) is steadily increasing, as technology evolves and cameras become more affordable. However, the volume of data in a hyperspectral image is large (order of Gigabytes) and standard off-the-shelf algorithms for multi-channel image analysis cannot be readily applied, due to the prohibitive computational time and large memory requirements. Therefore, new scalable approaches are required to perform hyperspectral image analysis. In this article we address an efficient methodology for conducting Unsupervised Image Segmentation – one of the basic and most fundamental image analysis operations. In the methodology proposed, unsupervised segmentation is conducted after transforming the spectral and spatial dimensions of the raw hyperspectral image into a more compact representation using multivariate and multiresolution techniques. The clusters identified in the compact image representation are then used to train a discriminative classifier. The classifier is then adapted and transferred for application to the raw image, where it will efficiently label all the original pixels. With the proposed methodology, the computational expensive operations (unsupervised clustering and classifier learning) are minimized, whereas the efficient implementation of the classifier guarantees the analysis at the native resolution. The effectiveness of the proposed methodology was tested on a real case study considering an industrial hyperspectral image capturing the reflectance spectrum for several objects made of different unknown materials. A significant reduction in the computational cost was achieved without compromising the quality of the unsupervised segmentation, demonstrating the potential of the proposed approach.",science
10.1016/j.chemolab.2021.104325,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2021-06-15,sciencedirect,Extended Gaussian mixture regression for forward and inverse analysis,https://api.elsevier.com/content/abstract/scopus_id/85104927275,"In molecular, material, and process designs, it is important to perform inverse analysis of the regression models constructed with machine learning using target values of the properties and activities. Although many approaches actually employ a pseudo-inverse analysis, Gaussian mixture regression (GMR) can achieve direct inverse analysis. This paper describes the development and use of extended GMR (EGMR), which offers improved predictive ability over conventional GMR. EGMR includes implementations of both GMR and Bayesian GMR, which is based on a variational Bayesian method. The hyperparameters for each model are optimized, and the choice of model for the specific data is determined, through cross-validation. The effectiveness of the proposed EGMR is verified using numerically simulated datasets, compound datasets, a material dataset, and spectral datasets. These datasets contain real data. The predictive ability of EGMR is found to be greater than or equal to that of GMR in all cases, and the prediction errors can be reduced by more than 30%. Furthermore, it is confirmed that EGMR can perform inverse analysis with high reproducibility, even in the extrapolation region of an objective variable. The Python code for EGMR is available at https://github.com/hkaneko1985/dcekit.",science
10.1016/j.dib.2021.107127,Journal,Data in Brief,scopus,2021-06-01,sciencedirect,H2020 project CAPTOR dataset: Raw data collected by low-cost MOX ozone sensors in a real air pollution monitoring network,https://api.elsevier.com/content/abstract/scopus_id/85106384559,"The H2020 CAPTOR project deployed three testbeds in Spain, Italy and Austria with low-cost sensors for the measurement of tropospheric ozone (O3). The aim of the H2020 CAPTOR project was to raise public awareness in a project focused on citizen science. Each testbed was supported by an NGO in charge of deciding how to raise citizen awareness according to the needs of each country. The data presented in this document correspond to the raw data captured by the sensor nodes in the Spanish testbed using SGX Sensortech MICS 2614 metal-oxide sensors. The Spanish testbed consisted of the deployment of twenty-five nodes. Each sensor node included four SGX Sensortech MICS 2614 ozone sensors, one temperature sensor and one relative humidity sensor. Each node underwent a calibration process by co-locating the node at an EU reference air quality monitoring station, followed by a deployment in a sub-urban or rural area in Catalonia, Spain. All nodes spent two to three weeks co-located at a reference station in Barcelona, Spain (urban area), followed by two to three weeks co-located at three sub-urban reference stations near the final deployment site. The nodes were then deployed in volunteers' homes for about two months and, finally, the nodes were co-located again at the sub-urban reference stations for two weeks for final calibration and assessment of potential drifts. All data presented in this paper are raw data taken by the sensors that can be used for scientific purposes such as calibration studies using machine learning algorithms, or once the concentration values of the nodes are obtained, they can be used to create tropospheric ozone pollution maps with heterogeneous data sources (reference stations and low-cost sensors).",science
10.1016/S2589-7500(21)00005-4,Journal,The Lancet Digital Health,scopus,2021-06-01,sciencedirect,Health information technology and digital innovation for national learning health and care systems,https://api.elsevier.com/content/abstract/scopus_id/85106359380,"Health information technology can support the development of national learning health and care systems, which can be defined as health and care systems that continuously use data-enabled infrastructure to support policy and planning, public health, and personalisation of care. The COVID-19 pandemic has offered an opportunity to assess how well equipped the UK is to leverage health information technology and apply the principles of a national learning health and care system in response to a major public health shock. With the experience acquired during the pandemic, each country within the UK should now re-evaluate their digital health and care strategies. After leaving the EU, UK countries now need to decide to what extent they wish to engage with European efforts to promote interoperability between electronic health records. Major priorities for strengthening health information technology in the UK include achieving the optimal balance between top-down and bottom-up implementation, improving usability and interoperability, developing capacity for handling, processing, and analysing data, addressing privacy and security concerns, and encouraging digital inclusivity. Current and future opportunities include integrating electronic health records across health and care providers, investing in health data science research, generating real-world data, developing artificial intelligence and robotics, and facilitating public–private partnerships. Many ethical challenges and unintended consequences of implementation of health information technology exist. To address these, there is a need to develop regulatory frameworks for the development, management, and procurement of artificial intelligence and health information technology systems, create public–private partnerships, and ethically and safely apply artificial intelligence in the National Health Service.",science
10.1016/j.comcom.2021.04.026,Journal,Computer Communications,scopus,2021-06-01,sciencedirect,Adversarial attacks on a lexical sentiment analysis classifier,https://api.elsevier.com/content/abstract/scopus_id/85105035567,"Social media has become a relevant information source for several decision-making processes and for the definition of business strategies. As various sentiment analysis techniques are used to transform collected data into intelligence information, the sentiment classifiers used in these collection environments must be carefully studied and observed before being considered trustful and ready to be installed in decision support systems. An important research area concerns the robustness of sentiment classifiers in view of new adversarial attacks, in which small perturbations may be created by malicious users to deceive the sentiment classifiers, generating a perception different from the one that should be observed in the environment. Thus, it is important to identify and analyze the vulnerabilities of these classifiers under different strategies of adversarial attacks to propose countermeasures that can be used to mitigate such attacks. In this context, this work presents adversarial attacks related to a lexical natural language classifier. Being the target of the attacks, this classifier is used to calculate the sentiment of collected data as posted by users in various social media applications. The results indicate that the found vulnerabilities, if exploited by malicious users in applications that use the same lexical classifier, could invert or cancel the classifiers’ perception, thus generating perceptions that do not correspond to the reality for decision making. This work also proposes some countermeasures that might mitigate the implemented attacks.",science
10.1016/j.scs.2021.102801,Journal,Sustainable Cities and Society,scopus,2021-06-01,sciencedirect,"Smart and sustainable logistics of Port cities: A framework for comprehending enabling factors, domains and goals",https://api.elsevier.com/content/abstract/scopus_id/85104756448,"Digital technologies integrated into port logistics are becoming increasingly decisive among port cities around the world. This growing importance is due to the need for policymakers, urban managers, port authorities, local administrators, shipping companies, couriers, and so on to develop increasingly digitalized and sustainable logistic processes. Therefore, in a global context characterized by intense datafication and globalization of trade, the data-based approach has become a necessary modus operandi to promote smart and sustainable logistics development. This forward-looking model of port logistics uses technologies such as IoT, sensors, cloud computing platforms, Big Data analytics, Artificial Intelligence (AI), GPS tracking systems, radars, drones, real-time monitoring stations, smart grids, and so on in order to collect, process, monitor and analyse data and information concerning the economic, environmental, social and technological sphere of port cities. In this sense, mobile and fixed platforms help logistics operators to optimize the management of flows (e.g., water, waste, emissions, raw materials, people, monetary investments, etc.) in an efficient and digitized manner. The study proposes a systematic literature review of the most recurring themes concerning smart and sustainable logistics initiatives within port cities in order to develop a multidimensional framework capable of holistically integrating the prevailing enabling factors (Ecosystem, Internal Organization, Data and Security, Policy and Regulation, Finance and Funding, and Digital and Technology), domains (Mobility, Environment, Economy, Telecommunications, Safety and Security, Government, and Community) and goals (Sustainable Development and Digitalization) that characterize smart and sustainable logistical development. To this end, the best practices of several pioneering port cities such as Rotterdam, Hamburg, Singapore, Los Angeles, Amsterdam, etc. implemented in partnerships with technology companies such as Cisco, IBM, Huawei and SAP were also analysed. Therefore, the results of this research show that smart and sustainable logistics initiatives in port cities: (a) have the potential to enhance the efficiency of the economic, environmental, social and technological flows; (b) increase the involvement and awareness of stakeholders such as couriers, shippers, shipping companies, citizens, port authorities, municipalities, security officers, gate and terminal personnel, and so on; and (c) provide a detailed overview of the enabling factors, domains and goals that must be activated by port cities to foster a smart and sustainable logistic transition.",science
10.1016/j.sysarc.2021.102139,Journal,Journal of Systems Architecture,scopus,2021-06-01,sciencedirect,Tracking and analysing social interactions in dairy cattle with real-time locating system and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85104583225,"There is a need for reliable and efficient methods for monitoring the activity and social behaviour in cows, in order to optimise management in modern dairy farms. This research presents an embedded system that could track individual cows using Ultra-wideband technology. At the same time, social interactions between individuals around the feeding area were analysed with a computer vision module. Detections of the dairy cows’ negative and positive interactions were performed on foreground video stream using a Long-term Recurrent Convolution Networks model. The sensor fusion system was implemented and tested on seven dairy cows during 45 days in an experimental dairy farm. The system performance was evaluated at the feeding area. The real-time locating system based on Ultra-wideband technology reached an accuracy with mean error 0.39 m and standard deviation 0.62 m. The accuracy of detecting the affiliative and agonistic social interactions reached 93.2%. This study demonstrates a potential system for monitoring social interactions between dairy cows.",science
10.1016/j.radonc.2021.03.032,Journal,Radiotherapy and Oncology,scopus,2021-06-01,sciencedirect,"First experience of autonomous, un-supervised treatment planning integrated in adaptive MR-guided radiotherapy and delivered to a patient with prostate cancer",https://api.elsevier.com/content/abstract/scopus_id/85104144940,"Background and purpose
                  Currently clinical radiotherapy (RT) planning consists of a multi-step routine procedure requiring human interaction which often results in a time-consuming and fragmented process with limited robustness. Here we present an autonomous un-supervised treatment planning approach, integrated as basis for online adaptive magnetic resonance guided RT (MRgRT), which was delivered to a prostate cancer patient as a first-in-human experience.
               
                  Materials and methods
                  For an intermediate risk prostate cancer patient OARs and targets were automatically segmented using a deep learning-based software and logical volume operators. A baseline plan for the 1.5 T MR-Linac (20x3 Gy) was automatically generated using particle swarm optimization (PSO) without any human interaction. Plan quality was evaluated by predefined dosimetric criteria including appropriate tolerances. Online plan adaptation during clinical MRgRT was defined as first checkpoint for human interaction.
               
                  Results
                  OARs and targets were successfully segmented (3 min) and used for automatic plan optimization (300 min). The autonomous generated plan satisfied 12/16 dosimetric criteria, however all remained within tolerance. Without prior human validation, this baseline plan was successfully used during online MRgRT plan adaptation, where 14/16 criteria were fulfilled. As postulated, human interaction was necessary only during plan adaptation.
               
                  Conclusion
                  Autonomous, un-supervised data preparation and treatment planning was first-in-human shown to be feasible for adaptive MRgRT and successfully applied. The checkpoint for first human intervention was at the time of online MRgRT plan adaptation. Autonomous planning reduced the time delay between simulation and start of RT and may thus allow for real-time MRgRT applications in the future.",science
10.1016/j.scp.2021.100415,Journal,Sustainable Chemistry and Pharmacy,scopus,2021-06-01,sciencedirect,Green chemistry and coronavirus,https://api.elsevier.com/content/abstract/scopus_id/85104072773,"The novel coronavirus pandemic has rapidly spread around the world since December 2019. Various techniques have been applied in identification of SARS-CoV-2 or COVID-19 infection including computed tomography imaging, whole genome sequencing, and molecular methods such as reverse transcription polymerase chain reaction (RT-PCR). This review article discusses the diagnostic methods currently being deployed for the SARS-CoV-2 identification including optical biosensors and point-of-care diagnostics that are on the horizon. These innovative technologies may provide a more accurate, sensitive and rapid diagnosis of SARS-CoV-2 to manage the present novel coronavirus outbreak, and could be beneficial in preventing any future epidemics. Furthermore, the use of green synthesized nanomaterials in the optical biosensor devices could leads to sustainable and environmentally-friendly approaches for addressing this crisis.",science
10.1016/S2214-109X(21)00059-0,Journal,The Lancet Global Health,scopus,2021-06-01,sciencedirect,The injustice of unfit clinical practice guidelines in low-resource realities,https://api.elsevier.com/content/abstract/scopus_id/85103953372,"To end the international crisis of preventable deaths in low-income and middle-income countries, evidence-informed and cost-efficient health care is urgently needed, and contextualised clinical practice guidelines are pivotal. However, as exposed by indirect consequences of poorly adapted COVID-19 guidelines, fundamental gaps continue to be reported between international recommendations and realistic best practice. To address this long-standing injustice of leaving health providers without useful guidance, we draw on examples from maternal health and the COVID-19 pandemic. We propose a framework for how global guideline developers can more effectively stratify recommendations for low-resource settings and account for predictable contextual barriers of implementation (eg, human resources) as well as gains and losses (eg, cost-efficiency). Such development of more realistic clinical practice guidelines at the global level will pave the way for simpler and achievable adaptation at local levels. We also urge the development and adaptation of high-quality clinical practice guidelines at national and subnational levels in low-income and middle-income countries through co-creation with end-users, and we encourage global sharing of these experiences.",science
10.1016/j.conengprac.2021.104795,Journal,Control Engineering Practice,scopus,2021-06-01,sciencedirect,Predictive power-split system of hybrid ship propulsion for energy management and emissions reduction,https://api.elsevier.com/content/abstract/scopus_id/85103377640,"In this work, an energy management system to address the optimal power-split problem in hybrid ship propulsion is developed. The torque of the diesel engine and the electric machine is regulated based on a predictive strategy with a weighting factor which determines the trade-off between fuel consumption and NOx emissions minimization. The modeling for the controller design is based on first principles and data gathered from the hybrid plant. In addition a disturbance observer is designed to estimate the propeller load characteristics. A neural network model that predicts rotational speed reference within the prediction horizon complements the control system design. It is used along with the observer to calculate the future load demand. A parametric simulation study is performed for the trade-off evaluation between fuel consumption and NOx emissions reduction of the control scheme. The control scheme is experimentally implemented and tested in real-time operation, where it has to cope with environmental disturbance rejection and follow the desired rotational speed reference, while performing the power-split in respect to the fuel to NOx weighting parameter and operate the plant within the desirable constraints.",science
10.1016/j.swevo.2021.100868,Journal,Swarm and Evolutionary Computation,scopus,2021-06-01,sciencedirect,"Major Advances in Particle Swarm Optimization: Theory, Analysis, and Application",https://api.elsevier.com/content/abstract/scopus_id/85102857299,"Over the ages, nature has constantly been a rich source of inspiration for science, with much still to discover about and learn from. Swarm Intelligence (SI), a major branch of artificial intelligence, was rendered to model the collective behavior of social swarms in nature. Ultimately, Particle Swarm Optimization algorithm (PSO) is arguably one of the most popular SI paradigms. Over the past two decades, PSO has been applied successfully, with good return as well, in a wide variety of fields of science and technology with a wider range of complex optimization problems, thereby occupying a prominent position in the optimization field. However, through in-depth studies, a number of problems with the algorithm have been detected and identified; e.g., issues regarding convergence, diversity, and stability. Consequently, since its birth in the mid-1990s, PSO has witnessed a myriad of enhancements, extensions, and variants in various aspects of the algorithm, specifically after the twentieth century, and the related research has therefore now reached an impressive state. In this paper, a rigorous yet systematic review is presented to organize and summarize the information on the PSO algorithm and the developments and trends of its most basic as well as of some of the very notable implementations that have been introduced recently, bearing in mind the coverage of paradigm, theory, hybridization, parallelization, complex optimization, and the diverse applications of the algorithm, making it more accessible. Ease for researchers to determine which PSO variant is currently best suited or to be invented for a given optimization problem or application. This up-to-date review also highlights the current pressing issues and intriguing open challenges haunting PSO, prompting scholars and researchers to conduct further research both on the theory and application of the algorithm in the forthcoming years.",science
10.1016/j.asoc.2021.107175,Journal,Applied Soft Computing,scopus,2021-06-01,sciencedirect,A real-time hostile activities analyses and detection system,https://api.elsevier.com/content/abstract/scopus_id/85101111750,"Over recent years, the development of online social media has dramatically changed the way people connect and share information. It is undeniable that social platform has promoted the quickest type of spread for fake stories. Almost all the current online fact-checking sources and researches are concentrating on the validating political content and context. The proposed system in this paper provides a complete visual data analytics methods to assist users in achieving a comprehensive understanding of malicious activities at multiple levels such as adversary’s behavior, victim’s behavior, content, and context level. In this paper, we investigate a variety of datasets from different aspects such as role, vulnerabilities, influential level, and distribution pattern. The proposed method in this paper focuses on automatic fake/hostile activity detection by utilizing a variety of machine learning (ML) techniques, deep learning models, natural language processes (NLP), and social network analysis (SNA) techniques. Different auxiliary models, such as bot detection, user credibility, and text readability, are deployed to generate additional influential features. The classification performance of ten different machine learning algorithms using a variety of well-known datasets is evaluated by utilizing 10-fold cross-validation.",science
10.1016/j.energy.2021.120109,Journal,Energy,scopus,2021-06-01,sciencedirect,Prediction of solar energy guided by pearson correlation using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85101063162,"Solar energy forecasting represents a key element in increasing the competitiveness of solar power plants in the energy market and reducing the dependence on fossil fuels in economic and social development. This paper presents an approach for predicting solar energy, based on machine and deep learning techniques. The relevance of the studied models was evaluated for real-time and short-term solar energy forecasting to ensure optimized management and security requirements in this field while using an integral solution based on a single tool and an appropriate predictive model. The datasets we used in this study, represent data from 2016 to 2018 and are related to Errachidia which is a semi-desert climate province in Morocco. Pearson correlation coefficient was deployed to identify the most relevant meteorological inputs from which the models should learn. RF and ANN have provided high accuracies against LR and SVR, which have reported very significant errors. ANN has shown good performance for both real-time and short-term predictions. The key findings were compared with Pirapora in Brazil, which is a tropical climate region, to show the quality and reproducibility of the study.",science
10.1016/j.ins.2020.12.080,Journal,Information Sciences,scopus,2021-06-01,sciencedirect,Rumor2vec: A rumor detection framework with joint text and propagation structure representation learning,https://api.elsevier.com/content/abstract/scopus_id/85101056591,"Rumors often yield adverse societal and economic impacts. Therefore, rumor detection has attracted a surge of research interests. Existing methods mainly focus on finding clues from textual contents, which is not quite effective as rumors can be intentionally manipulated. Recent studies have demonstrated that the propagation structure of rumors can significantly improve rumor detection performance. However, propagation-based methods are still limited as the propagation structure is often sparse at an early stage. In this study, we propose Rumor2vec, a novel rumor detection framework with joint text and propagation structure representation learning. First, we present the concept of the union graph to incorporate propagation structures of all tweets to mitigate the sparsity issue. Then, we leverage network embedding to learn representations of nodes in the union graph. Finally, we propose a framework for rumor representation learning and detection. Experimental results on three real-world datasets demonstrate that our proposed framework can achieve better performance than the state-of-the-art approaches. On two Twitter datasets, our method achieves 79.6% and 85.2% accuracies respectively. On the Weibo dataset, our method achieves a 95.1% accuracy. Further experiments on early rumor detection show that our method can identify rumors ahead of other methods by at least 12 h.",science
10.1016/j.bioelechem.2021.107744,Journal,Bioelectrochemistry,scopus,2021-06-01,sciencedirect,The electrochemical immunosensor for detection of prostatic specific antigen using quince seed mucilage-GNPs-SNPs as a green composite,https://api.elsevier.com/content/abstract/scopus_id/85100096246,"Prostatic specific antigen (PSA) is known as a biomarker of prostate cancer. In males, prostate cancer is ranked second as leading cause of death out of more than 200 different cancer types1. As a result, early detection of cancer can cause a significant reduction in mortality. PSA concentration directly is related to prostate cancer, so normal serum concentrations in healthy means are 4 ng and above 10 ng as abnormal concentration. Therefore, PSA determination is important to cancer progression. In this study, a free label electrochemical immunosensor was prepared based on a new green platform for the quantitative detection of the PSA. The used platform was formed from quince seed mucilage containing green gold and silver nanoparticles and synthesized by the green method (using Calendula officinalis L. extract). The quince mucilage biopolymer was used as a sub layer to assemble nanoparticles and increase the electrochemical performance. This nanocomposite was used to increase the antibody loading and accelerate the electron transfer, which can increase the biosensor sensitivity. The antibodies of the PSA biomarker were successfully incubated on the green platform. Under the optimal conditions, the electrochemical impedance spectroscopy (EIS) was proportional to the PSA biomarker concentration from 0.1 pg mL−1 to 100 ng mL−1 with low limit of detection (0.078 pg mL−1). The proposed green immunosensor exhibited high stability and reproducibility, which can be used for the quantitative assay of the PSA biomarker in clinical analyses. The results of real sample analysis presented another tool for the PSA biomarker detection in physiologic models.",science
10.1016/j.micpro.2021.103988,Journal,Microprocessors and Microsystems,scopus,2021-06-01,sciencedirect,Computer simulation of urban garden landscape design based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85099498199,"Digital Landscape is a combination of the system and the computer software and hardware system of a high simulation model. The author analyzes the application of computer simulation in landscape design and value analysis of a city garden. In the computer-aided design, the importance of digitizing information in the landscape design process, mainly human and the interaction of computer, is reflected in the digital model's creation and multimedia performance, becoming more and more evident. To form a two-dimensional or three-dimensional spatial data, to realize real-time, statistical Analysis, using the human living environment, multi-dimensional, efficient, and humane, and environmental landscape plan to more rational and practical, used the computer simulation techniques. Effective use of urban rainwater, to reduce the flooding of urban areas, it is possible to alleviate the water crisis, the organic combination of rainwater can be used in the course of the construction of the urban landscape as well as make-up landscape, visual beautification has optimized the ecosystem, and from many rainwater utilization functions; These functions in landscape design, rainwater garden, It can be realized the rooftop garden, and the city's green. The construction and sustainable economy and the promotion of the ecological park's social development will positively sign. Suitable for rainwater regulation, water (recovery) is stored—Computer-Aided Design (CAD) green space. Technical measures save of suggestions for practical application of the square: innovation and new of space design, new artificial wetland system, and garden rainwater in the application of the regulation (population) storage system design of the water-saving of these to the sustainable development of such new square of rainwater adjustment (group) storage system design and urban landscape environment. It is useful for the application of technology.",science
10.1016/j.cja.2020.09.011,Journal,Chinese Journal of Aeronautics,scopus,2021-06-01,sciencedirect,Framework and development of data-driven physics based model with application in dimensional accuracy prediction in pocket milling,https://api.elsevier.com/content/abstract/scopus_id/85097765922,"In the manufacturing of thin wall components for aerospace industry, apart from the side wall contour error, the Remaining Bottom Thickness Error (RBTE) for the thin-wall pocket component (e.g. rocket shell) is of the same importance but overlooked in current research. If the RBTE reduces by 30%, the weight reduction of the entire component will reach up to tens of kilograms while improving the dynamic balance performance of the large component. Current RBTE control requires the off-process measurement of limited discrete points on the component bottom to provide the reference value for compensation. This leads to incompleteness in the remaining bottom thickness control and redundant measurement in manufacturing. In this paper, the framework of data-driven physics based model is proposed and developed for the real-time prediction of critical quality for large components, which enables accurate prediction and compensation of RBTE value for the thin wall components. The physics based model considers the primary root cause, in terms of tool deflection and clamping stiffness induced Axial Material Removal Thickness (AMRT) variation, for the RBTE formation. And to incorporate the dynamic and inherent coupling of the complicated manufacturing system, the multi-feature fusion and machine learning algorithm, i.e. kernel Principal Component Analysis (kPCA) and kernel Support Vector Regression (kSVR), are incorporated with the physics based model. Therefore, the proposed data-driven physics based model combines both process mechanism and the system disturbance to achieve better prediction accuracy. The final verification experiment is implemented to validate the effectiveness of the proposed method for dimensional accuracy prediction in pocket milling, and the prediction accuracy of AMRT achieves 0.014 mm and 0.019 mm for straight and corner milling, respectively.",science
10.1016/j.eswa.2020.114538,Journal,Expert Systems with Applications,scopus,2021-05-15,sciencedirect,Parallel versus cascaded logistic regression trained single-hidden feedforward neural network for medical data,https://api.elsevier.com/content/abstract/scopus_id/85098990261,"Objective
                  An important step towards a better healthcare system is fast and accurate diagnosis. In the last decade, the application of intelligent systems in healthcare has led to impressive results. The goal of this paper is to extend the LogSLFN (single-hidden layer feedforward neural network trained using logistic regression) algorithm, which has been deployed successfully in the past for the case of a two-class decision problem, to the case of multiple classes. We have considered and statistically analyzed two approaches: a parallel LogSLFN, and a cascaded LogSLFN.
               
                  Materials and methods
                  According to the universal approximation theorem, a single-hidden layer feedforward neural network has the ability to approximate arbitrarily closely continuous functions of several real variables under certain reasonable assumptions. Essentially, a single hidden layer containing a finite fixed number of neurons is sufficient to provide an arbitrarily well approximation to a given training set of inputs and a desired target output represented by a continuous function. Parallel LogSLFN and cascaded LogSLFN are two novel approaches that can be applied to multiple-class decision problems. Both methods are extensions of the LogSLFN, which uses logistic regression to compute the weights between the input and hidden layer of a single-hidden layer feedforward network. No error correction is needed, the weights between the hidden and the output layer being computed using the Moore-Penrose pseudoinverse matrix. The proposed models have been tested on two medical datasets regarding cancer diagnosis and liver fibrosis staging. Experimental results and the subsequent statistical analysis have proved the robustness of the proposed models with other machine learning techniques reported in literature.
               
                  Main findings
                  The experimental results showed that the Parallel approach surpasses the Cascaded one. Still, both models are competitive to the other state-of-the-art techniques.
               
                  Conclusions
                  The LogSFLN algorithm can be successfully extended to multiple-class decision problems. By embedding knowledge extracted from the data into the architecture, we obtained a raise by 20% in accuracy when applied on the liver fibrosis dataset.",science
10.1016/j.jbi.2021.103787,Journal,Journal of Biomedical Informatics,scopus,2021-05-01,sciencedirect,Can technological advancements help to alleviate COVID-19 pandemic? a review,https://api.elsevier.com/content/abstract/scopus_id/85104674391,"The COVID-19 pandemic is continuing, and the innovative and efficient contributions of the emerging modern technologies to the pandemic responses are too early and cannot be completely quantified at this moment. Digital technologies are not a final solution but are the tools that facilitate a quick and effective pandemic response. In accordance, mobile applications, robots and drones, social media platforms (such as search engines, Twitter, and Facebook), television, and associated technologies deployed in tackling the COVID-19 (SARS-CoV-2) outbreak are discussed adequately, emphasizing the current-state-of-art. A collective discussion on reported literature, press releases, and organizational claims are reviewed. This review addresses and highlights how these effective modern technological solutions can aid in healthcare (involving contact tracing, real-time isolation monitoring/screening, disinfection, quarantine enforcement, syndromic surveillance, and mental health), communication (involving remote assistance, information sharing, and communication support), logistics, tourism, and hospitality. The study discusses the benefits of these digital technologies in curtailing the pandemic and ‘how’ the different sectors adapted to these in a shorter period. Social media and television’s role in ensuring global connectivity and serving as a common platform to share authentic information among the general public were summarized. The World Health Organization and Governments’ role globally in-line with the prevention of propagation of false news, spreading awareness, and diminishing the severity of the COVID-19 was discussed. Furthermore, this collective review is helpful to investigators, health departments, Government organizations, and policymakers alike to facilitate a quick and effective pandemic response.",science
10.1016/j.cjca.2020.12.009,Journal,Canadian Journal of Cardiology,scopus,2021-05-01,sciencedirect,Digital Health Approaches for the Assessment and Optimisation of Hypertension Care Provision,https://api.elsevier.com/content/abstract/scopus_id/85104335482,"Although many aspects of our lives have been transformed by digital innovation, widespread adoption of digital health advancements within the health care sector in general, and for hypertension care specifically, has been limited. However, it is likely that, over the next decade, material increases in the uptake of digital health innovations for hypertension care delivery will be seen. In this narrative review, we summarise those innovations thought to have the greatest chance for impact in the next decade. These include provision of virtual care combined with home blood pressure (BP) telemonitoring, use of digital registries and protocolised care, leveraging continuous BP measurement to collect vast amounts of individual and population-based BP data, and adoption of digital therapeutics to provide low-cost scalable interventions for patients with or at risk for hypertension. Of these, home BP telemonitoring is likely the most ready for implementation, but it needs to be done in a way that enables efficient guideline-concordant care in a cost-effective manner. In addition, efforts must be focused on implementing digital health solutions in a manner that addresses the major challenges to digital adoption. This entails ensuring that innovations are accessible, usable, secure, validated, evidence based, cost-effective, and integrated into the electronic systems that are already used by patients or providers. Increasing the use of broader digital innovations such as artificial/augmented intelligence, data analytics, and interactive voice response is also critically important. The digital revolution holds substantial promise, but success will depend on the ability of collaborative stakeholders to adopt and implement innovative, usable solutions.",science
10.1016/j.evopsy.2021.03.006,Journal,Evolution Psychiatrique,scopus,2021-05-01,sciencedirect,"From Digital Identity to Connected Personality, From Augmented Diagnostician to Virtual Caregiver: What Are the Challenges for the Psychology and the Psychiatry of the Future?",https://api.elsevier.com/content/abstract/scopus_id/85104125089,"Objectifs
                  Qui sommes-nous devenus, citoyens, patients, praticiens ? En quoi les moyens de communications et l’informatisation de notre société modifient-ils, intègrent-ils nos identités ? L’intelligence artificielle comprendrait-elle bientôt plus justement l’être humain dont elle s’émanciperait ?
               
                  Matériel et méthodes
                  Cheminons à partir de la lexicologie pour tenter de saisir, via le point de vue de la philosophie, l’identité contemporaine vers la notion d’« identité numérique » dont les incidents psychologiques normaux ou pathologiques entraînent ce que nous définissons « la personnalité numérique ». Puis, posant les bases d’une psychologie de l’identité contemporaine, nous envisageons comment « la psychologie » et « la psychiatrie » actuelles considèrent « la personnalité » du patient et, en retour, comment elles se définissent du point du vue du « praticien en ligne » ou du « chercheur connecté ».
               
                  Résultats
                  En échange de son utilisation « gratuite », l’action de l’internaute sur le Web 2.0 produit du contenu et alimente des bases de données, déclaratives ou non. En perte d’intimité au fur et à mesure que « ses » données ne lui appartiennent plus, l’identité du citoyen se décompose en fonctions des supports digitaux : site de rencontre amical, plateforme de liens amoureux, blog concernant un loisir ou un voyage, etc. Par le même mouvement, l’identité numérique se compose en autre-soi possédant une part d’intelligence artificielle pourvoyeuse de capacité d’existence propre. Plutôt que deux entités parallèlement différentiables, réelle ou augmentée, naît une identité hybride « réalistiquo-virtuelle ». Quelles conséquences normales ou pathologiques chez l’être humain ? Les tendances sociétales post-modernes issues du digital ou y trouvant expression peuvent entraîner, chez un individu donné, une exacerbation des traits de personnalité préalablement existants, voire des symptômes. Parallèlement, il arrive que les moyens de communication moderne deviennent une aide pour expérimenter le monde, majorer l’estime de soi, rêver favorablement ses phantasmes, se confier plus facilement à des « inconnu(e)s », etc. Mais dans tous les cas, chez le sujet souffrant, ou ne souffrant pas, préalablement à sa surexposition, de maladie neuropsychiatrique ou de trouble psychopathologique, il s’avère aujourd’hui scientifiquement documenté que la confrontation numérique accrue induit des atteintes neuropsychiques massives (affaiblissement de la mémoire de travail, des capacités d’attention et de concentration, des aptitudes à construire des opérations cognitives élaborées, etc.). Sur le plan psychopathologique, plutôt que la terminologie de « trouble de l’identité » ou une notion de « co-identités », le terme d’« identité trouble » nous paraît le mieux rendre compte de cette mutation du « moi » où la frontière entre réalité et virtualités s’amenuise : la dissociation prévaut. L’homme post-moderne et ses objets connectés ne font plus qu’un, mais cet « uniforme » apparaît constitué d’un patchwork de confettis identificatoires plus ou moins accolés, sans réelle harmonisation d’ensemble. La personnalité commune se marque d’hyperexpressivité et d’hyperémotivité, au détriment de la possibilité de contrôle des affects et du développement des capacités d’introspection. Contre le risque du vide, tend à se développer une contra-phobie par l’ordiphone, par l’objet lui-même, par la possibilité de contacter en permanence ses proches si nécessaire, et en retour rester toujours « disponible », ce qui alimente une forme d’égocentrisme addictogène. Résulte de ses évolutions, globalement dans la société, un affaiblissement des capacités langagières, et ainsi de réflexion, y compris pour l’espace clinique et scientifique.
               
                  Discussion
                  Pour les domaines de la psychologie et de la psychiatrie, s’associent actuellement deux évolutions : une velléité d’« objectivité-scientificité » et une numérisation de la relation patient–soignant. Du côté de la « science », la médecine objective « factuelle » s’intéresse de plus en plus à la pathologie aux dépens du sujet en souffrance, confondant signe et symptôme, glissant jusqu’à un niveau moléculaire, très en-deçà du patient, vers une psychiatrie ou une psychologie « post-clinique ». Qu’on veuille la promouvoir ou l’anéantir, du côté du clinicien ou du chercheur, la « subjectivité » est devenue un signifiant à la mode pour le domaine de la santé psychique. Ce retour actuel du « subjectif » prospère sur une sorte de peur de la subjectivité depuis la fin de la seconde guerre mondiale qui avait entraîné la nosographie américaine vers les « objectifs » des DSM (Manuel Diagnostique et Statistique des Troubles Psychiques publié par l’American Psychiatric Association depuis 1952). Mais plutôt qu’une connaissance validable, et/ou invariable concernant tel ou tel trouble psychique, le changement, la relativité des entités nosographiques d’une version à l’autre du manuel traduit, en miroir, la subjectivité d’une époque, ce que nous appelons « subjectivité sociétale ». Autant qu’elle témoigne de notre temps, la révolution bio-numérique s’imposera probablement dans une future édition de la nosographie : la validité diagnostique devrait se majorer par la définition précise de marqueurs biologiques et/ou neuroradiologiques, si ceux-ci participent à construire une théorie étiopathogénique des phénomènes psychiques observés. Cette orientation reste toutefois balbutiante : outre l’infime nombre de biomarqueurs identifiés, et surtout utilisables en pratique quotidienne, leurs liens de causalité ou de conséquentialité avec les symptômes ou le processus morbide restent le plus souvent incertains autant qu’ils sont fort divers et interreliés. Le chercheur en neurosciences vise à mesurer et analyser une multitude de données, intégrant en particulier les mimiques et les émotions authentifiables par caméra thermique, les mouvements des segments des corps et dynamiques des regards enregistrables par des capteurs, la standardisation des voix et des discours pour analyse par logiciel informatique de la prosodie, des signifiants employés, de la syntaxe… le tout s’intégrant dans un phénotypage digital de la souffrance. Pourra-t-on bientôt parler, en remplacement du psychologue ou du psychiatre, de « diagnosticien augmenté » ?
               
                  Conclusion
                  Apparaît-il actuellement hasardeux de faire confiance à un thérapeute entièrement virtuel… expérience déjà lancée il y a plus de 50 ans ! L’être humain est un « être de sens », or, selon le modèle de la clinique traumatique, le surgissement du tout-numérique peut entraîner un « effondrement du sens » générateur d’une tendance à la dissociation de la personnalité. Accordant le rétablissement des liens entre émotions, affects, comportements et cognitions, le langage parlé atténue puis fait disparaître la dissociation. Guidée par le praticien, cette parole thérapeutique est parfois qualifiée de « maïeutique », du nom de la science de l’accouchement : elle construit synchroniquement à son essence la pensée, et une prise de conscience de celle-ci, plutôt qu’elle n’en rendrait compte secondairement. Il s’agit d’une réinterprétation causale d’un sens compris ou plutôt « attribué » singulièrement par le sujet, après-coup, le passé revisité dans l’instant noue une synthèse, le hasard est transformé en destin. Le sujet qui parle réélabore son histoire vers une reconstruction sémantique, une densification de ses réseaux de signification. Reconquérant son être par la création d’un discours, de méandres véridiques comme fictionnels, la narration, voire la poétisation, offre l’illusion ponctuelle d’une meilleure cohérence, toujours relative, illusoire La parole thérapeutique et le discours sur celle-ci restent en devenir, inachevés, incertains autant que vivants, caractérisant une « post-psychothérapie », c’est-à-dire une psychothérapie et non pas une technique rééducative qui se trouverait figée dans des objectifs connus à l’avance. Les notions de faits et de réalité sont ici secondaires, non pas au sens de l’objectif, ni même du subjectif, mais du second degré, puis d’autres degrés successifs ou imbriqués portant l’effort intellectuel. Vers l’apaisement, si nous voulions amener la réflexion à son paroxysme, nous pourrions avancer qu’il suffirait de donner « n’importe quel sens », d’en choisir un quel qu’il soit, du côté du patient ou du praticien, sans qu’il ne soit nécessairement le même, témoignage d’une construction intersubjective formellement invalide.
               
                  Objectives
                  Who have we become, as citizens, patients, practitioners? How do the means of communication and the computerization of our society, its digitization, modify and integrate our identities? Can we assume that artificial intelligence will soon have a more accurate understanding of the human being from whom it will have emancipated itself?
               
                  Materials and methods
                  We move from lexicology to try to grasp, from the point of view of philosophy, a contemporary identity that is moving towards the notion of a “digital identity” whose normal or pathological psychological incidents lead to what we define as “the digital personality.” Then, laying the foundations for a contemporary psychology of identity, we consider how current “psychology” and “psychiatry” view the patient's “personality” and, in turn, how they define themselves from the point of view of “the patient,” or, inversely, from the point of view of the “online practitioner” or “connected researcher.”
               
                  Results
                  In exchange for its “free” use, the Internet user's action on Web 2.0 produces content and feeds databases, whether this is declared or not. Users’ privacy is lost, as “their” data no longer belongs to them; and citizens’ identity is broken down into digital media functions: a site for meeting friends, a dating platform, a blog about hobbies or travel, etc. At the same time, digital identity is made up of an other-self, including a part of artificial intelligence that provides capacity for its own existence. Rather than two parallel, differentiable entities, real or augmented, a “realistic-virtual” hybrid identity is born. What are the normal or pathological consequences for humans? Postmodern societal trends emerging from or finding expression in the digital can lead to an exacerbation of previously existing personality traits, or even symptoms, in a given individual. At the same time, it happens that the modern means of communication become an aid to experience the world, to increase self-esteem, to dream favorably about one's fantasies, to confide more easily in “strangers,” etc. But in all cases, in the subject suffering, or not suffering, prior to his overexposure, from a neuropsychiatric disease or a psychopathological disorder, it now turns out to be scientifically documented that the increased numerical confrontation induces massive neuropsychic damage (weakening working memory, attention and concentration skills, skills in constructing sophisticated cognitive operations, etc.). On the psychopathological level, rather than the terminology of “identity disorder” or a notion of “co-identities,” the term “identity elusive"" seems to us to best account for this mutation of the “me” where the border between reality and virtualities is shrinking: dissociation prevails. The postmodern human and its connected objects become one, but this “uniformity” appears to be made up of a patchwork of identifying confetti more or less joined together, without a real overall harmonization. The common personality is marked by hyperexpressiveness and hyperemotivity, to the detriment of the possibility of controlling affects and the development of introspective capacities. Against the risk of a vacuum, a contra-phobia tends to develop through the smartphone, by the object itself, by the possibility of constantly contacting relatives if necessary, and in return always remaining “available,” which fuels a form of addicting self-centeredness. The result of these developments, for society in general, is a weakening of language skills, and thus of reflection, including in the clinical and scientific space.
               
                  Discussion
                  For the areas of psychology and psychiatry, two developments are currently associated: a desire for “objectivity-scientificity” and a digitization of the patient–caregiver relationship. On the side of “science,” objective “factual” medicine is increasingly interested in pathology at the expense of the suffering subject, confusing sign and symptom, sliding down to a molecular level, far below the patient, towards psychiatry or postclinical psychology. Whether we want to promote it or destroy it, on the side of the clinician or the researcher, “subjectivity” has become a fashionable signifier in the field of mental health. This current return of the “subjective” thrives on a kind of fear of subjectivity present since the end of World War II, which had led American nosography towards the “objectives” of the DSM (Diagnostic and Statistical Manual of Mental Disorders, published by the American Psychiatric Association since 1952). But rather than a verifiable and/or invariable knowledge concerning a particular psychic disorder, the changes and the relativity of nosographic entities from one version of the manual to another provides us with a mirror image of the subjectivity of an era, which we propose to call “societal subjectivity.” As much as it is a product of our time, the bio-digital revolution will probably impose itself in a future edition of nosography: the diagnostic validity should be increased by the precise definition of biological and/or neuroradiological markers, if these participate in building an etiopathogenic theory of observed psychic phenomena. This orientation remains in its infancy, however: in addition to the tiny number of identified biomarkers, and above all, those that are usable in daily practice, their causal or consequential links with symptoms or with the morbid process remain most often uncertain, inasmuch as they are diverse and interrelated. The neuroscience researcher aims to measure and analyze a multitude of data, integrating, in particular, mimicry and emotions authenticated by thermal camera; movements of body segments and gaze dynamics recorded by sensors; the standardization of voices and speeches for computer software analysis of prosody, used signifiers, syntax… all of which is integrated into a digital phenotyping of suffering. Will we soon be able to speak, replacing the psychologist or the psychiatrist, of an “augmented diagnostician?”.
               
                  Conclusion
                  Does it currently appear risky to trust an entirely virtual therapist… an experiment already launched more than 50 years ago! The human being is a “being of meaning,” yet, according to the model of trauma, the emergence of the all-digital can lead to a “collapse of meaning,” generating a tendency to personality dissociation. Granting the reestablishment of the links between emotions, affects, behaviors, and cognitions, spoken language attenuates dissociation, then makes it disappear. Guided by the practitioner, this therapeutic word is sometimes qualified as “maieutics,” from the name of the science of childbirth: it builds thought synchronously to its essence, and an awareness of it, rather than nondisclosure, would account for it secondarily. It is a causal reinterpretation of a meaning understood or rather “attributed” singularly by the subject, after the fact: the past revisited in the present moment creates a synthesis, and chance is transformed into fate. The speaking subject re-elaborates her/his story towards a semantic reconstruction, a densification of her/his networks of signification. Reclaiming one's being by the creation of a discourse, of veridical as well as fictional meanders, narration, even poetization, offers the punctual illusion of a better coherence, always relative, illusory… Therapeutic speech and discourse about such speech–these are still being made, unfinished, uncertain, and alive. These are the characteristics of what we could a “post-psychotherapy,” that is, a psychotherapy and not a re-educational technique whose objectives would be fixed and known in advance. The notions of facts and reality are secondary here, not in the sense of the objective, nor even of the subjective, but of the second degree, then of other successive or overlapping degrees that require intellectual effort. Moving towards appeasement, if we wanted to bring the reflection to its paroxysm, we could advance that it would be enough to give “any meaning,” whatever it may be. This would apply both to the patient and to the practitioner, without each party's meaning necessarily being the same: a testimony to a formally invalid intersubjective construction.",science
10.1016/j.trc.2021.102967,Journal,Transportation Research Part C: Emerging Technologies,scopus,2021-05-01,sciencedirect,Automated eco-driving in urban scenarios using deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85103105894,"Urban settings are challenging environments to implement eco-driving strategies for automated vehicles. It is often assumed that sufficient information on the preceding vehicle pulk is available to accurately predict the traffic situation. Because vehicle-to-vehicle communication was introduced only recently, this assumption will not be valid until a sufficiently high penetration of the vehicle fleet has been reached. Thus, in the present study, we employed Reinforcement Learning (RL) to develop eco-driving strategies for cases where little data on the traffic situation are available.
                  An A-segment electric vehicle was simulated using detailed efficiency models to accurately determine its energy-saving potential. A probabilistic traffic environment featuring signalized urban roads and multiple preceding vehicles was integrated into the simulation model. Only information on the traffic light timing and minimal sensor data were provided to the control algorithm. A twin-delayed deep deterministic policy gradient (TD3) agent was implemented and trained to control the vehicle efficiently and safely in this environment.
                  Energy savings of up to 19% compared with a simulated human driver and up to 11% compared with a fine-tuned Green Light Optimal Speed Advice (GLOSA) algorithm were determined in a probabilistic traffic scenario reflecting real-world conditions. Overall, the RL agents showed a better travel time and energy consumption trade-off than the GLOSA reference.",science
10.1016/j.image.2021.116200,Journal,Signal Processing: Image Communication,scopus,2021-05-01,sciencedirect,Modelling spatio-temporal ageing phenomena with deep Generative Adversarial Networks,https://api.elsevier.com/content/abstract/scopus_id/85100999804,"Deterioration modelling of ageing phenomena on materials is an actively researched topic in computer graphics and vision, with a wide range of applications in domains such as cultural heritage, game programming, material science and virtual reality. As a result significant progress has been accomplished and existing methods are able to produce visually pleasing results that appear realistic. However, there is a very limited connection to comprehensive measurements that actually capture the ageing process of a material. This paper focuses on this gap, aiming to provide a link between physical measurements and deterioration modelling. Based on extensive measurements of texture and surface geometry of artificially aged reference materials, a Deep Learning (DL) framework is proposed that models spatio-temporal variations on the 3D surface geometry and the 2D colour–image appearance. Concretely, the problem of material degradation over time is formulated as an 2D/3D material-to-material translation problem, where the goal is, given an input material and a target degradation time, to output the degraded material at that time. At the core of the method lies a modified conditional Generative Adversarial Network (cGAN), which maps input materials to degraded materials over time. In order to train and deploy the proposed cGAN model, proper data parameterization and augmentation steps are introduced. As shown through extensive experimentation on real data coming from materials commonly found in artwork and from actual artworks, the proposed approach produces high quality results.",science
10.1016/j.ssci.2021.105190,Journal,Safety Science,scopus,2021-05-01,sciencedirect,Beirut explosion 2020: A case study for a large-scale urban blast simulation,https://api.elsevier.com/content/abstract/scopus_id/85100545374,"In the face of continued global urbanization, cities are challenged to satisfy increasing standards in terms of quality of life, environmental conditions, safety, security, health, economic growth and mobility. The concept of “smart cities” aims at utilising advanced technologies, artificial intelligence and high computational capacity to increase their resilience and improve the services provided to the citizens. Computation-based numerical simulations have been essentially used to estimate the effects of explosion events in urban environments in terms of both structural damage and human casualties. These provide urban planners and decision makers with valuable information for vulnerability assessment and aid developing prevention or mitigation solutions. In this article, we present a framework to generate a 3D large-scale urbanistic finite element model, where the desired geospatial data are extracted from the open-source world map OpenStreetMap. The model is used to simulate blast wave propagation effects in a wide urban area taking into account the reflections at building surfaces via a sophisticated Fluid-Structure interaction technique integrated in the EUROPLEXUS explicit finite element method software. The explosion in the Port of Beirut in Lebanon, which took place on the 4th of August 2020, was remarkable for the large amount of explosive material causing considerable damage to surrounding structures and a high number of deaths and injured. Such characteristics make the event suitable for assessing the performance of the proposed computational approach in a widely exposed (by the blast wave) urban zone.",science
10.1016/j.ins.2021.01.004,Journal,Information Sciences,scopus,2021-05-01,sciencedirect,A Blockchain-based approach for matching desired and real privacy settings of social network users,https://api.elsevier.com/content/abstract/scopus_id/85100433895,"Social networks store a considerable amount of personal data, which are also a source of information for business. To comply with users’ privacy rights, all social networks allow users to select the level of privacy they desire. However, what occurs if the privacy choices of a user are modified unilaterally by the social network? The privacy settings chosen by the user are stored by the social network, which acts as a privileged party, which could tamper with the user’s choices at any time. This paper addresses this problem and proposes a decentralized approach to manage the privacy settings of a user. Any change in the privacy settings of a social network user is validated by a smart contract to ensure that it is compliant with users’ expectations. The proposed solution has been implemented as an Ethereum-based decentralized application to validate the effectiveness of the proposed approach.",science
10.1016/j.ins.2020.12.091,Journal,Information Sciences,scopus,2021-05-01,sciencedirect,Positive opinion maximization in signed social networks,https://api.elsevier.com/content/abstract/scopus_id/85100398765,"Opinion maximization is a kind of optimization method, which leverages a subset of influential nodes in social networks to spread user opinions towards the target product and eventually obtains the largest opinion propagation. The current propagation models on the opinion maximization mainly focus on the activated nodes and the static opinion formation process. However, they neglect the combination between the activated nodes and the dynamic opinion formation process. Moreover, previous studies are more attentive to the positive relationships among users. In the real scenario, negative relationships among users may damage the product reputation. Therefore, in this paper, we study positive opinion maximization by using an Activated Opinion Maximization Framework (AOMF) in signed social networks. The proposed AOMF is composed of three phases: i) the selection of candidate seed nodes, ii) the activated opinion formation process and iii) the determination of seed nodes. We first use an effective heuristic rule to select candidate seed nodes. To model the activation and dynamic opinion formation process of network nodes, we devise the activated opinion formation model based on the multi-stage linear threshold model and the Degroot model. Then, we calculate the opinion propagation of each candidate seed node by using the activated opinion formation model. Based on the candidate seed nodes and the activated opinion formation process, seed nodes are further determined. Finally, experimental results on six social network datasets demonstrate that the proposed method has superior potential opinions and positive ratio than the chosen benchmarks.",science
10.1016/j.eswa.2020.114402,Journal,Expert Systems with Applications,scopus,2021-04-15,sciencedirect,Unsupervised feature selection for attributed graphs,https://api.elsevier.com/content/abstract/scopus_id/85097572982,"Many real-world applications generate attributed graphs that contain both link structures and content information associated with nodes. Content information in real networks always contains high dimensional feature space. In recent years, unsupervised feature selection has been widely used in handling high dimensional data without label information. Most existing unsupervised feature selection methods assume that instances in datasets are independent and identically distributed. However, instances in attributed graphs are intrinsically correlated. Considering the wide applications of feature selection in attributed graphs, we propose a new unsupervised feature selection method based on regularized sparse learning. We use pseudo class labels to learn the interdependency from both link and content information, and embed the obtained information into a sparse learning based feature selection framework. In particular, a new regularization term is designed to learn link information, which capture group behavior among the connected instances utilizing latent social dimensions. To solve the proposed feature selection model, we consider both convex and nonconvex cases and design the corresponding algorithms based on the Alternating Direction Method of Multipliers (ADMM) combined with ConCave Convex Procedure (CCCP). Numerical studies are implemented on real-world datasets to validate the advantage of our new method.",science
10.1016/j.patter.2021.100225,Journal,Patterns,scopus,2021-04-09,sciencedirect,Machine learning discovery of high-temperature polymers,https://api.elsevier.com/content/abstract/scopus_id/85104127192,"To formulate a machine learning (ML) model to establish the polymer's structure-property correlation for glass transition temperature 
                        
                           
                              T
                              g
                           
                        
                     , we collect a diverse set of nearly 13,000 real homopolymers from the largest polymer database, PoLyInfo. We train the deep neural network (DNN) model with 6,923 experimental 
                        
                           
                              T
                              g
                           
                        
                      values using Morgan fingerprint representations of chemical structures for these polymers. Interestingly, the trained DNN model can reasonably predict the unknown 
                        
                           
                              T
                              g
                           
                        
                      values of polymers with distinct molecular structures, in comparison with molecular dynamics simulations and experimental results. With the validated transferability and generalization ability, the ML model is utilized for high-throughput screening of nearly one million hypothetical polymers. We identify more than 65,000 promising candidates with 
                        
                           
                              T
                              g
                           
                        
                      > 200°C, which is 30 times more than existing known high-temperature polymers (∼2,000 from PoLyInfo). The discovery of this large number of promising candidates will be of significant interest in the development and design of high-temperature polymers.",science
10.1016/j.bioorg.2021.104719,Journal,Bioorganic Chemistry,scopus,2021-04-01,sciencedirect,Flavonoids from Pterogyne nitens as Zika virus NS2B-NS3 protease inhibitors,https://api.elsevier.com/content/abstract/scopus_id/85101244114,"Although the widespread epidemic of Zika virus (ZIKV) and its neurological complications are well-known there are still no approved drugs available to treat this arboviral disease or vaccine to prevent the infection. Flavonoids from Pterogyne nitens have already demonstrated anti-flavivirus activity, although their target is unknown. In this study, we virtually screened an in-house database of 150 natural and semi-synthetic compounds against ZIKV NS2B-NS3 protease (NS2B-NS3p) using docking-based virtual screening, as part of the OpenZika project. As a result, we prioritized three flavonoids from P. nitens, quercetin, rutin and pedalitin, for experimental evaluation. We also used machine learning models, built with Assay Central® software, for predicting the activity and toxicity of these flavonoids. Biophysical and enzymatic assays generally agreed with the in silico predictions, confirming that the flavonoids inhibited ZIKV protease. The most promising hit, pedalitin, inhibited ZIKV NS2B-NS3p with an IC50 of 5 μM. In cell-based assays, pedalitin displayed significant activity at 250 and 500 µM, with slight toxicity in Vero cells. The results presented here demonstrate the potential of pedalitin as a candidate for hit-to-lead (H2L) optimization studies towards the discovery of antiviral drug candidates to treat ZIKV infections.",science
10.1016/j.ijar.2021.01.004,Journal,International Journal of Approximate Reasoning,scopus,2021-04-01,sciencedirect,Revealed preference in argumentation: Algorithms and applications,https://api.elsevier.com/content/abstract/scopus_id/85100748656,"Argumentative agents in AI are inspired by how humans reason by exchange of arguments. Given the same set of arguments possibly attacking one another (Dung's AA framework) these agents are bound to accept the same subset of those arguments (aka extension) unless they reason by different argumentation semantics. However humans may not be so predictable, and in this paper we assume that this is because any real agent's reasoning is inevitably influenced by her own preferences over the arguments. Though such preferences are usually unobservable, their effects on the agent's reasoning cannot be washed out. Hence by reconstructing her reasoning process, we might uncover her hidden preferences, which then allow us to predict what else the agent must accept. Concretely we formalize and develop algorithms for such problems as uncovering the hidden argument preference relation of an agent from her expressed opinion, by which we mean a subset of arguments or attacks she accepted from a given AA framework; and uncovering the collective preferences of a group from a dataset of individual opinions. A major challenge we addressed in this endeavor is to deal with “answer sets” of argument preference relations which are generally exponential or even infinite. So we start by developing a compact representation for such answer sets called preference states. Preference revelation tasks are then structured as derivations of preference states from data, and reasoning prediction tasks are reduced to manipulations of derived preference states without enumerating the underlying (possibly infinite) answer sets. We also apply the presented results to two non-trivial problems: learning preferences over rules in structured argumentation with priorities – an open problem so far; and analyzing public polls in apparently deeper ways than existing social argumentation frameworks allow.",science
10.1016/j.autcon.2021.103603,Journal,Automation in Construction,scopus,2021-04-01,sciencedirect,A field parameters-based method for real-time wear estimation of disc cutter on TBM cutterhead,https://api.elsevier.com/content/abstract/scopus_id/85100241917,"In hard rock TBM tunneling, the loss caused by disc cutter wear accounts for a large proportion of time and cost for the entire project. However, existing disc cutter wear prediction models mainly focus on predicting cutter consumption before construction and cannot predict the wear of each disc cutter. Moreover, the accurate rock parameters required in these models are challenging to obtain. Hence, these models are not capable of determining which cutter on cutterhead should be replaced during construction. To solve the problems mentioned above, this paper presents a novel field parameters-based method for estimating the wear of each disc cutter in real-time. The proposed method is implemented through the following steps. To begin with, a new health index is constructed and defined as the ratio of the rolling distance of a cutter in a small excavated section to its maximum rolling distance. Then, specific field parameters related to the new health index are analyzed and selected. Thereafter, the mapping model between the new health index and the specific field parameters is established based on a one-dimensional convolutional neural network. Finally, on the basis of the established model, the estimated health indices corresponding to all excavated sections of a disc cutter are accumulated to obtain its health status. The field data obtained from Mumbai metro tunnel was utilized to verify the effectiveness of the proposed method, which demonstrates that the proposed method can estimate the wear of each disc cutter in real-time with average accuracy as high as 87.8% on the test set. Therefore, the proposed method is capable of significantly reducing the time and cost of cutter inspection, replacement, and repair for TBM, thereby improve tunneling efficiency and reduce construction cost.",science
10.1016/j.petrol.2020.108296,Journal,Journal of Petroleum Science and Engineering,scopus,2021-04-01,sciencedirect,Geological structure-guided hybrid MCMC and Bayesian linearized inversion methodology,https://api.elsevier.com/content/abstract/scopus_id/85100209414,"Seismic inversion is a common method for hydrocarbon reservoir characterization, as it consists of a proven and effective approach to derive elastic properties from reflectivity seismic data. Markov Chain Monte Carlo (MCMC) based seismic inversion approach is a suitable choice to numerically evaluate the posterior uncertainties associated with the inverse solution without assuming linear forward operators, Gaussian, or generalized Gaussian prior models. However, the existing MCMC based seismic inversion approaches are mostly performed trace-by-trace, which means that the spatial coupling of model parameters is not considered. When the results of trace-by-trace based inversion are combined to generate a 2D profile, the final results will be laterally discontinuous. Moreover, the large dimension of the model space causes low convergence efficiency of MCMC-based seismic inversion. To overcome these issues, a geological structure-guided hybrid MCMC and Bayesian linearized inversion (BLI) methodology for seismic inversion is implemented. The geological structure information obtained using plane wave destruction (PWD) is incorporated to the MCMC based inversion algorithm in the form of dips yields more geologically meaningful results. The hybrid MCMC and BLI strategy, which takes advantage of BLI's high efficiency to provide initial configuration for MCMC, is used to improve the convergence of MCMC-based inversion. Additionally, the block coordinate descent (BCD) algorithm is introduced to replace the large-scale matrix solution in geological structure-guided, and consequently reduce memory consumption and time cost. This methodology is validated on a synthetic seismic dataset, as well as on a real case. It has proven to be a reliable approach to obtain acoustic impedance (AI) from post-stack seismic data in an efficient way. It also addresses the uncertainty related with the ill-posed characteristics of the inversion methodology itself.",science
10.1016/j.ins.2020.11.042,Journal,Information Sciences,scopus,2021-04-01,sciencedirect,Information spreading with relative attributes on signed networks,https://api.elsevier.com/content/abstract/scopus_id/85098456560,"During the past years, network dynamics has been widely investigated in various disciplines. As a practical and convenient description for social networks, signed networks have also garnered significant attention. In this work, we study information spreading with relative attributes on signed networks, where edges are assigned positive or negative labels, describing friendly or hostile relationships. We define the attribute of information by a degree that can be either ‘good’ or ‘bad’ and assume that the spreading willingness of the information receiver depends on not only its relation with others but also the attribute of information. A pair-wise potential relation identification algorithm is designed based on the shortest path approach and structural balance theory. Both simulations on randomly signed networks and empirical experiments on real datasets show that the proposed information spreading could be approximately investigated within a local 2-order neighborhood. In addition, the ratio of potential friendly nodes with a target node is consist with network content. Finally, the propagation speed of ‘good’ information would unexpectedly slow down when the ratio of positive edges is larger than an estimated threshold. The presented model could be referred to in real social scenarios, such as product promotion, advertisement media, and rumor mongering.",science
10.1016/j.envpol.2020.115900,Journal,Environmental Pollution,scopus,2021-04-01,sciencedirect,Understanding the true effects of the COVID-19 lockdown on air pollution by means of machine learning,https://api.elsevier.com/content/abstract/scopus_id/85097107266,"During March 2020, most European countries implemented lockdowns to restrict the transmission of SARS-CoV-2, the virus which causes COVID-19 through their populations. These restrictions had positive impacts for air quality due to a dramatic reduction of economic activity and atmospheric emissions. In this work, a machine learning approach was designed and implemented to analyze local air quality improvements during the COVID-19 lockdown in Graz, Austria. The machine learning approach was used as a robust alternative to simple, historical measurement comparisons for various individual pollutants. Concentrations of NO2 (nitrogen dioxide), PM10 (particulate matter), O3 (ozone) and Ox (total oxidant) were selected from five measurement sites in Graz and were set as target variables for random forest regression models to predict their expected values during the city’s lockdown period. The true vs. expected difference is presented here as an indicator of true pollution during the lockdown. The machine learning models showed a high level of generalization for predicting the concentrations. Therefore, the approach was suitable for analyzing reductions in pollution concentrations. The analysis indicated that the city’s average concentration reductions for the lockdown period were: -36.9 to −41.6%, and −6.6 to −14.2% for NO2 and PM10, respectively. However, an increase of 11.6–33.8% for O3 was estimated. The reduction in pollutant concentration, especially NO2 can be explained by significant drops in traffic-flows during the lockdown period (−51.6 to −43.9%). The results presented give a real-world example of what pollutant concentration reductions can be achieved by reducing traffic-flows and other economic activities.",science
10.1016/j.adhoc.2020.102360,Journal,Ad Hoc Networks,scopus,2021-03-15,sciencedirect,VOCkit: A low-cost IoT sensing platform for volatile organic compound classification,https://api.elsevier.com/content/abstract/scopus_id/85097468733,"Improvements in small sized sensors allow the easy detection of the presence of Volatile Organic Compounds (VOCs) in the air using easy-to-deploy Internet of Things (IoT) devices. However, classifying what VOC exists in the environment still remains as a complex task. Knowing what VOCs are in the air can help us remove the main cause that vents VOC materials as a way to maintain clean air quality. In this work, we present VOCkit, an IoT sensor kit for non-chemical experts to easily detect and classify different types of VOCs. VOCkit combines miniature chemically-designed fluorometric sensors for recognizing VOCs with an embedded imaging system for classification. Exposing the fluorometric sensors with various VOCs, result in the photophysical property change of fluorescent compounds, which composes the sensors, and the synergistic combination of the changes create unique individual fluorescent color patterns respectively to the VOC material. The fluorescent color change pattern is captured using an embedded camera and the images are processed with machine learning algorithms on the embedded platform for VOC classification. Using 500 fluorometric sensor images collected for five different commonly contactable VOCs, we show the feasibility of VOC classification on small-sized IoT devices. For the VOC types of our interest, our results show a classification accuracy of 97%, implying the potential applicability of VOCkit for real-world usage.",science
10.1016/j.patter.2021.100220,Journal,Patterns,scopus,2021-03-12,sciencedirect,Safe Blues: The case for virtual safe virus spread in the long-term fight against epidemics,https://api.elsevier.com/content/abstract/scopus_id/85102307395,"Viral spread is a complicated function of biological properties, the environment, preventative measures such as sanitation and masks, and the rate at which individuals come within physical proximity. It is these last two elements that governments can control through social-distancing directives. However, infection measurements are almost always delayed, making real-time estimation nearly impossible. Safe Blues is one way of addressing the problem caused by this time lag via online measurements combined with machine learning methods that exploit the relationship between counts of multiple forms of the Safe Blues strands and the progress of the actual epidemic. The Safe Blues protocols and techniques have been developed together with an experimental minimal viable product, presented as an app on Android devices with a server backend. Following initial exploration via simulation experiments, we are now preparing for a university-wide experiment of Safe Blues.",science
10.1016/j.neucom.2020.11.066,Journal,Neurocomputing,scopus,2021-03-07,sciencedirect,BayeSuites: An open web framework for massive Bayesian networks focused on neuroscience,https://api.elsevier.com/content/abstract/scopus_id/85098065075,"BayeSuites is the first web framework for learning, visualizing, and interpreting Bayesian networks (BNs) that can scale to tens of thousands of nodes while providing fast and friendly user experience. All the necessary features that enable this are reviewed in this paper; these features include scalability, extensibility, interoperability, ease of use, and interpretability. Scalability is the key factor in learning and processing massive networks within reasonable time; for a maintainable software open to new functionalities, extensibility and interoperability are necessary. Ease of use and interpretability are fundamental aspects of model interpretation, fairly similar to the case of the recent explainable artificial intelligence trend. We present the capabilities of our proposed framework by highlighting a real example of a BN learned from genomic data obtained from Allen Institute for Brain Science. The extensibility properties of the software are also demonstrated with the help of our BN-based probabilistic clustering implementation, together with another genomic-data example.",science
10.1016/j.clineuro.2021.106524,Journal,Clinical Neurology and Neurosurgery,scopus,2021-03-01,sciencedirect,Clinical application of Myelopathy-hand Functional Evaluation System in evaluating the postoperative hand motor function for myelopathy patients,https://api.elsevier.com/content/abstract/scopus_id/85100619463,"Objective
                  Recovery of hand motor function after surgical treatment in myelopathy patients is commonly observed. Accurate evaluation of postoperative hand function contributes to assessing the efficacy of surgical treatment. However, no objective and effective evaluation method has been widely accepted in clinical practice. Therefore, the study aimed to explore the value of Myelopathy-hand Functional Evaluation System (MFES) in assessing the postoperative hand function for myelopathy patients.
               
                  Material and method
                  MFES mainly consist of a pair of wise-gloves and a computer with software. One hundred and thirty myelopathy patients were included and all of them received optimal surgery treatment. The Japanese Orthopaedic Association (JOA) scores were marked at preoperative and at 6 months after surgery. All patients were asked to perform the 10-s grip and release test, and the hand movements were simulated and converted into waveforms by MFES. The waveform parameters were measured and analyzed.
               
                  Results
                  The JOA scores and the number of grip-and-release (G–R) cycles significantly increased after surgery. Correspondingly, the waveforms of ulnar three fingers were significantly higher and narrower, along with the significantly declined average time per cycle in postoperative. The a/b ratio (Wave height/wave width) of five fingers were significantly higher in postoperative than that in preoperative. Based on the improvement rate of a/b, the excellent and good rate of surgical outcomes was 62.30 %, which was significantly higher than that (47.69 %) based on the improvement rate of JOA scores (P = 0.019).
               
                  Conclusion
                  MFES is an effective assessment tool in evaluating the postoperative hand function for myelopathy patients.",science
10.1016/j.ecoinf.2021.101231,Journal,Ecological Informatics,scopus,2021-03-01,sciencedirect,Collect and analysis of agro-biodiversity data in a participative context: A business intelligence framework,https://api.elsevier.com/content/abstract/scopus_id/85100383752,"In France and Europe, farmland represents a large fraction of land cover. The study and assessment of biodiversity in farmland is therefore a major challenge. To monitor biodiversity across wide areas, citizen science programs have demonstrated their effectiveness and relevance. The involvement of citizens in data collection offers a great opportunity to deploy extensive networks for biodiversity monitoring. But citizen science programs come with two issues: large amounts of data to manage and large numbers of participants with heterogeneous skills, needs and expectations about these data. In this article, we offer a solution to these issues, concretized by an information system. The study is based on a real life citizen science program tailored for farmers. This information system provides data and tools at several levels of complexity, to fit the needs and the skills of several users, from citizens with basic IT knowledge to scientists with strong statistical background. The proposed system is designed as follows. First, a data warehouse stores the data collected by citizens. This data warehouse is modelled depending on future data analysis. Secondly, associated with the data warehouse, a standard OLAP tool enables citizens and scientists to explore data. To complete the OLAP tool, we implement and compare four feature selection methods, in order to rank explanatory factors according to their relevance. Finally, for users with extended statistical skills, we use Generalized Linear Mixed Models to explore the temporal dynamics of invertebrate diversity in farmland ecosystems. The proposed system, a combination of business intelligence tools, data mining methods and advanced statistics, offers an example of complete exploitation of data by several user profiles. The proposition is supported by a real life citizen science program, and can be used as a guideline to design information systems in the same field.",science
10.1016/j.chemphyslip.2021.105049,Journal,Chemistry and Physics of Lipids,scopus,2021-03-01,sciencedirect,Antiviral activity of stearylamine against chikungunya virus,https://api.elsevier.com/content/abstract/scopus_id/85100148703,"Chikungunya, a mosquito-borne disease that causes high fever and severe joint pain in humans, is a profound global threat because of its high rate of contagion and lack of antiviral interventions or vaccines for controlling the infection. The present study was aimed to investigate the antiviral activity of Stearylamine (SA) against Chikungunya virus (CHIKV) in both in vitro and in vivo. The antiviral activity of SA was determined by foci forming unit (FFU) assay, quantitative RT-PCR and cell-based immune-fluorescence assay (IFA). Further in vivo studies were carried out to see the effect of SA treatment in CHIKV infected C57BL/6 mice. The anti-CHIKV activity was evaluated using qRT-PCR in serum and muscle tissues at different time points and by histopathology. In vitro treatment with SA at a concentration of 50 μM showed a reduction of 1.23 ± 0.19 log10 FFU/mL at 16 h and 1.56 ± 0.12 log10 FFU/mL at 24 h posttreatment by FFU assay. qRT-PCR studies indicated that SA treatment at 50μM concentration showed a singnificant reduction of 1.6 ± 0.1 log10 and 1.27 ± 0.12 log10 RNA copies when compared with that of virus control at 16 and 24 h post incubation. Treatments in the C57BL/6 mice model revealed that SA at 20 mg/kg dose per day up to 3, 5 and 7 days, produced stronger inhibition against CHIKV indicating substantially decrease viral loads and inflammatory cell migration in comparison to a dose of 10 mg/kg. This first in vivo study clearly indicates that SA is effective by significantly reducing virus replication in serum and muscles. As a next-generation antiviral therapeutic, these promising results can be translated for the use of SA to rationalize and develop an ideal delivery system alone or in combination against CHIKV.",science
10.1016/j.berh.2021.101662,Journal,Best Practice and Research: Clinical Rheumatology,scopus,2021-03-01,sciencedirect,Managing patients using telerheumatology: Lessons from a pandemic,https://api.elsevier.com/content/abstract/scopus_id/85100105533,"The coronavirus disease 2019 (COVID-19) pandemic has presented unique challenges to rheumatology provision. Measures to control the pandemic have limited face-to-face contact with rheumatology healthcare professionals. One innovation has been the widespread adoption of telerheumatology to assist in the care of patients with rheumatic and musculoskeletal diseases, building on an existing evidence base in rheumatology. Widespread adoption has only occurred following the COVID-19 pandemic. We discuss the evidence supporting telerheumatology adoption prior to the pandemic, and outline several innovative approaches used to assist in the care of rheumatology patients that have been introduced. Alongside the advantages of these interventions, we discuss the limitations and regulatory challenges. Advances must be balanced, considering wider issues of equity of access, implementation, adoption, and sustainability of telerheumatology post-pandemic. We propose it is not ‘if’, but ‘how’ rheumatologists embrace newer telerheumatology technology, outlining practice points and future research agenda.",science
10.1016/j.micpro.2020.103786,Journal,Microprocessors and Microsystems,scopus,2021-03-01,sciencedirect,Student Psychological Management System Based on FPGA Embedded System and Data Mining,https://api.elsevier.com/content/abstract/scopus_id/85099630662,"People are social beings and rarely live and work in detachment and intentionally and unknowingly create and deal with our relationships. Relationships depend primarily on the outcome of our activities and our ability to deal with our actions. From adolescence, each gains information and involvement in getting others and how to proceed in every single situation in daily life. In the existing method based on Convolution neural network and Image Processing system for the Psychological Management system. The drawback of the previous method is uneven communication in Psychological Management. So in the proposed method is based on FPGA (Field Programmable Gate Arrays) and Data Mining. The practice and understanding in communicating and monitoring relationships in our work environment. The whole arrangement of human resource management revolves around this central issue of overseeing relations in the workplace. Utilizing an individual's deep, social, psychological, and intellectual abilities in the planning and implementation from the focus on activity outcomes and emphasis on efforts made by combining the oriental hypothesis of ritual or activity benefit from complimentary brain research from the inadequate approach of the clinical infection model. The data mining method has an incredible guarantee for significant hypnotic commitment, set in real situations, and solves grave results' administrative problems.",science
10.1016/j.watres.2021.116806,Journal,Water Research,scopus,2021-03-01,sciencedirect,Soft sensor predictor of E. coli concentration based on conventional monitoring parameters for wastewater disinfection control,https://api.elsevier.com/content/abstract/scopus_id/85099446770,"Real-time acquisition of indicator bacteria concentration at the inlet of disinfection unit is a fundamental support to the control of chemical and ultraviolet wastewater disinfection. Culture-based enumeration methods need time-consuming laboratory analyses, which give results after several hours or days, while newest biosensors rarely provide information about specific strains and outputs are not directly comparable with regulatory limits as a consequence of measurement principles.
                  In this work, a novel soft sensor approach for virtual real-time monitoring of E. coli concentration is proposed. Conventional wastewater physical and chemical indicators (chemical oxygen demand, total nitrogen, nitrate, ammonia, total suspended solids, conductivity, pH, turbidity and absorbance at 254 nm) and flowrate were studied as potential predictors of E. coli concentration relying on data collected from three full-scale wastewater treatment plants. Different methods were compared: (i) linear modeling via ordinary least squares; (ii) ridge regression; (iii) principal component regression and partial least squares; (iv) non-linear modeling through artificial neural networks.
                  Linear soft sensors reached some degree of accuracy, but performances of the artificial neural network based models were by far superior. Sensitivity analysis allowed to prioritize the importance of each predictor and to highlight the site-specific nature of the approach, because of the site-specific nature of relationships between predictors and E. coli concentration. In one case study, pH and conductivity worked as good proxy variables when the occurrence of intense rain events caused sharp increases in E. coli concentration. Differently, in other case studies, chemical oxygen demand, total suspended solids, turbidity and absorbance at 254 nm accounted for the positive correlation between low wastewater quality and E. coli concentration. Moreover, sensitivity analysis of artificial neural network models highlighted the importance of interactions among predictors, contributing to 25 to 30% of the model output variance. This evidence, along with performance results, supported the idea that nonlinear families of models should be preferred in the estimation of E. coli concentration.
                  The artificial neural network based soft sensor deployment for control of peracetic acid disinfectant dosage was simulated over a realistic scenario of wastewater quality recorded by on-line sensors over 2 months. The scenario simulations highlighted the significant benefit of an E. coli soft sensor, which provided up to 57% of disinfectant saving.",science
10.1016/j.radphyschem.2020.109300,Journal,Radiation Physics and Chemistry,scopus,2021-03-01,sciencedirect,Development of a radionuclide identification algorithm based on a convolutional neural network for radiation portal monitoring system,https://api.elsevier.com/content/abstract/scopus_id/85097339486,"At border crossings around the world, plastic scintillator-based radiation portal monitors (RPMs) are employed to detect the presence of illicit radioactive materials in large trailer trucks. However, the RPM system shows a low energy resolution owing to the large size and physical characteristics of plastic scintillators; and thus, the identification of illicit artificial isotopes from naturally occurring radioactive material is difficult. This study aims to develop an advanced algorithm for radionuclide identification with commercial RPMs based on commercial plastic scintillators to reduce the occurrence of frequent nuisance alarms. Subsequently, machine learning models, namely, a convolutional neural network (CNN) was applied. The spectral distributions of energy weighted spectra were used as features of the CNN model. The energy spectra of 137Cs, 60Co, 226Ra, and 40K measured under static and moving conditions were used to implement the identification model. To evaluate the performance of the implemented model, the F-score was used. The trained CNN model correctly identified most of the radionuclides. That is, despite the theoretical Compton edge energies of 60Co and 40K being similar, the spectral distributions of 40K are distinctively different from those of 60Co. The result demonstrates that the CNN model-based identification algorithm performs robust radionuclide identification, thereby reducing the frequency of nuisance alarms at border crossings. Furthermore, considering that the actual cases of cargo passing by the RPMs are becoming more complicated, the algorithm would need to be continuously improved and trained with more complex scenarios in the future.",science
10.1016/j.jep.2020.113541,Journal,Journal of Ethnopharmacology,scopus,2021-03-01,sciencedirect,Identification and characterization of new potent inhibitors of dengue virus NS5 proteinase from Andrographis paniculata supercritical extracts on in animal cell culture and in silico approaches,https://api.elsevier.com/content/abstract/scopus_id/85095777426,"Ethnopharmacological relevance
                  About 2.5 billion peoples are at risk of dengue virus and the majority of people, use traditional plant-based medicines to combat dengue. The whole plant of Andrographis paniculata used traditionally over past decades for health promotion. Andrographolide isolated from Andrographis paniculata is used as natural remedy for the treatment of various diseases in different parts of the world. Andrographolide has been reported to have antiviral activity against hepatitis B virus, hepatitis C virus, herpes simplex virus, influenza virus, chikungunya virus, dengue virus 2 and 4.
               
                  Aim of the study
                  The aim of the present study to isolate the andrographolide from the A. paniculata by supercritical fluid extraction technique and to characterize the isolated compound along with it anti-dengue activity against DENV-2 in vitro and in silico methods.
               
                  Materials and methods
                  Supercritical extraction condition for A. paniculata was standardised to isolate andrographolide compound at definite temperature and pressure on the basis of previous study. The andrographolide was identified by using Ultraviolet–Visible Spectroscopy (UV-VIS), Fourier-Transform Infrared Spectroscopy (FT-IR) and High Performance Thin Layer Chromatography (HPTLC) and Proton Nuclear Magnetic Resonance (1HNMR). The maximum non-toxic dose of isolated andrographolide was detected by MTT assay using a micro plate reader at 595 nm. One hundred (100) copies/ml of the DENV-2 virus was used for antiviral assay in C6/36 cells lines and inhibition of virus due to andrographolide was determined by real-time PCR assay. The purity of isolated andrographolide was determined by Differential Scanning Calorimetry (DSC). The dengue NS5 receptor protein was docked with andrographolide and evaluated on the basis of the total energy and binding affinity score by Auto Dock (V4.2.6) software.
               
                  Results
                  Andrographolide, a diterpene lactone was isolated from the A. paniculata supercritical extract at 40 °C temperature and 15 Mpa pressure. UV spectrophotometer analysis revealed that the curve of andrographolide plant extract was overlapped with reference compound at 228 nm and the similar bands were detected from FT-IR spectroscopy analysis at 3315, 2917, 2849, 1673, 1462 and 1454 cm−
                     1 in isolated and standard andrographolide. HPTLC analysis shows the retention factor (Rf) of A. paniculata extract at 0.74 ± 0.06 as similar to standard andrographolide Rf values. The purity of isolated andrographolide was 99.76%. The maximum non-toxic dose of isolated andrographolide was found as 15.62 μg/ml on the C6/36 cell line calculated by using MTT assay. The andrographolide showed the 97.23% anti-dengue activity against the dengue-2 virus in C6/36 cell lines. Results of molecular docking showed that the interaction between andrographolide and NS5 of dengue protein with the maximum binding energy as −7.35 kcal/mol.
               
                  Conclusions
                  It is concluded that isolated andrographolide from the A. paniculata possess anti-dengue activity against dengue-2 virus as revealed from in vitro and in silico method. Due to lack of the vaccine and anti-viral agents, andrographolide extracted from A. paniculata play a major role to inhibit the dengue replication. Hence, it could be a source for drug design and help to reduce the dengue infection.",science
10.1016/j.knosys.2020.106734,Journal,Knowledge-Based Systems,scopus,2021-02-28,sciencedirect,Algorithm for detecting anomalous hosts based on group activity evolution,https://api.elsevier.com/content/abstract/scopus_id/85098969811,"Network behavior analysis is an active and challenging research direction in anomalous network traffic detection. With rapidly-increasing popularity of online applications, network traffic volume has grown exponentially. In the meantime, network communication is becoming more complex. Consequently, new interaction patterns of network behaviors, named group activity, need to be investigated. Group activities can be generated by various hosts over the network and changes in group activities usually cannot be captured by traditional anomaly detection methods. Currently, the primary limitation of traditional flow-based methods for network traffic analysis is the lack of a mechanism that studies the social relationship in the interaction patterns between hosts. Besides, existing approaches fail to detect group activities. To overcome these limitations, the paper aims to detect anomalous hosts based on the profile of group activity evolution. We propose a mathematical method to quantify and detect anomalous group activities based on the stability of group activity evolution, which fully captures both the temporal and structural characteristics of network evolution. This anomaly detection algorithm is called GAP (Group Activity Profile), which is a powerful supplement and extension of the traditional method of network behavior analysis. The main contributions of this paper are: (1) the paper introduces a new perspective of the group activity based on network evolution and network behavior anomaly detection; (2) the algorithm is suitable for measuring the changes in group activity evolution and determining whether the current evolution relatively conforms to or deviates from the normal evolution; and (3) this work defines the baseline of the group activity evolution by applying historical characteristics that can significantly reduce the false positive rate, and detect anomalous hosts accurately. The results of experiments conducted on real datasets demonstrate that GAP is capable of detecting anomalous host more effectively than traditional methods. GAP is free of parameters and achieves high scalability, which can effectively identify group activities as well as accurately detect anomalous hosts over time.",science
10.1016/j.quaint.2020.08.018,Journal,Quaternary International,scopus,2021-02-20,sciencedirect,Characterization of geomorphological features of lunar surface using Chandrayaan-1 Mini-SAR and LRO Mini-RF data,https://api.elsevier.com/content/abstract/scopus_id/85090021453,"The lunar surface comprises complex geomorphological features, which have been formed by the conjunction of processes namely impact cratering and volcanism. Geological features on the Lunar surface can be bifurcated into two main areas named Maria region and the Highland region. Taurus-Littrow valley, which was the Apollo-17 mission landing site, consisting of unique geomorphological characteristics by having a sample size of both Lunar Maria and Highland regions. The dielectric constant is a parameter that gives an approximate distribution of the constituent material of the target area. It is a complex quantity, which indicates a periodic variation of the electric field. The real part of dielectric constant indicates stored energy and the imaginary part indicates dielectric loss factor or the loss of the electric field in the medium due to continuous varying electric field. Planetary surfaces for which determining dielectric constant is an important analysis for most of the space missions, ground measurement is not feasible. This work includes the machine learning-based modeling of dielectric constant for the Apollo 17 landing site the Taurus-Littrow valley. Based on the surface roughness of the study area, two models Gaussian and Exponential have been implemented and compared for the modeled output of the dielectric constant values.The modeling approaches for dielectric characterization of the lunar surface were implemented on NASA's LRO Mini-RF SAR data and Mini-SAR hybrid-pol data of ISRO's Chandrayaan-1 mission. The coefficient of determination (r2) and the root mean square error (RMSE) of the theoretical Gaussian model was 0.995, 0.042 and the Exponential model was 0.948, 0.1349 respectively. When compared with the already calculated values of dielectric constant from Apollo 17 return samples and literature survey, the Gaussian model gives a better variation. Gaussian model was further applied to the Lunar north pole crater namely Hermite-A crater, whose distinctive geomorphological characteristics and location being lunar north pole region, makes it one of the coldest places in the Solar System and a prominent location of water ice deposits.",science
10.1016/j.jcp.2020.110069,Journal,Journal of Computational Physics,scopus,2021-02-15,sciencedirect,Active- and transfer-learning applied to microscale-macroscale coupling to simulate viscoelastic flows,https://api.elsevier.com/content/abstract/scopus_id/85098854830,"Active- and transfer-learning are applied to microscale dynamics of polymer flows for the multiscale discovery of effective constitutive approximations required in viscoelastic flow simulation. The result is macroscopic rheology directly connected to a microstructural model. Micro and macroscale simulations are adaptively coupled by means of Gaussian process regression (GPR) to run the expensive microscale computations only as necessary. This multiscale method is demonstrated with flows of a polymer solution as a model system. At the microscale level dissipative particle dynamics (DPD) is employed to model the fluid as a suspension of bead-spring micro-structures subjected to steady shear flow. The results yield the non-Newtonian viscosity and the first normal stress difference at strain rates as training data used in a GPR model. DPD parameters are calibrated with respect to experimental data for a real polymer solution. Compliance with these data requires adjustment of the DPD model's cutoff radius, which then becomes a function of the second invariant of the strain rate tensor. The FENE-P model is chosen for the macroscale description using the spectral element method (SEM) to simulate channel flow and flow past a circular cylinder. The DPD results at the lowest possible shear strain rate yield an estimate of the zero-shear rate viscosity, which allows the initiation of the macroscale flow by SEM as a Newtonian fluid. The resulting strain-rate field is surveyed to determine additional shear strain rate sampling points for the DPD system. This new information allows an initial fitting of parameters of the constitutive equation followed by new SEM simulations at the macroscale. Guided by active-learning GPR to select new sampling points, this process continues until convergence is achieved.
                  The effectiveness of this new simulation paradigm for viscoelastic flows is tested with different macroscale operating conditions. The effective closure learned in the channel simulation is then transferred directly to the flow past a circular cylinder at low Reynolds number, where the results show that only two additional DPD simulations are required to achieve a satisfactory constitutive model. With an increase of the Reynolds number, the active-learning scheme automatically detects the inaccuracy of the learned constitutive model, and initiates additional DPD simulations for the extra data needed to once again close the microscale-macroscale coupled system. This new paradigm of active- and transfer-learning for multiscale modeling is readily applicable to other microscale-macroscale coupled simulations of complex fluids and other materials. Furthermore, the coupling between microscale and macroscale solvers can be seamlessly implemented with our open source multiscale universal interface (MUI) library.",science
10.1016/j.abb.2020.108730,Journal,Archives of Biochemistry and Biophysics,scopus,2021-02-15,sciencedirect,Artificial intelligence in the early stages of drug discovery,https://api.elsevier.com/content/abstract/scopus_id/85098095696,"Although the use of computational methods within the pharmaceutical industry is well established, there is an urgent need for new approaches that can improve and optimize the pipeline of drug discovery and development. In spite of the fact that there is no unique solution for this need for innovation, there has recently been a strong interest in the use of Artificial Intelligence for this purpose. As a matter of fact, not only there have been major contributions from the scientific community in this respect, but there has also been a growing partnership between the pharmaceutical industry and Artificial Intelligence companies. Beyond these contributions and efforts there is an underlying question, which we intend to discuss in this review: can the intrinsic difficulties within the drug discovery process be overcome with the implementation of Artificial Intelligence? While this is an open question, in this work we will focus on the advantages that these algorithms provide over the traditional methods in the context of early drug discovery.",science
10.1016/j.patter.2020.100195,Journal,Patterns,scopus,2021-02-12,sciencedirect,Topic classification of electric vehicle consumer experiences with transformer-based deep learning,https://api.elsevier.com/content/abstract/scopus_id/85100638713,"The transportation sector is a major contributor to greenhouse gas (GHG) emissions and is a driver of adverse health effects globally. Increasingly, government policies have promoted the adoption of electric vehicles (EVs) as a solution to mitigate GHG emissions. However, government analysts have failed to fully utilize consumer data in decisions related to charging infrastructure. This is because a large share of EV data is unstructured text, which presents challenges for data discovery. In this article, we deploy advances in transformer-based deep learning to discover topics of attention in a nationally representative sample of user reviews. We report classification accuracies greater than 91% (F1 scores of 0.83), outperforming previously leading algorithms in this domain. We describe applications of these deep learning models for public policy analysis and large-scale implementation. This capability can boost intelligence for the EV charging market, which is expected to grow to US$27.6 billion by 2027.",science
10.1016/j.ins.2020.08.042,Journal,Information Sciences,scopus,2021-02-08,sciencedirect,Network-based evidential three-way theoretic model for large-scale group decision analysis,https://api.elsevier.com/content/abstract/scopus_id/85090357359,"Social relationships are critical to the group decision-making (GDM) process, especially for large-scale scenarios. Conventional GDM models have several drawbacks when applied to large-scale GDM problems. In this paper, we propose an evidential three-way theoretic model for large-scale group decision analysis based on the introduction of ego networks. A similarity matrix of all individuals is obtained after ego network generation via social network feature extraction. Rough and smooth detection are then conducted in the framework of three-way decisions. Specifically, the degree of organizational influence is analyzed based on the generated basic probability assignments (BPAs), and the individuals are divided into several organizations. After an opinion collection process, preference evolution is implemented via a social influence network (SIN) technique and a fuzzy preference relation (FPR) model. Then, the global final scores of all the alternatives are obtained using an aggregation process. Finally, we conduct a simulation experiment to illustrate the entire procedure. Based on a comparison of related methods, we believe that the proposed method can reasonably solve real-world large-scale group decision-making (LSGDM) problems and has good practicability and effectiveness.",science
10.1016/j.micpro.2020.103624,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,FPGA processor and visual keyword matching to optimize feature recognition of tourism resources,https://api.elsevier.com/content/abstract/scopus_id/85102975560,"Confirm information from the web page is a critical issue to support tourism activities. These steps are based on how the analysis and synthesis of a web page article grammatical structure. Semantic structure is a single sentence or a fragment of the marked text. This is a characteristic of developing useful tourist information from a web page to identify the way. Identification, development, and utilization of tourism resources have proved inseparable from the earth sciences. In this case, the literature does not explain Earth's tourism resources' characteristics from a scientific point of almost complete. Still, it is only mentioned in the discussion of the operation of tourism resources and tourism. Machine learning algorithms are used in the proposed architecture, which resembles a field-programmable gate array to achieve. FPGA implementation of a fixed point, comparing classification performance.",science
10.1016/j.wneu.2020.10.171,Journal,World Neurosurgery,scopus,2021-02-01,sciencedirect,Attitudes of the Surgical Team Toward Artificial Intelligence in Neurosurgery: International 2-Stage Cross-Sectional Survey,https://api.elsevier.com/content/abstract/scopus_id/85097780896,"Background
                  Artificial intelligence (AI) has the potential to disrupt how we diagnose and treat patients. Previous work by our group has demonstrated that the majority of patients and their relatives feel comfortable with the application of AI to augment surgical care. The aim of this study was to similarly evaluate the attitudes of surgeons and the wider surgical team toward the role of AI in neurosurgery.
               
                  Methods
                  In a 2-stage cross sectional survey, an initial open-question qualitative survey was created to determine the perspective of the surgical team on AI in neurosurgery including surgeons, anesthetists, nurses, and operating room practitioners. Thematic analysis was performed to develop a second-stage quantitative survey that was distributed via social media. We assessed the extent to which they agreed and were comfortable with real-world AI implementation using a 5-point Likert scale.
               
                  Results
                  In the first-stage survey, 33 participants responded. Six main themes were identified: imaging interpretation and preoperative diagnosis, coordination of the surgical team, operative planning, real-time alert of hazards and complications, autonomous surgery, and postoperative management and follow-up. In the second stage, 100 participants responded. Responders somewhat agreed or strongly agreed about AI being used for imaging interpretation (62%), operative planning (82%), coordination of the surgical team (70%), real-time alert of hazards and complications (85%), and autonomous surgery (66%). The role of AI within postoperative management and follow-up was less agreeable (49%).
               
                  Conclusions
                  This survey highlights that the majority of surgeons and the wider surgical team both agree and are comfortable with the application of AI within neurosurgery.",science
10.1016/j.jmps.2020.104277,Journal,Journal of the Mechanics and Physics of Solids,scopus,2021-02-01,sciencedirect,Thermodynamics-based Artificial Neural Networks for constitutive modeling,https://api.elsevier.com/content/abstract/scopus_id/85097707935,"Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications.
                  Here we propose a new class of data-driven, physics-based, neural networks for constitutive modeling of strain rate independent processes at the material point level, which we define as Thermodynamics-based Artificial Neural Networks (TANNs). The two basic principles of thermodynamics are encoded in the network’s architecture by taking advantage of automatic differentiation to compute the numerical derivatives of a network with respect to its inputs. In this way, derivatives of the free-energy, the dissipation rate and their relation with the stress and internal state variables are hardwired in the architecture of TANNs. Consequently, our approach does not have to identify the underlying pattern of thermodynamic laws during training, reducing the need of large data-sets. Moreover the training is more efficient and robust, and the predictions more accurate. Finally and more important, the predictions remain thermodynamically consistent, even for unseen data. Based on these features, TANNs are a starting point for data-driven, physics-based constitutive modeling with neural networks.
                  We demonstrate the wide applicability of TANNs for modeling elasto-plastic materials, using both hyper- and hypo-plasticity models. Strain hardening and softening are also considered for the hyper-plastic scenario. Detailed comparisons show that the predictions of TANNs outperform those of standard ANNs. Finally, we demonstrate that the implementation of the laws of thermodynamics confers to TANNs high robustness in the presence of noise in the training data, compared to standard approaches.
                  TANNs’ architecture is general, enabling applications to materials with different or more complex behavior, without any modification.",science
10.1016/j.ijmedinf.2020.104348,Journal,International Journal of Medical Informatics,scopus,2021-02-01,sciencedirect,Towards effective machine learning in medical imaging analysis: A novel approach and expert evaluation of high-grade glioma ‘ground truth’ simulation on MRI,https://api.elsevier.com/content/abstract/scopus_id/85097334964,"Purpose/objective(s)
                  Gliomas are uniformly fatal brain tumours with significant neurological and quality of life detriment to patients. Improvement in outcomes has remained largely unchanged in nearly 20 years. MRI (magnetic resonance imaging) is often used in diagnosis and management. Machine learning analyses of large-scale MRI data are pivotal in advancing the diagnosis, management and improve outcomes in neuro-oncology. A common challenge to robust machine learning approaches is the lack of large ‘ground truth’ datasets in supervised learning for building classification and prediction models. The creation of these datasets relies on human-expert input and is time-consuming and subjective error-prone, limiting effective machine learning applications. Simulation of mechanistic aspects such as geometry, location and physical properties of brain tumours can generate large-scale ground-truth datasets allowing for comparison of analysis techniques in clinical applications. We aimed to develop a transparent and convenient method for building ‘ground truth’ presentations of simulated glioma lesions on anatomical MRI.
               
                  Materials/methods
                  The simulation workflow was created using the Feature Manipulation Engine (FME®), a data integration platform specializing in the spatial data processing. By compiling and integrating FME’s functions to read, integrate, transform, validate, save, and display MRI data, and experimenting with ways to manipulate the parameters concerning location, size, shape, and signal intensity with the presentations of glioma, we were able to generate simulated appearances of high-grade gliomas on gadolinium-based high-resolution 3D T1-weighted MRI (1 mm3). Data of patients with canonical high-grade tumours were used as real-world tumours for validating the accuracy of the simulation. Twenty raters who are experienced with brain tumour interpretation on MRI independently completed a survey, designed to distinguish simulated and real-world brain tumours. Sensitivity and specificity were calculated for assessing the performance of the approach with the binary classification of simulated vs real-world tumours. Correlation and regression were used in run time analysis, assessing the software toolset’s efficiency in producing different numbers of simulated lesions. Differences in the group means were examined using the non-parametric Kruskal-Wallis test.
               
                  Results
                  The simulation method was developed as an interpretable and useful workflow for the easy creation of tumour simulations and incorporation into 3D MRI. A linear increase in the running time and memory usage was observed with an increasing number of generated lesions. The respondents' accuracy rate ranged between 33.3 and 83.3 %. The sensitivity and specificity were low for a human expert to differentiate simulated lesions from real gliomas (0.43 and 0.58) or vice versa (0.65 and 0.62). The mean scores ranking the real-world gliomas did not differ between the simulated and real tumours.
               
                  Conclusion
                  The reliable and user-friendly software method can allow for robust simulation of high-grade glioma on MRI. Ongoing research efforts include optimizing the workflow for generating glioma datasets as well as adapting it to simulating additional MRI brain changes.",science
10.1016/j.talanta.2020.121665,Journal,Talanta,scopus,2021-02-01,sciencedirect,Machine learning tools to estimate the severity of matrix effects and predict analyte recovery in inductively coupled plasma optical emission spectrometry,https://api.elsevier.com/content/abstract/scopus_id/85092729485,"Supervised and unsupervised machine learning methods are used to evaluate matrix effects caused by carbon and easily ionizable elements (EIEs) on analytical signals of inductively coupled plasma optical emission spectrometry (ICP OES). A simple experimental approach was used to produce a series of synthetic solutions with varying levels of matrix complexity. Analytical lines (n = 29), with total line energies (E
                     
                        sum
                     ) in the 5.0–15.5 eV range, and non-analyte signals (n = 24) were simultaneously monitored throughout the study. Labeled (supervised learning) and unlabeled (unsupervised learning) data on normalized non-analyte signals (from plasma species) were used to train machine learning models to characterize matrix effect severity and predict analyte recoveries. Dimension reduction techniques, including principal component analysis, uniform manifold approximation and projection and t-distributed stochastic neighborhood embedding, were able to provide visual and quantitative representations that correlated well with observed matrix effects on low-energy atomic and high-energy ionic emission lines. Predictive models, including partial least squares regression and generalized linear models fit with the elastic net penalty, were tuned to estimate analyte recovery error when using the external standard calibration method (EC). The best predictive results were found for high-energy ionic analytical lines, e.g. Zn II 202.548 nm (E
                     
                        sum
                      = 15.5 eV), with accuracy and R2 of 0.970 and 0.856, respectively. Two certified reference materials (CRMs) were used for method validation. The strategy described here may be used for flagging compromising matrix effects, and complement method validation based on addition/recovery experiments and CRMs analyses. Because the data analysis workflows feature signals from plasma-based species, there is potential for developing instrument software capable of alerting users in real time (i.e. before data processing) of inaccurate results when using EC.",science
10.1016/j.ins.2020.07.040,Journal,Information Sciences,scopus,2021-01-12,sciencedirect,An efficient approach to identify social disseminators for timely information diffusion,https://api.elsevier.com/content/abstract/scopus_id/85088924471,"In recent years, motivated by many practical applications such as viral marketing, researchers have paid significant attention to the circulation of information on social networks. The influential nodes that can influence the largest part of social networks are an essential topic in social network analysis. Most present solutions address the issue of discovering the influential nodes that could maximize influence effectiveness rather than minimize the cost of the information diffusion. In this paper, we investigate the circulation of emergency information, such as timely production promotion or disaster information, through a social network. These are real-life problems that have a significant effect in a very short time. We focus on how to minimize the total cost for all users in a specific social network to receive such information. We propose an efficient k-best social disseminator discovering algorithm in which the total diffusion cost on spreading timely information for each user in this social network is minimized.",science
10.1016/j.knosys.2020.106535,Journal,Knowledge-Based Systems,scopus,2021-01-09,sciencedirect,Topic sensitive hybrid expertise retrieval system in community question answering services,https://api.elsevier.com/content/abstract/scopus_id/85092942900,"Here, we propose a topic sensitive hybrid expertise retrieval system in community question answering services. We introduce three new expertise signatures: knowledge, reputation, and authority. These signatures consider the questions, and hence, their answerers from a topic sensitive perspective. We estimate the knowledge of an answerer on a new question based on the previously answered subset of questions with similar topic distributions to the new question. The reputation of an answerer, moreover, is derived from the qualities of previously answered questions by the answerer with similar distributions of topics. Furthermore, we propose a topic sensitive authority model. It considers some topic related information associated with questions and the relationships among their answerers. We compare the proposed method with 26 existing methods on 4 real-world datasets using 5 performance measures. It outperforms the comparing algorithms in 91.73% (477 out of 520) cases.",science
10.1016/j.knosys.2020.106623,Journal,Knowledge-Based Systems,scopus,2021-01-05,sciencedirect,A multi-objective linear threshold influence spread model solved by swarm intelligence-based methods,https://api.elsevier.com/content/abstract/scopus_id/85097710919,"The influence maximization problem (IMP) is one of the most important topics in social network analysis. It consists of finding the smallest seed of users that maximizes the influence spread in a social network. The main influence spread models are the linear threshold model (LT-model) and the independent cascade model (IC-model). These models have mainly been treated by using the single-objective paradigm which covers just one perspective: maximize the influence spread starting by given seed size, or minimize the seed set to reach a given number of influenced nodes. Sometimes, this minimization problem has been called the least cost influence problem (LCI). In this work, we propose a new optimization model for both perspectives under conflict, through the LT-model, by applying a binary multi-objective approach. Swarm intelligence methods are implemented to solve our proposal on real networks. Results are promising and suggest that the new multi-objective solution proposed can be properly solved in harder instances.",science
10.1016/j.ins.2020.06.047,Journal,Information Sciences,scopus,2021-01-04,sciencedirect,Sequential dynamic event recommendation in event-based social networks: An upper confidence bound approach,https://api.elsevier.com/content/abstract/scopus_id/85088022939,"In recent years, there have been some platforms that have focused on recommending commodities or events to users using event-based social networks (EBSNs). Some studies have attempted to find the optimal recommendation sequence of these items, assuming that the sequence stops once the user accepts one recommendation or the item list runs out. However, in reality, social media platforms will not stop recommending different commodities or social events to users until the user becomes bored and abandons the platform. Since it is 5 to 25 times more difficult to attract a new user than to retain an old one,
                        1
                     
                     
                        1
                        https://hbr.org/2014/10/the-value-of-keeping-the right-customers.
                      it would be helpful if the platform could determine when to stop making recommendations. In this work, we investigate the problem of sequential dynamic event recommendation with feedback (SDERF), where the platform continues recommending events even when the user has accepted one that is satisfactory. We first model the SDERF problem and provide two variants, namely, an online learning model with/without contextual information. Then, we apply an upper confidence bound (UCB) approach with an expected regret polynomial in the number of events and rounds. Finally, we evaluate the performance of our proposed algorithms using both real and synthetic datasets.",science
10.1016/j.compeleceng.2021.107570,Journal,Computers and Electrical Engineering,scopus,2021-01-01,sciencedirect,A social media-based over layer on the edge for handling emergency-related events,https://api.elsevier.com/content/abstract/scopus_id/85118991700,"Online Social Networks (OSNs), together with messaging services are tools for the exchange of entertainment-related information. However, they represent virtual environments capable of providing relevant information related to emergency or criminal events. Thanks to the simple way of using OSNs in combination to modern ubiquitous Internet of Things (IoT) smart devices, the generation and exploitation of such information is made available to users in real-time even more easily. Unfortunately, its reuse has not been taken into consideration yet due to the lack of specific models and related software tools. In this context, the paper presents a social media-based over layer for supporting the monitoring, detection, computation and information sharing of social media information related to emergency scenarios centered on smartphones and text mining techniques. The proposal is assessed through two different case studies, by evaluating the performances of different classifiers and by showing the logic of the functionalities of the related apps.",science
10.1016/j.dss.2021.113653,Journal,Decision Support Systems,scopus,2021-01-01,sciencedirect,AI-based industrial full-service offerings: A model for payment structure selection considering predictive power,https://api.elsevier.com/content/abstract/scopus_id/85114151068,"Artificial Intelligence and servitization reshape the way that manufacturing companies derive value. Aiming to sustain competitive advantage and intensify customer loyalty, full-service providers offer the use of their products as a service to achieve continuous revenues. For this purpose, companies implement AI classification algorithms to enable high levels of service at controllable costs. However, traditional asset sellers who become service providers require previously atypical payment structures, as classic payment methods involving a one-time fee for production costs and profit margins are unsuitable. In addition, a low predictive power of the implemented classification algorithm can lead to misclassifications, which diminish the achievable level of service and the intended net present value of the resultant service. While previous works focus solely on the costs of such misclassifications, our decision model highlights implications for payment structures, service levels, and – ultimately – the net present value of such data-driven service offerings. Our research suggests that predictive power can be a major factor in selecting a suitable payment structure and the overall design of service level agreements. Therefore, we compare common payment structures for data-driven services and investigate their relationship to predictive power. We develop our model using a design science methodology and iteratively evaluate our results using a four-step approach that includes interviews with industry experts and the application of our model to a real-world use case. In summary, our research extends the existing knowledge of servitization and data-driven services in the manufacturing industry through a quantitative decision model.",science
10.1016/j.ejor.2021.06.023,Journal,European Journal of Operational Research,scopus,2021-01-01,sciencedirect,"Fairness in credit scoring: Assessment, implementation and profit implications",https://api.elsevier.com/content/abstract/scopus_id/85109424638,"The rise of algorithmic decision-making has spawned much research on fair machine learning (ML). Financial institutions use ML for building risk scorecards that support a range of credit-related decisions. Yet, the literature on fair ML in credit scoring is scarce. The paper makes three contributions. First, we revisit statistical fairness criteria and examine their adequacy for credit scoring. Second, we catalog algorithmic options for incorporating fairness goals in the ML model development pipeline. Last, we empirically compare different fairness processors in a profit-oriented credit scoring context using real-world data. The empirical results substantiate the evaluation of fairness measures, identify suitable options to implement fair credit scoring, and clarify the profit-fairness trade-off in lending decisions. We find that multiple fairness criteria can be approximately satisfied at once and recommend separation as a proper criterion for measuring the fairness of a scorecard. We also find fair in-processors to deliver a good balance between profit and fairness and show that algorithmic discrimination can be reduced to a reasonable level at a relatively low cost. The codes corresponding to the paper are available on GitHub.",science
10.1016/j.isatra.2021.06.017,Journal,ISA Transactions,scopus,2021-01-01,sciencedirect,Multi-objective optimization technique for trajectory planning of multi-humanoid robots in cluttered terrain,https://api.elsevier.com/content/abstract/scopus_id/85108514869,"Humanoid robots hold a decent advantage over wheeled robots because of their ability to mimic human exile. The presented paper proposes a novel strategy for trajectory planning in a cluttered terrain using the hybridized controller modeled on the basis of modified MANFIS (multiple adaptive neuro-fuzzy inference system) and MOSFO (multi-objective sunflower optimization) techniques. The controller works in a two-step mechanism. The input parameters, i.e., obstacle distances and target direction, are first fed to the MANFIS controller, which generates a steering angle in both directions of an obstacle to dodge it. The intermediate steering angles are obtained based on the training model. The final steering angle to avoid obstacles is selected based on the direction of the target and additional obstacles in the path. It is further works as input for the MOSFO technique, which provides the ultimate steering angle. Using the proposed technique, various simulations are carried out in the WEBOT simulator, which shows a deviation under 5% when the results are validated in real-time experiments, revealing the technique to be robust. To resolve the complication of providing preference to the robot during deadlock condition in multi-humanoids system, the dining philosopher controller is implemented. The efficiency of the proposed technique is examined through the comparisons with the default controller of NAO based on toques produces at various joints that present an average improvement of 6.12%, 7.05% and 15.04% in ankle, knee and hip, respectively. It is further compared against the existed navigational strategy in multiple robot systems that also displays an acceptable improvement in travel length. In comparison in reference to the existing controller, the proposed technique emerges to be a clear winner by portraying its superiority.",science
10.1016/j.procs.2021.03.074,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Requirements towards optimizing analytics in industrial processes,https://api.elsevier.com/content/abstract/scopus_id/85106735396,"Modern production systems are composed of complex manufacturing processes with highly technology specific cause-effect relationships. Developments in sensor technology and computational science allow for data-driven decision making that facilitate effcient and objective production management. However, process data may only be beneficial if it is enriched with meta information and process expertise, reduced to relevant information and modelling results interpreted correctly. The importance of data integration in the heterogeneous industrial environment rises at the same momentum as new metrology techniques are deployed. In this paper, we focus on optimizing analytics, containing data-driven decision making for predictive quality and maintenance. We summarize key requirements for data analytics and machine learning application in industrial processes. With a use case from automotive component manufacturing we characterize industrial production, categorize process data and put requirements in context to a real-world example.",science
10.1016/j.imu.2021.100591,Journal,Informatics in Medicine Unlocked,scopus,2021-01-01,sciencedirect,The diagnostic accuracy of Artificial Intelligence-Assisted CT imaging in COVID-19 disease: A systematic review and meta-analysis,https://api.elsevier.com/content/abstract/scopus_id/85105522693,"Artificial intelligence (AI) systems have become critical in support of decision-making. This systematic review summarizes all the data currently available on the AI-assisted CT-Scan prediction accuracy for COVID-19. The ISI Web of Science, Cochrane Library, PubMed, Scopus, CINAHL, Science Direct, PROSPERO, and EMBASE were systematically searched. We used the revised Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool to assess all included studies' quality and potential bias. A hierarchical receiver-operating characteristic summary (HSROC) curve and a summary receiver operating characteristic (SROC) curve have been implemented. The area under the curve (AUC) was computed to determine the diagnostic accuracy. Finally, 36 studies (a total of 39,246 image data) were selected for inclusion into the final meta-analysis. The pooled sensitivity for AI was 0.90 (95% CI, 0.90–0.91), specificity was 0.91 (95% CI, 0.90–0.92) and the AUC was 0.96 (95% CI, 0.91–0.98). For deep learning (DL) method, the pooled sensitivity was 0.90 (95% CI, 0.90–0.91), specificity was 0.88 (95% CI, 0.87–0.88) and the AUC was 0.96 (95% CI, 0.93–0.97). In case of machine learning (ML), the pooled sensitivity was 0.90 (95% CI, 0.90–0.91), specificity was 0.95 (95% CI, 0.94–0.95) and the AUC was 0.97 (95% CI, 0.96–0.99). AI in COVID-19 patients is useful in identifying symptoms of lung involvement. More prospective real-time trials are required to confirm AI's role for high and quick COVID-19 diagnosis due to the possible selection bias and retrospective existence of currently available studies.",science
10.1016/j.procs.2021.03.075,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Input doubling method based on SVR with RBF kernel in clinical practice: Focus on small data,https://api.elsevier.com/content/abstract/scopus_id/85104314419,"In recent years, machine-learning-based approaches have become of considerable interest to the efficient processing of short or limited data samples. Its so-called small data approach. This is due to the significant growth of new intellectual analysis tasks in various industries, which are characterized by limited historical data. These include Materials Science, Economics, Medicine, and so on. An effective processing of short datasets is especially acute in medicine. Insufficient number of vectors, significant gaps in the data collected during the supervision of patient’s treatment or rehabilitation, reduces the effectiveness or prevents effective intellectual analysis based on them. This paper presents a new approach to processing short medical data samples. The basis of the developed method is SVR with RBF kernel. The algorithmic implementation of the method in both operation modes is described. Experimental modeling on a real short data set (Trabecular bone data) is conducted. It contained only 35 observations. A comparison of the method with a number of existing machine learning methods is conducted. It is experimental established the highest accuracy of the method among those considered. The developed method has potential opportunities for wide application in various fields of medicine.",science
10.1016/j.imu.2021.100566,Journal,Informatics in Medicine Unlocked,scopus,2021-01-01,sciencedirect,COVID-19 prediction using LSTM algorithm: GCC case study,https://api.elsevier.com/content/abstract/scopus_id/85104093246,"Coronavirus-19 (COVID-19) is the black swan of 2020. Still, the human response to restrain the virus is also creating massive ripples through different systems, such as health, economy, education, and tourism. This paper focuses on research and applying Artificial Intelligence (AI) algorithms to predict COVID-19 propagation using the available time-series data and study the effect of the quality of life, the number of tests performed, and the awareness of citizens on the virus in the Gulf Cooperation Council (GCC) countries at the Gulf area. So we focused on cases in the Kingdom of Saudi Arabia (KSA), United Arab of Emirates (UAE), Kuwait, Bahrain, Oman, and Qatar. For this aim, we accessed the time-series real-datasets collected from Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). The timeline of our data is from January 22, 2020 to January 25, 2021. We have implemented the proposed model based on Long Short-Term Memory (LSTM) with ten hidden units (neurons) to predict COVID-19 confirmed and death cases. From the experimental results, we confirmed that KSA and Qatar would take the most extended period to recover from the COVID-19 virus, and the situation will be controllable in the second half of March 2021 in UAE, Kuwait, Oman, and Bahrain. Also, we calculated the root mean square error (RMSE) between the actual and predicted values of each country for confirmed and death cases, and we found that the best values for both confirmed and death cases are 320.79 and 1.84, respectively, and both are related to Bahrain. While the worst values are 1768.35 and 21.78, respectively, and both are related to KSA. On the other hand, we also calculated the mean absolute relative errors (MARE) between the actual and predicted values of each country for confirmed and death cases, and we found that the best values for both confirmed and deaths cases are 37.76 and 0.30, and these are related to Kuwait and Qatar respectively. While the worst values are 71.45 and 1.33, respectively, and both are related to KSA.",science
10.1016/j.procir.2021.01.128,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,A Machine Vision-based Cyber-Physical Production System for Energy Efficiency and Enhanced Teaching-Learning Using a Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/85102656008,"Machine vision (MV) can help in achieving real-time data analysis in a manufacturing environment. This can be implemented in any industry to achieve real-time monitoring of workpieces for geometric defects and material irregularities. Identification of defects, sorting of workpieces based on their physical parameters, and analysis of process abnormalities can be achieved by using the real-time data from simple and cost-effective raspberry pi with camera and open source machine learning platform TensorFlow to run convolutional neural network (CNN) model. The proposed cyber-physical production system enables to develop a MV based system for data acquisition integrating physical entities of learning factory (LF) with the cyber world. Nowadays, LFs are widely used to train the workforce for developing competencies for emerging technologies and challenges faced due to technological advancements in Industry 4.0. This paper demonstrates the application of a cost-effective MV system in a learning factory environment to achieve real-time data acquisition and energy efficiency. The proposed low-cost machine vision is found to detect geometric irregularities, colours and surface defects. The simple cost effective MV system has enhanced the energy efficiency and reduced the total carbon footprint by 18.37 % and 78.83 % depending upon the location of MV system along the flow. The teaching-learning experience is also enhanced through action-based learning strategies. This not only ensures less rework, better control, unbiased decisions, 100% quality assurance but also the need of workers/operators can be reduced.",science
10.1016/j.procir.2021.01.115,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Development of a Decision Support System for 3D Printing Processes based on Cyber Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85102637852,"3D printing, an additive manufacturing (AM) technology, potentially provides sustainability advantages such as less waste generation, lightweight geometries, reduced material and energy consumption, lower inventory waste, etc. This paper proposes a decision support system for the 3D printing process based on Cyber Physical Production System (CPPS). The user is enabled to dynamically assess the carbon footprint based on the energy and material usage for their 3D printed object. A CPPS framework for the environmental sustainability of the 3D printing process is presented, which supports the derivation of improved strategies for product design and production. A physical world for 3D printing is used with the internet of things (IoT) devices like sensor node, webcam, smart plugs, and raspberry pi to host printer Management Software (PMS) for real-time monitoring and control of material and energy consumption during the printing process. Experiments have been conducted based on Taguchi L9 orthogonal array with polylactic Acid (PLA) as a filament material to estimate the product-related manufacturing energy consumption with the carbon footprint. The proposed framework can be effectively used by the users to supports the decision-making process for saving resources and energy; and minimizing the effect on the environment.",science
10.1016/j.patrec.2020.09.032,Journal,Pattern Recognition Letters,scopus,2021-01-01,sciencedirect,Gradient clustering algorithm based on deep learning aerial image detection,https://api.elsevier.com/content/abstract/scopus_id/85098118591,"In recent years, computer vision, especially deep learning, has been widely used in various fields. Through the deep learning aerial image detection gradient clustering algorithm automatic recognition, it can solve the limitations of manual shooting by humans, can shoot from a high altitude to a panoramic view of a specific area, and provide a more comprehensive solution. The traditional forest resource management and management work is mainly carried out by forestry personnel to carry out a large number of investigations and investigations on the forest. This method not only consumes a lot of manpower and material resources, but also does not have real-time nature. It is difficult to deal with all kinds of forest management. Problems, causing unnecessary losses. In this regard, this paper proposes an aerial image change detection algorithm based on H-KFCM, and designs related experiments to verify and demonstrate the performance of the algorithm. In this paper, we conduct a parallel study based on deep learning on the gradient clustering algorithm of deep learning in aerial image processing. By using CUDA (Compute Unified Device Architecture) to perform large-scale parallel processing of aerial data. Can greatly shorten the time to obtain results, improve the efficiency of relevant personnel. Experiment analysis. It can be seen from the results that the deep learning parallelization program implemented in this paper has a faster calculation speed and uses less time in high-resolution images, and has a good acceleration ratio compared to the CPU.",science
10.1016/j.psychres.2020.113585,Journal,Psychiatry Research,scopus,2021-01-01,sciencedirect,"Digital Gaming Interventions in Psychiatry: Evidence, Applications and Challenges",https://api.elsevier.com/content/abstract/scopus_id/85097734134,"Human evolution has regularly intersected with technology. Digitalization of various services has brought a paradigm shift in consumerism. Treading this path, mental health practice has gradually moved to Digital Mental Health Interventions (DMHI), to improve service access and delivery. Applied games are one such innovation that has gained recent popularity in psychiatry. Based on the principles of gamification, they target psychosocial and cognitive domains, according to the deficits in various psychiatric disorders. They have been used to deliver cognitive behaviour therapy, cognitive training and rehabilitation, behavioural modification, social motivation, attention enhancement, and biofeedback. Research shows their utility in ADHD, autistic spectrum disorders, eating disorders, post-traumatic stress, impulse control disorders, depression, schizophrenia, dementia, and even healthy aging. Virtual reality and artificial intelligence have been used in conjunction with gaming interventions to improvise their scope. Even though these interventions hold promise in engagement, ease of use, reduction of stigma, and bridging the mental-health gap, there are pragmatic challenges, especially in developing countries. These include network quality, infrastructure, feasibility, socio-cultural adaptability, and potential for abuse. Keeping this in the background, this review summarizes the scope, promise, and evidence of digital gaming in psychiatric practice, and highlights the potential caveats in their implementation.",science
10.1016/j.cogsys.2020.10.005,Journal,Cognitive Systems Research,scopus,2021-01-01,sciencedirect,Cogmic space for narrative-based world representation,https://api.elsevier.com/content/abstract/scopus_id/85096684889,"Representing a world or a physical/social environment in an agent’s cognitive system is essential for creating human-like artificial intelligence. This study takes a story-centered approach to this issue. In this context, a story refers to an internal representation involving a narrative structure, which is assumed to be a common form of organizing past, present, future, and fictional events and situations. In the artificial intelligence field, a story or narrative is traditionally treated as a symbolic representation. However, a symbolic story representation is limited in its representational power to construct a rich world. For example, a symbolic story representation is unfit to handle the sensory/bodily dimension of a world. In search of a computational theory for narrative-based world representation, this study proposes the conceptual framework of a Cogmic Space for a comic strip-like representation of a world. In the proposed framework, a story is positioned as a mid-level representation, in which the conceptual and sensory/bodily dimensions of a world are unified. The events and their background situations that constitute a story are unified into a sequence of panels. Based on this structure, a representation (i.e., a story) and the represented environment are connected via an isomorphism of their temporal, spatial, and relational structures. Furthermore, the framework of a Cogmic Space is associated with the generative aspect of representations, which is conceptualized in terms of unconscious- and conscious-level processes/representations. Finally, a proof-of-concept implementation is presented to provide a concrete account of the proposed framework.",science
10.1016/j.neunet.2020.10.002,Journal,Neural Networks,scopus,2021-01-01,sciencedirect,Gradient-based training and pruning of radial basis function networks with an application in materials physics,https://api.elsevier.com/content/abstract/scopus_id/85096164921,"Many applications, especially in physics and other sciences, call for easily interpretable and robust machine learning techniques. We propose a fully gradient-based technique for training radial basis function networks with an efficient and scalable open-source implementation. We derive novel closed-form optimization criteria for pruning the models for continuous as well as binary data which arise in a challenging real-world material physics problem. The pruned models are optimized to provide compact and interpretable versions of larger models based on informed assumptions about the data distribution. Visualizations of the pruned models provide insight into the atomic configurations that determine atom-level migration processes in solid matter; these results may inform future research on designing more suitable descriptors for use with machine learning algorithms.",science
10.1016/j.scs.2020.102582,Journal,Sustainable Cities and Society,scopus,2021-01-01,sciencedirect,Towards the sustainable development of smart cities through mass video surveillance: A response to the COVID-19 pandemic,https://api.elsevier.com/content/abstract/scopus_id/85096158767,"Sustainable smart city initiatives around the world have recently had great impact on the lives of citizens and brought significant changes to society. More precisely, data-driven smart applications that efficiently manage sparse resources are offering a futuristic vision of smart, efficient, and secure city operations. However, the ongoing COVID-19 pandemic has revealed the limitations of existing smart city deployment; hence; the development of systems and architectures capable of providing fast and effective mechanisms to limit further spread of the virus has become paramount. An active surveillance system capable of monitoring and enforcing social distancing between people can effectively slow the spread of this deadly virus. In this paper, we propose a data-driven deep learning-based framework for the sustainable development of a smart city, offering a timely response to combat the COVID-19 pandemic through mass video surveillance. To implementing social distancing monitoring, we used three deep learning-based real-time object detection models for the detection of people in videos captured with a monocular camera. We validated the performance of our system using a real-world video surveillance dataset for effective deployment.",science
10.1016/j.engfailanal.2020.104958,Journal,Engineering Failure Analysis,scopus,2021-01-01,sciencedirect,Wear prediction of a mechanism with multiple joints based on ANFIS,https://api.elsevier.com/content/abstract/scopus_id/85092083385,"Condition monitoring data of joints in a mechanism contains enormous useful information, and can comprehensively improve the wear prediction accuracy. However, condition data of the joints is sometimes hard to obtain due to technical reasons or cost reasons, especially for some complicated mechanical systems. To obtain the real-time wear data of joints in a mechanism with multiple joints, an ANFIS-based (adaptive-network-based fuzzy inference system) joints clearance size prognostic method is developed based on monitored motion outputs of the mechanism. Then, a framework for wear prediction based on multi-body dynamics theory is proposed to predict joints wear more accurately. In the framework, the Archard’s wear model is used. To reduce the uncertainty in the wear coefficient, wear coefficient is treated as a random variable, then a Bayesian updating process is implemented according to the wear data from the ANFIS-based method. The proposed framework is validated using wear experiments of a lock mechanism with three joints in a cabin door. The results show the prediction error is within 3%.",science
10.1016/j.envres.2020.110141,Journal,Environmental Research,scopus,2021-01-01,sciencedirect,Assessing personal exposure using Agent Based Modelling informed by sensors technology,https://api.elsevier.com/content/abstract/scopus_id/85092078286,"Technology innovations create possibilities to capture exposure-related data at a great depth and breadth. Considering, though, the substantial hurdles involved in collecting individual data for whole populations, this study introduces a first approach of simulating human movement and interaction behaviour, using Agent Based Modelling (ABM).
                  A city scale ABM was developed for urban Thessaloniki, Greece that feeds into population-based exposure assessment without imposing prior bias, basing its estimations onto emerging properties of the behaviour of the computerised autonomous decision makers (agents) that compose the city-system. Population statistics, road and buildings networks data were transformed into human, road and building agents, respectively. Survey outputs with time-use patterns were associated with human agent rules, aiming to model representative to real-world behaviours. Moreover, time-geography of exposure data, derived from a local sensors campaign, was used to inform and enhance the model. As a prevalence of an agent-specific decision-making, virtual individuals of different sociodemographic backgrounds express different spatiotemporal behaviours and their trajectories are coupled with spatially resolved pollution levels.
                  Personal exposure was evaluated by assigning PM concentrations to human agents based on coordinates, type of location and intensity of encountered activities. Study results indicated that PM2.5 inhalation adjusted exposure between housemates can differ by 56.5% whereas exposure between two neighbours can vary by as much as 87%, due to the prevalence of different behaviours.
                  This study provides details of a new methodology that permits the cost-effective construction of refined time-activity diaries and daily exposure profiles, taking into account different microenvironments and sociodemographic characteristics. The proposed method leads to a refined exposure assessment model, addressing effectively vulnerable subgroups of population. It can be used for evaluating the probable impacts of different public health policies prior to implementation reducing, therefore, the time and expense required to identify efficient measures.",science
10.1016/j.jneumeth.2020.108927,Journal,Journal of Neuroscience Methods,scopus,2021-01-01,sciencedirect,Automatic classification methods for detecting drowsiness using wavelet packet transform extracted time-domain features from single-channel EEG signal,https://api.elsevier.com/content/abstract/scopus_id/85091767602,"Background
                  Detecting human drowsiness during some critical works like vehicle driving, crane operating, mining blasting, etc. is one of the safeguards to prevent accidents. Among several drowsiness detection (DD) methods, a combination of neuroscience and computer science knowledge has a better ability to differentiate awake and sleep states. Most of the current models are implemented using multi-sensors electroencephalogram (EEG) signals, multi-domain features, predefined features selection algorithms. Therefore, there is great interest in the method of detecting drowsiness on embedded platforms with improved accuracy using generalized best features.
               
                  New-method
                  Single-channel EEG based drowsiness detection (DD) model is proposed in this by utilizing wavelet packet transform (WPT) to extract the time-domain features from considered channel EEG. The dimension of the feature vector is reduced by the proposed novel feature selection method.
               
                  Results
                  The proposed model on freely available real-time sleep analysis EEG and Simulated Virtual Driving Driver (SVDD) EEG achieves 94.45% and 85.3% accuracy, respectively.
               
                  Comparison-with-existing-method
                  The results show that the proposed DD method produces better accuracy compared to the state-of-the-art using the physiological dataset with the proposed time-domain sub-band-based features and feature selection method. This task of detecting drowsiness by analyzing the 5-seconds EEG signal with four features is an improvement to my previous work on detecting drowsiness using a 30-seconds EEG signal with 66 features.
               
                  Conclusions
                  Time-domain features obtained from EEG time-domain sub-bands collected using WPT achieving excellent accuracy rate by selecting unique optimization features for all subjects by the proposed feature selection algorithm.",science
10.1016/j.prro.2020.07.003,Journal,Practical Radiation Oncology,scopus,2021-01-01,sciencedirect,Time Analysis of Online Adaptive Magnetic Resonance–Guided Radiation Therapy Workflow According to Anatomical Sites,https://api.elsevier.com/content/abstract/scopus_id/85090017005,"Purpose
                  To document time analysis of detailed workflow steps for the online adaptive magnetic resonance–guided radiation therapy treatments (MRgRT) with the ViewRay MRIdian system and to identify the barriers to and solutions for shorter treatment times.
               
                  Methods and Materials
                  A total of 154 patients were treated with the ViewRay MRIdian system between September 2018 and October 2019. The time process of MRgRT workflow steps of 962 fractions for 166 treatment sites was analyzed in terms of patient and online adaptive treatment (ART) characteristics.
               
                  Results
                  Overall, 774 of 962 fractions were treated with online ART, and 83.2% of adaptive fractions were completed in less than 60 minutes. Sixty-three percent, 50.3%, and 4.2% of fractions were completed in less than 50 minutes, 45 minutes, and 30 minutes, respectively. Eight-point-three percent and 3% of fractions were completed in more than 70 minutes and 80 minutes, respectively. The median time (tmed) for ART workflow steps were as follows: (1) setup tmed: 5.0 minutes, (2) low-resolution scanning tmed: 1 minute, (3) high-resolution scanning tmed: 3 minutes, (4) online contouring tmed: 9 minutes, (5) reoptimization with online quality assurance tmed: 5 minutes, (6) real targeting tmed: 3 minutes, (7) beam delivery with gating tmed: 17 minutes, and (8) net total treatment time tmed: 45 minutes. The shortest and longest tmean rates of net total treatment time were 41.59 minutes and 64.43 minutes for upper-lung-lobe-located thoracic tumors and ultracentrally located thoracic tumors, respectively.
               
                  Conclusions
                  To our knowledge, this is the first broad treatment-time analysis for online ART in the literature. Although treatment times are long due to human- and technology-related limitations, benefits offered by MRgRT might be clinically important. In the future, implementation of artificial intelligence segmentation, an increase in dose rate, and faster multileaf collimator and gantry speeds may lead to achieving shorter MRgRT treatments.",science
10.1016/j.jmsy.2020.06.012,Journal,Journal of Manufacturing Systems,scopus,2021-01-01,sciencedirect,"A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence",https://api.elsevier.com/content/abstract/scopus_id/85087690907,"Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.",science
10.1016/j.patter.2020.100145,Journal,Patterns,scopus,2020-12-11,sciencedirect,A Machine Learning-Aided Global Diagnostic and Comparative Tool to Assess Effect of Quarantine Control in COVID-19 Spread,https://api.elsevier.com/content/abstract/scopus_id/85097386310,"We have developed a globally applicable diagnostic COVID-19 model by augmenting the classical SIR epidemiological model with a neural network module. Our model does not rely upon previous epidemics like SARS/MERS and all parameters are optimized via machine learning algorithms used on publicly available COVID-19 data. The model decomposes the contributions to the infection time series to analyze and compare the role of quarantine control policies used in highly affected regions of Europe, North America, South America, and Asia in controlling the spread of the virus. For all continents considered, our results show a generally strong correlation between strengthening of the quarantine controls as learnt by the model and actions taken by the regions' respective governments. In addition, we have hosted our quarantine diagnosis results for the top 70 affected countries worldwide, on a public platform.",science
10.1016/j.patter.2020.100139,Journal,Patterns,scopus,2020-12-11,sciencedirect,scTenifoldNet: A Machine Learning Workflow for Constructing and Comparing Transcriptome-wide Gene Regulatory Networks from Single-Cell Data,https://api.elsevier.com/content/abstract/scopus_id/85096558897,"We present scTenifoldNet—a machine learning workflow built upon principal-component regression, low-rank tensor approximation, and manifold alignment—for constructing and comparing single-cell gene regulatory networks (scGRNs) using data from single-cell RNA sequencing. scTenifoldNet reveals regulatory changes in gene expression between samples by comparing the constructed scGRNs. With real data, scTenifoldNet identifies specific gene expression programs associated with different biological processes, providing critical insights into the underlying mechanism of regulatory networks governing cellular transcriptional activities.",science
10.1016/j.patter.2020.100137,Journal,Patterns,scopus,2020-12-11,sciencedirect,Parallel Factor Analysis Enables Quantification and Identification of Highly Convolved Data-Independent-Acquired Protein Spectra,https://api.elsevier.com/content/abstract/scopus_id/85096522758,"High-throughput data-independent acquisition (DIA) is the method of choice for quantitative proteomics, combining the best practices of targeted and shotgun approaches. The resultant DIA spectra are, however, highly convolved and with no direct precursor-fragment correspondence, complicating biological sample analysis. Here, we present CANDIA (canonical decomposition of data-independent-acquired spectra), a GPU-powered unsupervised multiway factor analysis framework that deconvolves multispectral scans to individual analyte spectra, chromatographic profiles, and sample abundances, using parallel factor analysis. The deconvolved spectra can be annotated with traditional database search engines or used as high-quality input for de novo sequencing methods. We demonstrate that spectral libraries generated with CANDIA substantially reduce the false discovery rate underlying the validation of spectral quantification. CANDIA covers up to 33 times more total ion current than library-based approaches, which typically use less than 5% of total recorded ions, thus allowing quantification and identification of signals from unexplored DIA spectra.",science
10.1016/j.comnet.2020.107573,Journal,Computer Networks,scopus,2020-12-09,sciencedirect,AI-enabled mobile multimedia service instance placement scheme in mobile edge computing,https://api.elsevier.com/content/abstract/scopus_id/85091771160,"Leveraging cloud infrastructure to the mobile edge computing helps the mobile users to get real time multimedia services in Fifth Generation (5G) network system. To ensure higher Quality-of-Experience (QoE), faster migration of mobile multimedia service instances is required to cope up with user mobility. By deploying the mobile multimedia service instances proactively in multiple edge nodes (ENs) helps the users to get higher QoE. However, excessive deployment of service replicas might increase the cost of the overall network. To establish trade-off between these two conflicting objectives, we have formulated the problem as a Multi-objective Integer Linear Programming (MILP) by integrating the users’ path prediction model. This problem is proven to be an NP-hard one for large networks, thus we develop an artificial intelligence (AI) based meta-heuristic Binary Particle Swarm Optimization (BPSO) algorithm to achieve near-optimal solution within polynomial time. The performance analysis results show the significant performance improvement in terms of QoE and user satisfaction as compared to other state-of-the-art works.",science
10.1016/j.asoc.2020.106754,Journal,Applied Soft Computing Journal,scopus,2020-12-01,sciencedirect,Sentiment Analysis of COVID-19 tweets by Deep Learning Classifiers—A study to show how popularity is affecting accuracy in social media,https://api.elsevier.com/content/abstract/scopus_id/85092457474,"COVID-19 originally known as Corona VIrus Disease of 2019, has been declared as a pandemic by World Health Organization (WHO) on 11th March 2020. Unprecedented pressures have mounted on each country to make compelling requisites for controlling the population by assessing the cases and properly utilizing available resources. The rapid number of exponential cases globally has become the apprehension of panic, fear and anxiety among people. The mental and physical health of the global population is found to be directly proportional to this pandemic disease. The current situation has reported more than twenty four million people being tested positive worldwide as of 27th August, 2020. Therefore, it is the need of the hour to implement different measures to safeguard the countries by demystifying the pertinent facts and information. This paper aims to bring out the fact that tweets containing all handles related to COVID-19 and WHO have been unsuccessful in guiding people around this pandemic outbreak appositely. This study analyzes two types of tweets gathered during the pandemic times. In one case, around twenty three thousand most re-tweeted tweets within the time span from 1st Jan 2019 to 23rd March 2020 have been analyzed and observation says that the maximum number of the tweets portrays neutral or negative sentiments. On the other hand, a dataset containing 226,668 tweets collected within the time span between December 2019 and May 2020 have been analyzed which contrastingly show that there were a maximum number of positive and neutral tweets tweeted by netizens. The research demonstrates that though people have tweeted mostly positive regarding COVID-19, yet netizens were busy engrossed in re-tweeting the negative tweets and that no useful words could be found in WordCloud or computations using word frequency in tweets. The claims have been validated through a proposed model using deep learning classifiers with admissible accuracy up to 81%. Apart from these the authors have proposed the implementation of a Gaussian membership function based fuzzy rule base to correctly identify sentiments from tweets. The accuracy for the said model yields up to a permissible rate of 79%.",science
10.1016/j.arr.2020.101174,Journal,Ageing Research Reviews,scopus,2020-12-01,sciencedirect,"A research agenda for ageing in China in the 21st century (2nd edition): Focusing on basic and translational research, long-term care, policy and social networks",https://api.elsevier.com/content/abstract/scopus_id/85091870837,"One of the key issues facing public healthcare is the global trend of an increasingly ageing society which continues to present policy makers and caregivers with formidable healthcare and socio-economic challenges. Ageing is the primary contributor to a broad spectrum of chronic disorders all associated with a lower quality of life in the elderly. In 2019, the Chinese population constituted 18 % of the world population, with 164.5 million Chinese citizens aged 65 and above (65+), and 26 million aged 80 or above (80+). China has become an ageing society, and as it continues to age it will continue to exacerbate the burden borne by current family and public healthcare systems. Major healthcare challenges involved with caring for the elderly in China include the management of chronic non-communicable diseases (CNCDs), physical frailty, neurodegenerative diseases, cardiovascular diseases, with emerging challenges such as providing sufficient dental care, combating the rising prevalence of sexually transmitted diseases among nursing home communities, providing support for increased incidences of immune diseases, and the growing necessity to provide palliative care for the elderly. At the governmental level, it is necessary to make long-term strategic plans to respond to the pressures of an ageing society, especially to establish a nationwide, affordable, annual health check system to facilitate early diagnosis and provide access to affordable treatments. China has begun work on several activities to address these issues including the recent completion of the of the Ten-year Health-Care Reform project, the implementation of the Healthy China 2030 Action Plan, and the opening of the National Clinical Research Center for Geriatric Disorders. There are also societal challenges, namely the shift from an extended family system in which the younger provide home care for their elderly family members, to the current trend in which young people are increasingly migrating towards major cities for work, increasing reliance on nursing homes to compensate, especially following the outcomes of the ‘one child policy’ and the ‘empty-nest elderly’ phenomenon. At the individual level, it is important to provide avenues for people to seek and improve their own knowledge of health and disease, to encourage them to seek medical check-ups to prevent/manage illness, and to find ways to promote modifiable health-related behaviors (social activity, exercise, healthy diets, reasonable diet supplements) to enable healthier, happier, longer, and more productive lives in the elderly. Finally, at the technological or treatment level, there is a focus on modern technologies to counteract the negative effects of ageing. Researchers are striving to produce drugs that can mimic the effects of ‘exercising more, eating less’, while other anti-ageing molecules from molecular gerontologists could help to improve ‘healthspan’ in the elderly. Machine learning, ‘Big Data’, and other novel technologies can also be used to monitor disease patterns at the population level and may be used to inform policy design in the future. Collectively, synergies across disciplines on policies, geriatric care, drug development, personal awareness, the use of big data, machine learning and personalized medicine will transform China into a country that enables the most for its elderly, maximizing and celebrating their longevity in the coming decades. This is the 2nd edition of the review paper (Fang EF et al., Ageing Re. Rev. 2015).",science
10.1016/j.cma.2020.113371,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2020-12-01,sciencedirect,Stochastic nonlocal damage analysis by a machine learning approach,https://api.elsevier.com/content/abstract/scopus_id/85089593784,"A machine learning aided stochastic nonlocal damage analysis framework is proposed for quasi-brittle materials. The uncertain system parameters, including the material properties and loading actions, have been incorporated and analysed within a unified safety assessment framework against various working conditions. A three-dimensional integral-type nonlocal damage model through finite element method (FEM) has been adopted. For the purpose of investigating the probabilistic damage analysis problems, a freshly established machine learning approach, namely the capped-extended-support vector regression method (C-X-SVR), is proposed to eliminate the influences of random outliers in the first step, then establish the relationship between the uncertain systemic inputs and structural responses. Such that the training robustness and computational adaptability of the proposed regression model can be reinforced. Moreover, the proposed approach is competent of efficiently predicting the statistical information (i.e., means, standard deviations, probability density functions and cumulative density functions) of structural behaviours under continuous information update of the uncertain working condition from mercurial environment. One real-life experimental validation and two numerical investigations are implemented to further verify the effectiveness and efficiency of the uncertainty quantification framework against probabilistic damage analysis.",science
10.1016/j.pcd.2020.05.005,Journal,Primary Care Diabetes,scopus,2020-12-01,sciencedirect,The prevalence of chronic kidney disease and screening of renal function in type 2 diabetic patients in Finnish primary healthcare,https://api.elsevier.com/content/abstract/scopus_id/85086126998,"Aims
                  To estimate the prevalence of chronic kidney disease (CKD) in patients with type 2 diabetes (T2D) in Finnish primary healthcare, and to evaluate the screening for CKD and the proportions of patients receiving antihyperglycemic and cardiovascular preventive medication.
               
                  Material and methods
                  T2D patients treated at the Rovaniemi Health Center, Finland during the years 2015–2019. Data included patient characteristics, blood pressure, HbA1c, lipid levels, kidney function and albuminuria, and medications prescribed. CKD was defined as estimated glomerular filtration rate (eGFR) <60 ml/min/1.72 m2 and/or albuminuria.
               
                  Results
                  The study population comprised of 5112 T2D patients with a mean (SD) age of 66.7 (13.0) years. Of these, 60.2% were screened for CKD with both eGFR and albuminuria, and 30.1% of these patients had CKD. The prevalence of moderately increased and severely increased albuminuria was 19.6% and 3.2%, respectively. A total of 57.0% of the study population received angiotensin-converting enzyme (ACE) inhibitors or angiotensin receptor blockers (ARB).
               
                  Conclusions
                  Screening for CKD with both recommended measures (eGFR and albuminuria) was insufficiently performed among this T2D population. Additionally, just over half of the study population had been prescribed ACE inhibitors or ARB. These results suggest an incongruity between the gold standard of diabetes care and real-world clinical practice.",science
10.1016/j.patter.2020.100107,Journal,Patterns,scopus,2020-11-13,sciencedirect,Wiz: A Web-Based Tool for Interactive Visualization of Big Data,https://api.elsevier.com/content/abstract/scopus_id/85097417500,"In an age of information, visualizing and discerning meaning from data is as important as its collection. Interactive data visualization addresses both fronts by allowing researchers to explore data beyond what static images can offer. Here, we present Wiz, a web-based application for handling and visualizing large amounts of data. Wiz does not require programming or downloadable software for its use and allows scientists and non-scientists to unravel the complexity of data by splitting their relationships through 5D visual analytics, performing multivariate data analysis, such as principal component and linear discriminant analyses, all in vivid, publication-ready figures. With the explosion of high-throughput practices for materials discovery, information streaming capabilities, and the emphasis on industrial digitalization and artificial intelligence, we expect Wiz to serve as an invaluable tool to have a broad impact in our world of big data.",science
10.1016/j.mechatronics.2020.102436,Journal,Mechatronics,scopus,2020-11-01,sciencedirect,Modeling the thermo-mechanical deformations of machine tool structures in CFRP material adopting data-driven prediction schemes,https://api.elsevier.com/content/abstract/scopus_id/85092002525,"The thermo-mechanical effects in machine tools (MTs) are represented by complex models since they may produce non-linear distortions overtime, impacting significantly on the machining accuracy. This paper aims to model the deformation of CFRP (Carbon-Fiber-Reinforced-Polymers) structures using data-driven schemes to predict and compensate the structural thermo-mechanical behavior. A novel study is presented to investigate the thermally-induced distortions of CFPR structural materials, selecting and positioning sensors, simulating and validating models to compensate the error in real-time. Anisotropic materials are becoming an effective solution to reduce structure mass and increase damping of a MT, nevertheless their physical complexity and the different thermal-coefficients at the interface with conventional materials may generate undesired effects, limiting the obtained advantages. The proposed strategy is based on the evaluation of a set of data-driven models simultaneously, identifying the most suitable solution and comparing finite element simulations with machine learning approach. The study is developed on a vertical axis frame made of CFRP material. The experimental validation is executed on a commercial 5-axis machine tool by varying the temperature conditions and evaluating the structural thermo-mechanical deformation effect on the Tool-Tip-Point (TTP) displacement. The thermo-mechanical behavior is measured by fiber Bragg grating (FBG) sensing technology embedded in the CFRP structure. Data-driven lab tests are evaluated in operational conditions during 36 h, considering: i) training-deployment periods (875 min interval), ii) typical machining stresses and iii) environmental perturbations. The final selected data-driven model is able to reduce the detected error lower than 10 μm range. In particular, the achieved results indicate a congruence between the TTP displacement measured and predicted with a residual error lower than 7.0 μm (Y-direction) using the ANN- multilayer perceptron algorithm.",science
10.1016/j.mpdhp.2020.08.004,Journal,Diagnostic Histopathology,scopus,2020-11-01,sciencedirect,Artificial intelligence in pathology: an overview,https://api.elsevier.com/content/abstract/scopus_id/85090478810,"Artificial intelligence (AI) is at the forefront of modern technology and emerging uses within the healthcare sector are now being realised. Pathology will be a key area where the impact of AI will be felt. With more and more laboratories making the transition to digital pathology this will provide the key infrastructure in which to deploy these tools and their use will start to become a reality in diagnostic practice. The potential of AI in pathology is to create image analysis tools which could either be used for diagnostic support or to derive novel insights into disease biology, in addition to those achievable with a human observer. Some examples providing diagnostic support currently exist for a limited, but expanding number of applications, such as tumour detection, automated tumour grading, immunohistochemistry scoring, and predicting mutation status. There are a number of challenges to consider, not least the validation and regulatory framework for these tools. In this article, we set out an overview of AI in histopathology, discuss its potential workflow applications, and give key examples of the potential for AI in clinical practice. Considerations for the implementation of AI in practice are also explored.",science
10.1016/j.ipm.2020.102340,Journal,Information Processing and Management,scopus,2020-11-01,sciencedirect,A topic modeling framework for spatio-temporal information management,https://api.elsevier.com/content/abstract/scopus_id/85087504158,"Real-time processing and learning of conflicting data, especially messages coming from different ideas, locations, and time, in a dynamic environment such as Twitter is a challenging task that recently gained lots of attention. This paper introduces a framework for managing, processing, analyzing, detecting, and tracking topics in streaming data. We propose a model selector procedure with a hybrid indicator to tackle the challenge of online topic detection. In this framework, we built an automatic data processing pipeline with two levels of cleaning. Regular and deep cleaning are applied using multiple sources of meta knowledge to enhance data quality. Deep learning and transfer learning techniques are used to classify health-related tweets, with high accuracy and improved F1-Score. In this system, we used visualization to have a better understanding of trending topics. To demonstrate the validity of this framework, we implemented and applied it to health-related twitter data from users originating in the USA over nine months. The results of this implementation show that this framework was able to detect and track the topics at a level comparable to manual annotation. To better explain the emerging and changing topics in various locations over time the result is graphically displayed on top of the United States map.",science
10.1016/j.petrol.2020.107509,Journal,Journal of Petroleum Science and Engineering,scopus,2020-11-01,sciencedirect,"Design and construction of the knowledge base system for geological outfield cavities classifications: An example of the fracture-cavity reservoir outfield in Tarim basin, NW China",https://api.elsevier.com/content/abstract/scopus_id/85087076723,"Tahe oilfield, located in NW Tarim Basin, is one of the largest and most difficult fracture cavity reservoirs in the world. Different fracture cavities, different generated mechanisms, and different oil production capacities. In order to study the significant parameters that can characterize the categories of facture-cavity. This research adopted outfield manual measurement, 3D digital modeling technique to obtain characterization parameters. According to experienced geological survey, typical outcrops were selected, then scanned by UAV (Unmanned aerial vehicle). Consequently, 3D digital models, including real coordinates and parameter information, were established by Agisoft Photoscan. Through geological testing results, various combination characteristic patterns of relative categories were analyzed. By using digital measure tool, combined with manually measured data, the parameters were extracted from the 3D digital model (DM). Then an initial geological database was established. For furtherly analyzing the database, the mathematic statistics methods of multiple linear regression (MLR), neural network technique (NNT) and discriminative classification technique (DCT) were applied. Using software of SPSS statistics 17.0, more than 200 groups of geological data (various categories of fracture-cavity) were optimally processed. Consequently, the significant characteristic parameters were interpreted to determine diverse categories. The results showed that: (1) cavity width, height, fracture length and cavity aspect ratio were significant parameters to classify runoff cavity categories. (2) Fault-controlled cavities could be accurately classified by fracture length and fracture density. (3) The main cavity categories could be distinguished by cavity width, cavity height and fracture density. Performances of the approach have been examined with 10 percentages of the samples, and a good agreement performed in the simulated results, and anastomosis rate was more than 80%. The researched results have critical guiding significance to evaluate types of fracture-cavity, develop and explore of fracture-cavity reservoirs. The construction technique of knowledge base can be applied for diverse fracture-cavity reservoirs in the various formations in different areas in the world.",science
10.1016/j.jpdc.2020.06.010,Journal,Journal of Parallel and Distributed Computing,scopus,2020-11-01,sciencedirect,Performance enhancement of a dynamic K-means algorithm through a parallel adaptive strategy on multicore CPUs,https://api.elsevier.com/content/abstract/scopus_id/85086986038,"The K-means algorithm is one of the most popular algorithms in Data Science, and it is aimed to discover similarities among the elements belonging to large datasets, partitioning them in 
                        K
                      distinct groups called clusters. The main weakness of this technique is that, in real problems, it is often impossible to define the value of 
                        K
                      as input data. Furthermore, the large amount of data used for useful simulations makes impracticable the execution of the algorithm on traditional architectures. In this paper, we address the previous two issues. On the one hand, we propose a method to dynamically define the value of 
                        K
                      by optimizing a suitable quality index with special care to the computational cost. On the other hand, to improve the performance and the effectiveness of the algorithm, we propose a strategy for parallel implementation on modern multicore CPUs.",science
10.1016/j.jpba.2020.113544,Journal,Journal of Pharmaceutical and Biomedical Analysis,scopus,2020-10-25,sciencedirect,Development and validation of UHPLC-PDA method for simultaneous determination of bioactive polyphenols of horse-chestnut bark using numerical optimization with MS Excel Solver,https://api.elsevier.com/content/abstract/scopus_id/85089903464,"A fast and eﬃcient UHPLC-PDA method was developed for quantification of polyphenols of horse-chestnut (Aesculus hippocastanum) bark, an herbal medicinal agent used for the treatment of vascular ailments. To optimize a simple one step-gradient separation, four main factors were selected (initial acetonitrile concentration, gradient slope, flow rate and temperature). Retention times and widths of ten target peaks were modelled using central composite design, and the optimal compromise between the time of the analysis and resolution was found using Derringer’s desirability function and numerical optimization procedure with the use of MS Excel software. The optimal separation conditions were as follows: initial acetonitrile concentration of 9.9 %, gradient slope of 2.4 %/min, flow rate of 0.7 mL/min and temperature of 35 °C. Chromatographic separation was carried out on a Titan C18 column (1.9 μm, 100 × 2.1 mm i.d.; Supelco) and was completed within 5 min with minimal resolution between the observed critical pairs of 1.93. The experimental results were in good accordance with the ones calculated from the models. Stability of the analytes in standard solutions and processed plant samples was found acceptable up to 14 days of storage at 4 °C. The validation proved also good linearity (r > 0.99995), precision (RSD < 4 %), accuracy (96.8–99.2 %), and sensitivity (LOQs 0.14−0.61 ng; LODs 0.05–0.20 ng) of the method, and its applicability for quantification of six primary coumarins and flavan-3-ols in A. hippocastanum bark was demonstrated for real commercial samples (different providers and years of collection). Additionally, the good separation of the plant matrix allowed for determination of the approximate contents of three minor constituents. The developed procedure proved to be a useful tool in quality control of the herbal material, and MS Excel software was demonstrated to be valuable for optimization of Derringer’s function and visualization of the modelled separation.",science
10.1016/j.knosys.2020.106256,Journal,Knowledge-Based Systems,scopus,2020-10-12,sciencedirect,Community detection based on the Matthew effect,https://api.elsevier.com/content/abstract/scopus_id/85088031223,"Community structure exists in most real-world networks, such as social networks, smart grids, and transportation networks. Established approaches for community detection usually depend on some user-defined criteria (e.g., minimum cut, normalized cut, modularity, etc.). These criteria-based methods usually involve some optimization procedures and need to specify some parameters, which are thus time consuming and sensitive to parameters. In this paper, inspired by the Mathew effect of human society, we view a network as a social system and design a new algorithm called CDME (community detection based on the Matthew effect). Relying on the new concept, CDME has many desirable properties. It allows uncovering high-quality communities driven by dynamic CDME is also parameter free. More importantly, since CDME works in a local way and only needs to calculate the attractiveness of neighboring nodes, which lend itself to handling large-scale networks. Experiments on both synthetic and real-world data sets have demonstrated that CDME has many benefits and outperforms many state-of-the-art algorithms.",science
10.1016/j.patter.2020.100108,Journal,Patterns,scopus,2020-10-09,sciencedirect,Using Machine Learning to Identify Adverse Drug Effects Posing Increased Risk to Women,https://api.elsevier.com/content/abstract/scopus_id/85102228908,"Adverse drug reactions are the fourth leading cause of death in the US. Although women take longer to metabolize medications and experience twice the risk of developing adverse reactions compared with men, these sex differences are not comprehensively understood. Real-world clinical data provide an opportunity to estimate safety effects in otherwise understudied populations, i.e., women. These data, however, are subject to confounding biases and correlated covariates. We present AwareDX, a pharmacovigilance algorithm that leverages advances in machine learning to predict sex risks. Our algorithm mitigates these biases and quantifies the differential risk of a drug causing an adverse event in either men or women. AwareDX demonstrates high precision during validation against clinical literature and pharmacogenetic mechanisms. We present a resource of 20,817 adverse drug effects posing sex-specific risks. AwareDX, and this resource, present an opportunity to minimize adverse events by tailoring drug prescription and dosage to sex.",science
10.1016/j.nucengdes.2020.110817,Journal,Nuclear Engineering and Design,scopus,2020-10-01,sciencedirect,Machine learning enabled advanced manufacturing in nuclear engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85089553568,"Advanced manufacturing has gained tremendous interest in both research and industry in the past few years. Over nearly the same period of time, machine learning (ML) has made phenomenal advancements, finding its way into many aspects of manufacturing. For the nuclear engineering field, the adoption of advanced manufacturing is a compelling argument due to the ambitious challenges the field faces. The combination of advanced manufacturing with ML holds great potential in the nuclear engineering field, and even further development is needed to accelerate their deployment towards real-world applications. This review paper seeks to detail several key aspects of ML enabled advanced manufacturing that are used or could prove useful to nuclear applications ranging from radiation detector materials to reactor parts fabrication. The applications covered here include new material extrapolation, manufacturing defect detection, and additive manufacturing parameters’ optimization.",science
10.1016/j.ohx.2020.e00131,Journal,HardwareX,scopus,2020-10-01,sciencedirect,Partially RepRapable automated open source bag valve mask-based ventilator,https://api.elsevier.com/content/abstract/scopus_id/85089470505,"This study describes the development of a simple and easy-to-build portable automated bag valve mask (BVM) compression system, which, during acute shortages and supply chain disruptions can serve as a temporary emergency ventilator. The resuscitation system is based on the Arduino controller with a real-time operating system installed on a largely RepRap 3-D printable parametric component-based structure. The cost of the materials for the system is under $170, which makes it affordable for replication by makers around the world. The device provides a controlled breathing mode with tidal volumes from 100 to 800 mL, breathing rates from 5 to 40 breaths/minute, and inspiratory-to-expiratory ratio from 1:1 to 1:4. The system is designed for reliability and scalability of measurement circuits through the use of the serial peripheral interface and has the ability to connect additional hardware due to the object-oriented algorithmic approach. Experimental results after testing on an artificial lung for peak inspiratory pressure (PIP), respiratory rate (RR), positive end-expiratory pressure (PEEP), tidal volume, proximal pressure, and lung pressure demonstrate repeatability and accuracy exceeding human capabilities in BVM-based manual ventilation. Future work is necessary to further develop and test the system to make it acceptable for deployment outside of emergencies such as with COVID-19 pandemic in clinical environments, however, the nature of the design is such that desired features are relatively easy to add using protocols and parametric design files provided.",science
10.1016/j.cbpa.2020.06.002,Journal,Current Opinion in Chemical Biology,scopus,2020-10-01,sciencedirect,Sequencing enabling design and learning in synthetic biology,https://api.elsevier.com/content/abstract/scopus_id/85089023322,"The ability to read and quantify nucleic acids such as DNA and RNA using sequencing technologies has revolutionized our understanding of life. With the emergence of synthetic biology, these tools are now being put to work in new ways — enabling de novo biological design. Here, we show how sequencing is supporting the creation of a new wave of biological parts and systems, as well as providing the vast data sets needed for the machine learning of design rules for predictive bioengineering. However, we believe this is only the tip of the iceberg and end by providing an outlook on recent advances that will likely broaden the role of sequencing in synthetic biology and its deployment in real-world environments.",science
10.1016/j.berh.2020.101559,Journal,Best Practice and Research: Clinical Rheumatology,scopus,2020-10-01,sciencedirect,Innovations to improve access to musculoskeletal care,https://api.elsevier.com/content/abstract/scopus_id/85088788188,"Innovation is a form of realising a new way of doing something, often ignoring traditional wisdom, in order to meet new challenges. Globally, particularly in emerging economies, the high burden of musculoskeletal conditions and their contribution to multimorbidity continue to rise, as does the gap for services to deliver essential care. There is a growing need to find solutions to this challenge and deliver person-centred and integrated care, wherein empowering patients with the capacity for self-management is critical. Whilst there is an abundance of information available online to support consumer education, the number of sources for credible medical information is diluted by uninformed anecdotal social media solutions. Even with the provision of high-quality information, behavioural change does not necessarily follow, and more robust educational approaches are required.
                  In this chapter, we examine innovation, its management and the strategic directions required to improve musculoskeletal healthcare at macro (policy), meso (service delivery) and micro (clinical practice) levels. We discuss the critical role of consumer agency (patients and their families/carers) in driving innovation and the need to leverage this through empowerment by education.
                  We provide a snapshot of real-world examples of innovative practices including capacity building in consumer and interprofessional musculoskeletal education and practice; recommendations to transform the access and delivery of integrated, person-centred care; and initiatives in musculoskeletal care and implementation of models of care, enabled by digital health solutions including telehealth, remote monitoring, artificial intelligence, blockchain technology and big data. We provide emerging evidence for how innovation can support systems' strengthening and build capacity to support improved access to ‘right’ musculoskeletal care, and explore some of the ways to best manage innovations.
                  We conclude with recommended systematic steps to establish required leadership, collaboration, research, networking, dissemination, implementation and evaluation of future innovations in musculoskeletal health and care.",science
10.1016/j.jobcr.2020.07.015,Journal,Journal of Oral Biology and Craniofacial Research,scopus,2020-10-01,sciencedirect,Present and future of artificial intelligence in dentistry,https://api.elsevier.com/content/abstract/scopus_id/85088647471,"The last decennary has marked as the breakthrough in the advancement of technology with evolution of artificial intelligence, which is rapidly gaining the attention of researchers across the globe. Every field opted artificial intelligence with huge enthusiasm and so the field of dental science is no exception. With huge increases in patient documented information and data this is the need of the hour to use intelligent software to compile and save this data. From the basic step of taking a patient's history to data processing and then to extract the information from the data for diagnosis, artificial intelligence has many applications in dental and medical science. While in no case artificial intelligence can replace the role of a dental surgeon but it is important to be acquainted with the scope to amalgamate this advancement of technology in future for betterment of dental practice.",science
10.1016/j.compeleceng.2020.106766,Journal,Computers and Electrical Engineering,scopus,2020-10-01,sciencedirect,Imputation of Missing Values Affecting the Software Performance of Component-based Robots,https://api.elsevier.com/content/abstract/scopus_id/85088022516,"Intelligent robots are foreseen as a technology that would be soon present in most public and private environments. In order to increase the trust of humans, robotic systems must be reliable while both response and down times are minimized. In keeping with this idea, present paper proposes the application of machine learning (regression models more precisely) to preprocess data in order to improve the detection of failures. Such failures deeply affect the performance of the software components embedded in human-interacting robots. To address one of the most common problems of real-life datasets (missing values), some traditional (such as linear regression) as well as innovative (decision tree and neural network) models are applied. The aim is to impute missing values with minimum error in order to improve the quality of data and consequently maximize the failure-detection rate. Experiments are run on a public and up-to-date dataset and the obtained results support the viability of the proposed models.",science
10.1016/j.mcp.2020.101619,Journal,Molecular and Cellular Probes,scopus,2020-10-01,sciencedirect,Multiplex real-time SYBR Green I PCR assays for simultaneous detection of 15 common enteric pathogens in stool samples,https://api.elsevier.com/content/abstract/scopus_id/85087216854,"Diarrheal diseases account for more than 50% of foodborne diseases worldwide, the majority of which occur in infants and young children. The traditional bacterial detection method is complex and time-consuming; therefore, it is necessary to establish a rapid and convenient detection method that can detect multiple pathogens simultaneously. In this study, we developed a set of five multiplex real-time SYBR Green I PCR assays to simultaneously detect 15 common enteric pathogens based on the Homo-Tag Assisted Non-Dimer system. These assays effectively reduced primer-dimer formation and improved the stability, uniformity, and amplification efficiency of multiplex PCR. The detection limit of the multiplex SYBR Green I PCR system was approximately 104–106 CFU/mL for stool specimens. Furthermore, we vitrified heat-unstable components on the cap of a reaction tube, showing that Taq DNA polymerase, dNTPs, primers, and SYBR Green I remained stable at 25 °C. In summary, we developed multiplex SYBR Green I PCR assays that can simultaneously detect 15 enteric pathogens. This method is comprehensive, rapid, inexpensive, accurate, and simple and displays high specificity.",science
10.1016/j.ast.2020.105965,Journal,Aerospace Science and Technology,scopus,2020-10-01,sciencedirect,Towards a PDE-based large-scale decentralized solution for path planning of UAVs in shared airspace,https://api.elsevier.com/content/abstract/scopus_id/85086828428,"Recently, there has been a tremendous increase of interest in utilizing Unmanned Aerial Vehicles (UAVs) for a number of civilian applications. With this increased interest, it is imperative that these UAVs are able to operate in shared airspace for enhanced efficiency. Multi-UAV systems are inherently safety-critical systems, which means that safety guarantees must be made to ensure no undesirable configurations, such as collisions, occur. This paper proposes a decentralized method based on a Partial Differential Equation (PDE) to generate collision-free 3D trajectories for multiple UAVs operating in a shared airspace. This method exploits the dynamical properties of multi-phase fluids flowing through a porous medium by modeling the porosity values as a function of the risk of collision. To highlight the feasibility for on-board implementation, we propose propose a machine learning technique for obtaining computationally efficient solutions of the PDE describing flow movements in porous medium. This method has been compared via a simulation study to two other path planning strategies, centralized and sequential planning, and the advantages of this method are presented. Furthermore, results from an experiment using three UAVs have been presented to demonstrate the applicability of the proposed method to real-world implementation.",science
10.1016/j.ins.2020.06.004,Journal,Information Sciences,scopus,2020-10-01,sciencedirect,Finding dense subgraphs with maximum weighted triangle density,https://api.elsevier.com/content/abstract/scopus_id/85086798490,"Finding dense subgraphs from sparse graphs is a fundamental graph mining task that has been applied in various domains, such as social networks, biology, and spam detection. Because the standard formulation of this problem is difficult to solve owing to connections with the Maximum Clique Problem, some tractable formulations have been proposed. These formulations find a dense subgraph by optimizing some density function, such as the degree density or triangle density. In this paper, we introduce the weighted k-clique density, a novel formulation for dense subgraph extraction. We show that the problem of maximizing weighted k-clique density can be solved optimally in polynomial time by solving a series of minimum cut problems. For scalability, we also propose a more efficient greedy algorithm with performance guarantee. The experimental results on real-world network datasets show that, compared with established state-of-the-art algorithms, the proposed algorithm can find a much denser subgraph in terms of edge density and triangle density.",science
10.1016/j.cmi.2020.02.006,Journal,Clinical Microbiology and Infection,scopus,2020-10-01,sciencedirect,Machine learning in the clinical microbiology laboratory: has the time come for routine practice?,https://api.elsevier.com/content/abstract/scopus_id/85081582683,"Background
                  Machine learning (ML) allows the analysis of complex and large data sets and has the potential to improve health care. The clinical microbiology laboratory, at the interface of clinical practice and diagnostics, is of special interest for the development of ML systems.
               
                  Aims
                  This narrative review aims to explore the current use of ML In clinical microbiology.
               
                  Sources
                  References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, arXiV, ACM Digital Library and IEEE Xplore Digital Library up to November 2019.
               
                  Content
                  We found 97 ML systems aiming to assist clinical microbiologists. Overall, 82 ML systems (85%) targeted bacterial infections, 11 (11%) parasitic infections, nine (9%) viral infections and three (3%) fungal infections. Forty ML systems (41%) focused on microorganism detection, identification and quantification, 36 (37%) evaluated antimicrobial susceptibility, and 21 (22%) targeted the diagnosis, disease classification and prediction of clinical outcomes. The ML systems used very diverse data sources: 21 (22%) used genomic data of microorganisms, 19 (20%) microbiota data obtained by metagenomic sequencing, 19 (20%) analysed microscopic images, 17 (18%) spectroscopy data, eight (8%) targeted gene sequencing, six (6%) volatile organic compounds, four (4%) photographs of bacterial colonies, four (4%) transcriptome data, three (3%) protein structure, and three (3%) clinical data. Most systems used data from high-income countries (n = 71, 73%) but a significant number used data from low- and middle-income countries (n = 36, 37%). Performance measures were reported for the 97 ML systems, but no article described their use in clinical practice or reported impact on processes or clinical outcomes.
               
                  Implications
                  In clinical microbiology, ML has been used with various data sources and diverse practical applications. The evaluation and implementation processes represent the main gap in existing ML systems, requiring a focus on their interpretability and potential integration into real-world settings.",science
10.1016/j.jaim.2018.02.140,Journal,Journal of Ayurveda and Integrative Medicine,scopus,2020-10-01,sciencedirect,Effect of seeds of Entada phaseoloides on chronic restrain stress in mice,https://api.elsevier.com/content/abstract/scopus_id/85059584549,"Background
                  
                     Entada phaseoloides is a well-known medicinal plant traditionally used in Ayurvedic medicine for centuries.
               
                  Objective
                  To evaluate the anti-stress activity of seeds of E. phaseoloides in endoplasmic reticulum stress during chronic restrain stress in mice, based on our preliminary screening.
               
                  Materials and Methods
                  Mice (n = 6/group) were restrained daily for 6 h in 50 ml polystyrene tubes for 28 days. Methanolic extract of E. phaseoloides (MEEP) (100 and 200 mg/kg, p.o.) and standard drug, imipramine (10 mg/kg i.p.) were administered daily 45 min prior to restrain from day 22–28. Then, forced swim test (FST) was performed to assess despair behavior. Lipid peroxidation (LPO) and antioxidant enzymes Reduced glutathione (GSH), Superoxide dismutase (SOD) were measured in the hippocampus of mice. 78 kDa Glucose-regulated Protein, 94 kDa Glucose-regulated Protein, C/EBP homologous protein, Caspase-12 expression were quantified by Real Time PCR.
               
                  Results
                  MEEP significantly reduced the immobility time in FST (P < 0.001). Significant reduction of LPO (P < 0.05) level and restored antioxidant enzymes viz. GSH (P < 0.001) and SOD towards vehicle control group were observed. Down-regulation of genes GRP 78, GRP 94 (P < 0.001), CHOP and Caspase-12 (P < 0.001) as compared to the chronic restrain stress group was evident, which were upregulated following treatment. Isolation of the active components of the seeds revealed the presence of Oleic acid (1), Entadamide A (2), Entadamide A-beta-d-glucopyranoside (3) and 1-O-protocatechuoyl-β-d-glucose.
               
                  Conclusion
                  MEEP altered endoplasmic reticulum stress in chronic restrain stressed mice; however, as an antidepressant it showed a weaker response.",science
10.1016/j.patter.2020.100093,Journal,Patterns,scopus,2020-09-11,sciencedirect,Uncovering Effective Explanations for Interactive Genomic Data Analysis,https://api.elsevier.com/content/abstract/scopus_id/85102976970,"Better tools are needed to enable researchers to quickly identify and explore effective and interpretable feature-based explanations for discriminating multi-class genomic datasets, e.g., healthy versus diseased samples. We develop an interactive exploration tool, GENVISAGE, which rapidly discovers the most discriminative feature pairs that separate two classes of genomic objects and then displays the corresponding visualizations. Since quickly finding top feature pairs is computationally challenging, especially for large numbers of objects and features, we propose a suite of optimizations to make GENVISAGE responsive at scale and demonstrate that our optimizations lead to a 400× speedup over competitive baselines for multiple biological datasets. We apply our rapid and interpretable tool to identify literature-supported pairs of genes whose transcriptomic responses significantly discriminate several chemotherapy drug treatments. With its generalizable optimizations and framework, GENVISAGE opens up real-time feature-based explanation generation to data from massive sequencing efforts, as well as many other scientific domains.",science
10.1016/j.patter.2020.100083,Journal,Patterns,scopus,2020-09-11,sciencedirect,"The Veterans Affairs Precision Oncology Data Repository, a Clinical, Genomic, and Imaging Research Database",https://api.elsevier.com/content/abstract/scopus_id/85102968026,"The Veterans Affairs Precision Oncology Data Repository (VA-PODR) is a large, nationwide repository of de-identified data on patients diagnosed with cancer at the Department of Veterans Affairs (VA). Data include longitudinal clinical data from the VA's nationwide electronic health record system and the VA Central Cancer Registry, targeted tumor sequencing data, and medical imaging data including computed tomography (CT) scans and pathology slides. A subset of the repository is available at the Genomic Data Commons (GDC) and The Cancer Imaging Archive (TCIA), and the full repository is available through the Veterans Precision Oncology Data Commons (VPODC). By releasing this de-identified dataset, we aim to advance Veterans' health care through enabling translational research on the Veteran population by a wide variety of researchers.",science
10.1016/j.patter.2020.100057,Journal,Patterns,scopus,2020-09-11,sciencedirect,Explaining the Genetic Causality for Complex Phenotype via Deep Association Kernel Learning,https://api.elsevier.com/content/abstract/scopus_id/85102966019,"The genetic effect explains the causality from genetic mutations to the development of complex diseases. Existing genome-wide association study (GWAS) approaches are always built under a linear assumption, restricting their generalization in dissecting complicated causality such as the recessive genetic effect. Therefore, a sophisticated and general GWAS model that can work with different types of genetic effects is highly desired. Here, we introduce a deep association kernel learning (DAK) model to enable automatic causal genotype encoding for GWAS at pathway level. DAK can detect both common and rare variants with complicated genetic effects where existing approaches fail. When applied to four real-world GWAS datasets including cancers and schizophrenia, our DAK discovered potential casual pathways, including the association between dilated cardiomyopathy pathway and schizophrenia.",science
10.1016/j.mineng.2020.106457,Journal,Minerals Engineering,scopus,2020-09-01,sciencedirect,Effects of process history on the surface morphology of uranium ore concentrates extracted from ore,https://api.elsevier.com/content/abstract/scopus_id/85085988966,"The effects of process history on the particle morphology of uranium ore concentrates (UOC) synthesized from natural uranium ores were investigated. A purified UOC was extracted from a uranium-rich ore by sulfuric acid leaching and purified by two commercial solvent extraction processes: the Dapex process using di-(2-ethylhexyl) phosphoric acid (DEHPA), and the Amex process using Alamine® 336. The UOC from either route was precipitated from its respective strip solution by ammonia and calcined. Powder X-ray diffraction (P-XRD) was used to confirm the crystallography of the UOC; while Rietveld refinement was performed for quantification of U oxide phase composition. Inductively coupled plasma - mass spectrometry (ICP-MS) was used to quantify impurity concentrations of the calcined material. As expected, different impurities were present due to processing via the Dapex and Amex process. To further correlate the impurities to the process history, particle morphology was quantified from scanning electron microscopy (SEM) micrographs using the MAMA segmentation software. In addition, a deep neural network was trained on the SEM images to distinguish between process histories. The results provide a real-world representation of how particle morphology will likely change based on processing at commercial facilities as opposed to high purity material experiments in a laboratory setting.",science
10.1016/j.ijmedinf.2020.104176,Journal,International Journal of Medical Informatics,scopus,2020-09-01,sciencedirect,The development an artificial intelligence algorithm for early sepsis diagnosis in the intensive care unit,https://api.elsevier.com/content/abstract/scopus_id/85085579350,"Background
                  Severe sepsis and septic shock are still the leading causes of death in Intensive Care Units (ICUs), and timely diagnosis is crucial for treatment outcomes. The progression of electronic medical records (EMR) offers the possibility of storing a large quantity of clinical data that can facilitate the development of artificial intelligence (AI) in medicine. However, several difficulties, such as poor structure and heterogenicity of the raw EMR data, are encountered when introducing AI with ICU data. Labor-intensive work, including manual data entry, personal medical records sorting, and laboratory results interpretation may hinder the progress of AI. In this article, we introduce the developing of an AI algorithm designed for sepsis diagnosis using pre-selected features; and compare the performance of the AI algorithm with SOFA score based diagnostic method.
               
                  Materials and methods
                  This is a prospective open-label cohort study. A specialized EMR, named TED_ICU, was implemented for continuous data recording. One hundred six clinical features relevant to sepsis diagnosis were selected prospectively. A labeling work to allocate SEPSIS or NON_SEPSIS status for each ICU patient was performed by the in-charge intensivist according to SEPSIS-3 criteria, along with the automatic recording of selected features every day by TED_ICU. Afterward, we use de-identified data to develop the AI algorithm. Several machine learning methods were evaluated using 5-fold cross-validation, and XGBoost, a decision-tree based algorithm was adopted for our AI algorithm development due to best performance.
               
                  Results
                  The study was conducted between August 2018 and December 2018 for the first stage of analysis. We collected 1588 instances, including 444 SEPSIS and 1144 NON-SEPSIS, from 434 patients. The 434 patients included 259 (59.6%) male patients and 175 female patients. The mean age was 67.6-year-old, and the mean APACHE II score was 13.8. The SEPSIS cohort had a higher SOFA score and increased use of organ support treatment. The AI algorithm was developed with a shuffle method using 80% of the instances for training and 20% for testing. The established AI algorithm achieved the following: accuracy = 82% ± 1%; sensitivity = 65% ± 5%; specificity = 88% ± 2%; precision = 67% ± 3%; and F1 = 0.66 ± 0.02. The area under the receiver operating characteristic curve (AUROC) was approximately 0.89. The SOFA score was used on the same 1588 instances for sepsis diagnosis, and the result was inferior to our AI algorithm (AUROC = 0.596).
               
                  Conclusion
                  Using real-time data, collected by EMR, from the ICU daily practice, our AI algorithm established with pre-selected features and XGBoost can provide a timely diagnosis of sepsis with an accuracy greater than 80%. AI algorithm also outperforms the SOFA score in sepsis diagnosis and exhibits practicality as clinicians can deploy appropriate treatment earlier. The early and precise response of this AI algorithm will result in cost reduction, outcome improvement, and benefit for healthcare systems, medical staff, and patients as well.",science
10.1016/j.compind.2020.103226,Journal,Computers in Industry,scopus,2020-09-01,sciencedirect,Perspective on holonic manufacturing systems: PROSA becomes ARTI,https://api.elsevier.com/content/abstract/scopus_id/85085261123,"Looking back at 30 years of research into holonic manufacturing systems, these explorations made a lasting scientific contribution to the overall architecture of intelligent manufacturing systems. Most notably, holonic architectures are defined in terms of their world-of-interest (Van Brussel et al., 1998). They do not have an information layer, a communication layer, etc. Instead, they have components that relate to real-world assets (e.g. machine tools) and activities (e.g. assembly). And, they mirror and track the structure of their world-of-interest, which allows them to scale and adapt accordingly.
                  This research has wandered around, at times learning from its mistakes, and progressively carved out an invariant structure while it translated and applied scientific insights from complex-adaptive systems theory (e.g. autocatalytic sets) and from bounded rationality (e.g. holons). This paper presents and discusses the outcome of these research efforts.
                  At the top level, the holonic structure distinguishes intelligent beings (or digital twins) from intelligent agents. These digital twins inherit the consistency from reality, which they mirror. They are intelligent beings when they reflect what exists in the world without imposing artificial limitations in this reality. Consequently, a conflict with a digital twin is a conflict with reality.
                  In contrast, intelligent agents typically transform NP-hard challenges into computations with low-polynomial complexity. Unavoidably, this involves arbitrariness (e.g. don’t care choices). Likewise, relying on case-specific properties, to ensure an outcome in polynomial time, usually renders the validity of an agent’s choices both short-lived and situation-dependent. Here, intelligent agents create conflicts by imposing limitations of their own making in their world-of-interest.
                  Real-world smart systems are aggregates comprising both intelligent beings and intelligent agents. They are performers. Inside these performers, digital twins may constitute the foundations, supporting walls, support beams and pillars because these intelligent beings are protected by their real-world counterpart. Further refining the top-level of this architecture, a holonic structure enables these digital twins to shadow their real-world counterpart whenever it changes, adapts and evolves.
                  In contrast, the artificial limitations, imposed by the intelligent agents, cannot be allowed to build up inertia, which would hamper the undoing of arbitrary or case-specific limitations. To this end, performers explicitly manage the rights over their assets. Revoking such rights from a limitation-imposing agent will free the assets. This will be at the cost of reduced services from the agent. When other service providers rely on this agent, their services may be affected as well; that’s how the inertia builds up and how harmful legacy is created. Thus, the services of digital twins are to be preferred over the services of an intelligent agent by developers of holonic manufacturing systems.
                  Finally, digital twins corresponding to the decision making in the world-of-interest (a non-physical asset) allow to mirror the world-of-interest in a predictive mode (in addition to track and trace). It allows to generate short-term forecasts while preserving the benefits of intelligent beings. These twins are the intentions of the decision-making intelligent agents. Evidently, when intentions change, the forecasts needs to be regenerated (i.e. tracking the corresponding reality by the twin). This advanced feature can be deployed in a number of configurations (cf. annex).",science
10.1016/j.future.2020.04.018,Journal,Future Generation Computer Systems,scopus,2020-09-01,sciencedirect,Software-Defined Network for End-to-end Networked Science at the Exascale,https://api.elsevier.com/content/abstract/scopus_id/85083299223,"Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence.",science
10.1016/j.cpc.2020.107275,Journal,Computer Physics Communications,scopus,2020-09-01,sciencedirect,freud: A software suite for high throughput analysis of particle simulation data,https://api.elsevier.com/content/abstract/scopus_id/85082555873,"The freud Python package is a library for analyzing simulation data. Written with modern simulation and data analysis workflows in mind, freud provides a Python interface to fast, parallelized C++ routines that run efficiently on laptops, workstations, and supercomputing clusters. The package provides the core tools for finding particle neighbors in periodic systems, and offers a uniform API to a wide variety of methods implemented using these tools. As such, freud users can access standard methods such as the radial distribution function as well as newer, more specialized methods such as the potential of mean force and torque and local crystal environment analysis with equal ease. Rather than providing its own trajectory data structure, freud operates either directly on NumPy arrays or on trajectory data structures provided by other Python packages. This design allows freud to transparently interface with many trajectory file formats by leveraging the file parsing abilities of other trajectory management tools. By remaining agnostic to its data source, freud is suitable for analyzing any particle simulation, regardless of the original data representation or simulation method. When used for on-the-fly analysis in conjunction with scriptable simulation software such as HOOMD-blue, freud enables smart simulations that adapt to the current state of the system, allowing users to study phenomena such as nucleation and growth.
               
                  Program summary
                  
                     Program Title: 
                     freud
                  
                  
                     Program Files doi: 
                     http://dx.doi.org/10.17632/v7wmv9xcct.1
                  
                  
                     Licensing provisions: BSD 3-Clause
                  
                     Programming language: Python, C++
                  
                     Nature of problem: Simulations of coarse-grained, nano-scale, and colloidal particle systems typically require analyses specialized to a particular system. Certain more standardized techniques – including correlation functions, order parameters, and clustering – are computationally intensive tasks that must be carefully implemented to scale to the larger systems common in modern simulations.
                  
                     Solution method: 
                     freud performs a wide variety of particle system analyses, offering a Python API that interfaces with many other tools in computational molecular sciences via NumPy array inputs and outputs. The algorithms in freud leverage parallelized C++ to scale to large systems and enable real-time analysis. The library’s broad set of features encode few assumptions compared to other analysis packages, enabling analysis of a broader class of data ranging from biomolecular simulations to colloidal experiments.
                  
                     Additional comments including restrictions and unusual features:
                  
                  1. freud provides very fast parallel implementations of standard analysis methods like RDFs and correlation functions.
                  2. freud includes the reference implementation for the potential of mean force and torque (PMFT).
                  3. freud provides various novel methods for characterizing particle environments, including the calculation of descriptors useful for machine learning. The source code is hosted on GitHub (https://github.com/glotzerlab/freud), and documentation is available online (https://freud.readthedocs.io/). The package may be installed via pip install freud-analysis or conda install -c conda-forge freud.",science
10.1016/j.scitotenv.2020.139158,Journal,Science of the Total Environment,scopus,2020-08-20,sciencedirect,Development of a decision support system for the selection of wastewater treatment technologies,https://api.elsevier.com/content/abstract/scopus_id/85084373810,"Multiple factors including technical, social, economic, regulatory, governmental, and environmental add complexity in the process of selecting a suitable wastewater treatment technology. To overcome this issue, this paper aims to propose a decision support system (DSS) for the selection of wastewater treatment technologies. The proposed system has been developed using a detailed review of the state-of-the-art in wastewater treatment, implemented using Microsoft Visual Studio 2010 and validated through real-time case studies. The system is categorized into four treatment levels based on wastewater complexity and the required degree of treatment. These include preliminary, primary, secondary, and tertiary treatment. Based on the identified treatment levels, the proposed system suggests using any physical, biological, chemical, or hybrid treatment process. The developed DSS will aid the selection of suitable wastewater treatment technology from a set of alternatives while keeping user constraints, conflicting requirements, and prevailing conditions under consideration. Moreover, the system is capable to customize the treatment assembly at the planning stage with minimized costs, eliminate mistakes at the planning and design stage, facilitate decision making by narrowing down the alternative solution as per user requirements and prevailing conditions, incorporate customer demand, and promote sustainable development.",science
10.1016/j.patter.2020.100073,Journal,Patterns,scopus,2020-08-14,sciencedirect,dtoolAI: Reproducibility for Deep Learning,https://api.elsevier.com/content/abstract/scopus_id/85102966352,"Deep learning, a set of approaches using artificial neural networks, has generated rapid recent advancements in machine learning. Deep learning does, however, have the potential to reduce the reproducibility of scientific results. Model outputs are critically dependent on the data and processing approach used to initially generate the model, but this provenance information is usually lost during model training. To avoid a future reproducibility crisis, we need to improve our deep-learning model management. The FAIR principles for data stewardship and software/workflow implementation give excellent high-level guidance on ensuring effective reuse of data and software. We suggest some specific guidelines for the generation and use of deep-learning models in science and explain how these relate to the FAIR principles. We then present dtoolAI, a Python package that we have developed to implement these guidelines. The package implements automatic capture of provenance information during model training and simplifies model distribution.",science
10.1016/j.patter.2020.100074,Journal,Patterns,scopus,2020-08-14,sciencedirect,Machine-Learning Approaches in COVID-19 Survival Analysis and Discharge-Time Likelihood Prediction Using Clinical Data,https://api.elsevier.com/content/abstract/scopus_id/85092796371,"As a highly contagious respiratory disease, COVID-19 has yielded high mortality rates since its emergence in December 2019. As the number of COVID-19 cases soars in epicenters, health officials are warning about the possibility of the designated treatment centers being overwhelmed by coronavirus patients. In this study, several computational techniques are implemented to analyze the survival characteristics of 1,182 patients. The computational results agree with the outcome reported in early clinical reports released for a group of patients from China that confirmed a higher mortality rate in men compared with women and in older age groups. The discharge-time prediction of COVID-19 patients was also evaluated using different machine-learning and statistical analysis methods. The results indicate that the Gradient Boosting survival model outperforms other models for patient survival prediction in this study. This research study is aimed to help health officials make more educated decisions during the outbreak.",science
10.1016/j.jclepro.2020.121426,Journal,Journal of Cleaner Production,scopus,2020-08-10,sciencedirect,A sentiment reporting framework for major city events: Case study on the China-United States trade war,https://api.elsevier.com/content/abstract/scopus_id/85084032594,"Smart cities are conceptualized as a vehicle for sustainable urban development and a means to deliver high quality of life for residents. One of the core functions of a smart city consists in the continuous monitoring of events, assets and people and the use of this information and intelligence for the streamlining of the city’s operations. Public opinion represents one type of intelligence of particular importance and value. By monitoring public opinion, governments seek to understand prevalent views about the current events and policies, as well as identify extreme views and trends that may represent problematic situations or precursors to violent actions. Ultimately, maintaining a constant awareness of public opinion means that authorities can better assess and predict public reactions in relation to ongoing events, and thus take appropriate actions to maintain public safety. Due to the popular use of social media to express sentiments and emotions about current events, social media content analysis has been contemplated as a promising solution to capture public opinion. However, existing approaches take a coarse-grained retrospective approach to social media content analysis. Furthermore, those approaches suffer from the lack of scalability and efficiency, as they necessitate the collection and analysis of large volumes of social media content (often millions of posts), to come up with relevant conclusions. In this work, we address those limitations by proposing a novel framework for the real-time monitoring of public opinion. To ensure efficiency and scalability, our framework focuses on the analysis of high impact social media content generated by opinion leaders and their followers as means to offer in-depth insights and sentiment intelligence reports about events, as they are occurring in real time. The proposed framework was implemented and tested on data harvested from 52 economic opinion leaders, with a focus on the China-US trade war as case study. The results show that the convolutional neural network (CNN) classifier used for sentiment analysis yielded a classification accuracy of 86% when differentiating between four sentiment categories: Support, strong support, dissent, and strong dissent. The Support Vector Machine (SVM) classifier employed to perform in-depth emotional analysis attained an accuracy of 82% when differentiating between five emotions: Angry, depressed, excited, happy, and worried. Unlike existing retrospective social media analysis approaches that require the analysis of millions of posts, our approach focuses on the analysis of high-impact social media content in real-time, thus constituting an efficient, sustainable, and timely solution to public opinion monitoring.",science
10.1016/S2352-3018(20)30190-9,Journal,The Lancet HIV,scopus,2020-08-01,sciencedirect,Modern diagnostic technologies for HIV,https://api.elsevier.com/content/abstract/scopus_id/85088944280,"Novel diagnostic technologies, including nanotechnology, microfluidics, -omics science, next-generation sequencing, genomics big data, and machine learning, could contribute to meeting the UNAIDS 95-95-95 targets to end the HIV epidemic by 2030. Novel technologies include multiplexed technologies (including biomarker-based point-of-care tests and molecular platform technologies), biomarker-based combination antibody and antigen technologies, dried-blood-spot testing, and self-testing. Although biomarker-based rapid tests, in particular antibody-based tests, have dominated HIV diagnostics since the development of the first HIV test in the mid-1980s, targets such as nucleic acids and genes are now used in nanomedicine, biosensors, microfluidics, and -omics to enable early diagnosis of HIV. These novel technologies show promise as they are associated with ease of use, high diagnostic accuracy, rapid detection, and the ability to detect HIV-specific markers. Additional clinical and implementation research is needed to generate evidence for use of novel technologies and a public health approach will be required to address clinical and operational challenges to optimise their global deployment.",science
10.1016/j.compag.2020.105526,Journal,Computers and Electronics in Agriculture,scopus,2020-08-01,sciencedirect,Application of random forest classification to predict daily oviposition events in broiler breeders fed by precision feeding system,https://api.elsevier.com/content/abstract/scopus_id/85085736722,"In group-housed poultry, hormone and environment modulated variability in the processes of follicle maturation and egg formation make it difficult to predict a daily egg-laying event (oviposition). Recording daily egg laying events has required individual cages or expensive technology such as RFID equipped nests or labor intensive trap nests. The current study implemented the random forest classification algorithm to predict oviposition events of 202 free run Ross 708 broiler breeder hens fed by a precision feeding system from week 21 to 55, based on a dataset recording information of all visits to the station. The raw dataset from the precision feeding system was processed for 6 classes of features (34 features in total) in relation to feeding activity and real-time body weight of birds. The dataset of the features was then combined with a corresponding daily individual oviposition record. The processed data were shuffled and separated into 2 subsets: 90% for training, and 10% for testing. Important features were selected using random forest-recursive feature elimination with 5-fold cross-validation, and 28 features were selected to build a random forest classification model. Overall accuracy of the model using the testing samples was 0.8482, and out-of-bag score was 0.8510. Precision (a measure of purity in retrieving) of no egg-laying and egg-laying, recall (a measure of completeness in retrieving) of no egg-laying and egg-laying were 0.8814, 0.8090, 0.8520 and 0.8453, respectively. The Kappa coefficient of the model was 0.6931, indicating substantial agreement (substantial agreement range: 0.61–0.80). This model was able to identify whether a free run broiler breeder laid an egg or not on a certain day during the laying period with around 85% accuracy.",science
10.1016/j.scitotenv.2020.138895,Journal,Science of the Total Environment,scopus,2020-08-01,sciencedirect,A serious gaming framework for decision support on hydrological hazards,https://api.elsevier.com/content/abstract/scopus_id/85083823562,"With increasing population and human intervention on the natural environment, hazards are a growing threat, coming in many forms, including floods, droughts, soil erosion, and water pollution. A key approach to mitigate hydrological disaster risk at the community level is informed planning with decision support systems. The literature shows emerging efforts on multi-hazard decision support systems for hydrological disasters and demonstrates the need for an engaging, accessible, and collaborative serious game environment facilitating the relationship between the environment and communities. In this study, a web-based decision support tool (DST) was developed for hydrological multi-hazard analysis while employing gamification techniques to introduce a competitive element. The serious gaming environment provides functionalities for intuitive management, visualization, and analysis of geospatial, hydrological, and economic data to help stakeholders in the decision-making process regarding hydrological hazard preparedness and response. Major contributions of the presented DST include involving the community in environmental decision making by reducing the technical complexity required for analysis, increasing community awareness for the environmental and socio-economic consequences of hydrological hazards, and allowing stakeholders to discover and discuss potential trade-offs to hazardous scenarios considering the limitations in budget, regulations, and technicality. The paper describes the software design approaches and system architecture applied for a modular, secure, and scalable software as well as the framework's intuitive web-based user interfaces for real-time and collaborative data analysis and damage assessment. Finally, a case study was conducted to demonstrate the usability of DST in a formal setting and to measure user satisfaction with surveys.",science
10.1016/j.ins.2020.04.023,Journal,Information Sciences,scopus,2020-08-01,sciencedirect,Recurrent neural variational model for follower-based influence maximization,https://api.elsevier.com/content/abstract/scopus_id/85083696721,"Influence Maximization, aiming at selecting a small set of seed users in a social network to maximize the spread of influence, has attracted considerable attention recently. Most of the existing influence maximization algorithms focus on the diffusion model of one single-entity, which assumes that only one entity is propagated by users in social network. However, the diffusion situations in real world social networks often involve multiple entities, competitive or complementary, spreading through the whole network, and are more complex than the situations of single independent entity.
                  In this paper, we propose a novel optimization problem, namely, the follower-based influence maximization, which aims to promote a new product into the market by maximizing the influence of a social network where other competitive and complementary products have already been propagating. We tackle this problem by proposing a Recurrent Neural Variational model (RNV) and a follower-based greedy algorithm (RNVGA). The RNV model dynamically tracks entity correlations and cascade correlations through a deep generative model and recurrent neural variational inference, while the RNVGA algorithm applies the greedy approach for submodular maximization and efficiently computes the seed node set for the target product. Extensive experiments have been conducted to evaluate effectiveness and efficiency of our method, and the results show the superiority of our method compared with the state-of-the-art methods.",science
10.1016/j.ins.2020.03.030,Journal,Information Sciences,scopus,2020-08-01,sciencedirect,Efficient approach of recent high utility stream pattern mining with indexed list structure and pruning strategy considering arrival times of transactions,https://api.elsevier.com/content/abstract/scopus_id/85083463056,"One of various pattern mining techniques, the High Utility Pattern Mining (HUPM) is a method for finding meaningful patterns from non-binary databases by considering the characteristics of the items. Recently, new data continues to flow over time in diverse fields such as sales data of market, heartbeat sensor data, and social network service. Since these data have a feature that recently generated data have higher influence than the old data, research has been focused on how to efficiently extract hidden knowledge from time-sensitive databases. In this paper, we propose indexed list-based algorithm that mines recent high utility pattern considering the arrival time of inserted data in an environment where new data is continuously accumulated. In other words, to treat the importance of recent data higher than the that of old data, our algorithms reduces the utility values of old transactions according to the time the data is inserted by applying damped window model concept. Moreover, we carry out various experiments to compare our method with state-of-the-art algorithms using real and synthetic datasets in diverse circumstances. Experimental results show that our algorithm outperforms competitors in terms of execution time, memory usage, and scalability test.",science
10.1016/j.neucom.2018.09.104,Journal,Neurocomputing,scopus,2020-07-05,sciencedirect,A data-efficient deep learning approach for deployable multimodal social robots,https://api.elsevier.com/content/abstract/scopus_id/85065221778,"The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games—and use the game of ‘Noughts and Crosses’ with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the Pepper robot confirms that highly accurate visual perception is required for successful game play.",science
10.1016/j.comcom.2020.05.042,Journal,Computer Communications,scopus,2020-07-01,sciencedirect,Fastest adaptive estimation algorithms for topological structure errors in smart grid networks,https://api.elsevier.com/content/abstract/scopus_id/85086375513,"Compared with traditional wired networks, wireless sensor networks(WSN) have the characteristics of low cost and rapid deployment, and also guarantee the same level of fault tolerance as wired networks. The WSN can also monitor the operating status of the power grid in real time, collect physical information such as related parameters, and provide more comprehensive and complete power grid operation data as a reference basis for smart grid operation and related management personnel, and complete the diagnosis, monitoring and power statistics of smart grid equipment The rapid construction of the data communication network has become a key technology to effectively solve the problems of difficult optimization management and high cost and economic benefits in the smart grid. This paper discusses the application of WSNs in smart grids from two aspects. Firstly, construct a WSN topology that complies with the smart grid architecture, and establish a real-time routing mechanism that meets the requirements of smart distribution network communication reliability; secondly, propose a fastest adaptive algorithm for the fault of the WSN topology in the smart grid . The proposed adaptive routing mechanism has certain advantages in node energy consumption, which reduces energy consumption by nearly 4% compared to the directional diffusion method and the LEACH algorithm. Therefore, the algorithm is more suitable for the adaptation of WSN topology, and the method can Improve the life cycle of sensor nodes and networks.",science
10.1016/j.jobe.2020.101222,Journal,Journal of Building Engineering,scopus,2020-07-01,sciencedirect,Economy-energy trade off automation – A decision support system for building design development,https://api.elsevier.com/content/abstract/scopus_id/85081177322,"Building energy simulation is traditionally applied at late phases of the design, when most decisions on specifications of building systems are made. There has been a growing attempt in the literature to enable such simulations at earlier phases of design, when decisions’ impact on the lifecycle behavior is the strongest. While researchers have developed tools/models to support planning and conceptualization phases from this aspect; there have been not much developments for early design development stage (which is usually the scope of semi-detailed cost estimates). This paper introduces a software tool for generating multiple design scenarios and evaluating them, by combining energy simulation and economic analyses, at schematic design and early design development phases of building projects. In this regard, we used state-of-the art of open source technologies, as well as cloud computing, to integrate energy simulation into the alternative selection procedure. The developed system focuses on architectural parameters; lighting power density; and heating, ventilation and air conditioning (HVAC). Analysis of sensitivity of lifecycle energy to such parameters, in a group of representative real-world projects, helped us to limit variations of such parameters, and subsequently their combinations. Using OpenStudio measures and the computational power offered by the cloud, our system generates/simulates all possible design scenarios (combinations of alternative variations for design parameters); and compares them based on building economics performance measures. The workflow is showcased in a case study project, by automatically creating and evaluating 97 design scenarios, based on economic efficiency and liquidity.",science
10.1016/j.ipm.2020.102231,Journal,Information Processing and Management,scopus,2020-07-01,sciencedirect,Discovering web services in social web service repositories using deep variational autoencoders,https://api.elsevier.com/content/abstract/scopus_id/85081050739,"Web Service registries have progressively evolved to social networks-like software repositories. Users cooperate to produce an ever-growing, rich source of Web APIs upon which new value-added Web applications can be built. Such users often interact in order to follow, comment on, consume and compose services published by other users. In this context, Web Service discovery is a core functionality of modern registries as needed Web Services must be discovered before being consumed or composed. Many efforts to provide effective keyword-based service discovery mechanisms are based on Information Retrieval techniques as services are described using structured or unstructured textdocuments that specify the provided functionality. However, traditional techniques suffer from term-mismatch, which means that only the terms that are contained in both user queries and descriptions are exploited to perform service retrieval. Early feature learning techniques such as LSA or LDA tried to solve this problem by finding hidden or latent features in text documents. Recently, alternative feature learning based techniques such as Word Embeddings achieved state of the art results for Web Service discovery. In this paper, we propose to learn features from service descriptions by using Variational Autoencoders, a special kind of autoencoder which restricts the encoded representation to model latent variables. Autoencoders in turn are deep neural networks used for unsupervised learning of efficient codings. We train our autoencoder using a real 17 113-service dataset extracted from the ProgrammableWeb.com API social repository. We measure discovery efficacy by using both Recall and Precision metrics, achieving significant gains compared to both Word Embeddings and classic latent features modelling techniques. Also, performance-oriented experiments show that the proposed approach can be readily exploited in practice.",science
10.1016/j.petrol.2019.106741,Journal,Journal of Petroleum Science and Engineering,scopus,2020-07-01,sciencedirect,Connectionist and mutual information tools to determine water saturation and rank input log variables,https://api.elsevier.com/content/abstract/scopus_id/85081011589,"Characterization of petroleum reservoirs plays an important role to effectively manage and forecast the recovery performance. A number of subset log variables such as gamma-ray, resistivity, density, neutron, and sonic porosity logs are generally used to characterize/predict the reservoir properties. The data attributes selection and ranking in reservoir characterization are vital to determine the output variables with the best performance and cost-effective manner during exploration and production operations. The objectives of this research work are to estimate the water saturation in the reservoir with an acceptable accuracy and to rank the log variables according to their importance. To achieve the objectives, the mutual information (MI) and artificial neural network (ANN) techniques are implemented with the non-linear predictors using log variables. The feed-forward ANN model is employed and optimized to predict the water saturation, where the Levenberg-Marquardt algorithm is used for the network training. There is a good match between the real data and predictions so that the regression coefficient and the maximum error is 99.98% and 5.55%, respectively. In addition, both ANN and MI approaches lead to the same ranking levels of log variables, implying high accuracy and reliability of the introduced strategies. It is found that the primary (or most important) log variables are the true resistivity and bulk density to obtain the pore fluid saturation. The approach suggested in this study (connectionist and MI strategies) can assist engineers/operators to run a few numbers of logging tools for prediction of water saturation, resulting in saving the exploration costs through an efficient manner. In addition, further understanding is attained to conduct proper data selection for determination of reservoir petrophysical properties.",science
10.1016/j.patter.2020.100042,Journal,Patterns,scopus,2020-06-12,sciencedirect,Deep Learning Identifies Digital Biomarkers for Self-Reported Parkinson's Disease,https://api.elsevier.com/content/abstract/scopus_id/85095745471,"Large-scale population screening and in-home monitoring for patients with Parkinson's disease (PD) has so far been mainly carried out by traditional healthcare methods and systems. Development of mobile health may provide an independent, future method to detect PD. Current PD detection algorithms will benefit from better generalizability with data collected in real-world situations. In this paper, we report the top-performing smartphone-based method in the recent DREAM Parkinson's Disease Digital Biomarker Challenge for digital diagnosis of PD. Utilizing real-world accelerometer records, this approach differentiated PD from control subjects with an area under the receiver-operating characteristic curve of 0.87 by 3D augmentation of accelerometer records, a significant improvement over other state-of-the-art methods. This study paves the way for future at-home screening of PD and other neurodegenerative conditions affecting movement.",science
10.1016/j.heliyon.2020.e03966,Journal,Heliyon,scopus,2020-06-01,sciencedirect,Regression analysis for thermal properties of Al<inf>2</inf>O<inf>3</inf>/H<inf>2</inf>O nanofluid using machine learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85086086334,"Nanofluids possess higher thermal properties than the other conventional base fluids. Many investigators suggested that the nanofluids have the potential to apply in various engineering fields. In real time situation it is challenging to determine the thermal conductivity of nanofluids with accuracy as they have many depending factors. Moreover, numerous experimental tests are required to acquire the thermal conductivity of nanofluids accurately. In this research paper, thermal conductivity ratio and dynamic viscosity ratio of Al2O3/H2O nanofluid are predicted accurately by using Gaussian Process Regression (GPR) methods. The input predictor variables used in this model are temperature, volume fraction and size of the nanoparticles. 222 experimental data sets are taken to predict the thermal conductivity ratio (TCR), dynamic viscosity ratio (DVR) and also the effectiveness of the predictor variables in predicting the response variables are extensively studied and found that the temperature is the crucial factor to enhance the thermal conductivity ratio. The proposed modeling is performed by using MATLAB software. The predictions were evaluated by various evaluation criterions. It is observed that an optimized Gaussian process regression (GPR) method with matern kernel function shows an accurate agreement with experimental data with Root Mean Square Error (RMSE) value of 0.000126 for TCR and squared exponential kernel function show good agreement with experimental data with Root Mean Square Error (RMSE) value of 0.000045 for DVR. Regression coefficient value (R2) is 0.99; nearer to one hence the predicted results are reliable.",science
10.1016/j.cola.2020.100970,Journal,Journal of Computer Languages,scopus,2020-06-01,sciencedirect,"Visual Programming Environments for End-User Development of intelligent and social robots, a systematic review",https://api.elsevier.com/content/abstract/scopus_id/85085272330,"Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working “in-the-wild” rather than only in laboratories and structured settings.",science
10.1016/j.compbiomed.2020.103792,Journal,Computers in Biology and Medicine,scopus,2020-06-01,sciencedirect,Automated detection of COVID-19 cases using deep neural networks with X-ray images,https://api.elsevier.com/content/abstract/scopus_id/85083900518,"The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.",science
10.1016/j.engappai.2020.103670,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-06-01,sciencedirect,"Detecting, locating and recognising human touches in social robots with contact microphones",https://api.elsevier.com/content/abstract/scopus_id/85083676413,"There are many situations in our daily life where touch gestures during natural human–human interaction take place: meeting people (shaking hands), personal relationships (caresses), moments of celebration or sadness (hugs), etc. Considering that robots are expected to form part of our daily life in the future, they should be endowed with the capacity of recognising these touch gestures and the part of its body that has been touched since the gesture’s meaning may differ. Therefore, this work presents a learning system for both purposes: detect and recognise the type of touch gesture (stroke, tickle, tap and slap) and its localisation. The interpretation of the meaning of the gesture is out of the scope of this paper.
                  Different technologies have been applied to perceive touch by a social robot, commonly using a large number of sensors. Instead, our approach uses 3 contact microphones installed inside some parts of the robot. The audio signals generated when the user touches the robot are sensed by the contact microphones and processed using Machine Learning techniques. We acquired information from sensors installed in two social robots, Maggie and Mini (both developed by the RoboticsLab at the Carlos III University of Madrid), and a real-time version of the whole system has been deployed in the robot Mini. The system allows the robot to sense if it has been touched or not, to recognise the kind of touch gesture, and its approximate location. The main advantage of using contact microphones as touch sensors is that by using just one, it is possible to “cover” a whole solid part of the robot. Besides, the sensors are unaffected by ambient noises, such as human voice, TV, music etc. Nevertheless, the fact of using several contact microphones makes possible that a touch gesture is detected by all of them, and each may recognise a different gesture at the same time. The results show that this system is robust against this phenomenon. Moreover, the accuracy obtained for both robots is about 86%.",science
10.1016/j.compag.2020.105434,Journal,Computers and Electronics in Agriculture,scopus,2020-06-01,sciencedirect,Dynamic Simulation Tool of fertigation in drip irrigation subunits,https://api.elsevier.com/content/abstract/scopus_id/85083338592,"Agriculture consumes approximately 95 million tonnes of fertilizers and 97,000 tonnes of active ingredients of pesticides and herbicides. Reducing external input systems can result in significant economic, social and environmental impact. Therefore, establishing an optimal fertigation schedule is essential to achieve efficient drip irrigation management and, therefore, achieve an optimal irrigated agriculture management system that ensures productive, environmental and economic viability. The objective of this study was to develop a decision support system (DSS) to facilitate farmers’ decision-making process and optimize the design and management of farm fertigation scheduling. Implemented in MATLAB®, FERTI-DRIP, was tested in a regular irrigation subunit and in an irregular irrigation subunit of a real water user association. Both irrigation subunits were tested with two irrigation emitter types: pressure-compensating emitters and non-pressure-compensating emitters. Thus, FERTI-DRIP was applied to four scenarios to analyse the effect of the size and shape of the irrigation subunit on the fertigation process. The effect of the pressure head in the irrigation subunit and the effect of the fertilizer dynamics during the fertigation event were also analysed. FERTI-DRIP allows users to compute fertilizer quality parameters to determine how to implement fertigation. FERTI-DRIP also allows users to accurately select pre-fertigation and post-fertigation processes to optimize hydraulic stability at the beginning of the fertigation event and ensure that there is no fertilizer remaining in the irrigation system after the fertigation event. FERTI-DRIP can be very helpful for start-time irrigation events, such in the case of sandy soils where pulse drip irrigation should be performed.",science
10.1016/j.aca.2020.04.007,Journal,Analytica Chimica Acta,scopus,2020-06-01,sciencedirect,Dual-emission CdTe/AgInS<inf>2</inf> photoluminescence probe coupled to neural network data processing for the simultaneous determination of folic acid and iron (II),https://api.elsevier.com/content/abstract/scopus_id/85083073504,"This work focused on the combination of CdTe and AgInS2 quantum dots in a dual-emission nanoprobe for the simultaneous determination of folic acid and Fe(II) in pharmaceutical formulations. The surface chemistry of the used QDs was amended with suitable capping ligands to obtain appropriate reactivity in terms of selectivity and sensitivity towards the target analytes. The implementation of PL-based sensing schemes combining multiple QDs of different nature, excited at the same wavelength and emitting at different ones, allowed to obtain a specific analyte-response profile. The first-order fluorescence data obtained from the whole emission spectra of the CdTe/AgInS2 combined nanoprobe upon interaction with folic acid and Fe(II) were processed by using chemometric tools, namely partial least-squares (PLS) and artificial neural network (ANN). This enabled to circumvent the selectivity issues commonly associated with the use of QDs prone to indiscriminate interaction with multiple species, which impair reliable and accurate quantification in complex matrices samples.
                  ANN demonstrated to be the most efficient chemometric model for the simultaneous determination of both analytes in binary mixtures and pharmaceutical formulations due to the non-linear relationship between analyte concentration and fluorescence data that it could handle. The R2
                     P and SEP% obtained for both analytes quantification in pharmaceutical formulations through ANN modelling ranged from 0.92 to 0.99 and 5.7–9.1%, respectively. The obtained results revealed that the developed approach is able to quantify, with high reliability and accuracy, more than one analyte in complex mixtures and real samples with pharmaceutical interest.",science
10.1016/j.scitotenv.2020.137459,Journal,Science of the Total Environment,scopus,2020-06-01,sciencedirect,NO<inf>x</inf> removal efficiency of urban photocatalytic pavements at pilot scale,https://api.elsevier.com/content/abstract/scopus_id/85081197829,"Photocatalytic technology implemented in construction materials is a promising solution to contribute to alleviate air quality issues found in big cities. Photocatalysis has been proved able to mineralise most harmful contaminants. However, important problems associated with monitoring the efficiency of these solutions under real conditions still remain, including the lack of affordable analytical tools to measure NOx concentrations with enough accuracy. In this work, two pilot scale demonstration platforms were built at two different locations to assess the photocatalytic NOX removal efficiency of ten selected materials exposed outdoors for AQmesh low-cost sensor PODs were used to measure ground-level to measure NO and NO2 concentrations during nearly one year. The pollutant removal efficiency of the materials was then calculated based on a comparison with simultaneously concentration measurements carried-out on reference, non-active materials. It was found that the NO2 removal efficiency presented large variations across the seasons, with maxima during the warmer months, while NO efficiencies were comparatively steadier. Statistical analysis delivered evidence that the efficiencies significantly depend on different meteorological variables (irradiance and relative humidity) besides NO, NO2 ambient concentrations. Lower efficiencies were observed for higher concentration levels and vice versa. The influence of water vapour could be related to two different effects: a short-term contribution by the instantaneous air humidity and a long-term component associated with the hygroscopic state of the material. The contribution of wind to the pollutant removal efficiencies was principally related to the humidity of air masses moving above the location and to the advection of pollutants from specific emission sources.",science
10.1016/j.inffus.2019.12.012,Journal,Information Fusion,scopus,2020-06-01,sciencedirect,"Explainable Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",https://api.elsevier.com/content/abstract/scopus_id/85077515399,"In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.",science
10.1016/j.cels.2020.04.006,Journal,Cell Systems,scopus,2020-05-20,sciencedirect,Lattice Light-Sheet Microscopy Multi-dimensional Analyses (LaMDA) of T-Cell Receptor Dynamics Predict T-Cell Signaling States,https://api.elsevier.com/content/abstract/scopus_id/85084658613,"Lattice light-sheet microscopy provides large amounts of high-dimensional, high-spatiotemporal resolution imaging data of cell surface receptors across the 3D surface of live cells, but user-friendly analysis pipelines are lacking. Here, we introduce lattice light-sheet microscopy multi-dimensional analyses (LaMDA), an end-to-end pipeline comprised of publicly available software packages that combines machine learning, dimensionality reduction, and diffusion maps to analyze surface receptor dynamics and classify cellular signaling states without the need for complex biochemical measurements or other prior information. We use LaMDA to analyze images of T-cell receptor (TCR) microclusters on the surface of live primary T cells under resting and stimulated conditions. We observe global spatial and temporal changes of TCRs across the 3D cell surface, accurately differentiate stimulated cells from unstimulated cells, precisely predict attenuated T-cell signaling after CD4 and CD28 receptor blockades, and reliably discriminate between structurally similar TCR ligands. All instructions needed to implement LaMDA are included in this paper.",science
10.1016/j.enbuild.2020.109825,Journal,Energy and Buildings,scopus,2020-05-15,sciencedirect,Building temperature regulation in a multi-zone HVAC system using distributed adaptive control,https://api.elsevier.com/content/abstract/scopus_id/85081129562,"During recent years there have been considerable research efforts on improving energy efficiency of buildings. Since Heating, Ventilation and Air-Conditioning (HVAC) systems are responsible for a big part of energy consumption, developing efficient HVAC control systems is crucial. In most of the developed approaches, precise knowledge of system parameters and/or adequate historical data is required. However, these approaches may not perform as well in the presence of dynamic parameter changes due to human activity, material degradation, and wear and tear, or disturbances and other operational uncertainties due to occupancy, solar gains, electrical equipment, and weather conditions. In this paper, we consider buildings with several climate zones and propose a distributed adaptive control scheme for a multi-zone HVAC system which can effectively regulate zone temperature by applying on-line learning and assuming exchange of information between neighboring zones. The controller of each zone achieves the local objective of controlling zone temperature by compensating for the effects of neighboring zones as well as for possible changes in the parameters of the system. Despite the exchange of information, each local controller does not know how the control actions and temperature of a neighboring zone affect the temperature of its own zone. For this reason, each local controller is estimating the parameters of the interconnections in real time and uses them together with the exchanged information to provide a more accurate local zone temperature control. The proposed method is illustrated using an example of temperature control in a six-zone building as well as a large school building, which are implemented in a Building Controls Virtual Test Bed (BCVTB) environment using EnergyPlus and MATLAB/Simulink.",science
10.1016/j.patter.2020.100013,Journal,Patterns,scopus,2020-05-08,sciencedirect,Random Forest Models for Accurate Identification of Coordination Environments from X-Ray Absorption Near-Edge Structure,https://api.elsevier.com/content/abstract/scopus_id/85088690454,"Analyzing coordination environments using X-ray absorption spectroscopy has broad applications in solid-state physics and material chemistry. Here, we show that random forest models trained on 190,000 K-edge X-ray absorption near-edge structure (XANES) spectra can identify the main atomic coordination environment with a high accuracy of 85.4% and all associated coordination environments with a high Jaccard score of 81.8% for 33 cation elements in oxides, significantly outperforming other machine-learning models. In a departure from prior works, the coordination environment is described as a distribution over 25 distinct coordination motifs with coordination numbers ranging from 1 to 12. More importantly, we show that the random forest models can be used to predict coordination environments from experimental K-edge XANES with minimal loss in accuracy. A drop-variable feature importance analysis highlights the key roles that the pre-edge and main-peak regions play in coordination environment identification.",science
10.1016/j.rse.2020.111717,Journal,Remote Sensing of Environment,scopus,2020-05-01,sciencedirect,Assessing the relationship between macro-faunal burrowing activity and mudflat geomorphology from UAV-based Structure-from-Motion photogrammetry,https://api.elsevier.com/content/abstract/scopus_id/85079899077,"Characterisation of the ecosystem functioning of mudflats requires insight on the morphology and facies of these coastal features, but also on biological processes that influence mudflat geomorphology, such as crab bioturbation and the formation of benthic biofilms, as well as their heterogeneity at cm or less scales. Insight into this fine scale of ecosystem functioning is also important as far as minimizing errors in upscaling are concerned. The realisation of high-resolution ground surveys of these mudflats without perturbing their surface is a real challenge. Here, we address this challenge using UAV-supported photogrammetry based on the Structure-from-Motion (SfM) workflow. We produced a Digital Surface Model (DSM) and an orthophotograph at 1 cm and 0.5 cm pixel resolutions, respectively, of a mudflat in French Guiana, and mapped and classed into different size ranges intricate morphological features, including crab burrow apertures, tidal drainage creeks and depressions. We also determined subtle facies and elevation changes and slopes, and the footprint of different degrees of benthic biofilm development. The results generated at this scale of photogrammetric analysis also enabled us to relate macrofaunal crab burrowing activity to various parameters, including mudflat elevation, spatial distribution and sizes of creeks and depressions, benthic biofilm distribution, and flooding duration. SfM photogrammetry offers interesting new perspectives in fine-scale characterisation of the geomorphology, benthic activity and degree of biofilm development of dynamic muddy intertidal environments that are generally difficult of access. The main shortcomings highlighted in this study are a drift of accuracy of the DSM outside areas of ground control points and the deployment of which perturb the mudflat morphology and biology, the water-logged or very wet surfaces which generate reconstruction artefacts through the sun glint effect, and the time-consuming task of manual interpretation of extraction of features such as crab burrow apertures. On-going developments in UAV positioning integrating RTK/PPK GPS solutions for image-georeferencing and precise orientation with high-quality inertial measurement units will limit the difficulties inherent to ground control points, while conduction of surveys during homogeneous cloudy conditions could reduce the sun-glint effect. Manual extraction of image features could be automated in the future through the use of deep-learning algorithms.",science
10.1016/j.ijmultiphaseflow.2019.103194,Journal,International Journal of Multiphase Flow,scopus,2020-05-01,sciencedirect,Bubble patterns recognition using neural networks: Application to the analysis of a two-phase bubbly jet,https://api.elsevier.com/content/abstract/scopus_id/85079560188,"Gas-liquid two-phase bubbly flows are found in different areas of science and technology such as nuclear energy, chemical industry, or piping systems. Optical diagnostics of two-phase bubbly flows with modern panoramic techniques makes it possible to capture simultaneously instantaneous characteristics of both continuous and dispersed phases with a high spatial resolution. In this paper, we introduce a novel approach based on neural networks to recognize bubble patterns in images and identify their geometric parameters. The originality of the proposed method consists in training of a neural network ensemble using synthetic images that resemble real photographs gathered in experiment. The use of neural networks in combination with automatically generated data allowed us to detect overlapping, blurred, and non-spherical bubbles in a broad range of volume gas fractions. Experiments on a turbulent bubbly jet proved that the implemented method increases the identification accuracy, reducing errors of various kinds, and lowers the processing time compared to conventional recognition methods. Furthermore, utilizing the new method of bubbles recognition, the primary physical parameters of a dispersed phase, such as bubble size distribution and local gas content, were calculated in a near-to-nozzle region of the bubbly jet. The obtained results and integral experimental parameters, especially volume gas fraction, are in good agreement with each other.",science
10.1016/j.ijfatigue.2019.105458,Journal,International Journal of Fatigue,scopus,2020-05-01,sciencedirect,Steel railway bridge fatigue damage detection using numerical models and machine learning: Mitigating influence of modeling uncertainty,https://api.elsevier.com/content/abstract/scopus_id/85077500262,"Stringer-to-floor beam connections were reported as one of the most fatigue-prone details in riveted steel railway bridges. To detect stiffness degradation that results from the initiation and growth of fatigue cracks, an automated damage detection framework was proposed by the authors (Eftekhar Azam et al., 2019; Rageh et al., 2018). The proposed method relies on Proper Orthogonal Decomposition (POD) and Artificial Neural Networks (ANNs) to identify damage location and intensity under non-stationary, unknown train loads. Bridge computational models were used to simulate damage scenarios and for training the ANNs. Damage detection method efficiency and accuracy were shown to be significantly influenced by the level of modeling uncertainties (MUs). To investigate the applicability of the proposed framework to in-service bridges, a systematic analysis of the effect of MUs on the proposed POD-ANN framework was necessary. MU influence on the performance of the POD-ANN damage detection method was investigated and a new procedure for generating training data for ANNs was proposed. The procedure was based on synergizing Proper Orthogonal Modes (POMs) extracted from measured structural response and POMs calculated from the numerical model. The current study integrated numerical and field investigations. The main objective of the numerical investigation was to identify a robust damage feature independent of the level and location of assumed MUs. Results showed that Damage Location (DL) and Damage Intensity (DI) were detected with high accuracy for studied uncertainty cases; however, as expected, damage detection accuracy reduced as MU increased. A hybrid experimental-numerical approach was then implemented for the field investigation studies. This approach applied identified damage features from the numerical investigation to measurements from an in-service railway bridge to produce damage scenarios used to train the framework. MATLAB algorithms were developed that preprocessed field data and eliminated POM variations resulted from loading uncertainties. ANNs were trained and tested using the field strain estimated POMs from the hybrid approach and DL and DI results were obtained for the studied railway bridge under non-stationary, unknown train loads. These results show the promise of the POD-ANN method as a robust, real-time fatigue damage identification tool for steel railway bridges.",science
10.1016/j.cmpb.2019.105263,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-05-01,sciencedirect,Parallelized collision detection with applications in virtual bone machining,https://api.elsevier.com/content/abstract/scopus_id/85076248977,"Background and objectives
                  Virtual reality surgery simulators have been proved effective for training in several surgical disciplines. Nevertheless, this technology is presently underutilized in orthopaedics, especially for bone machining procedures, due to the limited realism in haptic simulation of bone interactions. Collision detection is an integral part of surgery simulators and its accuracy and computational efficiency play a determinant role on the fidelity of simulations. To address this, the primary objective of this study was to develop a new algorithm that enables faster and more accurate collision detection within 1 ms (required for stable haptic rendering) in order to facilitate the improvement of the realism of virtual bone machining procedures.
               
                  Methods
                  The core of the developed algorithm is constituted by voxmap point shell method according to which tool and osseous tissue geometries were sampled by points and voxels, respectively. The algorithm projects tool sampling points into the voxmap coordinates and compute an intersection condition for each point-voxel pair. This step is massively parallelized using Graphical Processing Units and it is further accelerated by an early culling of the unnecessary threads as instructed by the rapid estimation of the possible intersection volume. A contiguous array was used for implicit definition of voxmap in order to guarantee a fast access to voxels and thereby enable efficient material removal. A sparse representation of tool points was employed for efficient memory reductions. The effectiveness of the algorithm was evaluated at various bone sampling resolutions and was compared with prior relevant implementations.
               
                  Results
                  The results obtained with an average hardware configuration have indicated that the developed algorithm is capable to reliably maintain < 1 ms running time in severe tool-bone collisions, both sampled at 10243 resolutions. The results also showed the algorithm running time has a low sensitivity to bone sampling resolution. The comparisons performed suggested that the proposed approach is significantly faster than comparable methods while relying on lower or similar memory requirements.
               
                  Conclusions
                  The algorithm proposed through this study enables a higher numerical efficiency and is capable to significantly enlarge the maximum resolution that can be used by high fidelity/high realism haptic simulators targeting surgical orthopaedic procedures.",science
10.1016/j.knosys.2020.105580,Journal,Knowledge-Based Systems,scopus,2020-04-22,sciencedirect,Finding influential nodes in social networks based on neighborhood correlation coefficient,https://api.elsevier.com/content/abstract/scopus_id/85078759029,"Finding the most influential nodes in social networks has significant applications. A number of methods have been recently proposed to estimate influentiality of nodes based on their structural location in the network. It has been shown that the number of neighbors shared by a node and its neighbors accounts for determining its influence. In this paper, an improved cluster rank approach is presented that takes into account common hierarchy of nodes and their neighborhood set. A number of experiments are conducted on synthetic and real networks to reveal effectiveness of the proposed ranking approach. We also consider ground-truth influence ranking based on Susceptible–Infected–Recovered model, on which performance of the proposed ranking algorithm is verified. The experiments show that the proposed method outperforms state-of-the-art algorithms.",science
10.1016/j.jpowsour.2020.227964,Journal,Journal of Power Sources,scopus,2020-04-15,sciencedirect,Data-driven reinforcement-learning-based hierarchical energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles,https://api.elsevier.com/content/abstract/scopus_id/85080125591,"A reinforcement-learning-based energy management strategy is proposed in this paper for managing energy system of Fuel Cell Hybrid Electric Vehicles (FCHEV) equipped with three power sources. A hierarchical power splitting structure is employed to shrink large state-action space based on an adaptive fuzzy filter. Then, the reinforcement-learning-based algorithm using Equivalent Consumption Minimization Strategy (ECMS) is proposed for tackling high-dimensional state-action space, and finding a trade-off between global learning and real-time implementation. The power splitting policy based on experimental data is obtained by using reinforcement learning algorithm, which allows for many different driving cycles and traffic conditions. The proposed energy management strategy can achieve low computation cost, optimal fuel cell efficiency and energy consumption economy. Simulation results confirm that, compared with existing learning algorithms and optimization methods, the proposed reinforcement-learning-based energy management strategy using ECMS can achieve high computation efficiency, lower power fluctuation of fuel cell and optimal fuel economy of FCHEV.",science
10.1016/j.apenergy.2020.114680,Journal,Applied Energy,scopus,2020-04-15,sciencedirect,Household standards and socio-economic aspects as a factor determining energy consumption in the city,https://api.elsevier.com/content/abstract/scopus_id/85079833073,"Political or economic attempts to mitigate climate change by increasing fossil fuel prices lead to and an increase in energy poverty, i.e., social effects. The ideal solution would be to combine modernisation activities in terms of energy use in cities with sustainable strategies and redevelopment policies. The article's purpose is to estimate the potential for reducing energy consumption depending on socioeconomic factors (household standard and its location in the city) based on built-in scenarios and searching for the optimal way of conducting development policy at the local level. This assumption enables the implementation of the European Union climate policy. To this aim, modelling based on real and estimated data on the diversity of energy consumption in the structure of a medium-sized city in Europe (Zielona Góra) carried out. While creating scenarios, there used a modelling method based on radial artificial neural networks, which map the input set into the output set by matching many individual approximating functions to setpoints. This approach works well for data whose geolocation is in the city quarters. As a result of the simulations, the minimum and maximum achievable energy saving potential for low-intensity buildings in the quarters was estimated, taking into account the possibilities of investing in renewable energy by individual households. The observations included in the article may be relevant to other regions that are interested in reducing the energy consumption of buildings and pollution emissions from the cities. This is particularly important for the regions of Europe that benefit from the financial support of the European Union (including local development programmes based on financing European priority axes for economic development).",science
10.1016/j.cma.2019.112790,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2020-04-15,sciencedirect,"An energy approach to the solution of partial differential equations in computational mechanics via machine learning: Concepts, implementation and applications",https://api.elsevier.com/content/abstract/scopus_id/85077809695,"Partial Differential Equations (PDEs) are fundamental to model different phenomena in science and engineering mathematically. Solving them is a crucial step towards a precise knowledge of the behavior of natural and engineered systems. In general, in order to solve PDEs that represent real systems to an acceptable degree, analytical methods are usually not enough. One has to resort to discretization methods. For engineering problems, probably the best-known option is the finite element method (FEM). However, powerful alternatives such as mesh-free methods and Isogeometric Analysis (IGA) are also available. The fundamental idea is to approximate the solution of the PDE by means of functions specifically built to have some desirable properties. In this contribution, we explore Deep Neural Networks (DNNs) as an option for approximation. They have shown impressive results in areas such as visual recognition. DNNs are regarded here as function approximation machines. There is great flexibility to define their structure and important advances in the architecture and the efficiency of the algorithms to implement them make DNNs a very interesting alternative to approximate the solution of a PDE. We concentrate on applications that have an interest for Computational Mechanics. Most contributions explore this possibility have adopted a collocation strategy. In this work, we concentrate on mechanical problems and analyze the energetic format of the PDE. The energy of a mechanical system seems to be the natural loss function for a machine learning method to approach a mechanical problem. In order to prove the concepts, we deal with several problems and explore the capabilities of the method for applications in engineering.",science
10.1016/j.brainresbull.2020.01.009,Journal,Brain Research Bulletin,scopus,2020-04-01,sciencedirect,MicroRNAs expressed in neuronal differentiation and their associated pathways: Systematic review and bioinformatics analysis,https://api.elsevier.com/content/abstract/scopus_id/85079431489,"MicroRNAs (miRNAs) plays an important role in the human brain from the embryonic period to adulthood. In this sense, they influence the development of neural stem cells (NSCs), regulating cellular differentiation and survival. Therefore, due to the importance of better comprehending the regulation of miRNAs in NSCs differentiation and the lack of studies that show the panorama of miRNAs and their signaling pathways studied until now we aimed to systematically review the literature to identify which miRNAs are currently being associated with neuronal differentiation and using bioinformatics analysis to identify their related pathways. A search was carried out in the following databases: Scientific Electronic Library Online (Scielo), National Library of Medicine National Institutes of Health (PubMed), Scopus, Web of Science and Science Direct, using the descriptors “(microRNA [MeSH])” and “(neurogenesis [MeSH])”. From the articles found, two independent and previously calibrated reviewers, using the EndNote X7 (Thomson Reuters, New York, NY, US), selected those that concern miRNA in the development of NSCs, based on in vitro studies. After, bioinformatic analysis was performed using the software DIANA Tools, mirPath v.3. Subsequently, data was tabulated, analyzed and interpreted. Among the 106 miRNAs cited by included studies, 55 were up-regulated and 47 were down-regulated. The bioinformatics analysis revealed that among the up-regulated miRNAs there were 24 total and 6 union pathways, and 3 presented a statistically significant difference (p ≤ 0.05). Among the down-regulated miRNAs, 46 total and 13 union pathways were found, with 7 presenting a significant difference (p ≤ 0.05). The miR-125a-5p, miR-423-5p, miR-320 were the most frequently found miRNAs in the pathways determined by bioinformatics. In this study a panel of altered miRNAs in neuronal differentiation was created with their related pathways, which could be a step towards understanding the complex network of miRNAs in neuronal differentiation.",science
10.1016/j.ins.2019.12.033,Journal,Information Sciences,scopus,2020-04-01,sciencedirect,Relation constrained attributed network embedding,https://api.elsevier.com/content/abstract/scopus_id/85076856635,"Network embedding aims at learning a low-dimensional dense vector for each node in the network. In recent years, it has attracted great research attention due to its wide applications. Most existing studies model the graph structure only and neglect the attribute information. Although several attributed network embedding methods take the node attribute into account, they mainly focus on the basic relations between the nodes and their attributes like a user and his/her interests (attributes). The composite relations between two nodes, and two nodes’ attributes, and the related nodes and their attributes, contain rich information and can enhance the performance of many network analysis tasks. For example, two scholars having the common interests as “nature language processing” and “knowledge graph” may collaborate in the future and there will be a new edge in the network. However, such important information is still under-exploited.
                  To address this limitation, we propose a novel framework to exploit the abundant relation information to enhance attributed network embedding. The main idea is to employ the multiple types of relations in attributed networks as the constraints to improve the network representation. To this end, we first construct the composite relations between two nodes and their attributes in addition to the commonly used basic relations. We then develop a relation constrained attributed network (RCAN) framework to learn the node representations by constraining them with these relations. We conduct extensive experiments on three real-world datasets to show the effectiveness of our proposed RCAN as an attributed network embedding method for modeling various social networks. The results demonstrate that our method achieves significantly better performance than the state-of-the-art baselines in both the link prediction and node clustering tasks.",science
10.1016/j.talanta.2019.120664,Journal,Talanta,scopus,2020-04-01,sciencedirect,Modelling of bioprocess non-linear fluorescence data for at-line prediction of etanercept based on artificial neural networks optimized by response surface methodology,https://api.elsevier.com/content/abstract/scopus_id/85076829838,"In the last years, regulatory agencies in biopharmaceutical industry have promoted the design and implementation of Process Analytical Technology (PAT), which aims to develop rapid and high-throughput strategies for real-time monitoring of bioprocesses key variables, in order to improve their quality control lines. In this context, spectroscopic techniques for data generation in combination with chemometrics represent alternative analytical methods for on-line critical process variables prediction. In this work, a novel multivariate calibration strategy for the at-line prediction of etanercept, a recombinant protein produced in a mammalian cells-based perfusion process, is presented. For data generation, samples from etanercept processes were daily obtained, from which fluorescence excitation-emission matrices were generated in the spectral ranges of 225.0 and 495.0 nm and 250.0 and 599.5 nm for excitation and emission modes, respectively. These data were correlated with etanercept concentration in supernatant (measured by an off-line HPLC-based reference univariate technique) by implementing different chemometric strategies, in order to build predictive models. Partial least squares (PLS) regression evidenced a non-linear relation between signal and concentration when observing actual vs. predicted concentrations. Hence, a non-parametric approach was implemented, based on a multilayer perceptron artificial neural network (MLP). The MLP topology was optimized by means of the response surface methodology. The prediction performance of MLP model was superior to PLS, since the first is able to cope with non-linearity in calibration models, reaching percentage mean relative error in predictions of about 7.0% (against 12.6% for PLS). This strategy represents a fast and inexpensive approach for etanercept monitoring, which conforms the principles of PAT.",science
10.1016/j.eswa.2019.113083,Journal,Expert Systems with Applications,scopus,2020-04-01,sciencedirect,Real-time biomechanical modeling of the liver using Machine Learning models trained on Finite Element Method simulations,https://api.elsevier.com/content/abstract/scopus_id/85074768700,"The development of accurate real-time models of the biomechanical behavior of different organs and tissues still poses a challenge in the field of biomechanical engineering. In the case of the liver, specifically, such a model would constitute a great leap forward in the implementation of complex applications such as surgical simulators, computed-assisted surgery or guided tumor irradiation.
                  In this work, a relatively novel approach for developing such a model is presented. It consists in the use of a machine learning algorithm, which provides real-time inference, trained on tens of thousands of simulations of the biomechanical behavior of the liver carried out by the finite element method on more than 100 different liver geometries.
                  Considering a target accuracy threshold of 3 mm for the Euclidean Error, four different scenarios were modeled and assessed: a single liver with an arbitrary force applied (99.96% of samples within the accepted error range), a single liver with two simultaneous forces applied (99.84% samples in range), a single liver with different material properties and an arbitrary force applied (98.46% samples in range), and a much more general model capable of modeling the behavior of any liver with an arbitrary force applied (99.01% samples in range for the median liver).
                  The results show that the Machine Learning models perform extremely well on all the scenarios, managing to keep the Mean Euclidean Error under 1 mm in all cases. Furthermore, the proposed model achieves working frequencies above 100Hz on modest hardware (with frequencies above 1000Hz being easily achievable on more powerful GPUs) thus fulfilling the real-time requirements. These results constitute a remarkable improvement in this field and may involve a prompt implementation in clinical practice.",science
10.1016/j.cmpb.2019.105019,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-04-01,sciencedirect,Automatic diagnosis of fungal keratitis using data augmentation and image fusion with deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85070552720,"Background and objectives
                  Fungal keratitis is caused by inflammation of the cornea that results from infection by fungal organisms. The lack of an early effective diagnosis often results in serious complications even blindness. Confocal microscopy is one of the most effective methods in the diagnosis of fungal keratitis, but the diagnosis depends on the subjective judgment of medical experts.
               
                  Methods
                  To address this problem, this paper proposes a novel convolutional neural network framework for the automatic diagnosis of fungal keratitis using data augmentation and image fusion. Firstly, a normal image is augmented by flipping to solve the problem of having a limited and imbalanced database. Secondly, a sub-area contrast stretching algorithm is proposed for image preprocessing to highlight the key structures in the images and to filter out irrelevant information. Thirdly, the histogram matching fusion algorithm is implemented, then the preprocessed image is fused with the original image to form a new algorithm framework and a new database. Finally, the traditional convolutional neural network is integrated into the novel algorithm framework to perform the experiments.
               
                  Results
                  Experiments show that the accuracy of traditional AlexNet and VGGNet is 99.35% and 99.14%, that of AlexNet and VGGNet based on MF fusion is 99.80% and 99.83%, and that of AlexNet and VGGNet based on histogram matching fusion (HMF) is 99.95% and 99.89%. The experimental results show that the AlexNet framework using data augmentation and image fusion achieves a perfect trade-off between the diagnostic performance and the computational complexity, with a diagnostic accuracy of 99.95%.
               
                  Conclusions
                  These experimental results demonstrate the novel convolutional neural network framework perfectly balances the diagnostic performance and computational complexity, and can improve the effect and real-time performance in the diagnosis of fungal keratitis.",science
10.1016/j.amar.2020.100113,Journal,Analytic Methods in Accident Research,scopus,2020-03-01,sciencedirect,"Big data, traditional data and the tradeoffs between prediction and causality in highway-safety analysis",https://api.elsevier.com/content/abstract/scopus_id/85078666924,"The analysis of highway accident data is largely dominated by traditional statistical methods (standard regression-based approaches), advanced statistical methods (such as models that account for unobserved heterogeneity), and data-driven methods (artificial intelligence, neural networks, machine learning, and so on). These methods have been applied mostly using data from observed crashes, but this can create a problem in uncovering causality since individuals that are inherently riskier than the population as a whole may be over-represented in the data. In addition, when and where individuals choose to drive could affect data analyses that use real-time data since the population of observed drivers could change over time. This issue, the nature of the data, and the implementation target of the analysis imply that analysts must often tradeoff the predictive capability of the resulting analysis and its ability to uncover the underlying causal nature of crash-contributing factors. The selection of the data-analysis method is often made without full consideration of this tradeoff, even though there are potentially important implications for the development of safety countermeasures and policies. This paper provides a discussion of the issues involved in this tradeoff with regard to specific methodological alternatives and presents researchers with a better understanding of the trade-offs often being inherently made in their analysis.",science
10.1016/j.prro.2019.11.011,Journal,Practical Radiation Oncology,scopus,2020-03-01,sciencedirect,"Computed Tomography to Cone Beam Computed Tomography Deformable Image Registration for Contour Propagation Using Head and Neck, Patient-Based Computational Phantoms: A Multicenter Study",https://api.elsevier.com/content/abstract/scopus_id/85078045320,"Purpose
                  To investigate the performance of various algorithms for deformable image registration (DIR) for propagating regions of interest (ROIs) using multiple commercial platforms, from computed tomography to cone beam computed tomography (CBCT) and megavoltage computed tomography.
               
                  Methods and Materials
                  Fourteen institutions participated in the study using 5 commercial platforms: RayStation (RaySearch Laboratories, Stockholm, Sweden), MIM (Cleveland, OH), VelocityAI and SmartAdapt (Varian Medical Systems, Palo Alto, CA), and ABAS (Elekta AB, Stockholm, Sweden). Algorithms were tested on synthetic images generated with the ImSimQA (Oncology Systems Limited, Shrewsbury, UK) package by applying 2 specific deformation vector fields (DVF) to real head and neck patient datasets. On-board images from 3 systems were used: megavoltage computed tomography from Tomotherapy and 2 kinds of CBCT from a clinical linear accelerator. Image quality of the system was evaluated. The algorithms’ accuracy was assessed by comparing the DIR-mapped ROIs returned by each center with those of the reference, using the Dice similarity coefficient and mean distance to conformity metrics. Statistical inference on the validation results was carried out to identify the prognostic factors of DIR performance.
               
                  Results
                  Analyzing 840 DIR-mapped ROIs returned by the centers, it was demonstrated that DVF intensity and image quality were significant prognostic factors of DIR performance. The accuracy of the propagated contours was generally high, and acceptable DIR performance can be obtained with lower-dose CBCT image protocols.
               
                  Conclusions
                  The performance of the systems proved to be image quality specific, depending on the DVF type and only partially on the platforms. All systems proved to be robust against image artifacts and noise, except the demon-based software.",science
10.1016/j.entcom.2019.100336,Journal,Entertainment Computing,scopus,2020-03-01,sciencedirect,Smart Reckoning: Reducing the traffic of online multiplayer games using machine learning for movement prediction,https://api.elsevier.com/content/abstract/scopus_id/85077019240,"Massively Multiplayer Online Game (MMOG) players maintain consistent views of the positions of each other by periodically exchanging messages. Besides the fact that these messages can suffer delays that cause rendering inconsistencies, they also represent an overhead on the network. This overhead can be significant, as the number of MMOG players is very large, but reducing the number of messages is not trivial. The classic strategy to predict movement avoiding message exchange is based on the Dead Reckoning algorithm, which has several limitations. Other strategies have been proposed more recently that improve the results, but rely on expert knowledge. In this work we propose Smart Reckoning, a movement prediction strategy based on machine learning. The strategy consists of two phases. In the first phase, a learning model classifies whether the classical Dead Reckoning algorithm is able to predict the new avatar position correctly or not. In case the conclusion is negative, another learning model is used to predict the new direction. The proposed strategy was applied to the World of Warcraft game. The learning models were implemented with the Weka tool using real game trace data, and results are presented for the accuracy of multiple algorithms.",science
10.1016/j.envres.2019.109024,Journal,Environmental Research,scopus,2020-03-01,sciencedirect,An efficient tool for the continuous monitoring on adsorption of sub-ppm level gaseous benzene using an automated analytical system based on thermal desorption-gas chromatography/mass spectrometry approach,https://api.elsevier.com/content/abstract/scopus_id/85076492466,"It became an important task to effectively adsorb volatile organic compounds (VOCs) at or near real-world levels for efficient control of airborne pollution in ambient environments. Nonetheless, most studies carried out previously for the control of VOCs are confined to significantly polluted conditions (e.g., >100 ppm) that are far different from real-world or ambient conditions. To help acquire the meaningful data for the adsorptive removal of VOCs at near real-world levels, a new approach was designed and implemented to measure adsorption of gaseous benzene (as a representative or model VOC) at trace-level quantities (as low as 0.14 ng (0.43 ppb) for a 100 mL sample) using activated carbon (sieved to 212 μm mesh size) as a model sorbent. With the aid of a thermal desorption-gas chromatography/mass spectrometry system, the key adsorption performance metrics (such as 10% breakthrough volume (10% BTV) points: 10% as the key reference) were determined: 1018 L atm g−1 at 0.1 ppm benzene with the corresponding partition coefficient of 3.85 mol kg−1 Pa−1. If the adsorption capacity values (at 10% BTV) are compared across the varying concentration levels of benzene, the maximum value of 1.07 mg g−1 was observed at 1 ppm benzene (within the concentration range selected in this work). As such, it was possible to quantitatively assess the sorbate-sorbent interactions at significantly low concentrations of VOCs that actually prevail under the near real-world conditions.",science
10.1016/j.actaastro.2019.11.037,Journal,Acta Astronautica,scopus,2020-03-01,sciencedirect,Pattern recognition in time series for space missions: A rosetta magnetic field case study,https://api.elsevier.com/content/abstract/scopus_id/85076239823,"Time series analysis is a technique widely employed in space science. In unpredictable environments like space, scientific analysis relies on large data sets to enable interpretation of observations. Artificial signal interferences caused by the spacecraft itself further impede this process. The most time consuming part of these studies is the efficient identification of recurrent pattern in observations, both of artificial and natural origin, often forcing researchers to limit their analysis to a reduced set of observations. While pattern recognition techniques for time series are well known, their application is discussed and evaluated primarily on purpose built or heavily preprocessed data sets. The aim of this paper is to evaluate the performance of state of the art pattern recognition techniques in terms of computational efficiency and validity on a real-life testcase. For this purpose the most suitable techniques for different types of pattern are discussed and subsequently evaluated on various hardware in comparison to manual identification. Using magnetic field observations of the ESA Rosetta mission as a representative example, both disturbances and natural patterns are identified. Compared to manual selection a speed-up of a factor up to 100 is achieved, with values for recall and precision above 80%. Moreover, the detection process is fully automated and reproducible. Using the presented method it was possible to detect and correct artificial interference. Finally, the feasibility of onboard deployment is briefly discussed.",science
10.1016/j.renene.2019.09.092,Journal,Renewable Energy,scopus,2020-03-01,sciencedirect,Wind turbine fatigue reduction based on economic-tracking NMPC with direct ANN fatigue estimation,https://api.elsevier.com/content/abstract/scopus_id/85072713179,"The aim of this work is to deploy an advanced Nonlinear Model Predictive Control (NMPC) approach for reducing the tower fatigue of a wind turbine (WT) tower while guaranteeing efficient energy extraction from the wind. To achieve this, different Artificial Neural Network (ANN) architectures are trained and tested in order to estimate the tower fatigue as a surrogate of the traditional Rainflow Counting (RFC) method. The ANNs receive data stemming from the tower top oscillation velocity and the previous fatigue state to directly estimate the fatigue progression. The results are compared to select the most convenient architecture for control implementation. Once an ANN is selected, an economic-tracking NMPC (etNMPC) solution to reduce the fatigue of the WT tower is deployed in real-time. The closed-loop results are then compared to a baseline controller from a renowned WT simulation tool and a classic etNMPC implementation with indirect fatigue minimisation to demonstrate the improvement achieved with the proposed strategy. Finally, conclusions regarding computational cost and real-time deployment capabilities are discussed, as well as future lines of research.",science
10.1016/j.knosys.2019.105308,Journal,Knowledge-Based Systems,scopus,2020-02-29,sciencedirect,An OWA-based hierarchical clustering approach to understanding users’ lifestyles,https://api.elsevier.com/content/abstract/scopus_id/85076253160,"Based on users’ interactions with social networks, a method to understand users’ life-styles is developed. Descriptions of their lifestyles are obtained from previously reported experiences on these sites. Contextual information and contributed reviews lend insight into which elements are important for different lifestyles. In this paper, an ordered weighted averaging operator (OWA) is integrated with hierarchical clustering in order to find the similarity between users and clusters. Specifically, a two step measure is defined to compare and aggregate two clusters. To illustrate the efficiency of the methodology, a real case is implemented for 499 Yelp reviewers associated with 134,102 reviews across 11 variables and 373 Airbnb reviewers associated with 1,826 reviews across 14 variables.",science
10.1016/j.physa.2019.123151,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,Early warning system: From face recognition by surveillance cameras to social media analysis to detecting suspicious people,https://api.elsevier.com/content/abstract/scopus_id/85074532417,"Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the person’s network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",science
10.1016/j.physa.2019.123174,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,Fake news detection within online social media using supervised artificial intelligence algorithms,https://api.elsevier.com/content/abstract/scopus_id/85074460484,"Along with the development of the Internet, the emergence and widespread adoption of the social media concept have changed the way news is formed and published. News has become faster, less costly and easily accessible with social media. This change has come along with some disadvantages as well. In particular, beguiling content, such as fake news made by social media users, is becoming increasingly dangerous. The fake news problem, despite being introduced for the first time very recently, has become an important research topic due to the high content of social media. Writing fake comments and news on social media is easy for users. The main challenge is to determine the difference between real and fake news. In this paper, a two-step method for identifying fake news on social media has been proposed, focusing on fake news. In the first step of the method, a number of pre-processing is applied to the data set to convert un-structured data sets into the structured data set. The texts in the data set containing the news are represented by vectors using the obtained TF weighting method and Document-Term Matrix. In the second step, twenty-three supervised artificial intelligence algorithms have been implemented in the data set transformed into the structured format with the text mining methods. In this work, an experimental evaluation of the twenty-three intelligent classification methods has been performed within existing public data sets and these classification models have been compared depending on four evaluation metrics.",science
10.1016/j.cmpb.2019.105277,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-02-01,sciencedirect,An extensible software platform for interdisciplinary cardiovascular imaging research,https://api.elsevier.com/content/abstract/scopus_id/85076945151,"Background and objective
                  Cardiovascular imaging is an exponentially growing field with aspects ranging from image acquisition and analysis to disease characterization, and evaluation of therapy approaches.The transfer of innovative new technological and algorithmic solutions into clinical practice is still slow. In addition to the verification of solutions, their integration in the clinical processing workflow must be enabled for the assessment of clinical impact and risks. The goal of our software platform for cardiac image processing – CAIPI – is to support researchers from different specialties such as imaging physics, computer science, and medicine by a common extensible platform to address typical challenges and hurdles in interdisciplinary cardiovascular imaging research. It provides an integrated solution for method comparison, integrated analysis, and validation in the clinical context. The interface concept enables a combination with existing frameworks that address specific aspects of the pipeline, such as modeling (e.g., OpenCMISS, CARP) or image reconstruction (Gadgetron).
               
                  Methods
                  In our platform, we developed a concept for import, integration, and management of cardiac image data. The integration approach considers the spatiotemporal properties of the beating heart through a specific data model. The solution is based on MeVisLab and provides functionalities for data retrieval and storage. Two types of plugins can be added. While ToolPlugins usually provide processing algorithms such as image correction and segmentation, AnalysisPlugins enable interactive data exploration and reporting. GUI integration concepts are presented for both plugin types. We developed domain-specific reporting and visualization tools (e.g., AHA segment model) to enable validation studies by clinical experts. The platform offers plugins for calculating and reporting quantitative parameters such as cardiac function, which can be used to, e.g., evaluate the effect of processing algorithms on clinical parameters. Export functionalities include quantitative measurements to Excel, image data to PACS, and STL models to modeling and simulation tools.
               
                  Results
                  To demonstrate the applicability of this concept both for method development and clinical application, we present use cases representing different problems along the innovation chain in cardiac MR imaging.
                  Validation of an image reconstruction method (MRI T1 mapping)
                  Validation of an image correction method for real-time 2D-PC MRI
                  Comparison of quantification methods for blood flow analysis
                  Training and integration of machine learning solutions with expert annotations
                  Clinical studies with new imaging techniques (flow measurements in the carotid arteries and peripheral veins as well as cerebral spinal fluid).
               
                  Conclusion
                  The presented platform can be used in interdisciplinary teams, in which engineers or data scientists perform the method validation, followed by clinical research studies in patient collectives. The demonstrated use cases show how it enables the transfer of innovations through validation in the cardiovascular application context.",science
10.1016/j.cie.2019.106225,Journal,Computers and Industrial Engineering,scopus,2020-02-01,sciencedirect,Fuzzy possibility regression integrated with fuzzy adaptive neural network for predicting and optimizing electrical discharge machining parameters,https://api.elsevier.com/content/abstract/scopus_id/85076689961,"An electrical discharge machining (EDM) is one of the special production methods that are widely used in moldings, repairs and production of specific industrial components. Due to extensive production costs, optimal machining specifications are significant. Machining specifications are effective on output quality and thus attract more customers leading to higher profits. In this study, the impact of EDM parameters on surface roughness, material removal rate and electrode corrosion percentage have been investigated. In order to consider uncertainty of real production environments, the fuzzy theory is employed. Also, using the design of experiment (DOE) parameters calibration is performed and mathematical programming approach is applied for optimization purpose. The relationship between the machining parameters and the output process specification is examined by a fuzzy possibility regression model. Then, the mathematical relation of exact inputs and fuzzy outputs of the EDM process are extracted. The effectiveness of the three outputs is evaluated by interfacing models and fuzzy hypothesis testing. To determine the optimal levels of each output, a fuzzy adaptive neural network is used and appropriate models are prepared to be adapted with a fitted model of fuzzy possibility regression for comparison purposes. Validation tests imply the effectiveness of the proposed method. The integrated model is implemented in real case study. The results show that, fitted models can predict the material removal rate, surface fineness, and corrosion percentage of the electrode. The prediction accuracy of the proposed method is shown in comparison with the optimal fuzzy adaptive neural network outputs considering error value. Also, the proposed method is successful in identifying the optimal process parameters for EDM with reliable accuracy. The proposed integrated prediction and optimization model can be used as a calibration decision support in production systems to handle dynamic data structures and provide real time machining specifications to increase the output quality.",science
10.1016/j.breast.2019.10.001,Journal,Breast,scopus,2020-02-01,sciencedirect,"The ethical, legal and social implications of using artificial intelligence systems in breast cancer care",https://api.elsevier.com/content/abstract/scopus_id/85074099299,"Breast cancer care is a leading area for development of artificial intelligence (AI), with applications including screening and diagnosis, risk calculation, prognostication and clinical decision-support, management planning, and precision medicine. We review the ethical, legal and social implications of these developments. We consider the values encoded in algorithms, the need to evaluate outcomes, and issues of bias and transferability, data ownership, confidentiality and consent, and legal, moral and professional responsibility. We consider potential effects for patients, including on trust in healthcare, and provide some social science explanations for the apparent rush to implement AI solutions. We conclude by anticipating future directions for AI in breast cancer care. Stakeholders in healthcare AI should acknowledge that their enterprise is an ethical, legal and social challenge, not just a technical challenge. Taking these challenges seriously will require broad engagement, imposition of conditions on implementation, and pre-emptive systems of oversight to ensure that development does not run ahead of evaluation and deliberation. Once artificial intelligence becomes institutionalised, it may be difficult to reverse: a proactive role for government, regulators and professional groups will help ensure introduction in robust research contexts, and the development of a sound evidence base regarding real-world effectiveness. Detailed public discussion is required to consider what kind of AI is acceptable rather than simply accepting what is offered, thus optimising outcomes for health systems, professionals, society and those receiving care.",science
10.1016/j.talanta.2019.120446,Journal,Talanta,scopus,2020-02-01,sciencedirect,Paper-based colorimetric spot test utilizing smartphone sensing for detection of biomarkers,https://api.elsevier.com/content/abstract/scopus_id/85073026488,"The need for a continuous, real-time monitoring of specific diseases represents an unmet scientific need. Evidently, cancer is one of the most important diseases where it is crucial to increase the rates of patient survival and monitor disease prognosis. Herein, a novel type of immunoassay was developed for detection of cancer biomarkers, using alpha-fetoprotein (AFP) and mucin-16 (MUC16) as model analytes. Using gold nanoparticle (AuNP) bioconjugates as a signal production tool, relevant antibody (Ab)-conjugated AuNPs were prepared on the nitrocellulose (NC) membrane. To construct a spot-like point-of-care (POC) immunoassay, cysteamine conjugated AuNPs (AuNP-Cys) were immobilized on the NC membrane and antibodies were conjugated to the nanoparticle on the detection pad, following a treatment with the samples that contains AFP or MUC16 which are well-known protein biomarkers for liver and ovarian cancer. By using the change in the colorimetric properties of AuNPs, detection of tumor markers was achieved by using a smartphone image and color analysis software at the final stage. Image J application was used for the evaluation of color changes depending on the biomarker concentration in buffer or spiked synthetic serum samples. The linear range was found as 0.1 ng/mL-100 ng/mL for AFP and 0.1–10 ng/mL for MUC16. Limit-of-detection (LOD) was calculated as 1.054 ng/mL and 0.413 ng/mL for AFP and MUC16, respectively. Interferent molecules, Her2, Immunoglobulin G (IgG) and bovine serum albumin (BSA) were tested on the system. Furthermore, synthetic serum samples spiked with selected analyte molecule were applied on the system and measured successfully.",science
10.1016/j.chb.2019.09.008,Journal,Computers in Human Behavior,scopus,2020-02-01,sciencedirect,A collaborative working model for enhancing the learning process of science &amp; engineering students,https://api.elsevier.com/content/abstract/scopus_id/85072768570,"Science and engineering education are mostly based on content assimilation and development of skills. However, to adequately prepare students for today's world, it is also necessary to stimulate critical thinking and make them reflect on how to improve current practices using new tools and technologies. In this line, the main motivation of this research consists in exploring ways supported by technology to enhance the learning process of students and to better prepare them to face the challenges of today's world. To this end, the purpose of this work is to design an innovative learning project based on collaborative work among students, and research its impact in achieving better learning outcomes, generating of collective intelligence and further motivation. The proposed collaborative working model is based on peer review assessment methodology implemented through a learning web-platform. Thus, students were encouraged to peer review their classmates' works. They had to make comments, suggest improvements, and assess final assignments. Teaching staff managed and supervised the whole process. Students were selected from computer science engineering at the University of Alicante (Spain). Results suggested greater content assimilation and enhanced learning in several scientific skills. The students' final grade exceeded what any student could produce individually, but we cannot conclude that real collective intelligence was generated. Learning methodologies based on the possibilities of Information and Communication Technologies (ICT) provide new ways to transmit and manage knowledge in higher education. Collaborating in peer assessment enhances the students' motivation and promotes the active learning. In addition, this method can be very helpful and time saving for instructors in the management of large groups.",science
10.1016/j.knosys.2019.07.015,Journal,Knowledge-Based Systems,scopus,2020-01-05,sciencedirect,User intimacy model for question recommendation in community question answering,https://api.elsevier.com/content/abstract/scopus_id/85069593237,"In this paper, we address the problem of automatic recommendation of new questions to suitable users in community question answering (CQA). The major challenge is the accurate selection of suitable users to answer a given question. Most approaches seek suitable users for a question by estimating their capability, interests or a blend of both. However, this ignores intimacy between the user and the asker of a question over different topics. Intimacy between askers and answerers is an important factor in question recommendation. For example, a user is likely to post an answer if interested in a question and intimate with its asker. We propose to model and learn intimacy between users over topics with social interaction in CQA for question recommendation using a novel topic model. We believe this paper is the first to estimate the intimacy between users over different topics and investigate influences on the performance of question recommendation in CQA. We propose a user intimacy model (UIM), an LDA-style model that incorporates social interaction in the generative process of a question-answer (QA) pair to model and learn intimacy between users over topics. Experiments using real-world data from Stack Overflow show that our UIM-based approach consistently and significantly improves the performance of question recommendation, demonstrating that our approach can increase question recommendation accuracy in CQA by utilizing the intimacy between users over topics and that this is an important factor in question recommendation.",science
10.1016/j.neucom.2019.09.004,Journal,Neurocomputing,scopus,2020-01-02,sciencedirect,Digital neuromorphic real-time platform,https://api.elsevier.com/content/abstract/scopus_id/85072526243,"Hardware implementations of spiking neural networks in portable devices can improve many applications of robotics, neurorobotics or prosthetic fields in terms of power consumption, high-speed processing and learning mechanisms. Analog and digital platforms have been previously proposed to run these networks. Analog designs are closer to biology since they implement the original mathematical model. However, digital platforms are, to some extent, abstractions of this model so far. In this paper, a full digital platform to design, implement and run real-time analog-like spiking neural networks is presented. Specifically, we present the design and implementation of digital circuits to run real-time biologically plausible spiking neural networks on a Field Programmable Gate Array (FPGA). The circuit designed for the neuron implements the Leaky Integrate and Fire (LIF) model. The synapsis implemented is a bi-exponential current-based one. The synaptic circuit design consists of one static memory with the baseline current and a dynamic memory which stores the updated contribution over time of each pre-synaptic connection. All the parameters of both the neuron and the synapse are configurable. The results of the circuits are validated by running the same experiments on the Brian simulator. The circuits, which are totally original and independent of the technology, use only 136 slice registers of hardware resources. Thus, these designs allow the scale of the network. These circuits aim to be the basis of the spiking neural networks on digital devices. This platform allows the user to first simulate their network within the Brian simulator and then, confidently, move to the hardware platform replicating the same performance or even replace their analog platform with the digital one.",science
10.1016/j.ifacol.2021.04.197,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Cognitive Artificial Population System: Framework and Application,https://api.elsevier.com/content/abstract/scopus_id/85107879835,"Agent-based social simulation has been comprehensively applied in the research of social and ecological systems. At its core is an artificial population, which endogenously drives the system evolution for particular applications, such as urban transportation, reginal economics, analysis of infectious disease transmission, and military simulation. In contrast with the previous population simulations where simple mathematical models are used to ‘reproduce’ actual demographic features, this paper proposes a self-evolutionary digital population system, named as Cognitive Artificial Population System (CAPS). At a more fine-grained level, CAPS focuses on the agent cognitive, reasoning and learning process in their surrounding environment, thus can exploit most advantages from cognitive computing and Artificial Intelligence. As a case study, Chinese population evolution is implemented using the proposed framework. Computational experiments indicate that CAPS is able to achieve good predicted population structures for real social systems.",science
10.1016/j.procir.2020.07.006,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Operator support in human-robot collaborative environments using AI enhanced wearable devices,https://api.elsevier.com/content/abstract/scopus_id/85100836551,"Nowadays, in order to cover the needs of market for product mass customization, industries have started to move to hybrid production cells, involving both robots and human operators. Research has been done during previous years to promote and improve the collaboration between humans and robots, trying to address topics such as safety, awareness and cognitive support in form of Augmented Reality based instructions. Results of previous research show bottlenecks related to the way of interaction of the operators with such supportive systems though. Direct interaction approach with the use of push buttons or indirect-gesture based interaction, which are most often adopted by the researchers, require operators to constantly occupy their hands performing the relevant button presses or gestures. Moreover, previous approaches are hardware dependent and need a lot of customization to work with different hardware. This work tries to address these bottlenecks proposing the usage of wearable devices enhanced with AI in order to support the interaction of human operators with robots in human-robot collaborative environments in a seamless and non-intrusive way, wrapped around a framework called “Operator Support Module” (OSM). Among others, OSM supports a variety of hardware to easily fit in various industrial scenarios. Two case studies will be presented to demonstrate the approach.",science
10.1016/j.promfg.2020.11.012,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Application of machine learning and vision for real-time condition monitoring and acceleration of product development cycles,https://api.elsevier.com/content/abstract/scopus_id/85100766330,"Development work within an experimental environment, in which certain properties are investigated and optimized, requires many test runs and is therefore often associated with long execution times, costs and risks. This can affect product, material and technology development in industry and research. New digital driver technologies offer the possibility to automate complex manual work steps in a cost-effective way, to increase the relevance of the results and to accelerate the processes many times over. In this context, this article presents a low-cost, modular and open-source machine vision system for test execution and evaluates it on the basis of a real industrial application. For this purpose a methodology for the automated execution of the load intervals, the process documentation and for the evaluation of the generated data by means of machine learning to classify wear levels. The software and the mechanical structure are designed to be adaptable to different conditions, components and for a variety of tasks in industry and research. The mechanical structure is required for tracking the test object and represents a motion platform with independent positioning by machine vision operators or machine learning. An evaluation of the state of the test object is performed by the transfer learning after the initial documentation run. The manual procedure for classifying the visually recorded data on the state of the test object is described for the training material. This leads to an increased resource efficiency on the material as well as on the personnel side since on the one hand the significance of the tests performed is increased by the continuous documentation and on the other hand the responsible experts can be assigned time efficiently. The presence and know-how of the experts are therefore only required for defined and decisive events during the execution of the experiments. Furthermore, the generated data are suitable for later use as an additional source of data for predictive maintenance of the developed object.",science
10.1016/j.promfg.2020.05.123,Conference Proceeding,,scopus,2020-01-01,sciencedirect,Integrated tool condition monitoring systems and their applications: A comprehensive review,https://api.elsevier.com/content/abstract/scopus_id/85095576577,"In conventional metal cutting, different tool wear modes, and their individual deterioration rates play vital roles in overall production performance. For a given tool (i.e., geometry or materials), many shop floors still follow a standard rule by pre-setting a tool life, which is conservative but not realistic. Premature failure of a tool can cause unexpected machine downtime and material losses, while another tool could serve beyond that pre-set life. As a result, optimized tool life and productivity cannot be achieved. Moreover, nowadays, there is an increased demand of process monitoring and optimization on the unmanned and the semi-automated shop floors.
                  Tool condition monitoring (TCM) systems for process improvement and optimization have been in research for several decades. Both offline and online TCM systems are invented and discussed. A wide range of original publications are reported focusing on different sub-topics, e.g., specific machining process-based TCM methods, measurement or signal acquisition methods, processing methods, and classifiers. With the recent evolution of smart sensors in the era of Industry 4.0, development of online TCM systems received much attention to the researchers. Accordingly, research on some sub-topics also gets motivated into different directions, such as, feasibility of power or current sensors, machine vision technique, and combination of multi-sensors. Thus, from the industrial viewpoint, the current state of implementation of the proposed TCM systems for (near) real-time process monitoring and control needs to be clear. This paper presents the state-of-the-art of the TCM systems covering three major machining operations, discusses their application feasibility in industry environments, and states some current TCMS implementations. Challenges being faced by the industry are concluded, along with direction and suggestions for future researches.",science
10.1016/j.jksuci.2020.09.013,Journal,Journal of King Saud University - Computer and Information Sciences,scopus,2020-01-01,sciencedirect,Affect detection from arabic tweets using ensemble and deep learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85095572698,"Affect detection from text has captured the attention of researchers recently. This is due to the rapid use of social media sites (e.g. Twitter, Facebook), which allows users to express their feelings, emotions, and thoughts in textual format. Analyzing emotion-rich textual data of social networks has many real-life applications. The context of an emotional text can be measured by analyzing certain features of this rich source of emotional information. Classifying text into emotional labels/intensities is considered a difficult problem. This paper resolves one of the state-of-the-art NLP research emotion and intensity detection tasks using Deep Learning and ensemble implementations. In this paper, we developed several innovative approaches; (a) bidirectional GRU_CNN (BiGRU_CNN), (b) conventional neural networks (CNN), and (c) XGBoost regressor (XGB). The ensemble of BiGRU_CNN, CNN, and XGB is used to solve an emotion intensity (EI-reg) task of the SemEval-2018 Task1 (Affect in Tweets). Our proposed ensemble approach was evaluated using a reference dataset of the SemEval-2018 Task1. Results show that our approach is well above the baseline for this task. It also achieved a Pearson of (69.2%), with an enhancement of 0.7% in comparison with previous best performing models.",science
10.1016/j.procs.2020.04.220,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Design and Fabrication of SHRALA: Social Humanoid Robot Based on Autonomous Learning Algorithm,https://api.elsevier.com/content/abstract/scopus_id/85086626621,"This paper presents the preliminary research work in the Design, Fabrication of a Social Humanoid Robot based on Autonomous Learning Algorithm (SHRALA). Virtual Model of the humanoid robot was developed using Solidworks environment. This model is then fabricated using Creality Ender-3 3D printer. The electronic control circuit was designed and interfaced to computer using ATMEGA 2650 controller board, based on 8-bit AVR microcontroller. In order to easily and efficiently control the SHRALA a Graphical User Interface (GUI) was created using Unity3D editor, where a simple USB joystick was used to actuate the motions of the SHRALA in the virtual environment. The fabricated SHRALA was controlled in real time using a serial communication interface created between the GUI and Arduino Mega 2650 board. The humanoid robot was successfully controlled using the GUI environment and the preliminary results are satisfactory as it is performing the task as per the desired instructions. This research work is a part of the real time humanoid robot development project “SHRALA”, In near future autonomous learning algorithm will also be implemented in the robot and the same will be published as research article in a modular approach.",science
10.1016/j.csbj.2020.05.022,Journal,Computational and Structural Biotechnology Journal,scopus,2020-01-01,sciencedirect,Software tools for 3D nuclei segmentation and quantitative analysis in multicellular aggregates,https://api.elsevier.com/content/abstract/scopus_id/85086379946,"Today, we are fully immersed into the era of 3D biology. It has been extensively demonstrated that 3D models: (a) better mimic the physiology of human tissues; (b) can effectively replace animal models; (c) often provide more reliable results than 2D ones. Accordingly, anti-cancer drug screenings and toxicology studies based on multicellular 3D biological models, the so-called “-oids” (e.g. spheroids, tumoroids, organoids), are blooming in the literature. However, the complex nature of these systems limit the manual quantitative analyses of single cells’ behaviour in the culture. Accordingly, the demand for advanced software tools that are able to perform phenotypic analysis is fundamental. In this work, we describe the freely accessible tools that are currently available for biologists and researchers interested in analysing the effects of drugs/treatments on 3D multicellular -oids at a single-cell resolution level. In addition, using publicly available nuclear stained datasets we quantitatively compare the segmentation performance of 9 specific tools.",science
10.1016/bs.pmbts.2020.04.003,Book Series,Progress in Molecular Biology and Translational Science,scopus,2020-01-01,sciencedirect,Correlation and association analyses in microbiome study integrating multiomics in health and disease,https://api.elsevier.com/content/abstract/scopus_id/85085165614,"Correlation and association analyses are one of the most widely used statistical methods in research fields, including microbiome and integrative multiomics studies. Correlation and association have two implications: dependence and co-occurrence. Microbiome data are structured as phylogenetic tree and have several unique characteristics, including high dimensionality, compositionality, sparsity with excess zeros, and heterogeneity. These unique characteristics cause several statistical issues when analyzing microbiome data and integrating multiomics data, such as large p and small n, dependency, overdispersion, and zero-inflation. In microbiome research, on the one hand, classic correlation and association methods are still applied in real studies and used for the development of new methods; on the other hand, new methods have been developed to target statistical issues arising from unique characteristics of microbiome data. Here, we first provide a comprehensive view of classic and newly developed univariate correlation and association-based methods. We discuss the appropriateness and limitations of using classic methods and demonstrate how the newly developed methods mitigate the issues of microbiome data. Second, we emphasize that concepts of correlation and association analyses have been shifted by introducing network analysis, microbe-metabolite interactions, functional analysis, etc. Third, we introduce multivariate correlation and association-based methods, which are organized by the categories of exploratory, interpretive, and discriminatory analyses and classification methods. Fourth, we focus on the hypothesis testing of univariate and multivariate regression-based association methods, including alpha and beta diversities-based, count-based, and relative abundance (or compositional)-based association analyses. We demonstrate the characteristics and limitations of each approaches. Fifth, we introduce two specific microbiome-based methods: phylogenetic tree-based association analysis and testing for survival outcomes. Sixth, we provide an overall view of longitudinal methods in analysis of microbiome and omics data, which cover standard, static, regression-based time series methods, principal trend analysis, and newly developed univariate overdispersed and zero-inflated as well as multivariate distance/kernel-based longitudinal models. Finally, we comment on current association analysis and future direction of association analysis in microbiome and multiomics studies.",science
10.1016/j.procs.2020.02.122,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Theoretical and hypothetical pathways to real-time neuromorphic AGI/post-AGI ecosystems,https://api.elsevier.com/content/abstract/scopus_id/85084485262,"While Homo sapiens is without doubt our planet’s most advanced species capable of imagining, creating and implementing tools, one of the many observable trends in evolution is the accelerating merger of biology and technology at increasing levels of scale. This is not surprising, given that our technology can be seen from a perspective in which the sensorimotor and, subsequently, prefrontal areas of our brain increasingly extending its motor (as did our evolutionary predecessors), perceptual, and—with computational advances, cognitive and memory capacities—into the exogenous environment. As such, this trajectory has taken us to a point in the above-mentioned merger at which the brain itself is beginning to meld with its physically expressed hardware and software counterparts—functionally at first, but increasingly structurally as well, initially by way of neural prostheses and brain-machine interfaces. Envisioning the extension of this trend, I propose theoretical technological pathways to a point at which humans and non-biological human counterparts may have the option to have identical neural substrates that—when integrated with Artificial General Intelligence (AGI), counterfactual quantum communications and computation, and AGI ecosystems—provide a global advance in shared knowledge and cognitive function while ameliorating current concerns associated with advanced AGI, as well as suggesting (and, if realized, accelerating) the far-future emergence of Transentity Universal Intelligence (TUI).",science
10.1016/j.softx.2020.100426,Journal,SoftwareX,scopus,2020-01-01,sciencedirect,Connecting the CoppeliaSim robotics simulator to virtual reality,https://api.elsevier.com/content/abstract/scopus_id/85080071825,"The CoppeliaSim VR Toolbox provides a set of tools to experience CoppeliaSim robot simulation software in Virtual Reality and to return user interactions. Its primary focus is to create a platform that enables the fast prototyping and verification of robotic systems. Moreover, the generality of the toolbox ensures that it can be valuable in other contexts like robotics education, human–robot interaction or reinforcement learning. The software is designed to have a low entry threshold for moderately complex use cases, but can be extended to perform very complex visualizations for more experienced users.",science
10.1016/j.neunet.2019.07.020,Journal,Neural Networks,scopus,2020-01-01,sciencedirect,Deep neural network and data augmentation methodology for off-axis iris segmentation in wearable headsets,https://api.elsevier.com/content/abstract/scopus_id/85072296970,"A data augmentation methodology is presented and applied to generate a large dataset of off-axis iris regions and train a low-complexity deep neural network. Although of low complexity the resulting network achieves a high level of accuracy in iris region segmentation for challenging off-axis eye-patches. Interestingly, this network is also shown to achieve high levels of performance for regular, frontal, segmentation of iris regions, comparing favourably with state-of-the-art techniques of significantly higher complexity. Due to its lower complexity this network is well suited for deployment in embedded applications such as augmented and mixed reality headsets.",science
10.1016/j.knosys.2019.07.014,Journal,Knowledge-Based Systems,scopus,2020-01-01,sciencedirect,A new decision support system for knowledge management in archaeological activities”,https://api.elsevier.com/content/abstract/scopus_id/85068965128,"The use of Information Technologies (IT) has today become an added value for appropriate decision making. This has contributed to improving the companies’ strategies in the market. However, the full potential of these technologies in the relevant field of Archaeology has yet to be fully exploited. To contribute to reducing this gap, this paper presents a new and original design of a Process Maturity Framework for archaeological knowledge and data management which may be applied for high-level timely decision making, supported by an ‘IT Governance’ reference frame, in order to improve the quality and efficiency of the services provided by the Diagnostic, Prospecting, Monitoring and Excavation processes of the Preemptive Archaeology Program.
                  This new Process Maturity Model (PMM) takes the processes which are currently established in each phase of archaeological projects as its reference to improve information analysis, reports generation and support decision-making processes, as well as to manage and control the materials and context found in the field. This is achieved by emphasizing the use of the information required for future queries and projections, ensuring its’ quality and integrity in order to generate reports more efficiently, whilst also allowing a more agile and timely decision-making process. Said information has been collected during the field and laboratory processes by analysing the proper application and management of the technology from an ‘IT Governance’ framework in companies which offer archaeological services.
                  The different phases of the implementation of the model designed, based in ITIL, since it is the most holistic of the current benchmarks in Technology Services Management, are shown by means of a hypothetical, yet real, application of the PMM in an Archaeology Consultancy firm. Thus, a set of basic parameters is initially established in order to implement a PMM. Then, a diagnostic on the processes and IT Service Management applied to each archaeological phase is performed. Afterwards, an evaluation of the current maturity level of the processes is carried out and, finally, the continuous improvement plan is described.",science
10.1016/j.jmapro.2019.10.020,Journal,Journal of Manufacturing Processes,scopus,2019-12-01,sciencedirect,Data-driven smart manufacturing: Tool wear monitoring with audio signals and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85074281429,"Tool wear in machining could result in poor surface finish, excessive vibration and energy consumption. Monitoring tool wear in real-time is crucial to improve manufacturing productivity and quality. While numerous sensor-based tool wear monitoring techniques have been demonstrated in laboratory environments, few tool wear monitoring systems have been deployed in factories because it is not realistic to install some of the important sensors such as dynamometers on manufacturing machines. To address this issue, a novel audio signal processing approach is introduced. This technique does not require expensive sensors but audio sensors only. A blind source separation method is used to separate source signals from noise. An extended principal component analysis is used for dimensionality reduction. Real-time multi-channel audio signals are collected during a set of milling tests under varying cutting conditions. The experimental data are used to develop and validate a predictive model. Experimental results have shown that the predictive model is capable of classifying tool wear conditions with high accuracy.",science
10.1016/j.cmpb.2019.105056,Journal,Computer Methods and Programs in Biomedicine,scopus,2019-12-01,sciencedirect,An iterative finite element-based method for solving inverse problems in traction force microscopy,https://api.elsevier.com/content/abstract/scopus_id/85072277991,"Background and Objective
                  During the last years different model solutions were proposed for solving cell forces under different conditions. The solution relies on a deformation field that is obtained under cell relaxation with a chemical cocktail. Once the deformation field of the matrix is determined, cell forces can be computed by an inverse algorithm, given the mechanical properties of the matrix. Most of the Traction Force Microscopy (TFM) methods presented so far relied on a linear stress-strain response of the matrix. However, the mechanical response of some biopolymer networks, such as collagen gels is more complex. In this work, we present a numerical method for solving cell forces on non-linear materials.
               
                  Methods
                  The proposed method relies on solving the inverse problem based on an iterative optimization. The objective function is defined by least-square minimization of the difference between the target and the current computed deformed configuration of the cell, and the iterative formulation is based on the solution of several direct mechanical problems. The model presents a well-posed discretized inverse elasticity problem in the absence of regularization. The algorithm can be easily implemented in any kind of Finite Element (FE) code as a sequence of different standard FE analysis.
               
                  Results
                  To illustrate the proposed iterative formulation we apply the theoretical model to some illustrative examples by using real experimental data of Normal Human Dermal Fibroblast cells (NHDF) migrating inside a 2 mg/ml collagen-based gel. Different examples of application have been simulated to test the inverse numerical model proposed and to investigate the effect of introducing the correct cell properties onto the obtained cell forces. The algorithm converges after a small number of iterations, generating errors of around 5% for the tractions field in the cell contour domain. The resulting maximum traction values increased by 11% as a consequence of doubling the mechanical properties of the cell domain.
               
                  Conclusions
                  With the results generated from computations we demonstrate the application of the algorithm and explain how the mechanical properties of both, the cell and the gel, domains are important for arriving to the correct results when using inverse traction force reconstruction algorithms, however, have only a minor effect on the resulting traction values.",science
10.1016/j.ssci.2019.06.025,Journal,Safety Science,scopus,2019-12-01,sciencedirect,Securing instant messaging based on blockchain with machine learning,https://api.elsevier.com/content/abstract/scopus_id/85067872085,"Instant Messaging (IM) offers real-time communications between two or more participants on Internet. Nowadays, most IMs take place on mobile applications, such as WhatsApp, WeChat, Viber and Facebook Messenger, which have more users than social networks, such as Twitter and Facebook. Among the applications of IMs, online shopping has become a part of our everyday life, primarily those who are busiest. However, transaction disputes are often occurred online shopping. Since most IMs are centralized and message history is not stored in the center, the messaging between users and owners of online shops are not reliable and traceable. In China, online shopping sales have soared from practically zero in 2003 to nearly 600 hundred million dollars last year, and now top those in the United States. It is very crucial to secure the instant messaging in online shopping in China. We present techniques to exploit blockchain and machine learning algorithms to secure instant messaging. Since the cryptography of Chinese national standard is encouraged to adopt in security applications of China, we propose a blockchain-based IM scheme with the Chinese cryptographic bases. First, we design a message authentication model based on SM2 to avoid the counterfeit attack and replay attack. Second, we design a cryptographic hash mode based on SM3 to verify the integrity of message. Third, we design a message encryption model based on SM4 to protect the privacy of users. Besides, we propose a method based on machine learning algorithms to monitor the activity on blockchain to detect anomaly. To prove and verify the blockchain-based IM scheme, a blockchain-based IM system has been designed on Linux platforms. The implementation result shows that it is a practical and secure IM system, which can be applied to a variety of instant messaging applications directly.",science
10.1016/j.patrec.2018.04.009,Journal,Pattern Recognition Letters,scopus,2019-12-01,sciencedirect,A real-time and unsupervised face re-identification system for human-robot interaction,https://api.elsevier.com/content/abstract/scopus_id/85046146958,"In the context of Human-Robot Interaction (HRI), face Re-Identification (face Re-ID) aims to verify if certain detected faces have already been observed by robots. The ability of distinguishing between different users is crucial in social robots as it will enable the robot to tailor the interaction strategy toward the users’ individual preferences. So far face recognition research has achieved great success, however little attention has been paid to the realistic applications of Face Re-ID in social robots. In this paper, we present an effective and unsupervised face Re-ID system which simultaneously re-identifies multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural Networks to extract features, and an online clustering algorithm to determine the face's ID. Its performance is evaluated on two datasets: the TERESA video dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF Dataset). We demonstrate that the optimised combination of techniques achieves an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on YTF dataset. We have implemented the proposed method into a software module in the HCI^2 Framework [1] for it to be further integrated into the TERESA robot [2], and has achieved real-time performance at 10–26 Frames per second.",science
10.1016/j.eswa.2019.05.052,Journal,Expert Systems with Applications,scopus,2019-11-30,sciencedirect,Unsupervised collective-based framework for dynamic retraining of supervised real-time spam tweets detection model,https://api.elsevier.com/content/abstract/scopus_id/85067174995,"Twitter is one of the most popular social platforms. It has changed the way of communication and information dissemination through its real-time messaging mechanism. Recently, it has been used by researchers and industries as a new source of data for various intelligent systems, such as tweet sentiment analysis and recommendation systems, which require high data quality. However, due to its flexibility and popularity, Twitter has become the main target for spamming activities such as phishing legitimate users or spreading malicious software, which introduces new security issues and waste resources. Therefore, researchers have developed various machine-learning algorithms to reveal Twitter spam. However, as spammers have become smarter and more crafty, the characteristics of the spam tweets are varying over time making these methods inefficient to detect new spammers tricks and strategies. In addition, some of the employed methods (e.g. blacklisting) or spammer features (e.g. graph-based features) are extremely time-consuming, which hinders the ability to detect spammer activities in real-time. In this paper, we introduce a framework to deal with the volatility of the spam contents and new spamming patterns, called the spam drift. The framework combines the strength of unsupervised machine learning approach, which learns from unlabeled tweets, to retrain a real-time supervised tweet-level spam detection model in a batch mode. A set of experiments on a large-scale data set show the effectiveness of the proposed online unsupervised method in adaptively discovers and learns the patterns of new spam activities and achieve stable recall values reaching more than 95%. Although the average spam precision of our method is around 60%, the high spam recall values show the ability of our proposed method in reducing spam drift problems compared to traditional machine learning algorithms.",science
10.1016/j.eswa.2019.05.035,Journal,Expert Systems with Applications,scopus,2019-11-15,sciencedirect,Social mimic optimization algorithm and engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85066806914,"Increase in complexity of real world problems has provided an area to explore efficient methods to solve computer science problems. Meta-heuristic methods based on evolutionary computations and swarm intelligence are instances of techniques inspired by nature. This paper presents a novel social mimic optimization (SMO) algorithm inspired by mimicking behavior to solve optimization problems. The proposed algorithm is evaluated using 23 test functions. Obtained results are compared with 14 known optimization algorithms including Whale optimization algorithm (WOA), Grasshopper optimization algorithm (GOA), Particle Swarm Optimization (PSO), Stochastic fractal search (SFS), Grey Wolf Optimizer (GWO), Optics Inspired Optimization (OIO), League Championship Algorithm (LCA), Wind Driven Optimization (WDO), Harmony search (HS), Firefly Algorithm (FA), Artificial Bee Colony (ABC), Biogeography Based Optimization (BBO), Bat Algorithm (BA), and Teaching Learning Based Optimization (TLBO). Obtained results indicate higher capability of the SMO algorithm in solving high-dimensional decision variables. Furthermore, SMO is used to solve two classic engineering design problems. Three important features of SMO are simple implementation, solving optimization problems with minimum population size and not requiring control parameters. Results of various evaluations show superiority of the proposed method in finding the optimal solution with minimum function evaluations. This superiority is achieved based on reducing number of initial population. The proposed method can be applied to applications like automatic evolution of robotics, automatic control of machines and innovation of machines in finding better solutions with less cost.",science
10.1016/j.enbuild.2019.109440,Journal,Energy and Buildings,scopus,2019-11-01,sciencedirect,A deep reinforcement learning-based autonomous ventilation control system for smart indoor air quality management in a subway station,https://api.elsevier.com/content/abstract/scopus_id/85072289855,"Mechanical ventilation has been widely implemented to alleviate poor indoor air quality (IAQ) in confined underground public facilities. However, due to time-varying IAQ properties that are influenced by unpredictable factors, including outdoor air quality, subway schedules, and passenger volumes, real-time control that incorporates a trade-off between energy saving and IAQ is limited in conventional rule-based and model-based approaches. We propose a data-driven and intelligent approach for a smart ventilation control system based on a deep reinforcement learning (DeepRL) algorithm. This study utilized a deep Q-network (DQN) algorithm of DeepRL to design the ventilation system. The DQN agent was trained in a virtual environment defined by a gray-box model to simulate an IAQ system in a subway station. Performance of the proposed method over three weeks was evaluated by a comprehensive indoor air-quality index (CIAI) and energy consumption under different outdoor air quality scenarios. The results show that the proposed DeepRL-based ventilation control system reduced energy consumption by up to 14.4% for the validation dataset time interval and improved IAQ from unhealthy to acceptable.",science
10.1016/j.cie.2019.106031,Journal,Computers and Industrial Engineering,scopus,2019-11-01,sciencedirect,Machine learning based concept drift detection for predictive maintenance,https://api.elsevier.com/content/abstract/scopus_id/85071975175,"In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope.",science
10.1016/j.micpro.2019.102853,Journal,Microprocessors and Microsystems,scopus,2019-11-01,sciencedirect,Using Machine Learning for predicting area and Firmware metrics of hardware designs from abstract specifications,https://api.elsevier.com/content/abstract/scopus_id/85070295298,"Advancements of Machine Learning (ML) in the field of computer vision have paved the way for its potential application in many other fields. Researchers and Hardware domain experts are exploring possible applications of Machine Learning in optimizing many aspects of the Hardware development process.
                  In this paper, we propose a novel approach for predicting area and multiple Firmware metrics of Hardware components from specifications. The flow uses an existing RTL generation framework for generating valid data samples that enable ML algorithms to train the learning models. The approach has been successfully employed to predict the area and Firmware measurements of real-life Hardware components such as Control and Status Register (CSR) interfaces, that are ubiquitous in embedded systems. With our method we are able to perform an estimation on the area of an Hardware component with more than 98% accuracy and 600x faster than the existing methods. In addition, we are able to rank the features according to their importance in final area estimations. Finally, we are as well able to predict with an accuracy of approx. 85% the size and the CPU running cycles of a Firmware program embedded on the same Hardware component. This method, as a whole, is an important approach towards an accurate and fast estimation in the context of Hardware/Software trade-off analysis.",science
10.1016/j.ins.2019.07.019,Journal,Information Sciences,scopus,2019-11-01,sciencedirect,Labeled graph sketches: Keeping up with real-time graph streams,https://api.elsevier.com/content/abstract/scopus_id/85068588796,"Currently, graphs serve as fundamental data structures for many applications, such as road networks, social and communication networks, and web requests. In many applications, graph edges stream in and users are only interested in the recent data. In data exploration, the storage and processing of such massive amounts of graph stream data has become a significant problem. As the categorical attributes of vertices and edges are often referred to as labels, we propose a labeled graph sketch that stores real-time graph structural information using only sublinear space and that supports graph queries of diverse types. This sketch also works for sliding-window queries. We conduct extensive experiments on real-world datasets in six different domains and compare the results with a state-of-the-art method to show the accuracy, efficiency, and practicability of our proposed approach.",science
10.1016/j.ress.2019.106555,Journal,Reliability Engineering and System Safety,scopus,2019-11-01,sciencedirect,A cognitive architecture safety design for safety critical systems,https://api.elsevier.com/content/abstract/scopus_id/85068360978,"This research is presented as a safety analysis of a cognitive architecture with an intelligent decision support model (IDSM) that is embedded into an autonomous non-deterministic safety-critical system.
                  Cognitive technology is currently simulated within safety-critical systems in order to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves the system's cognitive performance. In this research, the safety of the architecture was analyzed on an actual safety-critical system, an unmanned surface vehicle (USV). The safety analysis was conducted in both a simulated and a real world nautical based environment. The objective was to define the safety design of a cognitive architecture. The input to the safety design was provided through an approach that identified and mitigated hazards associated with a USV controlled by a cognitive architecture. This analysis provided a structured, task-oriented approach for the dissemination of information concerning safety requirements. This approach was necessary to achieve a safe execution of the USV's capabilities through a design that reduces the potential for injury to personnel and damage to equipment.
                  Other real time applications that would benefit from advancing the safety of cognitive technologies are unmanned platforms, transportation technologies, and service robotics. The results will provide cognitive science researchers with a reference for safety engineering of artificially intelligent safety-critical systems.",science
10.1016/j.neucom.2019.07.015,Journal,Neurocomputing,scopus,2019-10-14,sciencedirect,Merging visual features and temporal dynamics in sequential recommendation,https://api.elsevier.com/content/abstract/scopus_id/85069687170,"With the development of social networking and mobile computing technologies, data analysis in the fashion field has increasingly focused on visual features. The main features currently used in the recommendation methods include non-visual user attributes, item attributes, explicit ratings, and implicit feedbacks. How to understand visual features and integrate them with non-visual features becomes the key to building a good recommender system. In this paper, we consider both non-visual text data and visual image data and their time dynamics to build a large-scale recommender system. An advanced visual Bayesian personalized ranking (aVBPR) model is proposed, which integrates three models. Factorized personalized Markov chains (FPMC) model is used to simulate users’ sequence behaviors, intelligent field-aware factorization machine (iFFM) model also put forward by us is used to predict users’ preferences based on non-visual features, and visual Bayesian personalized ranking (VBPR) model is used to analyze users’ visual preferences. We design a learning algorithm based on AdaGrad method to optimize model aVBPR. The high complexity of the model does not affect the performance of the system by adopting multi-thread technology in the implementation of the learning algorithm. Experimental results of two real-world datasets Women's and Men's Clothing & Accessories from Amazon demonstrate that our model can obtain better recommendation results than the recent popular models for Amazon datasets. Although the model is complicated, multi-thread technique can be used to greatly improve the speed of the implementation.",science
10.1016/j.bios.2019.111549,Journal,Biosensors and Bioelectronics,scopus,2019-10-01,sciencedirect,Efficient electron-mediated electrochemical biosensor of gold wire for the rapid detection of C-reactive protein: A predictive strategy for heart failure,https://api.elsevier.com/content/abstract/scopus_id/85071785022,"C-reactive protein (CRP) is considered a promising biomarker for the rapid and high-throughput real-time monitoring of cardiovascular disease and inflammation in unprocessed clinical samples. Implementation of this monitoring would enable various transformative biomedical applications. We have fabricated a highly specific sensor chip to detect CRP with a detection limit of 2.25 fg/mL. The protein was immobilized on top of a gold (Au) wire/polycarbonate (PC) substrate using 1-ethyl-3-(3-dimethylamino-propyl) carbodiimide hydrochloride/N-hydroxy succinimide-activated 3-mercaptoproponic acid (MPA) as a self-assembled monolayer agent and bovine serum albumin (BSA) as a blocking agent. In contrast to the bare PC substrate, the CRP/BSA/anti-CRP/MPA/Au substrate exhibited a considerably high electrochemical signal toward CRP. The influence of the experimental parameters on CRP detection was assessed via various analysis methods, and these parameters were then optimized. The linear dynamic range of the CRP was 5–220 fg/mL for voltammetric and impedance analysis. Morever, the strategy exhibited high selectivity against various potential interfering species and was capable of directly probing trace amounts of the target CRP in human serum with excellent selectivity. The analytical assay based on the CRP/BSA/anti-CRP/MPA/Au substrate could be exploited as a potentially useful tool for detecting CRP in clinical samples.",science
10.1016/j.engfracmech.2019.106642,Journal,Engineering Fracture Mechanics,scopus,2019-10-01,sciencedirect,Necking-induced fracture prediction using an artificial neural network trained on virtual test data,https://api.elsevier.com/content/abstract/scopus_id/85071523401,"The imperfection-based necking model by Marciniak and Kuczyński (MK) is frequently used for predicting the onset of localized necking under proportional and non-proportional loading, which can be considered a lower limit for the occurrence of fracture in a vehicle body structure subjected to crash loading. A large number of virtual imperfection lines at different orientation angles have to be analysed simultaneously in order to find the critical imperfection causing necking under arbitrary loading. This, and the continuous computation of a “distance to necking” quantity, representing a crucial output quantity for the simulation engineer, makes the model computationally expensive and limits industrial use in full-scale vehicle crash simulations.
                  In this work, an extended MK model is used for creating a virtual test data base under proportional and non-proportional loading for training of a computationally more efficient simple feed-forward neural network (NN). Both models are implemented in a User Material routine of an explicit crash code, where the predictions of the NN are in good agreement with the predictions of the MK reference model, however at a significantly reduced computational cost. Besides a pure numerical validation study, an experimental validation study has been performed, imposing biaxial tension loading followed by plane strain tension loading until necking using a special punch test apparatus. Whereas MK and NN are in good agreement with the experimental observations, the agreement of classical necking models, applied in conjunction with a linear damage accumulation (forming severity) concept was less accurate.",science
10.1016/j.jacr.2019.06.009,Journal,Journal of the American College of Radiology,scopus,2019-10-01,sciencedirect,Bending the Artificial Intelligence Curve for Radiology: Informatics Tools From ACR and RSNA,https://api.elsevier.com/content/abstract/scopus_id/85071398084,"Artificial intelligence (AI) will reshape radiology over the coming years. The radiology community has a strong history of embracing new technology for positive change, and AI is no exception. As with any new technology, rapid, successful implementation faces several challenges that will require creation and adoption of new integration technology. Use cases important to real-world application of AI are described, including clinical registries, AI research, AI product validation, and computer assistance for radiology reporting. Furthermore, the informatics technologies required for successful implementation of the use cases are described, including open Computer-Assisted Radiologist Decision Support, ACR Assist, ACR Data Science Institute use cases, common data elements (radelement.org), RadLex (radlex.org), LOINC/RSNA RadLex Playbook (loinc.org), and Radiology Report Templates (radreport.org).",science
10.1016/j.compag.2019.104948,Journal,Computers and Electronics in Agriculture,scopus,2019-10-01,sciencedirect,Depthwise separable convolution architectures for plant disease classification,https://api.elsevier.com/content/abstract/scopus_id/85071251967,"Convolutional neural network has a huge partake and is still a dominating tool in the field of computer vision. In this study, we introduce a model with depthwise separable convolution architecture for plant disease detection based on images of leaves. We present two versions of depthwise separable convolution comprising two varieties of building blocks. Training and testing of the models were performed on a subset of publicly available PlantVillage dataset of 82,161 images containing 55 distinct classes of healthy and diseased plants. These depthwise separable convolutions achieved less accuracy and high gain in convergence speed. Several models were trained and tested, of which Reduced MobileNet achieved a classification accuracy of 98.34% with 29 times fewer parameters compared to VGG and 6 times lesser than that of MobileNet. However, MobileNet outperformed existing models with 36.03% accuracy when testing the model on a set of images taken under conditions different from those of the images used for training. Thin models were also introduced, which showed effective trade-off between latency and accuracy. The satisfactory accuracy and small size of this model makes it suitable for real-time crop diagnosis in resource constrained mobile devices.",science
10.1016/j.sna.2019.111561,Journal,"Sensors and Actuators, A: Physical",scopus,2019-10-01,sciencedirect,High-precision smart calibration system for temperature sensors,https://api.elsevier.com/content/abstract/scopus_id/85071100929,"High precision and smart sensors make up an indispensable data entry for the Internet of Things technology. Nonetheless, conventional calibration algorithms mainly implemented on the software, such as least squares, polynomial fitting, and interpolation, exhibit limited calibration accuracy that does not reflect a real-time measurement of the sensors. The problem can be resolved with an MCU-based sensor calibration system proposed herein, which mainly employs particle swarm optimization (PSO)-back propagation (BP) neural network. The system firstly reads sensor data through I2C bus and then uses the BP neural network and PSO algorithm to automatically calibrate these data in real time. Sigmoid activation function was implemented via a piecewise polynomial fitting to create a trade-off between hardware resource and precision. A performance test conducted on temperature sensors showed a maximum error of 0.16 °C within the measurement range of −40–100 °C with three times the standard deviation (3
                        σ
                     ) error of ±0.23 °C and overall linearity of 0.1143% after the calibration system was added as compared to the significantly higher error of ±0.63 °C without the calibration.",science
10.1016/j.engappai.2019.07.008,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-10-01,sciencedirect,A semisupervised autoencoder-based approach for anomaly detection in high performance computing systems,https://api.elsevier.com/content/abstract/scopus_id/85069910646,"High Performance Computing (HPC) systems are complex machines with heterogeneous components that can break or malfunction. Automated anomaly detection in these systems is a challenging and critical task, as HPC systems are expected to work 24/7. The majority of the current state-of-the-art methods dealing with this problem are Machine Learning techniques or statistical models that rely on a supervised approach, namely the detection mechanism is trained to recognize a fixed number of different states (i.e. normal and anomalous conditions).
                  In this paper a novel semi-supervised approach for anomaly detection in supercomputers is proposed, based on a type of neural network called autoencoder. The approach learns the normal state of the supercomputer nodes and after the training phase can be used to discern anomalous conditions from normal behavior; in doing so it relies only on the availability of data characterizing only the normal state of the system. This is different from supervised methods that require data sets with many examples of anomalous states, which are in general very rare and/or hard to obtain.
                  The approach was tested on a real-life High Performance Computing system equipped with a monitoring infrastructure capable to generate large amount of data describing the system state. The proposed approach definitely outperforms the best current techniques for semi-supervised anomaly detection, with an increase in accuracy detection of around 12%. Two different implementations are discussed: one where each supercomputer node has a specific model and one with a single, generalized model for all nodes, in order to explore the trade-off between accuracy and ease of deployment.",science
10.1016/j.talanta.2019.05.089,Journal,Talanta,scopus,2019-10-01,sciencedirect,Combination of LEDs and cognitive modeling to quantify sheep cheese whey in watercourses,https://api.elsevier.com/content/abstract/scopus_id/85066257571,"The concentration of sheep cheese whey (CW) in water obtained from two Spanish reservoirs, two Spanish rivers, and distilled water has been estimated by combining spectroscopic measurements, obtained with light-emitting diodes (LEDs), and linear or non-linear algorithms. The concentration range of CW that has been studied covers from 0 to 25% in weight. Every sample was measured by six different types of LEDs possessing different emission wavelengths (blue, orange, green, pink, white, and UV). 1,800 fluorescence measurements were carried out and used to design different types of models to estimate the concentration of CW in water. The fluorescence spectra provided by the pink LED originated the most accurate mathematical models, with mean square errors lower than 3.3% and 2.5% for the linear and non-linear approaches, respectively. The pink LED combined with the non-linear model, which was an artificial neural network, was further validated through a k-fold cross-validation and an internal validation. It should be noted that the sensor used here has been designed and produced by a 3D printer and has the potential of being implemented in situ for real-time and cost-effective analysis of natural watercourses.",science
10.1016/j.eswa.2019.04.035,Journal,Expert Systems with Applications,scopus,2019-10-01,sciencedirect,Classifying Periodic Astrophysical Phenomena from non-survey optimized variable-cadence observational data,https://api.elsevier.com/content/abstract/scopus_id/85064934432,"Modern time-domain astronomy is capable of collecting a staggeringly large amount of data on millions of objects in real time. Therefore, the production of methods and systems for the automated classification of time-domain astronomical objects is of great importance. The Liverpool Telescope has a number of wide-field image gathering instruments mounted upon its structure, the Small Telescopes Installed at the Liverpool Telescope. These instruments have been in operation since March 2009 gathering data of large areas of sky around the current field of view of the main telescope generating a large dataset containing millions of light sources. The instruments are inexpensive to run as they do not require a separate telescope to operate but this style of surveying the sky introduces structured artifacts into our data due to the variable cadence at which sky fields are resampled. These artifacts can make light sources appear variable and must be addressed in any processing method.
                  The data from large sky surveys can lead to the discovery of interesting new variable objects. Efficient software and analysis tools are required to rapidly determine which potentially variable objects are worthy of further telescope time. Machine learning offers a solution to the quick detection of variability by characterising the detected signals relative to previously seen exemplars. In this paper, we introduce a processing system designed for use with the Liverpool Telescope identifying potentially interesting objects through the application of a novel representation learning approach to data collected automatically from the wide-field instruments. Our method automatically produces a set of classification features by applying Principal Component Analysis on set of variable light curves using a piecewise polynomial fitted via a genetic algorithm applied to the epoch-folded data. The epoch-folding requires the selection of a candidate period for variable light curves identified using a genetic algorithm period estimation method specifically developed for this dataset. A Random Forest classifier is then used to classify the learned features to determine if a light curve is generated by an object of interest. This system allows for the telescope to automatically identify new targets through passive observations which do not affect day-to-day operations as the unique artifacts resulting from such a survey method are incorporated into the methods.
                  We demonstrate the power of this feature extraction method compared to feature engineering performed by previous studies by training classification models on 859 light curves of 12 known variable star classes from our dataset. We show that our new features produce a model with a superior mean cross-validation F1 score of 0.4729 with a standard deviation of 0.0931 compared with the engineered features at 0.3902 with a standard deviation of 0.0619. We show that the features extracted from the representation learning are given relatively high importance in the final classification model. Additionally, we compare engineered features computed on the interpolated polynomial fits and show that they produce more reliable distributions than those fit to the raw light curve when the period estimation is correct.",science
10.1016/j.neucom.2019.05.064,Journal,Neurocomputing,scopus,2019-09-17,sciencedirect,Improving novelty detection with generative adversarial networks on hand gesture data,https://api.elsevier.com/content/abstract/scopus_id/85066314268,"We propose a novel way of solving the issue of classification of out-of-vocabulary gestures using Artificial Neural Networks (ANNs) trained in the Generative Adversarial Network (GAN) framework. A generative model augments the data set in an online fashion with new samples and stochastic target vectors, while a discriminative model determines the class of the samples. The approach was evaluated on the UC2017 SG and UC2018 DualMyo data sets. The generative models’ performance was measured with a distance metric between generated and real samples. The discriminative models were evaluated by their accuracy on trained and novel classes. In terms of sample generation quality, the GAN is significantly better than a random distribution (noise) in mean distance, for all classes. In the classification tests, the baseline neural network was not capable of identifying untrained gestures. When the proposed methodology was implemented, we found that there is a trade-off between the detection of trained and untrained gestures, with some trained samples being mistaken as novelty. Nevertheless, a novelty detection accuracy of 95.4% or 90.2% (depending on the data set) was achieved with just 5% loss of accuracy on trained classes.",science
10.1016/j.ifacol.2019.11.102,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Sustainable operations management for industry 4.0 and its social return,https://api.elsevier.com/content/abstract/scopus_id/85078948022,"In today’s industrial environment, where concepts of smart factories are consolidating their application in companies, it is still necessary to approach management decision making from a perspective that encompasses all aspects of sustainability without losing sight of the social return to which they must contribute. In order to obtain a reliable prediction, of the operation of a Sustainable Manufacturing System (SMS) and its Social Return (SR), this paper develops a methodology and procedures that allow predicting the system performance as a whole. This will allow us to assist management decision making in industries 4.0, supported by multi-criteria methods in knowledge management, simulation, value analysis and operational research by means of:
                  a) Study the economic, social and environmental impacts in the organization and management of the efficient operation of an SMS with the selection of strategies and alternatives in production chains to minimize and / or mitigate environmental and labor risks.
                  b) Encourage of industrial symbiosis or eco-industries networks that create opportunities increasing eco-efficiency and the positive social return of production systems.
                  This proposed methodology will facilitate changes in the structure of production systems in order to implement industry 4.0 paradigms through facilitator technologies such as simulation and virtual reality. This framework will allow Small and Medium Enterprises (SMEs) and other companies to address the decision-making activities that improve the economic-functional efficiency, which will lead to reduce the environmental impact and increase the positive social return of certain production strategies, considering working conditions.
                  The proposed approach went validated, in the area of the Euroregion Galicia North of Portugal, to favour the implementation of the decision-making through the Industry 4.0 Technologies.",science
10.1016/j.ifacol.2019.11.172,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Machine learning framework for predictive maintenance in milling,https://api.elsevier.com/content/abstract/scopus_id/85078904429,"In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process.",science
10.1016/j.ifacol.2019.11.385,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Towards a data-driven predictive-reactive production scheduling approach based on inventory availability,https://api.elsevier.com/content/abstract/scopus_id/85078884096,"To survive in a competitive business environment, manufacturing systems require the proper deployment of advanced technologies coming from Industry 4.0. These technologies allow access to quasi-real-time data that provide a continuously updated picture of the production system, including the state of available inventory. Data-driven predictive-reactive production scheduling has the potential to support the anticipation and prompt reaction to overcome different kinds of disruptions that occur in production execution nowadays. This research paper aims to propose a conceptual model for a data-driven predictive-reactive production scheduling approach combining machine learning and simulation-based optimization, considering current inventory of raw material, work in process and final products inventory to characterize a job-shop production execution state. The approach supports decision-making in dynamic situations related to inventory availability that can affect production schedules.",science
10.1016/j.tips.2019.07.005,Journal,Trends in Pharmacological Sciences,scopus,2019-09-01,sciencedirect,Artificial Intelligence for Drug Toxicity and Safety,https://api.elsevier.com/content/abstract/scopus_id/85071055144,"Interventional pharmacology is one of medicine’s most potent weapons against disease. These drugs, however, can result in damaging side effects and must be closely monitored. Pharmacovigilance is the field of science that monitors, detects, and prevents adverse drug reactions (ADRs). Safety efforts begin during the development process, using in vivo and in vitro studies, continue through clinical trials, and extend to postmarketing surveillance of ADRs in real-world populations. Future toxicity and safety challenges, including increased polypharmacy and patient diversity, stress the limits of these traditional tools. Massive amounts of newly available data present an opportunity for using artificial intelligence (AI) and machine learning to improve drug safety science. Here, we explore recent advances as applied to preclinical drug safety and postmarketing surveillance with a specific focus on machine and deep learning (DL) approaches.",science
10.1016/j.clineuro.2019.105442,Journal,Clinical Neurology and Neurosurgery,scopus,2019-09-01,sciencedirect,Artificial intelligence for assisting diagnostics and assessment of Parkinson's disease—A review,https://api.elsevier.com/content/abstract/scopus_id/85069629950,"Artificial intelligence, specifically machine learning, has found numerous applications in computer-aided diagnostics, monitoring and management of neurodegenerative movement disorders of parkinsonian type. These tasks are not trivial due to high inter-subject variability and similarity of clinical presentations of different neurodegenerative disorders in the early stages. This paper aims to give a comprehensive, high-level overview of applications of artificial intelligence through machine learning algorithms in kinematic analysis of movement disorders, specifically Parkinson’s disease (PD). We surveyed papers published between January 2007 and January 2019, within online databases, including PubMed and Science Direct, with a focus on the most recently published studies. The search encompassed papers dealing with the implementation of machine learning algorithms for diagnosis and assessment of PD using data describing motion of upper and lower extremities. This systematic review presents an overview of 48 relevant studies published in the abovementioned period, which investigate the use of artificial intelligence for diagnostics, therapy assessment and progress prediction in PD based on body kinematics. Different machine learning algorithms showed promising results, particularly for early PD diagnostics. The investigated publications demonstrated the potentials of collecting data from affordable and globally available devices. However, to fully exploit artificial intelligence technologies in the future, more widespread collaboration is advised among medical institutions, clinicians and researchers, to facilitate aligning of data collection protocols, sharing and merging of data sets.",science
10.1016/j.cmpb.2019.06.029,Journal,Computer Methods and Programs in Biomedicine,scopus,2019-09-01,sciencedirect,C-HMOSHSSA: Gene selection for cancer classification using multi-objective meta-heuristic and machine learning methods,https://api.elsevier.com/content/abstract/scopus_id/85068374888,"Background and objective: Over the last two decades, DNA microarray technology has emerged as a powerful tool for early cancer detection and prevention. It helps to provide a detailed overview of disease complex microenvironment. Moreover, online availability of thousands of gene expression assays made microarray data classification an active research area. A common goal is to find a minimum subset of genes and maximizing the classification accuracy.
                  
                     Methods: In pursuit of a similar objective, we have proposed framework (C-HMOSHSSA) for gene selection using multi-objective spotted hyena optimizer (MOSHO) and salp swarm algorithm (SSA). The real-life optimization problems with more than one objective usually face the challenge to maintain convergence and diversity. Salp Swarm Algorithm (SSA) maintains diversity but, suffers from the overhead of maintaining the necessary information. On the other hand, the calculation of MOSHO requires low computational efforts hence is used for maintaining the necessary information. Therefore, the proposed algorithm is a hybrid algorithm that utilizes the features of both SSA and MOSHO to facilitate its exploration and exploitation capability.
                  
                     Results:Four different classifiers are trained on seven high-dimensional datasets using a subset of features (genes), which are obtained after applying the proposed hybrid gene selection algorithm. The results show that the proposed technique significantly outperforms existing state-of-the-art techniques.
                  
                     Conclusion: It is also shown that the new sets of informative and biologically relevant genes are successfully identified by the proposed technique. The proposed approach can also be applied to other problem domains of interest which involve feature selection.",science
10.1016/j.cie.2019.06.040,Journal,Computers and Industrial Engineering,scopus,2019-09-01,sciencedirect,"Bernard, an energy intelligent system for raising residential users awareness",https://api.elsevier.com/content/abstract/scopus_id/85067600850,"Energy efficiency is still a hot topic today. Coming roughly the 25% of the energy consumption in EU from the residential sector, very few cheap and simple tools to promote energy efficiency in home users have been developed. The purpose of this paper is to present Bernard, a concept proof designed for filling this gap. This aims that householders become aware of their energy habits and have useful information that help them to redirect their consumption pattern. To achieve these goals, Bernard offers, through a mobile application, the home energy consumption monitoring in real time, the energy price forecast for the next hour and the appliances which are switched on, among others. Furthermore, it is important to highlight that the system has been designed with the premises of being cheap, non-intrusive, reliable and easily scalable, in order that utilities can gradually deploy and provide it to their customers, gaining at the same time valuable information for decision making and improving its corporate social image. Therefore, the adopted solution is based on a real time streaming data architecture suitable for handling huge volumes of data and applying predictive techniques on a cloud-computing environment. The paper provides a detailed description of the system and experimental results evaluating the performance of the predictive modules built. As case study, REFIT and REDD datasets were used.",science
10.1016/j.knosys.2019.05.010,Journal,Knowledge-Based Systems,scopus,2019-09-01,sciencedirect,Augmented label propagation for seed set expansion,https://api.elsevier.com/content/abstract/scopus_id/85065873438,"In many applications such as social network analysis and recommendation systems, it is of particular interest to identify a group of similar nodes/users/items. However, in networks of massive size, manual labeling process becomes intractable. A practical means is to mark a small number of nodes as seeds, and then expand them to the rest (unlabeled) ones, which is also known as seed set expansion. We present a novel method for seed set expansion by leveraging information spreading dynamics through label propagation. In particular, by devising an augmented, community-based label propagation, we can fully exploit the information of the limited seed nodes, and apply the connectivity structure of the whole network in imposing a larger number of constraints on the label propagation process, thus achieving an improved estimation. Our method can increase the effective number of seed nodes in that it can achieve a better estimation than other propagation methods using the same number of seeds. Extensive experiments on real-world datasets demonstrate the effectiveness and adaptiveness of our method, compared to the state-of-the-art approaches.",science
10.1016/j.sysarc.2019.01.007,Journal,Journal of Systems Architecture,scopus,2019-09-01,sciencedirect,A Survey and Taxonomy of FPGA-based Deep Learning Accelerators,https://api.elsevier.com/content/abstract/scopus_id/85063404030,"Deep learning, the fastest growing segment of Artificial Neural Network (ANN), has led to the emergence of many machine learning applications and their implementation across multiple platforms such as CPUs, GPUs and reconfigurable hardware (Field-Programmable Gate Arrays or FPGAs). However, inspired by the structure and function of ANNs, large-scale deep learning topologies require a considerable amount of parallel processing, memory resources, high throughput and significant processing power. Consequently, in the context of real time hardware systems, it is crucial to find the right trade-off between performance, energy efficiency, fast development, and cost. Although limited in size and resources, several approaches have showed that FPGAs provide a good starting point for the development of future deep learning implementation architectures. Through this paper, we briefly review recent work related to the implementation of deep learning algorithms in FPGAs. We will analyze and compare the design requirements and features of existing topologies to finally propose development strategies and implementation architectures for better use of FPGA-based deep learning topologies. In this context, we will examine the frameworks used in these studies, which will allow testing a lot of topologies to finally arrive at the best implementation alternatives in terms of performance and energy efficiency.",science
10.1016/j.dsx.2018.07.014,Journal,Diabetes and Metabolic Syndrome: Clinical Research and Reviews,scopus,2019-09-01,sciencedirect,Prevalence of metabolic syndrome in Iranian patients with schizophrenia: A systematic review and meta-analysis,https://api.elsevier.com/content/abstract/scopus_id/85050864479,"Industry 4.0 is an updated concept of smart production, which is identified with the fourth industrial revolution and the emergence of cyber-physical systems. Industry 4.0 is the next stage in the digitization of productions and industries, where such technologies and concepts as the Internet of things, big data, predictive analytics, cloud computing, machine learning, machine interaction, artificial intelligence, robotics, 3D printing, augmented reality.
                  As an area of therapy with the best market potential and one of the most expensive global diseases, diabetes attracts the best healthcare players, who use innovative technologies.
                  Current trends in digitalization of diabetes management are presented.",science
10.1016/j.oceaneng.2019.106129,Journal,Ocean Engineering,scopus,2019-08-15,sciencedirect,Study on wavelet neural network based anomaly detection in ocean observing data series,https://api.elsevier.com/content/abstract/scopus_id/85067611959,"In this paper, a novel method is presented for detecting anomalies in ocean fixed-point observing time series, which combines wavelet neural network (WNN), classifying threshold and two detecting strategies. The WNN was developed without any labeled training data to simulate the non-anomalous behaviors for next-step prediction. The classifying threshold was constructed according to the estimated distribution of long-term historical residual errors. The observation strategy (OS) and prediction strategy (PS) were designed to detect new unknown anomalies. Two types of marine observing time series from a buoy, deployed at the National Ocean Test Site of China, were selected for verifying the method. The results show that 99% of classifying confidence level is adequate to provide a reasonable trade-off between the false negative and false positive. By using the two detecting strategies and selecting proper estimated distribution of the threshold, the method is efficient for identifying the anomalous points and patterns which were caused by the natural factors or equipment failures. Compared with traditional ANN and wavelet-ANN, the WNN-based method is more tolerant to noise and more sensitive to anomalies with temporal dependencies. Furthermore, this approach introduced here can work in a real-time way and will help ocean engineering managers to obtain informed decisions.",science
10.1016/j.ymeth.2019.03.012,Journal,Methods,scopus,2019-08-15,sciencedirect,IVS2vec: A tool of Inverse Virtual Screening based on word2vec and deep learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85064277603,"Inverse Virtual Screening is a powerful technique in the early stage of drug discovery process. This technique can provide important clues for biologically active molecules, which is useful in the following researches of durg discovery. In this work, combining with Word2vec, a natural language processing technique, dense fully connected neural network (DFCNN) algorithm is utilized to build up a prediction model. This model is able to perform a binary classification. Based on the query molecule, the input protein candidates can be classified into two subsets. One set is that potential targets with high possibilities to bind with the query molecule and the other one is that the proteins with low possibilities to bind with the query molecule. This model is named as IVS2vec. IVS2vec also can output a score reflecting binding possibility of the association between a protein and a molecule, which is useful to improve efficiency of research. We applied IVS2vec on several databases related to drug development and shown that our model can detect possible therapeutic targets. In addition, our model can identify targets related to adverse drug reactions which is useful to improve medication safety and repurpose drugs. Moreover, IVS2vec can give a very fast speed to perform prediction jobs. It is suitable for processing a large number of compounds in the chemical databases. We also find that IVS2vec has potential capabilities and outperform other state-of-the-art docking tools such as Autodock vina. In this study, IVS2vec brings many convincing results than Autodock vina in the reverse target searching case of Quercetin.",science
10.1016/S1361-3723(19)30083-1,Journal,Computer Fraud and Security,scopus,2019-08-01,sciencedirect,AI vs AI: fraudsters turn defensive technology into an attack tool,https://api.elsevier.com/content/abstract/scopus_id/85070686326,"The first rule of managing online fraud and mitigating risk is to remember that fraudsters are entrepreneurs. While it's tempting to think of those committing digital fraud as hoody-wearing lone wolves spending hours in their bedroom working to weasel their way into someone's online account, in reality professional fraud operations look more like the JP Morgan trading floor.
                  Cyber criminals are not simple, hoodie-wearing lone wolves. Many are sophisticated fraud operations using the most advanced technology, including artificial intelligence (AI).
                  The energy and ingenuity with which fraud rings and cyber criminals have deployed AI-based solutions has matched that of the businesses and organisations that work to protect themselves from bad actors. Machines have been put to malicious use in ways ranging from click farms to complex model extraction schemes, explains Swami Vaithianathasamy of Signifyd.",science
10.1016/j.ece.2019.05.003,Journal,Education for Chemical Engineers,scopus,2019-07-01,sciencedirect,"Learning distillation by a combined experimental and simulation approach in a three steps laboratory: Vapor pressure, vapor-liquid equilibria and distillation column",https://api.elsevier.com/content/abstract/scopus_id/85066038830,"Distillation is one of the most important separation process in industrial chemistry. This operation is based on a deep knowledge of the fluid phase equilibria involved in the mixture to be separated. In particular, the most important aspects are the determination of the vapor pressures of the single compounds and the correct representation of the eventual not ideality of the mixture. Simulation science is a fundamental tool for managing these complex topics and chemical engineers students have to learn and to use it on real case-studies. To give to the students a complete overview of these complex aspects, a laboratory experience is proposed. Three different work stations were set up: i) determination of vapor pressure of two pure compounds; ii) the study of vapor-liquid equilibria of a binary mixture; iii) the use of a continuous multistage distillation column in dynamic and steady-state conditions. The simulation of all these activities by a commercial software, PRO II by AVEVA, allows to propose and verify the thermodynamic characteristics of the mixture and to correctly interpret the distillation column data. Moreover, the experimental plants and the data elaboration by classical equations are presented. The students are request to prepare a final report in which the description of the experimental plants and experimental procedure, the interpretation of the results and the simulation study are critically discussed in order to encourage them to reason and to acquire the concepts of the course.
                  Two different questionnaires each with 7 questions, for the course and for the laboratory, are proposed and analyzed. The final evaluation of the students was strongly positive both for the course as a whole and for the proposed laboratory activities.",science
10.1016/j.cmpb.2019.04.007,Journal,Computer Methods and Programs in Biomedicine,scopus,2019-07-01,sciencedirect,MLSeq: Machine learning interface for RNA-sequencing data,https://api.elsevier.com/content/abstract/scopus_id/85064937612,"Background and Objective
                  In the last decade, RNA-sequencing technology has become method-of-choice and prefered to microarray technology for gene expression based classification and differential expression analysis since it produces less noisy data. Although there are many algorithms proposed for microarray data, the number of available algorithms and programs are limited for classification of RNA-sequencing data. For this reason, we developed MLSeq, to bring not only frequently used classification algorithms but also novel approaches together and make them available to be used for classification of RNA sequencing data. This package is developed using R language environment and distributed through BIOCONDUCTOR network.
               
                  Methods
                  Classification of RNA-sequencing data is not straightforward since raw data should be preprocessed before downstream analysis. With MLSeq package, researchers can easily preprocess (normalization, filtering, transformation etc.) and classify raw RNA-sequencing data using two strategies: (i) to perform algorithms which are directly proposed for RNA-sequencing data structure or (ii) to transform RNA-sequencing data in order to bring it distributionally closer to microarray data structure, and perform algorithms which are developed for microarray data. Moreover, we proposed novel algorithms such as voom (an acronym for variance modelling at observational level) based nearest shrunken centroids (voomNSC), diagonal linear discriminant analysis (voomDLDA), etc. through MLSeq.
               
                  Materials
                  Three real RNA-sequencing datasets (i.e cervical cancer, lung cancer and aging datasets) were used to evalute model performances. Poisson linear discriminant analysis (PLDA) and negative binomial linear discriminant analysis (NBLDA) were selected as algorithms based on dicrete distributions, and voomNSC, nearest shrunken centroids (NSC) and support vector machines (SVM) were selected as algorithms based on continuous distributions for model comparisons. Each algorithm is compared using classification accuracies and sparsities on an independent test set.
               
                  Results
                  The algorithms which are based on discrete distributions performed better in cervical cancer and aging data with accuracies above 0.92. In lung cancer data, the most of algorithms performed similar with accuracies of 0.88 except that SVM achieved 0.94 of accuracy. Our voomNSC algorithm was the most sparse algorithm, and able to select 2.2% and 6.6% of all features for cervical cancer and lung cancer datasets respectively. However, in aging data, sparse classifiers were not able to select an optimal subset of all features.
               
                  Conclusion
                  MLSeq is comprehensive and easy-to-use interface for classification of gene expression data. It allows researchers perform both preprocessing and classification tasks through single platform. With this property, MLSeq can be considered as a pipeline for the classification of RNA-sequencing data.",science
10.1016/j.ins.2019.03.044,Journal,Information Sciences,scopus,2019-07-01,sciencedirect,A hybrid group decision making framework for achieving agreed solutions based on stable opinions,https://api.elsevier.com/content/abstract/scopus_id/85063603136,"Polarization in a group’s opinions drives to disagreements and dissent among individuals, which make it harder to achieve group satisfactory decisions. Within Group Decision Making (GDM) problems to soften disagreements, lots of consensus reaching processes (CRPs) have been proposed to converge opinions but rarely consider the existing dynamic relationships among the experts. Meanwhile, Opinion Dynamics studies the evolution of opinions based on the relationships existing among the group members by using Social Network Analysis (SNA). In real-world GDM problems the application of CRPs alone may not be enough to achieve the desired level of agreement when there is too much dissent among experts. In this paper, a novel framework is proposed that hybridizes both the process of making closer opinions realized by CRPs and the evolving relationships among experts based on SNA. This new framework addresses when it might be impossible to achieve the agreement through CRPs, which tries to achieve a potential consensus considering that if opinions are too polarized, maybe different stable opinions states are still suitable and easier to achieve by applying a SNA together with the CRP. This framework is further analyzed through simulation experiments for demonstrating its validity and some properties.",science
10.1016/j.eswa.2019.01.077,Journal,Expert Systems with Applications,scopus,2019-07-01,sciencedirect,Deep learning in material recovery: Development of method to create training database,https://api.elsevier.com/content/abstract/scopus_id/85061339627,"Increasing the rate of material identification, separation and recovery is a priority in resource management and recovery, and rapid, low cost imaging and interpretation is key. This study uses different combinations of cameras, illuminations and data augmentation techniques to create databases of images to train deep neural networks for the recognition of fibre materials. Using a limited set of 24 material samples sized 1200 cm2, it compares the outcome of reducing them to 30 cm2. The best classification accuracies obtained range from 76.6% to 77.5% indicating it is possible to overcome problems such as limited available materials, time, or storage capabilities, by using a setup with 5 cameras, 5 lights and applying simple software image manipulation techniques. The same method can be used to create deep neural network training databases to recognise a wider range of materials typically found in solid waste streams, in real-time. Furthermore, it offers flexibility as the classification cameras could be deployed at different stages within solid waste processing plants, providing feedback for process control, with the potential of increasing plant efficiency and reducing costs.",science
10.1016/j.future.2018.01.043,Journal,Future Generation Computer Systems,scopus,2019-07-01,sciencedirect,Combining humans and machines for the future: A novel procedure to predict human interest,https://api.elsevier.com/content/abstract/scopus_id/85042366370,"This paper proposes a method to quantify interest. In common terminology, when we engage with an object, e.g. Online Games, Social Networking Websites, Mobile Apps, etc., there is a degree of interest between us and the object. But, owing to the lack of a procedure that can quantify interest, we are unable to tell by how ‘much’ of a factor are we interested in the object. In other words, can we find a number for someone’s interest? In this article, we propose a method that uses the principle of Bayesian Inference to tackle this issue. We formulate the “interest estimation problem” as a state estimation problem to deduce interest (in any object) indirectly from user activity. Activity caused by interest is computed through a subjective–objectiveweighted approach, then using indirect inference rules, we provide numerical estimates of interest. To do that, we model the dynamics of interest through the Ornstein–Uhlenbeck process. To further enhance the base performance, we draw inspiration from Stochastic Volatility models from Finance. Subsequently, drawing upon a self-adapting transfer function, we provide an avant-garde statistical procedure to model the transformation of interest into activity. The individual contributions are then combined and a solution is provided via Particle filters. Validation of the method is done in two ways. (1) Experimentation is performed on real datasets. Through numerical investigation we have found that the method shows good performance. (2) We implement the framework as a Web application and deploy it on an Enterprise Service Bus. The framework has been successfully hosted on a Cloud based Virtualized testbed consisting of several Virtual Machines constructed over XENServer as the underlying hypervisor. Through this experimental setup, we show the efficacy of the proposed algorithm in estimating interest, at much the same time, we demonstrate the viability of the method in practical cloud based deployment scenarios.",science
10.1016/j.heliyon.2019.e01806,Journal,Heliyon,scopus,2019-06-01,sciencedirect,Prediction of students’ awareness level towards ICT and mobile technology in Indian and Hungarian University for the real-time: preliminary results,https://api.elsevier.com/content/abstract/scopus_id/85067313361,"An experimental study was conducted to predict the student's awareness of Information and Communication Technology (ICT) and Mobile Technology (MT) in Indian and Hungarian university's students. A primary dataset was gathered from two popular universities located in India and Hungary in the academic year 2017–2018. This paper focuses on the prediction of two major parameters from dataset such as usability and educational benefits using four machine learning classifiers multilayer perceptron (ANN), Support vector machine (SVM), K-nearest neighbor (KNN) and Discriminant (DISC). The multi-classification problem was solved with test, train and validated datasets using machine learning classifiers. One hand, feature aggregation with the train-test-validation technique improved the ANN's prediction accuracy of educational benefits for both countries. Another hand, ANN's accuracy decreases significantly in the prediction of usability. Further, SVM and ANN outperformed the KNN and the DISC in the prediction of awareness level towards ICT and MT in India and Hungary. Also, this paper reveals that the future awareness level for the educational benefits will be Very High or Moderate in both countries. Also, the awareness level is predicted as High and Moderate for usability parameter in both countries. Further, ANN and SVM accuracy and prediction time is compared with T-test at 0.05 significance level which distinguished CPU training time is taken by ANN and SVM using K-fold and Hold out method. Also, K-fold enhanced the significant prediction accuracy of SVM and ANN. the authors also used a STAC web platform to compare the accuracy datasets using T-test and ANOVA test at 0.05 significant level and we found ANN and SVM classifier has no significant difference in prediction accuracy in each dataset. Also, the authors recommend presented predictive models to be deployed as a real-time module of the institute's website for the real-time prediction of ICT & MT awareness level.",science
10.1016/j.mimet.2019.03.003,Journal,Journal of Microbiological Methods,scopus,2019-06-01,sciencedirect,A duplex quantitative real-time PCR assay for the detection and quantification of Xanthomonas phaseoli pv. dieffenbachiae from diseased and latently infected anthurium tissue,https://api.elsevier.com/content/abstract/scopus_id/85064711930,"Anthurium bacterial blight caused by Xanthomonas phaseoli pv. dieffenbachiae (formerly Xanthomonas axonopodis pv. dieffenbachiae) is the major phytosanitary threat in many anthurium growing areas worldwide. Reliable and sensitive diagnostic tools are required for surveillance and certification programs. A duplex real–time quantitative PCR assay was developed for the detection and quantification of X. phaseoli pv. dieffenbachiae from anthurium tissue. This PCR assay targeted a X. phaseoli pv. dieffenbachiae–specific gene encoding an ABC transporter and an internal control encoding for chalcone synthase in Anthurium andreanum. A cycle threshold (Ct), using a receiver-operating characteristic approach (ROC), was implemented to ensure that the declaration of a positive sample was reliable. The duplex real–time assay displayed very high performance with regards to analytical specificity (100% inclusivity, 98.9% exclusivity), analytical sensitivity (LOD95% = 894 bacteria/ml corresponding to 18 bacteria per reaction) and repeatability. We demonstrated the pertinence of this real–time quantitative PCR assay for detecting X. phaseoli pv. dieffenbachiae from diseased leaf tissue (collected from outbreaks on anthurium) and from asymptomatic, latently infected anthurium plants. This assay could be useful for surveillance, as well as for indexing propagative plant material for the presence of X. phaseoli pv. dieffenbachiae.",science
10.1016/j.asoc.2019.03.057,Journal,Applied Soft Computing Journal,scopus,2019-06-01,sciencedirect,Compression of recurrent neural networks for efficient language modeling,https://api.elsevier.com/content/abstract/scopus_id/85064251763,"Recurrent neural networks have proved to be an effective method for statistical language modeling. However, in practice their memory and run-time complexity are usually too large to be implemented in real-time offline mobile applications. In this paper we consider several compression techniques for recurrent neural networks including Long–Short Term Memory models. We make particular attention to the high-dimensional output problem caused by the very large vocabulary size. We focus on effective compression methods in the context of their exploitation on devices: pruning, quantization, and matrix decomposition approaches (low-rank factorization and tensor train decomposition, in particular). For each model we investigate the trade-off between its size, suitability for fast inference and perplexity. We propose a general pipeline for applying the most suitable methods to compress recurrent neural networks for language modeling. It has been shown in the experimental study with the Penn Treebank (PTB) dataset that the most efficient results in terms of speed and compression–perplexity balance are obtained by matrix decomposition techniques.",science
10.1016/j.compbiolchem.2019.03.014,Journal,Computational Biology and Chemistry,scopus,2019-06-01,sciencedirect,Discovery of perturbation gene targets via free text metadata mining in Gene Expression Omnibus,https://api.elsevier.com/content/abstract/scopus_id/85063864313,"There exists over 2.5 million publicly available gene expression samples across 101,000 data series in NCBI's Gene Expression Omnibus (GEO) database. Due to the lack of the use of standardised ontology terms in GEO's free text metadata to annotate the experimental type and sample type, this database remains difficult to harness computationally without significant manual intervention.
                  In this work, we present an interactive R/Shiny tool called GEOracle that utilises text mining and machine learning techniques to automatically identify perturbation experiments, group treatment and control samples and perform differential expression. We present applications of GEOracle to discover conserved signalling pathway target genes and identify an organ specific gene regulatory network.
                  GEOracle is effective in discovering perturbation gene targets in GEO by harnessing its free text metadata. Its effectiveness and applicability has been demonstrated by cross validation and two real-life case studies. It opens up new avenues to unlock the gene regulatory information embedded inside large biological databases such as GEO. GEOracle is available at https://github.com/VCCRI/GEOracle.",science
10.1016/j.envsoft.2019.02.015,Journal,Environmental Modelling and Software,scopus,2019-06-01,sciencedirect,Building complex event processing capability for intelligent environmental monitoring,https://api.elsevier.com/content/abstract/scopus_id/85061785554,"Rapid evolution of Internet-of-Things is driving the increased deployment of smart sensors in environmental applications, contributing to many big data characteristics of environmental monitoring. Most of the current environmental monitoring systems are not designed to handle real-time datastreams, and the best practices for datastream processing and predictive analytics are yet to be established. This work presents a complex event processing (CEP) engine for detecting anomalies in real time, and demonstrates it using a series of real monitoring data from the geological carbon sequestration domain. We show that the service-based CEP engine is instrumental for enabling environmental intelligent monitoring systems to ingest heterogeneous datastreams with scalable performance. Our CEP framework requires minimal coding from the user and can be easily extended to other similar environmental monitoring applications.",science
10.1016/j.cogsys.2019.01.003,Journal,Cognitive Systems Research,scopus,2019-06-01,sciencedirect,The CORTEX cognitive robotics architecture: Use cases,https://api.elsevier.com/content/abstract/scopus_id/85060622773,"CORTEX is a cognitive robotics architecture inspired by three key ideas: modularity, internal modelling and graph representations. CORTEX is also a computational framework designed to support early forms of intelligence in real world, human interacting robots, by selecting an a priori functional decomposition of the capabilities of the robot. This set of abilities was then translated to computational modules or agents, each one built as a network of software interconnected components. The nature of these agents can range from pure reactive modules connected to sensors and/or actuators, to pure deliberative ones, but they can only communicate with each other through a graph structure called Deep State Representation (DSR). DSR is a short-term dynamic representation of the space surrounding the robot, the objects and the humans in it, and the robot itself. All these entities are perceived and transformed into different levels of abstraction, ranging from geometric data to high-level symbolic relations such as “the person is talking and gazing at me”. The combination of symbolic and geometric information endows the architecture with the potential to simulate and anticipate the outcome of the actions executed by the robot. In this paper we present recent advances in the CORTEX architecture and several real-world human-robot interaction scenarios in which they have been tested. We describe our interpretation of the ideas inspiring the architecture and the reasons why this specific computational framework is a promising architecture for the social robots of tomorrow.",science
10.1016/j.future.2018.12.038,Journal,Future Generation Computer Systems,scopus,2019-06-01,sciencedirect,Code authorship identification using convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85059761266,"Although source code authorship identification creates a privacy threat for many open source contributors, it is an important topic for the forensics field and enables many successful forensic applications, including ghostwriting detection, copyright dispute settlements, and other code analysis applications. This work proposes a convolutional neural network (CNN) based code authorship identification system. Our proposed system exploits term frequency-inverse document frequency, word embedding modeling, and feature learning techniques for code representation. This representation is then fed into a CNN-based code authorship identification model to identify the code’s author. Evaluation results from using our approach on data from Google Code Jam demonstrate an identification accuracy of up to 99.4% with 150 candidate programmers, and 96.2% with 1,600 programmers. The evaluation of our approach also shows high accuracy for programmers identification over real-world code samples from 1987 public repositories on GitHub with 95% accuracy for 745 C programmers and 97% for the C++ programmers. These results indicate that the proposed approaches are not language-specific techniques and can identify programmers of different programming languages.",science
10.1016/j.net.2018.12.020,Journal,Nuclear Engineering and Technology,scopus,2019-06-01,sciencedirect,Numerical evaluation of gamma radiation monitoring,https://api.elsevier.com/content/abstract/scopus_id/85059358720,"Airborne Gamma Ray Spectrometry (AGRS) with its important applications such as gathering radiation information of ground surface, geochemistry measuring of the abundance of Potassium, Thorium and Uranium in outer earth layer, environmental and nuclear site surveillance has a key role in the field of nuclear science and human life. The Broyden–Fletcher–Goldfarb–Shanno (BFGS), with its advanced numerical unconstrained nonlinear optimization in collaboration with Artificial Neural Networks (ANNs) provides a noteworthy opportunity for modern AGRS. In this study a new AGRS system empowered by ANN-BFGS has been proposed and evaluated on available empirical AGRS data. To that effect different architectures of adaptive ANN-BFGS were implemented for a sort of published experimental AGRS outputs. The selected approach among of various training methods, with its low iteration cost and non-diagonal scaling allocation is a new powerful algorithm for AGRS data due to its inherent stochastic properties. Experiments were performed by different architectures and trainings, the selected scheme achieved the smallest number of epochs, the minimum Mean Square Error (MSE) and the maximum performance in compare with different types of optimization strategies and algorithms. The proposed method is capable to be implemented on a cost effective and minimum electronic equipment to present its real-time process, which will let it to be used on board a light Unmanned Aerial Vehicle (UAV). The advanced adaptation properties and models of neural network, the training of stochastic process and its implementation on DSP outstands an affordable, reliable and low cost AGRS design. The main outcome of the study shows this method increases the quality of curvature information of AGRS data while cost of the algorithm is reduced in each iteration so the proposed ANN-BFGS is a trustworthy appropriate model for Gamma-ray data reconstruction and analysis based on advanced novel artificial intelligence systems.",science
10.1016/j.amsu.2019.04.001,Journal,Annals of Medicine and Surgery,scopus,2019-05-01,sciencedirect,"Artificial intelligence, regenerative surgery, robotics? What is realistic for the future of surgery?",https://api.elsevier.com/content/abstract/scopus_id/85064430299,"The potential of surgery lies in the technological advances that would complement it. The landscape of the field will differ depending on the time period being looked at and would no doubt include conjecture. Initial breakthroughs will need to pave the way for future medical technology and apply to the surgical sciences. Within the next 10 years we would expect to see the emergence of big data analysis, cuttingedge image processing techniques for surgical planning and better implementation of virtual and augmented reality in operating theatres for both patient care and teaching purposes. Over the next 50 to 100 years, the use of quantum computing should lead to increased automation in our healthcare systems. The inception of novel biomaterial invention and advanced genetic engineering will usher in the new age of regenerative medicine in the clinical setting. The future of surgery includes many predictions and promises, but it is apparent that the development will lead to bettering outcome and focus on patient care.",science
10.1016/j.engappai.2019.03.001,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,Affective analytics of demonstration sites,https://api.elsevier.com/content/abstract/scopus_id/85062991999,"Multiple-criteria decision-making (MCDM) typically assumes that crowds make completely rational decisions. In MCDM, a crowd as a whole, or its individual members, generally make decisions free from any influence of valence, arousal, emotional state or environment. In contrast, various theories dealing with crowd psychology (Gustave Le Bon, Freudian, Deindividuation, Convergence, Emergent norm, Social identity) analyze, in one form or another, the emotions of the crowd. According to above theories, crowd is influenced by a range of behavioral factors, such as physical, social, psychological, culture, norms, and emotions. It can be argued that the emotional state, valence and arousal of crowds affect their decision making to a considerable degree and multiple criteria crowd behavior modeling must, therefore, consider this impact as well. In this light, the integration of crowd simulation and biometric methods, behavioral operations research and emotions in decision making has taken a prominent place as it leads to a better understanding of crowd emotions and crowd decision making. In this context, the authors developed the Affective Analytics of Demonstration Sites (ANDES) that added to this body of research in four ways. The crowd analysis and simulations conducted with ANDES used a neuro decision matrix. The matrix contains a detailed description of demonstration sites (public spaces) in question and the emotions, valence, arousal and physiological parameters of people present there. With ANDES’s Remote Sensor Network, emotional (emotions, valence, arousal) and physiological (average crowd facial temperature, crowd composition by gender and age group, etc.) parameters of people present at demonstration sites can be mapped. ANDES can assist experts in more effective implementations of public spaces planning and a participation process by attendees by collecting and examining various layers of data on the emotional and physiological parameters of visitors based on a visitors-centric public spaces planning approach. ANDES can determine the public space and real estate values.",science
10.1016/j.jad.2019.03.044,Journal,Journal of Affective Disorders,scopus,2019-05-01,sciencedirect,Short-term prediction of suicidal thoughts and behaviors in adolescents: Can recent developments in technology and computational science provide a breakthrough?,https://api.elsevier.com/content/abstract/scopus_id/85062497590,"Background
                  Suicide is one of the leading causes of death among adolescents, and developing effective methods to improve short-term prediction of suicidal thoughts and behaviors (STBs) is critical. Currently, the most robust predictors of STBs are demographic or clinical indicators that have relatively weak predictive value. However, there is an emerging literature on short-term prediction of suicide risk that has identified a number of promising candidates, including (but not limited to) rapid escalation of: (a) emotional distress, (b) social dysfunction (e.g., bullying, rejection), and (c) sleep disturbance. However, these prior studies are limited in two critical ways. First, they rely almost entirely on self-report. Second, most studies have not focused on assessment of these risk factors using intensive longitudinal assessment techniques that are able to capture the dynamics of changes in risk states at the individual level.
               
                  Method
                  In this paper we explore how to capitalize on recent developments in real-time monitoring methods and computational analysis in order to address these fundamental problems.
               
                  Results
                  We now have the capacity to use: (a) smartphone, wearable computing, and smart home technology to conduct intensive longitudinal assessments monitoring of putative risk factors with minimal participant burden and (b) modern computational techniques to develop predictive algorithms for STBs. Current research and theory on short-term risk processes for STBs, combined with the emergent capabilities of new technologies, suggest that this is an important research agenda for the future.
               
                  Limitations
                  Although these approaches have enormous potential to create new knowledge, the current empirical literature is limited. Moreover, passive monitoring of risk for STBs raises complex ethical issues that will need to be resolved before large scale clinical applications are feasible.
               
                  Conclusions
                  Smartphone, wearable, and smart home technology may provide one point of access that might facilitate both early identification and intervention implementation, and thus, represents a key area for future STB research.",science
10.1016/j.petrol.2019.01.089,Journal,Journal of Petroleum Science and Engineering,scopus,2019-05-01,sciencedirect,Predicting seismic-based risk of lost circulation using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85061035639,"Lost circulation during well drilling and completion wastes productive time, and even kills the well in severe cases. Timely identifying lost circulation events and taking countermeasures has been the focus of related study. However, a real prediction of lost circulation risk before drilling would be an active response to the challenge. In this paper, a technical solution is proposed to evaluate geological lost-circulation risk in the field using 3D seismic data attributes and machine learning technique. First, four seismic attributes (variance, attenuation, sweetness, RMS amplitude) that are the most correlated with lost circulation incidents are recommended. Then a prediction model is built by conducting supervised learning that involves a majority voting algorithm. The performance of the model is illustrated by six unseen drilled wells and shows the ability and potential to forecast lost circulation probability both along well trajectory and in the region far away from the drilled wells. The prediction resolution in the lateral and vertical direction is about 25 m and 6 m (2 ms), respectively, which are distinct advantages over the traditional description of geological structures using seismic data. It shows that the lost circulation risk can be hardly recognized by interpreting one specific seismic attribute, which is a common practice. Finally, the challenges in predicting lost circulation risk using seismic data are summarized. Overall, the study suggests that machine learning would be a practical solution to predict various construction risks that are related to seismic-based geological issues. Knowing in advance the risks, people could avoid or at least minimize the losses by optimizing well deployment in the field and taking preventive measures.",science
10.1016/j.ins.2019.01.028,Journal,Information Sciences,scopus,2019-05-01,sciencedirect,A divide and agglomerate algorithm for community detection in social networks,https://api.elsevier.com/content/abstract/scopus_id/85059953640,"Communities, or clusters, are usually subgraphs of nodes densely interconnected but sparsely linked with others. The nodes with similar properties or behaviors are more likely to be in the same community, and vice versa. However, due to the complexity and diversity of networks, the accurate organization or function of communities in many real networks is often extremely difficult to be recognized. Hence, methods for community detection would have immediate impact on understanding the organizations and functions of networks. Therefore, algorithm design becomes a fundamental problem for many networks. In this paper, the local and global information are applied together to propose a divide and agglomerate (DA) algorithm for community detection in social networks. The DA algorithm achieves the result with a two-stage strategy: Dividing a network into small groups according to node pairs’ similarities, and merging a group with the other who has the biggest attraction for it until the community criterion is steady. The novel similarity, constrained AA index captures the local and global information ensuring the optimal communities detection. The results of experiments show that DA algorithm obtains superior community results compared with six other widely used algorithms, which indicate that DA algorithm has advantages for community detection.",science
10.1016/j.ipm.2018.04.011,Journal,Information Processing and Management,scopus,2019-05-01,sciencedirect,Real-time processing of social media with SENTINEL: A syndromic surveillance system incorporating deep learning for health classification,https://api.elsevier.com/content/abstract/scopus_id/85048575075,"Interest in real-time syndromic surveillance based on social media data has greatly increased in recent years. The ability to detect disease outbreaks earlier than traditional methods would be highly useful for public health officials. This paper describes a software system which is built upon recent developments in machine learning and data processing to achieve this goal. The system is built from reusable modules integrated into data processing pipelines that are easily deployable and configurable. It applies deep learning to the problem of classifying health-related tweets and is able to do so with high accuracy. It has the capability to detect illness outbreaks from Twitter data and then to build up and display information about these outbreaks, including relevant news articles, to provide situational awareness. It also provides nowcasting functionality of current disease levels from previous clinical data combined with Twitter data.
                  The preliminary results are promising, with the system being able to detect outbreaks of influenza-like illness symptoms which could then be confirmed by existing official sources. The Nowcasting module shows that using social media data can improve prediction for multiple diseases over simply using traditional data sources.",science
10.15232/aas.2018-01801,Journal,Applied Animal Science,scopus,2019-04-01,sciencedirect,INVITED REVIEW: Current state of wearable precision dairy technologies in disease detection,https://api.elsevier.com/content/abstract/scopus_id/85067379776,"Purpose
                  The primary objective of this review article is to provide insight into the role of wearable precision dairy technologies (WPDT) in detection of lameness, mastitis, metabolic disorders, and metritis.
               
                  Sources
                  This review is separated into 3 sections: overview of technology development; WPDT behavioral variables linked to disease; and WPDT detection of disease and disorders. Through Web of Science, Google Scholar, and SPAC (Searchable Proceedings of Animal Conferences, ADSA), 99 publications were identified that discuss WPDT that can be used for disease detection and associated similar abnormal behaviors.
               
                  Synthesis
                  Precision dairy technology is the real-time monitoring of animals through behavior monitoring, milk constituents, milk yield, video analysis, record analysis, and physiological monitoring. Technologies can be wearable, incorporated into the milking system, stand alone, or part of the management software. Real-time monitoring has the potential to improve individual cow management and overall farm efficiency. Wearable precision dairy technologies reside on or within the cow for some amount of time. These WPDT currently can measure an individual cow’s time spent at the feed bunk, rumination time, eating time, lying time, standing time, walking time, activity, lying-to-standing transitions, temperature, and rumen pH and provide a cow’s location. Recently, WPDT marketed for estrus detection were adapted for disease detection.
               
                  Conclusions and Applications
                  Potential does exist for WPDT disease detection. Technologies can identify changes in behavior associated with disease or disorders, although no technologies currently provide disease-specific alerts. Future studies should focus on incorporating multiple behavior, physiological, and herd records with machine-learning techniques to create timely, disease-specific alerts.",science
10.1016/j.eng.2018.11.027,Journal,Engineering,scopus,2019-04-01,sciencedirect,The State of the Art of Data Science and Engineering in Structural Health Monitoring,https://api.elsevier.com/content/abstract/scopus_id/85062663661,"Structural health monitoring (SHM) is a multi-discipline field that involves the automatic sensing of structural loads and response by means of a large number of sensors and instruments, followed by a diagnosis of the structural health based on the collected data. Because an SHM system implemented into a structure automatically senses, evaluates, and warns about structural conditions in real time, massive data are a significant feature of SHM. The techniques related to massive data are referred to as data science and engineering, and include acquisition techniques, transition techniques, management techniques, and processing and mining algorithms for massive data. This paper provides a brief review of the state of the art of data science and engineering in SHM as investigated by these authors, and covers the compressive sampling-based data-acquisition algorithm, the anomaly data diagnosis approach using a deep learning algorithm, crack identification approaches using computer vision techniques, and condition assessment approaches for bridges using machine learning algorithms. Future trends are discussed in the conclusion.",science
10.1016/j.resuscitation.2019.02.019,Journal,Resuscitation,scopus,2019-04-01,sciencedirect,Development of a novel cardiopulmonary resuscitation measurement tool using real-time feedback from wearable wireless instrumentation,https://api.elsevier.com/content/abstract/scopus_id/85062471861,"Aim
                  The design and implementation of a wearable training device to improve cardiopulmonary resuscitation (CPR) is presented.
               
                  Methods
                  The MYO contains both Electromyography (EMG) and Inertial Measurement Unit (IMU) sensors which are used to detect effective CPR, and the four common incorrect hand and arm positions viz. relaxed fingers; hands too low on the sternum; patient too close; or patient too far. The device determines the rate and depth of compressions calculated using a Fourier transform and dual-quaternions respectively. In addition, common positional mistakes are determined using classification algorithms (six machine learning algorithms are considered and tested). Feedback via Graphical User Interface (GUI) and audio is integrated.
               
                  Results
                  The system is tested by performing CPR on a mannequin and comparing real-time results to theoretical values. Tests show that although the classification algorithm performed well in testing (98%), in real time, it had low accuracy for certain categories (60%), which are attributable to the MYO calibration, sampling rate and misclassification of similar hand positions. Combining these similar incorrect positions into more general categories significantly improves accuracy, and produces the same improved outcome of improved CPR. The rate and depth measures have a general accuracy of 97%.
               
                  Conclusion
                  The system allows for portable, real-time feedback for use in training and in the field, and shows promise toward classifying and improving the administration of CPR.",science
10.1016/j.forsciint.2019.02.028,Journal,Forensic Science International,scopus,2019-04-01,sciencedirect,Chat Analysis Triage Tool: Differentiating contact-driven vs. fantasy-driven child sex offenders,https://api.elsevier.com/content/abstract/scopus_id/85062400557,"Investigating crimes against children, specifically sexual solicitations, are complicated because not all offenders are contact-driven, meaning they want to meet the minor for sex in the physical world; instead, some offenders are fantasy-driven, in that they are more interested in cybersex and role-play. In addition, the sheer volume of cases involving the online sexual solicitation of minors makes it difficult for law enforcement to determine whether an offender is contact-driven vs. fantasy-driven. However, research shows that there are language-based differences between minors and contact-driven offenders vs. fantasy driven-offenders. Thus, we developed the Chat Analysis Triage Tool (CATT), a forensically sound investigative tool that, based on natural language processing methods, analyzes and compares chats between minors and contact-driven vs. non-contract driven offenders. Using an SVM classifier, we were successful in differentiating the classes based on character trigrams. In a matter of seconds, the existing algorithms provide an identification of an offender’s risk level based on the likelihood of contact offending as inferred from the model, which assists law enforcement in their ability to triage and prioritize cases involving the sexual solicitation of minors.",science
10.1016/j.mehy.2019.02.021,Journal,Medical Hypotheses,scopus,2019-04-01,sciencedirect,Percolation theory for the recognition of patterns in topographic images of the cortical activity,https://api.elsevier.com/content/abstract/scopus_id/85061357293,"Electroencephalogram (EEG) is one of the mechanisms used to collect complex data. Its use includes evaluating neurological disorders, investigating brain function and correlations between EEG signals and real or imagined movements. The Topographic Image of Cortical Activity (TICA) records obtained by the EEG make it possible to observe, through color discrimination, the cortical areas that represent greater or lesser activity. Percolation Theory (PT) reveals properties on the aspects of fluid spreading from a central point, these properties being related to the aspects of the medium, topological characteristics and ease of penetration of a fluid in materials. The hypothesis presented so far considers that synaptic activities originate in points and spread from them, causing different areas of the brain to interact in a diffusive associative behavior, generating electric and magnetic fields by the currents that spread through the brain tissue and have an effect on the scalp sensors. Brain areas spatially separated create large-scale dynamic networks that are described by functional and effective connectivity. The proposition is that this phenomenon behaves like a fluidic spreading, so we can use the PT, through the topological analysis we detect specific signatures related to neural phenomena that manifest changes in the behavior of synaptic diffusion. This signature must be characterized by the Fractal Dimension (FD) values of the scattering clusters, these values will be used as properties in the k-Nearest Neighbors (kNN) method, an TICA will be categorized according to the degree of similarity to the preexisting patterns. In this context, our hypothesis will consolidate as a more computational resource in the service of medicine and another way that opens with the possibility of analysis and detailed inferences of the brain through TICA that go beyond a simply visual observation, as it happens in the present day.",science
10.1016/j.ins.2018.11.028,Journal,Information Sciences,scopus,2019-04-01,sciencedirect,Machine learning based privacy-preserving fair data trading in big data market,https://api.elsevier.com/content/abstract/scopus_id/85056879362,"In the era of big data, the produced and collected data explode due to the emerging technologies and applications that pervade everywhere in our daily lives, including internet of things applications such as smart home, smart city, smart grid, e-commerce applications and social network. Big data market can carry out efficient data trading, which provides a way to share data and further enhances the utility of data. However, to realize effective data trading in big data market, several challenges need to be resolved. The first one is to verify the data availability for a data consumer. The second is privacy of a data provider who is unwilling to reveal his real identity to the data consumer. The third is the payment fairness between a data provider and a data consumer with atomic exchange. In this paper, we address these challenges by proposing a new blockchain-based fair data trading protocol in big data market. The proposed protocol integrates ring signature, double-authentication-preventing signature and similarity learning to guarantee the availability of trading data, privacy of data providers and fairness between data providers and data consumers. We show the proposed protocol achieves the desirable security properties that a secure data trading protocol should have. The implementation results with Solidity smart contract demonstrate the validity of the proposed blockchain-based fair data trading protocol.",science
10.1016/j.future.2018.01.054,Journal,Future Generation Computer Systems,scopus,2019-04-01,sciencedirect,Scalable distributed control plane for On-line social networks support cognitive neural computing in software defined networks,https://api.elsevier.com/content/abstract/scopus_id/85042335028,"Though most of the current proposed distributed control planes maintain strong consistency among their controllers, this paper argues the strong consistency is not a prerequisite and proposes an Event Coordination System (ECS) that enables an efficient event replaying system and a distributed control plane (DisCon) using this event replaying system to construct eventually consistent global network topologies among its controllers without sacrificing scalability. Our ECS implements a novel request handling procedure that ensures a firstly received write request is firstly multi-casted, notified, and updated, so thus our DisCon can maximally ensure the same time sequence in which topology events get updated at different controllers and the constructed topologies can reflect the real network change in practice. We highlight the major mechanisms used, discuss the major causes of this eventual consistency, estimate the inconsistency window among controllers, and show how this eventual consistency does not make a big difference in supporting network applications. Experiments are conducted to evaluate our ECS and DisCon. The results show our DisCon has a larger event replay throughput and a lower event converging delay than HyperFlow, and larger flow setup rate and lower flow setup delay than most of the current distributed control planes.",science
10.1016/j.matdes.2018.107577,Journal,Materials and Design,scopus,2019-03-05,sciencedirect,Ensemble Kalman filter-based data assimilation for three-dimensional multi-phase-field model: Estimation of anisotropic grain boundary properties,https://api.elsevier.com/content/abstract/scopus_id/85059744257,"Data assimilation (DA) has been used as a machine learning approach to estimate a system's state and the unknown parameters in its numerical model by integrating observed data into model predictions. In this paper, we propose using the DA methodology based on the ensemble Kalman filter (EnKF) to improve the accuracy of microstructure prediction using three-dimensional multi-phase-field (3D-MPF) model and estimate the model parameters simultaneously. To demonstrate the applicability of the DA methodology, we performed numerical experiments in which a priori assumed true parameters related to the grain boundary (GB) energy cusp and GB mobility peak of Σ7 coincidence site lattice GB were estimated from synthetic data of time-evolving polycrystalline microstructure. Four model parameters related to the Σ7 GB properties were successfully estimated by assimilating the synthetic microstructure data to the 3D-MPF model predictions using the EnKF-based DA method. Furthermore, we accurately reproduced the preliminarily assumed true shapes of GB energy cusp and GB mobility peak by using the estimated parameters. The results suggest that implementation of the EnKF-based DA method in the MPF model has great potential for identifying unknown material properties and estimating unmeasurable microstructure evolutions in polycrystalline materials based on real time-series 3D microstructure observation data.",science
10.1016/j.resourpol.2018.12.013,Journal,Resources Policy,scopus,2019-03-01,sciencedirect,State of the art about metaheuristics and artificial neural networks applied to open pit mining,https://api.elsevier.com/content/abstract/scopus_id/85059167765,"In search of the best way to extract and take advantage of minerals, highlighting that these are part of the most important raw materials for the economic development of today's society, the following bibliographical review is presented, which covers the main metaheuristic techniques highlighted in the optimization of mining processes and artificial neural networks (ANN), fundamental for predicting them; With this, the applications and results of these methods can be observed in mining unit operations such as: blasting, transport and mineral processing, which until now have models or techniques for their prediction that are not applicable in all mining complexes, as well as metaheuristics for three fundamental variables of open-pit planning, which are: geological uncertainty, cutting law and extraction programming. In addition to this, the proposals that have been developed in the global optimization of mining complexes are shown. There is also a brief description of how these techniques were applied to optimize the operations and previous variables of the mining planning, as well as their implementation in several mines around the world. The information shown shows available alternatives for the implementation of new actions in favor of reaching the objectives for real and hypothetical sites, yielding satisfactory results. Finally, the conclusions of this work are presented.",science
10.1016/j.ress.2018.07.024,Journal,Reliability Engineering and System Safety,scopus,2019-03-01,sciencedirect,A new approach for estimating the parameters of Weibull distribution via particle swarm optimization: An application to the strengths of glass fibre data,https://api.elsevier.com/content/abstract/scopus_id/85056745362,"Three-parameter Weibull is one of the most popular and most widely-used distribution in many fields of science. Therefore, many studies have been conducted concerning the statistical inferences of the parameters of Weibull distribution. In general, the maximum likelihood (ML) methodology is used in the estimation process of unknown parameters. In this study, the ML estimation of the parameters of Weibull distribution is considered using particle swarm optimization (PSO). As in other heuristic optimization methods, the performance of PSO is affected by initial conditions. The novelty of this study comes from the fact that we propose a new adaptive search space based on confidence intervals in PSO. The modified maximum likelihood (MML) estimators are utilized for constructing the confidence intervals. MML based confidence intervals allow a narrower search space for the parameters of Weibull distribution than the search space used in the literature. Therefore, the performance of PSO increases, since the search space is wisely narrowed. In order to show the performance of the proposed approach, an extensive Monte-Carlo simulation study is conducted. Simulation results show that the proposed approach works well. In addition, real world data is analyzed to show implementation of the proposed method.",science
10.1016/j.future.2018.02.011,Journal,Future Generation Computer Systems,scopus,2019-03-01,sciencedirect,Collaborative prognostics in Social Asset Networks,https://api.elsevier.com/content/abstract/scopus_id/85042391186,"With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the field of Asset Management has moved from fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must find a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, defined as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they ignore their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares fit of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet.",science
10.1016/j.oceaneng.2019.01.003,Journal,Ocean Engineering,scopus,2019-02-01,sciencedirect,Data management for structural integrity assessment of offshore wind turbine support structures: data cleansing and missing data imputation,https://api.elsevier.com/content/abstract/scopus_id/85061324147,"Structural Health Monitoring (SHM) and Condition Monitoring (CM) Systems are currently utilised to collect data from offshore wind turbines (OWTs), to enhance the accurate estimation of their operational performance. However, industry accepted practices for effectively managing the information that these systems provide have not been widely established yet. This paper presents a four-step methodological framework for the effective data management of SHM systems of OWTs and illustrates its applicability in real-time continuous data collected from three operational units, with the aim of utilising more complete and accurate datasets for fatigue life assessment of support structures. Firstly, a time-efficient synchronisation method that enables the continuous monitoring of these systems is presented, followed by a novel approach to noise cleansing and the posterior missing data imputation (MDI). By the implementation of these techniques those data-points containing excessive noise are removed from the dataset (Step 2), advanced numerical tools are employed to regenerate missing data (Step 3) and fatigue is estimated for the results of these two methodologies (Step 4). Results show that after cleansing, missing data can be imputed with an average absolute error of 2.1%, while this error is kept within the [+ 15.2%−11.0%] range in 95% of cases. Furthermore, only 0.15% of the imputed data fell outside the noise thresholds. Fatigue is found to be underestimated both, when data cleansing does not take place and when it takes place but MDI does not. This makes this novel methodology an enhancement to conventional structural integrity assessment techniques that do not employ continuous datasets in their analyses.",science
10.1016/j.esd.2018.12.002,Journal,Energy for Sustainable Development,scopus,2019-02-01,sciencedirect,Identifying urban geometric types as energy performance patterns,https://api.elsevier.com/content/abstract/scopus_id/85058706592,"This paper aims to find the impact of geometric parameters on the energy performance of buildings, to using them to identify types regarding major geometric characteristics of a target area. Conventional approaches to control energy efficiency of buildings mainly focus on materials and capacity of insulation, but rarely consider urban and building geometries. By examining energy impacts on urban blocks by urban geometric forms, this paper seeks to identify urban geometric types and energy patterns on urban blocks. To achieve the aims of this study, this paper follows two steps: First, significant indicators for analyzing energy performance are identified in urban geometries; second, the types that capture urban geometry of a real city are categorized. As a result, as a reference for urban planning and design, the paper identifies 13 types that represent the characteristics of urban geometries regarding energy performance. The geometric indicators are carefully measured and their significance to energy performance of buildings is examined through regression analysis. According to these indicators, the 13 types are categorized using a hierarchical clustering algorithm, a machine learning method. Additionally, the 13 types are discussed for implementation as references in urban planning and design, particularly in block planning for a city.",science
10.1016/j.engappai.2018.10.014,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-02-01,sciencedirect,An effective Decision Support System for social media listening based on cross-source sentiment analysis models,https://api.elsevier.com/content/abstract/scopus_id/85056789877,"Nowadays, companies and enterprises are more and more incline to exploit the pervasive action of on-line social media, such as Facebook, Twitter and Instagram. Indeed, several promotional and marketing campaigns are carried out by concurrently adopting several social medial channels. These campaigns reach very quickly a wide range of different categories of users, since many people spend most of their time on on-line social media during the day.
                  In this work, a Decision Support System (DSS) is presented, which is able to efficiently support companies and enterprises in managing promotional and marketing campaigns on multiple social media channels. The proposed DSS continuously monitors multiple social channels, by collecting social media users’ comments on promotions, products, and services. Then, through the analysis of these data, the DSS estimates the reputation of brands related to specific companies and provides feedbacks about a digital marketing campaign.
                  The core of the proposed DSS is a Sentiment Analysis Engine (SAE), which is able to estimate the users’ sentiment in terms of positive, negative or neutral polarity, expressed in a comment. The SAE is based on a machine learning text classification model, which is initially trained by using real data streams coming from different social media platforms specialized in user reviews (e.g., TripAdvisor). Then, the monitoring and the sentiment classification are carried out on the comments continuously extracted from a set of public pages and channels of publicly available social networking platforms (e.g., Facebook, Twitter, and Instagram). This approach is labeled as cross-source sentiment analysis.
                  After some discussions on the design and the implementation of the proposed DSS, some results are shown about the experimentation of the proposed DSS on two scenarios, namely restaurants and consumer electronics online shops. Specifically, first the application of effective sentiment analysis models, created relying on user reviews is discussed: the models achieve accuracies higher than 90%. Then, such models are embedded into the proposed DSS. Finally, the results of a social listening campaign are presented. The campaign was carried out by fusing data streams coming from real social channels of popular companies belonging to the selected scenarios.",science
10.1016/j.micpro.2018.09.008,Journal,Microprocessors and Microsystems,scopus,2019-02-01,sciencedirect,Autonomous power management in mobile devices using dynamic frequency scaling and reinforcement learning for energy minimization,https://api.elsevier.com/content/abstract/scopus_id/85053917815,"Embedded systems execute applications that execute hardware differently depending on the computation task, generating time-varying workloads. Energy minimization can be reached by using the low-power central processing unit (CPU) frequency for each workload. We propose an autonomous and online approach, capable of reducing energy consumption from adaptation to workload variations even in an unknown environment. In this approach, we improved the AEWMA algorithm into a new algorithm called AEWMA-MSE, adding new functionality to detect workload changes and demonstrating why it is better to use statistical analysis for real user cases in a mobile environment. Also, a new power model for mobile devices based on k-NN algorithm for regression was proposed and validated proving to have a better trade-off between execution time and precision than neural networks and linear regression-based models. AEWMA-MSE and the proposed power model are integrated into a novel algorithm for energy management based on reinforcement learning that suitably selects the appropriate CPU frequency based on workload predictions to minimize energy consumption. The proposed approach is validated through simulation by using real smartphone data from an ARM Cortex A7 processor used in a commercial smartphone. Our proposal proved to have an improvement in the Q-learning cost function and can effectively minimize the average energy consumption by 21% and up to 29% when compared to the already existing approaches.",science
10.1016/j.cie.2018.08.018,Journal,Computers and Industrial Engineering,scopus,2019-02-01,sciencedirect,Ensemble-based big data analytics of lithofacies for automatic development of petroleum reservoirs,https://api.elsevier.com/content/abstract/scopus_id/85052098750,"Big data-driven ensemble learning is explored in this paper for quantitative geological lithofacies modeling, which is an integral and challenging part of petroleum reservoir development and characterization. Quantitative lithofacies modeling involves detection and recognition of underlying subsurface rock’s lithofacies. It requires real-time data acquisition, handling, storage, conditioning, analysis, and interpretation of raw sensory petroleum logging data. The real-time well-logs data collected from the sensor-based tools suffer from complications such as noise, nonlinearity, imbalance, and high-dimensionality which makes the prediction task more challenging. The existing literature on quantitative lithofacies modeling includes several data-driven techniques ranging from conventional well-logs to artificial intelligence (AI). Recently, multiple classifiers based Ensemble learners have been found to be more robust and reliable paradigms for detection and identification tasks in various machine learning applications, however, these are not well embraced in the petroleum industry. Ensemble methodology combines diverse expert’s opinions to obtain overall ensemble decision which in turn reduces the risk of a wrong decision. Thus, the uncertainties associated with complex reservoir data can be better handled by the use of Ensemble learners than the existing single learner based conventional models. Ensemble-based big data analytics, proposed in the paper, includes development and comparative performance testing of five popular ensemble methods (viz. Bagging, AdaBoost, Rotation forest, Random subspace, and DECORATE) for quantitative lithofacies modeling. Seven state-of-the-art base classifiers were used as members of different Ensemble learners for the analysis of Kansas (U.S.A.) oil-field data. The proposed techniques have been implemented on the widely used WEKA platform. The comparative performance analysis of the proposed techniques, presented in the paper, confirms its supremacy over the existing techniques used for quantitative lithofacies modeling.",science
10.1016/B978-0-12-815956-9.00006-5,Book,Technology in Supply Chain Management and Logistics: Current Practice and Future Applications,scopus,2019-01-01,sciencedirect,Emerging technologies in the health-care supply chain,https://api.elsevier.com/content/abstract/scopus_id/85082603106,"In this chapter, the background and organization of the health-care supply chain are reviewed, and the impact of emerging technologies is described. Maturing technologies, including optimization software, sensors/telematics, cloud computing, data warehouse systems, and automated storage and retrieval, are examined. Growth technologies, including mobility, wearable devices, data analytics, and social media, are examined as they potentially relate to the health-care supply chain. Emerging technologies, including 3D printing, drone delivery, and autonomous vehicles, are presented and examples provided on their use in the health-care supply chain. Exponential technologies, including blockchain, the Internet of Things, virtual/augmented reality, and artificial intelligence, are described with respect to potential applications in the health-care supply chain. Future changes in the external environment of health care, including decentralization, new competitors, and the increased use of telemedicine, are described with respect to impacts on the health-care supply chain.",science
10.1016/B978-0-12-816176-0.00045-4,Book,Handbook of Medical Image Computing and Computer Assisted Intervention,scopus,2019-01-01,sciencedirect,Challenges in computer assisted interventions,https://api.elsevier.com/content/abstract/scopus_id/85082596227,"Challenges in design, implementation, clinical evaluation, and deployment of computer assisted intervention solutions are manifold. Some of these challenges will be discussed in this chapter.
               Computer assistance in both surgical procedures and radiology interventions aim at augmenting the clinicians with the overall objective of providing better clinical outcome. Multimodal imaging, robotics, artificial intelligence, and augmented reality play a major role in computer assisted interventions. After a brief analysis of the state-of-the-art and practice in this field, we discuss the challenges in design and development, as well as translation and deployment of the technology, from research projects motivated by clinical needs to solutions routinely used within clinical setups. We also consider the required training of surgeons and the surgical team as a major component for smooth and successful translation. We present simulation as an important tool not only for the design and development of computer assisted intervention solutions but also in their fast and smooth translation into daily practice.",science
10.1016/B978-0-12-815956-9.00002-8,Book,Technology in Supply Chain Management and Logistics: Current Practice and Future Applications,scopus,2019-01-01,sciencedirect,Technologies in supply chain management and logistics,https://api.elsevier.com/content/abstract/scopus_id/85082571600,"Until recently, technology has been considered an enabler for improvements in underlying supply chain and logistics operations. However, recent trends in society and business, such as mobile computing, social media, and online retailing, have significantly changed almost every aspect of the supply chain and logistics landscape. In this chapter the following technologies were found to have a pervasive role in altering this landscape:
               
                  
                     
                        •
                        Maturing technologies
                     
                     
                        •
                        Optimization software
                     
                     
                        •
                        Sensors/Telematics
                     
                     
                        •
                        Cloud computing
                     
                     
                        •
                        Data warehouse and integration
                     
                     
                        •
                        Automated storage and retrieval
                     
                     
                        •
                        Growth technologies
                     
                     
                        •
                        Mobility
                     
                     
                        •
                        Wearability
                     
                     
                        •
                        Data analytics
                     
                     
                        •
                        Social media
                     
                     
                        •
                        Emerging technologies
                     
                     
                        •
                        3D printing
                     
                     
                        •
                        Drones
                     
                     
                        •
                        Autonomous vehicles
                     
                     
                        •
                        Exponential
                     
                     
                        •
                        Blockchain
                     
                     
                        •
                        Internet of Things
                     
                     
                        •
                        Virtual reality
                     
                     
                        •
                        Machine learning
                     
                  
               
               Each of these technologies will be discussed along with videos illustrating their use.",science
10.1016/j.procir.2019.05.017,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,The growing path in search of an industrial design identity,https://api.elsevier.com/content/abstract/scopus_id/85076752868,"Knowing that the education system must be reinvented periodically to face the changes of social and cultural paradigm, was reviewed the pedagogical organization of a set of disciplines of an industrial design course that were in operation for a decade. Thus, in view of the objective of restructuring the disciplinary group of industrial design, a new structure has been developed and implemented that could offer students the opportunity to explore problems and challenges that have real applications, increasing the possibility of acquiring competences effectively needed to practice the profession of designer.
                  This restructuring had as its starting point the concept of Project-based learning, which is designated as student-centered pedagogy that involves a dynamic classroom approach in which it is believed that students acquire a deeper knowledge through active exploration of real-world challenges and problems. Consequently, resulting in a learning process organized into levels with increasing degree of complexity. As well, different assimilations of markets and design scenarios.
                  Starting from the first year of the course, where students are still understanding the context of industrial design and its potentialities. At a time when their techniques, principles and methods are still very raw and basic. They are initiated in a LOW-ID and local industry context, to acquire basic skills. The second year allows embark on an intermediate level called MID-ID, with new skills in international brands approach. In the last year of the course the 3rd level is reached, HIGH-ID, with projects with the national industry.
                  The first year of implementation of this curriculum structure showed good results. Thus, favoring a solid interdisciplinary formation with, skills and competences that allow future designers to intervene creatively and competently in a variety of fields. This process allows to progress to the next academic degree to complete and validate the entire formation of the student.",science
10.1016/j.procs.2019.05.057,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Educating I-Shaped Computer Science Students to Become T-Shaped System Engineers,https://api.elsevier.com/content/abstract/scopus_id/85074996601,"With every passing day, software becomes more and more important to the success of the artifacts that we make, sell, buy, use, and evolve. Software increasingly provides a competitive differentiator for products, ways of tailoring them for various uses and users, and ways of fixing or evolving them without expensive product recalls.
                  Unfortunately, as software becomes more and more ubiquitous and complex, an increasing number of new computer science (CS) courses in web services, big-data analytics, computing security, and machine learning fill up CS students’ schedules, leaving little room for non-CS courses providing skills outside of CS. This paper summarizes our experiences in developing and evolving an MS-level software engineering (MSCS-SE) curriculum that takes I-shaped CS BA graduates and enables them to become sufficiently T-shaped to be able to immediately contribute to overall system definition and development on being hired, and to improve their T-shaped skills along their careers.
                  Section 2 summarizes the primary origins and problems with an I-shaped software workforce. Section 3 describes the origins, development, and evolution of the USC MSCS-Software Engineering program and its foundation-stone, real-client, 2-semester project course. Section 4 elaborates on the team-project course and its mechanisms for strengthening the transition from I-shaped to T-shaped systems thinking. Section 5 provides conclusions.",science
10.1016/j.entcs.2019.04.014,Journal,Electronic Notes in Theoretical Computer Science,scopus,2019-01-01,sciencedirect,IoT sensors in sea water environment: Ahoy! Experiences from a short summer trial,https://api.elsevier.com/content/abstract/scopus_id/85074842288,"IoT sensors for measuring various sea water parameters, are explored here, aiming towards an educational context, in order to lead to a deeper understanding of the use of aquatic environments as natural resources, and towards the adoption of environmentally friendly behaviors. Sea-water sensing via IoT has not been extensively explored, due to practical difficulties in deployment, and the same applies to devising appropriate scenaria for understanding aquatic parameters in STEM education. A short hands-on IoT sensing trial, that has been conducted in various location of the Aegean sea, is reported in this paper. This research set out to gain insight into real data sets on which to base observations for devising realistic educational scenaria pertaining aquatic parameters. The results of this experiment are meant to guide research further, by shedding light into the IoT sensing issues that are involved in an educational scientific context. The goal is conducting broader research in the area of IoT water sensing towards its further utilization in STEM education.",science
10.1016/j.entcs.2019.04.012,Journal,Electronic Notes in Theoretical Computer Science,scopus,2019-01-01,sciencedirect,An Augmented Reality Prototype for supporting IoT-based Educational Activities for Energy-efficient School Buildings,https://api.elsevier.com/content/abstract/scopus_id/85074700429,"The use of Augmented Reality (AR) technologies is currently being investigated in numerous and diverse application domains. In this work, we discuss the ways in which we are integrating AR into educational in-class activities for the GAIA project, aiming to enhance existing tools that target behavioral changes towards energy efficiency in schools. We combine real-time IoT data from a sensing infrastructure inside a fleet of school buildings with AR software running on tablets and smartphones, as companions to a set of educational lab activities aimed at promoting energy awareness in a STEM context. We also utilize this software as a means to ease access to IoT data and simplify device maintenance. We report on the design and current status of our implementation, describing functionality in the context of our target applications, while also relaying our experiences from the use of such technologies in this application domain.",science
10.1016/j.procs.2019.09.007,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Increase the interest in learning by implementing augmented reality: Case studies studying rail transportation.,https://api.elsevier.com/content/abstract/scopus_id/85073117730,"Learn a subject, for some people, might be an uninteresting and boring activity, especially when the subject to learn are difficult subjects to understand. Many methods used to change learning activities become more enjoyable and interested. This study proposed a new method in learning activities, by applied augmented reality technology in the learning process. The case study used in this paper are implementation the augmented reality in studied subjects related to train technology. In this study, author implement augmented reality on learning material, combines real and virtual things in one media, in this case a mobile device. The impact of implementation of augmented studied, at the end of experiment, author can conclude when implement augmented reality technology in learning material helps the learning process and increasing the impressive and fun factor in learning process and make the learning process more interested. Implementation of Augmented Reality in learning material gives more information about the object being studied, information about on shapes, textures, and provide more visualization for the object.",science
10.1016/j.promfg.2018.12.017,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,AI based injection molding process for consistent product quality,https://api.elsevier.com/content/abstract/scopus_id/85072584818,"In manufacturing processes, Injection Molding is widely used for producing plastic components with large lot size. So, continuous improvements in product quality consistency is crucial to maintaining a competitive edge in the injection molding industry. Various optimization techniques like ANN, GA, Iterative method, and simulation based are being used for optimization of Injection Molding process and obtaining optimal processing conditions. But still due to variation during molding cycles, quality failure occurs. As many constituents like process, Material, machine together yields product quality. This paper is focused on Real time AI based control of process parameters in injection molding cycle. Process parameters and their interrelationship with quality failure has been studied and later supposed to be used to generate algorithm for compensating the deviation of process parameters. Pressure and temperature sensor assisted monitoring system is used to collect data in real time and based on its comparison with the standard values an interrelationship is formed between parameters and plastic material properties. Algorithm generates new process parameter values to compensate the deviation and machine control follows the same. The entire process is supposed to be smart and automatic after being trained with AI and machine learning techniques. Simulation using Moldflow software and real industry collected data has been used for understanding whole molding process establishing relationship between failure and parameters. An automotive product in real industry is chosen for data acquisition, implementation and validation of entire AI based system.",science
10.1016/j.procir.2019.03.041,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,"Design, implementation and evaluation of reinforcement learning for an adaptive order dispatching in job shop manufacturing systems",https://api.elsevier.com/content/abstract/scopus_id/85068485505,"Modern production systems tend to have smaller batch sizes, a larger product variety and more complex material flow systems. Since a human oftentimes can no longer act in a sufficient manner as a decision maker under these circumstances, the demand for efficient and adaptive control systems is rising. This paper introduces a methodical approach as well as guideline for the design, implementation and evaluation of Reinforcement Learning (RL) algorithms for an adaptive order dispatching. Thereby, it addresses production engineers willing to apply RL. Moreover, a real-world use case shows the successful application of the method and remarkable results supporting real-time decision-making. These findings comprehensively illustrate and extend the knowledge on RL.",science
10.1016/j.jagp.2019.05.013,Journal,American Journal of Geriatric Psychiatry,scopus,2019-01-01,sciencedirect,A Future Research Agenda for Digital Geriatric Mental Healthcare,https://api.elsevier.com/content/abstract/scopus_id/85067070294,"The proliferation of mobile, online, and remote monitoring technologies in digital geriatric mental health has the potential to lead to the next major breakthrough in mental health treatments. Unlike traditional mental health services, digital geriatric mental health has the benefit of serving a large number of older adults, and in many instances, does not rely on mental health clinics to offer real-time interventions. As technology increasingly becomes essential in the everyday lives of older adults with mental health conditions, these technologies will provide a fundamental service delivery strategy to support older adults’ mental health recovery. Although ample research on digital geriatric mental health is available, fundamental gaps in the scientific literature still exist. To begin to address these gaps, we propose the following recommendations for a future research agenda: 1) additional proof-of-concept studies are needed; 2) integrating engineering principles in methodologically rigorous research may help science keep pace with technology; 3) studies are needed that identify implementation issues; 4) inclusivity of people with a lived experience of a mental health condition can offer valuable perspectives and new insights; and 5) formation of a workgroup specific for digital geriatric mental health to set standards and principles for research and practice. We propose prioritizing the advancement of digital geriatric mental health research in several areas that are of great public health significance, including 1) simultaneous and integrated treatment of physical health and mental health conditions; 2) effectiveness studies that explore diagnostics and treatment of social determinants of health such as “social isolation” and “loneliness;” and 3) tailoring the development and testing of innovative strategies to minority older adult populations.",science
10.1016/j.procs.2019.01.012,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Combining supervised and unsupervised machine learning algorithms to predict the learners' learning styles,https://api.elsevier.com/content/abstract/scopus_id/85062675875,"The implementation of an efficient adaptive e-learning system requires the construction of an effective student model that represents the student’s characteristics, among those characteristics, there is the learning style that refers to the way in which a student prefers to learn. Knowing learning styles helps adaptive E-learning systems to improve the learning process by providing customized materials to students. In this work, we have proposed an approach to identify the learning style automatically based on the existing learners’ behaviors and using web usage mining techniques and machine learning algorithms. The web usage mining techniques were used to pre-process the log file extracted from the E-learning environment and capture the learners’ sequences. The captured learners’ sequences were given as an input to the K-modes clustering algorithm to group them into 16 learning style combinations based on the Felder and Silverman learning style model. Then the naive Bayes classifier was used to predict the learning style of a student in real time. To perform our approach, we used a real dataset extracted from an e-learning system’s log file, and in order to evaluate the performance of the used classifier, the confusion matrix method was used. The obtained results demonstrate that our approach yields excellent results.",science
10.1016/j.impact.2018.12.001,Journal,NanoImpact,scopus,2019-01-01,sciencedirect,SUNDS probabilistic human health risk assessment methodology and its application to organic pigment used in the automotive industry,https://api.elsevier.com/content/abstract/scopus_id/85058641247,"The increasing use of engineered nanomaterials (ENMs) in nano-enabled products (NEPs) has raised societal concerns about their possible health and ecological implications. To ensure a high level of human and environmental protection it is essential to properly estimate the risks of these new materials and to develop adequate risk management strategies. To this end, we propose a quantitative Human Health Risk Assessment (HHRA) methodology, which was developed in the European Seventh Framework research project SUN (Sustainable Nanotechnologies) and implemented in the web-based SUN Decision Support System (SUNDS). One of the major strengths of this probabilistic approach as compared to its deterministic alternatives is its ability to clearly communicate the uncertainties in the estimated risks in order to support better risk communication for more objective decision making by industries and regulators.
                  To demonstrate this methodology, we applied it in a real case study involving a nanoscale organic red pigment used in the automotive industry. Our analysis clearly showed that the main source of uncertainty was the extrapolation from (sub)acute in vivo toxicity data to long-term risk. This extrapolation was necessary due to a lack of (sub)chronic in vivo studies for the investigated nanomaterial. Despite the high uncertainty in the final results due to the conservative assumptions made in the risks assessment, the estimated risks are acceptable for all investigated exposure scenarios along the product lifecycle.",science
10.1016/j.iree.2018.12.001,Journal,International Review of Economics Education,scopus,2019-01-01,sciencedirect,Active financial analysis: Stimulating engagement using Bloomberg for introductory finance students,https://api.elsevier.com/content/abstract/scopus_id/85058411728,"There is increasing interest in the adoption of real-world interactive and participative learning techniques within economics and finance teaching through the use of trading room software. Previous research suggests that the integration of trading room software can improve knowledge development and performance. However, the time constraints of providing software training and requirements for foundation knowledge of basic maths and economics has restricted the adoption of trading room software to advanced courses. This paper outlines how the Bloomberg Professional Software was used in an introductory finance course and analyses student engagement, learning and attainment using feedback and performance data. We find that students valued the novelty of Bloomberg as part of a mix of different learning activities which facilitated the practical application of theory. Results also indicate that the alignment of teaching, learning and assessment promotes deeper engagement, and is associated with higher attainment. We demonstrate that trading room software can be effectively used in introductory courses to enhance the student experience and deepen understanding.",science
10.1016/j.cmpb.2018.11.002,Journal,Computer Methods and Programs in Biomedicine,scopus,2019-01-01,sciencedirect,Predicting combinative drug pairs via multiple classifier system with positive samples only,https://api.elsevier.com/content/abstract/scopus_id/85056787891,"Background and Objective
                  Due to the synergistic effects of drugs, drug combination is one of the effective approaches for treating complex diseases. However, the identification of drug combinations by dose-response methods is still costly. It is promising to develop supervised learning-based approaches to predict potential drug combinations on a large scale. Nevertheless, these approaches have the inadequate utilization of heterogeneous features, which causes the loss of information useful to classification. Moreover, they have an intrinsic bias, because they assume unknown drug pairs as non-combinations, of which some could be real drug combinations in practice.
               
                  Methods
                  To address above issues, this work first designs a two-layer multiple classifier system (TLMCS) to effectively integrate heterogeneous features involving anatomical therapeutic chemical codes of drugs, drug-drug interactions, drug-target interactions, gene ontology of drug targets, and side effects. To avoid the bias caused by labelling unknown samples as negative, it then utilizes the one-class support vector machines, (which requires no negative instance and only labels approved drug combinations as positive instances), as the member classifiers in TLMCS. Last, both a 10-fold cross validation (10-CV) and a novel prediction are performed to validate the performance of TLMCS.
               
                  Results
                  The comparison with three state-of-the-art approaches under 10-CV exhibits the superiority of TLMCS, which achieves the area under the receiver operating characteristic curve = 0.824 and the area under the precision-recall curve = 0.372. Moreover, the experiment under the novel prediction demonstrates its ability, where 9 out of the top-20 predicted combinative drug pairs are validated by checking the published literature. Furthermore, for each of the newly-validated drug combinations, this work analyses the combining mode of the member drugs and investigates their relationship in terms of drug targeting pathways.
               
                  Conclusions
                  The proposed TLMCS provides an effective framework to integrate those heterogeneous features and is trained by only positive samples such that the bias of taking unknown drug pairs as negative samples can be avoided. Furthermore, its results in the novel prediction reveal five types of drug combinations and three types of drug relationships in terms of pathways.",science
10.1016/j.ins.2018.09.023,Journal,Information Sciences,scopus,2019-01-01,sciencedirect,On analyzing and evaluating privacy measures for social networks under active attack,https://api.elsevier.com/content/abstract/scopus_id/85053787383,"Widespread usage of complex interconnected social networks such as Facebook, Twitter and LinkedInin modern internet era has also unfortunately opened the door for privacy violation of users of such networks by malicious entities. In this article we investigate, both theoretically and empirically, privacy violation measures of large networks under active attacks that was recently introduced in Trujillo-Rasua and Yero (2016). Our theoretical result indicates that the network manager responsible for prevention of privacy violation must be very careful in designing the network if its topology does not contain a cycle. Our empirical results shed light on privacy violation properties of eight real social networks as well as a large number of synthetic networks generated by both the classical Erdös–Rényi model and the scale-free random networks generated by the Barábasi–Albert preferential-attachment model.",science
10.1016/j.jvs.2017.10.069,Journal,Journal of Vascular Surgery,scopus,2019-01-01,sciencedirect,Optimization of rifampin coating on covered Dacron endovascular stent grafts for infected aortic aneurysms,https://api.elsevier.com/content/abstract/scopus_id/85042558741,"Objective
                  In the treatment of an infected aorta, open repair and replacement with a rifampin-impregnated Dacron vascular graft decrease the risk of prosthetic graft infections, with several protocols available in the literature. We hypothesize that the same holds true for endovascular aneurysm repair, and after studying and optimizing rifampin solution concentration and incubation period to maximize the coating process of rifampin on Dacron endovascular stent grafts (ESGs), we propose a rapid real-time perioperative protocol.
               
                  Methods
                  Several prepared rifampin solutions, including a negative control solution, were used to coat multiple triplicate sets of Dacron endovascular aortic stent grafts at different but set incubation periods. Rifampin elution from the grafts was studied by spectroscopic analysis. Once an optimized solution concentration and incubation time were determined, the elution of rifampin over time from the graft and the graft's surface characteristics were studied by ultraviolet-visible spectroscopy and atomic force microscopy.
               
                  Results
                  All coated ESGs with any concentration of prepared rifampin solution, regardless of incubation time, immediately demonstrated a visible bright orange discoloration and subsequently after elution procedures returned to the original noncolored state. At the 25-minute incubation time (standard flush), there was no statistical difference in the amount of rifampin coated to the ESGs with 10-mg/mL, 30-mg/mL, and 60-mg/mL solutions (0.06 ± 0.01, 0.07 ± 0.05, and 0.044 ± 0.01, respectively; P > .05). This was also true for a 10-minute incubation time (express flush) of 10-mg/mL and 60-mg/mL rifampin solution concentrations (0.04 ± 0.007 and 0.066 ± 0.014, respectively; P = .22). The elution-over-time of coated rifampin ESG, although not statistically significant, did seem to plateau and to reach a steady state by 50 hours and was confirmed by surface characteristics using atomic force microscopy.
               
                  Conclusions
                  Having studied two variables of rifampin coating techniques to Dacron ESGs, the authors propose a rapid real-time perioperative coating protocol by using a 10-mg/mL rifampin solution for a 10-minute incubation period. As rifampin loosely binds to Dacron ESGs by weak intermolecular forces, a rifampin-coated ESG would need to be inserted in a timely fashion to treat the diseased aorta and to deliver its antibiotic affect. A rapid perioperative coating protocol followed by immediate deployment makes our proposed technique especially useful in an urgent and unstable clinical scenario.
               
                  Clinical Relevance
                  In the treatment of an infected aorta, open repair with rifampin-impregnated vascular grafts has been described to minimize the risk of prosthetic graft infections, with several protocols available in the literature. We hypothesize that the same holds true for endovascular aneurysm repair. After studying and optimizing several variables to maximize the coating process of rifampin on Dacron endovascular stent grafts (ESGs), we propose a standardized rapid real-time perioperative protocol especially useful in an urgent and unstable clinical scenario. To further provide greater personalized patient care, future directions to ESG coating may include coating with a broader spectrum antibiotic (eg, piperacillin and tazobactam) as well as testing of antibiotic-coated ESG elution properties in vivo with animal models.",science
10.1016/j.jbiotec.2018.10.003,Journal,Journal of Biotechnology,scopus,2018-12-20,sciencedirect,Monitoring of antibody-drug conjugation reactions with UV/Vis spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85055668476,"The conjugation reaction of monoclonal antibodies (mAbs) with small-molecule drugs is a central step during production of antibody-drug conjugates (ADCs). The ability to monitor this step in real time can be advantageous for process understanding and control. Here, we propose a method based on UV/Vis spectroscopy in conjunction with partial least squares (PLS) regression for non-invasive monitoring of conjugation reactions. In experiments, the method was applied to conjugation reactions with two surrogate drugs in microplate format as well as at 20 ml scale. All calibrated PLS models performed well in cross-validation (
                        
                           Q
                           2
                        
                        >
                        0.975
                      for all models). In microplate format, the PLS models were furthermore successfully validated with an independent prediction set (
                        
                           R
                           
                              p
                              r
                              e
                              d
                           
                           2
                        
                         
                        =
                         
                        0.9770
                      resp. 0.8940). In summary, the proposed method provides a quick and easily implementable tool for reaction monitoring of ADC conjugation reactions and may in the future support the implementation of Process Analytical Technologies (PAT).",science
10.1016/j.pmcj.2018.10.008,Journal,Pervasive and Mobile Computing,scopus,2018-12-01,sciencedirect,Asfault: A low-cost system to evaluate pavement conditions in real-time using smartphones and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85055864820,"Modern smartphones have a large variety of built-in sensors that can measure different information about users and the environment around them. Given the increasing popularity of these devices, their high processing power, and the ability to transfer data over wireless networks, different smartphone-based applications have emerged in the last years to solve old problems with new approaches more efficiently and cheaply. One example is the assessment and monitoring of asphalt quality. This task has been done manually by experts since the 1930s, and with the help of expensive equipment since the 1960s. Currently, we are experiencing the emergence of next-generation tools to perform this monitoring with smartphones, significantly reducing costs, time, and effort of experts. However, there is a trade-off between the costs and precision of smartphone sensors, requiring the use of sophisticated software solutions. In this paper, we propose Asfault, a low-cost system to evaluate and monitor road pavement conditions in real-time using smartphone sensors and machine learning algorithms. The system is composed of an Android application responsible for doing automatic evaluations and a web application that aims to show the evaluations in an informative way. We propose to employ accelerometer sensors to measure the vehicle vibration while driving and use this data to evaluate the pavement conditions. Asfault achieves a classification performance superior to 90% in a 5-class problem considering the following road qualities: Good, Average, Fair, and Poor, as well the occurrence of obstacles in the road. Our system is publicly available for use and could be useful for practitioners responsible for urban and highway maintenance, as well for regular drivers in the planning of better routes based on the pavement quality and comfort of the travel.",science
10.1016/j.cag.2018.10.004,Journal,Computers and Graphics (Pergamon),scopus,2018-12-01,sciencedirect,Simulating complex social behaviours of virtual agents through case-based planning,https://api.elsevier.com/content/abstract/scopus_id/85055350464,"In commercial video games and simulations, non-player characters are capable of quite complex behaviour. Very often though, each class of non-player characters (that we further call virtual agents) is manually programmed or scripted. This means that instead of possessing some level of intelligence, allowing the agent to decide dynamically on the actions it needs to perform, we supply the agent with a list of possible situations that may arise in the game. For each such situation, we give the agent a pre-programmed script that tells it how to behave. Producing such scripts for every role an agent might play in a game or simulation is a very costly exercise. This may be acceptable in commercial game development, where budgets for modern games are sometimes comparable to budgets of Hollywood movies, but not adequate for research simulations and indie games. In this paper, we discuss how indie games and research simulations can be enriched with the sophisticated social behaviour of virtual agents in a semi-automatic manner through the use of AI planning. By supplying agents with roles and developing a computational model of their needs, we can use AI planning (also known as dynamic planning) to increase the complexity of agent behaviour dramatically and at the same time achieve a high degree of automation and reduce the development costs. AI planning is gaining popularity in games development, but it is often discarded due to performance issues. We will show how to improve the performance of planning process through the use of dynamic institutions and case-based planning. We will illustrate the aforementioned ideas on an example of developing a Virtual Reality simulation of everyday life in Ancient Mesopotamia.",science
10.1016/j.asoc.2018.09.035,Journal,Applied Soft Computing Journal,scopus,2018-12-01,sciencedirect,Data-driven MIMO model-free reference tracking control with nonlinear state-feedback and fractional order controllers,https://api.elsevier.com/content/abstract/scopus_id/85054906128,"In this paper we suggest an extension of the Virtual Reference Feedback Tuning (VRFT) framework to nonlinear state-feedback and fractional order (FO) controllers. Theoretical analysis incentivizes the use of VRFT for tuning general nonlinear controllers to achieve model reference matching because it is expected that the more complex controller parameterization of the nonlinear-state-feedback and FO controllers leads to improved control performance. Key factors needed for successful controller tuning are discussed: good exploration of process dynamics depending on careful input excitation signal selection, the influence of the controller parameterization and the selection of the reference model. VRFT is next applied to a Multi Input-Multi Output (MIMO) nonlinear coupled vertical tank system as a case study, to tune MIMO proportional–integral (PI), fractional order-proportional–integral (FO-PI) and neural network state-feedback controllers. PI and FO-PI controllers are tuned in continuous time but implemented in discrete time to enable their real-world applications. Controllers’ complexity vs. control performance trade-off is revealed. For comparisons purposes, an original combination of VRFT and Batch Fitted Q-Learning is employed as a two-step model-free controller tuning procedure for dramatic performance improvement.",science
10.1016/j.future.2018.07.013,Journal,Future Generation Computer Systems,scopus,2018-12-01,sciencedirect,Using behavioral features in tablet-based auditory emotion recognition studies,https://api.elsevier.com/content/abstract/scopus_id/85050510126,"The recognition of emotions in spoken words is one of the most important aspects in human communication and social relationships. Traditional approaches to the study of vocal emotional recognition involve instructing listeners to choose which one of several words describing emotion categories best characterize linguistically neutral utterances or vocalizations uttered by actors portraying various emotional states. To this end, generic experiment control software is usually used, which has some disadvantages. In this paper, we present a system that digitalizes the whole process involved in understanding how people perceive and understand vocal emotions, improving data collection, processing and analysis. Moreover, this system provides a new group of features that allows a more comprehensive characterization of the behavioral dimension underlying vocal emotional recognition. In this paper we describe this system and analyze the relationship between emotional perception, gender, age and Human–Computer Interaction.",science
10.1016/j.neucom.2018.08.009,Journal,Neurocomputing,scopus,2018-11-17,sciencedirect,Evaluation of deep neural networks for traffic sign detection systems,https://api.elsevier.com/content/abstract/scopus_id/85052108235,"Traffic sign detection systems constitute a key component in trending real-world applications, such as autonomous driving, and driver safety and assistance. This paper analyses the state-of-the-art of several object-detection systems (Faster R-CNN, R-FCN, SSD, and YOLO V2) combined with various feature extractors (Resnet V1 50, Resnet V1 101, Inception V2, Inception Resnet V2, Mobilenet V1, and Darknet-19) previously developed by their corresponding authors. We aim to explore the properties of these object-detection models which are modified and specifically adapted to the traffic sign detection problem domain by means of transfer learning. In particular, various publicly available object-detection models that were pre-trained on the Microsoft COCO dataset are fine-tuned on the German Traffic Sign Detection Benchmark dataset. The evaluation and comparison of these models include key metrics, such as the mean average precision (mAP), memory allocation, running time, number of floating point operations, number of parameters of the model, and the effect of traffic sign image sizes. Our findings show that Faster R-CNN Inception Resnet V2 obtains the best mAP, while R-FCN Resnet 101 strikes the best trade-off between accuracy and execution time. YOLO V2 and SSD Mobilenet merit a special mention, in that the former achieves competitive accuracy results and is the second fastest detector, while the latter, is the fastest and the lightest model in terms of memory consumption, making it an optimal choice for deployment in mobile and embedded devices.",science
10.1016/j.neucom.2018.07.044,Journal,Neurocomputing,scopus,2018-11-13,sciencedirect,Detection of spam-posting accounts on Twitter,https://api.elsevier.com/content/abstract/scopus_id/85052064942,"Online Social Media platforms, such as Facebook and Twitter, enable all users, independently of their characteristics, to freely generate and consume huge amounts of data. While this data is being exploited by individuals and organisations to gain competitive advantage, a substantial amount of data is being generated by spam or fake users. One in every 200 social media messages and one in every 21 tweets is estimated to be spam. The rapid growth in the volume of global spam is expected to compromise research works that use social media data, thereby questioning data credibility. Motivated by the need to identify and filter out spam contents in social media data, this study presents a novel approach for distinguishing spam vs. non-spam social media posts and offers more insight into the behaviour of spam users on Twitter. The approach proposes an optimised set of features independent of historical tweets, which are only available for a short time on Twitter. We take into account features related to the users of Twitter, their accounts and their pairwise engagement with each other. We experimentally demonstrate the efficacy and robustness of our approach and compare it to a typical feature set for spam detection in the literature, achieving a significant improvement on performance. In contrast to prior research findings, we observe that an average automated spam account posted at least 12 tweets per day at well defined periods. Our method is suitable for real-time deployment in a social media data collection pipeline as an initial preprocessing strategy to improve the validity of research data.",science
10.1016/j.heliyon.2018.e00972,Journal,Heliyon,scopus,2018-11-01,sciencedirect,Modeling the output power of heterogeneous photovoltaic panels based on artificial neural networks using low cost microcontrollers,https://api.elsevier.com/content/abstract/scopus_id/85057181952,"Many implementations of artificial neural networks have been reported in scientific papers. However, few of these implementations allow the direct use of off-line trained networks. Moreover, no implementation reported the use of relatively small network adequate to run on low cost microcontroller. Hence, this work, which presents a small artificial neural network, which models the output power of heterogeneous photovoltaic panel. In addition, the work discuss the hardware implementation that allows such network to run on low cost microcontroller. The hardware implementation has the ability to model heterogeneous photovoltaic panel's output power with very high accuracy and fast response time. Feedforward back propagation has been used because of its high resolution and accurate activation function. Real-time measured parameters can be used as inputs for the developed system. The resulting hardware data is tested with data from real photovoltaic panels; to confirm that it can efficiently implement the models prepared off-line with Matlab. The comparison revealed the robustness of the proposed heterogeneous photovoltaic model system at different conditions. The proposed heterogeneous photovoltaic model system offer a proper and efficient tool that can be used in monitoring photovoltaic panels, such as the ones used in smart-house applications.",science
10.1016/j.measurement.2018.05.099,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-11-01,sciencedirect,Parallel three-dimensional electrical capacitance data imaging using a nonlinear inversion algorithm and L<sup>p</sup> norm-based model regularization,https://api.elsevier.com/content/abstract/scopus_id/85049483981,"In order to improve image reconstructions, different classes of nonlinear inversion algorithms are developed and used in different research topics like imaging processes in oil industry or the characterization of complex porous media or multiphase flows. These algorithms are able to avoid local minima and to reach more adapted minima of a given misfit function between observed/measured and computed data. Techniques as different as electrical, ultrasound or potential methods, are used. We present here a nonlinear algorithm that allows us to produce permittivity images by using electrical capacitance tomography (ECT). ECT is a non-invasive technique to image non-conductive permittivity distributions and is used in many oil industry imaging applications such as multiphase flows in pipelines, fluidized bed reactors, mixing vessels, and tanks of phase separation. Even if the ECT technique provides low resolution reconstructions, it is cheap, robust and very fast when compared to other imaging tools. In this method one or more rings of electrodes excite a medium to be imaged at high frequencies, and more particularly at frequencies for which a static electrical potential field has fully developed. In many studies of other research groups only one ring of sources is introduced but the reconstruction accuracy was not totally satisfactory due to the 3D nature of the problem to be solved. Instead of using nonlinear stochastic algorithms like the simulated annealing (SA) technique that we optimized in previous studies to image permittivity distributions of granular or solid materials as well as real oil–gas or two-phase flows in 2D cylindrical vessel configurations, we propose here a new ECT inversion tool to image permittivities in a 3D cylindrical configuration. 3D stochastic optimization methods such as SA, neural networks, genetic algorithms can become computationally too prohibitive, and classical local or linear inversion methods excessively smooth images in many cases. Therefore, we propose here a 3D parallel inversion procedure with different numbers of rings and different 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                      norms, with
                        
                           1
                           <
                           p
                           ⩽
                           2
                        
                     , applied to the model regularization of the misfit function to increase the resolution of the models after inversion. We are able to better reconstruct two-phase and three-phase (oil, gas and solids) mixtures by combining 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                     -norm regularizations of the misfit function to minimize and several rings of electrodes. All these algorithms have been implemented in a more general parallel framework TOMOFAST-X designed for multi-physics joint inversion purposes, and could also be used in other fields of research such as larger-scale geophysical exploration for instance.",science
10.1016/j.mfglet.2018.09.002,Journal,Manufacturing Letters,scopus,2018-10-01,sciencedirect,Industrial Artificial Intelligence for industry 4.0-based manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85053749537,"The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.",science
10.1016/j.jcms.2018.07.006,Journal,Journal of Cranio-Maxillofacial Surgery,scopus,2018-10-01,sciencedirect,Accuracy in orthognathic surgery─comparison of preoperative plan and postoperative outcome using computer-assisted two-dimensional cephalometry by the Onyx Ceph <sup>®</sup> system,https://api.elsevier.com/content/abstract/scopus_id/85052740333,"Purpose
                  This retrospective study analyzes deviations between preoperative planning and postoperative outcome in orthognathic surgery using 2D Onyx Ceph®-cephalometric analyzing and planning system.
               
                  Materials and methods
                  A total of 100 patients with a mean age 25.1 of years were included in this study. In 33 patients a bilateral sagittal split osteotomy and in seven patients a Le Fort I osteotomy was performed. A total of 60 patients were treated by a bimaxillary approach. Onyx Ceph® was used as cephalometric planning software (Onyx Ceph®), followed by mock operations. Postoperative cephalograms were obtained after 3.3 days and compared to preoperative planning cephalograms for sagittal (SNA, SNB, ANB) and vertical (ArGoMe, ML-NSL, NL-NSL) angle measurements. Real and absolute mean deviation were documented.
               
                  Results
                  Absolute mean deviation (degrees) between postoperative and planned jaw movement was lower for the sagittal parameters SNA (0.58), SNB (1.15) and ANB (1.05) compared to the vertical parameters NL-NSL (1.47), ML-NSL (1.96) and ArGoMe (3.20). SNA, SNB and ANB showed constant deviations independent from the extent of jaw movement. With regard to the vertical parameters ML-NSL, ArGoMe and NL-NSL the extent of the postoperative rotational jaw movement was not as much as planned, particularly for vertical shifts of more than 4°.
               
                  Conclusion
                  By using the 2D Onyx Ceph® cephalometric software for orthognathic surgery, the deviations between planned and actual movements are within an acceptable and predictable range. Planning of extensive vertical alterations may result in greater deviations after surgery.",science
10.1016/j.jmgm.2018.07.007,Journal,Journal of Molecular Graphics and Modelling,scopus,2018-10-01,sciencedirect,Proposing novel TNFα direct inhibitor Scaffolds using fragment-docking based e-pharmacophore modeling and binary QSAR-based virtual screening protocols pipeline,https://api.elsevier.com/content/abstract/scopus_id/85052113589,"Tumor necrosis factor alpha (TNFα) is a homotrimer protein that plays a pivotal role for critical immune functions, including infection, inflammation and antitumor responses. It also plays a primary role in autoimmune diseases like rheumatoid arthritis (RA). So far, only biological therapeutics like infliximab, etanercept, and adalimumab are available as treatment of inflammatory diseases. They directly bind to TNFα and interrupt its binding to its receptor protein tumor necrosis factor receptor (TNFR). However, they may also cause serious side effects such as activating an autoimmune anti-antibody response or the weakening of the body's immune defenses. Thus, small molecule-based therapies can be considered as alternative methods. In this study, a novel method is applied to develop energetically optimized, structure-based pharmacophore models for rapid in silico drug screening. Fragment-based docking results were used in the construction of an universal e-pharmacophore model development. The developed model is then used for screening of small-molecule library Specs-screening compounds (Specs-SC) which includes more than 200.000 drug-like molecules. In another approach, binary QSAR-based models were used to screen Specs-SC, as well as Specs-natural products (NP) which has around 750 compounds, and a library of drugs registered or approved for use in humans NIH's NCGC pharmaceutical collection (NPC) which has around 7500 molecules. The MetaCore/MetaDrug platform was used for binary QSAR models for therapeutic activity prediction as well as pharmacokinetic and toxicity profile predictions of screening molecules. This platform is constructed based on a manually curated database of molecular interactions, molecular pathways, gene-disease associations, chemical metabolism, and toxicity information. Molecular docking and molecular dynamics (MD) simulations were performed for the selected hit molecules. As target protein, both homodimer and homotrimer forms of TNFα were considered. The screening results showed that indinavir and medroxalol from NPC chemical library and a set of compounds (AT-057/43115940, AP-970/42897107, AK-968/41925665, AI-204/31679053, AN-648/41666950, AN-698/42006940) from Specs-SC database were identified as safe and active direct inhibitors of TNFα.",science
10.1016/j.ins.2018.07.064,Journal,Information Sciences,scopus,2018-10-01,sciencedirect,Identifying advisor-advisee relationships from co-author networks via a novel deep model,https://api.elsevier.com/content/abstract/scopus_id/85050884529,"Advisor-advisee is one of the most important relationships in research publication networks. Identifying it can benefit many interesting applications, such as double-blind peer review, academic circle mining, and scientific community analysis. However, the advisor-advisee relationships are often hidden in research publication network and vary over time, thus are difficult to detect. In this paper, we present a time-aware Advisor-advisee Relationship Mining Model (tARMM) to better identify such relationships. It is a deep model equipped with improved Refresh Gate Recurrent Units (RGRU). Extensive experiments over real-world DBLP data have well verified the effectiveness of our proposed model.",science
10.1016/j.cell.2018.08.028,Journal,Cell,scopus,2018-09-20,sciencedirect,Intelligent Image-Activated Cell Sorting,https://api.elsevier.com/content/abstract/scopus_id/85054484004,"A fundamental challenge of biology is to understand the vast heterogeneity of cells, particularly how cellular composition, structure, and morphology are linked to cellular physiology. Unfortunately, conventional technologies are limited in uncovering these relations. We present a machine-intelligence technology based on a radically different architecture that realizes real-time image-based intelligent cell sorting at an unprecedented rate. This technology, which we refer to as intelligent image-activated cell sorting, integrates high-throughput cell microscopy, focusing, and sorting on a hybrid software-hardware data-management infrastructure, enabling real-time automated operation for data acquisition, data processing, decision-making, and actuation. We use it to demonstrate real-time sorting of microalgal and blood cells based on intracellular protein localization and cell-cell interaction from large heterogeneous populations for studying photosynthesis and atherothrombosis, respectively. The technology is highly versatile and expected to enable machine-based scientific discovery in biological, pharmaceutical, and medical sciences.",science
10.1016/j.jgg.2018.08.002,Journal,Journal of Genetics and Genomics,scopus,2018-09-20,sciencedirect,CGPS: A machine learning-based approach integrating multiple gene set analysis tools for better prioritization of biologically relevant pathways,https://api.elsevier.com/content/abstract/scopus_id/85054137776,"Gene set enrichment (GSE) analyses play an important role in the interpretation of large-scale transcriptome datasets. Multiple GSE tools can be integrated into a single method as obtaining optimal results is challenging due to the plethora of GSE tools and their discrepant performances. Several existing ensemble methods lead to different scores in sorting pathways as integrated results; furthermore, it is difficult for users to choose a single ensemble score to obtain optimal final results. Here, we develop an ensemble method using a machine learning approach called Combined Gene set analysis incorporating Prioritization and Sensitivity (CGPS) that integrates the results provided by nine prominent GSE tools into a single ensemble score (R score) to sort pathways as integrated results. Moreover, to the best of our knowledge, CGPS is the first GSE ensemble method built based on a priori knowledge of pathways and phenotypes. Compared with 10 widely used individual methods and five types of ensemble scores from two ensemble methods, we demonstrate that sorting pathways based on the R score can better prioritize relevant pathways, as established by an evaluation of 120 simulated datasets and 45 real datasets. Additionally, CGPS is applied to expression data involving the drug panobinostat, which is an anticancer treatment against multiple myeloma. The results identify cell processes associated with cancer, such as the p53 signaling pathway (hsa04115); by contrast, according to two ensemble methods (EnrichmentBrowser and EGSEA), this pathway has a rank higher than 20, which may cause users to miss the pathway in their analyses. We show that this method, which is based on a priori knowledge, can capture valuable biological information from numerous types of gene set collections, such as KEGG pathways, GO terms, Reactome, and BioCarta. CGPS is publicly available as a standalone source code at ftp://ftp.cbi.pku.edu.cn/pub/CGPS_download/cgps-1.0.0.tar.gz.",science
10.1016/j.chroma.2018.06.035,Journal,Journal of Chromatography A,scopus,2018-09-14,sciencedirect,Tandem column isolation of zirconium-89 from cyclotron bombarded yttrium targets using an automated fluidic platform: Anion exchange to hydroxamate resin columns,https://api.elsevier.com/content/abstract/scopus_id/85051041242,"The development of a tandem column purification method for the preparation of high-purity 89Zr(IV) oxalate is presented. The primary column was a macroporous strongly basic anion exchange resin on styrene divinylbenzene co-polymer. The secondary column, with an internal volume of 33 μL, was packed with hydroxamate resin. A condition of inverted selectivity was developed, whereby the 89Zr eluent solution for the primary column is equivalent to the 89Zr load solution for the secondary column. The ability to transfer 89Zr from one column to the next allows two sequential column clean-up methods to be performed prior to the final elution of the 89Zr(IV) oxalate. This approach assures delivery of high purity 89Zr product and assures a 89Zr product that is eluted in a substantially smaller volume than is possible when using the traditionally-employed single hydroxamate resin column method. The tandem column purification process has been implemented into a prototype automated fluidic system. The system is configured with on-line gamma detection so column effluents can be monitored in near-real time. The automated method was tested using seven cyclotron bombarded Y foil targets. It was found that 95.1 ± 1.3% of the 89Zr present in the foils was recovered in the secondary column elution fraction. Furthermore, elution peak analysis of several 89Zr elution profile radiochromatograms made possible the determination of 89Zr recovery as a function of volume; a 89Zr product volume that contains 90% of the mean secondary column elution peak can be obtained in 0.29 ± 0.06 mL (representing 86 ± 5% of the 89Zr activity in the target). This product volume represents a significant improvement in radionuclide product concentration over the predominant method used in the field. In addition to the reduced 89Zr product elution volume, titrations of the 89Zr product with deferoxamine mesylate salt across two preparatory methods resulted in mean effective specific activity (ESA) values of 279 and 340 T Bq·mmole−1 and mean bindable metals concentrations ([MB]) of 13.5 and 16.7 nmole·g−1. These ESA and [MB] values infer that the 89Zr(IV) oxalate product resulting from this tandem column isolation method has the highest purity reported to date.",science
10.1016/j.cmpb.2018.05.029,Journal,Computer Methods and Programs in Biomedicine,scopus,2018-09-01,sciencedirect,Estimating the refractive index of oxygenated and deoxygenated hemoglobin using genetic algorithm – support vector regression model,https://api.elsevier.com/content/abstract/scopus_id/85048604435,"Background and objectives
                  The refractive index of hemoglobin plays important role in hematology due to its strong correlation with the pathophysiology of different diseases. Measurement of the real part of the refractive index remains a challenge due to strong absorption of the hemoglobin especially at relevant high physiological concentrations. So far, only a few studies on direct measurement of refractive index have been reported and there are no firm agreements on the reported values of refractive index of hemoglobin due to measurement artifacts. In addition, it is time consuming, laborious and expensive to perform several experiments to obtain the refractive index of hemoglobin. In this work, we proposed a very rapid and accurate computational intelligent approach using Genetic Algorithm/Support Vector Regression models to estimate the real part of the refractive index for oxygenated and deoxygenated hemoglobin samples.
               
                  Methods
                  These models utilized experimental data of wavelengths and hemoglobin concentrations in building highly accurate Genetic Algorithm/Support Vector Regression model (GA-SVR).
               
                  Results
                  The developed methodology showed high accuracy as indicated by the low root mean square error values of 4.65 × 10−4 and 4.62 × 10−4 for oxygenated and deoxygenated hemoglobin, respectively. In addition, the models exhibited 99.85 and 99.84% correlation coefficients (r) for the oxygenated and deoxygenated hemoglobin, thus, validating the strong agreement between the predicted and the experimental results
               
                  Conclusions
                  Due to the accuracy and relative simplicity of the proposed models, we envisage that these models would serve as important references for future studies on optical properties of blood.",science
10.1016/j.cmpb.2018.06.002,Journal,Computer Methods and Programs in Biomedicine,scopus,2018-09-01,sciencedirect,Fuzzy decision support systems to diagnose musculoskeletal disorders: A systematic literature review,https://api.elsevier.com/content/abstract/scopus_id/85048589929,"Background and objective
                  Musculoskeletal disorders (MSDs) are one of the most important causes of disability with a high prevalence. The accurate and timely diagnosis of these disorders is often difficult. Clinical decision support systems (CDSSs) can help physicians to diagnose diseases quickly and accurately. Given the ambiguous nature of MSDs, fuzzy logic can be helpful in designing the CDSSs knowledge bases. The present study aimed to review the studies on fuzzy CDSSs to diagnose MSDs.
               
                  Methods
                  A comprehensive search was conducted in Medline, Scopus, Cochrane Library, and ISI Web of Science databases to identify relevant studies published until March 15, 2016. Studies were included in which CDSSs were developed using fuzzy logic to diagnose MSDs, and tested their accuracy using real data from patients.
               
                  Results
                  Of the 3188 papers examined, 23 papers included according to the inclusion criteria. The results showed that among all the designed CDSSs only one (CADIAG-2) was implemented in the clinical environment. In about half of the included studies (52%), CDSSs were designed to diagnose inflammatory/infectious disorder of the bone and joint. In most of the included studies (70%), the knowledge was extracted using a combination of three methods (acquiring from experts, analyzing the data, and reviewing the literature). The median accuracy of fuzzy rule-based CDSSs was 91% and it was 90% for other fuzzy models. The most frequently used membership functions were triangular and trapezoidal functions, and the most used method for inference was the Mamdani.
               
                  Conclusions
                  In general, fuzzy CDSSs have a high accuracy to diagnose MSDs. Despite the high accuracy, these systems have been used to a limited extent in the clinical environments. To design of knowledge base for CDSSs to diagnose MSDs, rule-based methods are used more than other fuzzy methods.",science
10.1016/j.diin.2018.05.004,Journal,Digital Investigation,scopus,2018-09-01,sciencedirect,Laying foundations for effective machine learning in law enforcement. Majura – A labelling schema for child exploitation materials,https://api.elsevier.com/content/abstract/scopus_id/85047981760,"The health impacts of repeated exposure to distressing concepts such as child exploitation materials (CEM, aka ‘child pornography’) have become a major concern to law enforcement agencies and associated entities. Existing methods for ‘flagging’ materials largely rely upon prior knowledge, whilst predictive methods are unreliable, particularly when compared with equivalent tools used for detecting ‘lawful’ pornography. In this paper we detail the design and implementation of a deep-learning based CEM classifier, leveraging existing pornography detection methods to overcome infrastructure and corpora limitations in this field. Specifically, we further existing research through direct access to numerous contemporary, real-world, annotated cases taken from Australian Federal Police holdings, demonstrating the dangers of overfitting due to the influence of individual users' proclivities. We quantify the performance of skin tone analysis in CEM cases, showing it to be of limited use. We assess the performance of our classifier and show it to be sufficient for use in forensic triage and ‘early warning’ of CEM, but of limited efficacy for categorising against existing scales for measuring child abuse severity.
                  We identify limitations currently faced by researchers and practitioners in this field, whose restricted access to training material is exacerbated by inconsistent and unsuitable annotation schemas. Whilst adequate for their intended use, we show existing schemas to be unsuitable for training machine learning (ML) models, and introduce a new, flexible, objective, and tested annotation schema specifically designed for cross-jurisdictional collaborative use.
                  This work, combined with a world-first ‘illicit data airlock’ project currently under construction, has the potential to bring a ‘ground truth’ dataset and processing facilities to researchers worldwide without compromising quality, safety, ethics and legality.",science
10.1016/j.chroma.2018.06.016,Journal,Journal of Chromatography A,scopus,2018-08-24,sciencedirect,Development and optimization of a solid-phase microextraction gas chromatography–tandem mass spectrometry methodology to analyse ultraviolet filters in beach sand,https://api.elsevier.com/content/abstract/scopus_id/85048474681,"A methodology based on solid-phase microextraction (SPME) followed by gas chromatography–tandem mass spectrometry (GC–MS/MS) has been developed for the simultaneous analysis of eleven multiclass ultraviolet (UV) filters in beach sand. To the best of our knowledge, this is the first time that this extraction technique is applied to the analysis of UV filters in sand samples, and in other kind of environmental solid samples. Main extraction parameters such as the fibre coating, the amount of sample, the addition of salt, the volume of water added to the sand, and the temperature were optimized. An experimental design approach was implemented in order to find out the most favourable conditions. The final conditions consisted of adding 1 mL of water to 1 g of sample followed by the headspace SPME for 20 min at 100 °C, using PDMS/DVB as fibre coating. The SPME-GC–MS/MS method was validated in terms of linearity, accuracy, limits of detection and quantification, and precision. Recovery studies were also performed at three concentration levels in real Atlantic and Mediterranean sand samples. The recoveries were generally above 85% and relative standard deviations below 11%. The limits of detection were in the pg g−1 level. The validated methodology was successfully applied to the analysis of real sand samples collected from Atlantic Ocean beaches in the Northwest coast of Spain and Portugal, Canary Islands (Spain), and from Mediterranean Sea beaches in Mallorca Island (Spain). The most frequently found UV filters were ethylhexyl salicylate (EHS), homosalate (HMS), 4-methylbenzylidene camphor (4MBC), 2-ethylhexyl methoxycinnamate (2EHMC) and octocrylene (OCR), with concentrations up to 670 ng g−1.",science
10.1016/j.scs.2018.05.043,Journal,Sustainable Cities and Society,scopus,2018-08-01,sciencedirect,Can HVAC really learn from users? A simulation-based study on the effectiveness of voting for comfort and energy use optimization,https://api.elsevier.com/content/abstract/scopus_id/85048481957,"The usage of Building Automation Systems (BAS) and Energy Management Systems (EMS) is indeed becoming ever more common and sophisticated, and seeking to promote energy savings by integrating new sources of data, such as user preferences, in real-time. This paper reviews the existing systems and proposes an innovation in HVAC systems management: a system that tracks the occupants’ preferences, learns from them, and manages HVAC automatically. Our hypothesis was that by developing a learning system based on feedback acquired through the mobile devices of room occupants to optimize the control of a HVAC system, in order to minimize energy consumption while maximizing average user comfort.
                  A prototype solution is described and evaluated by simulation. We show that ambient intelligent systems can be used to control a building’s EMS, effectively reducing energy consumption while maintaining acceptable comfort levels. Our results indicate that employing a k-means machine learning technique enables the automatic configuration of an HVAC system to reduce energy consumption while keeping the majority of occupants within acceptable comfort levels. The developed prototype provides occupants with feedback on ambient variables on a mobile user interface. © 2017 Elsevier Science. All rights reserved.",science
10.1016/j.ins.2018.04.071,Journal,Information Sciences,scopus,2018-08-01,sciencedirect,Filtering out the noise in short text topic modeling,https://api.elsevier.com/content/abstract/scopus_id/85046762548,"Nowadays, massive short texts, such as social media posts and newspaper titles, are available on the Internet. Analyzing these short texts is very significant for many content analysis tasks. However, the commonly used text analysis tools, i.e., topic models, lose effectiveness on short texts because of the sparsity and noise problems. Recent topic models mainly attempt to solve the sparsity problem, but neglect the noise issue. To address this, we propose a common semantics topic model (CSTM) in this paper. The key idea is to introduce a new type of topic, namely common topic, to gather the noise words. The experimental results on real-world datasets indicate that our CSTM outperforms the existing short text topic models on the traditional tasks.",science
10.1016/j.ijantimicag.2018.02.018,Journal,International Journal of Antimicrobial Agents,scopus,2018-07-01,sciencedirect,Identification and bioevaluation of SRI-12742 as an antimicrobial agent against multidrug-resistant Acinetobacter baumannii,https://api.elsevier.com/content/abstract/scopus_id/85048519265,"Multidrug-resistant Acinetobacter baumannii (MDR-Ab) is one of the most significant nosocomial pathogens that is being increasingly isolated in healthcare settings worldwide. Owing to its inherent drug-resistant nature, coupled with its ability to readily acquire resistance to other antibiotic classes, there is a real dearth of antibiotics available to treat infections with MDR-Ab. A commercially available library was screened against MDR-Ab BAA-1605 to identify novel inhibitory molecules. The selectivity index of a hit was tested against Vero cells and in vitro efficacy was profiled against a panel of clinical MDR-Ab. The bacteriostatic or bactericidal nature was determined by time–kill experiments, and synergy with clinically approved drugs was determined by the chequerboard method. Additionally, in vivo efficacy was measured in a murine neutropenic A. baumannii thigh infection model. SRI-12742 was identified as a potent active hit, with a minimum inhibitory concentration (MIC) of 4 mg/L against BAA-1605. Its activity was then profiled against a MDR-Ab clinical strain panel (MICs 4 mg/L to >64 mg/L). SRI-12742 exhibited concentration-dependent bactericidal activity and caused an ca. 16 log10 CFU/mL reduction at 10 × MIC in 24 h, which is comparable with minocycline. In a murine neutropenic thigh infection model of A. baumannii infection, SRI-12742 reduced CFU counts by ca. 0.9 log10 CFU, which is comparable with polymyxin B. In addition, SRI-12742 synergised with all classes of antibiotics tested. SRI-12742 exhibits all of the criteria necessary to be positioned as a novel lead with potential to be deployed for the treatment of infections caused by MDR-Ab.",science
10.1016/j.yofte.2018.05.004,Journal,Optical Fiber Technology,scopus,2018-07-01,sciencedirect,Fiber Bragg grating based temperature profiling in ferromagnetic nanoparticles-enhanced radiofrequency ablation,https://api.elsevier.com/content/abstract/scopus_id/85047266101,"In this work, we report the real-time temperature profiling performed with a fiber Bragg grating (FBG) sensing system, applied to a ferromagnetic nanoparticles (NP)-enhanced radiofrequency ablation (RFA) for interventional cancer care. A minimally invasive RFA setup has been prepared and applied ex vivo on a liver phantom; NPs (with concentrations of 5 and 10 mg/mL) have been synthesized and injected within the tissue prior to ablation, in order to facilitate the heat distribution to the peripheral sides of the treated tissue. A network of 15 FBG sensors has been deployed in situ in order to detect the parenchymal temperature distribution and estimate the thermal profiles in real time during the ablation, highlighting the impact of the NPs on the RFA mechanism. The results confirm that NP-enhanced ablation with 5 mg/mL density shows a better heat penetration that a standard RFA achieving an almost double-sized lesion, while a higher density (10 mg/mL) does not improve the heat distribution. Thermal data are reported highlighting both spatial and temporal gradients, evaluating the capability of NPs to deliver sufficient heating to the peripheral sides of the tumor borders.",science
10.1016/j.ijmedinf.2018.04.003,Journal,International Journal of Medical Informatics,scopus,2018-07-01,sciencedirect,Mortality prediction system for heart failure with orthogonal relief and dynamic radius means,https://api.elsevier.com/content/abstract/scopus_id/85045708416,"Objective
                  This paper constructs a mortality prediction system based on a real-world dataset. This mortality prediction system aims to predict mortality in heart failure (HF) patients. Effective mortality prediction can improve resources allocation and clinical outcomes, avoiding inappropriate overtreatment of low-mortality patients and discharging of high-mortality patients. This system covers three mortality prediction targets: prediction of in-hospital mortality, prediction of 30-day mortality and prediction of 1-year mortality.
               
                  Materials and methods
                  HF data are collected from the Shanghai Shuguang hospital. 10,203 in-patients records are extracted from encounters occurring between March 2009 and April 2016. The records involve 4682 patients, including 539 death cases. A feature selection method called Orthogonal Relief (OR) algorithm is first used to reduce the dimensionality. Then, a classification algorithm named Dynamic Radius Means (DRM) is proposed to predict the mortality in HF patients.
               
                  Results and discussions
                  The comparative experimental results demonstrate that mortality prediction system achieves high performance in all targets by DRM. It is noteworthy that the performance of in-hospital mortality prediction achieves 87.3% in AUC (35.07% improvement). Moreover, the AUC of 30-day and 1-year mortality prediction reach to 88.45% and 84.84%, respectively. Especially, the system could keep itself effective and not deteriorate when the dimension of samples is sharply reduced.
               
                  Conclusions
                  The proposed system with its own method DRM can predict mortality in HF patients and achieve high performance in all three mortality targets. Furthermore, effective feature selection strategy can boost the system. This system shows its importance in real-world applications, assisting clinicians in HF treatment by providing crucial decision information.",science
10.1016/j.knosys.2018.03.016,Journal,Knowledge-Based Systems,scopus,2018-07-01,sciencedirect,Enhancing user creativity: Semantic measures for idea generation,https://api.elsevier.com/content/abstract/scopus_id/85044259734,"Human creativity generates novel ideas to solve real-world problems. This thereby grants us the power to transform the surrounding world and extend our human attributes beyond what is currently possible. Creative ideas are not just new and unexpected, but are also successful in providing solutions that are useful, efficient and valuable. Thus, creativity optimizes the use of available resources and increases wealth. The origin of human creativity, however, is poorly understood, and semantic measures that could predict the success of generated ideas are currently unknown. Here, we analyze a dataset of design problem-solving conversations in real-world settings by using 49 semantic measures based on WordNet 3.1 and demonstrate that a divergence of semantic similarity, an increased information content, and a decreased polysemy predict the success of generated ideas. The first feedback from clients also enhances information content and leads to a divergence of successful ideas in creative problem solving. These results advance cognitive science by identifying real-world processes in human problem solving that are relevant to the success of produced solutions and provide tools for real-time monitoring of problem solving, student training and skill acquisition. A selected subset of information content (IC Sánchez–Batet) and semantic similarity (Lin/Sánchez–Batet) measures, which are both statistically powerful and computationally fast, could support the development of technologies for computer-assisted enhancements of human creativity or for the implementation of creativity in machines endowed with general artificial intelligence.",science
10.1016/j.lfs.2018.04.041,Journal,Life Sciences,scopus,2018-06-15,sciencedirect,miRNA-150-5p associate with antihypertensive effect of epigallocatechin-3-gallate revealed by aorta miRNome analysis of spontaneously hypertensive rat,https://api.elsevier.com/content/abstract/scopus_id/85046167889,"Aims
                  The antihypertensive mechanism (s) of the epigallocatechin-3-gallate (EGCG), a major effective component in green tea, might associate with microRNAs (miRNAs). Here, we aimed to investigate which microRNA in aorta of spontaneously hypertensive rats (SHRs) were modulated by administration of EGCG and its mechanism.
               
                  Main methods
                  The pharmacokinetic behaviors of EGCG and epigallocatechin (EGC) in Sprague-Dawley rats were analyzed by HPLC and DRUG AND STATISTICS software. Blood pressure of SHRs was monitored by the tail-cuff method, the miRNomes of aorta from SHRs was analyzed with deep sequencing, and expression of hypertension-associated miRNAs with significant change and their host genes and target genes were validated by real-time PCR and Western blot.
               
                  Key findings
                  The plasma deposition of EGCG and EGC best fitted a mono-compartmental model with maximum plasma concentration post-dose (Cmax, 6.65 vs 4.45 μg/ml) and the corresponding time (Tmax, 15 vs 10 min). Systolic blood pressure (SBP) of SHRs decreased to the lowest point by 34.04 mmHg and recovered by 23.39 mmHg after 15 and 30 min of administration at dose of 300 mg/kg BW EGCG, respectively, and it decreased again at 60 min and recovered at time 2 h. Total 35 upregulated and 18 downregulated miRNAs were identified compared to the control group (p < .01) after EGCG administration. Expression of hypertension-associated miRNA-126a-3p and miRNA-150-5p were further validated. In turn, their host gene and target genes were up-regulated and down-regulated, respectively.
               
                  Significance
                  Our results indicated that miRNA-150-5p might be involved in the antihypertensive effect of EGCG through SP1/AT1R pathway.",science
10.1016/j.transci.2018.05.004,Journal,Transfusion and Apheresis Science,scopus,2018-06-01,sciencedirect,Artificial intelligence: A joint narrative on potential use in pediatric stem and immune cell therapies and regenerative medicine,https://api.elsevier.com/content/abstract/scopus_id/85047095825,"Artificial Intelligence (AI) reflects the intelligence exhibited by machines and software. It is a highly desirable academic field of many current fields of studies. Leading AI researchers describe the field as “the study and design of intelligent agents”. McCarthy invented this term in 1955 and defined it as “the science and engineering of making intelligent machines”. The central goals of AI research are reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects. In fact the multidisplinary AI field is considered to be rather interdisciplinary covering numerous number of sciences and professions, including computer science, psychology, linguistics, philosophy and neurosciences. The field was founded on the claim that a central intellectual property of humans, intelligence-the sapience of Homo Sapiens “can be so precisely described that a machine can be made to simulate it”. This raises philosophical issues about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence. Artificial Intelligence has been the subject of tremendous optimism but has also suffered stunning setbacks.
                  The goal of this narrative is to review the potential use of AI approaches and their integration into pediatric cellular therapies and regenerative medicine. Emphasis is placed on recognition and application of AI techniques in the development of predictive models for personalized treatments with engineered stem cells, immune cells and regenerated tissues in adults and children. These intelligent machines could dissect the whole genome and isolate the immune particularities of individual patient’s disease in a matter of minutes and create the treatment that is customized to patient’s genetic specificity and immune system capability. AI techniques could be used for optimization of clinical trials of innovative stem cell and gene therapies in pediatric patients by precise planning of treatments, predicting clinical outcomes, simplifying recruitment and retention of patients, learning from input data and applying to new data, thus lowering their complexity and costs. Complementing human intelligence with machine intelligence could have an exponentially high impact on continual progress in many fields of pediatrics. However how long before we could see the real impact still remains the big question. The most pertinent question that remains to be answered therefore, is can AI effectively and accurately predict properties of newer DDR strategies?
                  The goal of this article is to review the use of AI method for cellular therapy and regenerative medicine and emphasize its potential to further the progress in these fields of medicine.",science
10.1016/j.dib.2018.02.075,Journal,Data in Brief,scopus,2018-06-01,sciencedirect,Neuro-fuzzy inference system Prediction of stability indices and Sodium absorption ratio in Lordegan rural drinking water resources in west Iran,https://api.elsevier.com/content/abstract/scopus_id/85044120214,"According to World Health Organization guidelines, corrosion control is an important aspect of safe drinking-water supplies. Water always includes ingredients, dissolved gases and suspended materials. Although some of these water ingredients is indispensable for human beings, these elements more than permissible limits, could be endanger human health. The aim of this study is to assess physical and chemical parameters of drinking water in the rural areas of Lordegan city, also to determine corrosion indices. This cross-sectional study has carried out with 141 taken samples during 2017 with 13 parameters, which has been analyzed based on standard method and to estimate the water quality indices from groundwater using ANFIS. Also with regard to standard conditions, results of this paper are compared with Environmental Protection Agency and Iran national standards. Five indices, Ryznar Stability Index (RSI), Langlier Saturation Index (LSI), Larson-Skold Index (LS), Puckorius Scaling Index (PSI), and Aggressive Index (AI) programmed by using Microsoft Excel software. Owing to its simplicity, the program, can easily be used by researchers and operators. Parameters included Sulfate, Sodium, Chloride, and Electrical Conductivity respectively were 13.5, 28, 10.5, and 15% more than standard level. The amount of Nitrate, in 98% of cases were in permissible limits and about 2% were more than standard level. Result of presented research indicate that water is corrosive at 10.6%,89.4%,87.2%,59.6% and 14.9% of drinking water supply reservoirs, according to LSI, RSI, PSI, LS and AI, respectively.",science
10.1016/j.knosys.2018.03.005,Journal,Knowledge-Based Systems,scopus,2018-06-01,sciencedirect,Unsupervised geographically discriminative feature learning for landmark tagging,https://api.elsevier.com/content/abstract/scopus_id/85043325506,"Recently, a large number of geo-tagged landmark images have been uploaded through various social media services. Usually, these geo-tagged images are annotated by users with GPS and tags related to the landmarks where they are taken. Landmark tagging aims to automatically annotate an image with the tags to describe the landmark where the image is taken. It has been observed that the images and tags show strong correlation with the geographical locations. The widely used assumption by many existing tagging methods is that images are independently and identically distributed is not effective to capture the geographical correlation. In this paper, we study the novel problem of utilizing the geographical correlation among images and landmarks for better tagging landmark images. In particular, we propose an unsupervised feature learning approach to learn the geographically discriminative features across geographical locations, by integrating latent space learning and geographically structural analysis (LSGSA) into a joint model. A latent space learning model is proposed to effectively fuse the heterogeneous features of visual content and tags. Meanwhile, the geographical structure analysis and group sparsity are applied to learn the geographically discriminative features. Then, a geo-guided sparse reconstruction method is proposed to tag images by utilizing the discriminative information of features, in which the landmark-specific tags are boosted by a weighting method. Experiments on the real-world datasets demonstrate the superiority of our approach.",science
10.1016/j.measurement.2018.02.060,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-06-01,sciencedirect,Development of a novel machine vision procedure for rapid and non-contact measurement of soil moisture content,https://api.elsevier.com/content/abstract/scopus_id/85042874145,"Soil moisture measurement is one of the essential management components to decrease water consumption and prevent water stresses in plants. In this study, a fast and non-contact method using machine vision and artificial intelligence was developed so as to make operators capable of having an estimate of soil moisture by taking only one image. Three soil textures along with three levels of added organic matter were applied. Mean comparison and the subsequent stepwise multiple regression were applied to find superior features from different color spaces. ANFIS and stepwise multiple regression were used to predict the soil moisture. Results indicated that the general model could predict the soil moisture with mean absolute error of less than 1.1%. This value reached to 0.3% for some sub-models belonging to the texture–organic matter group. Application of the present method is highly recommended for soil moisture measurement because of simple implementation and potential for online measurements.",science
10.1016/j.ijmedinf.2018.01.015,Journal,International Journal of Medical Informatics,scopus,2018-06-01,sciencedirect,Humanitarian health computing using artificial intelligence and social media: A narrative literature review,https://api.elsevier.com/content/abstract/scopus_id/85040945850,"Introduction
                  According to the World Health Organization (WHO), over 130 million people are in constant need of humanitarian assistance due to natural disasters, disease outbreaks, and conflicts, among other factors. These health crises can compromise the resilience of healthcare systems, which are essential for achieving the health objectives of the sustainable development goals (SDGs) of the United Nations (UN). During a humanitarian health crisis, rapid and informed decision making is required. This is often challenging due to information scarcity, limited resources, and strict time constraints. Moreover, the traditional approach to digital health development, which involves a substantial requirement analysis, a feasibility study, and deployment of technology, is ill-suited for many crisis contexts. The emergence of Web 2.0 technologies and social media platforms in the past decade, such as Twitter, has created a new paradigm of massive information and misinformation, in which new technologies need to be developed to aid rapid decision making during humanitarian health crises.
               
                  Objective
                  Humanitarian health crises increasingly require the analysis of massive amounts of information produced by different sources, such as social media content, and, hence, they are a prime case for the use of artificial intelligence (AI) techniques to help identify relevant information and make it actionable. To identify challenges and opportunities for using AI in humanitarian health crises, we reviewed the literature on the use of AI techniques to process social media.
               
                  Methodology
                  We performed a narrative literature review aimed at identifying examples of the use of AI in humanitarian health crises. Our search strategy was designed to get a broad overview of the different applications of AI in a humanitarian health crisis and their challenges. A total of 1459 articles were screened, and 24 articles were included in the final analysis.
               
                  Results
                  Successful case studies of AI applications in a humanitarian health crisis have been reported, such as for outbreak detection. A commonly shared concern in the reviewed literature is the technical challenge of analyzing large amounts of data in real time. Data interoperability, which is essential to data sharing, is also a barrier with regard to the integration of online and traditional data sources.
                  Human and organizational aspects that might be key factors for the adoption of AI and social media remain understudied. There is also a publication bias toward high-income countries, as we identified few examples in low-income countries. Further, we did not identify any examples of certain types of major crisis, such armed conflicts, in which misinformation might be more common.
               
                  Conclusions
                  The feasibility of using AI to extract valuable information during a humanitarian health crisis is proven in many cases. There is a lack of research on how to integrate the use of AI into the work-flow and large-scale deployments of humanitarian aid during a health crisis.",science
10.1016/j.asoc.2017.05.039,Journal,Applied Soft Computing Journal,scopus,2018-06-01,sciencedirect,Teranga Go!: Carpooling Collaborative Consumption Community with multi-criteria hesitant fuzzy linguistic term set opinions to build confidence and trust,https://api.elsevier.com/content/abstract/scopus_id/85020448290,"Boosting collaborative or participatory consumption is a priority for the European Commission. It is in line with the provisions of the Europe 2020 Strategy, which proposes that consumption of goods and services should take place in accordance with smart, sustainable and inclusive growth. These have motivated us to develop an online community for collaborative consumption centered in the Senegalese community that travels by car from Europe to Africa named Teranga Go!. Carpooling relationships are based on the sense of a real existing community, social experiences among users, and connection through technology, where confidence is the key concept. To help creating values of confidence, trust and safety among the members of the Teranga Go! community, we have implemented an intelligent decision support system in the platform based on computing with words. The participants of a carpooling experience act as experts that assess the driver aptitudes and determine, together with the history of the driver, a linguistic value for the driver's karma which represents the collective opinion of people that have traveled with the driver. The karma is a public label attached to the site user profiles. A Multi-Expert Multi-Criteria Decision Making model is applied using Hesitant Fuzzy Linguistic Terms to represent the expert opinions.",science
10.1016/j.jtbi.2018.02.015,Journal,Journal of Theoretical Biology,scopus,2018-05-14,sciencedirect,An approach for reduction of false predictions in reverse engineering of gene regulatory networks,https://api.elsevier.com/content/abstract/scopus_id/85042729548,"A gene regulatory network discloses the regulatory interactions amongst genes, at a particular condition of the human body. The accurate reconstruction of such networks from time-series genetic expression data using computational tools offers a stiff challenge for contemporary computer scientists. This is crucial to facilitate the understanding of the proper functioning of a living organism. Unfortunately, the computational methods produce many false predictions along with the correct predictions, which is unwanted. Investigations in the domain focus on the identification of as many correct regulations as possible in the reverse engineering of gene regulatory networks to make it more reliable and biologically relevant. One way to achieve this is to reduce the number of incorrect predictions in the reconstructed networks. In the present investigation, we have proposed a novel scheme to decrease the number of false predictions by suitably combining several metaheuristic techniques. We have implemented the same using a dataset ensemble approach (i.e. combining multiple datasets) also. We have employed the proposed methodology on real-world experimental datasets of the SOS DNA Repair network of Escherichia coli and the IMRA network of Saccharomyces cerevisiae. Subsequently, we have experimented upon somewhat larger, in silico networks, namely, DREAM3 and DREAM4 Challenge networks, and 15-gene and 20-gene networks extracted from the GeneNetWeaver database. To study the effect of multiple datasets on the quality of the inferred networks, we have used four datasets in each experiment. The obtained results are encouraging enough as the proposed methodology can reduce the number of false predictions significantly, without using any supplementary prior biological information for larger gene regulatory networks. It is also observed that if a small amount of prior biological information is incorporated here, the results improve further w.r.t. the prediction of true positives.",science
10.1016/j.jneumeth.2017.07.020,Journal,Journal of Neuroscience Methods,scopus,2018-04-15,sciencedirect,Automated face recognition of rhesus macaques,https://api.elsevier.com/content/abstract/scopus_id/85026478684,"Background
                  Rhesus macaques are widely used in biomedical research. Automated behavior monitoring can be useful in various fields (including neuroscience), as well as having applications to animal welfare but current technology lags behind that developed for other species. One difficulty facing developers is the reliable identification of individual macaques within a group especially as pair- and group-housing of macaques becomes standard. Current published methods require either implantation or wearing of a tracking device.
               
                  New method
                  I present face recognition, in combination with face detection, as a method to non-invasively identify individual rhesus macaques in videos. The face recognition method utilizes local-binary patterns in combination with a local discriminant classification algorithm.
               
                  Results
                  A classification accuracy of between 90 and 96% was achieved for four different groups. Group size, number of training images and challenging image conditions such as high contrast all had an impact on classification accuracy. I demonstrate that these methods can be applied in real time using standard affordable hardware and a potential application to studies of social structure.
               
                  Comparison with existing method(s)
                  Face recognition methods have been reported for humans and other primate species such as chimpanzees but not rhesus macaques. The classification accuracy with this method is comparable to that for chimpanzees. Face recognition has the advantage over other methods for identifying rhesus macaques such as tags and collars of being non-invasive.
               
                  Conclusions
                  This is the first reported method for face recognition of rhesus macaques, has high classification accuracy and can be implemented in real time.",science
10.1016/j.energy.2018.01.159,Journal,Energy,scopus,2018-04-01,sciencedirect,Multiobjective optimization of ethylene cracking furnace system using self-adaptive multiobjective teaching-learning-based optimization,https://api.elsevier.com/content/abstract/scopus_id/85041748366,"The ethylene cracking furnace system is crucial for an olefin plant. Multiple cracking furnaces are used to convert various hydrocarbon feedstocks to smaller hydrocarbon molecules, and the operational conditions of these furnaces significantly influence product yields and fuel consumption. This paper develops a multiobjective operational model for an industrial cracking furnace system that describes the operation of each furnace based on current feedstock allocations, and uses this model to optimize two important and conflicting objectives: maximization of key products yield, and minimization of the fuel consumed per unit ethylene. The model incorporates constraints related to material balance and the outlet temperature of transfer line exchanger. The self-adaptive multiobjective teaching-learning-based optimization algorithm is improved and used to solve the designed multiobjective optimization problem, obtaining a Pareto front with a diverse range of solutions. A real industrial case is investigated to illustrate the performance of the proposed model: the set of solutions returned offers a diverse range of options for possible implementation, including several solutions with both significant improvement in product yields and lower fuel consumption, compared with typical operational conditions.",science
10.1016/j.cptl.2017.12.018,Journal,Currents in Pharmacy Teaching and Learning,scopus,2018-04-01,sciencedirect,Utilizing desirable difficulties for sterile compounding training in a skills-based laboratory course,https://api.elsevier.com/content/abstract/scopus_id/85040361754,"Background and purpose
                  Sterile compounding skills are essential components of a professional pharmacy curriculum. The theory of desirable difficulties has been used to facilitate deeper learning of material in other disciplines, but has not been described in pharmacy sterile compounding instruction. The purpose of this work was to evaluate whether challenges introduced in sterile compounding would act as desirable difficulties and result in greater student confidence in their sterile compounding competency.
               
                  Educational activity and setting
                  Students in the fourth semester of Pharmacy Skills and Applications, a laboratory-based skills course, were presented with challenges in sterile compounding and were asked to complete a questionnaire rating their confidence and describing their experience.
               
                  Findings
                  The majority (92.8%) of students reported that the activity increased their confidence in their sterile compounding skills. Students’ open-ended responses suggested that most of the knowledge gained was strategic in nature.
               
                  Discussion
                  The results of this activity met the instructors’ initial goals by positively impacting students’ confidence in their ability to overcome challenges with sterile products compounding. Course instructors may explore additional skills in which to introduce desirable difficulties in order to build student confidence.
               
                  Summary
                  Course instructors were pleased with the implementation and results of this desirable difficulties activity and plan to continue its use again in future semesters. Incorporating more real-world challenges throughout the skills-lab course may be beneficial to student learning and confidence. With thoughtful planning, faculty at other institutions can readily incorporate similar activities within their own courses.",science
10.1016/j.knosys.2018.01.006,Journal,Knowledge-Based Systems,scopus,2018-04-01,sciencedirect,Who to select: Identifying critical sources in social sensing,https://api.elsevier.com/content/abstract/scopus_id/85039997946,"Social sensing has emerged as a new data collection paradigm in networked sensing applications where humans are used as “sensors” to report their observations about the physical world. While many previous studies in social sensing focus on the problem of ascertaining the reliability of data sources and the correctness of their reported claims (often known as truth discovery), this paper investigates a new problem of critical source selection. The goal of this problem is to identify a subset of critical sources that can help effectively reduce the computational complexity of the original truth discovery problem and improve the accuracy of the analysis results. In this paper, we propose a new scheme, Critical Source Selection (CSS), to find the critical set of sources by explicitly exploring both dependency and speak rate of sources. We evaluated the performance of our scheme and compared it to the state-of-the-art baselines using two data traces collected from a real world social sensing application. The results showed that our scheme significantly outperforms the baselines by finding more truthful information at a higher speed.",science
10.1016/j.ins.2017.04.045,Journal,Information Sciences,scopus,2018-04-01,sciencedirect,SRMCS: A semantic-aware recommendation framework for mobile crowd sensing,https://api.elsevier.com/content/abstract/scopus_id/85018303999,"With the rapidly growing number of wireless sensors and mobile phones, massive amounts of semantic-aware sensory data are being produced at every moment. Meanwhile, an overload problem has been generated in insular systems and has caused semantic-aware information to be ineffectively utilised. A central challenge for cognitive information processing is to design a scheme that can dispatch semantic-aware information to the appropriate context and to push it to users who are truly interested. This study proposes a framework for semantic-aware recommendations for mobile crowd sensing (SRMCS), which focuses on the relationships among sensory information rather than the information itself, and is based on a multi-layer graphic model (MLGM) that we propose in this study. We leverage semantic relatedness from different dimensions to construct contexts, allowing semantic-aware information to disperse into different relative contexts to obtain a better perception of the real world for the users. We implement the ability to push semantic-aware information for recommendations in terms of individual preferences, demands and social relationships, allowing the users to obtain more valuable semantic-aware information. The users can design different evaluation mechanisms with the objective of applying the framework to different occasions and to evaluate its effect. The framework that we propose is applied to one recommendation test study; the evaluation demonstrates the power of its reasonableness and effectiveness.",science
10.1016/j.jappgeo.2018.01.006,Journal,Journal of Applied Geophysics,scopus,2018-03-01,sciencedirect,Inferring the most probable maps of underground utilities using Bayesian mapping model,https://api.elsevier.com/content/abstract/scopus_id/85044639536,"Mapping the Underworld (MTU), a major initiative in the UK, is focused on addressing social, environmental and economic consequences raised from the inability to locate buried underground utilities (such as pipes and cables) by developing a multi-sensor mobile device. The aim of MTU device is to locate different types of buried assets in real time with the use of automated data processing techniques and statutory records. The statutory records, even though typically being inaccurate and incomplete, provide useful prior information on what is buried under the ground and where. However, the integration of information from multiple sensors (raw data) with these qualitative maps and their visualization is challenging and requires the implementation of robust machine learning/data fusion approaches. An approach for automated creation of revised maps was developed as a Bayesian Mapping model in this paper by integrating the knowledge extracted from sensors raw data and available statutory records. The combination of statutory records with the hypotheses from sensors was for initial estimation of what might be found underground and roughly where. The maps were (re)constructed using automated image segmentation techniques for hypotheses extraction and Bayesian classification techniques for segment-manhole connections. The model consisting of image segmentation algorithm and various Bayesian classification techniques (segment recognition and expectation maximization (EM) algorithm) provided robust performance on various simulated as well as real sites in terms of predicting linear/non-linear segments and constructing refined 2D/3D maps.",science
10.1016/j.diin.2017.12.003,Journal,Digital Investigation,scopus,2018-03-01,sciencedirect,Criminal motivation on the dark web: A categorisation model for law enforcement,https://api.elsevier.com/content/abstract/scopus_id/85041635143,"Research into the nature and structure of ‘Dark Webs’ such as Tor has largely focused upon manually labelling a series of crawled sites against a series of categories, sometimes using these labels as a training corpus for subsequent automated crawls. Such an approach is adequate for establishing broad taxonomies, but is of limited value for specialised tasks within the field of law enforcement. Contrastingly, existing research into illicit behaviour online has tended to focus upon particular crime types such as terrorism. A gap exists between taxonomies capable of holistic representation and those capable of detailing criminal behaviour. The absence of such a taxonomy limits interoperability between agencies, curtailing development of standardised classification tools.
                  We introduce the Tor-use Motivation Model (TMM), a two-dimensional classification methodology specifically designed for use within a law enforcement context. The TMM achieves greater levels of granularity by explicitly distinguishing site content from motivation, providing a richer labelling schema without introducing inefficient complexity or reliance upon overly broad categories of relevance. We demonstrate this flexibility and robustness through direct examples, showing the TMM's ability to distinguish a range of unethical and illegal behaviour without bloating the model with unnecessary detail.
                  The authors of this paper received permission from the Australian government to conduct an unrestricted crawl of Tor for research purposes, including the gathering and analysis of illegal materials such as child pornography. The crawl gathered 232,792 pages from 7651 Tor virtual domains, resulting in the collation of a wide spectrum of materials, from illicit to downright banal. Existing conceptual models and their labelling schemas were tested against a small sample of gathered data, and were observed to be either overly prescriptive or vague for law enforcement purposes - particularly when used for prioritising sites of interest for further investigation.
                  In this paper we deploy the TMM by manually labelling a corpus of over 4000 unique Tor pages. We found a network impacted (but not dominated) by illicit commerce and money laundering, but almost completely devoid of violence and extremism. In short, criminality on this ‘dark web’ is based more upon greed and desire, rather than any particular political motivations.",science
10.1016/j.ssci.2017.10.020,Journal,Safety Science,scopus,2018-03-01,sciencedirect,Safety engineering of computational cognitive architectures within safety-critical systems,https://api.elsevier.com/content/abstract/scopus_id/85034114889,"This paper presents the integration of a cognitive architecture with an intelligent decision support model (IDSM) that is embedded into an autonomous non-deterministic safety critical system. The IDSM will integrate multi-criteria decision making via intelligent technologies like expert systems, fuzzy logic, machine learning and genetic algorithms.
                  Cognitive technology is currently simulated in safety–critical systems to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves a system’s cognitive performance. In this study, the IDSM is being applied to an actual safety–critical system, an unmanned surface vehicle (USV) with embedded artificial intelligence (AI) software. The USV’s safety performance is being researched in a simulated and a real world nautical based environment. The objective is to build a dynamically changing model to evaluate a cognitive architecture’s ability to ensure safe performance of an intelligent safety–critical system. The IDSM does this by finding a set of key safety performance parameters that can be critiqued via safety measurements, mechanisms and methodologies. The uniqueness of this research will be on bounding the decision making associated with the cognitive architecture’s key safety parameters (KSP).
                  Other real-time applications that could benefit from advancing the safety of cognitive technologies are unmanned platforms, transportation technologies, and service robotics. The results will provide cognitive science researchers a reference for safety engineering artificially intelligent safety–critical systems.",science
10.1016/j.inffus.2017.05.003,Journal,Information Fusion,scopus,2018-03-01,sciencedirect,A Social-aware online short-text feature selection technique for social media,https://api.elsevier.com/content/abstract/scopus_id/85019594702,"Large-scale text categorisation in social environments, characterised by the high dimensionality of feature spaces, is one of the most relevant problems in machine learning and data mining nowadays. Short-texts, which are posted at unprecedented rates, accentuate both the importance of learning tasks and the challenges posed by such large feature space. A collection of social media short-texts does not only provide textual information but also topological information given by the relationships between posts and their authors. The linked nature of social data causes new complementary data dimensions to be added to the feature space, which, at the same time, becomes sparser. Additionally, in the context of social media, posts usually arrive simultaneously in streams, which hinders the deployment of efficient traditional feature selection techniques that assume a feature space fully known in advance. Hence, efficient and scalable online feature selection becomes an important requirement in numerous large-scale social applications. This work presents an online feature selection technique for high-dimensional data based on the integration of two information sources, social and content-based, for the real-time classification of short-text streams coming from social media. It focuses on discovering implicit relations amongst new posts, already known ones and their corresponding authors to identify groups of socially related posts. Then, each discovered group is represented by a set of non-redundant and relevant textual features. Finally, such features are used to train different learning models for classifying newly arriving posts. Extensive experiments conducted on real-world short-texts demonstrate that the proposed approach helps to improve classification results when compared to state-of-the-art and traditional online feature selection techniques.",science
10.1016/j.ecolind.2017.11.031,Journal,Ecological Indicators,scopus,2018-02-01,sciencedirect,Multiclass classification methods in ecology,https://api.elsevier.com/content/abstract/scopus_id/85036460929,"Multiclass classification refers to the construction of a model able to classify a response variable that can take more than two classes. Most ecological indices are naturally multiclass (e.g. water quality index: bad, regular, good) and the generation of models able to predict the output class in novel situations is required. In this study, we introduce seven representative multiclass classification techniques, classic and more recent, their rationale, advantages, disadvantages and a practical R code to implement them. These methods are: (1) Linear discriminant analysis (LDA), (2) a consensus of binomial logistic regression (CLR), (3) multinomial regression (MNR) and (4) support vector machine (SVM), (5) Classification and Regression Trees (CART), (6) Random Forest (RF) and (7) Stagewise Additive Modelling using a Multi-class Exponential (SAMME) loss function. We showed their implementation under simulated and a real data set to classify phytoplankton organisms into morphology-based functional groups. Results suggest that the nature of the data (i.e. linear vs non-linear) influence the predictive ability of multi-class classification models. Real phytoplankton data was accurately classified (error<0.05) by RF, SAMME and CLR, while SVM and CART were close to nominal 0.05 and LDA performed worst, with the higher error rate (ca. 0.7). The expected behaviour of the response variable should be considered when choosing a model for multiclass classification. The use of the generalization error allows to objectively rank among competing models. We showed the differences in interpretability among models, which is critical to decipher causal relationships among variables or to design management plans. We hope this article contributes to increase the use of these techniques, some of them flexible and distribution-free, and to improve the quality of multiclass classification models applied to ecological problems.",science
10.1016/j.ins.2017.10.041,Journal,Information Sciences,scopus,2018-02-01,sciencedirect,A Q-learning-based memetic algorithm for multi-objective dynamic software project scheduling,https://api.elsevier.com/content/abstract/scopus_id/85032349573,"Software project scheduling is the problem of allocating employees to tasks in a software project. Due to the large scale of current software projects, many studies have investigated the use of optimization algorithms to find good software project schedules. However, despite the importance of human factors to the success of software projects, existing work has considered only a limited number of human properties when formulating software project scheduling as an optimization problem. Moreover, the changing environments of software companies mean that software project scheduling is a dynamic optimization problem. However, there is a lack of effective dynamic scheduling approaches to solve this problem. This work proposes a more realistic mathematical model for the dynamic software project scheduling problem. This model considers that skill proficiency can improve over time and, different from previous work, it considers that such improvement is affected by the employees’ properties of motivation and learning ability, and by the skill difficulty. It also defines the objective of employees’ satisfaction with the allocation. It is considered together with the objectives of project duration, cost, robustness and stability under a variety of practical constraints. To adapt schedules to the dynamically changing software project environments, a multi-objective two-archive memetic algorithm based on Q-learning (MOTAMAQ) is proposed to solve the problem in a proactive-rescheduling way. Different from previous work, MOTAMAQ learns the most appropriate global and local search methods to be used for different software project environment states by using Q-learning. Extensive experiments on 18 dynamic benchmark instances and 3 instances derived from real-world software projects were performed. A comparison with seven other meta-heuristic algorithms shows that the strategies used by our novel approach are very effective in improving its convergence performance in dynamic environments, while maintaining a good distribution and spread of solutions. The Q-learning-based learning mechanism can choose appropriate search operators for the different scheduling environments. We also show how different trade-offs among the five objectives can provide software managers with a deeper insight into various compromises among many objectives, and enabling them to make informed decisions.",science
10.1016/j.snb.2017.07.155,Journal,"Sensors and Actuators, B: Chemical",scopus,2018-02-01,sciencedirect,Calibrating chemical multisensory devices for real world applications: An in-depth comparison of quantitative machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85029469409,"Chemical multisensor devices need calibration algorithms to estimate gas concentrations. Their possible adoption as indicative air quality measurements devices poses new challenges due to the need to operate in continuous monitoring modes in uncontrolled environments. Several issues, including slow dynamics, continue to affect their real world performances. At the same time, the need for estimating pollutant concentrations on board the devices, especially for wearables and IoT deployments, is becoming highly desirable. In this framework, several calibration approaches have been proposed and tested on a variety of proprietary devices and datasets; still, no thorough comparison is available to researchers. This work attempts a benchmarking of the most promising calibration algorithms according to recent literature with a focus on machine learning approaches. We test the techniques against absolute and dynamic performances, generalization capabilities and computational/storage needs using three different datasets sharing continuous monitoring operation methodology. Our results can guide researchers and engineers in the choice of optimal strategy. They show that non-linear multivariate techniques yield reproducible results, outperforming linear approaches. Specifically, the Support Vector Regression method consistently shows good performances in all the considered scenarios. We highlight the enhanced suitability of shallow neural networks in a trade-off between performance and computational/storage needs. We confirm, on a much wider basis, the advantages of dynamic approaches with respect to static ones that only rely on instantaneous sensor array response. The latter have been shown to be best choice whenever prompt and precise response is needed.",science
10.1016/B978-1-4557-0760-7.00003-6,Book,Critical Heart Disease in Infants and Children,scopus,2018-01-01,sciencedirect,Streaming analytics in pediatric cardiac care,https://api.elsevier.com/content/abstract/scopus_id/85082263171,"The cardiac intensive care unit (ICU) is an analytic environment, one in which insight about a patient's condition and evolving trajectory is gleaned from data and information from monitoring systems, electronic health records, and other data repositories. That data, both structured and unstructured, presents rapidly to clinicians and must be effectively managed in a timely manner to meet goals for patient care. With advances in computer and information technology, the evolution of the fields of data science and biomedical informatics, and examples of high-impact experiences with advanced data analytics approaches inside and outside of health care, the ability to effectively process and utilize clinical data, particularly high-frequency data, is on the rise. In the ICU, real-time data analytics, including streaming analytics that employ refined and validated predictive algorithms, have the potential to improve our accuracy in assessing a patient's condition, to support better assessment of the patient's course and response to treatment, and eventually to guide decisions and actions toward achieving the best possible outcomes. In this chapter an introduction to streaming analytics is provided, and experience with a recently developed predictive analytics software platform is discussed as an example of the current state of data analytics applications in the pediatric cardiac care environment.",science
10.1016/j.procs.2018.10.237,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,High Performance Geological Disaster Recognition using Deep Learning,https://api.elsevier.com/content/abstract/scopus_id/85062030356,"Geological disaster recognition on optical image is one of the key techniques in disaster control and disaster relief. Comparing with optical images, remote sensing images contain much higher resolution and more visualized contents. In this paper, we propose a landslide recognition framework which trains a deep auto-encoder network on the compressed domain. ANN or SVM is used as the classifier for decision making. In addition, in order to meet the requirement of some real-time applications, a high performance training network on CUDA-enabled GPUs is designed and implemented. Experiments are conducted on optical images from Google Earth.",science
10.1016/j.procs.2018.11.031,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,"Relating language, logic, and imagery",https://api.elsevier.com/content/abstract/scopus_id/85059472144,"The world is a continuum, but words are discrete. Sensory organs map the continuous world to continuous mental models of sights, sounds, actions, and feelings. Those mental models, which represent a moving 3-D virtual reality (VR) are the semantic foundation for all versions of language and logic. A common model for cognition must be able to process and relate all modalities. Kyndi technology represents all information in graphs. They include conceptual graphs for symbolic information and arbitrary graphs for 2-D icons or 3-D VR. All graphs are stored in Cognitive Memory, which can find approximate mappings for arbitrary graphs in logarithmic time. Those mappings include formal unification for logics, and informal analogies for case-based reasoning. The analogies can even map conceptual graphs to the graphs derived from imagery or VR simulations. For reasoning, Peirce’s rules of inference for existential graphs can support operations on arbitrary icons, such as the diagrams in Euclid’s geometry. Those rules, when adapted to conceptual graphs, can map symbolic languages to and from Euclidean style geometrical reasoning. With two new rules of inference, called observation and imagination, the Standard Model of Cognition can support mental models without making any current software obsolete.",science
10.1016/j.procS.2018.08.134,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Towards an adaptable task model within an evolutionary environment,https://api.elsevier.com/content/abstract/scopus_id/85056670455,"Task models are powerful artifacts as they are the result of requirement analysis at the design and development stages. Unfortunately, their uses are limited at gathering requirements and specification for systems and they are almost not employed anymore after design step despite their cost in terms of time and effort. In fact, task models bring many benefits at operation time, when the system has been already deployed as: it represents user documentation; it is used for training operators, etc. Moreover, human performance depends on cognitive mechanisms. It evolves simultaneously with personal, environmental and social factor which defines the context. Any changes within the context can affect the reliability of human performance by committing errors or deviations which can cause serious accidents especially for life-critical systems. These deviations are not taken into consideration within task model as it is produced at design time and remains unchanged even when it is reused at operation time. The work reported in this paper aims at proposing an approach to create and generate at real time a deduced task model from the original one corresponding to a certain situation at a specified context. The goal is also to ensure that user system interaction in a critical situation is performed as planned and there is no detected deviation. Machine learning techniques are used to predict what tasks user should perform and what ones he/she should not. They enable to learn about human computer interaction variations in the future in order to mend task model. This approach will be evaluated through experiments in simulated scenarios about on-board car system.",science
10.1016/j.procs.2018.08.197,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Web based Augmented Reality for Human Body Anatomy Learning,https://api.elsevier.com/content/abstract/scopus_id/85053157912,"Human body anatomy becomes an important topic in Biology subject that must be understood since junior high school. Learning materials are mostly available in form of book and anatomy mannequin (puppet), but it is still insufficient enough to help students in understanding human body anatomy. Augmented Reality (AR) is a technology that combines a real thing into virtual environment interactively. This research purpose is to develop an AR application for human body anatomy learning to be more interesting and easier for student to understand. This application enables student to learn human body anatomy with 3D object interaction while previously using textbook and mannequin. Research method in for this study is by using quantitative method that collects data and then develops the prototype to prove the impact. Application development method is done by using waterfall method that includes planning (collect data and analysis), design (user interface and diagram), implementation, and testing. Research result is AR application for human body anatomy learning that contains 3D object, organ explanation and position that can be accessible on web.",science
10.1016/j.ifacol.2018.07.176,Conference Proceeding,,scopus,2018-01-01,sciencedirect,Artificial Colloquist: Treating Social Anxiety Disorder Using Altera FPGA,https://api.elsevier.com/content/abstract/scopus_id/85052906741,Purpose of this paper is to present the project that will use artificial intelligence and features of Altera FPGA board to imitate human. This problem was solved by using Cleverbot Google API and Altera FPGA board which has Linux system installed to run backend of our application. Frontend input and output peripherals will be used to read input from user and to present output to user. We present an implementation which has all the necessary features to be used for treating social anxiety disorder by simulating real human interaction by effectively employing such algorithms and system.,science
10.1016/j.promfg.2018.04.026,Conference Proceeding,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Design and implementation of a low cost RFID track and trace system in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85052890798,"The factories of the future will make use of actuators, sensors and cyber-physical systems (CPS) to provide an environment in which human beings, machines, and resources will communicate as in a social network. In such a network, communication between various “objects” relay the current state of the physical world. Business decisions are made using the information and it is therefore critical that this information is accurate and in real-time. Information flow is a key enabler of such future factories. Industrial engineers, as designers and improvement agents of such factories of the future, will need to develop better skills in various aspects of data analytics and information communication technologies. This paper describes the development and implementation of a low cost RFID track and trace system (by students) for application in a Learning Factory for teaching undergraduate industrial engineering students key concepts related to Industry 4.0 and “smart factories”. The benefit of this system is not only a demonstrator to be used in the Learning Factory, but also can be used to teach students in a “learning by doing” fashion critical skills related to real time tracking in a manufacturing environment. The system also demonstrates potential low cost implementation of such technologies in SME’s.",science
10.1016/j.ifacol.2018.08.421,Conference Proceeding,,scopus,2018-01-01,sciencedirect,A Multi Agent System architecture to implement Collaborative Learning for social industrial assets,https://api.elsevier.com/content/abstract/scopus_id/85052888258,"The ‘Industrial Internet of Things’ aims to connect industrial assets with one another and benefit from the data that is generated, and shared, among these assets. In recent years, the extensive instrumentation of machines and the advancements in Information Communication Technologies are re-shaping the role of assets in our industrial systems. An emerging concept here is that of ‘social assets’: assets that collaborate with each other in order to improve system optimisation. Cyber-Physical Systems (CPSs) are formed by embedding the assets with computers, or microcontrollers, which run real-time decision-making algorithms over the data originating from the asset. These are known as the ‘Digital Twins’ of the assets, and form the backbone of social assets. It is essential to have an architecture which enables a seamless integration of these technological advances for an industry. This paper proposes a Multi Agent System (MAS) architecture for collaborative learning, and presents the findings of an implementation of this architecture for a prognostics problem. Collaboration among assets is performed by calculating inter-asset similarity during operating condition to identify ‘friends’ and sharing operational data within these clusters of friends. The architecture described in this paper also presents a generic model for the Digital Twins of assets. Prognostics is demonstrated for the C-MAPSS turbofan engine degradation simulated data-set (Saxena and Goebel (2008)).",science
10.1016/j.procs.2018.05.111,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Real-Time Sentiment Analysis of Twitter Streaming data for Stock Prediction,https://api.elsevier.com/content/abstract/scopus_id/85049105669,"In this study, an attempt has been made for making financial decisions such as stock market prediction, to predict the potential prices of a company’s stock and to serve the need of this, Twitter data 1 2 has been considered for scoring the impression that is carried for a particular firm. Streaming data proves to be a perennial source of data analysis collected in real-time. Streaming data basically deals with the continuous flow of data which carries information from sources like websites, mobile phone applications, server logs, social websites, trading floors, etc. The major characteristics of such data being its accessibility and availability, help in proper analysis and prediction of user behavior in a ceaseless manner. The classifying model made out of historical data can be relentlessly honed to give even more accurate results since its outcome is always compared to the next tick of the clock. Spark streaming has been considered for the processing of humongous data and data ingestion tools like Twitter API and Apache Flume have been further implemented for analysis.",science
10.1016/j.procs.2018.05.028,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Analysis and Visualisation of Geo-Referenced Tweets for Real-Time Information Diffusion,https://api.elsevier.com/content/abstract/scopus_id/85049081380,"This paper focuses on techniques that can model information diffusion by the development of a prototype - a web application based on a JavaScript Web-client and a Java Enterprise Edition server. The motive is to use a distributed system, instead of a standalone/monolithic system, to present a new and unique way to visualise the data available through twitter, and analyze it to extract useful information. The prototype processes real-time data in the data stream with corresponding high processing speed and suitable communication patterns, with the use of hashtags to define the type of information to be analysed.",science
10.1016/j.procs.2018.05.053,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Gender Recognition Through Face Using Deep Learning,https://api.elsevier.com/content/abstract/scopus_id/85049059241,"Automatic gender recognition has now pertinent to an extension of its usage in various software and hardware, particularly because of the growth of online social networking websites and social media. However the performance of already exist system with the physical world face pictures, images are somewhat not excellent, particularly in comparison with the result of task related to face recognition. Within this paper, we have explored that by doing learn and classification method and with the utilization of Deep Convolutional Neural Networks (D-CNN) technique, a satisfied growth in performance can be achieved on such gender classification tasks that is a reason why we decided to propose an efficient convolutional network VGGnet architecture which can be used in extreme case when the amount of training data used to learn D-CNN based on VGGNet architecture is limited. We examine our related work on the current unfiltered image of the face for gender recognition and display it to dramatics outplay current advance updated methods.",science
10.1016/j.procs.2018.01.127,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Machine learning and graph theory to optimize drinking water,https://api.elsevier.com/content/abstract/scopus_id/85045645653,"The preservation of the water quality in a distribution network requires maintenance of a permanent minimum residual chlorine at any point of the network. This is possible only if we plan chlorine injections at various points of the network for intermediate rechlorination. Given the high cost of the implementation of such stations, the optimization of the number and the choice of location of these stations are the two main difficulties facing managers. To optimize the placement of these locations, we have adopted two different approaches: one based on dynamic programming while the other is based on graph theory. We also proposed a regression model of pipes determined by Machine Learning. Performance tests of our decision support system were done on real sites of the Wilaya Rabat-Sale (network of Morocco’s capital). The results obtained show that the contribution of graph theory is better than that of dynamic programming in that the response time (could you explain: response time of what) is improved.",science
10.1016/j.procs.2018.01.096,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,A method of data mining using Hidden Markov Models (HMMs) for protein secondary structure prediction,https://api.elsevier.com/content/abstract/scopus_id/85045629519,"The prediction of the secondary structure of proteins is one of the most studied problems in computational biology. However, the accuracy of the predicted secondary structure is insufficient for practical utility. In this paper, we propose an algorithmic approach based on Hidden Markov Models (HMM) to model the problem of prediction. Therefore, HMM are often used for data mining in bioinformatics. In this research, we have built a HMM that models the prediction problem of protein secondary structure. Moreover, two procedures for estimating the probability parameters were performed by the Maximum Likelihood Estimation (MLE) of protein sequences from a public database (Brookhaven PDB). Finally, a new prediction approach based on a posteriori probability of hidden regimes has been implemented. Our model appears to be very efficient on single sequences, with a score of 66.6% by comparing the first results obtained with the real secondary sequence and encouraging for an improvement of the system.",science
10.1016/j.jvcir.2017.12.005,Journal,Journal of Visual Communication and Image Representation,scopus,2018-01-01,sciencedirect,Leveraging deep neural networks to fight child pornography in the age of social media,https://api.elsevier.com/content/abstract/scopus_id/85038891116,"Over the past two decades, the nature of child pornography in terms of generation, distribution and possession of images drastically changed, evolving from basically covert and offline exchanges of content to a massive network of contacts and data sharing. Nowadays, the internet has become not only a transmission channel but, probably, a child pornography enabling factor by itself. As a consequence, most countries worldwide consider a crime to take, or permit to be taken, to store or to distribute images or videos depicting any child pornography grammar. But before action can even be taken, we must detect the very existence or presence of sexually exploitative imagery of children when gleaning over vast troves of data. With this backdrop, veering away from virtually all off-the-shelf solutions and existing methods in the literature, in this work, we leverage cutting-edge data-driven concepts and deep convolutional neural networks (CNNs) to harness enough characterization aspects from a wide range of images and point out the presence of child pornography content in an image. We explore different transfer-learning strategies for CNN modeling. CNNs are first trained with problems for which we can gather more training examples and upon which there are no serious concerns regarding collection and storage and then fine-tuned with data from the target problem of interest. The learned networks outperform different existing solutions and seem to represent an important step forward when dealing with child pornography content detection. The proposed solutions are encapsulated in a sandbox virtual machine ready for deployment by experts and practitioners. Experimental results with tens of thousands of real cases show the effectiveness of the proposed methods.",science
10.1016/j.artmed.2017.10.004,Journal,Artificial Intelligence in Medicine,scopus,2018-01-01,sciencedirect,Development of an intelligent surgical training system for Thoracentesis,https://api.elsevier.com/content/abstract/scopus_id/85035010472,"Surgical training improves patient care, helps to reduce surgical risks, increases surgeon’s confidence, and thus enhances overall patient safety. Current surgical training systems are more focused on developing technical skills, e.g. dexterity, of the surgeons while lacking the aspects of context-awareness and intra-operative real-time guidance. Context-aware intelligent training systems interpret the current surgical situation and help surgeons to train on surgical tasks. As a prototypical scenario, we chose Thoracentesis procedure in this work. We designed the context-aware software framework using the surgical process model encompassing ontology and production rules, based on the procedure descriptions obtained through textbooks and interviews, and ontology-based and marker-based object recognition, where the system tracked and recognised surgical instruments and materials in surgeon’s hands and recognised surgical instruments on the surgical stand. The ontology was validated using annotated surgical videos, where the system identified “Anaesthesia” and “Aspiration” phase with 100% relative frequency and “Penetration” phase with 65% relative frequency. The system tracked surgical swab and 50mL syringe with approximately 88.23% and 100% accuracy in surgeon’s hands and recognised surgical instruments with approximately 90% accuracy on the surgical stand. Surgical workflow training with the proposed system showed equivalent results as the traditional mentor-based training regime, thus this work is a step forward a new tool for context awareness and decision-making during surgical training.",science
10.1016/j.oceaneng.2017.08.047,Journal,Ocean Engineering,scopus,2018-01-01,sciencedirect,Sea state identification based on vessel motion response learning via multi-layer classifiers,https://api.elsevier.com/content/abstract/scopus_id/85033575900,"In order to extend the operational weather window for marine vessels under Dynamic Positioning (DP) control, a novel sea state identification method with multi-layer classifiers is proposed in this paper. Due to the distinction of system responses for various sea states, four motion signals including surge, sway, roll and yaw are adopted for classification purpose. Firstly, preprocessing techniques, like filtration and k-means clustering are performed to the raw data to filter out the “corrupted” low frequency (LF) information and generate the band-pass filter bank. Then, the processed data is decomposed into 20 categories via Hilbert-Huang transform (HHT), filter bank method and wavelet transform and 11 statistical features are extracted for each category. Subsequently, Max-relevance Min-redundancy (mRMR) method helps to select salient features with best trade-off between relevance and redundancy. With these selected features, a newly developed three-layer classification structure with Adaptive Neuro-Fuzzy Inference System (ANFIS), Random Forest (RF) and Particle Swarm Optimization (PSO) based combination classifiers is proposed to derive the current sea state. The simulation results demonstrate that the proposed identification system can achieve satisfactory classification accuracy. Moreover, the multi-layer classifier outperforms single layer classifier and can rapidly classify the sea state in real-time implementation.",science
10.1016/j.neunet.2017.09.011,Journal,Neural Networks,scopus,2018-01-01,sciencedirect,SNAVA—A real-time multi-FPGA multi-model spiking neural network simulation architecture,https://api.elsevier.com/content/abstract/scopus_id/85031793480,"Spiking Neural Networks (SNN) for Versatile Applications (SNAVA) simulation platform is a scalable and programmable parallel architecture that supports real-time, large-scale, multi-model SNN computation. This parallel architecture is implemented in modern Field-Programmable Gate Arrays (FPGAs) devices to provide high performance execution and flexibility to support large-scale SNN models. Flexibility is defined in terms of programmability, which allows easy synapse and neuron implementation. This has been achieved by using a special-purpose Processing Elements (PEs) for computing SNNs, and analyzing and customizing the instruction set according to the processing needs to achieve maximum performance with minimum resources. The parallel architecture is interfaced with customized Graphical User Interfaces (GUIs) to configure the SNN’s connectivity, to compile the neuron-synapse model and to monitor SNN’s activity. Our contribution intends to provide a tool that allows to prototype SNNs faster than on CPU/GPU architectures but significantly cheaper than fabricating a customized neuromorphic chip. This could be potentially valuable to the computational neuroscience and neuromorphic engineering communities.",science
10.1016/j.jcp.2017.09.057,Journal,Journal of Computational Physics,scopus,2018-01-01,sciencedirect,Sparsity enabled cluster reduced-order models for control,https://api.elsevier.com/content/abstract/scopus_id/85030978137,"Characterizing and controlling nonlinear, multi-scale phenomena are central goals in science and engineering. Cluster-based reduced-order modeling (CROM) was introduced to exploit the underlying low-dimensional dynamics of complex systems. CROM builds a data-driven discretization of the Perron–Frobenius operator, resulting in a probabilistic model for ensembles of trajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a linear framework, which enables the application of standard linear techniques to the nonlinear system. CROM is typically computed on high-dimensional data; however, access to and computations on this full-state data limit the online implementation of CROM for prediction and control. Here, we address this key challenge by identifying a small subset of critical measurements to learn an efficient CROM, referred to as sparsity-enabled CROM. In particular, we leverage compressive measurements to faithfully embed the cluster geometry and preserve the probabilistic dynamics. Further, we show how to identify fewer optimized sensor locations tailored to a specific problem that outperform random measurements. Both of these sparsity-enabled sensing strategies significantly reduce the burden of data acquisition and processing for low-latency in-time estimation and control. We illustrate this unsupervised learning approach on three different high-dimensional nonlinear dynamical systems from fluids with increasing complexity, with one application in flow control. Sparsity-enabled CROM is a critical facilitator for real-time implementation on high-dimensional systems where full-state information may be inaccessible.",science
10.1016/j.compedu.2017.09.007,Journal,Computers and Education,scopus,2018-01-01,sciencedirect,Educational apps from the Android Google Play for Greek preschoolers: A systematic review,https://api.elsevier.com/content/abstract/scopus_id/85029678082,"In the seven years since the introduction of the tablet (Apple iPad) in 2010, the use of software for smart mobile devices has grown rapidly in popularity and has become a hotly debated issue in the field of education and child development. However, the rise in popularity of mobile applications (apps) mainly addressed to young children is not in line with a corresponding increase in their quality, as there is conflicting evidence about the real value and suitability of educational apps. The purpose of this study was to examine whether self-proclaimed educational apps for Greek preschoolers have been designed in accordance with developmentally appropriate standards to contribute to the social, emotional and cognitive development of children in formal and informal learning environments. The study results were discouraging. The majority of the apps aimed to teach children the basics about numbers and letters. Overall, they were drill-and-practice-style, based on a low level of thinking skills, thereby promoting rote learning, and were unable to contribute to a deeper conceptual understanding of certain concepts.",science
10.1016/j.compeleceng.2017.03.009,Journal,Computers and Electrical Engineering,scopus,2018-01-01,sciencedirect,Applying spark based machine learning model on streaming big data for health status prediction,https://api.elsevier.com/content/abstract/scopus_id/85015331423,"Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action.",science
10.1016/j.bios.2017.06.063,Journal,Biosensors and Bioelectronics,scopus,2017-12-15,sciencedirect,Viologen-functionalized single-walled carbon nanotubes as carrier nanotags for electrochemical immunosensing. Application to TGF-β1 cytokine,https://api.elsevier.com/content/abstract/scopus_id/85021706214,"Viologen-SWCNT hybrids are synthesized by aryl-diazonium chemistry in the presence of isoamyl nitrite followed by condensation reaction of the resulting HOOC-Phe-SWCNT with 1-(3-aminoethyl)-4,4′-bipyridinium bromine and N-alkylation with 2-bromoethylamine. The V-Phe-SWCNT hybrids were characterized by using different spectroscopic techniques (FT-IR, Raman, UV–vis), TGA and Kaiser test. Viologen-SWCNTs were used for the preparation of an electrochemical immunosensor for the determination of the transforming growth factor β1 (TGF-β1) cytokine considered as a reliable biomarker in several human diseases. The methodology involved preparation of V-Phe-SWCNT(-HRP)-anti-TGF conjugates by covalent linkage of HRP and anti-TGF onto V-Phe-SWCNT hybrids. Biotinylated anti-TGF antibodies were immobilized onto 4-carboxyphenyl-functionalized SPCEs modified with streptavidin and a sandwich type immunoassay was implemented for TGF-β1 with signal amplification using V-Phe-SWCNT(-HRP)-anti-TGF conjugates as carrier tags. The analytical characteristics exhibited by the as prepared immunosensor (range of linearity between 2.5 and 1000pgmL−1 TGF-β1; detection limit of 0.95pgmL−1) improve notably those reported with other previous immunosensors or ELISA kits. A great selectivity against other proteins was also found. The prepared immunosensor was validated by determining TGF-β1 in real saliva samples. Minimal sample treatment was required and the obtained results were in excellent agreement with those obtained by using a commercial ELISA kit.",science
10.1016/j.biosystems.2017.10.001,Journal,BioSystems,scopus,2017-12-01,sciencedirect,Towards a first implementation of the WLIMES approach in living system studies advancing the diagnostics and therapy in personalized medicine,https://api.elsevier.com/content/abstract/scopus_id/85033459793,"The goal of this paper is to advance an extensible theory of living systems using an approach to biomathematics and biocomputation that suitably addresses self-organized, self-referential and anticipatory systems with multi-temporal multi-agents. Our first step is to provide foundations for modelling of emergent and evolving dynamic multi-level organic complexes and their sustentative processes in artificial and natural life systems. Main applications are in life sciences, medicine, ecology and astrobiology, as well as robotics, industrial automation, man-machine interface and creative design. Since 2011 over 100 scientists from a number of disciplines have been exploring a substantial set of theoretical frameworks for a comprehensive theory of life known as Integral Biomathics. That effort identified the need for a robust core model of organisms as dynamic wholes, using advanced and adequately computable mathematics. The work described here for that core combines the advantages of a situation and context aware multivalent computational logic for active self-organizing networks, Wandering Logic Intelligence (WLI), and a multi-scale dynamic category theory, Memory Evolutive Systems (MES), hence WLIMES. This is presented to the modeller via a formal augmented reality language as a first step towards practical modelling and simulation of multi-level living systems. Initial work focuses on the design and implementation of this visual language and calculus (VLC) and its graphical user interface. The results will be integrated within the current methodology and practices of theoretical biology and (personalized) medicine to deepen and to enhance the holistic understanding of life.",science
10.1016/j.radonc.2017.09.013,Journal,Radiotherapy and Oncology,scopus,2017-12-01,sciencedirect,Tumour auto-contouring on 2d cine MRI for locally advanced lung cancer: A comparative study,https://api.elsevier.com/content/abstract/scopus_id/85030775146,"Background and purpose
                  Radiotherapy guidance based on magnetic resonance imaging (MRI) is currently becoming a clinical reality. Fast 2d cine MRI sequences are expected to increase the precision of radiation delivery by facilitating tumour delineation during treatment. This study compares four auto-contouring algorithms for the task of delineating the primary tumour in six locally advanced (LA) lung cancer patients.
               
                  Material and methods
                  Twenty-two cine MRI sequences were acquired using either a balanced steady-state free precession or a spoiled gradient echo imaging technique. Contours derived by the auto-contouring algorithms were compared against manual reference contours. A selection of eight image data sets was also used to assess the inter-observer delineation uncertainty.
               
                  Results
                  Algorithmically derived contours agreed well with the manual reference contours (median Dice similarity index: 
                        
                           ⩾
                           0.91
                        
                     ). Multi-template matching and deformable image registration performed significantly better than feature-driven registration and the pulse-coupled neural network (PCNN). Neither MRI sequence nor image orientation was a conclusive predictor for algorithmic performance. Motion significantly degraded the performance of the PCNN. The inter-observer variability was of the same order of magnitude as the algorithmic performance.
               
                  Conclusion
                  Auto-contouring of tumours on cine MRI is feasible in LA lung cancer patients. Despite large variations in implementation complexity, the different algorithms all have relatively similar performance.",science
10.1016/j.future.2017.06.010,Journal,Future Generation Computer Systems,scopus,2017-12-01,sciencedirect,"BigFCM: Fast, precise and scalable FCM on hadoop",https://api.elsevier.com/content/abstract/scopus_id/85023611123,"Clustering plays an important role in mining big data both as a modeling technique and a preprocessing step in many data mining process implementations. Fuzzy clustering provides more flexibility than non-fuzzy methods by allowing each data record to belong to more than one cluster to some degree. However, a serious challenge in fuzzy clustering is the lack of scalability. Massive datasets in emerging fields such as geosciences, biology, and networking do require parallel and distributed computations with high performance to solve real-world problems. Although some clustering methods are already improved to execute on big data platforms, their execution time is highly increased for gigantic datasets. In this paper, a scalable Fuzzy C-Means (FCM) clustering method named BigFCM is proposed and designed for the Hadoop distributed data platform. Based on the MapReduce programming model, the proposed algorithm exploits several mechanisms including an efficient caching design to achieve several orders of magnitude reduction in execution time. The BigFCM performance compared with Apache Mahout K-Means and Fuzzy K-Means through an evaluation framework developed in this research. Extensive evaluation using over multi-gigabyte datasets including SUSY and HIGGS shows that BigFCM is scalable while it preserves the quality of clustering.",science
10.1016/j.chb.2017.02.064,Journal,Computers in Human Behavior,scopus,2017-12-01,sciencedirect,Shopping with a robotic companion,https://api.elsevier.com/content/abstract/scopus_id/85015734591,"In this paper, we present a robotic shopping assistant, designed with a cognitive architecture, grounded in machine learning systems, in order to study how the human-robot interaction (HRI) is changing the shopping behavior in smart technological stores. In the software environment of the NAO robot, connected to the Internet with cloud services, we designed a social-like interaction where the robot carries out actions with the customer. In particular, we focused our design on two main skills the robot has to learn: the first is the ability to acquire social input communicated by relevant clues that humans provide about their emotional state (emotions, emotional speech), or collected in the Social Media (such as, information on the customer's tastes, cultural background, etc.). The second is the skill to express in turn its own emotional state, so that it can affect the customer buying decision, refining in the user the sense of interacting with a human-like companion. By combining social robotics and machine learning systems the potential of robotics to assist people in real life situations will increase, providing a gentle customers' acceptance of advanced technologies.",science
10.1016/j.knosys.2017.07.029,Journal,Knowledge-Based Systems,scopus,2017-10-15,sciencedirect,Community-based influence maximization in social networks under a competitive linear threshold model,https://api.elsevier.com/content/abstract/scopus_id/85025820402,"The main purpose in influence maximization, which is motivated by the idea of viral marketing in social networks, is to find a subset of key users that maximize influence spread under a certain propagation model. A number of studies have been done over the past few years that try to solve this problem by considering a non-adversarial environment in which there exists only one player with no competitor. However, in real world scenarios, there is always more than one player competing with other players to influence the most nodes. This is called competitive influence maximization. Motivated by this, we try to solve the competitive influence maximization problem by proposing a new propagation model which is an extension of the Linear Threshold model and gives decision-making ability to nodes about incoming influence spread. We also propose an efficient algorithm for finding the influential nodes in a given social graph under the proposed propagation model which exploits the community structure of this graph to compute the spread of each node locally within its own community. The aim of our algorithm is to find the minimum number of seed nodes which can achieve higher spread in comparison with the spread achieved by nodes selected by other competitor. Our experiments on real world and synthetic datasets show that our approach can find influential nodes in an acceptable running time.",science
10.1016/j.enconman.2017.05.006,Journal,Energy Conversion and Management,scopus,2017-10-15,sciencedirect,Diagnostic information system dynamics in the evaluation of machine learning algorithms for the supervision of energy efficiency of district heating-supplied buildings,https://api.elsevier.com/content/abstract/scopus_id/85019049599,"Modern ways of exploring the diagnostic knowledge provided by data mining and machine learningraise some concern about the ways of evaluating the quality of output knowledge, usually represented by information systems. Especially in district heating, the stationarity of efficiency models, and thus the relevance of diagnostic classification system, cannot be ensured due to the impact of social, economic or technological changes, which are hard to identify or predict. Therefore, data mining and machine learning have become an attractive strategy for automatically and continuously absorbing such dynamics.
                  This paper presents a new method of evaluation and comparison of diagnostic information systems gathered algorithmically in district heating efficiency supervision based on exploring the evolution of information system and analyzing its dynamic features.
                  The process of data mining and knowledge discovery was applied to the data acquired from district heating substations’ energy meters to provide the automated discovery of diagnostic knowledge base necessary for the efficiency supervision of district heating-supplied buildings. The implemented algorithm consists of several steps of processing the billing data, including preparation, segmentation, aggregation and knowledge discovery stage, where classes of abstract modelsrepresenting energy efficiency constitute an information system representing diagnostic knowledge about the energy efficiency of buildings favorably operatingunder similar climate conditions and supplied from the same district heating network.
                  The authors analyzed the evolution of a series of information systems originating from the same knowledge discovery algorithm applied to a sequence of energy consumption-related data. Specifically, the rough sets theory was applied to describe the knowledge base and measure the uncertainty of machine learning predictions of currentclassification based on a past knowledge base. Fluctuations of diagnostic class membership were identified and provided for the differentiationbetween returning and novel fault detections, thus introducing the qualities of information system uncertainty and its sustainability. Theusability of the new method was demonstrated in the comparison of results for exemplary data mining algorithms implemented on real data from over one thousand buildings.",science
10.1016/j.jlumin.2017.05.073,Journal,Journal of Luminescence,scopus,2017-10-01,sciencedirect,Structural characterizations of organic-based materials with extensive mechanoluminescence properties,https://api.elsevier.com/content/abstract/scopus_id/85020301840,"The organic mechanoluminescent (ML) material europium-doped dibenzoylmethide triethylammonium (EuD4TEA) is known as a triboluminescent material. Our group has synthesized a novel ML material with the addition of 1-ethenylpyrrolidin-2-one [(polyvinylpyrrolidone) (PVP)], which changed the ligand of the mechanoluminescent material. This research investigated the ML material structure, molecular orbital electron distributions of the ligands, ML mechanism and enhancement in the photoluminescence (PL) intensity with PVP. For the first time, the ML material structure was characterized using nuclear magnetic resonance spectroscopy (NMR), X-ray diffraction (XRD), X-ray photoelectron spectroscopy (XPS) and Gaussian DFT/B3LYP/6–31G(d,p) software. The ML properties were characterized using a Hamamatsu Photonic Multichannel Analyzer (PMA). The material can be applied to the real-time visualization of the stress field near the tip of a crack, an ML light source, determination of laser and ultrasonic powers, secret message writing and earthquake detection sensors.",science
10.1016/j.jpdc.2016.11.009,Journal,Journal of Parallel and Distributed Computing,scopus,2017-10-01,sciencedirect,Goal-based composition of scalable hybrid analytics for heterogeneous architectures,https://api.elsevier.com/content/abstract/scopus_id/85009460393,"Crafting scalable analytics in order to extract actionable business intelligence is a challenging endeavour, requiring multiple layers of expertise and experience. Often, this expertise is irreconcilably split between an organisation’s engineers and subject matter domain experts. Previous approaches to this problem have relied on technically adept users with tool-specific training.
                  Such an approach has a number of challenges: Expertise — There are few data-analytic subject domain experts with in-depth technical knowledge of compute architectures; Performance — Analysts do not generally make full use of the performance and scalability capabilities of the underlying architectures; Heterogeneity — calculating the most performant and scalable mix of real-time (on-line) and batch (off-line) analytics in a problem domain is difficult; Tools — Supporting frameworks will often direct several tasks, including, composition, planning, code generation, validation, performance tuning and analysis, but do not typically provide end-to-end solutions embedding all of these activities.
                  In this paper, we present a novel semi-automated approach to the composition, planning, code generation and performance tuning of scalable hybrid analytics, using a semantically rich type system which requires little programming expertise from the user. This approach is the first of its kind to permit domain experts with little or no technical expertise to assemble complex and scalable analytics, for hybrid on- and off-line analytic environments, with no additional requirement for low-level engineering support.
                  This paper describes (i) an abstract model of analytic assembly and execution, (ii) goal-based planning and (iii) code generation for hybrid on- and off-line analytics. An implementation, through a system which we call Mendeleev, is used to (iv) demonstrate the applicability of this technique through a series of case studies, where a single interface is used to create analytics that can be run simultaneously over on- and off-line environments. Finally, we (v) analyse the performance of the planner, and (vi) show that the performance of Mendeleev’s generated code is comparable with that of hand-written analytics.",science
10.1016/j.molliq.2017.05.139,Journal,Journal of Molecular Liquids,scopus,2017-09-01,sciencedirect,Cloud point-magnetic dispersive solid phase extraction for the spectrofluorometric determination of citalopram,https://api.elsevier.com/content/abstract/scopus_id/85020314451,"Cloud point-magnetic solid phase extraction was used for the purpose of extraction and preconcentration of citalopram prior to its determination by the spectrofluorometric method. Non-ionic surfactant Triton X-100 was used for cloud point extraction and nickel zinc ferrite magnetic nanocomposite was used as a sorbent for the adsorption of surfactant rich phase. The implemented method involves initial extraction of the target analytes by CPE in the micelles of a non-ionic surfactant medium followed by using nickel zinc ferrite magnetic nanocomposite to retrieve the micellar phase. Magnetic materials are then collected by applying an adscititious magnetic field to overcome the need for specific steps associated with CPE such as refrigeration of the condensed micellar phase to reduce its viscosity, centrifugation to separate the surfactant-rich phase and/or appropriate apparatus that enable direct sampling of the surfactant-rich phase. Under the optimal extraction conditions (i.e. pH: 9, adsorbent dosage: 60mg, Triton X-100 concentration: 50mg and t=50°C) the linear dynamic range and RSD were 3.00–800ngmL−1 and 2.93%, respectively. A limit of detection of 0.42ngmL−1 of citalopram was obtained for the method. Obtained results showed successful application of the method for the analysis of citalopram in the real samples.",science
10.1016/j.epsr.2017.05.012,Journal,Electric Power Systems Research,scopus,2017-09-01,sciencedirect,Adaptive real-time congestion management in smart power systems using a real-time hybrid optimization algorithm,https://api.elsevier.com/content/abstract/scopus_id/85019432514,"In deregulated power systems, where a short period of service interruption causes extreme financial and social damages to customers and service providers, it is necessary to develop optimal intelligent algorithms in order to minimize unforeseen service interruptions due to unavoidable real-time contingencies. Nowadays, regarding the high implementation of communication infrastructure in smart power systems, as well as accurate sensors for a variety of purposes, it is possible to effectively collect and analyze real-time and synchronized data, run fast intelligent algorithms and send control commands to controllers. This paper proposes an adaptive real-time congestion management (RTCM) method which optimally employs adaptive thermal ratings of transmission lines to manage real-time congestions using all power system capabilities. This algorithm is considered as an essential ancillary service in a power market, where all generation companies and customers can participate. In this algorithm, a demand response program is modeled and also a real-time hybrid optimization algorithm is developed to solve the RTCM problem aimed at finding the optimal solution during a short time span. Incorporating an adaptive artificial neural network along with a modified particle swarm optimization (PSO) algorithm is proposed in this paper as a real-time hybrid optimization method. Advantages and effectiveness of this method are demonstrated by numerical results from analyzing the modified 39-bus New England system.",science
10.1016/j.bios.2017.03.047,Journal,Biosensors and Bioelectronics,scopus,2017-08-15,sciencedirect,Multiplexed nanoplasmonic biosensor for one-step simultaneous detection of Chlamydia trachomatis and Neisseria gonorrhoeae in urine,https://api.elsevier.com/content/abstract/scopus_id/85016288582,"Development of rapid and multiplexed diagnostic tools is a top priority to address the current epidemic problem of sexually transmitted diseases. Here we introduce a novel nanoplasmonic biosensor for simultaneous detection of the two most common bacterial infections: Chlamydia trachomatis and Neisseria gonorrhoeae. Our plasmonic microarray is composed of gold nanohole sensor arrays that exhibit the extraordinary optical transmission (EOT), providing highly sensitive analysis in a label-free configuration. The integration in a microfluidic system and the precise immobilization of specific antibodies on the individual sensor arrays allow for selective detection and quantification of the bacteria in real-time. We achieved outstanding sensitivities for direct immunoassay of urine samples, with a limit of detection of 300 colony forming units (CFU)/mL for C. trachomatis and 1500CFU/mL for N. gonorrhoeae. The multiplexing capability of our biosensor was demonstrated by analyzing different urine samples spiked with either C. trachomatis or N. gonorrhoeae, and also containing both bacteria. We could successfully detect, identify and quantify the levels of the two bacteria in a one-step assay, without the need for DNA extraction or amplification techniques. This work opens up new possibilities for the implementation of point-of-care biosensors that enable fast, simple and efficient diagnosis of sexually transmitted infections.",science
10.1016/j.mineng.2017.02.013,Journal,Minerals Engineering,scopus,2017-08-01,sciencedirect,Designing gold extraction processes: Performance study of a case-based reasoning system,https://api.elsevier.com/content/abstract/scopus_id/85014728806,"This paper presents a method for externalising and formalising knowledge involving the selection of hydrometallurgical process flowsheets for gold extraction from ores. A case-based reasoning (CBR) system was built using an open source software myCBR 3.0. The aim of the systems is to recommend flowsheet alternatives for processing a potential gold ore deposit. Nine attributes: Ore type, Gold ore grade, Gold distribution, Gold grain size, Sulfide present, Arsenic sulfide, Copper sulfide, Iron sulfide and Clay present were modelled and several literature sources of actual gold mines and processes were used for acquiring cases for the system. After preliminary testing, functional evaluation of the built CBR system was carried out by using five real mining projects as test cases. Additionally, human experts in the field of gold hydrometallurgy were interviewed to demonstrate the benefits of the CBR system as it holds no human biases towards any processing techniques. It was found that the suggestions of the CBR system provided useful information and direction for further process design and performed well compared to the interviewed human experts, thus confirming that the system is of practical relevance to the process engineer designing an industrial gold processing plant. The current model was found to be a functioning basis for further development through additional attributes, adjusted attribute weighting and increased number of cases.",science
10.1016/j.cogsys.2016.12.001,Journal,Cognitive Systems Research,scopus,2017-06-01,sciencedirect,A socially-based distributed self-organizing algorithm for holonic multi-agent systems: Case study in a task environment,https://api.elsevier.com/content/abstract/scopus_id/85009476397,"Holonic multi-agent systems (HMASs) have recently attracted many researches in multi-agent systems community. Inspired from the multi-level and self-similar structures of social and biological system, holonic multi-agent systems have been widely used to model and solve complex real-world problems. The main concern in deploying HMASs is the problem of building the hierarchical holonic structure, called holarchy, and dynamically managing it during its lifetime. The way an HMAS is organized has a great impact on its applicability and performance. This paper proposes a self-organizing algorithm to build and manage the holoic structures in multi-agent systems. This algorithm is based on the local information of the agents about other agents they can communicate with. Using common social concepts, like skills, diversity, social exchange theory, and norms in definition of the algorithm, the outcomes of this research can be used in wide ranges of distributed applications. The proposed model is extensively tested in a task allocation problem; and its performance based on various design parameters is studied. Empirical results show that the proposed model properly increases the performance of the system in terms of effectiveness and efficiency.",science
10.1016/j.cosrev.2017.03.001,Journal,Computer Science Review,scopus,2017-05-01,sciencedirect,Agent Based Modelling and Simulation tools: A review of the state-of-art software,https://api.elsevier.com/content/abstract/scopus_id/85016821258,"The key intent of this work is to present a comprehensive comparative literature survey of the state-of-art in software agent-based computing technology and its incorporation within the modelling and simulation domain. The original contribution of this survey is two-fold: (1) Present a concise characterization of almost the entire spectrum of agent-based modelling and simulation tools, thereby highlighting the salient features, merits, and shortcomings of such multi-faceted application software; this article covers eighty five agent-based toolkits that may assist the system designers and developers with common tasks, such as constructing agent-based models and portraying the real-time simulation outputs in tabular/graphical formats and visual recordings. (2) Provide a usable reference that aids engineers, researchers, learners and academicians in readily selecting an appropriate agent-based modelling and simulation toolkit for designing and developing their system models and prototypes, cognizant of both their expertise and those requirements of their application domain. In a nutshell, a significant synthesis of Agent Based Modelling and Simulation (ABMS) resources has been performed in this review that stimulates further investigation into this topic.",science
10.1016/j.artint.2017.02.006,Journal,Artificial Intelligence,scopus,2017-05-01,sciencedirect,"Lakatos-style collaborative mathematics through dialectical, structured and abstract argumentation",https://api.elsevier.com/content/abstract/scopus_id/85015708272,"The simulation of mathematical reasoning has been a driving force throughout the history of Artificial Intelligence research. However, despite significant successes in computer mathematics, computers are not widely used by mathematicians apart from their quotidian applications. An oft-cited reason for this is that current computational systems cannot do mathematics in the way that humans do. We draw on two areas in which Automated Theorem Proving (ATP) is currently unlike human mathematics: firstly in a focus on soundness, rather than understandability of proof, and secondly in social aspects. Employing techniques and tools from argumentation to build a framework for mixed-initiative collaboration, we develop three complementary arcs. In the first arc – our theoretical model – we interpret the informal logic of mathematical discovery proposed by Lakatos, a philosopher of mathematics, through the lens of dialogue game theory and in particular as a dialogue game ranging over structures of argumentation. In our second arc – our abstraction level – we develop structured arguments, from which we induce abstract argumentation systems and compute the argumentation semantics to provide labelings of the acceptability status of each argument. The output from this stage corresponds to a final, or currently accepted proof artefact, which can be viewed alongside its historical development. Finally, in the third arc – our computational model – we show how each of these formal steps is available in implementation. In an appendix, we demonstrate our approach with a formal, implemented example of real-world mathematical collaboration. We conclude the paper with reflections on our mixed-initiative collaborative approach.",science
10.1016/j.cmpb.2017.02.016,Journal,Computer Methods and Programs in Biomedicine,scopus,2017-05-01,sciencedirect,A study of EMR-based medical knowledge network and its applications,https://api.elsevier.com/content/abstract/scopus_id/85014111750,"Background and Objective
                  Electronic medical records (EMRs) contain an amount of medical knowledge which can be used for clinical decision support. We attempt to integrate this medical knowledge into a complex network, and then implement a diagnosis model based on this network.
               
                  Methods
                  The dataset of our study contains 992 records which are uniformly sampled from different departments of the hospital. In order to integrate the knowledge of these records, an EMR-based medical knowledge network (EMKN) is constructed. This network takes medical entities as nodes, and co-occurrence relationships between the two entities as edges. Selected properties of this network are analyzed. To make use of this network, a basic diagnosis model is implemented. Seven hundred records are randomly selected to re-construct the network, and the remaining 292 records are used as test records. The vector space model is applied to illustrate the relationships between diseases and symptoms. Because there may exist more than one actual disease in a record, the recall rate of the first ten results, and the average precision are adopted as evaluation measures.
               
                  Results
                  Compared with a random network of the same size, this network has a similar average length but a much higher clustering coefficient. Additionally, it can be observed that there are direct correlations between the community structure and the real department classes in the hospital. For the diagnosis model, the vector space model using disease as a base obtains the best result. At least one accurate disease can be obtained in 73.27% of the records in the first ten results.
               
                  Conclusion
                  We constructed an EMR-based medical knowledge network by extracting the medical entities. This network has the small-world and scale-free properties. Moreover, the community structure showed that entities in the same department have a tendency to be self-aggregated. Based on this network, a diagnosis model was proposed. This model uses only the symptoms as inputs and is not restricted to a specific disease. The experiments conducted demonstrated that EMKN is a simple and universal technique to integrate different medical knowledge from EMRs, and can be used for clinical decision support.",science
10.1016/j.knosys.2017.01.031,Journal,Knowledge-Based Systems,scopus,2017-04-15,sciencedirect,A visual interaction consensus model for social network group decision making with trust propagation,https://api.elsevier.com/content/abstract/scopus_id/85011271331,"A theoretical visual interaction framework to model consensus in social network group decision making (SN-GDM) is put forward with following three main components: (1) construction of trust relationship; (2) trust based recommendation mechanism; and (3) visual adoption mechanism. To do that, dual trust propagation is investigated to connect incomplete trust relationship by trusted third partners, in a way that it can fit our intuition in these cases: trust values decrease while distrust values increase. Trust relationship is proposed to be used in determining the trust degree of experts and in aggregating individual opinions into a collective one. Three levels of consensus degree are defined and used to identify the inconsistent experts. A trust based recommendation mechanism is developed to generate advices according to individual trust relationship, making recommendations more likeable to be implemented by the inconsistent experts to achieve higher levels of consensus. Therefore, it has an advantage with respect to existing interaction models because it does not force the inconsistent experts to accept advices irrespective of their trust on them. Finally, a visual adoption mechanism, which provides visual information representations on experts’ individual consensus positions before and after adopting the recommendation advices, is presented and analysed theoretically. Experts can select their appropriate feedback parameters to achieve a balance between group consensus and individual independence. Consequently, the proposed visual interaction model adds real and needed flexibility in guiding the consensus reaching process in SN-GDM.",science
10.1016/j.bios.2016.09.094,Journal,Biosensors and Bioelectronics,scopus,2017-04-15,sciencedirect,Sensing metabolites for the monitoring of tissue engineered construct cellularity in perfusion bioreactors,https://api.elsevier.com/content/abstract/scopus_id/85006115783,"As the field of tissue engineering progresses ever-further toward realizing clinical implementation of tissue-engineered constructs for wound regeneration, perhaps the most significant hurdle remains the establishment of non-destructive means for real-time in vitro assessment. In order to address this barrier, the study presented herein established the viability of the development of correlations between metabolic rates (specifically oxygen uptake, glucose consumption, and lactate production) and the cellularity of tissue-engineered cultures comprised of rat mesenchymal stem cells dynamically seeded on 85% porous nonwoven spunbonded poly(l-lactic acid) fiber mesh scaffolds. Said scaffolds were cultured for up to 21 days in a flow perfusion bioreactor system wherein α-MEM (supplemented with 10% fetal bovine serum and 1% antibiotic-antimycotic) was perfused directly through each scaffold at low flow rates (~0.15mL/min). Metabolite measurements were obtained intermittently through the use of a fiber-optic probe (for the case of oxygen) and biochemical assays (for glucose and lactate). Such measurements were subsequently correlated with cellularity data obtained utilizing current-standard destructive means. The resulting correlations, all exhibiting high R2 values, serve as a proof-on-concept for the use of metabolic data for the determination of scaffold cellularity in real-time non-destructively. This study can be easily adapted for use with various cell types, media formulations, and potentially different bioreactor systems. Implementation of more advanced in situ measurement devices could be easily accommodated to allow for true real-time, on-line metabolite monitoring and cellularity estimation.",science
10.1016/j.robot.2016.11.001,Journal,Robotics and Autonomous Systems,scopus,2017-04-01,sciencedirect,"Reprint of “Cognition, cognitics, and team action—Overview, foundations, and five theses for a better world”",https://api.elsevier.com/content/abstract/scopus_id/85011101312,"Consider now a shift of attention onto cognition. Novel definitions and metrics have been made, and it is time to reap the benefits, and to boost the development of intelligent autonomous systems. Mankind has gained a decisive advantage, in the race for survival and in the perspective of enjoyable lives, when cognitive abilities, i.e. cognition, appeared and started to develop in humans. Now cognition appears also as a crucial faculty to harness, i.e. to implement on machines; this is the field of cognitics. What is learnt about cognition for the purpose of machines, by a mirror effect, also affects the way we may recognize the role of cognition for ourselves, as humans. What is cognition? How does it relate to classical concepts, which appear much less well defined than expected? A summary of critical answers to these questions is sketched below. Then five theses about cognition are summarized: cognition to know the real world, to explore and perceive, to model; cognition for defining alternative worlds and possible futures, visions, and anticausality; cognition for effective control; cognitics for a large scale, technical deployment of cognition; and social cognitics, a foundation for team action and increased momentum for change. The five theses can be seen both as paths toward better insights in human and social nature and also as a roadmap for simultaneous and iterative processes capable to freely foster a better future for individuals and society. The paper finally includes as well an overview of MCS cognition theory, with some additional contributions, notably relating to foundations and time derivative aspects.",science
10.1016/j.ins.2016.12.047,Journal,Information Sciences,scopus,2017-04-01,sciencedirect,Combining tag correlation and user social relation for microblog recommendation,https://api.elsevier.com/content/abstract/scopus_id/85008402190,"With the development of social networking applications, microblog has turned to be an indispensable online communication network in our daily life. For microblog users, recommending high quality information is a demanding service. Some microblog services encourage users to annotate themselves with tags, which are used to describe their interests or attributes. However, few users are willing to create tags and available tags are not fully exploited for microblog recommendation. Besides, following/follower relationship in microblog is asymmetric, which can be used not only for communicating with friends or acquaintances but also for getting information on particular subjects. So far, there is no microblog recommendation algorithm which employs all the above mentioned information. This paper aims to investigate a joint framework to combine tag correlation and user social relation for microblog recommendation. Our approach identifies users’ interests via their personal tags and social relations. More specifically, a user tag retrieval strategy is established to add tags for users without or with few tags, and the user-tag matrix is then built and user-tag weights are then obtained. In order to solve the problem of sparsity of the matrix, both inner and outer correlation between tags are investigated to update the user-tag matrix. Considering the significance of user social relation for microblog recommendation, a user–user social relation similarity matrix is constructed. Moreover, an iterative updating scheme is developed to get the final tag-user matrix for computing the similarities between microblogs and users. We illustrate the capability of our algorithm by making experiments on real microblog datasets. Experimental results show that the algorithm is effective for microblog recommendation.",science
10.1016/j.neucom.2016.03.107,Journal,Neurocomputing,scopus,2017-03-29,sciencedirect,Influencing over people with a social emotional model,https://api.elsevier.com/content/abstract/scopus_id/85005873049,"This paper presents an approach of a social emotional model, which allows to extract the social emotion of a group of intelligent entities. The emotional model PAD allows to represent the emotion of an intelligent entity in 3-D space, allowing the representation of different emotional states. The social emotional model presented in this paper uses individual emotions of each one of the entities, which are represented in the emotional space PAD. Using a social emotional model within intelligent entities allows the creation of more real simulations, in which emotional states can influence decision-making. The result of this social emotional mode is represented by a series of examples, which are intended to represent a number of situations in which the emotions of each individual modify the emotion of the group. Moreover, the paper introduces an example which employs the proposed model in order to learn and predict future actions trying to influence in the social emotion of a group of people.",science
10.1016/j.aca.2016.12.034,Journal,Analytica Chimica Acta,scopus,2017-03-22,sciencedirect,Electrochemical immunosensor for simultaneous determination of interleukin-1 beta and tumor necrosis factor alpha in serum and saliva using dual screen printed electrodes modified with functionalized double–walled carbon nanotubes,https://api.elsevier.com/content/abstract/scopus_id/85008425442,"Dual screen-printed carbon electrodes modified with 4-carboxyphenyl-functionalized double-walled carbon nanotubes (HOOC-Phe-DWCNTs/SPCEs) have been used as scaffolds for the preparation of electrochemical immunosensors for the simultaneous determination of the cytokines Interleukin-1β (IL-1β) and factor necrosis tumor α (TNF-α). IL-1β. Capture antibodies were immobilized onto HOOC-Phe-DWCNTs/SPCEs in an oriented form making using the commercial polymeric coating Mix&Go™. Sandwich type immunoassays with amperometric signal amplification through the use of poly-HRP-streptavidin conjugates and H2O2 as HRP substrate and hydroquinone as redox mediator were implemented. Upon optimization of the experimental variables affecting the immunosensor performance, the dual immunosensor allows ranges of linearity extending between 0.5 and 100 pg/mL and from 1 to 200 pg/mL for IL-1β and TNF-α, respectively, these ranges being adequate for the determination of the cytokines in clinical samples. The achieved limits of detection were 0.38 pg/mL (IL-1β) and 0.85 pg/mL (TNF-α). In addition, the dual immunosensor exhibits excellent reproducibility of the measurements, storage stability of the anti-IL-Phe-DWCNTs/SPCE and anti-TNF-Phe-DWCNTs/SPCE conjugates, and selectivity as well as negligible cross-talking. The dual immunosensor was applied to the simultaneous determination of IL-1β and TNF-α in human serum spiked at clinically relevant concentration levels and in real saliva samples.",science
10.1016/j.chroma.2017.02.002,Journal,Journal of Chromatography A,scopus,2017-03-17,sciencedirect,Facile and fast preparation of low-cost silica-supported graphitic carbon nitride for solid-phase extraction of fluoroquinolone drugs from environmental waters,https://api.elsevier.com/content/abstract/scopus_id/85012907535,"The analytical application of silica-supported graphitic carbon nitride (g-C3N4@silica) for solid-phase extraction (SPE) of fluoroquinolone (FQ) pollutants from water is presented for the first time. g-C3N4@silica was easily and quickly prepared by one-pot thermal condensation of dicyandiamide and characterized by powder X-ray diffraction, thermogravimetric analysis, scanning electron microscopy, Fourier transform infrared spectroscopy and surface area measurements. The novel composite was applied as sorbent for SPE of FQs from water prior high-performance liquid chromatography with fluorescence detection. The extraction efficiency of g-C3N4 was tested in tap and surface waters at actual concentrations (10–100ngL−1). Quantitative adsorption was achieved using 100mg sorbent (20wt% g-C3N4) for pre-concentration of 50–500mL sample, at the native pH (∼7.5–8). Elution was performed with 25mM H3PO4 aqueous solution-acetonitrile (80:20), obtaining recoveries in the range 70–114%, enrichment factors up to 500 and inter-day RSDs≤12%. The batch-to-batch reproducibility was assessed on three independently synthesized g-C3N4@silica preparations (RSD 6–12%). g-C3N4 supported on silica microparticles proved to be of easy preparation, inexpensive, reusable for at least 4 extractions of raw surface waters, and suitable for determination in real matrices.",science
10.1016/j.eswa.2016.10.046,Journal,Expert Systems with Applications,scopus,2017-03-15,sciencedirect,Competence assessment as an expert system for human resource management: A mathematical approach,https://api.elsevier.com/content/abstract/scopus_id/84996720572,"Efficient human resource management needs accurate assessment and representation of available competences as well as effective mapping of required competences for specific jobs and positions. In this regard, appropriate definition and identification of competence gaps express differences between acquired and required competences. Using a detailed quantification scheme together with a mathematical approach is a way to support accurate competence analytics, which can be applied in a wide variety of sectors and fields. This article describes the combined use of software technologies and mathematical and statistical methods for assessing and analyzing competences in human resource information systems. Based on a standard competence model, which is called a Professional, Innovative and Social competence tree, the proposed framework offers flexible tools to experts in real enterprise environments, either for evaluation of employees towards an optimal job assignment and vocational training or for recruitment processes. The system has been tested with real human resource data sets in the frame of the European project called ComProFITS.",science
10.1016/j.egypro.2017.03.271,Conference Proceeding,Energy Procedia,scopus,2017-03-01,sciencedirect,Predicting Large Scale Fine Grain Energy Consumption,https://api.elsevier.com/content/abstract/scopus_id/85017254936,"Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window.",science
10.1016/j.juro.2016.09.091,Journal,Journal of Urology,scopus,2017-03-01,sciencedirect,Nanoknife Electroporation Ablation Trial: A Prospective Development Study Investigating Focal Irreversible Electroporation for Localized Prostate Cancer,https://api.elsevier.com/content/abstract/scopus_id/85010427109,"Purpose
                  Irreversible electroporation has attractive attributes for focal ablation, namely nonthermal effect, precise demarcation of treatment and tissue selectivity. We report a prospective development study investigating focal irreversible electroporation.
               
                  Materials and Methods
                  A total of 20 men with certain characteristics were recruited for study, including a visible index lesion on anterior magnetic resonance imaging that was concordant with transperineal targeted and template prostate mapping biopsy, absent clinically significant disease noted elsewhere (University College London definition 2) and prostate specific antigen 15 ng/ml or less. Our primary objective was to determine the side effect profile at 12 months. Secondary objectives included the domain specific toxicity profile using patient reported outcomes and early disease control using magnetic resonance imaging targeted biopsy.
               
                  Results
                  A total of 19 patients with median age of 60 years (IQR 53–66) and median prostate specific antigen 7.75 ng/ml (IQR 5.5–10.03) were treated. Of the patients 16 were available for estimating the first outcome as 1 was lost to followup and 2 had received another form of treatment by study end. All 16 men had pad-free/leak-free continence at 12 months. The proportion of men with erection sufficient for penetration decreased from 12 of 16 (75%) to 11 of 16 (69%). No serious adverse events were recorded. There was a statistically significant improvement in urinary symptoms according to changes in UCLA-EPIC (UCLA Expanded Prostate Cancer Index Composite) and I-PSS (International Prostate Symptom Score) (p = 0.039 and 0.001, respectively). Erectile function remained stable according to the change in IIEF-15 (15-Item International Index of Erectile Function) (p = 0.572). Median prostate specific antigen significantly decreased to 1.71 ng/ml (p = 0.001). One man refused followup biopsy. No residual disease was found in 11 patients (61.1%). One man (5.6%) harbored clinically insignificant disease and the remaining 6 (33.3%) harbored clinically significant disease.
               
                  Conclusions
                  Focal irreversible electroporation has low genitourinary toxicity. Additional studies are needed to optimize patient selection and treatment parameters.",science
10.1016/j.compositesb.2016.12.050,Journal,Composites Part B: Engineering,scopus,2017-03-01,sciencedirect,Digitisation of manual composite layup task knowledge using gaming technology,https://api.elsevier.com/content/abstract/scopus_id/85009923562,"Increased market demand for composite products and shortage of expert laminators is compelling the composite industry to explore ways to acquire layup skills from experts and transfer them to novices and eventually to machines. There is a lack of holistic methods in literature for capturing composite layup skills especially involving complex moulds. This research aims to develop an informatics-based method, enabled by consumer-grade gaming technology and machine learning, to capture and digitise manufacturing task knowledge from skill-intensive hand layup. The digitisation is underpinned by the proposed human-workpiece interaction theory and implemented to automatically extract and decode key knowledge constituents such as layup strategies, ply manipulation techniques, motion mechanics and problem-solving during hand layup, collectively categorised as layup skills. The significance of this research is its potential to facilitate cost-effective transfer of skills from experts to novices, real-time automated supervision of hand layup and automation of layup tasks in the future.",science
10.1016/j.jenvman.2016.10.056,Journal,Journal of Environmental Management,scopus,2017-02-01,sciencedirect,Improving the efficiency of dissolved oxygen control using an on-line control system based on a genetic algorithm evolving FWNN software sensor,https://api.elsevier.com/content/abstract/scopus_id/85006851639,"This work proposes an on-line hybrid intelligent control system based on a genetic algorithm (GA) evolving fuzzy wavelet neural network software sensor to control dissolved oxygen (DO) in an anaerobic/anoxic/oxic process for treating papermaking wastewater. With the self-learning and memory abilities of neural network, handling the uncertainty capacity of fuzzy logic, analyzing local detail superiority of wavelet transform and global search of GA, this proposed control system can extract the dynamic behavior and complex interrelationships between various operation variables. The results indicate that the reasonable forecasting and control performances were achieved with optimal DO, and the effluent quality was stable at and below the desired values in real time. Our proposed hybrid approach proved to be a robust and effective DO control tool, attaining not only adequate effluent quality but also minimizing the demand for energy, and is easily integrated into a global monitoring system for purposes of cost management.",science
10.1016/j.bios.2016.08.051,Journal,Biosensors and Bioelectronics,scopus,2017-01-15,sciencedirect,Novel hybrid probe based on double recognition of aptamer-molecularly imprinted polymer grafted on upconversion nanoparticles for enrofloxacin sensing,https://api.elsevier.com/content/abstract/scopus_id/84989815792,"A novel luminescent “double recognition” method for the detection of enrofloxacin (ENR) is developed to overcome some of the challenges faced by conventional molecularly imprinting. Biotinylated ENR aptamers immobilised on upconversion nanoparticles (UCNPs) surface are implemented to capture and concentrate ENR as the first imprinting recognition safeguard. After correct folding of the aptamer upon the existing targets, polymerization of methacrylic acid monomers around the ENR-aptamer complexes to interact with the residual functional groups of ENR by using molecularly imprinting techniques is the second imprinting recognition safeguard. The “double recognition” imprinting cavities are formed after removal of ENR, displaying recognition properties superior to that of aptamer or traditional molecularly imprinting alone. Another interest of this method is simultaneous molecular recognition and signal conversion by fabricating the “double recognition” receptor on to the surface of UCNPs to form a hybrid sensing system of apta-MIP/UCNPs. The proposed sensing method is applied to measure ENR in different fish samples. Good recoveries between 87.05% and 96.24%, and relative standard deviation (RSD) values in the range of 1.19–4.83% are obtained, with the limits of detection and quantification of 0.04 and 0.12ng/mL, respectively. It indicates that the sensing method is feasible for the quantification of target ENRs in real samples, and show great potential for wide-ranging application in bioassays.",science
10.1016/j.bios.2016.08.040,Journal,Biosensors and Bioelectronics,scopus,2017-01-15,sciencedirect,A contemporary approach for design and characterization of fiber-optic-cortisol sensor tailoring LMR and ZnO/PPY molecularly imprinted film,https://api.elsevier.com/content/abstract/scopus_id/84982300232,"A fiber optic salivary cortisol sensor using a contemporary approach of lossy mode resonance and molecular imprinting of nanocomposites of zinc oxide (ZnO) and polypyrrole (PPY) is structured and depicted for the concentration range of 0–10−6
                     g/ml of cortisol prepared in artificial saliva. Components of polymer preparation and the nanocomposite of polymer with ZnO are optimized for realizing the molecular imprinted layer of the sensor. Nanocomposite having 20% of ZnO in PPY is found to give highest sensitivity of the sensor. The sensor reports the best limit of detection ever reported with better stability, repeatability and response time. Lossy mode resonance based salivary cortisol sensor using nanocomposite molecular imprinted layer reported first time boosts the specificity of the sensor. The implementation of sensor over optical fiber adds up other advantages such as real time and online monitoring along with remote sensing abilities which makes the sensor usable for nonintrusive clinical applications.",science
10.1016/j.egypro.2017.11.100,Conference Proceeding,Energy Procedia,scopus,2017-01-01,sciencedirect,Constraint-Based Feedback for the Interactive Design of Buildings Thermal Insulating Envelopes,https://api.elsevier.com/content/abstract/scopus_id/85040023524,This paper discusses how to support the manual interactive design of buildings thermal insulating envelopes by means of a decision support system. The system provides visual feedback to the user design actions thus assisting its decision-making in real-time. The design problem has been modeled as a constrained two-dimensional packing problem that acts as foundation for an algorithmic solution for designing envelopes. Both model and implementation choices are discussed.,science
10.1016/j.procs.2017.10.053,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Simulation Game as a Reference to Smart City Management,https://api.elsevier.com/content/abstract/scopus_id/85040001312,"The purpose of this research is to know exactly what a ‘Smart City’ and its aspects are. Furthermore, to ensure this ‘Smart City’ aspects and their problems can be implemented in a game, so the game will be able to teach people about missed problem solving solutions. This research paper will discuss about the influence gained from playing a game upon city management skills, and careful planning on what must be done first, in order to maintain or increase the well-being of the citizen and the city welfare. By implementing real life applicable condition in the game which will be played by a person through a sandbox simulation style of gameplay, the player will be able to learn and experience certain types of city problems and the solutions to mitigate those problems without actually making it occur in a real city with real casualties, so it will be a safe vessel to gain new knowledge or to rediscover old effective methods, regarding city management, through a game. The results will portray, how can ‘Smart City’ aspects be implemented into a game, and how a city management simulation game can influence other people’ actions on their city management knowledge.",science
10.1016/B978-0-12-809859-2.00014-0,Book,Smart Sensors Networks: Communication Technologies and Intelligent Applications,scopus,2017-01-01,sciencedirect,Approaching Hardware Solutions for Massive E-Health Sensor Data Analysis,https://api.elsevier.com/content/abstract/scopus_id/85032163664,"The increase of life expectation and low birth rates have deeply impacted on the worldwide demographic structure. These changes impact on the public budgets and growing demands and expectations from citizens for higher quality services and social care. Several research efforts are devoted to provide alternatives to the traditional management of patients with innovative and not-intrusive systems to monitor in real-time the state and behavior of patients. Remote health monitoring systems can be used to monitor several vital parameters within a variety of ranges. These systems rely on heterogeneous data acquisition from sensors, video, historical and simulated data, performing inferences, and data elaboration in order to provide alternatives to the traditional management of patients. Depending on the functionalities to implement, the amount of data that has to be elaborated could represent the bottleneck of a monitoring system and it is critical in real-time applications. In this chapter we present a layered architecture infrastructure, based on two Decision Tree predictor hardware implementations, suitable for medical data analysis in real-time and aimed at dealing with a wide data volume and preserving a good hardware resources efficiency. We show how we improved the classification components and feature selection in order to achieve high-level throughput and high-level accuracy for the classification task of big data.",science
10.1016/j.procir.2017.03.125,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Ant Colony Optimization Algorithms to Enable Dynamic Milkrun Logistics,https://api.elsevier.com/content/abstract/scopus_id/85028644167,"Flexibility in combination with high capacities are the main reasons for milkruns being one of the most popular intralogistics solutions. In most cases they are only used for static routes to always deliver the same material to the same stations. However, in the context of Industry 4.0, milkrun logistic also has become very popular for use cases where different materials have to be delivered to different stations in little time, so routes cannot be planned in advance anymore. As loading and unloading the milkrun requires a significant amount of time, beside the routing problem itself, both driving and loading times have to be taken into account. Especially in scenarios where high flexibility is required those times will vary significantly and thus are a crucial factor for obtaining the optimal solution. Although containing stochastic components, those times can be predicted by considering the optimal point of time for delivery. In consequence, the best tradeoff between short routes and optimal delivery times is in favor of the shortest route. To solve this optimization problem a biology-inspired method – the ant colony optimization algorithm – has been enhanced to obtain the best solution regarding the above-mentioned aspects. A manufacturing scenario was used to prove the ability of the algorithm in real world problems. It also demonstrates the ability to adapt to changes in manufacturing systems very quickly by dynamically modelling and simulating the processes in intralogistics. The paper describes the ant colony optimization algorithm with the necessary extensions to enable it for milkrun logistic problems. Additionally the implemented software environment to apply the algorithm in practice is explained.",science
10.1016/j.watres.2017.07.035,Journal,Water Research,scopus,2017-01-01,sciencedirect,Short-term forecasting of turbidity in trunk main networks,https://api.elsevier.com/content/abstract/scopus_id/85025123888,"Water discolouration is an increasingly important and expensive issue due to rising customer expectations, tighter regulatory demands and ageing Water Distribution Systems (WDSs) in the UK and abroad. This paper presents a new turbidity forecasting methodology capable of aiding operational staff and enabling proactive management strategies. The turbidity forecasting methodology developed here is completely data-driven and does not require hydraulic or water quality network model that is expensive to build and maintain. The methodology is tested and verified on a real trunk main network with observed turbidity measurement data. Results obtained show that the methodology can detect if discolouration material is mobilised, estimate if sufficient turbidity will be generated to exceed a preselected threshold and approximate how long the material will take to reach the downstream meter. Classification based forecasts of turbidity can be reliably made up to 5 h ahead although at the expense of increased false alarm rates. The methodology presented here could be used as an early warning system that can enable a multitude of cost beneficial proactive management strategies to be implemented as an alternative to expensive trunk mains cleaning programs.",science
10.1016/j.ultsonch.2016.06.004,Journal,Ultrasonics Sonochemistry,scopus,2017-01-01,sciencedirect,Ultrasonic assisted dispersive solid-phase microextraction of Eriochrome Cyanine R from water sample on ultrasonically synthesized lead (II) dioxide nanoparticles loaded on activated carbon: Experimental design methodology,https://api.elsevier.com/content/abstract/scopus_id/84975525028,"The present research focus on designing an appropriate dispersive solid-phase microextraction (UA-DSPME) for preconcentration and determination of Eriochrome Cyanine R (ECR) in aqueous solutions with aid of sonication using lead (II) dioxide nanoparticles loaded on activated carbon (PbO-NPs-AC). This material was fully identified with XRD and SEM. Influence of pH, amounts of sorbent, type and volume of eluent, and sonication time on response properties were investigated and optimized by central composite design (CCD) combined with surface response methodology using STATISTICA. Among different solvents, dimethyl sulfoxide (DMSO) was selected as an efficient eluent, which its combination by present nanoparticles and application of ultrasound waves led to enhancement in mass transfer. The predicted maximum extraction (100%) under the optimum conditions of the process variables viz. pH 4.5, eluent 200μL, adsorbent dosage 2.5mg and 5min sonication was close to the experimental value (99.50%). at optimum conditions some experimental features like wide 5–2000ngmL−1 ECR, low detection limit (0.43ngmL−1, S/N=3:1) and good repeatability and reproducibility (relative standard deviation, <5.5%, n=12) indicate versatility in successful applicability of present method for real sample analysis. Investigation of accuracy by spiking known concentration of ECR over 200–600ngmL−1 gave mean recoveries from 94.850% to 101.42% under optimal conditions. The procedure was also applied for the pre-concentration and subsequent determination of ECR in tap and waste waters.",science
10.1016/j.comnet.2016.07.010,Journal,Computer Networks,scopus,2016-12-24,sciencedirect,Multi-criteria optimization of wireless connectivity over sparse networks,https://api.elsevier.com/content/abstract/scopus_id/84994311143,"Opportunistic networking is at the basis of cyber-physical Mobile Networks in Proximity (MNP), through its unique perspective over mobility and the incorporation of socio-inspired networking algorithms. However, results in the field are mostly theoretical, proven to account for stricter hit rate and latency requirements in specific environments. They generally assume that two devices being in proximity automatically see one-another, an assumption which might not stand under real-world conditions (Bluetooth assumes a peering session and close-proximity, WiFi Direct implementations are different between manufacturers, etc.).
                  Our previous studies in the area show that WiFi is still the most feasible media for opportunistic contacts. WiFi-enabled devices, with out-of-the-box networking capabilities, can connect in an ad-hoc opportunistic network, over wireless routers, and thus support a cyber-physical infrastructure for opportunistically spreading information.
                  In this article, we propose a machine learning algorithm that aims to increase the number of contacts between mobile nodes by using a smarter WiFi access point selection heuristic. The algorithm is based on properly balancing signal strength, latency, bandwidth, and, most importantly, the number of friends predicted to connect to the respective access point. We show through simulations based on real-life tracing data-sets that our proposed solution not only increases the likelihood of opportunistic contacts, but it also evenly distributes social subgraphs of users over wireless networks while improving the overall hit rate.",science
10.1016/j.bios.2016.06.080,Journal,Biosensors and Bioelectronics,scopus,2016-12-15,sciencedirect,Amplified impedimetric immunosensor based on instant catalyst for sensitive determination of ochratoxin A,https://api.elsevier.com/content/abstract/scopus_id/84978152277,"A new impedimetric immunosensor for the fast determination of ochratoxin A (OTA) in food samples was developed based on the instant catalyst as enhancer. Initially, the signal tags were prepared via co-immobilization of anti-OTA antibody and amine-terminated dendrimer (PAMAM) on the graphene oxide nanosheets through the covalent interaction, which were utilized as a good platform for combining manganese ion (anti-OTA-GO-PAMAM-Mn2+). Upon target OTA introduction, a competitive-type immunoreaction was implemented between the analyte and the immobilized OTA-BSA on the electrode for the anti-OTA antibody on the graphene oxide nanosheets labels. After a competitive immunoassay format, the anti-OTA-GO-PAMAM-Mn2+ were captured onto the electrode surface, which could induce the in situ formation of MnO2 
                     via classical redox reaction between Mn2+ and KMnO4 on the immunesensing platform. Moreover, the generated MnO2 nanoparticles act as efficient catalyst could catalyze the 4-chloro-1-naphthol (4-CN) oxidation without H2O2 to generate an insoluble precipitation on the platform. Under the optimal conditions, the instant catalyst based impedimetric immunosensor displayed a wide dynamic working range between 0.1pgmL−1 and 30ngmL−1. The detection limit (LOD) of the assay was 0.055pgmL−1. The developed method exhibited high selectivity and can be used for the determination of OTA in real red wine samples.",science
10.1016/j.jbi.2016.10.012,Journal,Journal of Biomedical Informatics,scopus,2016-12-01,sciencedirect,Sensitivity analysis of gene ranking methods in phenotype prediction,https://api.elsevier.com/content/abstract/scopus_id/84995705666,"Introduction
                  It has become clear that noise generated during the assay and analytical processes has the ability to disrupt accurate interpretation of genomic studies. Not only does such noise impact the scientific validity and costs of studies, but when assessed in the context of clinically translatable indications such as phenotype prediction, it can lead to inaccurate conclusions that could ultimately impact patients. We applied a sequence of ranking methods to damp noise associated with microarray outputs, and then tested the utility of the approach in three disease indications using publically available datasets.
               
                  Materials and methods
                  This study was performed in three phases. We first theoretically analyzed the effect of noise in phenotype prediction problems showing that it can be expressed as a modeling error that partially falsifies the pathways. Secondly, via synthetic modeling, we performed the sensitivity analysis for the main gene ranking methods to different types of noise. Finally, we studied the predictive accuracy of the gene lists provided by these ranking methods in synthetic data and in three different datasets related to cancer, rare and neurodegenerative diseases to better understand the translational aspects of our findings.
               
                  Results and discussion
                  In the case of synthetic modeling, we showed that Fisher’s Ratio (FR) was the most robust gene ranking method in terms of precision for all the types of noise at different levels. Significance Analysis of Microarrays (SAM) provided slightly lower performance and the rest of the methods (fold change, entropy and maximum percentile distance) were much less precise and accurate. The predictive accuracy of the smallest set of high discriminatory probes was similar for all the methods in the case of Gaussian and Log-Gaussian noise. In the case of class assignment noise, the predictive accuracy of SAM and FR is higher. Finally, for real datasets (Chronic Lymphocytic Leukemia, Inclusion Body Myositis and Amyotrophic Lateral Sclerosis) we found that FR and SAM provided the highest predictive accuracies with the smallest number of genes. Biological pathways were found with an expanded list of genes whose discriminatory power has been established via FR.
               
                  Conclusions
                  We have shown that noise in expression data and class assignment partially falsifies the sets of discriminatory probes in phenotype prediction problems. FR and SAM better exploit the principle of parsimony and are able to find subsets with less number of high discriminatory genes. The predictive accuracy and the precision are two different metrics to select the important genes, since in the presence of noise the most predictive genes do not completely coincide with those that are related to the phenotype. Based on the synthetic results, FR and SAM are recommended to unravel the biological pathways that are involved in the disease development.",science
10.1016/j.compbiomed.2016.10.006,Journal,Computers in Biology and Medicine,scopus,2016-12-01,sciencedirect,CAFÉ-Map: Context Aware Feature Mapping for mining high dimensional biomedical data,https://api.elsevier.com/content/abstract/scopus_id/84995610155,"Feature selection and ranking is of great importance in the analysis of biomedical data. In addition to reducing the number of features used in classification or other machine learning tasks, it allows us to extract meaningful biological and medical information from a machine learning model. Most existing approaches in this domain do not directly model the fact that the relative importance of features can be different in different regions of the feature space. In this work, we present a context aware feature ranking algorithm called CAFÉ-Map. CAFÉ-Map is a locally linear feature ranking framework that allows recognition of important features in any given region of the feature space or for any individual example. This allows for simultaneous classification and feature ranking in an interpretable manner. We have benchmarked CAFÉ-Map on a number of toy and real world biomedical data sets. Our comparative study with a number of published methods shows that CAFÉ-Map achieves better accuracies on these data sets. The top ranking features obtained through CAFÉ-Map in a gene profiling study correlate very well with the importance of different genes reported in the literature. Furthermore, CAFÉ-Map provides a more in-depth analysis of feature ranking at the level of individual examples.
                  
                     Availability: CAFÉ-Map Python code is available at:
                  
                     http://faculty.pieas.edu.pk/fayyaz/software.html#cafemap .
                  The CAFÉ-Map package supports parallelization and sparse data and provides example scripts for classification. This code can be used to reconstruct the results given in this paper.",science
10.1016/j.watres.2016.10.020,Journal,Water Research,scopus,2016-12-01,sciencedirect,Enhanced detection of pathogenic enteric viruses in coastal marine environment by concentration using methacrylate monolithic chromatographic supports paired with quantitative PCR,https://api.elsevier.com/content/abstract/scopus_id/84991227463,"Currently, around 50% of the world's population lives in towns and cities within 100 km of the coast. Monitoring of viruses that are frequently present in contaminated coastal environments, such as rotavirus (RoV) and norovirus (NoV), which are also the major cause of human viral gastroenteritis, is essential to ensure the safe use of these water bodies. Since exposure to as few as 10–100 particles of RoV or NoV may induce gastrointestinal disease, there is a need to develop a rapid and sensitive diagnostic method for their detection in coastal water samples. In this study, we evaluate the application of methacrylate monolithic chromatographic columns, commercially available as convective interaction media (CIM®), to concentrate pathogenic enteric viruses from saline water samples prior to virus quantification by one-step reverse transcription quantitative PCR (RT-qPCR). Using RoV and NoV as model enteric viruses, we present our results on the most effective viral concentration conditions from saline water matrices using butyl (C4) hydrophobic interaction monolithic support (CIM® C4). C4 monolithic columns exhibit a good capacity to bind both RoV and NoV and both viruses can be eluted in a single step. Our protocol using a 1 ml C4 column enables processing of 400 ml saline water samples in less than 60 min and increases the sensitivity of RoV and NoV detection by approximately 50-fold and 10-fold respectively. The protocol was also scaled up using larger capacity 8 ml C4 columns to process 4000 ml of seawater samples with concentration factors of 300-fold for RoV and 40-fold for NoV, without any significant increase in processing time. Furthermore, C4 monolithic columns were adapted for field use in an on-site application of RoV concentration from seawater samples with performance equivalent to that of the reference laboratory setup. Overall, the results from successful deployment of CIM C4 columns for concentration of rotavirus and norovirus in seawater samples reiterate the utility of monolithic supports as efficient, scalable and modular preparative tools for processing environmental water samples to enhance viral detection using molecular methods.",science
10.1016/j.cogsys.2016.03.001,Journal,Cognitive Systems Research,scopus,2016-12-01,sciencedirect,Simulation within simulation for agent decision-making: Theoretical foundations from cognitive science to operational computer model,https://api.elsevier.com/content/abstract/scopus_id/84963705117,"This article deals with artificial intelligence models inspired from cognitive science. The scope of this paper is the simulation of the decision-making process for virtual entities. The theoretical framework consists of concepts from the use of internal behavioral simulation for human decision-making. Inspired from such cognitive concepts, the contribution consists in a computational framework that enables a virtual entity to possess an autonomous world of simulation within the simulation. It can simulate itself (using its own model of behavior) and simulate its environment (using its representation of other entities). The entity has the ability to anticipate using internal simulations, in complex environments where it would be extremely difficult to use formal proof methods. Comparing the prediction and the original simulation, its predictive models are improved through a learning process. Illustrations of this model are provided through two implementations. First illustration is an example showing a shepherd, his herd and dogs. The dog simulates the sheep’s behavior in order to make predictions testing different strategies. Second, an artificial 3D juggler plays in interaction with virtual jugglers, humans and robots. For this application, the juggler predicts the behavior of balls in the air and uses prediction to coordinate its behavior in order to juggle.",science
10.1016/j.knosys.2016.09.004,Journal,Knowledge-Based Systems,scopus,2016-11-15,sciencedirect,FindMal: A file-to-file social network based malware detection framework,https://api.elsevier.com/content/abstract/scopus_id/84994082416,"The rapid development of malicious software programs has posed severe threats to Computer and Internet security. Therefore, it motivates anti-malware vendors and researchers to develop novel methods which are capable of protecting users against new threats. Existing malware detectors mostly treat the file samples separately using supervised learning algorithms. However, ignoring the relationship among file samples limits the capability of malware detectors. In this paper, based on the file-to-file social network, we present a new malware detection framework, FindMal(File-to-File Social Network based 
                     Malware Detection Framework), including graph-based features extraction, Label Propagation algorithm, and active learning strategy. Nearest neighbors are first chosen as adjacent nodes for each file node to construct kNN file relation graph. Three file relation graph features are proposed to sample the representative file samples for labeling. Then, Label Propagation algorithm, which propagates the label information from labeled file samples to unlabeled files, is applied to learn the probability that one unknown file is classified as malicious or benign. A batch mode active learning method is employed to reduce the labeling cost and improve the performance of Label Propagation. Comprehensive experiments on real and large scale dataset obtained from an anti-malware company are performed. The results demonstrate that our proposed FindMal outperforms other existing detection models in classifying file samples.",science
10.1016/j.ins.2016.06.040,Journal,Information Sciences,scopus,2016-11-10,sciencedirect,A topic-enhanced word embedding for Twitter sentiment classification,https://api.elsevier.com/content/abstract/scopus_id/84976582889,"Word representation is crucial to lexical features used in Twitter sentiment analysis models. Recent work has demonstrated that dense, low-dimensional and real-valued word embedding gives competitive performance for Twitter sentiment classification. We follow this line of work, and propose a topic-enhanced word embedding for the task, which is generally neglected in previous work. Firstly, we exploit a recursive autoencoder framework to learn topic-enhanced word embedding, where topic information is generated through topic modeling based on an effective implementation of Latent Dirichlet Allocation (LDA). Then we use a uniform framework by adopting Support Vector Machine (SVM) classifier, to compare existing word representation methods with our method. Experimental results on the dataset show that topic-enhanced word embedding is very effective for Twitter sentiment classification.",science
10.1016/j.resconrec.2016.03.012,Journal,"Resources, Conservation and Recycling",scopus,2016-11-01,sciencedirect,Implementation of OPTIMASS to optimise municipal wastewater sludge processing chains: Proof of concept,https://api.elsevier.com/content/abstract/scopus_id/85028239611,"In sludge management, sludge is increasingly perceived as a marketable product rather than as a waste material. This awareness in combination with the variety of factors influencing the optimal management strategy and disposal route, introduces the need to optimise the sludge treatment throughout the whole chain instead of only minimising its production. In this paper, OPTIMASS, a mixed integer linear programming model to optimise strategic and tactical decisions in biomass-based supply chains, is proposed in order to meet this need. The applicability of OPTIMASS is illustrated through its implementation with a view to minimise the overall global warming impact of a real municipal wastewater sludge processing chain in “region X”. A first scenario addresses the optimisation of the allocation and treatment of municipal wastewater sludge within the current network. Second, OPTIMASS is used to identify the optimal location(s) for new drying facilities in this chain. Finally, the effect on the optimal chain of changes in municipal wastewater sludge production and of changes in global warming impact of the cement industry as a disposal route is evaluated.
                  The analysis reveals that municipal wastewater sludge processing chains can be considered to be instances of the generic biomass-based supply chain and that the OPTIMASS tool can be applied to support strategic and tactical decisions for optimising sludge management in case new technologies, new treatment facility locations, new disposal options, etc. are at stake. The validity of the OPTIMASS approach is confirmed by the close correspondence between its outcome and the results of a decision support system, specifically developed for the municipal wastewater sludge processing chain.",science
10.1016/j.robot.2016.08.008,Journal,Robotics and Autonomous Systems,scopus,2016-11-01,sciencedirect,"Cognition, cognitics, and team action—Overview, foundations, and five theses for a better world",https://api.elsevier.com/content/abstract/scopus_id/85011032498,"Consider now a shift of attention onto cognition. Novel definitions and metrics have been made, and it is time to reap the benefits, and to boost the development of intelligent autonomous systems. Mankind has gained a decisive advantage, in the race for survival and in the perspective of enjoyable lives, when cognitive abilities, i.e. cognition, appeared and started to develop in humans. Now cognition appears also as a crucial faculty to harness, i.e. to implement on machines; this is the field of cognitics. What is learnt about cognition for the purpose of machines, by a mirror effect, also affects the way we may recognize the role of cognition for ourselves, as humans. What is cognition? How does it relate to classical concepts, which appear much less well defined than expected? A summary of critical answers to these questions is sketched below. Then five theses about cognition are summarized: cognition to know the real world, to explore and perceive, to model; cognition for defining alternative worlds and possible futures, visions, and anticausality; cognition for effective control; cognitics for a large scale, technical deployment of cognition; and social cognitics, a foundation for team action and increased momentum for change. The five theses can be seen both as paths toward better insights in human and social nature and also as a roadmap for simultaneous and iterative processes capable to freely foster a better future for individuals and society. The paper finally includes as well an overview of MCS cognition theory, with some additional contributions, notably relating to foundations and time derivative aspects.",science
10.1016/j.jmoldx.2016.07.001,Journal,Journal of Molecular Diagnostics,scopus,2016-11-01,sciencedirect,CNV-RF Is a Random Forest–Based Copy Number Variation Detection Method Using Next-Generation Sequencing,https://api.elsevier.com/content/abstract/scopus_id/84992517622,"Simultaneous detection of small copy number variations (CNVs) (<0.5 kb) and single-nucleotide variants in clinically significant genes is of great interest for clinical laboratories. The analytical variability in next-generation sequencing (NGS) and artifacts in coverage data because of issues with mappability along with lack of robust bioinformatics tools for CNV detection have limited the utility of targeted NGS data to identify CNVs. We describe the development and implementation of a bioinformatics algorithm, copy number variation–random forest (CNV-RF), that incorporates a machine learning component to identify CNVs from targeted NGS data. Using CNV-RF, we identified 12 of 13 deletions in samples with known CNVs, two cases with duplications, and identified novel deletions in 22 additional cases. Furthermore, no CNVs were identified among 60 genes in 14 cases with normal copy number and no CNVs were identified in another 104 patients with clinical suspicion of CNVs. All positive deletions and duplications were confirmed using a quantitative PCR method. CNV-RF also detected heterozygous deletions and duplications with a specificity of 50% across 4813 genes. The ability of CNV-RF to detect clinically relevant CNVs with a high degree of sensitivity along with confirmation using a low-cost quantitative PCR method provides a framework for providing comprehensive NGS-based CNV/single-nucleotide variant detection in a clinical molecular diagnostics laboratory.",science
10.1016/j.forsciint.2016.09.010,Journal,Forensic Science International,scopus,2016-11-01,sciencedirect,Pornography classification: The hidden clues in video space–time,https://api.elsevier.com/content/abstract/scopus_id/84989311576,"As web technologies and social networks become part of the general public's life, the problem of automatically detecting pornography is into every parent's mind — nobody feels completely safe when their children go online. In this paper, we focus on video-pornography classification, a hard problem in which traditional methods often employ still-image techniques — labeling frames individually prior to a global decision. Frame-based approaches, however, ignore significant cogent information brought by motion. Here, we introduce a space-temporal interest point detector and descriptor called Temporal Robust Features (TRoF). TRoF was custom-tailored for efficient (low processing time and memory footprint) and effective (high classification accuracy and low false negative rate) motion description, particularly suited to the task at hand. We aggregate local information extracted by TRoF into a mid-level representation using Fisher Vectors, the state-of-the-art model of Bags of Visual Words (BoVW). We evaluate our original strategy, contrasting it both to commercial pornography detection solutions, and to BoVW solutions based upon other space-temporal features from the scientific literature. The performance is assessed using the Pornography-2k dataset, a new challenging pornographic benchmark, comprising 2000 web videos and 140h of video footage. The dataset is also a contribution of this work and is very assorted, including both professional and amateur content, and it depicts several genres of pornography, from cartoon to live action, with diverse behavior and ethnicity. The best approach, based on a dense application of TRoF, yields a classification error reduction of almost 79% when compared to the best commercial classifier. A sparse description relying on TRoF detector is also noteworthy, for yielding a classification error reduction of over 69%, with 19× less memory footprint than the dense solution, and yet can also be implemented to meet real-time requirements.",science
10.1016/j.engappai.2016.09.004,Journal,Engineering Applications of Artificial Intelligence,scopus,2016-11-01,sciencedirect,A feature selection method for author identification in interactive communications based on supervised learning and language typicality,https://api.elsevier.com/content/abstract/scopus_id/84988025909,"Authorship attribution, conceived as the identification of the origin of a text between different authors, has been a very active area of research in the scientific community mainly supported by advances in Natural Language Processing (NLP), machine learning and Computational Intelligence. This paradigm has been mostly addressed from a literary perspective, aiming at identifying the stylometric features and writeprints which unequivocally typify the writer patterns and allow their unique identification. On the other hand, the upsurge of social networking platforms and interactive messaging have undoubtedly made the anonymous expression of feelings, the sharing of experiences and social relationships much easier than in other traditional communication media. Unfortunately, the popularity of such communities and the virtual identification of their users deploy a rich substrate for cybercrimes against unsuspecting victims and other forms of illegal uses of social networks that call for the activity tracing of accounts. In the context of one-to-one communications this manuscript postulates the identification of the sender of a message as a useful approach to detect impersonation attacks in interactive communication scenarios. In particular this work proposes to select linguistic features extracted from messages via NLP techniques by means of a novel feature selection algorithm based on the dissociation between essential traits of the sender and receiver influences. The performance and computational efficiency of different supervised learning models when incorporating the proposed feature selection method is shown to be promising with real SMS data in terms of identification accuracy, and paves the way towards future research lines focused on applying the concept of language typicality in the discourse analysis field.",science
10.1016/j.scico.2016.03.009,Journal,Science of Computer Programming,scopus,2016-11-01,sciencedirect,Model-driven processes and tools to design robot-based generative learning objects for computer science education,https://api.elsevier.com/content/abstract/scopus_id/84971228847,"In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes CS learning variability modelling using the feature-based approaches borrowed from the SW engineering domain. Firstly, we define the CS learning domain using the known educational framework TPACK (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the CS learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The GLO design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design GLOs manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the GLO design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of GLOs themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented GLOs as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented GLOs and their supporting tools.",science
10.1016/j.autcon.2016.03.012,Journal,Automation in Construction,scopus,2016-11-01,sciencedirect,"SmartSite: Intelligent and autonomous environments, machinery, and processes to realize smart road construction projects",https://api.elsevier.com/content/abstract/scopus_id/84969580061,"This article presents an overview of the SmartSite research project that adopts machine learning, decision theory and distributed artificial intelligence to design and test a multi-agent system (MAS) for asphalt road construction. SmartSite puts major emphasis on sensing and communication technologies that integrate real-time automated information exchange in the supply chain of road construction. As part of the larger SmartSite project, this article introduces a novel real-time path planning system for compactors and presents the results of several simulation and field realistic experiments conducted to evaluate the system in a sophisticated simulation and harsh construction environment, respectively. The system operates based on Belief-Desire-Intention (BDI) software agents and real-time sensory inputs. The newly developed integrated and information rich process benefits asphalt compactor operators, as they are now capable to control their machinery and react to changing environmental, material-related and process-related disturbances or changes. This improves the quality of the delivery and laying of asphalt material, prevents compactors from over-compacting certain road segments, increases the road's pavement longevity during the operational life cycle phase; refocuses the work tasks of the site managers, and reduces the construction budget and schedule. The system's ability to maneuver an asphalt roller during real-word operation also makes it an important step towards a fully automated asphalt compactor.",science
10.1016/j.neucom.2015.10.142,Journal,Neurocomputing,scopus,2016-10-19,sciencedirect,Highly efficient epidemic spreading model based LPA threshold community detection method,https://api.elsevier.com/content/abstract/scopus_id/84991200414,"Community structure is the most basic and important topological characteristic of complex network and community detection method is therefore significant to its character statistics. Community detection in heterogeneous structured network is an attractive research problem while most of the previous approaches and algorithms attempt to divide networks into communities based on node or edge measurement. The Label Propagation Algorithm (LPA) adopted semi-supervised machine learning and implemented community detection intelligently with automatic convergent process of network entity label iteration. But LPA often resulted in low efficiency in the final convergent process. In this paper, aiming to promote the low running efficiency and converging rate of LPA in overlapping community detection and focusing on user label selection for behavior analysis, we proposed a new method ESLPA (Epidemic Spreading based LPA) for community detection using epidemic spreading model combined with network connection matrix's largest Eigenvalue as label propagation threshold. Extensive experiments in artificial network dataset and real-life large networks derived from online social media have been conducted to explore the optimal mechanism of the most suitable community detection method by virus infection threshold. According to experimental result, it has been proved that our method is more accurate and faster than several traditional modularity based community detection methods.",science
10.1016/j.knosys.2016.07.027,Journal,Knowledge-Based Systems,scopus,2016-10-15,sciencedirect,A local dynamic method for tracking communities and their evolution in dynamic networks,https://api.elsevier.com/content/abstract/scopus_id/84979706641,"The analysis of communities and their evolutionary behaviors in dynamic networks is a challenging topic. Although a growing body of work on this topic is emerging, there are few methods which can reveal and track meaningful communities over time and can also deal with large networks efficiently. In this paper, we propose a method to track dynamic communities and their evolutionary behaviors. The main idea behind our method is to discover dynamic communities by exploring the local views of nodes that change. Moreover, based on the discovered dynamic communities, the global community structure can be derived by updating the historical community structure and the evolutionary behaviors of communities can also be tracked. To discover the dynamic communities, we apply the technique of approximate personalized PageRank vector; to track the evolutionary behaviors of the communities, we introduce a partial evolutionary graph. We compare the proposed method with several existing methods by performing experiments on nine synthetic networks and one real network. The experimental results show that the proposed method performs well on discovering communities as well as tracking their evolution in dynamic networks, and spends much less running time than the existing methods.",science
10.1016/j.dental.2016.07.013,Journal,Dental Materials,scopus,2016-10-01,sciencedirect,A novel experimental approach to investigate the effect of different agitation methods using sodium hypochlorite as an irrigant on the rate of bacterial biofilm removal from the wall of a simulated root canal model,https://api.elsevier.com/content/abstract/scopus_id/84991063671,"Objective
                  Root canal irrigation is an important adjunct to control microbial infection. This study aimed primarily to develop a transparent root canal model to study in situ Enterococcus faecalis biofilm removal rate and remaining attached biofilm using passive or active irrigation solution for 90s. The change in available chlorine and pH of the outflow irrigant were assessed.
               
                  Methods
                  A total of forty root canal models (n
                     =10 per group) were manufactured using 3D printing. Each model consisted of two longitudinal halves of an 18mm length simulated root canal with size 30 and taper 0.06. E. faecalis biofilms were grown on the apical 3mm of the models for 10days in Brain Heart Infusion broth. Biofilms were stained using crystal violet for visualization. The model halves were reassembled, attached to an apparatus and observed under a fluorescence microscope. Following 60s of 9mL of 2.5% NaOCl irrigation using syringe and needle, the irrigant was either left stagnant in the canal or activated using gutta-percha, sonic and ultrasonic methods for 30s. Images were then captured every second using an external camera. The residual biofilm percentages were measured using image analysis software. The data were analyzed using Kruskal–Wallis test and generalized linear mixed model.
               
                  Results
                  The highest level of biofilm removal was with ultrasonic agitation (90.13%) followed by sonic (88.72%), gutta-percha (80.59%), and passive irrigation group (control) (43.67%) respectively. All agitation groups reduced the available chlorine and pH of NaOCl more than that in the passive irrigation group.
               
                  Significance
                  The 3D printing method provided a novel model to create a root canal simulation for studying and understanding a real-time biofilm removal under microscopy. Ultrasonic agitation of NaOCl left the least amount of residual biofilm in comparison to sonic and gutta-percha agitation methods.",science
10.1016/j.biopha.2016.08.002,Journal,Biomedicine and Pharmacotherapy,scopus,2016-10-01,sciencedirect,Therapeutic potential of Taraxacum officinale against HCV NS5B polymerase: In-vitro and In silico study,https://api.elsevier.com/content/abstract/scopus_id/84982824816,"Discovery of alternative and complementary regimens for HCV infection treatment is a need of time from clinical as well as economical point of views. Low cost of bioactive natural compounds production, high biochemical diversity and inexistent/milder side effects contribute to new therapies. Aim of this study is to clarify anti-HCV role of Taraxacum officinale, a natural habitat plant rich of flavonoids. In this study, methanol extract of T. officinale leaves was initially analyzed for its cytotoxic activity in human hepatoma (Huh-7) and CHO cell lines. Hepatoma cells were transfected with pCR3.1/Flagtag/HCV NS5B gene cloned vector (genotype 1a) along with T. officinale extract. Considering NS5B polymerase as potential therapeutic drug target, twelve phytochemicals of T. officinale were selected as ligands for molecular interaction with NS5B protein using Molecular Operating Environment (MOE) software. Sofosbuvir (Sovaldi: brand name) currently approved as new anti-HCV drug, was used as standard in current study for comparative analysis in computational docking screening. HCV NS5B polymerase as name indicates plays key role in viral genome replication. On the basis of which NS5B gene is targeted for determining antiviral role of T. officinale extract and 65% inhibition of NS5B expression was documented at nontoxic dose concentration (200μg/ml) using Real-time PCR. In addition, 57% inhibition of HCV replication was recorded when incubating Huh-7 cells with high titer serum of HCV infected patients along with leaves extract. Phytochemicals for instance d-glucopyranoside (−31.212 Kcal/mol), Quercetin (−29.222 Kcal/mol), Luteolin (−26.941 Kcal/mol) and some others displayed least binding energies as compared to standard drug Sofosbuvir (−21.0746 Kcal/mol). Results of our study strongly revealed that T. officinale leaves extract potentially blocked the viral replication and NS5B gene expression without posing any toxic effect on normal fibroblast cells of body.",science
10.1016/j.enbuild.2016.08.023,Journal,Energy and Buildings,scopus,2016-10-01,sciencedirect,A multi-criteria group decision-making method for the thermal renovation of masonry buildings: The case of Algeria,https://api.elsevier.com/content/abstract/scopus_id/84981505010,"The future of masonry buildings with heritage values is certain ⿿ the investments in making such buildings energy-efficient during renovations to meet the energy consumption requirements will increase over the next decade. However, decision makers fail to address the concerns of each project actor and give specific answers on how basic requirements on such historical buildings can be implemented. This paper proposes a new multi-criteria group decision-making method for the thermal renovation of masonry buildings. The aim of the proposed method is to rank different renovation solutions. The method uses; the structured group interaction method Delphi to define the evaluation criteria and the thermal renovations solutions, Swing method to facilitate the process of the determination of the criteria weights, the group decision support system (PROMETHEE GDSS) to reach a global ranking of the renovations solutions, PROMETHEE V to introduce additional constraints, as well as the Graphical Analysis for Interactive Aid (GAIA) analysis to get a better understanding of conflicts and similarities between the criteria and among the decision makers. We proceed to exemplify by means of a real-life case project in Algeria and offer suggestions on what front-ended stakeholders could do to reduce the energy consumption in masonry buildings.",science
10.1016/j.chroma.2016.06.078,Journal,Journal of Chromatography A,scopus,2016-08-19,sciencedirect,A polythiophene–silver nanocomposite for headspace needle trap extraction,https://api.elsevier.com/content/abstract/scopus_id/84979519284,"A nanocomposite consisting of polythiophene–silver was prepared and implemented as a desired sorbent for headspace needle trap extraction. Colloidal silver nanoparticles (Ag NPs) with narrow size distribution and high stability were synthesized in water–in–oil microemulsion. This simple procedure was adapted to prepare highly monodispersed Ag NPs, starting from an initial synthesis in sodium bis(2-ethylhexyl) sulfosuccinate (AOT) reverse micelles. Polythiophene (PT) was synthesized by chemical oxidative polymerization in the presence of anhydrous ferric chloride while its polymeric structure was confirmed by Fourier transform infrared spectrometry (FTIR). Eventually, the prepared PT was dispersed in an AOT/n-decane solution containing Ag NPs for 1h in which the NPs were adsorbed on the polymer surface. The dynamic light scattering (DLS) analysis of NPs solution revealed that the monodisperse Ag NPs have been synthesized successfully with the size distribution below 10nm. Other instrumentations such as scanning electron microscopy (SEM), energy dispersive spectroscopy (EDS) and atomic absorption spectrometry (AAS) confirmed the fabrication of the PT–Ag nanocomposite. The applicability of the synthesized sorbent was examined by needle trap extraction of some polycyclic aromatic hydrocarbons (PAHs) in aqueous samples in conjunction with gas chromatography–mass spectrometry detection (GC–MS). Important parameters influencing the extraction process were optimized. The linearity for all analytes was in the concentration range of 0.01–10ngmL−1. The limits of detections were in the range of 0.002–0.01ngmL−1, using time–scheduled selected ion monitoring (SIM) mode while the RSD% values (n
                     
                     =
                     
                     3) were all below 12%. The developed method was successfully applied to real water samples obtained from different rivers and Persian Gulf, while the relative recovery percentages were in the range of 85–103%.",science
10.1016/j.cose.2016.05.005,Journal,Computers and Security,scopus,2016-08-01,sciencedirect,Íntegro: Leveraging victim prediction for robust fake account detection in large scale OSNs,https://api.elsevier.com/content/abstract/scopus_id/84974733314,"Detecting fake accounts in online social networks (OSNs) protects both OSN operators and their users from various malicious activities. Most detection mechanisms attempt to classify user accounts as real (i.e., benign, honest) or fake (i.e., malicious, Sybil) by analyzing either user-level activities or graph-level structures. These mechanisms, however, are not robust against adversarial attacks in which fake accounts cloak their operation with patterns resembling real user behavior.
                  In this article, we show that victims – real accounts whose users have accepted friend requests sent by fakes – form a distinct classification category that is useful for designing robust detection mechanisms. In particular, we present Íntegro – a robust and scalable defense system that leverages victim classification to rank most real accounts higher than fakes, so that OSN operators can take actions against low-ranking fake accounts. Íntegro starts by identifying potential victims from user-level activities using supervised machine learning. After that, it annotates the graph by assigning lower weights to edges incident to potential victims. Finally, Íntegro ranks user accounts based on the landing probability of a short random walk that starts from a known real account. As this walk is unlikely to traverse low-weight edges in a few steps and land on fakes, Íntegro achieves the desired ranking.
                  We implemented Íntegro using widely-used, open-source distributed computing platforms, where it scaled nearly linearly. We evaluated Íntegro against SybilRank, which is the state-of-the-art in fake account detection, using real-world datasets and a large-scale deployment at Tuenti – the largest OSN in Spain with more than 15 million active users. We show that Íntegro significantly outperforms SybilRank in user ranking quality, with the only requirement that the employed victim classifier is better than random. Moreover, the deployment of Íntegro at Tuenti resulted in up to an order of magnitude higher precision in fake account detection, as compared to SybilRank.",science
10.1016/j.bica.2016.07.006,Journal,Biologically Inspired Cognitive Architectures,scopus,2016-07-01,sciencedirect,Analyzing and discussing primary creative traits of a robotic artist,https://api.elsevier.com/content/abstract/scopus_id/84994896174,"We present a robot aimed at producing a collage formed by a mix of photomontage and digital collage. The artwork is created after a visual and verbal interaction with a human user. The proposed system, through a cognitive architecture, allows the robot to manage the three different phases of the real-time artwork process: (i) taking inspiration from information captured during the postural and verbal interaction with the human user and from the analysis of his/her social web items; (ii) performing a creative process to obtain a model of the artwork; (iii) executing the creative collage composition and providing a significant title. The paper explains, primarily, how the creativity traits of the robot are implemented in the proposed architecture: how ideas are generated through an elaboration that is modulated by affective influences; how the personality and the artistic behavior are modeled by learning and guided by external evaluations; the motivation and the confidence evolution as a function of successes or failures.",science
10.1016/j.cmpb.2016.04.005,Journal,Computer Methods and Programs in Biomedicine,scopus,2016-07-01,sciencedirect,A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset,https://api.elsevier.com/content/abstract/scopus_id/84964534094,"Background
                  In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
               
                  Method
                  In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
               
                  Results
                  To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
               
                  Conclusions
                  The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",science
10.1016/j.future.2015.10.013,Journal,Future Generation Computer Systems,scopus,2016-06-01,sciencedirect,Real-time data mining of massive data streams from synoptic sky surveys,https://api.elsevier.com/content/abstract/scopus_id/84961285522,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arises in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments.",science
10.1016/j.ins.2016.01.063,Journal,Information Sciences,scopus,2016-06-01,sciencedirect,A hybrid approach with agent-based simulation and clustering for sociograms,https://api.elsevier.com/content/abstract/scopus_id/84959322738,"In the last years, some features of sociograms have proven to be strongly related to the performance of groups. However, the prediction of sociograms according to the features of individuals is still an open issue. In particular, the current approach presents a hybrid approach between agent-based simulation and clustering for simulating sociograms according to the psychological features of their members. This approach performs the clustering extracting certain types of individuals regarding their psychological characteristics, from training data. New people can then be associated with one of the types in order to run a sociogram simulation. This approach has been implemented with the tool called CLUS-SOCI (an agent-based and CLUStering tool for simulating SOCIograms). The current approach has been experienced with real data from four different secondary schools, with 38 real sociograms involving 714 students. Two thirds of these data were used for training the tool, while the remaining third was used for validating it. In the validation data, the resulting simulated sociograms were similar to the real ones in terms of cohesion, coherence of reciprocal relations and intensity, according to the binomial test with the correction of Bonferroni.",science
10.1016/j.ijepes.2015.12.013,Journal,International Journal of Electrical Power and Energy Systems,scopus,2016-06-01,sciencedirect,Demand response governed swarm intelligent grid scheduling framework for social welfare,https://api.elsevier.com/content/abstract/scopus_id/84952836614,"Peak load defines the generation, transmission and distribution capacity of interconnected power network. As load changes throughout the day and the year, electricity systems must be able to deliver the maximum load at all times, which will be hard trade for a practical power network. Smart grid technologies show strong potential to optimize asset utilization by shifting peak load to off peak times, thereby decoupling the electricity growth from peak load growth. Under Smart grid trade regulation, with continuous varying demand pattern, electricity price will be uneven as well. On this view point, in order to obtain a flatten demand, without affecting the welfare of the market participants, this paper presents an on-going effort to develop Demand Response (DR) governed swarm intelligence based stochastic peak load modeling methodology capable of restoring the market equilibrium during price and demand oscillations of the real-time smart power networks. This proposed DR based methodology allows generators and loads to interact in an automated fashion in real time, coordinating demand to flatten spikes and thereby minimizing erratic variations of price of electricity. For proper utilization of DR connectivity, a Curtailment Limiting Index (CLI) has been formulated, monitoring which in real time, for each of the Load Dispatch Centers (LDCs), the system operator can shape the electricity demand according to the available capacity of generation, transmission and distribution assets. The proposed methodology can also be highlighted for generating the most economical schedule for social welfare with standard operational status in terms of voltage profile, system loss and optimal load curtailment. The case study has been carried out in IEEE 30 bus scenario as well as on a practical 203 bus-265 line power network (Indian Eastern Grid) with both generator characteristics and price responsive demand characteristics or DR as inputs and illustrious Particle Swarm Optimization (PSO) technique has assisted the fusion of the proposed model and methodology. Encouraging simulation results suggest that, the effective deployment of this methodology may lead to an operating condition where an overall benefit of all the power market participants with standard operational status can be ensured and the misuse of electricity will be minimized.",science
10.1016/j.jbi.2016.01.005,Journal,Journal of Biomedical Informatics,scopus,2016-04-01,sciencedirect,Toward rapid learning in cancer treatment selection: An analytical engine for practice-based clinical data,https://api.elsevier.com/content/abstract/scopus_id/84962787234,"Objective
                  Wide-scale adoption of electronic medical records (EMRs) has created an unprecedented opportunity for the implementation of Rapid Learning Systems (RLSs) that leverage primary clinical data for real-time decision support. In cancer, where large variations among patient features leave gaps in traditional forms of medical evidence, the potential impact of a RLS is particularly promising. We developed the Melanoma Rapid Learning Utility (MRLU), a component of the RLS, providing an analytical engine and user interface that enables physicians to gain clinical insights by rapidly identifying and analyzing cohorts of patients similar to their own.
               
                  Materials and methods
                  A new approach for clinical decision support in Melanoma was developed and implemented, in which patient-centered cohorts are generated from practice-based evidence and used to power on-the-fly stratified survival analyses. A database to underlie the system was generated from clinical, pharmaceutical, and molecular data from 237 patients with metastatic melanoma from two academic medical centers. The system was assessed in two ways: (1) ability to rediscover known knowledge and (2) potential clinical utility and usability through a user study of 13 practicing oncologists.
               
                  Results
                  The MRLU enables physician-driven cohort selection and stratified survival analysis. The system successfully identified several known clinical trends in melanoma, including frequency of BRAF mutations, survival rate of patients with BRAF mutant tumors in response to BRAF inhibitor therapy, and sex-based trends in prevalence and survival. Surveyed physician users expressed great interest in using such on-the-fly evidence systems in practice (mean response from relevant survey questions 4.54/5.0), and generally found the MRLU in particular to be both useful (mean score 4.2/5.0) and useable (4.42/5.0).
               
                  Discussion
                  The MRLU is an RLS analytical engine and user interface for Melanoma treatment planning that presents design principles useful in building RLSs. Further research is necessary to evaluate when and how to best use this functionality within the EMR clinical workflow for guiding clinical decision making.
               
                  Conclusion
                  The MRLU is an important component in building a RLS for data driven precision medicine in Melanoma treatment that could be generalized to other clinical disorders.",science
10.1016/j.physb.2015.12.006,Journal,Physica B: Condensed Matter,scopus,2016-04-01,sciencedirect,A Neural-FEM tool for the 2-D magnetic hysteresis modeling,https://api.elsevier.com/content/abstract/scopus_id/84959509589,"The aim of this work is to present a new tool for the analysis of magnetic field problems considering 2-D magnetic hysteresis. In particular, this tool makes use of the Finite Element Method to solve the magnetic field problem in real device, and fruitfully exploits a neural network (NN) for the modeling of 2-D magnetic hysteresis of materials. The NS has as input the magnetic inductions components B at the k-th simulation step and returns as output the corresponding values of the magnetic field H corresponding to the input pattern. It is trained by vector measurements performed on the magnetic material to be modeled. This input/output scheme is directly implemented in a FEM code employing the magnetic potential vector A formulation. Validations through measurements on a real device have been performed.",science
10.1016/j.talanta.2015.11.048,Journal,Talanta,scopus,2016-03-01,sciencedirect,Development of an semi-automatic and sensitive photochemically induced fluorescence sensor for the determination of thiamethoxam in vegetables,https://api.elsevier.com/content/abstract/scopus_id/84962860340,"The determination of thiamethoxam (TMX), a widely known neonicotinoid pesticide, by a multicommutated optosensing device implemented with photochemically induced fluorescence (PIF) has been developed. The combination of both methodologies allows, on one hand a quick on-line photodegradation of TMX and, on the other hand, the preconcentration, quantification and desorption of the fluorescent photoproduct generated once retained on C18 silica gel filling the flow-cell which was monitored at 353 and 407nm for excitation and emission wavelengths, respectively.
                  The proposed analytical method presents a detection limit of 3.6ngmL−1 by using Multicommutated Flow Injection Analysis (MCFIA) as flow methodology. Recovery experiments have been carried out in different kinds of vegetables at levels same or below the legislated maximum residue limit, demonstrating that this method combines advantages such as simplicity, high sensibility and high selectivity, in addition to fulfill the requirements for its applications in quality control. The obtained results in the analysis of real samples were in good agreement with those provided by a reference liquid chromatography (HPLC) method.",science
10.1016/j.tics.2015.11.007,Journal,Trends in Cognitive Sciences,scopus,2016-03-01,sciencedirect,Conceptual Alignment: How Brains Achieve Mutual Understanding,https://api.elsevier.com/content/abstract/scopus_id/84958161462,"We share our thoughts with other minds, but we do not understand how. Having a common language certainly helps, but infants’ and tourists’ communicative success clearly illustrates that sharing thoughts does not require signals with a pre-assigned meaning. In fact, human communicators jointly build a fleeting conceptual space in which signals are a means to seek and provide evidence for mutual understanding. Recent work has started to capture the neural mechanisms supporting those fleeting conceptual alignments. The evidence suggests that communicators and addressees achieve mutual understanding by using the same computational procedures, implemented in the same neuronal substrate, and operating over temporal scales independent from the signals’ occurrences.",science
10.1016/j.seppur.2015.12.056,Journal,Separation and Purification Technology,scopus,2016-02-29,sciencedirect,Rapid cultivation of aerobic granule for the treatment of solvent recovery raffinate in a bench scale sequencing batch reactor,https://api.elsevier.com/content/abstract/scopus_id/84954169665,"Aerobic granular sludge (AGS) was cultivated in a bench scale sequencing batch reactor within 21days. Strategy of the rapid startup was inoculated with part of mature AGS during cultivation, while aerobic biological selector was implemented for the inhibiting outgrowth of filamentous bacteria and fast selection of zoogloea bacteria. Then, the cultivated AGS was employed for the treatment of solvent recovery raffinate. Stable AGS was successfully domesticated after 55days under strategy of gradually increase the proportion of real wastewater in influent. The domesticated AGS was orange, irregular shape, smooth and compact. SVI, SV30/SV5, MLVSS/MLSS, EPS, PN/PS, average particle size, granulation rate, (SOUR)H and (SOUR)N of AGS were 19.06mL/g, 0.97, 0.55, 30.05mg/g MLVSS, 1.10, 1.28mm, 98.87%, 32.47 and 7.97mg O2/hgVSS respectively. Finally, COD, TIN, NH4
                     +–N and TP of the effluent were lower than 25.9mg/L, 1.64mg/L, 1.13mg/L and 0.21mg/L, and their removal rate was more than 98.43%, 97.12%, 98.02% and 98.09% respectively. Thus, COD, TP removal, nitrification and denitrification were realized in a single bioreactor. The result indicated that the feasibility of AGS for high C/N ratio industrial wastewater treatment.",science
10.1016/j.tplants.2015.10.015,Journal,Trends in Plant Science,scopus,2016-02-01,sciencedirect,Machine Learning for High-Throughput Stress Phenotyping in Plants,https://api.elsevier.com/content/abstract/scopus_id/84958049448,"Advances in automated and high-throughput imaging technologies have resulted in a deluge of high-resolution images and sensor data of plants. However, extracting patterns and features from this large corpus of data requires the use of machine learning (ML) tools to enable data assimilation and feature identification for stress phenotyping. Four stages of the decision cycle in plant stress phenotyping and plant breeding activities where different ML approaches can be deployed are (i) identification, (ii) classification, (iii) quantification, and (iv) prediction (ICQP). We provide here a comprehensive overview and user-friendly taxonomy of ML tools to enable the plant community to correctly and easily apply the appropriate ML tools and best-practice guidelines for various biotic and abiotic stress traits.",science
10.1016/j.juro.2015.09.090,Journal,Journal of Urology,scopus,2016-02-01,sciencedirect,Use of Artificial Intelligence and Machine Learning Algorithms with Gene Expression Profiling to Predict Recurrent Nonmuscle Invasive Urothelial Carcinoma of the Bladder,https://api.elsevier.com/content/abstract/scopus_id/84953746356,"Purpose
                  Due to the high recurrence risk of nonmuscle invasive urothelial carcinoma it is crucial to distinguish patients at high risk from those with indolent disease. In this study we used a machine learning algorithm to identify the genes in patients with nonmuscle invasive urothelial carcinoma at initial presentation that were most predictive of recurrence. We used the genes in a molecular signature to predict recurrence risk within 5 years after transurethral resection of bladder tumor.
               
                  Materials and Methods
                  Whole genome profiling was performed on 112 frozen nonmuscle invasive urothelial carcinoma specimens obtained at first presentation on Human WG-6 BeadChips (Illumina®). A genetic programming algorithm was applied to evolve classifier mathematical models for outcome prediction. Cross-validation based resampling and gene use frequencies were used to identify the most prognostic genes, which were combined into rules used in a voting algorithm to predict the sample target class. Key genes were validated by quantitative polymerase chain reaction.
               
                  Results
                  The classifier set included 21 genes that predicted recurrence. Quantitative polymerase chain reaction was done for these genes in a subset of 100 patients. A 5-gene combined rule incorporating a voting algorithm yielded 77% sensitivity and 85% specificity to predict recurrence in the training set, and 69% and 62%, respectively, in the test set. A singular 3-gene rule was constructed that predicted recurrence with 80% sensitivity and 90% specificity in the training set, and 71% and 67%, respectively, in the test set.
               
                  Conclusions
                  Using primary nonmuscle invasive urothelial carcinoma from initial occurrences genetic programming identified transcripts in reproducible fashion, which were predictive of recurrence. These findings could potentially impact nonmuscle invasive urothelial carcinoma management.",science
10.1016/j.asoc.2015.11.011,Journal,Applied Soft Computing Journal,scopus,2016-02-01,sciencedirect,Optimal design of a 3D-printed scaffold using intelligent evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84947908568,"Fabrication of three-dimensional structures has gained increasing importance in the bone tissue engineering (BTE) field. Mechanical properties and permeability are two important requirement for BTE scaffolds. The mechanical properties of the scaffolds are highly dependent on the processing parameters. Layer thickness, delay time between spreading each powder layer, and printing orientation are the major factors that determine the porosity and compression strength of the 3D printed scaffold.
                  In this study, the aggregated artificial neural network (AANN) was used to investigate the simultaneous effects of layer thickness, delay time between spreading each layer, and print orientation of porous structures on the compressive strength and porosity of scaffolds. Two optimization methods were applied to obtain the optimal 3D parameter settings for printing tiny porous structures as a real BTE problem. First, particle swarm optimization algorithm was implemented to obtain the optimum topology of the AANN. Then, Pareto front optimization was used to determine the optimal setting parameters for the fabrication of the scaffolds with required compressive strength and porosity. The results indicate the acceptable potential of the evolutionary strategies for the controlling and optimization of the 3DP process as a complicated engineering problem.",science
10.1016/j.procs.2016.07.417,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,The Virtual Reality of the Mind,https://api.elsevier.com/content/abstract/scopus_id/85006412942,"In evolutionary terms, imagery developed hundreds of millions of years before symbolic or language-like systems of cognition. Even the most abstract reasoning in science and mathematics requires imagery: diagrams and written symbols supplement short-term memory, and richer imagery is essential for novel analogies and creative insights. A cognitive architecture must relate symbols to the perceptions and purposive actions of an embodied mind that interacts with the world and with other minds in it. This article reviews the evidence for an internal virtual reality as the foundation for the perception, action, and cognition of an embodied mind. Peirce's theory of signs is a unifying framework that relates all branches of cognitive science, including AI implementations. The result is a theory of virtual reality for cognitive architectures (VRCA) that spans the minds from fish to humans and perhaps beyond.",science
10.1016/j.ifacol.2016.07.192,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,Proactive Teaching of Mechatronics in Master Courses – Project Case Study,https://api.elsevier.com/content/abstract/scopus_id/84994890825,"University education of young people in very complex branches like robotics is nowadays an issue. Methods like e-learning, distance education etc. are implemented. The authors of the article consider that practice courses play an irreplaceable role in mechatronics education. The paper deals with description of internship concept of French Military Academy of Saint-Cyr for master degree students. A real system of robot arm manipulator playing a Tic Tac Toe game with a human is presented as a case study of a typical issue solved by the students.",science
10.1016/j.ifacol.2016.07.172,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,HOME I/O: a virtual house for control and STEM education from middle schools to Universities,https://api.elsevier.com/content/abstract/scopus_id/84994876201,"This paper deals with the result of a R&D project called ""DOMUS"" partially founded by the French Ministry of National Education in order to design a virtual house adapted to control and STEM (Science, Technology, Engineering and Mathematics) education, and usable from middle schools to Universities. The result is software called HOME I/O. It is much more a serious game than a simulation tool which has been designed and developed in order to be adapted to the Y generation. This 3-year R&D project (2011-2014) is a fruitful collaboration between CReSTIC lab from the University of Reims Champagne-Ardenne and Real Games, a Portuguese company. The main idea has been to bring a virtual house into the class room, adapted to learners and teachers and suitable for control and STEM interdisciplinary education. With this innovative pedagogical tool, the possibilities of pedagogical scenarios in control and STEM education depend only on teachers’ imagination. In this paper, the main ideas which are involved the design of HOME I/O are presented as well as the main features of the software. An experimental stage with 50 teachers from middle and high schools has shown that this kind of tool changes the way to teach and requires new training for teachers.",science
10.1016/B978-0-444-63428-3.50407-0,Book Series,Computer Aided Chemical Engineering,scopus,2016-01-01,sciencedirect,Process Integration: Pinch Analysis and Mathematical Programming - Directions for Future Development,https://api.elsevier.com/content/abstract/scopus_id/84994259521,"Numerous studies have been performed process systems engineering field for improving the efficiency of supplying and using energy, water and other resources and consequently for reducing the emissions of greenhouse gases, volatile organic compounds and other pollutants, accumulating a significant body of methods, applications and results. It has become apparent that the resource inputs and effluents of industrial processes and the other units including the business centres, civic objects and even agricultural plants can and are often connected with each other. Most industrial plants and the other units throughout the world still use more energy and water than necessary, they are proven cases in the range 20 – 30 %, emitting too large volumes of Greenhouse Gases and other pollutants.
                  Water-saving measures and the reuse of water may reduce groundwater consumption by as much as 25 – 30 %. Usually reducing resource consumption is achieved by increasing internal recycling and the reuse of energy and material streams. Projects for improving process resource efficiencies can be very beneficial and also potentially improve the public perception of the companies.
                  Motivating, launching and carrying out such projects, however, involve appropriate optimisation, based on adequate process models, applied within the framework of appropriate resource minimisation strategies and procedures. Process Integration supporting process design, integration and optimisation has been around for nearly 45 years. It has been closely related to the development of process systems engineering, as well as utilising mathematical modelling and information technology.
                  In the broader sense Process Integration methods can be classified into those relying on process based insight and targeting on the one hand, mainly employing targeting, heuristics and artificial intelligence—AI. On the other hand are the methods employing detailed mathematical models usually implemented as algebraic models with embedded superstructures in the case of process network synthesis. The methods relying on thermodynamic insights have been first published in the early 1980-s (Linnhoff and Flower, 1978) as well as those using mathematical programming—MP (Papoulias and Grossmann, 1983). There can also be a combined approach (Klemeš and Kravanja, 2013).
                  On the one hand, the concept relying on thermodynamic and/or physical insights using the well-known Pinch Analysis has been the more widely accepted in both academia and industry. Process Integration has thus converged towards two schools of thought, the thermodynamic based (Pinch) and the mathematically based MP, each having its own advantages and drawbacks. The thermodynamic school has mostly preceded that of the MP in generating ideas based on engineering creativity. The MP school has enacted its ideas and described them as explicit mathematical models for solving advanced PI problems.
                  The collaboration between both approaches has been widening, taking from each other the more applicable parts. Its development has been accelerating as the combined methodology has been able to provide answers and support for important issues regarding economic development—energy, water and resources better utilisation and savings. This contribution is targeted towards a short overview of recent achievements and future challenges.",science
10.1016/j.proeng.2016.08.064,Conference Proceeding,Procedia Engineering,scopus,2016-01-01,sciencedirect,Primer for image informatics in personalized medicine,https://api.elsevier.com/content/abstract/scopus_id/84993990077,"Image informatics encompasses the concept of extracting and quantifying information contained in image data. Scenes, what an image contains, come from many imager devices such as consumer electronics, medical imaging systems, 3D laser scanners, microscopes, or satellites. There is a marked increase in image informatics applications as there have been simultaneous advances in imaging platforms, data availability due to social media, and big data analytics. An area ready to take advantage of these developments is personalized medicine, the concept where the goal is tailor healthcare to the individual. Patient health data is computationally profiled against a large of pool of feature-rich data from other patients to ideally optimize how a physician chooses care. One of the daunting challenges is how to effectively utilize medical image data in personalized medicine. Reliable data analytics products require as much automation as possible, which is a difficulty for data like histopathology and radiology images because we require highly trained expert physicians to interpret the information. This review targets biomedical scientists interested in getting started on tackling image analytics. We present high level discussions of sample preparation and image acquisition; data formats; storage and databases; image processing; computer vision and machine learning; and visualization and interactive programming. Examples will be covered using existing open-source software tools such as ImageJ, CellProfiler, and IPython Notebook. We discuss how difficult real-world challenges faced by image informatics and personalized medicine are being tackled with open-source biomedical data and software.",science
10.1016/j.procs.2016.08.136,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,Formal Description and Automatic Generation of Learning Spaces Based on Ontologies,https://api.elsevier.com/content/abstract/scopus_id/84988908359,"A good virtual Learning Space (LS) should convey pertinent learning information to the visitors at the most adequate time and locations to favor their knowledge acquisition.
                  Considering the consolidation of the internet and the improvement of the interaction, searching, and learning mechanisms, we propose a generic architecture, called CaVa, to create virtual Learning Spaces building up on cultural institution documents. More precisely, our proposal is to automatically create ontology-based virtual learning environments.
                  Thus, to impart relevant learning materials to the virtual LS, we propose the use of ontologies to represent the key concepts and semantic relations in an user- and machine-understandable format. These concepts together with the data (extracted from the real documents) stored in a digital storage format (XML datasets, relational databases, etc.) are displayed in an ontology-based learning space that enables the visitors to use the available features and tools to learn about a specific domain.
                  According to the approach here discussed, each desired virtual LS must be specified rigorously through a domain specific language (DSL) that was designed and implemented.
                  To validate the proposed architecture, three case studies will be used as instances of CaVa architecture.",science
10.1016/j.procs.2016.05.474,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,WOWMON: A machine learning-based profiler for self-adaptive instrumentation of scientific workflows,https://api.elsevier.com/content/abstract/scopus_id/84978472514,"Performance debugging using program profiling and tracing for scientific workflows can be extremely difficult for two reasons. 1) Existing performance tools lack the ability to automatically produce global performance data based on local information from coupled scientific applications of workflows, particularly at runtime. 2) Profiling/tracing with static instrumentation may incur high overhead and significantly slow down science-critical tasks. To gain more insights on workflows we introduce a lightweight workflow monitoring infrastructure, WOW-MON (WOrkfloW MONitor), which enables user's access not only to cross-application performance data such as end-to-end latency and execution time of individual workflow components at runtime, but also to customized performance events. To reduce profiling overhead, WOW-MON uses adaptive selection of performance metrics based on machine learning algorithms to guide profilers collecting only metrics that have most impact on performance of workflows. Through the study of real scientific workflows (e.g., LAMMPS) with the help of WOWMON, we found that the performance of the workflows can be significantly affected by both software and hardware factors, such as the policy of process mapping and in-situ buffer size. Moreover, we experimentally show that WOWMON can reduce data movement for profiling by up to 54% without missing the key metrics for performance debugging.",science
10.1016/j.jclepro.2015.07.075,Journal,Journal of Cleaner Production,scopus,2016-01-01,sciencedirect,Virtual laboratory on biomass for energy generation,https://api.elsevier.com/content/abstract/scopus_id/84959516733,"Virtual laboratories (VL) and interactive simulations are excellent approaches for training students to understand technical principles. This can be useful in many fields of science and engineering teaching. For this purpose, we have developed a virtual environment that can simulate real-laboratory operations, effectively enhancing the teaching process with an intuitive and appealing interface. Such software shows the virtual-studio practice characterization of basic biofuel-properties, simulating reality step-by-step. This virtual laboratory has been used by students of the postgraduate master subject on Biomass for Power Generation, belonging to the Master of Distributed Renewable Energy. The aim of this tool is to help students to study, learn and investigate on their own. The virtual lab is made of a web-based application to complement experimental laboratory training, allowing students to prepare their experimental practices before going to the lab, and to review them at any time afterwards. The computer application exhibits key virtual-lab educational features, like an integrative layout and self-evaluating tests. It allows a personalized and active learning-process, adaptability to teacher's aims and versatility and simplicity, using different multimedia resources. A satisfaction questionnaire was carried out between master degree students to evaluate the usefulness of the VL. Such a tool was positively evaluated, achieving a mean score of 6 out of 7 points. Additionally, the VL efficiency in the learning process showed a final examination main score of 8 out of 10 points. Last but not least, most students considered that the VL promoted learning and personal effort, being an excellent preparatory tool to real experiments.",science
10.1016/j.ultsonch.2015.07.022,Journal,Ultrasonics Sonochemistry,scopus,2016-01-01,sciencedirect,"Impact of ultrasound on solid-liquid extraction of phenolic compounds from maritime pine sawdust waste. Kinetics, optimization and large scale experiments",https://api.elsevier.com/content/abstract/scopus_id/84938390300,"Maritime pine sawdust, a by-product from industry of wood transformation, has been investigated as a potential source of polyphenols which were extracted by ultrasound-assisted maceration (UAM). UAM was optimized for enhancing extraction efficiency of polyphenols and reducing time-consuming. In a first time, a preliminary study was carried out to optimize the solid/liquid ratio (6g of dry material per mL) and the particle size (0.26cm2) by conventional maceration (CVM). Under these conditions, the optimum conditions for polyphenols extraction by UAM, obtained by response surface methodology, were 0.67W/cm2 for the ultrasonic intensity (UI), 40°C for the processing temperature (T) and 43min for the sonication time (t). UAM was compared with CVM, the results showed that the quantity of polyphenols was improved by 40% (342.4 and 233.5mg of catechin equivalent per 100g of dry basis, respectively for UAM and CVM). A multistage cross-current extraction procedure allowed evaluating the real impact of UAM on the solid–liquid extraction enhancement. The potential industrialization of this procedure was implemented through a transition from a lab sonicated reactor (3L) to a large scale one with 30L volume.",science
10.1016/j.jneumeth.2015.08.031,Journal,Journal of Neuroscience Methods,scopus,2015-12-30,sciencedirect,Online visualization of brain connectivity,https://api.elsevier.com/content/abstract/scopus_id/84941264480,"Background
                  While visualization of brain activity has well established practical applications such as real-time functional mapping or neurofeedback, visual representation of brain connectivity is not widely used. In addition, technically challenging single-trial connectivity estimation may have hindered practical usage of connectivity in online applications.
               
                  New method
                  In this work, we developed algorithms that are capable of estimating and visualizing (effective) connectivity between independent cortical sources during online EEG recordings.
               
                  Results
                  The core routines of our procedure, such as CSPVARICA source extraction and regularized connectivity estimation, are available in our open source Python-based toolbox SCoT. We demonstrate for the first time that online connectivity visualization is feasible. We show this in a feasibility study with twelve participants performing two different tasks, namely motor execution and resting with eyes open or closed. Connectivity patterns were significantly different between two motor tasks in four participants, whereas significant differences between resting task patterns were found in seven participants.
               
                  Comparison with existing methods
                  Existing connectivity studies have focused on offline methods. In contrast, there are only a small number of examples in the literature that explored online connectivity estimation. For example, a system based on wearable EEG has been demonstrated to work for one subject, and the Glass Brain project has received considerable attention in popular sciences last year. However, none of these attempts validate their methods on multiple subjects.
               
                  Conclusions
                  Our results show that causal connectivity patterns can be observed online during EEG measurements, which is a first step towards real-time connectivity analysis.",science
10.1016/j.chroma.2015.11.031,Journal,Journal of Chromatography A,scopus,2015-12-01,sciencedirect,Determination of ultratrace levels of tributyltin in waters by isotope dilution and gas chromatography coupled to tandem mass spectrometry,https://api.elsevier.com/content/abstract/scopus_id/84974628214,"The current EU legislation lays down the Environmental Quality Standards (EQS) of 45 priority substances in surface water bodies. In particular, the concentration of tributyltin (TBT) must not exceed 0.2ngL−1 and analytical methodologies with a Limit of Quantification (LOQ) equal or below 0.06ngL−1 are urged to be developed. This work presents a procedure for the determination of ultratrace levels of TBT in water samples by Isotope Dilution and GC–MS/MS operating in Selected Reaction Monitoring (SRM) mode which meets current EU requirements. The method requires the monitorization of five consecutive transitions (287>175 to 291>179) for the sensitive and selective detection of TBT. The measured isotopic distribution of TBT fragment ions was in agreement with the theoretical values computed by a polynomial expansion algorithm. The combined use of Tandem Mass Spectrometry, a sample volume of 250mL, the preconcentration of 1mL of organic phase to 30μL and an injection volume of 25μL by Programmed Temperature Vaporization provided a LOQ of 0.0426ngL−1 for TBT (calculated as ten times the standard deviation of nine independent blanks). The recovery for TBT calculated in Milli-Q water at the EQS level was 106.3±4%. A similar procedure was also developed for the quantification of dibutyltin (DBT) and monobutyltin (MBT) in water samples showing satisfactory results. The method was finally implemented in a routine testing laboratory to demonstrate its applicability to real samples obtaining quantitative recoveries for TBT at the EQS level in mineral water, river water and seawater.",science
10.1016/j.patcog.2015.06.006,Journal,Pattern Recognition,scopus,2015-12-01,sciencedirect,Understanding social relationships in egocentric vision,https://api.elsevier.com/content/abstract/scopus_id/84941419626,"The understanding of mutual people interaction is a key component for recognizing people social behavior, but it strongly relies on a personal point of view resulting difficult to be a-priori modeled. We propose the adoption of the unique head mounted cameras first person perspective (ego-vision) to promptly detect people interaction in different social contexts. The proposal relies on a complete and reliable system that extracts people׳s head pose combining landmarks and shape descriptors in a temporal smoothed HMM framework. Finally, interactions are detected through supervised clustering on mutual head orientation and people distances exploiting a structural learning framework that specifically adjusts the clustering measure according to a peculiar scenario. Our solution provides the flexibility to capture the interactions disregarding the number of individuals involved and their level of acquaintance in context with a variable degree of social involvement. The proposed system shows competitive performances on both publicly available ego-vision datasets and ad hoc benchmarks built with real life situations.",science
10.1016/j.artmed.2015.08.003,Journal,Artificial Intelligence in Medicine,scopus,2015-11-01,sciencedirect,A fuzzy-ontology-oriented case-based reasoning framework for semantic diabetes diagnosis,https://api.elsevier.com/content/abstract/scopus_id/84983487369,"Objective
                  Case-based reasoning (CBR) is a problem-solving paradigm that uses past knowledge to interpret or solve new problems. It is suitable for experience-based and theory-less problems. Building a semantically intelligent CBR that mimic the expert thinking can solve many problems especially medical ones.
               
                  Methods
                  Knowledge-intensive CBR using formal ontologies is an evolvement of this paradigm. Ontologies can be used for case representation and storage, and it can be used as a background knowledge. Using standard medical ontologies, such as SNOMED CT, enhances the interoperability and integration with the health care systems. Moreover, utilizing vague or imprecise knowledge further improves the CBR semantic effectiveness. This paper proposes a fuzzy ontology-based CBR framework. It proposes a fuzzy case-base OWL2 ontology, and a fuzzy semantic retrieval algorithm that handles many feature types.
               
                  Material
                  This framework is implemented and tested on the diabetes diagnosis problem. The fuzzy ontology is populated with 60 real diabetic cases. The effectiveness of the proposed approach is illustrated with a set of experiments and case studies.
               
                  Results
                  The resulting system can answer complex medical queries related to semantic understanding of medical concepts and handling of vague terms. The resulting fuzzy case-base ontology has 63 concepts, 54 (fuzzy) object properties, 138 (fuzzy) datatype properties, 105 fuzzy datatypes, and 2640 instances. The system achieves an accuracy of 97.67%. We compare our framework with existing CBR systems and a set of five machine-learning classifiers; our system outperforms all of these systems.
               
                  Conclusion
                  Building an integrated CBR system can improve its performance. Representing CBR knowledge using the fuzzy ontology and building a case retrieval algorithm that treats different features differently improves the accuracy of the resulting systems.",science
10.1016/j.compbiomed.2015.07.015,Journal,Computers in Biology and Medicine,scopus,2015-11-01,sciencedirect,"Implementation of a web based universal exchange and inference language for medicine: Sparse data, probabilities and inference in data mining of clinical data repositories",https://api.elsevier.com/content/abstract/scopus_id/84941884468,"We extend Q-UEL, our universal exchange language for interoperability and inference in healthcare and biomedicine, to the more traditional fields of public health surveys. These are the type associated with screening, epidemiological and cross-sectional studies, and cohort studies in some cases similar to clinical trials. There is the challenge that there is some degree of split between frequentist notions of probability as (a) classical measures based only on the idea of counting and proportion and on classical biostatistics as used in the above conservative disciplines, and (b) more subjectivist notions of uncertainty, belief, reliability, or confidence often used in automated inference and decision support systems. Samples in the above kind of public health survey are typically small compared with our earlier “Big Data” mining efforts. An issue addressed here is how much impact on decisions should sparse data have. We describe a new Q-UEL compatible toolkit including a data analytics application DiracMiner that also delivers more standard biostatistical results, DiracBuilder that uses its output to build Hyperbolic Dirac Nets (HDN) for decision support, and HDNcoherer that ensures that probabilities are mutually consistent. Use is exemplified by participating in a real word health-screening project, and also by deployment in a industrial platform called the BioIngine, a cognitive computing platform for health management.",science
10.1016/j.ins.2015.05.014,Journal,Information Sciences,scopus,2015-10-20,sciencedirect,Discovering missing me edges across social networks,https://api.elsevier.com/content/abstract/scopus_id/84930844095,"Distinct social networks are interconnected via membership overlap, which plays a key role when crossing information is investigated in the context of multiple-social-network analysis. Unfortunately, users do not always make their membership to two distinct social networks explicit, by specifying the so-called me edge (practically, corresponding to a link between the two accounts), thus missing a potentially very useful information. As a consequence, discovering missing me edges is an important problem to address in this context with potential powerful applications. In this paper, we propose a common-neighbor approach to detecting missing me edges, which returns good results in real-life settings. Indeed, an experimental campaign shows both that the state-of-the-art common-neighbor approaches cannot be effectively applied to our problem and, conversely, that our approach returns precise and complete results.",science
10.1016/j.ijpe.2015.06.028,Journal,International Journal of Production Economics,scopus,2015-10-01,sciencedirect,"Real options, learning cost and timing software upgrades: Towards an integrative model for enterprise software upgrade decision analysis",https://api.elsevier.com/content/abstract/scopus_id/84940035379,"A key challenge facing information technology (IT) managers is how to carefully analyze the decision options available to them when considering enterprise software infrastructure upgrades. We present an illustrative case that not only captures the trade-offs involved in retaining an existing software infrastructure as opposed to adopting a new one at an appropriate time, but also demonstrates how the combined application of various analytical tools including real options analysis may provide richer information than a single approach. Our model offers IT managers the potential to arrive at a deeper understanding of software upgrade timing decisions while generating important information relevant to practical decision situations.",science
10.1016/j.solener.2015.07.020,Journal,Solar Energy,scopus,2015-10-01,sciencedirect,A model tree approach to forecasting solar irradiance variability,https://api.elsevier.com/content/abstract/scopus_id/84939607270,"As the penetration of solar power increases, the variable generation from this renewable resource will necessitate solar irradiance forecasts for utility companies to balance the energy grid. In this study, the temporal irradiance variability is calculated by the temporal standard deviation of the Global Horizontal Irradiance (GHI) at eight sites in the Sacramento Valley and the spatial irradiance variability is quantified by the standard deviation across those same sites. Our proposed artificial intelligence forecasting technique is a model tree with a nearest neighbor option to predict the irradiance variability directly. The model tree technique reduces the mean absolute error of the variability prediction between 10% and 55% compared to using climatological average values of the temporal and spatial GHI standard deviation. These forecasts are made at 15-min intervals out to 180-min. A data denial experiment showed that the addition of surface weather observations improved the forecasting skill of the model tree by approximately 10%. These results indicate that the model tree technique can be implemented in real-time to produce solar variability forecasts to aid utility companies in energy grid management.",science
10.1016/j.mimet.2015.07.015,Journal,Journal of Microbiological Methods,scopus,2015-10-01,sciencedirect,"MCaVoH: A toolkit for mining, classification and visualization of human microbiota",https://api.elsevier.com/content/abstract/scopus_id/84937710825,"Human body is the home for a large number of microbes. The complexity of enterotype depends on the body site. Microbial communities in various samples from different regions are being classified on the basis of 16S rRNA gene sequences. With the improvement in sequencing technologies various computational methods have been used for the analysis of microbiome data. Despite several available machine learning techniques there is no single platform available which could provide several techniques for clustering, multiclass classification, comparative analysis and the most significantly the identification of the subgroups present within larger groups of human microbial communities.
                  We present a tool named MCaVoH for this purpose which performs clustering and classification of 16S rRNA sequence data and highlight various groups. Our tool has an added facility of biclustering which produces local group of communities present within larger groups (clusters). The core objective of our work was to identify the interaction between various bacterial species along with monitoring the composition and variations in microbial communities. MCaVoH also evaluates the performance and efficiency of different techniques using comparative analysis. The results are visualized through different plots and graphs. We implemented our tool in MATLAB. We tested our tool on several real and simulated 16S rRNA data sets and it outperforms several existing methods. Our tool provides a single platform for using multiple clustering, classification algorithms, local community identification along with their comparison which has not been done so far. Tool is available at https://sourceforge.net/projects/mcavoh/.",science
10.1016/j.ins.2015.04.049,Journal,Information Sciences,scopus,2015-10-01,sciencedirect,Density-based modularity for evaluating community structure in bipartite networks,https://api.elsevier.com/content/abstract/scopus_id/84930146641,"A bipartite network is an important type of complex network in human social activities. Newman defined modularity as a measurement for evaluating community structure in unipartite networks. Due to the success of modularity in unipartite networks, bipartite modularities were developed according to different understandings of community in bipartite networks. However, these modularity measurements are subject to resolution limits that could reduce the quality of community partitioning. These modularity measurements contain an intrinsic scale that depends on the total size of links and ignores the number of nodes in a bipartite network. In this paper, we first illustrate such resolution limits of traditional bipartite modularities using several examples of bipartite networks. Next, we propose a quantitative measurement called density-based modularity to evaluate community partitioning in bipartite networks. We verify that optimization of the density-based modularity proposed has no resolution limit. By optimizing this density-based modularity, we can partition the network into the appropriate communities. Experiments on synthetic and real-world bipartite networks verify the accuracy and reliability of our bipartite density-based modularity.",science
10.1016/j.cose.2015.04.002,Journal,Computers and Security,scopus,2015-09-01,sciencedirect,BankSealer: A decision support system for online banking fraud analysis and investigation,https://api.elsevier.com/content/abstract/scopus_id/84940722662,"The significant growth of online banking frauds, fueled by the underground economy of malware, raised the need for effective fraud analysis systems. Unfortunately, almost all of the existing approaches adopt black box models and mechanisms that do not give any justifications to analysts. Also, the development of such methods is stifled by limited Internet banking data availability for the scientific community. In this paper we describe BankSealer, a decision support system for online banking fraud analysis and investigation. During a training phase, BankSealer builds easy-to-understand models for each customer's spending habits, based on past transactions. First, it quantifies the anomaly of each transaction with respect to the customer historical profile. Second, it finds global clusters of customers with similar spending habits. Third, it uses a temporal threshold system that measures the anomaly of the current spending pattern of each customer, with respect to his or her past spending behavior. With this threefold profiling approach, it mitigates the under-training due to the lack of historical data for building well-trained profiles, and the evolution of users' spending habits over time. At runtime, BankSealer supports analysts by ranking new transactions that deviate from the learned profiles, with an output that has an easily understandable, immediate statistical meaning.
                  Our evaluation on real data, based on fraud scenarios built in collaboration with domain experts that replicate typical, real-world attacks (e.g., credential stealing, banking trojan activity, and frauds repeated over time), shows that our approach correctly ranks complex frauds. In particular, we measure the effectiveness, the computational resource requirements and the capabilities of BankSealer to mitigate the problem of users that performed a low number of transactions. Our system ranks frauds and anomalies with up to 98% detection rate and with a maximum daily computation time of 4 min. Given the good results, a leading Italian bank deployed a version of BankSealer in their environment to analyze frauds.",science
10.1016/j.jenvman.2015.06.003,Journal,Journal of Environmental Management,scopus,2015-09-01,sciencedirect,Developing the remote sensing-based early warning system for monitoring TSS concentrations in Lake Mead,https://api.elsevier.com/content/abstract/scopus_id/84934926165,"Adjustment of the water treatment process to changes in water quality is a focus area for engineers and managers of water treatment plants. The desired and preferred capability depends on timely and quantitative knowledge of water quality monitoring in terms of total suspended solids (TSS) concentrations. This paper presents the development of a suite of nowcasting and forecasting methods by using high-resolution remote-sensing-based monitoring techniques on a daily basis. First, the integrated data fusion and mining (IDFM) technique was applied to develop a near real-time monitoring system for daily nowcasting of the TSS concentrations. Then a nonlinear autoregressive neural network with external input (NARXNET) model was selected and applied for forecasting analysis of the changes in TSS concentrations over time on a rolling basis onward using the IDFM technique. The implementation of such an integrated forecasting and nowcasting approach was assessed by a case study at Lake Mead hosting the water intake for Las Vegas, Nevada, in the water-stressed western U.S. Long-term monthly averaged results showed no simultaneous impact from forest fire events on accelerating the rise of TSS concentration. However, the results showed a probable impact of a decade of drought on increasing TSS concentration in the Colorado River Arm and Overton Arm. Results of the forecasting model highlight the reservoir water level as a significant parameter in predicting TSS in Lake Mead. In addition, the R-squared value of 0.98 and the root mean square error of 0.5 between the observed and predicted TSS values demonstrates the reliability and application potential of this remote sensing-based early warning system in terms of TSS projections at a drinking water intake.",science
10.1074/mcp.M114.047050,Journal,Molecular and Cellular Proteomics,scopus,2015-09-01,sciencedirect,"Large-scale interlaboratory study to develop, analytically validate and apply highly multiplexed, quantitative peptide assays to measure cancer-relevant proteins in plasma",https://api.elsevier.com/content/abstract/scopus_id/84929586811,"There is an increasing need in biology and clinical medicine to robustly and reliably measure tens to hundreds of peptides and proteins in clinical and biological samples with high sensitivity, specificity, reproducibility, and repeatability. Previously, we demonstrated that LC-MRM-MS with isotope dilution has suitable performance for quantitative measurements of small numbers of relatively abundant proteins in human plasma and that the resulting assays can be transferred across laboratories while maintaining high reproducibility and quantitative precision. Here, we significantly extend that earlier work, demonstrating that 11 laboratories using 14 LC-MS systems can develop, determine analytical figures of merit, and apply highly multiplexed MRM-MS assays targeting 125 peptides derived from 27 cancer-relevant proteins and seven control proteins to precisely and reproducibly measure the analytes in human plasma. To ensure consistent generation of high quality data, we incorporated a system suitability protocol (SSP) into our experimental design. The SSP enabled real-time monitoring of LC-MRM-MS performance during assay development and implementation, facilitating early detection and correction of chromatographic and instrumental problems. Low to subnanogram/ml sensitivity for proteins in plasma was achieved by one-step immunoaffinity depletion of 14 abundant plasma proteins prior to analysis. Median intra- and interlaboratory reproducibility was <20%, sufficient for most biological studies and candidate protein biomarker verification. Digestion recovery of peptides was assessed and quantitative accuracy improved using heavy-isotope-labeled versions of the proteins as internal standards. Using the highly multiplexed assay, participating laboratories were able to precisely and reproducibly determine the levels of a series of analytes in blinded samples used to simulate an interlaboratory clinical study of patient samples. Our study further establishes that LC-MRM-MS using stable isotope dilution, with appropriate attention to analytical validation and appropriate quality control measures, enables sensitive, specific, reproducible, and quantitative measurements of proteins and peptides in complex biological matrices such as plasma.",science
10.1016/j.asoc.2015.03.024,Journal,Applied Soft Computing Journal,scopus,2015-08-22,sciencedirect,A consensus model for Delphi processes with linguistic terms and its application to chronic pain in neonates definition,https://api.elsevier.com/content/abstract/scopus_id/84939650749,"This paper proposes a new model of consensus based on linguistic terms to be implemented in Delphi processes. The model of consensus involves qualitative reasoning techniques and is based on the concept of entropy. The proposed model has the ability to reach consensus automatically without the need for either a moderator or a final interaction among panelists. In addition, it permits panelists to answer with different levels of precision depending on their knowledge on each question. The model defined has been used to establish the relevant features for the definition of a type of chronic disease. A real-case application conducted in the Department of Neonatology of Máxima Medical Center in The Netherlands is presented. This application considers the opinions of stakeholders of neonate health-care in order to reach a final consensual definition of chronic pain in neonates.",science
10.1016/j.ijfoodmicro.2015.03.010,Journal,International Journal of Food Microbiology,scopus,2015-07-02,sciencedirect,A strategy to establish food safety model repositories,https://api.elsevier.com/content/abstract/scopus_id/84926308733,"Transferring the knowledge of predictive microbiology into real world food manufacturing applications is still a major challenge for the whole food safety modelling community. To facilitate this process, a strategy for creating open, community driven and web-based predictive microbial model repositories is proposed. These collaborative model resources could significantly improve the transfer of knowledge from research into commercial and governmental applications and also increase efficiency, transparency and usability of predictive models. To demonstrate the feasibility, predictive models of Salmonella in beef previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models were made publicly available in a Food Safety Model Repository within the OpenML for Predictive Modelling in Food community project. Three different approaches were used to create new models in the model repositories: (1) all information relevant for model re-implementation is available in a scientific publication, (2) model parameters can be imported from tabular parameter collections and (3) models have to be generated from experimental data or primary model parameters. All three approaches were demonstrated in the paper. The sample Food Safety Model Repository is available via: http://sourceforge.net/projects/microbialmodelingexchange/files/models and the PMM-Lab software can be downloaded from http://sourceforge.net/projects/pmmlab/. This work also illustrates that a standardized information exchange format for predictive microbial models, as the key component of this strategy, could be established by adoption of resources from the Systems Biology domain.",science
10.1016/j.ygeno.2015.04.001,Journal,Genomics,scopus,2015-07-01,sciencedirect,Woods: A fast and accurate functional annotator and classifier of genomic and metagenomic sequences,https://api.elsevier.com/content/abstract/scopus_id/84930542687,"Functional annotation of the gigantic metagenomic data is one of the major time-consuming and computationally demanding tasks, which is currently a bottleneck for the efficient analysis. The commonly used homology-based methods to functionally annotate and classify proteins are extremely slow. Therefore, to achieve faster and accurate functional annotation, we have developed an orthology-based functional classifier ‘Woods’ by using a combination of machine learning and similarity-based approaches. Woods displayed a precision of 98.79% on independent genomic dataset, 96.66% on simulated metagenomic dataset and >97% on two real metagenomic datasets. In addition, it performed >87 times faster than BLAST on the two real metagenomic datasets. Woods can be used as a highly efficient and accurate classifier with high-throughput capability which facilitates its usability on large metagenomic datasets.",science
10.1016/j.knosys.2015.02.023,Journal,Knowledge-Based Systems,scopus,2015-07-01,sciencedirect,An enhanced trust propagation approach with expertise and homophily-based trust networks,https://api.elsevier.com/content/abstract/scopus_id/84928081718,"The transitivity property of trust enables the propagation of a trust value through a chain of trusting users in social networks and then provides an expected trust value for another user. Logically, a user in social networks can assess a large number of other users, even if two users have not been directly connected previously. However, a large percentage of trust propagation efforts fail to find reliable trust paths from a source user to a target user because the web of trust in real-world online social networks is too sparse. The success (both quality and quantity) of a trust propagation algorithm strongly relies on the density of a web of trust. The more trust paths that are able to reach the given target user, the more reliable will be the trust estimates based on the trust path with the highest strength. In this paper, we propose an enriched trust propagation approach by combining a homophily-based trust network with an expertise-based trust network, which enhances the density of the trust network. We then evaluate the prediction accuracy and coverage of trust propagation based on various aggregation methods and highlight the most promising method.",science
10.1016/j.cageo.2015.04.001,Journal,Computers and Geosciences,scopus,2015-07-01,sciencedirect,Development of a spatial decision support system for flood risk management in Brazil that combines volunteered geographic information with wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/84927739494,"Effective flood risk management requires updated information to ensure that the correct decisions can be made. This can be provided by Wireless Sensor Networks (WSN) which are a low-cost means of collecting updated information about rivers. Another valuable resource is Volunteered Geographic Information (VGI) which is a comparatively new means of improving the coverage of monitored areas because it is able to supply supplementary information to the WSN and thus support decision-making in flood risk management. However, there still remains the problem of how to combine WSN data with VGI. In this paper, an attempt is made to investigate AGORA-DS, which is a Spatial Decision Support System (SDSS) that is able to make flood risk management more effective by combining these data sources, i.e. WSN with VGI. This approach is built over a conceptual model that complies with the interoperable standards laid down by the Open Geospatial Consortium (OGC) – e.g. Sensor Observation Service (SOS) and Web Feature Service (WFS) – and seeks to combine and present unified information in a web-based decision support tool. This work was deployed in a real scenario of flood risk management in the town of São Carlos in Brazil. The evidence obtained from this deployment confirmed that interoperable standards can support the integration of data from distinct data sources. In addition, they also show that VGI is able to provide information about areas of the river basin which lack data since there is no appropriate station in the area. Hence it provides a valuable support for the WSN data. It can thus be concluded that AGORA-DS is able to combine information provided by WSN and VGI, and provide useful information for supporting flood risk management.",science
10.1016/j.talanta.2015.06.010,Journal,Talanta,scopus,2015-06-15,sciencedirect,A new immobilization procedure for development of an electrochemical immunosensor for parathyroid hormone detection based on gold electrodes modified with 6-mercaptohexanol and silane,https://api.elsevier.com/content/abstract/scopus_id/84931292123,"Fabrication of a new electrochemical impedance-based biosensor for the analysis of parathyroid hormone (PTH), using self-assembled monolayers (SAMs) of mercaptohexanol and (3-Aminopropyl) triethoxysilane on gold electrodes, was investigated for the first time in the field. Anti-PTH was used as a biorecognition element. To monitor immobilization processes in the biosensor fabrication, cyclic voltammetry (CV), electrochemical impedance spectroscopy (EIS), and scanning electron microscopy (SEM) techniques were successfully operated. CV and EIS techniques were also used in quantification of PTH. Energy-dispersive X-ray analysis (EDAX) was also applied to identify surface modifications. Fabrication and working parameters of the biosensor were optimized. Moreover, Kramers–Kronig transformations were performed for validation of obtained EIS data in all steps of biosensor fabrication. The linear PTH detection range of the presented biosensor was 10–50pg/mL PTH. The chrono-impedance technique for real-time monitoring of PTH binding was also implemented. The biosensor has exhibited good repeatability (with a correlation) and reproducibility. Finally, artificial serum samples spiked with known concentrations of PTH were analyzed by the proposed biosensor. To demonstrate the feasibility of the biosensor in practical analysis, real human serum samples and the artificial serum samples were analyzed.",science
10.1016/j.enbuild.2014.10.083,Journal,Energy and Buildings,scopus,2015-06-03,sciencedirect,Building optimization and control algorithms implemented in existing BEMS using a web based energy management and control system,https://api.elsevier.com/content/abstract/scopus_id/84930673408,"The aim of the present paper is to analyze a building optimization and control (BOC) algorithm which is implemented in the existing building energy management system (BEMS) of the Saint George Hospital in Chania, Greece. The developed algorithm consists of predicted models for outdoor/indoor air temperature using artificial neural networks, multi-step optimization using genetic algorithms and Real Time control using fuzzy techniques. The algorithm is developed in Matlab™ environment and is implemented to the existing BEMS of the Hospital, using a specialized web-based energy management and control system (Web-EMCS). The implementation of the BOC algorithm is realized by developing a “.net assembly” code, which interconnects the Web-EMCS with the existing conventional hospital's BEMS, without the need of Matlab™. The annual primary energy efficiency achieved is almost 36%.",science
10.1016/j.anaerobe.2015.02.001,Journal,Anaerobe,scopus,2015-06-01,sciencedirect,Development of a recombinant flagellin based ELISA for the detection of Clostridium chauvoei,https://api.elsevier.com/content/abstract/scopus_id/84922419204,"Blackleg, an economically important and highly fatal disease of ruminants, is caused by anaerobic bacillus, Clostridium chauvoei. Identification and differentiation of the causative agent is crucial for implementation of therapeutic and control measures in real time. Most of the diagnostic tests available for blackleg are PCR based, and only a couple of serological tests have been reported. In this study, we targeted flagellin, an important immunogenic protein of C. chauvoei, to develop a sandwich ELISA for detection of C. chauvoei. Sequence analysis of flagellin gene of related Clostridium species showed that central region of flagellin gene is unique to C. chauvoei. Hence, we cloned and expressed central region of flagellin in a prokaryotic expression system. Antiserum against recombinant flagellin was generated in rabbits and chickens. A sandwich ELISA was developed, in which rabbit anti-flagellin antibodies were used as capture antibodies and chicken anti-flagellin antibodies as detecting antibodies. The test was specific and sensitive in detection of up to 104 CFU/ml of C. chauvoei. This study shows that assay developed can be used for detection of C. chauvoei in suspected samples.",science
10.1016/j.bios.2014.08.018,Journal,Biosensors and Bioelectronics,scopus,2015-05-05,sciencedirect,Integrated potentiostat for electrochemical sensing of urinary 3-hydroxyanthranilic acid with molecularly imprinted poly(ethylene-co-vinyl alcohol),https://api.elsevier.com/content/abstract/scopus_id/84922318833,"Changing demographics, the rise of personalized medicine and increased identification of biomarkers for diagnosis and management of chronic disease have increased the demand for portable bioanalytical instrumentation and point-of-care. The recent development of molecularly imprinted polymers enables production of low cost and highly stable sensing chips; however, the commercially available and full functional instruments employed for electrochemical analysis have shortcomings in actual homecare applications. In this work, integrated circuits (ICs) for monolithic implementation of voltammeter potentiostat with a large dynamic current range (5nA to 1.2mA) and short conversion time (10ms) were fabricated in a 0.35μm complementary metal-oxide-semiconductor (CMOS) process. The new instrumentation was tested with molecular imprinted sensors for 3-hydroxyanthranilic acid (3HAA) in urine. The sensor consisted of molecular imprinted of poly(ethylene-co-vinyl alcohol)s (abbreviated as EVALs) for implementation in a flow injection analysis system. The EVAL containing 32 ethylene mol% had the highest imprinting effectiveness for the target molecules. Fit-for-purpose figures of merit were achieved with a limit-of-detection (LOD) of 3.06pg/mL. The measurements obtained in real undiluted urine samples fell within the reference concentration range of 50–550ng/mL.",science
10.1016/j.bios.2014.07.084,Journal,Biosensors and Bioelectronics,scopus,2015-05-05,sciencedirect,"Lytic enzymes as selectivity means for label-free, microfluidic and impedimetric detection of whole-cell bacteria using ALD-Al<inf>2</inf>O<inf>3</inf> passivated microelectrodes",https://api.elsevier.com/content/abstract/scopus_id/84922271489,"Point-of-care (PoC) diagnostics for bacterial detection offer tremendous prospects for public health care improvement. However, such tools require the complex combination of the following performances: rapidity, selectivity, sensitivity, miniaturization and affordability. To meet these specifications, this paper presents a new selectivity method involving lysostaphin together with a CMOS-compatible impedance sensor for genus-specific bacterial detection. The method enables the sample matrix to be directly flown on the polydopamine-covered sensor surface without any pre-treatment, and considerably reduces the background noise. Experimental proof-of-concept, explored by simulations and confirmed through a setup combining simultaneous optical and electrical real-time monitoring, illustrates the selective and capacitive detection of Staphylococcus epidermidis in synthetic urine also containing Enterococcus faecium. While providing capabilities for miniaturization and system integration thanks to CMOS compatibility, the sensors show a detection limit of ca. 108 (CFU/mL).min in a 1.5μL microfluidic chamber with an additional setup time of 50min. The potentials, advantages and limitations of the method are also discussed.",science
10.1016/j.compind.2015.05.001,Journal,Computers in Industry,scopus,2015-05-04,sciencedirect,Artificial cognitive control with self-x capabilities: A case study of a micro-manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/84955410573,"Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup.",science
10.1016/j.ifacol.2015.06.162,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Supporting urban home health care in daily business and times of disasters,https://api.elsevier.com/content/abstract/scopus_id/84953882203,"Home health care (HHC) services are of vital importance for today's society. They allow old and frail people a self-determined living in their familiar environment. Due to the current demographic and social developments further increases in demand for HHC must be expected. Additionally, people with limited mobility or relying on medical supply usually need consistent care. Thus, HHC service providers will be faced with two challenges: an increased organizational effort due to the rising demand and the need for an anticipatory risk management. Previous research combining optimization and risk management in the field of HHC limits itself to rural regions, where nurses are solely using cars. The presented work specifically aims to deal with the peculiarities of urban regions. Together with the Austrian Red Cross (ARC), a vulnerability analysis has been conducted in order to identify the critical success factors and processes of HHC as well as potential threats. To support the daily scheduling, a Tabu Search (TS) based metaheuristic has been implemented. As nurses can choose between different transport modes (public transport, car, bike, and walking), time-dependent multimodal transport has been considered. The TS has been tested with real-world data from the ARC in Vienna to support both, daily business and scheduling in times of disasters. Significant reductions of travel and waiting times can be obtained, such that more time remains for serving the clients. Through sensitivity analysis the effects of disasters (esp. blackout, pandemics, and heat waves) are visualized and the operational limits during such events are shown.",science
10.1016/j.jep.2015.01.030,Journal,Journal of Ethnopharmacology,scopus,2015-04-02,sciencedirect,Reporting effectiveness of an extract of three traditional Cretan herbs on upper respiratory tract infection: Results from a double-blind randomized controlled trial,https://api.elsevier.com/content/abstract/scopus_id/84922793641,"Ethnopharmacological relevance
                  Observations from the island of Crete, Greece suggest that infusions of traditional Cretan aromatic plants, well known for their ethnopharmacological use in Eastern Mediterranean region and Near East, could be effective in the prevention and treatment of upper respiratory tract infections, including viral-induced infections. The aim of this study was to report the effectiveness of an essential-oil extract of three Cretan aromatic plants in the treatment of cases with an upper respiratory tract infection.
               
                  Materials and methods
                  A double blind randomized controlled trial was implemented between October 2013 and February 2014. An essential-oil extract of Cretan aromatic plants in olive oil (total volume of 15ml of essential oil per litre of olive oil) was administered as 0.5ml soft gel capsules, twice a day, for 7 days. Placebo treatment was 0.5ml olive oil in soft gel capsules. Eligible patients were those presenting for clinical examination in the selected setting with signs and symptoms of upper respiratory tract infection that had begun within the previous 24hours. Real-Time Polymerase Chain Reaction (PCR) was used for the detection of respiratory viruses. The primary outcome was the severity and duration of symptoms of upper respiratory tract infection, assessed using the Wisconsin Upper Respiratory System Survey (WURSS-21) questionnaire. A secondary outcome of interest was the change in C-reactive protein (CRP) status.
               
                  Results
                  One hundred and five patients completed the study: 51 in the placebo group, and 54 in the intervention (treated) group. Baseline characteristics were similar in the two groups. No statistically significant differences were found in symptom duration or severity between the two groups, although small and clinically favorable effects were observed. When the analysis was restricted to subjects with a laboratory-documented viral infection, the percentage of patients with cessation of symptoms after 6 days of treatment was 91% in the intervention group and 70% in the control group (p=0.089). At baseline, one third of the patients in each group had elevated CRP levels. At follow-up, the respective proportions were 0% in the intervention group and 15% in the placebo group (p=0.121). The data were also in a favorable direction when 50% and 80% symptom reduction points were considered for specific virus types.
               
                  Conclusions
                  Compared with placebo the essential-oil extract of three Cretan aromatic plants provided no detectable statistically significant benefit or harm in the patients with upper respiratory illness, although descriptive differences were identified in favorable direction mainly in the virus-positive population.",science
10.1016/j.cmpb.2015.02.002,Journal,Computer Methods and Programs in Biomedicine,scopus,2015-04-01,sciencedirect,GPU-based parallel group ICA for functional magnetic resonance data,https://api.elsevier.com/content/abstract/scopus_id/84924548399,"The goal of our study is to develop a fast parallel implementation of group independent component analysis (ICA) for functional magnetic resonance imaging (fMRI) data using graphics processing units (GPU). Though ICA has become a standard method to identify brain functional connectivity of the fMRI data, it is computationally intensive, especially has a huge cost for the group data analysis. GPU with higher parallel computation power and lower cost are used for general purpose computing, which could contribute to fMRI data analysis significantly. In this study, a parallel group ICA (PGICA) on GPU, mainly consisting of GPU-based PCA using SVD and Infomax-ICA, is presented. In comparison to the serial group ICA, the proposed method demonstrated both significant speedup with 6–11 times and comparable accuracy of functional networks in our experiments. This proposed method is expected to perform the real-time post-processing for fMRI data analysis.",science
10.1016/j.ascom.2015.01.002,Journal,Astronomy and Computing,scopus,2015-04-01,sciencedirect,The overlooked potential of Generalized Linear Models in astronomy-II: Gamma regression and photometric redshifts,https://api.elsevier.com/content/abstract/scopus_id/84922370372,"Machine learning techniques offer a precious tool box for use within astronomy to solve problems involving so-called big data. They provide a means to make accurate predictions about a particular system without prior knowledge of the underlying physical processes of the data. In this article, and the companion papers of this series, we present the set of Generalized Linear Models (GLMs) as a fast alternative method for tackling general astronomical problems, including the ones related to the machine learning paradigm. To demonstrate the applicability of GLMs to inherently positive and continuous physical observables, we explore their use in estimating the photometric redshifts of galaxies from their multi-wavelength photometry. Using the gamma family with a log link function we predict redshifts from the PHoto-z Accuracy Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from Data Release 10. We obtain fits that result in catastrophic outlier rates as low as ∼1% for simulated and ∼2% for real data. Moreover, we can easily obtain such levels of precision within a matter of seconds on a normal desktop computer and with training sets that contain merely thousands of galaxies. Our software is made publicly available as a user-friendly package developed in Python, R and via an interactive web application. This software allows users to apply a set of GLMs to their own photometric catalogues and generates publication quality plots with minimum effort. By facilitating their ease of use to the astronomical community, this paper series aims to make GLMs widely known and to encourage their implementation in future large-scale projects, such as the Large Synoptic Survey Telescope.",science
10.1016/B978-0-12-800341-1.00001-2,Book,Industrial Agents: Emerging Applications of Software Agents in Industry,scopus,2015-03-12,sciencedirect,Software Agent Systems,https://api.elsevier.com/content/abstract/scopus_id/84944408386,"Agents and multi-agent systems are one of the most fascinating topics in computer science. They attracted and unified not only researchers from nearly all computer science areas but also researchers from other core disciplines such as psychology, sociology, biology, or control engineering. In the meantime, agent-based systems successfully prove their usefulness in many different real-life application areas, especially industrial ones. This is a clear sign that this discipline has become mature. This chapter presents a comprehensive state-of-the-art introduction into advanced software agents and multi-agent systems. Properties and types of agents and multi-agent systems are discussed, which include precise definitions of both. A successful cooperation between agents is only possible if they can communicate in an efficient and semantically meaningful way. Thus, relevant communication strategies are discussed. Agent-based applications can be very powerful, complex systems. Their development can profit a lot from adequate support tools. Different development support options and environments are discussed in some detail. Due to their nature, multi-agent systems are excellent candidates for the realization of comprehensive simulations, especially if the individuality and uniqueness of components of the simulation environment play an important role. The second part of the chapter addresses supporting technologies and concepts. Ontologies, self-organization and emergence, and swarm intelligence and stigmergy are introduced and discussed in some detail.",science
10.1016/j.artmed.2014.08.006,Journal,Artificial Intelligence in Medicine,scopus,2015-02-01,sciencedirect,"A systems approach to healthcare: Agent-based modeling, community mental health, and population well-being",https://api.elsevier.com/content/abstract/scopus_id/84928207619,"Purpose
                  Explore whether agent-based modeling and simulation can help healthcare administrators discover interventions that increase population wellness and quality of care while, simultaneously, decreasing costs. Since important dynamics often lie in the social determinants outside the health facilities that provide services, this study thus models the problem at three levels (individuals, organizations, and society).
               
                  Methods
                  The study explores the utility of translating an existing (prize winning) software for modeling complex societal systems and agent's daily life activities (like a Sim City style of software), into a desired decision support system. A case study tests if the 3 levels of system modeling approach is feasible, valid, and useful. The case study involves an urban population with serious mental health and Philadelphia's Medicaid population (n
                     =527,056), in particular.
               
                  Results
                  Section 3 explains the models using data from the case study and thereby establishes feasibility of the approach for modeling a real system. The models were trained and tuned using national epidemiologic datasets and various domain expert inputs. To avoid co-mingling of training and testing data, the simulations were then run and compared (Section 4.1) to an analysis of 250,000 Philadelphia patient hospital admissions for the year 2010 in terms of re-hospitalization rate, number of doctor visits, and days in hospital. Based on the Student t-test, deviations between simulated vs. real world outcomes are not statistically significant. Validity is thus established for the 2008–2010 timeframe. We computed models of various types of interventions that were ineffective as well as 4 categories of interventions (e.g., reduced per-nurse caseload, increased check-ins and stays, etc.) that result in improvement in well-being and cost.
               
                  Conclusions
                  The 3 level approach appears to be useful to help health administrators sort through system complexities to find effective interventions at lower costs.",science
10.1016/j.jbi.2014.10.009,Journal,Journal of Biomedical Informatics,scopus,2015-02-01,sciencedirect,Quantifying the determinants of outbreak detection performance through simulation and machine learning,https://api.elsevier.com/content/abstract/scopus_id/84924493147,"Objective
                  To develop a probabilistic model for discovering and quantifying determinants of outbreak detection and to use the model to predict detection performance for new outbreaks.
               
                  Materials and methods
                  We used an existing software platform to simulate waterborne disease outbreaks of varying duration and magnitude. The simulated data were overlaid on real data from visits to emergency department in Montreal for gastroenteritis. We analyzed the combined data using biosurveillance algorithms, varying their parameters over a wide range. We then applied structure and parameter learning algorithms to the resulting data set to build a Bayesian network model for predicting detection performance as a function of outbreak characteristics and surveillance system parameters. We evaluated the predictions of this model through 5-fold cross-validation.
               
                  Results
                  The model predicted performance metrics of commonly used outbreak detection methods with an accuracy greater than 0.80. The model also quantified the influence of different outbreak characteristics and parameters of biosurveillance algorithms on detection performance in practically relevant surveillance scenarios. In addition to identifying characteristics expected a priori to have a strong influence on detection performance, such as the alerting threshold and the peak size of the outbreak, the model suggested an important role for other algorithm features, such as adjustment for weekly patterns.
               
                  Conclusion
                  We developed a model that accurately predicts how characteristics of disease outbreaks and detection methods will influence on detection. This model can be used to compare the performance of detection methods under different surveillance scenarios, to gain insight into which characteristics of outbreaks and biosurveillance algorithms drive detection performance, and to guide the configuration of surveillance systems.",science
10.1016/j.compag.2014.12.010,Journal,Computers and Electronics in Agriculture,scopus,2015-02-01,sciencedirect,A Decision Support System to design modified atmosphere packaging for fresh produce based on a bipolar flexible querying approach,https://api.elsevier.com/content/abstract/scopus_id/84921031926,"To design new packaging for fresh food, stakeholders of the food chain express their needs and requirements, according to some goals and objectives. These requirements can be gathered into two groups: (i) fresh food related characteristics and (ii) packaging intrinsic characteristics. Modified Atmosphere Packaging (MAP) is an efficient way to delay senescence and spoilage and thus to extend the very short shelf life of respiring products such as fresh fruits and vegetables. Consequently, packaging O2/CO2 permeabilities must fit the requirements of fresh fruits and vegetable as predicted by virtual MAP simulating tools. Beyond gas permeabilities, the choice of a packaging material for fresh produce includes numerous other factors such as the cost, availability, potential contaminants of raw materials, process ability, and waste management constraints. For instance, the user may have the following multi-criteria query for his/her product asking for a packaging with optimal gas permeabilities that guarantee product quality and optionally a transparent packaging material made from renewable resources with a cost for raw material less than 3€/kg. To help stakeholders taking a rational decision based on the expressed needs, a new multi-criteria Decision Support System (DSS) for designing biodegradable packaging for fresh produce has been built. In this paper we present the functional specification, the software architecture and the implementation of the developed tool. This tool includes (i) a MAP simulation module combining mass transfer models and respiration of the food, (ii) a multi-criteria flexible querying module which handles imprecise, uncertain and missing data stored in the database. We detail its operational functioning through a real life case study to determine the most satisfactory materials for apricots packaging.",science
10.1016/j.knosys.2014.11.029,Journal,Knowledge-Based Systems,scopus,2015-02-01,sciencedirect,"Collaborator recommendation in interdisciplinary computer science using degrees of collaborative forces, temporal evolution of research interest, and comparative seniority status",https://api.elsevier.com/content/abstract/scopus_id/84920525842,"Currently, the research in computer science has been exponentially expanded beyond its own fields into the other research fields such as medical science, business, and social science in forms of collaborative researches. This collaborative researches stimulate a new recommending algorithm for determining a potential research collaborator under the interdisciplinary environment. Unlike other research fields, the research problems in computer science can be transformed to other known and solvable problems. In this paper, a new hybrid algorithm based on dynamic collaboration over time was proposed for recommending an appropriate collaborator. Besides considering only three basic factors concerning social proximity, friendship, and complementarity skill as employed by others’, three new additional factors related to research interest, up-to-date publication data, and seniority of researcher are involved in our analysis. A set of new measures for all six recommending factors were proposed. The experiments were conducted with real bibliographic data within six continuous years of publication and over six topics in computer science. Our results were significantly higher than the results of the other methods at 90% confidence level.",science
10.1016/j.ipm.2014.04.001,Journal,Information Processing and Management,scopus,2015-01-01,sciencedirect,Active learning for sentiment analysis on data streams: Methodology and workflow implementation in the ClowdFlows platform,https://api.elsevier.com/content/abstract/scopus_id/85027927860,"Sentiment analysis from data streams is aimed at detecting authors’ attitude, emotions and opinions from texts in real-time. To reduce the labeling effort needed in the data collection phase, active learning is often applied in streaming scenarios, where a learning algorithm is allowed to select new examples to be manually labeled in order to improve the learner’s performance. Even though there are many on-line platforms which perform sentiment analysis, there is no publicly available interactive on-line platform for dynamic adaptive sentiment analysis, which would be able to handle changes in data streams and adapt its behavior over time. This paper describes ClowdFlows, a cloud-based scientific workflow platform, and its extensions enabling the analysis of data streams and active learning. Moreover, by utilizing the data and workflow sharing in ClowdFlows, the labeling of examples can be distributed through crowdsourcing. The advanced features of ClowdFlows are demonstrated on a sentiment analysis use case, using active learning with a linear Support Vector Machine for learning sentiment classification models to be applied to microblogging data streams.",science
10.1016/j.promfg.2015.07.372,Journal,Procedia Manufacturing,scopus,2015-01-01,sciencedirect,Case Study: Use of Online Tools in the Classroom and their Impact on Industrial Design Pedagogy,https://api.elsevier.com/content/abstract/scopus_id/85009959445,"Industrial Design education is going through a rapid evolution with more Design students making use of internet resources and tools such as crowdsourcing, 3D printing services, and other web-based tools to validate their ideas more quickly. The popularity of online 3D printing services such as Shapeways and Sculpteo accelerate the design process and learning. These services allow the designer to “print” virtually in any material such as plastics or metals. The impact of this new technology and other new web-based tools is significant not only in the industry but in the classroom as well. Current Industrial Design pedagogy is still partially based on technology, materials and processes that were developed a century ago. For example, pencils and paper are still the primary idea development tool. Books and magazines used to be the primary research tool but already have been surpassed by the Internet. Computer technology has improved significantly since the appearance of the first PCs, Macs and CNC machines. With all these advances in technology, one aspect of Industrial Design education that needs to be re-visited is the pedagogy of Design Drafting in this new age of online 3D printing services. The traditional Design pedagogy that was based on the development of different skills or competencies in separate courses or classes have not changed significantly in the last 40 years, 3D printing technology could potentially change this situation. Some new academic papers discuss this newer trend in Industrial Design schools but very few provide examples on how they implemented the new Internet-based 3D printing services in their curriculum. Industrial Design schools need to adapt quickly to the new reality, embracing Internet resources and online tools as core skills that every designer must have. This paper will discuss one case in particular where student projects were developed using online 3D printing services.",science
10.1016/j.procs.2015.12.257,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Improving the Skills and Knowledge of Future Designers in the Field of Ecodesign Using Virtual Reality Technologies,https://api.elsevier.com/content/abstract/scopus_id/84964057185,"Nowadays future designers are required to explore different fields of engineering (e.g. basis of design and operation of machines, methodology of process design, characteristics of phases of a product life cycle, ergonomics, etc.) in order to obtain sufficient knowledge about the product, the problems of its operation and related services. Currently, there is a strong tendency to take into account the impact of a product on the natural environment. The handling of products that are withdrawn from usage is one of the big problems emerging in a modern society. One of the possibilities to solve these problems is taking into account the environmental issues at the very early stage of the product design. Authors created the methodology to include recycling requirements in the phase of design of the product with support of the CAD 3D systems14,15. In details, the method of ecological-oriented product assessment during the design process was implemented in CAD 3D environment in order to improve the skills and knowledge of future designers about environmental aspects of the designed product12. In order to enhance the effectiveness of the training of designers, authors decided to use immersive Virtual Reality (VR) technologies that are more and more often used as advanced training systems. The user/trained person can explore Virtual Environment (VE) for educational and exercise purposes5. Immersive VR technologies are gaining wider use as engineering design tools to support the product design phase because of their ability to deliver an immersive and user friendly environment that can be used as digital test-bed for prototypes of the product30. Paper briefly describes the analysis of the recycling of selected product designed in the CAD 3D system with the support of Virtual Reality technologies that were based on the recycling product model (RpM) and the agent technology12,14,15. The RpM, developed during the geometric modelling phase, includes the data necessary for a comprehensive product recyclability evaluation already at the design stage. This approach allows designer to select appropriate solutions that facilitate future disassembly and to choose materials most suitable in terms of future recycling. The analysis allows to make an assessment of the susceptibility of the product for recycling using the agent system. The agent system, in accordance with the recycling product model (RpM) that was implemented in the 3D CAD system, is an innovative tool for the designer to enable a comprehensive assessment of the product in terms of its susceptibility to recycling. Future designers (students of mechanical engineering fields) gained possibility to improve their skills and knowledge in the field of ecodesign through the immersive trainings of virtual product design for recycling. Applying the Virtual Reality technologies for training of the future designers was aimed at enhancing the effectiveness of training mainly due to immersion and interaction with the virtual environment and to explore the product before it is constructed, as well as provide learning of the environments in which the finished product will be operated. As the example of the recycling-oriented virtual product design, case study of the design of small household appliance is presented. Participants of the immersive training (students) have a possibility to design the product in the Virtual Reality and select the optimal variant, which meets all environmental requirements. Short description of the recycling modeling (creation of connections, defining the extended materials and disassembly attributes) as well as an example of recycling assessment such as calculation of recycling assessment measures or providing tips and suggestions generated by agent system is also presented.",science
10.1016/j.procs.2015.09.269,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Virtual Sign - A Real Time Bidirectional Translator of Portuguese Sign Language,https://api.elsevier.com/content/abstract/scopus_id/84962793739,"Promoting equity, equal opportunities to all and social inclusion of people with disabilities is a concern of modern societies at large and a key topic in the agenda of European Higher Education. Despite all the progress, we cannot ignore the fact that the conditions provided by the society for the deaf are still far from being perfect. The communication with deaf by means of written text is not as efficient as it might seem at first. In fact, there is a very deep gap between sign language and spoken/written language. The vocabulary, the sentence construction and the grammatical rules are quite different among these two worlds. These facts bring significant difficulties in reading and understanding the meaning of text for deaf people and, on the other hand, make it quite difficult for people with no hearing disabilities to understand sign language. The deployment of tools to assist the daily communication, in schools, in public services, in museums and other, between deaf people and the rest may be a significant contribution to the social inclusion of the deaf community. The work described in this paper addresses the development of a bidirectional translator between Portuguese Sign Language and Portuguese text. The translator from sign language to text resorts to two devices, namely the Microsoft Kinect and 5DT Sensor Gloves in order to gather data about the motion and shape of the hands. The hands configurations are classified using Support Vector Machines. The classification of the movement and orientation of the hands are achieved through the use of Dynamic Time Warping algorithm. The translator exhibits a precision higher than 90%. In the other direction, the translation of Portuguese text to Portuguese Sign Language is supported by a 3D avatar which interprets the entered text and performs the corresponding animations.",science
10.1016/j.procs.2015.08.504,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Fitness based position update in spider monkey optimization algorithm,https://api.elsevier.com/content/abstract/scopus_id/84962597795,Spider Monkey Optimization (SMO) technique is most recent member in the family of swarm optimization algorithms.SMO algorithm fall in class of Nature Inspired Algorithm (NIA). SMO algorithm is good in exploration and exploitation of local search space and it is well balanced algorithm most of the times. This paper presents a new strategy to update position of solution during local leader phase using fitness of individuals. The proposed algorithm is named as Fitness based Position Update in SMO (FPSMO) algorithm as it updates position of individuals based on their fitness. The anticipated strategy enhances the rate of convergence. The planned FPSMO approach tested over nineteen benchmark functions and for one real world problem so as to establish superiority of it over basic SMO algorithm.,science
10.1016/j.neucom.2014.08.103,Journal,Neurocomputing,scopus,2015-01-01,sciencedirect,An experiment of subconscious intelligent social computing on household appliances,https://api.elsevier.com/content/abstract/scopus_id/84952629652,"Subconscious Social Intelligence refers to the design of social services oriented towards user problem solving, providing an underlying innovation layer is able to generate new solutions to yet unknown problems. The innovation layer is achieved by Computational Intelligence techniques, encompassing machine learning to build models of user satisfaction over solution quality, and stochastic search as the means for innovation generation. The SandS project provides an instance of such paradigm, where household appliances are the subject of the social service. This paper proposes a specific architecture, reporting results on a synthetic database build according to SandS project current designs. Database synthesis for system tuning and validation is a critical issue, hence the paper details the considerations guiding its design and generation, as well as the validation procedure ensuring the ecological validity of the innovation process simulation. The architecture is composed of a Support Vector Regression (SVR) module for user satisfaction modeling, and an Evolution Strategy (ES) achieving recipe innovation. The paper reports some computational experiments that may guide the real life implementation. The reported results are methodologically sound as far as they are independent of the generation process.",science
10.1016/j.procs.2015.07.566,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,"Using Augmented Reality to Enhance Aetherpet, a Prototype of a Social Game",https://api.elsevier.com/content/abstract/scopus_id/84948432639,The objective of this research is to create a social game that can be shared not only by digital means but also by non-digital means while at the same time providing updated information. The same utilizes augmented reality (AR) technology and a cloud marker recognition system and allows creations of unique marker for each player without the need to do an update every time. Aetherpet is a virtual pet game designed to become a prototype in implementing augmented reality technology in mobile gaming. This application is developed for the Android platform and Vuforia library. The marker image generated from the application is converted into features; that can be detected on a grayscale marker image. The prototype result shows that augmented reality is possible to be implemented. There are only small restrictions on the implementation such as incompatibility of some features and requirement for a reliable server to handle the game information processing.,science
10.1016/j.bica.2015.09.002,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,Imitation of honey bees' concept learning processes using Vector Symbolic Architectures,https://api.elsevier.com/content/abstract/scopus_id/84945980580,"This article presents a proof-of-concept validation of the use of Vector Symbolic Architectures as central component of an online learning architectures. It is demonstrated that Vector Symbolic Architectures enable the structured combination of features/relations that have been detected by a perceptual circuitry and allow such relations to be applied to novel structures without requiring the massive training needed for classical neural networks that depend on trainable connections.
                  The system is showcased through the functional imitation of concept learning in honey bees. Data from real-world experiments with honey bees (Avarguès-Weber et al., 2012) are used for benchmarking. It is demonstrated that the proposed pipeline features a similar learning curve and accuracy of generalization to that observed for the living bees. The main claim of this article is that there is a class of simple artificial systems that reproduce the learning behaviors of certain living organisms without requiring the implementation of computationally intensive cognitive architectures. Consequently, it is possible in some cases to implement rather advanced cognitive behavior using simple techniques.",science
10.1016/B978-0-12-800881-2.00006-2,Book,Household Service Robotics,scopus,2015-01-01,sciencedirect,A Household Service Robot with a Cellphone Interface,https://api.elsevier.com/content/abstract/scopus_id/84944415545,"In this chapter, an efficient and low-cost cellphone-commandable mobile manipulation system is described. Aiming at home use and elderly caring, this system can be easily commanded through a common cellphone network to efficiently grasp objects in a household environment, utilizing several low-cost off-the-shelf devices. Unlike the visual servo technology using a high quality vision system with the associated high cost, the household-service robot would not be able to afford such a high quality vision servo system, and thus it is essential to use some low-cost devices. However, it is extremely challenging to create such a vision system with precise localization, as well as motion control. To tackle this challenge, we developed a real-time vision system with which a reliable grasping algorithm combining machine vision, robotic kinematics and motor control technology is presented. After the target is captured by the arm camera, the arm camera keeps tracking the target while the arm keeps stretching until the end effector reaches the target. However, if the target is not captured by the arm camera, the arm will make a move to help the arm camera capture the target under the guidance of the head camera. This algorithm is implemented on two robot systems: one with a fixed base and another with a mobile base. The results demonstrate the feasibility and efficiency of the algorithm and system we developed, and our study shows the significance of developing a service robot in a modern household environment.",science
10.1016/j.procs.2015.08.126,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Applying bipartite network approach to scarce data: Modeling habitat suitability of a marine mammal species,https://api.elsevier.com/content/abstract/scopus_id/84941090534,"Current study intends to formulate a habitat suitability model of a newly surveyed marine mammal species where the research scenario is characterized by real-world data that is scarce with no detail demographic value available. It is extremely challenging to solve it using either traditional statistical approaches where huge amount of data are required or deterministic approaches that commonly employ partial differential equations (PDE) model which are based strongly on well-established physical laws and entail detail species-specific demographic values. Conversely, the graph-theoretic based bipartite network modeling (BNM) approach is not bound by the above limitations and is thus employed in this study. The result produced is a bipartite habitat suitability network model consisting thirteen location nodes and thirteen species nodes, each with their respective parameters of which some are quantified through a machine learning algorithm, and thirty-eight weighted edges that are quantified through multiplication rule. Habitat suitability index, generated through implementation of an adapted web-based search algorithm, is produced and utilized for the ranking of these location nodes. The ranking result obtained is in good agreement with the past literature. The results produced also provide pertinent input to the related practitioners for the conservation of the species and preservation of the habitat and environment ecology.",science
10.1016/j.ins.2015.06.040,Journal,Information Sciences,scopus,2015-01-01,sciencedirect,On the design of shared memory approaches to parallelize a multiobjective bee-inspired proposal for phylogenetic reconstruction,https://api.elsevier.com/content/abstract/scopus_id/84940668551,"Current efforts in solving computationally demanding optimization problems in bioinformatics rely on the combination of bioinspired computing and parallelism. The multiobjective nature shown by a wide variety of these problems represents an additional challenge, as the optimization of multiple objective functions involves growing computational requirements. In this work, a multiobjective metaheuristic inspired by honey bees is applied to tackle the phylogenetic inference problem. For this purpose, two parallel implementations for shared memory architectures are proposed: a synchronous generational model and a novel non-generational design inspired by the asynchronous behaviour of bees in nature. Experiments on six real biological data sets and comparisons with other parallel biological methods point out the relevance of applying nature-inspired parallelization strategies, addressing the performance pitfalls shown by traditional parallelization schemes.",science
10.1016/j.procs.2015.07.308,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,"Scalability of stochastic gradient descent based on ""smart"" sampling techniques",https://api.elsevier.com/content/abstract/scopus_id/84939131818,"Various appealing ideas have been recently proposed in the statistical literature to scale-up machine learning techniques and solve predictive/inferential problems from “Big Datasets”. Beyond the massively parallelized and distributed approaches exploiting hardware architectures and programming frameworks that have received increasing interest these last few years, several variants of the Stochastic Gradient Descent (SGD) method based on “smart” sampling procedures have been designed for accelerating the model fitting stage. Such techniques exploit either the form of the objective functional or some supposedly available auxiliary information, and have been thoroughly investigated from a theoretical viewpoint. Though attractive, such statistical methods must be also analyzed from a computational perspective, bearing the possible options offered by recent technological advances in mind. It is thus of vital importance to investigate how to implement efficiently these inferential principles in order to achieve the best trade-off between computational time and accuracy. In this paper, we explore the scalability of the SGD techniques introduced in [9,11] from an experimental perspective. Issues related to their implementation on distributed computing platforms such as Apache Spark are also discussed and experimental results based on large-scale real datasets are displayed in order to illustrate the relevance of the promoted approaches.",science
10.1002/jps.24409,Journal,Journal of Pharmaceutical Sciences,scopus,2015-01-01,sciencedirect,A novel microwave sensor for real-time online monitoring of roll compacts of pharmaceutical powders online - A comparative case study with NIR,https://api.elsevier.com/content/abstract/scopus_id/84926368685,"Control of particulate processes is hard to achieve because of the ease with which powders tend to segregate. Thus, proper sensing methods must be employed to ensure content uniformity during operation. The role of sensing schemes becomes even more critical while operating the process continuously as measurements are essential for implementation of feedback control (Austin et al. 2013. J Pharm Sci 102(6):1895-1904; Austin et al. 2014. Anal Chim Acta 819:82-93). A microwave sensor was developed and shown to be effective in online measurement of active pharmaceutical ingredient (API) concentration in a powder blend. During powder transport and hopper storage before processing, powder blends may segregate and cause quality deviations in the subsequent tableting operation. Therefore, it is critical to know the API concentration in the ribbons as the content uniformity is fixed once the ribbon is processed. In this study, a novel microwave sensor was developed that could provide measurement of a roller compacted ribbon's API concentration online, along with its density and moisture content. The results indicate that this microwave sensor is capable of increased accuracy compared with a commercially available near-IR probe for the determination of content uniformity and density in roller compacted ribbons online.",science
10.1016/j.snb.2014.12.136,Journal,"Sensors and Actuators, B: Chemical",scopus,2015-01-01,sciencedirect,Zebra GC: A mini gas chromatography system for trace-level determination of hazardous air pollutants,https://api.elsevier.com/content/abstract/scopus_id/84923321088,"A ready-to-deploy implementation of a microfabricated gas chromatography (μGC) system characterized for detecting hazardous air pollutants (HAPs) at parts-per-billion (ppb) concentrations in complex mixtures has been described. A microfabricated preconcentrator (μPC), MEMS separation column with on-chip thermal conductivity detector (μSC-TCD), flow controller unit, and all necessary flow and thermal management as well as user interface circuitry are integrated to realize a fully functional μGC system. The work reports extensive characterization of μPC and μSC-TCD for target analytes: benzene, toluene, tetrachloroethylene, chlorobenzene, ethylbenzene, and p-xylene. A Limit of Detection (LOD) of ∼1ng was achieved, which corresponds to a sampling time of 10min at a flow rate of 1mL/min for an analyte present at ∼25ppbv. An innovative method using flow-manipulation generated sharp injection plugs from the μPC even in the presence of a flow-sensitive detector like a μTCD. The μGC system is compared against conventional automated thermal desorption–gas chromatography–flame ionization detector (ATD–GC–FID) system for real gasoline samples in simulated car refueling scenario. The μGC system detected five peaks, including three of the target analytes and required ∼3 orders of magnitude lower sample volume as compared to the conventional system.",science
10.1016/j.asoc.2014.12.018,Journal,Applied Soft Computing Journal,scopus,2015-01-01,sciencedirect,Implementation of the RBF neural chip with the back-propagation algorithm for on-line learning,https://api.elsevier.com/content/abstract/scopus_id/84921719882,This article presents the hardware implementation of the floating-point processor (FPP) to develop the radial basis function (RBF) neural network for the general purpose of pattern recognition and nonlinear control. The floating-point processor is designed on a field programmable gate array (FPGA) chip to execute nonlinear functions required in the parallel calculation of the back-propagation algorithm. Internal weights of the RBF network are updated by the online learning back-propagation algorithm. The on-line learning process of the RBF chip is compared numerically with the results of the RBF neural network learning process written in the MATLAB program. The performance of the designed RBF neural chip is tested for the real-time pattern classification of the XOR logic. Performances are evaluated by comparing results from the MATLAB through extensive experimental studies.,science
10.1016/j.measurement.2014.12.037,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2015-01-01,sciencedirect,FPGA-based reconfigurable system for tool condition monitoring in high-speed machining process,https://api.elsevier.com/content/abstract/scopus_id/84921369568,"We present a reconfigurable system to detect and indicate online and in real time, the cutting tool conditions in high-speed face milling. It consists of a data acquisition system (DAS) and a hardware signal processing (HSP) unit. The DAS acquires and digitalizes the cutting vibration signals generated from machining tests performed under different tool conditions and cutting parameters. The HSP unit processes the digitalized vibration signals using reconfigurable IIR band-pass digital filter and statistical techniques, designed and implemented into a single field-programmable gate array (FPGA) and coefficients read-only memories. The system operation is divided into learning and monitoring modes. The tool condition is indicated by an alarm signal, one LED indicator, and a message shown on four-digit seven-segment displays. In all experiments, the system correctly detected the tool condition. The proposed system is fast, compact, reliable, and economical, and no modification of the machine-tool structure is required.",science
10.1016/j.ijepes.2014.11.030,Journal,International Journal of Electrical Power and Energy Systems,scopus,2015-01-01,sciencedirect,FPGA based on-line Artificial Neural Network Selective Harmonic Elimination PWM technique,https://api.elsevier.com/content/abstract/scopus_id/84920749572,"During the last decade, many interests were given to the speed control of induction motor based electrical vehicles (EVs). The calculated Pulse Width Modulation technique with Selective Harmonic Elimination and Voltage Control (SHE PWM) is an attractive alternative for speed control of an induction motor. However, its application is unfeasible in real time application, as in EVs, because the switching angles cannot be calculated and then generated online. To overcome this problem, this paper proposes a new online PWM algorithm based on the Artificial Neural Network (ANN) theory in combination with the SHE PWM. In this paper, the proposed ANNSHE PWM algorithm is first described and simulated. Then, an extensive angle error analysis is carried out in order to check the accuracy of this algorithm. Finally, an FPGA implementation of the proposed algorithm to generate the switching angles is presented in order to validate this algorithm on a real time application. The results obtained shows that the proposed ANNSHE PWM algorithm controls the fundamental voltage and eliminates efficiently the desired harmonics in real time and in the whole range of speed variation.",science
10.1016/j.future.2014.11.015,Journal,Future Generation Computer Systems,scopus,2015-01-01,sciencedirect,On-line failure prediction in safety-critical systems,https://api.elsevier.com/content/abstract/scopus_id/84917709364,"In safety-critical systems such as Air Traffic Control system, SCADA systems, Railways Control Systems, there has been a rapid transition from monolithic systems to highly modular ones, using off-the-shelf hardware and software applications possibly developed by different manufactures. This shift increased the probability that a fault occurring in an application propagates to others with the risk of a failure of the entire safety-critical system. This calls for new tools for the on-line detection of anomalous behaviors of the system, predicting thus a system failure before it happens, allowing the deployment of appropriate mitigation policies.
                  The paper proposes a novel architecture, namely CASPER, for online failure prediction that has the distinctive features to be (i) black-box: no knowledge of applications internals and logic of the system is required (ii) non-intrusive: no status information of the components is used such as CPU or memory usage; The architecture has been implemented to predict failures in a real Air Traffic Control System. CASPER exhibits high degree of accuracy in predicting failures with low false positive rate. The experimental validation shows how operators are provided with predictions issued a few hundred of seconds before the occurrence of the failure.",science
10.1016/j.asoc.2014.10.018,Journal,Applied Soft Computing Journal,scopus,2015-01-01,sciencedirect,Performance assessment of heat exchanger using intelligent decision making tools,https://api.elsevier.com/content/abstract/scopus_id/84912132496,"Process and manufacturing industries today are under pressure to deliver high quality outputs at lowest cost. The need for industry is therefore to implement cost savings measures immediately, in order to remain competitive. Organizations are making strenuous efforts to conserve energy and explore alternatives. This paper explores the development of an intelligent system to identify the degradation of heat exchanger system and to improve the energy performance through online monitoring system. The various stages adopted to achieve energy performance assessment are through experimentation, design of experiments and online monitoring system. Experiments are conducted as per full factorial design of experiments and the results are used to develop artificial neural network models. The predictive models are used to predict the overall heat transfer coefficient of clean/design heat exchanger. Fouled/real system value is computed with online measured data. Overall heat transfer coefficient of clean/design system is compared with the fouled/real system and reported. It is found that neural net work model trained with particle swarm optimization technique performs better comparable to other developed neural network models. The developed model is used to assess the performance of heat exchanger with the real/fouled system. The performance degradation is expressed using fouling factor, which is derived from the overall heat transfer coefficient of design system and real system. It supports the system to improve the performance by asset utilization, energy efficient and cost reduction in terms of production loss. This proposed online energy performance system is implemented into the real system and the adoptability is validated.",science
10.1016/j.asoc.2014.10.034,Journal,Applied Soft Computing Journal,scopus,2015-01-01,sciencedirect,A sensor-software based on a genetic algorithm-based neural fuzzy system for modeling and simulating a wastewater treatment process,https://api.elsevier.com/content/abstract/scopus_id/84911940897,"In this paper, a software sensor based on a genetic algorithm-based neural fuzzy system (GA-NFS) was proposed for real-time estimation of nutrient concentrations in a biological wastewater treatment process. In order to improve the network performance, self-adapted fuzzy c-means clustering algorithm and genetic algorithm were employed to extract and optimize the structure of the network. The GA-NFS was applied to a biological wastewater treatment process for nutrient removal. The simulative results indicate that the learning and generalization ability of the model performed well and also worked well for normal batch i.e., two data points. Real-time estimation of COD, NO3
                     − and PO4
                     3− concentration based on GA-NFS functioned effectively with the simple on-line information on the anoxic/oxic system.",science
10.1016/j.pnucene.2014.10.013,Journal,Progress in Nuclear Energy,scopus,2015-01-01,sciencedirect,On-line fault detection of a fuel rod temperature measurement sensor in a nuclear reactor core using ANNs,https://api.elsevier.com/content/abstract/scopus_id/84911865656,"In this paper a detailed method for fault detection of an in-core three wires Resistance Temperature Detectors (RTD) sensor is introduced. The method is mainly based on the dependence of the fuel rod temperature profile on control rods elevation and coolant flow rate in a given nuclear reactor. For the implementation, an artificial neural network (ANN) technique has been developed to model the dynamic behaviour of the considered temperature sensor. In order to have more refined model estimation, ANN has been combined with additional noise reduction algorithms. The effective denoising work was done via the discrete wavelet transform (DWT) to remove various kinds of artefacts such as inherent measurement noise. The principle of the adopted fault detection task is based on the calculation of the difference between the ANN model estimated temperature and the online being measured temperature and then compare the deviation with a certain detection threshold to decide the sensor fault. The efficiency of the method is evaluated first on a simulated case and then on the on-line measurements obtained from a real plant. Results confirm the capacity of the developed ANN-based model to estimate a fuel rod temperature with a reasonable accuracy.",science
10.1016/j.talanta.2014.08.010,Journal,Talanta,scopus,2015-01-01,sciencedirect,Application of neural networks with novel independent component analysis methodologies to a Prussian blue modified glassy carbon electrode array,https://api.elsevier.com/content/abstract/scopus_id/84906751295,"Sodium potassium absorption ratio (SPAR) is an important measure of agricultural water quality, wherein four exchangeable cations (K+, Na+, Ca2+ and Mg2+) should be simultaneously determined. An ISE-array is suitable for this application because its simplicity, rapid response characteristics and lower cost. However, cross-interferences caused by the poor selectivity of ISEs need to be overcome using multivariate chemometric methods. In this paper, a solid contact ISE array, based on a Prussian blue modified glassy carbon electrode (PB-GCE), was applied with a novel chemometric strategy. One of the most popular independent component analysis (ICA) methods, the fast fixed-point algorithm for ICA (fastICA), was implemented by the genetic algorithm (geneticICA) to avoid the local maxima problem commonly observed with fastICA. This geneticICA can be implemented as a data preprocessing method to improve the prediction accuracy of the Back-propagation neural network (BPNN). The ISE array system was validated using 20 real irrigation water samples from South Australia, and acceptable prediction accuracies were obtained.",science
10.1016/j.ymssp.2014.04.018,Journal,Mechanical Systems and Signal Processing,scopus,2015-01-01,sciencedirect,Traceability of Acoustic Emission measurements for a proposed calibration method - Classification of characteristics and identification using signal analysis,https://api.elsevier.com/content/abstract/scopus_id/84905824212,"When using Acoustic Emission (AE) technologies, tensile, compressive and shear stress/strain tests can provide a detector for material deformation and dislocations. In this paper improvements are made to standardise calibration techniques for AE against known metrics such as force. AE signatures were evaluated from various calibration energy sources based on the energy from the first harmonic (dominant energy band) [1,2]. The effects of AE against its calibration identity are investigated: where signals are correlated to the average energy and distance of the detected phenomena. In addition, extra tests are investigated in terms of the tensile tests and single grit tests characterising different materials. Necessary translations to the time–frequency domain were necessary when segregating salient features between different material properties. Continuing this work the obtained AE is summarised and evaluated by a Neural Network (NN) regression classification technique which identifies how far the malformation has progressed (in terms of energy/force) during material transformation. Both genetic-fuzzy clustering and tree rule based classifier techniques were used as the second and third classification techniques respectively to verify the NN output giving a weighted three classifier system. The work discussed in this paper looks at both distance and force relationships for various prolonged Acoustic Emission stresses. Later such analysis was realised with different classifier models and finally implemented into the Simulink simulations. Further investigations were made into classifier models for different material interactions in terms of force and distance which add further dimension to this work with different materials based simulation realisations.
                  Within the statistical analysis section there are two varying prolonged stress tests which together offer the mechanical calibration system (automated solenoid and pencil break calibration system). Taking such a mechanical system with the real-time simulations gives a fully automated accurate AE calibration system to force and distance measurement phenomena.",science
10.1016/j.chroma.2014.07.061,Journal,Journal of Chromatography A,scopus,2014-09-19,sciencedirect,Large pesticide multiresidue screening method by liquid chromatography-Orbitrap mass spectrometry in full scan mode applied to fruit and vegetables,https://api.elsevier.com/content/abstract/scopus_id/84906787496,"The present work is focused on evaluating the main operational parameters for multiresidue screening of an Orbitrap mass spectrometer for pesticide residue analysis in fruits and vegetables. Operational parameters such as resolution, software for the automatic detection, mass tolerance and retention time extraction window, along with the analytical performance, were evaluated in an updated UHPLC-Orbitrap-mass spectrometer working in full scan mode. The evaluation was performed using QuEChERS extracts of tomato, pepper, orange and green tea. The extracts were spiked with 170 selected pesticides at four concentration levels (10μg/kg, 50μg/kg, 100μg/kg and 500μg/kg). Extracts were diluted 5 fold before injection. Three different resolution settings (17,500, 35,000 and 70,000) were evaluated at various concentration levels. At 10μg/kg, using a resolution of 17,500 and 5ppm of mass tolerance, the detected pesticide rates were from 91% in tomato, to 83% in green tea. These percentages increased at higher resolution values. A resolution of 70,000 was adequate for such analysis even when a small percentage of false detect at low concentration was obtained. The rates of detected compounds increased and were from 98% in tomato to 88% in green tea. Mass tolerance of 5ppm was the most adequate for screening purposes. The observed false negative detects were mainly a consequence of a lack of compound sensitivity exacerbated by ion suppression effects in the experimental conditions applied. With reporting limits of 10μg/kg, reproducibility improved with resolution levels of 35,000 or higher. Linearity was investigated in the 2–100ng/mL (equivalent to 10–500μg/kg in the sample) range. Particularly good automatic screening effectiveness was obtained using the selected settings in the analysis of real samples where no false negatives detects and 5% of false positives detects were obtained.",science
10.1016/j.ins.2014.03.085,Journal,Information Sciences,scopus,2014-09-10,sciencedirect,Measuring the impact of MVC attack in large complex networks,https://api.elsevier.com/content/abstract/scopus_id/84901836846,"Measuring the impact of network attack is an important issue in network science. In this paper, we study the impact of maximal vertex coverage (MVC) attack in large complex networks, where the attacker aims at deleting as many edges of the network as possible by attacking a small fraction of nodes. First, we present two metrics to measure the impact of MVC attack. To compute these metrics, we propose an efficient randomized greedy algorithm with near-optimal performance guarantee. Second, we generalize the MVC attack into an uncertain setting, in which a node is deleted by the attacker with a prior probability. We refer to the MVC attack under such uncertain environment as the probabilistic MVC attack. Based on the probabilistic MVC attack, we propose two adaptive metrics, and then present an adaptive greedy algorithm for calculating such metrics accurately and efficiently. Finally, we conduct extensive experiments on 20 real datasets. The results show that P2P and co-authorship networks are extremely robust under the MVC attack while both the online social networks and the Email communication networks exhibit vulnerability under the MVC attack. In addition, the results demonstrate the efficiency and effectiveness of the proposed algorithms for computing the proposed metrics.",science
10.1016/j.jep.2014.03.049,Journal,Journal of Ethnopharmacology,scopus,2014-05-28,sciencedirect,Ocimum sanctum leaf extracts attenuate human monocytic (THP-1) cell activation,https://api.elsevier.com/content/abstract/scopus_id/84900418927,"Ethnopharmacological relevance
                  
                     Ocimum sanctum (OS), commonly known as Holy basil/Tulsi, has been traditionally used to treat cardiovascular diseases (CVD) and manage general cardiac health. The present study is designed to evaluate the antiinflammatory effect of O. sanctum and its phenolic compound and eugenol (EUG) in human monocytic (THP-1) cells and validate its traditional use for treating cardiovascular diseases.
               
                  Materials and methods
                  The phytochemical analysis of alcoholic and water extracts of OS-dry leaves (OSAE and OSWE) was done using LC–QTOF–MS. A phenolic compound, EUG was quantified in both OSAE and OSWE by an LC–MS technique using a mass hunter work station software quantitative analysis system. The effect of both OSAE, OSWE, pure compound EUG and positive control imatinib (IMT) was investigated in THP-1 cells by studying the following markers: lipopolysaccharide (LPS) induced tumor necrosis factor alpha (TNF-α) secretion by ELISA, gene expression of inflammatory markers (TNF-α, IL-6, MIP-1α and MCP-1) by real time PCR and translocation of nuclear factor kappa B (NF-κB) by confocol microscopy. Furthermore, the effect of the extracts, EUG and IMT, was studied on phorbol-12-myristate-13-acetate (PMA) induced monocyte to macrophage differentiation and gene expression of CD14, TLR2 and TLR4.
               
                  Results
                  The LC–MS analysis of OSAE and OSWE revealed the presence of several bioactive compounds including eugenol. Quantitative analysis revealed that OSAE and OSWE had EUG of 12ng/mgdwt and 19ng/mgdwt respectively. OSAE, OSWE (1mg dwt/mL) pure compound EUG (60µg/mL) and positive control IMT (20µg/mL) showed marked inhibition on LPS induced TNF-α secretion by THP-1 cells. At the selected concentration, the plant extracts, EUG and IMT inhibited gene expression of cytokines and chemokines (IL-6, TNF-α, MIP-1α, MCP-1) and translocation of NF-κB-p65 to the nuclei. In addition, they showed significant inhibition on PMA induced monocyte to macrophage differentiation and the gene expression of CD14, TLR2 and TLR4 markers.
               
                  Conclusion
                  The result of the present study validated traditional use of Ocimum sanctum for treating cardiovascular disease for the first time by testing antiinflammatory activity of Ocimum sanctum in acute inflammatory model, LPS induced THP-1 cells. The plant extracts showed significant antiinflammatory activity, however, further to be evaluated using chronic inflammatory animal models like diabetic or apolipoprotein E-deficient mice to make it evidence based medicine. The phenolic compound eugenol (60µg/mL) showed significant antiinflammatory activity. However the amount of eugenol present in 1mg of OSAE and OSWE (12ng/mg and 19ng/mg dwt respectively) used for cell based assays was very low. It suggests that several other metabolites along with eugenol are responsible for the efficacy of the extracts.",science
10.1016/j.ecoinf.2014.02.001,Journal,Ecological Informatics,scopus,2014-03-01,sciencedirect,A machine learning approach to investigate the reasons behind species extinction,https://api.elsevier.com/content/abstract/scopus_id/84896804521,"Species extinction is one of the most important phenomena in conservation biology. Many factors are involved in the disappearance of species, including stochastic population fluctuations, habitat change, resource depletion, and inbreeding. Due to the complexity of the interactions between these various factors and the lengthy time period required to make empirical observations, studying the phenomenon of species extinction can prove to be very difficult in nature. On the other hand, an investigation of the various features involved in species extinction using individual-based simulation modeling and machine learning techniques can be accomplished in a reasonably short period of time. Thus, the aim of this paper is to investigate multiple factors involved in species extinction using computer simulation modeling. We apply several machine learning techniques to the data generated by EcoSim, a predator–prey ecosystem simulation, in order to select the most prominent features involved in species extinction, along with extracting rules that outline conditions that have the potential to be used for predicting extinction. In particular, we used five feature selection methods resulting in the selection of 25 features followed by a reduction of these to 14 features using correlation analysis. Each of the remaining features was placed in one of three broad categories, viz., genetic, environmental, or demographic. The experimental results suggest that factors such as population fluctuation, reproductive age, and genetic distance are important in the occurrence of species extinction in EcoSim, similar to what is observed in nature. We argue that the study of the behavior of species through Individual-Based Modeling has the potential to give rise to new insights into the central factors involved in extinction for real ecosystems. This approach has the potential to help with the detection of early signals of species extinction that could in turn lead to conservation policies to help prevent extinction.",science
10.1016/j.bmc.2014.01.036,Journal,Bioorganic and Medicinal Chemistry,scopus,2014-03-01,sciencedirect,"Antiprotozoan lead discovery by aligning dry and wet screening: Prediction, synthesis, and biological assay of novel quinoxalinones",https://api.elsevier.com/content/abstract/scopus_id/84896737372,"Protozoan parasites have been one of the most significant public health problems for centuries and several human infections caused by them have massive global impact. Most of the current drugs used to treat these illnesses have been used for decades and have many limitations such as the emergence of drug resistance, severe side-effects, low-to-medium drug efficacy, administration routes, cost, etc. These drugs have been largely neglected as models for drug development because they are majorly used in countries with limited resources and as a consequence with scarce marketing possibilities. Nowadays, there is a pressing need to identify and develop new drug-based antiprotozoan therapies. In an effort to overcome this problem, the main purpose of this study is to develop a QSARs-based ensemble classifier for antiprotozoan drug-like entities from a heterogeneous compounds collection. Here, we use some of the TOMOCOMD-CARDD molecular descriptors and linear discriminant analysis (LDA) to derive individual linear classification functions in order to discriminate between antiprotozoan and non-antiprotozoan compounds as a way to enable the computational screening of virtual combinatorial datasets and/or drugs already approved. Firstly, we construct a wide-spectrum benchmark database comprising of 680 organic chemicals with great structural variability (254 of them antiprotozoan agents and 426 to drugs having other clinical uses). This series of compounds was processed by a k-means cluster analysis in order to design training and predicting sets. In total, seven discriminant functions were obtained, by using the whole set of atom-based linear indices. All the LDA-based QSAR models show accuracies above 85% in the training set and values of Matthews correlation coefficients (
                        C
                     ) vary from 0.70 to 0.86. The external validation set shows rather-good global classifications of around 80% (92.05% for best equation). Later, we developed a multi-agent QSAR classification system, in which the individual QSAR outputs are the inputs of the aforementioned fusion approach. Finally, the fusion model was used for the identification of a novel generation of lead-like antiprotozoan compounds by using ligand-based virtual screening of ‘available’ small molecules (with synthetic feasibility) in our ‘in-house’ library. A new molecular subsystem (quinoxalinones) was then theoretically selected as a promising lead series, and its derivatives subsequently synthesized, structurally characterized, and experimentally assayed by using in vitro screening that took into consideration a battery of five parasite-based assays. The chemicals 11(12) and 16 are the most active (hits) against apicomplexa (sporozoa) and mastigophora (flagellata) subphylum parasites, respectively. Both compounds depicted good activity in every protozoan in vitro panel and they did not show unspecific cytotoxicity on the host cells. The described technical framework seems to be a promising QSAR-classifier tool for the molecular discovery and development of novel classes of broad—antiprotozoan—spectrum drugs, which may meet the dual challenges posed by drug-resistant parasites and the rapid progression of protozoan illnesses.",science
10.1016/j.ijmedinf.2013.11.004,Journal,International Journal of Medical Informatics,scopus,2014-03-01,sciencedirect,Regenstrief Institute's Medical Gopher: A next-generation homegrown electronic medical record system,https://api.elsevier.com/content/abstract/scopus_id/84892488437,"Objective
                  Regenstrief Institute developed one of the seminal computerized order entry systems, the Medical Gopher, for implementation at Wishard Hospital nearly three decades ago. Wishard Hospital and Regenstrief remain committed to homegrown software development, and over the past 4 years we have fully rebuilt Gopher with an emphasis on usability, safety, leveraging open source technologies, and the advancement of biomedical informatics research. Our objective in this paper is to summarize the functionality of this new system and highlight its novel features.
               
                  Materials and methods
                  Applying a user-centered design process, the new Gopher was built upon a rich-internet application framework using an agile development process. The system incorporates order entry, clinical documentation, result viewing, decision support, and clinical workflow. We have customized its use for the outpatient, inpatient, and emergency department settings.
               
                  Results
                  The new Gopher is now in use by over 1100 users a day, including an average of 433 physicians caring for over 3600 patients daily. The system includes a wizard-like clinical workflow, dynamic multimedia alerts, and a familiar ‘e-commerce’-based interface for order entry. Clinical documentation is enhanced by real-time natural language processing and data review is supported by a rapid chart search feature.
               
                  Discussion
                  As one of the few remaining academically developed order entry systems, the Gopher has been designed both to improve patient care and to support next-generation informatics research. It has achieved rapid adoption within our health system and suggests continued viability for homegrown systems in settings of close collaboration between developers and providers.",science
10.1016/j.jep.2013.12.025,Journal,Journal of Ethnopharmacology,scopus,2014-02-12,sciencedirect,Effects of hydroxysafflor yellow A on the activity and mRNA expression of four CYP isozymes in rats,https://api.elsevier.com/content/abstract/scopus_id/84895072228,"Ethnopharmacological relevance
                  In traditional therapy with Chinese medicine, hydroxysafflor yellow A (HSYA), a main active component isolated from the dried flower of Carthamus tinctorius L., is the principal efficiency ingredient of Safflor Yellow Injection. Now HSYA has been demonstrated to have good pharmacological activities of antioxidation, myocardial and cerebral protective and neuroprotective effects. The purpose of this study was to find out whether HSYA influences the effect on rat cytochrome P450 (CYP) enzymes (CYP1A2, CYP2C11, CYP2D4 and CYP3A1) by using cocktail probe drugs in vivo; the influence on the levels of CYP mRNA was also studied.
               
                  Materials and methods
                  A cocktail solution at a dose of 5mL/kg, which contained phenacetin (20mg/kg), tolbutamide (5mg/kg), dextromethorphan (20mg/kg) and midazolam (10mg/kg), was given as oral administration to rats treated with short or long period of intravenous HSYA via the caudal vein. Blood samples were collected at a series of time-points and the concentrations of probe drugs in plasma were determined by HPLC–MS/MS. The corresponding pharmacokinetic parameters were calculated by the software of DAS 2.0. In addition, real-time RT-PCR was performed to determine the effect of HSYA on the mRNA expression of CYP1A2, CYP2C11, CYP2D4 and CYP3A1 in rat liver.
               
                  Results
                  HSYA had significant inhibition effects on CYP1A2 and CYP2C11 in rats as oriented from the pharmacokinetic profiles of the probe drugs. Furthermore, HSYA had no effects on rat CYP2D4. However, CYP3A1 enzyme activity was induced by HSYA. The mRNA expression results were in accordance with the pharmacokinetic results.
               
                  Conclusions
                  HSYA can either inhibit or induce activities of CYP1A2, CYP2C11 and CYP3A1. Therefore, co-administration of some CYP substrates with HSYA may need dose adjustment to avoid an undesirable herb–drug interaction.",science
10.1016/j.jep.2013.11.016,Journal,Journal of Ethnopharmacology,scopus,2014-01-10,sciencedirect,Effects of aescin on cytochrome P450 enzymes in rats,https://api.elsevier.com/content/abstract/scopus_id/84891491520,"Ethnopharmacological relevance
                  Aescin, the main active component found in extracts of horse chestnut (Aesculus hippocastanum) seed a traditional medicinal herb, is a mixture of triterpene saponins. It has been shown to be effective in inflammatory, chronic venous and edematous treatment conditions in vitro and in vivo, and is broadly used to treat chronic venous insufficiency. The purpose of this study was to find out whether aescin influences the effect on rat cytochrome P450 (CYP) enzymes (CYP1A2, CYP2C9, CYP2E1 and CYP3A4) by using cocktail probe drugs in vivo; the influence on the levels of CYP mRNA was also studied.
               
                  Materials and methods
                  A cocktail solution at a dose of 5mL/kg, which contained phenacetin (20mg/kg), tolbutamide (5mg/kg), chlorzoxazone (20mg/kg) and midazolam (10mg/kg), was given as oral administration to rats treated with a single dose or multiple doses of intravenous aescin via the caudal vein. Blood samples were collected at a series of time-points and the concentrations of probe drugs in plasma were determined by HPLC–MS/MS. The corresponding pharmacokinetic parameters were calculated by the software of DAS 2.0. In addition, real-time RT-PCR was performed to determine the effects of aescin on the mRNA expression of CYP1A2, CYP2C9, CYP2E1 and CYP3A4 in rat liver.
               
                  Results
                  Treatment with a single dose or multiple doses of aescin had inductive effects on rat CYP1A2, while CYP2C9 and CYP3A4 enzyme activities were inhibited. Moreover, aescin has no inductive or inhibitory effect on the activity of CYP2E1. The mRNA expression results were in accordance with the pharmacokinetic results.
               
                  Conclusions
                  Aescin can either inhibit or induce activities of CYP1A2, CYP2C9 and CYP3A4. Therefore, caution is needed when aescin is co-administration with some CYP1A2, CYP2C9 or CYP3A4 substrates in clinic, which may result in treatment failure and herb-drug interactions.",science
10.1016/j.procs.2014.11.102,Conference Proceeding,Procedia Computer Science,scopus,2014-01-01,sciencedirect,How gamification applies for educational purpose specially with college algebra,https://api.elsevier.com/content/abstract/scopus_id/84939230410,"Gaming environments have been used to teach mathematical topics such as addition and division in a fun manner
                        *
                     
                     
                        *
                        http://www.thefreedictionary.com/fun.
                     . However, when it comes to college level mathematical concepts such as the use of the quadratic formula, there are very few software that explain these concepts in a fun way. In this paper, we present a first step towards using video game elements and Artificial Intelligence Tutoring system techniques to teach mathematical concepts such as factoring and the quadratic formula. These concepts are explained in a way that helps learners make a connection between the mathematical concepts and their real life experience. These methods of learning are supported by several studies (Bonwell & Eison, 1991; Donovan & Bransford, 2004; Scarlatos, 2006). We use gamification techniques during the training and test phases to help students learn the mathematical concepts. We then compare the performance of students who used our system (MathDungeon) with that of students who used the most popular math tutoring programs used in US colleges: Assessment and Learning, K-12, Higher Education (ALEKS). The number of students who used MathDungeon and scored above the median score on the test of math performance was greater than the number of students who used ALEKS and scored higher than the median score.",science
10.1016/j.neunet.2014.09.002,Journal,Neural Networks,scopus,2014-01-01,sciencedirect,Person-by-person prediction of intuitive economic choice,https://api.elsevier.com/content/abstract/scopus_id/84929618289,"Decision making is an interdisciplinary field, which is explored with methods spanning from economic experiments to brain scanning. Its dominant paradigms such as utility theory, prospect theory, and the modern dual-process theories all resort to formal algebraic models or non-mathematical postulates, and remain purely phenomenological. An approach introduced by Grossberg deployed differential equations describing neural networks and bridged the gap between decision science and the psychology of cognitive–emotional interactions. However, the limits within which neural models can explain data from real people’s actions are virtually untested and remain unknown. Here we show that a model built around a recurrent gated dipole can successfully forecast individual economic choices in a complex laboratory experiment. Unlike classical statistical and econometric techniques or machine learning algorithms, our method calibrates the equations for each individual separately, and carries out prediction person-by-person. It predicted very well the behaviour of 15%–20% of the participants in the experiment–half of them extremely well–and was overall useful for two thirds of all 211 subjects. The model succeeded with people who were guided by gut feelings and failed with those who had sophisticated strategies. One hypothesis is that this neural network is the biological substrate of the cognitive system for primitive–intuitive thinking, and so we believe that we have a model of how people choose economic options by a simple form of intuition. We anticipate our study to be useful for further studies of human intuitive thinking as well as for analyses of economic systems populated by heterogeneous agents.",science
10.1016/j.rser.2014.07.009,Journal,Renewable and Sustainable Energy Reviews,scopus,2014-01-01,sciencedirect,A fuzzy cognitive maps decision support system for renewables local planning,https://api.elsevier.com/content/abstract/scopus_id/84905252053,"Initiatives as the Covenant of Mayors and the European Union (EU) binding targets of 20-20-20 are bringing Regional Planning of Renewable Energy Sources (RES) at the center of attention nowadays. This situation creates the need for simplified and straight forward decision support systems for local governance officers. This paper presents the design and implementation of a fuzzy cognitive maps (FCM) decision support toolkit (DST) for local RES planning. DST provides the decision maker with an overall qualitative evaluation of the examined investment promptly with minimum effort. All the related parameters (legal/regulative/administrative, financial, technical, social and environmental) that affect the evaluation of RES investment in a local community are investigated. A tool based on fuzzy cognitive maps is designed and implemented on a web platform. The DST has been tested and validated successfully through application in real investments οn Crete Island and comparison to the evaluation results reached by a panel of experts.",science
10.3168/jds.2013-7760,Journal,Journal of Dairy Science,scopus,2014-01-01,sciencedirect,Genotype-specific risk factors for Staphylococcus aureus in Swiss dairy herds with an elevated yield-corrected herd somatic cell count,https://api.elsevier.com/content/abstract/scopus_id/84904427880,"Bovine mastitis is a frequent problem in Swiss dairy herds. One of the main pathogens causing significant economic loss is Staphylococcus aureus. Various Staph. aureus genotypes with different biological properties have been described. Genotype B (GTB) of Staph. aureus was identified as the most contagious and one of the most prevalent strains in Switzerland. The aim of this study was to identify risk factors associated with the herd-level presence of Staph. aureus GTB and Staph. aureus non-GTB in Swiss dairy herds with an elevated yield-corrected herd somatic cell count (YCHSCC). One hundred dairy herds with a mean YCHSCC between 200,000 and 300,000cells/mL in 2010 were recruited and each farm was visited once during milking. A standardized protocol investigating demography, mastitis management, cow husbandry, milking system, and milking routine was completed during the visit. A bulk tank milk (BTM) sample was analyzed by real-time PCR for the presence of Staph. aureus GTB to classify the herds into 2 groups: Staph. aureus GTB-positive and Staph. aureus GTB-negative. Moreover, quarter milk samples were aseptically collected for bacteriological culture from cows with a somatic cell count ≥150,000cells/mL on the last test-day before the visit. The culture results allowed us to allocate the Staph. aureus GTB-negative farms to Staph. aureus non-GTB and Staph. aureus-free groups. Multivariable multinomial logistic regression models were built to identify risk factors associated with the herd-level presence of Staph. aureus GTB and Staph. aureus non-GTB. The prevalence of Staph. aureus GTB herds was 16% (n=16), whereas that of Staph. aureus non-GTB herds was 38% (n=38). Herds that sent lactating cows to seasonal communal pastures had significantly higher odds of being infected with Staph. aureus GTB (odds ratio: 10.2, 95% CI: 1.9–56.6), compared with herds without communal pasturing. Herds that purchased heifers had significantly higher odds of being infected with Staph. aureus GTB (rather than Staph. aureus non-GTB) compared with herds without purchase of heifers. Furthermore, herds that did not use udder ointment as supportive therapy for acute mastitis had significantly higher odds of being infected with Staph. aureus GTB (odds ratio: 8.5, 95% CI: 1.6–58.4) or Staph. aureus non-GTB (odds ratio: 6.1, 95% CI: 1.3–27.8) than herds that used udder ointment occasionally or regularly. Herds in which the milker performed unrelated activities during milking had significantly higher odds of being infected with Staph. aureus GTB (rather than Staph. aureus non-GTB) compared with herds in which the milker did not perform unrelated activities at milking. Awareness of 4 potential risk factors identified in this study guides implementation of intervention strategies to improve udder health in both Staph. aureus GTB and Staph. aureus non-GTB herds.",science
10.1016/j.artmed.2014.01.002,Journal,Artificial Intelligence in Medicine,scopus,2014-01-01,sciencedirect,Twitter mining for fine-grained syndromic surveillance,https://api.elsevier.com/content/abstract/scopus_id/84904296459,"Background
                  Digital traces left on the Internet by web users, if properly aggregated and analyzed, can represent a huge information dataset able to inform syndromic surveillance systems in real time with data collected directly from individuals. Since people use everyday language rather than medical jargon (e.g. runny nose vs. respiratory distress), knowledge of patients’ terminology is essential for the mining of health related conversations on social networks.
               
                  Objectives
                  In this paper we present a methodology for early detection and analysis of epidemics based on mining Twitter messages. In order to reliably trace messages of patients that actually complain of a disease, first, we learn a model of naïve medical language, second, we adopt a symptom-driven, rather than disease-driven, keyword analysis. This approach represents a major innovation compared to previous published work in the field.
               
                  Method
                  We first developed an algorithm to automatically learn a variety of expressions that people use to describe their health conditions, thus improving our ability to detect health-related “concepts” expressed in non-medical terms and, in the end, producing a larger body of evidence. We then implemented a Twitter monitoring instrument to finely analyze the presence and combinations of symptoms in tweets.
               
                  Results
                  We first evaluate the algorithm's performance on an available dataset of diverse medical condition synonyms, then, we assess its utility in a case study of five common syndromes for surveillance purposes. We show that, by exploiting physicians’ knowledge on symptoms positively or negatively related to a given disease, as well as the correspondence between patients’ “naïve” terminology and medical jargon, not only can we analyze large volumes of Twitter messages related to that disease, but we can also mine micro-blogs with complex queries, performing fine-grained tweets classification (e.g. those reporting influenza-like illness (ILI) symptoms vs. common cold or allergy).
               
                  Conclusions
                  Our approach yields a very high level of correlation with flu trends derived from traditional surveillance systems. Compared with Google Flu, another popular tool based on query search volumes, our method is more flexible and less sensitive to changes in web search behaviors.",science
10.1016/j.artmed.2014.05.002,Journal,Artificial Intelligence in Medicine,scopus,2014-01-01,sciencedirect,Advanced portable remote monitoring system for the regulation of treadmill running exercises,https://api.elsevier.com/content/abstract/scopus_id/84902537151,"Objective
                  This study aims to develop an advanced portable remote monitoring system to supervise high intensity treadmill exercises.
               
                  Materials and methods
                  The supervisory level of the developed hierarchical system is implemented on a portable monitoring device (iPhone/iPad) as a client application, while the real-time control of treadmill exercises is accomplished by using an on-line adaptive neural network control scheme in a local computer system. During training or rehabilitation exercises, the intensity (measured by heart rate) is regulated by simultaneously manipulating both treadmill speed and gradient. In order to achieve adaptive tracking performance, a neural network controller has been designed and implemented.
               
                  Results
                  Six real-time experiments have been conducted to test the performance of the developed monitoring system. Experimental results obtained in real-time with heart-rate set-point varying from 145bpm to 180bmp, demonstrate that the proposed system can quickly and accurately regulate exercise intensity of treadmill running exercises with desired performance (no overshoot, settling time T
                     
                        s
                     
                     ≤100s). Subjects aged from 29 to 38 years old participated in different set-point experiments to confirm the system's adaptability to inter- and intra-model uncertainty. The desired system performance under external disturbances has also been confirmed in a final real-time experiment demonstrating a user carrying the 10kg bag then removing it during the exercise.
               
                  Conclusion
                  In contrast with conventional control approaches, the proposed adaptive controller achieves better heart rate tracking performance under inter- and intra-model uncertainty and external disturbances. The developed system can automatically adapt to various individual exercisers and a range of exercise intensity.",science
10.1016/j.neubiorev.2013.12.003,Journal,Neuroscience and Biobehavioral Reviews,scopus,2014-01-01,sciencedirect,A model of the temporal dynamics of multisensory enhancement,https://api.elsevier.com/content/abstract/scopus_id/84899490661,"The senses transduce different forms of environmental energy, and the brain synthesizes information across them to enhance responses to salient biological events. We hypothesize that the potency of multisensory integration is attributable to the convergence of independent and temporally aligned signals derived from cross-modal stimulus configurations onto multisensory neurons. The temporal profile of multisensory integration in neurons of the deep superior colliculus (SC) is consistent with this hypothesis. The responses of these neurons to visual, auditory, and combinations of visual–auditory stimuli reveal that multisensory integration takes place in real-time; that is, the input signals are integrated as soon as they arrive at the target neuron. Interactions between cross-modal signals may appear to reflect linear or nonlinear computations on a moment-by-moment basis, the aggregate of which determines the net product of multisensory integration. Modeling observations presented here suggest that the early nonlinear components of the temporal profile of multisensory integration can be explained with a simple spiking neuron model, and do not require more sophisticated assumptions about the underlying biology. A transition from nonlinear “super-additive” computation to linear, additive computation can be accomplished via scaled inhibition. The findings provide a set of design constraints for artificial implementations seeking to exploit the basic principles and potency of biological multisensory integration in contexts of sensory substitution or augmentation.",science
10.1016/j.jvir.2013.12.561,Journal,Journal of Vascular and Interventional Radiology,scopus,2014-01-01,sciencedirect,Endovascular abdominal aortic aneurysm repair using transvenous intravascular us catheter guidance in patients with chronic renal failure,https://api.elsevier.com/content/abstract/scopus_id/84899113639,"Purpose
                  To describe the transvenous application of intracardiac echocardiography (ICE) for guidance during endovascular aortic repair (EVAR).
               
                  Materials and Methods
                  Eight patients with an infrarenal abdominal aortic aneurysm (AAA) and chronic renal failure were determined suitable for EVAR. The procedure was performed by deploying the transcaval and transiliac vein guidance of an ICE catheter to reduce the dosage of iodinated contrast medium. Multiple guidance parameters were assessed. The present study describes the EVAR procedure and postprocedure transabdominal ultrasound (US) follow-up results at 3–4 months.
               
                  Results
                  The eight procedures were completed by using transvenous ICE guidance. No contrast medium was used in five patients, and 3–20 mL of isoosmolar contrast medium was administered in the other three. No endoleaks were detected by ICE immediately after stent deployment. One patient who had a single functioning kidney developed renal failure that was attributed to manipulation-related cholesterol embolization. That patient became dependent on dialysis and died 3.5 months after the procedure. No endoleaks were detected at 3–4-month US follow-up in the other seven patients.
               
                  Conclusions
                  Transvenous ICE guidance is a promising method to reduce the dosage of iodinated contrast medium in patients with renal dysfunction undergoing EVAR. A prospective trial comparing this modality versus digital subtraction angiography guidance with iodinated contrast medium in terms of safety, accuracy, and long-term efficacy is recommended.",science
10.1016/j.talanta.2013.11.090,Journal,Talanta,scopus,2014-01-01,sciencedirect,Ultrasensitive electrochemical detection of cancer associated biomarker HER3 based on anti-HER3 biosensor,https://api.elsevier.com/content/abstract/scopus_id/84891686219,"The development of a new impedimetric biosensor for the detection of HER3, based on self-assembled monolayers (SAMs) of 4-aminothiophenol on gold electrodes, is reported. Anti-HER3 was used as a biorecognition element for the first time in an impedimetric biosensor. Cyclic voltammetry (CV) and electrochemical impedance spectroscopy (EIS) techniques were applied to characterize the immobilization process and to detect HER3. To provide the best biosensor response all experimental parameters were optimized. In addition, Kramers–Kronigs transform was also performed on the immobilization and measurement processes successfully. The biosensor had a linear detection range of 0.4–2.4pg/mL. The chrono-impedance technique to real time monitor the interaction between HER3 and anti-HER3 is also implemented. The biosensor has exhibited good repeatability and reproducibility. To demonstrate the feasibility of the biosensor in practical analysis, the artificial serum samples were experienced.",science
10.1016/j.jbtep.2013.09.002,Journal,Journal of Behavior Therapy and Experimental Psychiatry,scopus,2014-01-01,sciencedirect,A virtual reality-integrated program for improving social skills in patients with schizophrenia: A pilot study,https://api.elsevier.com/content/abstract/scopus_id/84884755111,"Background and objectives
                  Social skills training (SST) intervention has shown its efficacy to improve social dysfunction in patients with psychosis; however the implementation of new skills into patients' everyday functioning is difficult to achieve. In this study, we report results from the application of a virtual reality (VR) integrated program as an adjunct technique to a brief social skills intervention for patients with schizophrenia. It was predicted that the intervention would improve social cognition and performance of patients as well as generalisation of the learned responses into patient's daily life.
               
                  Methods
                  Twelve patients with schizophrenia or schizoaffective disorder completed the study. They attended sixteen individual one-hour sessions, and outcome assessments were conducted at pre-treatment, post-treatment and four-month follow-up.
               
                  Results
                  The results of a series of repeated measures ANOVA revealed significant improvement in negative symptoms, psychopathology, social anxiety and discomfort, avoidance and social functioning. Objective scores obtained through the use of the VR program showed a pattern of learning in emotion perception, assertive behaviours and time spent in a conversation. Most of these gains were maintained at four-month follow-up.
               
                  Limitations
                  The reported results are based on a small, uncontrolled pilot study. Although there was an independent rater for the self-reported and informant questionnaires, assessments were not blinded.
               
                  Conclusions
                  The results showed that the intervention may be effective for improving social dysfunction. The use of the VR program contributed to the generalisation of new skills into the patient's everyday functioning.",science
10.1016/j.ultras.2013.07.018,Journal,Ultrasonics,scopus,2014-01-01,sciencedirect,Ultrasonic sensor based defect detection and characterisation of ceramics,https://api.elsevier.com/content/abstract/scopus_id/84884211045,"Ceramic tiles, used in body armour systems, are currently inspected visually offline using an X-ray technique that is both time consuming and very expensive. The aim of this research is to develop a methodology to detect, locate and classify various manufacturing defects in Reaction Sintered Silicon Carbide (RSSC) ceramic tiles, using an ultrasonic sensing technique. Defects such as free silicon, un-sintered silicon carbide material and conventional porosity are often difficult to detect using conventional X-radiography. An alternative inspection system was developed to detect defects in ceramic components using an Artificial Neural Network (ANN) based signal processing technique. The inspection methodology proposed focuses on pre-processing of signals, de-noising, wavelet decomposition, feature extraction and post-processing of the signals for classification purposes. This research contributes to developing an on-line inspection system that would be far more cost effective than present methods and, moreover, assist manufacturers in checking the location of high density areas, defects and enable real time quality control, including the implementation of accept/reject criteria.",science
10.1016/j.chemolab.2013.08.009,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2013-10-15,sciencedirect,Genetic algorithm search space splicing particle swarm optimization as general-purpose optimizer,https://api.elsevier.com/content/abstract/scopus_id/84884383367,"A heuristic search space splicing scheme has been implemented to aid the convergence of the particle swarm optimization (PSO) algorithm to the global optimum. Genetic algorithm (GA) was used to splice the search space into smaller subspaces, thereby reducing the number of local minima. PSO algorithm was subsequently used to locate the global optima in the subspaces. A set of 11 well-known test functions had been used for the assessment of this novel GA search space splicing PSO (GA-SSS-PSO) architecture. Of the methods tested in this study, the GA-SSS-PSO approach was the only one that could optimize all functions to a desirable level. To demonstrate the algorithm's applicability, three optimization tasks of different categories commonly faced in the field of chemometrics were subjected to optimization by GA-SSS-PSO and results indicated that the novel hybrid algorithm provided robust performance for both theoretical and real life problems and may be suited as general-purpose optimizer for medium-sized optimization tasks.",science
10.1016/j.bica.2013.07.009,Journal,Biologically Inspired Cognitive Architectures,scopus,2013-10-01,sciencedirect,Emotional biologically inspired cognitive architecture,https://api.elsevier.com/content/abstract/scopus_id/84883206490,"Human-like artificial emotional intelligence is vital for integration of future robots into the human society. This work introduces a general framework for representation and processing of emotional contents in a cognitive architecture, called “emotional biologically inspired cognitive architecture” (eBICA). Unlike in previous attempts, in this framework emotional elements are added virtually to all cognitive representations and processes by modifying the main building blocks of the prototype architectures. The key elements are appraisals associated as attributes with schemas and mental states, moral schemas that control patterns of appraisals and represent social emotions, and semantic spaces that give values to appraisals. Proposed principles are tested in an experiment involving human subjects and virtual agents, based on a simple paradigm in imaginary virtual world. It is shown that with moral schemas, but probably not without them, eBICA can account for human behavior in the selected paradigm. The model sheds light on clustering of social emotions and allows for their elegant mathematical description. The new framework will be suitable for implementation of believable emotional intelligence in artifacts, necessary for emotionally informed behavior, collaboration of virtual partners with humans, and self-regulated learning of virtual agents.",science
10.1016/j.asoc.2013.07.009,Journal,Applied Soft Computing Journal,scopus,2013-08-27,sciencedirect,Synergizing fitness learning with proximity-based food source selection in artificial bee colony algorithm for numerical optimization,https://api.elsevier.com/content/abstract/scopus_id/84886723307,"Evolutionary computation (EC) paradigm has undergone extensions in the recent years diverging from the natural process of genetic evolution to the simulation of natural life processes exhibited by the living organisms. Bee colonies exemplify a high level of intrinsic interdependence and co-ordination among its members, and algorithms inspired from the bee colonies have gained recent prominence in the field of swarm based metaheuristics. The artificial bee colony (ABC) algorithm was recently developed, by simulating the minimalistic foraging model of honeybees in search of food sources, for solving real-parameter, non-convex, and non-smooth optimization problems. The single parameter perturbation in classical ABC resulted in fairly commendable performance for simple problems without epistasis of variables (separable). However, it suffered from narrow search zone and slow convergence which eventually led to poor exploitation tendency. Even with the increase in dimensionality, a significant deterioration was observed in the ability of ABC to locate the optimum in a huge search volume. Some of the probable shortcomings in the basic ABC approach, as observed, are the single parameter perturbation instead of a multiple one, ignoring the fitness to reward ratio while selecting food sites, and most importantly the absence of environmental factors in the algorithm design. Research has shown that spatial environmental factors play a crucial role in insect locomotion and foragers seem to learn the direction to be undertaken based on the relative analysis of its proximal surroundings. Most importantly, the mapping of the forager locomotion from three dimensional search spaces to a multidimensional solution space calls forth the implementation of multiple modification schemes. Based on the fundamental observation pertaining to the dynamics of ABC, this article proposes an improved variant of ABC aimed at improving the optimizing ability of the algorithm over an extended set of problems. The hybridization of the proposed fitness learning mechanism with a weighted selection scheme and proximity based stimuli helps to achieve a fine blending of explorative and exploitative behaviour by enhancing both local and global searching ability of the algorithm. This enhances the ability of the swarm agents to detect optimal regions in the unexplored fitness basins. With respect to its immediate surroundings, a proximity based component is added to the normal positional modification of the onlookers and is enacted through an improved probability selection scheme that takes the T/E (total reward to distance) ratio metric into account. The biologically-motivated, hybridized variant of ABC achieves a statistically superior performance on majority of the tested benchmark instances, as compared to some of the most prominent state-of-the-art algorithms, as is demonstrated through a detailed experimental evaluation and verified statistically.",science
10.1016/j.cpc.2013.02.026,Journal,Computer Physics Communications,scopus,2013-07-01,sciencedirect,Hyper-Fractal Analysis v04: Implementation of a fuzzy box-counting algorithm for image analysis of artistic works,https://api.elsevier.com/content/abstract/scopus_id/84875187796,"This work presents a new version of a Visual Basic 6.0 application for estimating the fractal dimension of images and 4D objects (Grossu et al. 2013 [1]). Following our attempt of investigating artistic works by fractal analysis of craquelure, we encountered important difficulties in filtering real information from noise. In this context, trying to avoid a sharp delimitation of “black” and “white” pixels, we implemented a fuzzy box-counting algorithm.
               
                  
                     New version program summary
                  
                  
                     Program title: Hyper-Fractal Analysis v04
                  
                     Catalogue identifier: AEEG_v4_0
                  
                     Program summary URL:
                     http://cpc.cs.qub.ac.uk/summaries/AEEG_v4_0.html
                  
                  
                     Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland
                  
                     Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html
                  
                  
                     No. of lines in distributed program, including test data, etc.: 745999
                  
                     No. of bytes in distributed program, including test data, etc.: 12844235
                  
                     Distribution format: tar.gz
                  
                     Programming language: MS Visual Basic 6.0
                  
                     Computer: PC
                  
                     Operating system: MS Windows 98 or later
                  
                     RAM: 100M
                  
                     Classification: 14
                  
                     Catalogue identifier of previous version: AEEG_v3_0
                  
                     Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 1344
                  
                     Does the new version supercede the previous version?: Yes
                  
                     Nature of problem: estimating the fractal dimension of images
                  
                     Solution method: fuzzy box-counting algorithm
                  
                     Reasons for new version:
                  
                  Following the idea [2, 3] of investigating old paintings by fractal analysis of craquelure [4, 5], we faced with significant difficulties involved by the band-pass filter limitations. Trying to find a smoother way of separating information from noise, we implemented a fuzzy box-counting algorithm [6–8].
                  The fractal dimension [9] can be defined as: 
                        
                           (1)
                           
                              
                                 
                                    d
                                 
                                 
                                    f
                                 
                              
                              =
                              
                                 
                                    lim
                                 
                                 
                                    r
                                    →
                                    0
                                 
                              
                              
                                 
                                    log
                                    N
                                    
                                       (
                                       r
                                       )
                                    
                                 
                                 
                                    log
                                    
                                       (
                                       1
                                       /
                                       r
                                       )
                                    
                                 
                              
                           
                        
                      where 
                        N
                        
                           (
                           r
                           )
                        
                      represents the number of boxes, with length 
                        r
                     , needed to cover the object. The main change considered is related to the significance of 
                        N
                        
                           (
                           r
                           )
                        
                     . As opposed to the classical approach, where each box contributes to 
                        N
                        
                           (
                           r
                           )
                        
                      with either 1 (black), or 0 (white), in the fuzzy version (Fig. 1) each box contributes to 
                        N
                        
                           (
                           r
                           )
                        
                      with a rational number 
                        p
                        =
                        1
                        −
                        color code
                        /
                        
                           (
                           total number of colors
                           −
                           1
                           )
                        
                     .
                  
                     Summary of revisions:
                     
                        
                           1.
                           Implementation of a fuzzy box-counting algorithm for estimating the fractal dimension of images
                        
                        
                           2.
                           Optimization of the file open procedure.
                        
                     
                     
                        
                           Fig. 1
                           
                              Hyper-Fractal Analysis v04 example of use. Fuzzy fractal dimension of painting craquelure.
                           
                           
                        
                     
                  
                  
                     Running time: In a first approximation, the algorithm is linear [2].
                  
                     References: 
                        
                           [1]
                           I.V. Grossu, I. Grossu, D. Felea, C. Besliu, Al. Jipa, T. Esanu, C.C. Bordeianu, E. Stan, Computer Physics Communications, 184 (2013) 1344–1345.
                        
                        
                           [2]
                           I.V. Grossu, C. Besliu, M.V. Rusu, Al. Jipa, C. C. Bordeianu, D. Felea, Computer Physics Communications, 180 (2009) 1999–2001.
                        
                        
                           [3]
                           I.V. Grossu, M.V. Rusu, A. Teodosiu, Fractals in a particular process. Fractals in the investigation of artistic works, in National Conference of Physics, Romania, Constanta, 21–23 September 2000.
                        
                        
                           [4]
                           A. Teodosiu, Din universul ascuns al operei de arta, Allfa, Romania (2001) pp. 113–122.
                        
                        
                           [5]
                           S Bucklow, Consensus in the Classification of Craquelure, Hamilton Kerr Institute Bulletin, number 3, ed. A Massing, Hamilton Kerr Institute, University of Cambridge 2000: pp. 61–73.
                        
                        
                           [6]
                           X. Zhuang, Q. Meng, Artificial Intelligence in Medicine (2004) 32, 29–36.
                        
                        
                           [7]
                           D. Dumitrescu, Hariton Costin, Retele Neuronale. Teorie si aplicatii, Teora, Bucuresti, 1996, pp. 228–262.
                        
                        
                           [8]
                           D. Dumitrescu, Fuzzy Measures and the entropy of fuzzy partitions, J. Math, Anal. Appl., 176 (1993b) 359–373.
                        
                        
                           [9]
                           R.H. Landau, M.J. Paez and C.C. Bordeianu, Computational physics: Problem solving with computers, Wiley-VCH-Verlag, Weinheim, 2007, pp. 293–306.",science
10.1016/j.eswa.2012.12.036,Journal,Expert Systems with Applications,scopus,2013-06-15,sciencedirect,Formalizing dialectical explanation support for argument-based reasoning in knowledge-based systems,https://api.elsevier.com/content/abstract/scopus_id/84874646994,"The concept of explanation has received attention from different areas in Computer Science, particularly in the knowledge-based systems and expert systems communities. At the same time, argumentation has evolved as a new paradigm for conceptualizing commonsense reasoning, resulting in the formalization of different argumentation frameworks and the development of several real-world argument-based applications. Although the notions of explanation and argument for a claim share many common elements in knowledge-based systems their interrelationships have not yet been formally studied in the context of the current argumentation research in Artificial Intelligence. This article explores these ideas by providing a new perspective on how to formalize dialectical explanation support for argument-based reasoning. To do this, we propose a formalization of explanations for abstract argumentation frameworks with dialectical constraints where different emerging properties are studied and analyzed. As a concrete example of the formalism introduced we show how it can be fleshed out in an implemented rule-based argumentation system.",science
10.1016/j.aap.2013.04.026,Journal,Accident Analysis and Prevention,scopus,2013-06-03,sciencedirect,Characterization and simulation of optical sensors,https://api.elsevier.com/content/abstract/scopus_id/84887043277,"Numerical simulation is gradually becoming an advantage in active safety. This is why the development of realistic numerical models enabling to substitute real truth by simulated truth is primordial. In order to provide an accurate and cost effective solution to simulate real optical sensor behavior, the software Pro-SiVIC™ has been developed. Simulations with the software Pro-SiVIC™ can replace real tests with optical sensors and hence allow substantial cost and time savings during the development of solutions for driver assistance systems. An optical platform has been developed by IFSTTAR (French Institute of Science and Technology for Transport, Development and Networks) to characterize and validate any existing camera, in order to measure their characteristics as distortion, vignetting, focal length, etc. By comparing real and simulated sensors with this platform, this paper demonstrates that Pro-SiVIC™ accurately reproduces real sensors’ behavior.",science
10.1016/j.nancom.2013.02.002,Journal,Nano Communication Networks,scopus,2013-06-01,sciencedirect,A review of experimental opportunities for molecular communication,https://api.elsevier.com/content/abstract/scopus_id/84878145161,"The growth of nanotechnology has led to miniature devices that are able to perform limited functionalities in hard to access areas. Example nanodevice applications in the healthcare domain include early detection of harmful diseases. The current field of molecular communication is aiming to increase the functionalities of nanodevices, by enabling communication to be performed. Since its first introduction, communication researchers have been proposing various solutions that could possibly realize molecular communications (e.g., molecular diffusion and bacteria nanonetworks). These solutions have largely been limited to theoretical simulation modeling. However, to fully realize a future for real deployments and developments of molecular communication, a strong synergy will be required with molecular biologists. The aim of this paper is to create this link, and at the same time provide guidance for current molecular communication researchers of possible real developments of molecular communication based on the current state-of-the-art experimental work. In particular, we present a review on bacteria communication and membrane nanotubes, as well as neuronal networks. We also discuss possible applications in the future focusing in particular on Body Area NanoNetworks (BAN2).",science
10.1016/j.cmpb.2012.09.011,Journal,Computer Methods and Programs in Biomedicine,scopus,2013-05-01,sciencedirect,Algorithms for modeling structural changes in human chromosomes,https://api.elsevier.com/content/abstract/scopus_id/84875893848,"Human cytogenetics is the study of chromosomes (typically at mitotic metaphase). The study of chromosomes has recently become integrated with molecular biology and genomics. Thus, it is an important part of genetics education. However, it is time consuming to train students and clinical technologists to recognize patterns of G-banded human chromosomes because of the dynamic nature of G-band resolutions in different metaphase spreads. Moreover, there are limited resources to obtain the images of abnormal chromosomes. We present in this paper an advanced version of computer based interactive tutorial program capable of simulating chromosome abnormalities, altering chromosome shapes, and manipulating G-band resolutions for human cytogenetic seduction. By simulating chromosomes using digital image processing and pattern recognition, the versatile software, together with various strategies such as website links and dialogs, will provide students with a virtual learning environment for self-practicing and testing, thus transforming the traditionally dry and ineffective approach into an exciting and efficient learning process.",science
10.1016/j.robot.2012.12.005,Journal,Robotics and Autonomous Systems,scopus,2013-05-01,sciencedirect,A survey of bio-inspired robotics hands implementation: New directions in dexterous manipulation,https://api.elsevier.com/content/abstract/scopus_id/84875695547,"Recently, significant advances have been made in ROBOTICS, ARTIFICIAL INTELLIGENCE and other COGNITIVE related fields, allowing to make much sophisticated biomimetic robotics systems. In addition, enormous number of robots have been designed and assembled, explicitly realize biological oriented behaviors. Towards much skill behaviors and adequate grasping abilities (i.e. ARTICULATION and DEXTEROUS MANIPULATION), a new phase of dexterous hands have been developed recently with biomimetically oriented and bio-inspired functionalities. In this respect, this manuscript brings a detailed survey of biomimetic based dexterous robotics multi-fingered hands. The aim of this survey, is to find out the state of the art on dexterous robotics end-effectors, known in literature as (ROBOTIC HANDS) or (DEXTEROUS MULTI-FINGERED) robot hands. Hence, this review finds such biomimetic approaches using a framework that permits for a common description of biological and technical based hand manipulation behavior. In particular, the manuscript focuses on a number of developments that have been taking place over the past two decades, and some recent developments related to this biomimetic field of research. In conclusions, the study found that, there are rich research efforts in terms of KINEMATICS, DYNAMICS, MODELING and CONTROL methodologies. The survey is also indicating that, the topic of biomimetic inspired robotics systems make significant contributions to robotics hand design, in four main directions for future research. First, they provide a genuine world test of models of biologically inspired hand designs and dexterous manipulation behaviors. Second, they provide novel manipulation articulations and mechanisms available for industrial and domestic uses, most notably in the field of human like hand design and real world applications. Third, this survey has also indicated that, there are quite large number of attempts to acquire biologically inspired hands. These attempts were almost successful, where they exposed more novel ideas for further developments. Such inspirations were directed towards a number of topics related (HAND MECHANICS AND DESIGN), (HAND TACTILE SENSING), (HAND FORCE SENSING), (HAND SOFT ACTUATION) and (HAND CONFIGURATION AND TOPOLOGY). FOURTH, in terms of employing AI related sciences and cognitive thinking, it was also found that, rare and exceptional research attempts were directed towards the employment of biologically inspired thinking, i.e. (AI, BRAIN AND COGNITIVE SCIENCES) for hand upper control and towards much sophisticated dexterous movements. Throughout the study, it has been found there are number of efforts in terms of mechanics and hand designs, tactical sensing, however, for hand soft actuation, it seems this area of research is still far away from having a realistic muscular type fingers and hand movements.",science
10.1016/j.dss.2012.05.047,Journal,Decision Support Systems,scopus,2013-04-01,sciencedirect,Agent-enabled service-oriented decision support systems,https://api.elsevier.com/content/abstract/scopus_id/84877755057,"The design of Decision Support Systems have recently emphasized web enablement as the next step in design improvements for this class of applications. We argue that these approaches fail to address the key notion of adaptability in the support for decision makers. Instead of focusing exclusively on automation in decision making, we believe it is also necessary to pay attention to the interplay between decision makers and organizational processes. The service oriented view of organizations recognizes the need to accommodate the changing reality of organizational dynamics. For example, the service science approach focuses on interactions between service providers, their clients, and consumers as important interacting components of a service system. Current approaches to DSS design are constrained in terms of their ability to adapt to changes in user requirements and to provide support for the evolution of systems. This situation worsens when resources are distributed at different locations across organizations, decision making processes are required to be integrated at different points in time, and when collaboration is needed among decision makers. However, this typically characterizes the needs of collaborative decision making in networked organizations as exemplified by systems used for supply chain management. To address these problems we leverage the power of services for designing a framework that explicitly recognizes the need for design based on service delivery. We develop an agent-enabled service-oriented architecture to realize the proposed framework with service and agent paradigms. The architecture is refined and validated with an implementation in the supply chain context.",science
10.1016/j.fss.2012.03.011,Journal,Fuzzy Sets and Systems,scopus,2013-03-01,sciencedirect,Fuzzy logic and semiotic methods in modeling of medical concepts,https://api.elsevier.com/content/abstract/scopus_id/84872126207,"The field of medicine is a quickly growing area of application for computer-based systems. However, the use of computerized methods in this knowledge-intensive and expert-based discipline brings multiple challenges. The major problem is the modeling, representing, and interpreting of diverse medical concepts. For example, some symptoms and their etiologies are described in terms of molecular biology and genetics, physiological processes are defined using models from chemistry and physics; yet mental disorders are defined in more subjective terms of feelings, behaviours, habits, and life events. Thus, the representation of medical concepts must be sufficiently expressive to model concepts which are inherently complex, context-dependent, evolving, and often imprecise. Furthermore, the representation must be formal or, at least, sufficiently rigorous in order to be processed by computers and at the same time, the representation must be human-readable in order to be validated by humans. In this paper, we describe the modeling process of medical concepts as a mapping from the real-world medical concepts into their computational models, and further into their physical implementation. First, we define the notion of a concept as a fundamental unit of knowledge and specify the fundamental principles of the computational representation of a concept. Second, we describe the characteristics of medical concepts, specifically their historical and cultural changeability, their social and cultural ambiguity, and their varied levels of precision. Third, we present a meta-modeling framework for computational representation of medical concepts. Our framework is based on fuzzy logic and semiotic methods which allow us to explicitly model two important characteristics of medical concepts: imprecision and context-dependency. We present the framework using an example of a mental disorder, specifically, the concept of clinical depression. To exemplify the changeable and evolutionary character of medical concepts, we discuss the development of the diagnostic criteria for depression. Finally, we use the example of the assessment of depression to describe the computational representation for polythetic and multi-dimensional concepts and for categorical and non-categorical concepts. We demonstrate how the proposed modeling framework utilizes (1) a fuzzy-logic approach to represent the non-categorical (continuous) nature of the symptoms and (2) a semiotic approach to represent the polythetic (contextual interpretation) and dimensional nature of the symptoms.",science
10.1016/j.snb.2012.10.100,Journal,"Sensors and Actuators, B: Chemical",scopus,2013-01-07,sciencedirect,Development of a sol-gel photonic sensor platform for the detection of biofilm formation,https://api.elsevier.com/content/abstract/scopus_id/84871821839,"This paper focuses on the development and characterization of a waveguide-based photonic sensing platform for the detection of biofilm. This integrated photonic platform is based upon the high sensitivity of an optical field distribution formed in optical waveguides and the resulting changes in the refractive index and absorption of this environment.
                  The sensor platform and materials formulations were established from simulation studies conducted with the Olympios software. These simulations demonstrated the importance of correctly specifying the material refractive index to achieve single-mode waveguides. They also highlighted the necessity to deposit a high refractive index layer (HRIL) on top of the optical waveguides in order to increase the intensity of the evanescent field responsible for the sensing performance of the platform.
                  Platform fabrication exploited a low-cost process using photocurable organic–inorganic hybrid sol–gel materials, which were microstructured by UV-photolithography to form channel optical waveguides. A tantalum-based material was synthesized using the sol–gel process with refractive index as high as 1.87. This material was developed, optically characterized and applied as an evanescent field enhancement layer, deposited at the surface of the waveguide to increase the sensitivity of the sensing platform.
                  The sensor characterization was performed by monitoring the output intensity of the optical waveguide while contaminated water was monitored in a quasi-static flow-rate (0.5ml/s) on the platform. It is shown that our biosensor platform was able to detect the biofilm formation after 10min of reactions, demonstrating the early stage biofilm formation in quasi-static flow-rate. Furthermore, the sensing performances of our photonic platform were found to be strongly dependent on the thickness of the HRIL, confirming the simulations studies.
                  This work proved the concept of employing a waveguide-based photonic platform for the early detection of biofilm formation, including the induction phase, and as such, we believe this system has immense potential for future applications as a label-free and real-time biosensor platform.",science
10.1016/j.nantod.2013.05.001,Journal,Nano Today,scopus,2013-01-01,sciencedirect,Nanoelectronics-biology frontier: From nanoscopic probes for action potential recording in live cells to three-dimensional cyborg tissues,https://api.elsevier.com/content/abstract/scopus_id/84883599662,"Semiconductor nanowires configured as the active channels of field-effect transistors (FETs) have been used as detectors for high-resolution electrical recording from single live cells, cell networks, tissues and organs. Extracellular measurements with substrate supported silicon nanowire (SiNW) FETs, which have projected active areas orders of magnitude smaller than conventional microfabricated multielectrode arrays (MEAs) and planar FETs, recorded action potential and field potential signals with high signal-to-noise ratio and temporal resolution from cultured neurons, cultured cardiomyocytes, acute brain slices and whole animal hearts. Measurements made with modulation-doped nanoscale active channel SiNW FETs demonstrate that signals recorded from cardiomyocytes are highly localized and have improved time resolution compared to larger planar detectors. In addition, several novel three-dimensional (3D) transistor probes, which were realized using advanced nanowire synthesis methods, have been implemented for intracellular recording. These novel probes include (i) flexible 3D kinked nanowire FETs, (ii) branched intracellular nanotube SiNW FETs, and (iii) active silicon nanotube FETs. Following phospholipid modification of the probes to mimic the cell membrane, the kinked nanowire, branched intracellular nanotube and active silicon nanotube FET probes recorded full-amplitude intracellular action potentials from spontaneously firing cardiomyocytes. Moreover, these probes demonstrated the capability of reversible, stable, and long-term intracellular recording, thus indicating the minimal invasiveness of the new nanoscale structures and suggesting biomimetic internalization via the phospholipid modification. Simultaneous, multi-site intracellular recording from both single cells and cell networks were also readily achieved by interfacing independently addressable nanoprobe devices with cells. Finally, electronic and biological systems have been seamlessly merged in 3D for the first time using macroporous nanoelectronic scaffolds that are analogous to synthetic tissue scaffold and the extracellular matrix in tissue. Free-standing 3D nanoelectronic scaffolds were cultured with neurons, cardiomyocytes and smooth muscle cells to yield electronically-innervated synthetic or ‘cyborg’ tissues. Measurements demonstrate that innervated tissues exhibit similar cell viability as with conventional tissue scaffolds, and importantly, demonstrate that the real-time response to drugs and pH changes can be mapped in 3D through the tissues. These results open up a new field of research, wherein nanoelectronics are merged with biological systems in 3D thereby providing broad opportunities, ranging from a nanoelectronic/tissue platform for real-time pharmacological screening in 3D to implantable ‘cyborg’ tissues enabling closed-loop monitoring and treatment of diseases. Furthermore, the capability of high density scale-up of the above extra- and intracellular nanoscopic probes for action potential recording provide important tools for large-scale high spatio-temporal resolution electrical neural activity mapping in both 2D and 3D, which promises to have a profound impact on many research areas, including the mapping of activity within the brain.",science
10.1016/j.ultrasmedbio.2013.04.013,Journal,Ultrasound in Medicine and Biology,scopus,2013-01-01,sciencedirect,Novel Method for Localization of Common Carotid Artery Transverse Section in Ultrasound Images Using Modified Viola-Jones Detector,https://api.elsevier.com/content/abstract/scopus_id/84883453413,"This article describes a novel method for highly accurate and effective localization of the transverse section of the carotis comunis artery in ultrasound images. The method has a high success rate, approximately 97%. Unlike analytical methods based on geometric descriptions of the object sought, the method proposed here can cover a large area of shape variation of the artery under study, which normally occurs during examinations as a result of the pressure on the examined tissue, tilt of the probe, setup of the sonographic device, and other factors. This method shows great promise in automating the process of determining circulatory system parameters in the non-invasive clinical diagnostics of cardiovascular diseases. The method employs a Viola-Jones detector that has been specially adapted for efficient detection of transverse sections of the carotid artery. This algorithm is trained on a set of labeled images using the AdaBoost algorithm, Haar-like features and the Matthews coefficient. The training algorithm of the artery detector was modified using evolutionary algorithms. The method for training a cascade of classifiers achieves on a small number of positive and negative training data samples (about 500 images) a high success rate in a computational time that allows implementation of the detector in real time. Testing was performed on images of different patients for whom different ultrasonic instruments were used under different conditions (settings) so that the algorithm developed is applicable in general radiologic practice.",science
10.1016/j.asoc.2012.05.031,Journal,Applied Soft Computing Journal,scopus,2013-01-01,sciencedirect,Optimal design of laser solid freeform fabrication system and real-time prediction of melt pool geometry using intelligent evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84881665462,"With the rapid growth of laser applications and the introduction of high efficiency lasers (e.g. fiber lasers), laser material processing has gained increasing importance in a variety of industries. Among the applications of laser technology, laser cladding has received significant attention due to its high potential for material processing such as metallic coating, high value component repair, prototyping, and even low-volume manufacturing. In this paper, two optimization methods have been applied to obtain optimal operating parameters of Laser Solid Freeform Fabrication Process (LSFF) as a real world engineering problem. First, Particle Swarm Optimization (PSO) algorithm was implemented for real-time prediction of melt pool geometry. Then, a hybrid evolutionary algorithm called Self-organizing Pareto based Evolutionary Algorithm (SOPEA) was proposed to find the optimal process parameters. For further assurance on the performance of the proposed optimization technique, it was compared to some well-known vector optimization algorithms such as Non-dominated Sorting Genetic Algorithm (NSGA-II) and Strength Pareto Evolutionary Algorithm (SPEA 2). Thereafter, it was applied for simultaneous optimization of clad height and melt pool depth in LSFF process. Since there is no exact mathematical model for the clad height (deposited layer thickness) and the melt pool depth, the authors developed two Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to estimate these two process parameters. Optimization procedure being done, the archived non-dominated solutions were surveyed to find the appropriate ranges of process parameters with acceptable dilutions. Finally, the selected optimal ranges were used to find a case with the minimum rapid prototyping time. The results indicate the acceptable potential of evolutionary strategies for controlling and optimization of LSFF process as a complicated engineering problem.",science
10.1016/j.ipm.2012.07.006,Journal,Information Processing and Management,scopus,2013-01-01,sciencedirect,SocConnect: A personalized social network aggregator and recommender,https://api.elsevier.com/content/abstract/scopus_id/84878112328,"Users of Social Networking Sites (SNSs) like Facebook, LinkedIn or Twitter, are facing two problems: (1) it is difficult for them to keep track of their social friendships and friends’ social activities scattered across different SNSs; and (2) they are often overwhelmed by the huge amount of social data (friends’ updates and other activities). To address these two problems, we propose a user-centric system called “SocConnect” (Social Connect) for aggregating social data from different SNSs and allowing users to create personalized social and semantic contexts for their social data. Users can blend and group friends on different SNSs, and rate the friends and their activities as favourite, neutral or disliked. SocConnect then provides personalized recommendation of friends’ activities that may be interesting to each user, using machine learning techniques. A prototype is also implemented to demonstrate these functionalities of SocConnect. Evaluation on real users confirms that users generally like the proposed functionalities of our system, and machine learning can be effectively applied to provide personalized recommendation of friends’ activities and help users deal with cognitive overload.",science
10.1016/S1872-2040(13)60621-0,Journal,Fenxi Huaxue/ Chinese Journal of Analytical Chemistry,scopus,2013-01-01,sciencedirect,High throughput screening of tranquilizers in dairy products using ultra-performance liquid chromatography coupled to high resolution time-of-flight mass spectrometry,https://api.elsevier.com/content/abstract/scopus_id/84873150354,"A high throughput screening method was developed for the simultaneous analysis of twenty representative tranquilizers in dairy products using the combination of ultra-performance liquid chromatography-high resolution time-of-flight mass spectrometry and screening database built with Target Analysis software. The protein and fat in the sample was removed using a two-step precipitation of acetonitrile and acidic acetonitrile, and the supernatant was combined and concentrated with Speedvac concentrator. The chromatographic separation was performed on an ACQUITY BEH column with gradient elution using 0.1% formic acid and acetonitrile as mobile phase at a flow rate of 0.3 mL min−1. The monitoring of tranquilizers was achieved by time-of-flight mass spectrometry under positive ionization mode in 9 min. The quantification was performed with matrix-matching method. The linear ranges were 1–500 μg L−1 or 5–1000 μg L−1. The LODs and LOQs were 0.3–1.5 μg L−1 and 1–5 μg L−1, respectively. At 5 and 50μg L−1 spiked levels, the average recoveries were 76.1%–108.2% with relative standard deviations of 2.5%–9.0%. The screening result for spiked sample shows all the spiked tranquilizers could be correctly identified with low deviations of retention time (< 0.1 min) and mass (< 3 mDa) and high degrees of isotope pattern match (> 89.5%). The developed method was further applied for the analysis of 50 real dairy products, and no positive sample was detected.",science
10.1016/j.medengphy.2012.04.016,Journal,Medical Engineering and Physics,scopus,2013-01-01,sciencedirect,A self-adaptive foot-drop corrector using functional electrical stimulation (FES) modulated by tibialis anterior electromyography (EMG) dataset,https://api.elsevier.com/content/abstract/scopus_id/84872349449,"We developed a functional electrical stimulator for correcting the gait patterns of patients with foot-drop problem. The stimulating electrical pulses of the system are modulated to evoke contractions of the tibialis anterior muscle, by emulating the normal patterns. The modulation is adaptive, i.e. the system can predict the user's step frequency and the generated stimulation can match each step in real-time. In this study, step data from 11 young healthy volunteers were acquired, and five prediction algorithms were evaluated by the acquired data, including the average of Previous N steps (P-N), the Previous Nth step (P-Nth), General Regression Neural Network (GRNN), Autoregressive (AR) and Kalman filter (KF). The algorithm with the best efficiency-accuracy trade-off (P-N, when N
                     =5) was implemented in the FES system. System evaluation results obtained from a post-stroke patient with foot-drop showed that the system of this study demonstrated better performance on gait pattern correction than the methods widely adopted in commercial products.",science
10.3168/jds.2012-5630,Journal,Journal of Dairy Science,scopus,2013-01-01,sciencedirect,The gradient boosting algorithm and random boosting for genome-assisted evaluation in large data sets,https://api.elsevier.com/content/abstract/scopus_id/84871610662,"In the next few years, with the advent of high-density single nucleotide polymorphism (SNP) arrays and genome sequencing, genomic evaluation methods will need to deal with a large number of genetic variants and an increasing sample size. The boosting algorithm is a machine-learning technique that may alleviate the drawbacks of dealing with such large data sets. This algorithm combines different predictors in a sequential manner with some shrinkage on them; each predictor is applied consecutively to the residuals from the committee formed by the previous ones to form a final prediction based on a subset of covariates. Here, a detailed description is provided and examples using a toy data set are included. A modification of the algorithm called “random boosting” was proposed to increase predictive ability and decrease computation time of genome-assisted evaluation in large data sets. Random boosting uses a random selection of markers to add a subsequent weak learner to the predictive model. These modifications were applied to a real data set composed of 1,797 bulls genotyped for 39,714 SNP. Deregressed proofs of 4 yield traits and 1 type trait from January 2009 routine evaluations were used as dependent variables. A 2-fold cross-validation scenario was implemented. Sires born before 2005 were used as a training sample (1,576 and 1,562 for production and type traits, respectively), whereas younger sires were used as a testing sample to evaluate predictive ability of the algorithm on yet-to-be-observed phenotypes. Comparison with the original algorithm was provided. The predictive ability of the algorithm was measured as Pearson correlations between observed and predicted responses. Further, estimated bias was computed as the average difference between observed and predicted phenotypes. The results showed that the modification of the original boosting algorithm could be run in 1% of the time used with the original algorithm and with negligible differences in accuracy and bias. This modification may be used to speed the calculus of genome-assisted evaluation in large data sets such us those obtained from consortiums.",science
10.1016/j.compchemeng.2012.06.021,Journal,Computers and Chemical Engineering,scopus,2012-12-20,sciencedirect,SmartGantt - An interactive system for generating and updating rescheduling knowledge using relational abstractions,https://api.elsevier.com/content/abstract/scopus_id/84869501412,"Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy.",science
10.1016/j.asoc.2012.01.014,Journal,Applied Soft Computing Journal,scopus,2012-12-01,sciencedirect,Autonomous real-time landing site selection for Venus and Titan using Evolutionary Fuzzy Cognitive Maps,https://api.elsevier.com/content/abstract/scopus_id/84869086510,"Future science-driven landing missions, conceived to collect in situ data on regions of planetary bodies that have the highest potential to yield important scientific discoveries, will require a higher degree of autonomy. The latter includes the ability of the spacecraft to autonomously select the landing site using real-time data acquired during the descent phase. This paper presents the development of an Evolutionary Fuzzy Cognitive Map (E-FCM) model that implements an artificial intelligence system capable of autonomously selecting a landing site with the highest potential for scientific discoveries constrained by the requirement of soft landing in a region with safe terrains. The proposed E-FCM evolves its internal states and interconnections as a function of real-time data collected during the descent phase, therefore improving the decision process as more accurate information becomes available. The E-FCM is constructed using knowledge accumulated by planetary experts and it is tested on scenarios that simulate the decision process during the descent phase toward the Hyndla Regio on Venus. The E-FCM is shown to quickly reach conclusions that are consistent with what would be the choice of a planetary expert if the scientist were presented with the same information. The proposed methodology is fast and efficient and may be suitable for on-board spacecraft implementation and real-time decision making during the course of robotic exploration of the Solar System.",science
10.1016/j.knosys.2012.07.009,Journal,Knowledge-Based Systems,scopus,2012-12-01,sciencedirect,"PHIS: A system for scouting potential hubs and for favoring their ""growth"" in a Social Internetworking Scenario",https://api.elsevier.com/content/abstract/scopus_id/84867856485,"In this paper we propose ΦΦ (PHIS), a system for scouting current and potential hubs in a Social Internetworking Scenario (SIS) and for favoring the “growth” of these last ones in such a way that they can become real hubs quickly. ΦΦ operates on a SIS, rather than on a single social network. It uses a hypergraph-based model to represent the SIS. In order to verify if a user is a (current or potential) hub, it considers a range of criteria encompassing her reputation, her capability of receiving and delivering information, her position in the social networks of the SIS and the wideness of her profile. The same criteria are also exploited by ΦΦ to organize a campaign of stimulations to “train” a potential hub to become a real one.",science
10.1016/j.gene.2012.07.075,Journal,Gene,scopus,2012-11-01,sciencedirect,Codon substitution models based on residue similarity and their applications,https://api.elsevier.com/content/abstract/scopus_id/84866142617,"Codon models are now widely used to draw evolutionary inferences from alignments of homologous sequence data. Incorporating physicochemical properties of amino acids into codon models, two novel codon substitution models describing the evolution of protein-coding DNA sequences are presented based on the similarity scores of amino acids. To describe substitutions between codons a continue-time Markov process is used. Transition/transversion rate bias and nonsynonymous codon usage bias are allowed in the models. In our implementation, the parameters are estimated by maximum-likelihood (ML) method as in previous studies. Furthermore, instantaneous mutations involving more than one nucleotide position of a codon are considered in the second model. Then the two suggested models are applied to five real data sets. The analytic results indicate that the new codon models considering physicochemical properties of amino acids can provide a better fit to the data comparing with existing codon models, and then produce more reliable estimates of certain biologically important measures than existing methods.",science
10.1016/j.actaastro.2012.06.003,Journal,Acta Astronautica,scopus,2012-11-01,sciencedirect,"Robotic Mission to Mars: Hands-on, minds-on, web-based learning",https://api.elsevier.com/content/abstract/scopus_id/84865224510,"Problem-based learning has been demonstrated as an effective methodology for developing analytical skills and critical thinking. The use of scenario-based learning incorporates problem-based learning whilst encouraging students to collaborate with their colleagues and dynamically adapt to their environment. This increased interaction stimulates a deeper understanding and the generation of new knowledge. The Victorian Space Science Education Centre (VSSEC) uses scenario-based learning in its Mission to Mars, Mission to the Orbiting Space Laboratory and Primary Expedition to the M.A.R.S. Base programs. These programs utilize methodologies such as hands-on applications, immersive-learning, integrated technologies, critical thinking and mentoring to engage students in Science, Technology, Engineering and Mathematics (STEM) and highlight potential career paths in science and engineering. The immersive nature of the programs demands specialist environments such as a simulated Mars environment, Mission Control and Space Laboratory, thus restricting these programs to a physical location and limiting student access to the programs. To move beyond these limitations, VSSEC worked with its university partners to develop a web-based mission that delivered the benefits of scenario-based learning within a school environment. The Robotic Mission to Mars allows students to remotely control a real rover, developed by the Australian Centre for Field Robotics (ACFR), on the VSSEC Mars surface. After completing a pre-mission training program and site selection activity, students take on the roles of scientists and engineers in Mission Control to complete a mission and collect data for further analysis. Mission Control is established using software developed by the ACRI Games Technology Lab at La Trobe University using the principles of serious gaming. The software allows students to control the rover, monitor its systems and collect scientific data for analysis. This program encourages students to work scientifically and explores the interaction between scientists and engineers. This paper presents the development of the program, including the involvement of university students in the development of the rover, the software, and the collation of the scientific data. It also presents the results of the trial phase of this program including the impact on student engagement and learning outcomes.",science
10.1016/j.landusepol.2011.11.010,Journal,Land Use Policy,scopus,2012-10-01,sciencedirect,"Land-use changes and policy dimension driving forces in China: Present, trend and future",https://api.elsevier.com/content/abstract/scopus_id/84857797332,"China has extremely scarce land resources compared to the world average. There is an urgent need for studies of the current situation and the trends in land-use change and assessment of the performance of land policies in China. Assessment of land-use change has long been hindered by a lack of accurate and reliable data. This paper uses the data obtained from the national land surveys of 1996 and land-use change surveys from 1997 to 2008, to analyze changes in land use and the policy dimension driving forces related to the changes, especially cultivated land, forestry land, grassland, as well as developed land. The aim of this analysis will be to derive the physical, social and economical driving forces of those changes to grasp the trends in land-use change and the effects of land policies and to formulate strategies for the protection and sustainable use of agricultural land. The results indicate that, although the overall change in land use was not large, cultivated land was significantly reduced and developed land rapidly increased. A great deal of high quality cultivated land was changed to developed land and low quality cultivated land generated from unused land, which has resulted in a serious threat to food supplies in China. Predictions using the methods of linear extrapolation and a BP neural network indicate that it is impossible to keep to a target of 0.12 billion hectares of cultivated land in the future under the mode of economic development used between 1996 and 2008. The results also indicate that the implementation of the laws and regulations about controlling the developed land and preserving cultivated land had significant effects on changes in land use, especially cultivated land and developed land. The results suggest that the changes in land use are closely related to economic fluctuation and the enaction and implementation of these land policies had a little time lag for cultivated land protection. There is a pressing need for China to use its limited land resources more efficiently and effectively by enacting or re-enforcing the laws and regulations on land resources protection and economic development, not only for its own growing population, but also the world. Therefore, we must formulate strategies for the protection and sustainable use of agricultural land.",science
10.1016/j.cmpb.2011.02.007,Journal,Computer Methods and Programs in Biomedicine,scopus,2012-09-01,sciencedirect,Information system analysis of an e-learning system used for dental restorations simulation,https://api.elsevier.com/content/abstract/scopus_id/84863864660,"The goal of using virtual and augmented reality technologies in therapeutic interventions simulation, in the fixed prosthodontics (VirDenT) project, is to increase the quality of the educational process in dental faculties, by assisting students in learning how to prepare teeth for all-ceramic restorations. Its main component is an e-learning virtual reality-based software system that will be used for the developing skills in grinding teeth, needed in all-ceramic restorations. The complexity of the domain problem that the software system dealt with made the analysis of the information system supported by VirDenT necessary. The analysis contains the following activities: identification and classification of the system stakeholders, description of the business processes, formulation of the business rules, and modelling of business objects. During this stage, we constructed the context diagram, the business use case diagram, the activity diagrams and the class diagram of the domain model. These models are useful for the further development of the software system that implements the VirDenT information system.",science
10.1016/j.aap.2011.04.011,Journal,Accident Analysis and Prevention,scopus,2012-09-01,sciencedirect,How much benefit does Intelligent Speed Adaptation deliver? - An analysis of its potential contribution to safety and environment,https://api.elsevier.com/content/abstract/scopus_id/84861881609,"The UK Intelligent Speed Adaptation (ISA) project produced a rich database with high-resolution data on driver behaviour covering a comprehensive range of road environment. The field trials provided vital information on driver behaviour in the presence of ISA. The purpose of this paper is to exploit the information gathered in the field trials to predict the impacts of various forms of ISA and to assess whether ISA is viable in terms of benefit-to-cost ratio. ISA is predicted to save up to 33% of accidents on urban roads, and to reduce CO2 emissions by up to 5.8% on 70mph roads. In order to investigate the long-term impacts of ISA, two hypothetical deployment scenarios were envisaged covering a 60-year appraisal period. The results indicate that ISA could deliver a very healthy benefit-to-cost ratio, ranging from 3.4 to 7.4, depending on the deployment scenarios. Under both deployment scenarios, ISA has recovered its implementation costs in less than 15 years. It can be concluded that implementation of ISA is clearly justified from a social cost and benefit perspective. Of the two deployment scenarios, the Market Driven one is substantially outperformed by the Authority Driven one. The benefits of ISA on fuel saving and emission reduction are real but not substantial, in comparison with the benefits on accident reduction; up to 98% of benefits are attributable to accident savings. Indeed, ISA is predicted to lead to a savings of 30% in fatal crashes and 25% in serious crashes over the 60-year period modelled.",science
10.1016/j.neunet.2012.02.029,Journal,Neural Networks,scopus,2012-08-01,sciencedirect,Real-time human-robot interaction underlying neurorobotic trust and intent recognition,https://api.elsevier.com/content/abstract/scopus_id/84861781307,"In the past three decades, the interest in trust has grown significantly due to its important role in our modern society. Everyday social experience involves “confidence” among people, which can be interpreted at the neurological level of a human brain. Recent studies suggest that oxytocin is a centrally-acting neurotransmitter important in the development and alteration of trust. Its administration in humans seems to increase trust and reduce fear, in part by directly inhibiting the amygdala. However, the cerebral microcircuitry underlying this mechanism is still unknown. We propose the first biologically realistic model for trust, simulating spiking neurons in the cortex in a real-time human–robot interaction simulation. At the physiological level, oxytocin cells were modeled with triple apical dendrites characteristic of their structure in the paraventricular nucleus of the hypothalamus. As trust was established in the simulation, this architecture had a direct inhibitory effect on the amygdala tonic firing, which resulted in a willingness to exchange an object from the trustor (virtual neurorobot) to the trustee (human actor). Our software and hardware enhancements allowed the simulation of almost 100,000 neurons in real time and the incorporation of a sophisticated Gabor mechanism as a visual filter. Our brain was functional and our robotic system was robust in that it trusted or distrusted a human actor based on movement imitation.",science
10.1016/j.bmcl.2012.05.123,Journal,Bioorganic and Medicinal Chemistry Letters,scopus,2012-07-15,sciencedirect,CCLab - A multi-objective genetic algorithm based combinatorial library design software and an application for histone deacetylase inhibitor design,https://api.elsevier.com/content/abstract/scopus_id/84863430722,"The introduction of the multi-objective optimization has dramatically changed the virtual combinatorial library design, which can consider many objectives simultaneously, such as synthesis cost and drug-likeness, thus may increase positive rates of biological active compounds. Here we described a software called CCLab (Combinatorial Chemistry Laboratory) for combinatorial library design based on the multi-objective genetic algorithm. Tests of the convergence ability and the ratio to re-take the building blocks in the reference library were conducted to assess the software in silico, and then it was applied to a real case of designing a 5×6 HDAC inhibitor library. Sixteen compounds in the resulted library were synthesized, and the histone deactetylase (HDAC) enzymatic assays proved that 14 compounds showed inhibitory ratios more than 50% against tested 3 HDAC enzymes at concentration of 20μg/mL, with IC50 values of 3 compounds comparable to SAHA. These results demonstrated that the CCLab software could enhance the hit rates of the designed library and would be beneficial for medicinal chemists to design focused library in drug development (the software can be downloaded at: http://202.127.30.184:8080/drugdesign.html).",science
10.1016/j.eswa.2012.01.059,Journal,Expert Systems with Applications,scopus,2012-07-01,sciencedirect,Analyzing the solutions of DEA through information visualization and data mining techniques: SmartDEA framework,https://api.elsevier.com/content/abstract/scopus_id/84858339895,"Data envelopment analysis (DEA) has proven to be a useful tool for assessing efficiency or productivity of organizations, which is of vital practical importance in managerial decision making. DEA provides a significant amount of information from which analysts and managers derive insights and guidelines to promote their existing performances. Regarding to this fact, effective and methodologic analysis and interpretation of DEA results are very critical. The main objective of this study is then to develop a general decision support system (DSS) framework to analyze the results of basic DEA models. The paper formally shows how the results of DEA models should be structured so that these solutions can be examined and interpreted by analysts through information visualization and data mining techniques effectively. An innovative and convenient DEA solver, SmartDEA, is designed and developed in accordance with the proposed analysis framework. The developed software provides DEA results which are consistent with the framework and are ready-to-analyze with data mining tools, thanks to their specially designed table-based structures. The developed framework is tested and applied in a real world project for benchmarking the vendors of a leading Turkish automotive company. The results show the effectiveness and the efficacy of the proposed framework.",science
10.1016/j.jconrel.2011.11.029,Journal,Journal of Controlled Release,scopus,2012-06-10,sciencedirect,Quantitative structure - Property relationship modeling of remote liposome loading of drugs,https://api.elsevier.com/content/abstract/scopus_id/84861710385,Remote loading of liposomes by trans-membrane gradients is used to achieve therapeutically efficacious intra-liposome concentrations of drugs. We have developed Quantitative Structure Property Relationship (QSPR) models of remote liposome loading for a data set including 60 drugs studied in 366 loading experiments internally or elsewhere. Both experimental conditions and computed chemical descriptors were employed as independent variables to predict the initial drug/lipid ratio (D/L) required to achieve high loading efficiency. Both binary (to distinguish high vs. low initial D/L) and continuous (to predict real D/L values) models were generated using advanced machine learning approaches and 5-fold external validation. The external prediction accuracy for binary models was as high as 91–96%; for continuous models the mean coefficient R2 for regression between predicted versus observed values was 0.76–0.79. We conclude that QSPR models can be used to identify candidate drugs expected to have high remote loading capacity while simultaneously optimizing the design of formulation experiments.,science
10.1016/j.neuroscience.2012.01.048,Journal,Neuroscience,scopus,2012-04-19,sciencedirect,Microarray analysis of the global gene expression profile following hypothermia and transient focal cerebral ischemia,https://api.elsevier.com/content/abstract/scopus_id/84858620587,"Background
                  Hypothermia is one of the most robust experimental neuroprotective interventions against cerebral ischemia. Identification of molecular pathways and gene networks together with single genes or gene families that are significantly associated with neuroprotection might help unravel the mechanisms of therapeutic hypothermia.
               
                  Material and Methods
                  We performed a microarray analysis of ischemic rat brains that underwent 90 min of middle cerebral artery occlusion (MCAO) and 48 h of reperfusion. Hypothermia was induced for 4 h, starting 1 h after MCAO in male Wistar rats. At 48 h, magnetic resonance imaging (MRI) was performed for infarct volumetry, and functional outcome was determined by a neuroscore. The brain gene expression profile of sham (S), ischemia (I), and ischemia plus hypothermia (HI) treatment were compared by analyzing changes of individual genes, pathways, and networks. Real-time reverse-transcribed polymerase chain reaction (RT-PCR) was performed on selected genes to validate the data.
               
                  Results
                  Rats treated with HI had significantly reduced infarct volumes and improved neuroscores at 48 h compared with I. Of 4067 genes present on the array chip, HI compared with I upregulated 50 (1.23%) genes and downregulated 103 (3.20%) genes equal or greater than twofold. New genes potentially mediating neuroprotection by hypothermia were HNRNPAB, HIG-1, and JAK3. On the pathway level, HI globally suppressed the ischemia-driven gene response. Twelve gene networks were identified to be significantly altered by HI compared with I. The most significantly altered network contained genes participating in apoptosis suppression.
               
                  Conclusions
                  Our data suggest that although hypothermia at the pathway level restored gene expression to sham levels, it selectively regulated the expression of several genes implicated in protein synthesis and folding, calcium homeostasis, cellular and synaptic integrity, inflammation, cell death, and apoptosis.",science
10.1016/j.cmpb.2011.09.003,Journal,Computer Methods and Programs in Biomedicine,scopus,2012-04-01,sciencedirect,A prescription fraud detection model,https://api.elsevier.com/content/abstract/scopus_id/84857451503,"Prescription fraud is a main problem that causes substantial monetary loss in health care systems. We aimed to develop a model for detecting cases of prescription fraud and test it on real world data from a large multi-center medical prescription database. Conventionally, prescription fraud detection is conducted on random samples by human experts. However, the samples might be misleading and manual detection is costly. We propose a novel distance based on data-mining approach for assessing the fraudulent risk of prescriptions regarding cross-features. Final tests have been conducted on adult cardiac surgery database. The results obtained from experiments reveal that the proposed model works considerably well with a true positive rate of 77.4% and a false positive rate of 6% for the fraudulent medical prescriptions. The proposed model has the potential advantages including on-line risk prediction for prescription fraud, off-line analysis of high-risk prescriptions by human experts, and self-learning ability by regular updates of the integrative data sets. We conclude that incorporating such a system in health authorities, social security agencies and insurance companies would improve efficiency of internal review to ensure compliance with the law, and radically decrease human-expert auditing costs.",science
10.1016/j.pain.2011.10.025,Journal,Pain,scopus,2012-02-01,sciencedirect,A randomized controlled evaluation of an online chronic pain self management program,https://api.elsevier.com/content/abstract/scopus_id/84855963874,"Internet-based educational and therapeutic programs (e-health applications) are becoming increasingly popular for a variety of psychological and physical disorders. We tested the efficacy of an online Chronic Pain Management Program, a comprehensive, fully self-directed and self-paced system that integrates social networking features and self-management tools into an interactive learning environment. Of 305 adult participants (196 women, 109 men), a total of 162 individuals with chronic pain were randomly assigned unsupervised access to the program for approximately 6weeks; 143 were assigned to the wait-listed control group with treatment as usual. A comprehensive assessment was administered before the study and approximately 7 and 14weeks thereafter. All recruitment, data collection, and participant involvement took place online. Participation was fully self-paced, permitting the evaluation of program effectiveness under real-world conditions. Intent-to-treat analysis that used linear growth models was used as the primary analytic tool. Results indicated that program utilization was associated with significant decreases in pain severity, pain-related interference and emotional burden, perceived disability, catastrophizing, and pain-induced fear. Further, program use led to significant declines in depression, anxiety, and stress. Finally, as compared to the wait-listed control group, the experimental group displayed a significant increase in knowledge about the principles of chronic pain and its management. Study limitations are considered, including the recognition that not all persons with chronic pain are necessarily good candidates for self-initiated, self-paced, interactive learning.",science
10.1533/9780857096302.3.444,Book,"Ultrasonic Transducers: Materials and Design for Sensors, Actuators and Medical Applications",scopus,2012-01-01,sciencedirect,Analysis and synthesis of frequency-diverse ultrasonic flaw-detection systems using order statistics and neural network processors,https://api.elsevier.com/content/abstract/scopus_id/84903222245,Ultrasonic imaging is used for nondestructive evaluation of materials and flaw detection. Flaw detection in the presence of microstructure scattering noise is a challenging problem. Frequency-diverse ultrasonic detection algorithms decorrelate the microstructure scattering noise and enhance the visibility of echoes associated with defects. The performance of ranked order-statistic processors is examined using both theory and ultrasonic experiments. Split-spectrum processing combined with neural networks as post-processors can improve flaw detection. Neural networks have robust performance and are capable of outperforming conventional detection techniques. A field-programmable gate array (FPGA)-based case study demonstrates the real-time operation of ultrasonic flaw-detection algorithms. Architecture details and implementation results with various hardware/software partitioning schemes are discussed.,science
10.3182/20120403-3-DE-3010.00041,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Embedded system for controlling a mini underwater vehicle in autonomous hover mode,https://api.elsevier.com/content/abstract/scopus_id/84866098847,"This work presents the development of a mini underwater vehicle (Triton-PR), the embedded system, and the experiments in real-time for autonomous hover operation. Artificial vision allows the vehicle to obtain the translational position and velocity. The main characteristic of the embedded system is the implementation of low cost devices and materials, besides the number and location of the thrusters was chosen in order to have enough power and generate the rotation and translation movements. The dynamical model of the (Triton-PR) is described by the classic Euler-Lagrange equations, and a PD controller based on saturation functions is proposed for providing autonomous attitude and position of the robot. Finally, the performance of the vehicle is shown in simulation and real-time experimental results.",science
10.1016/B978-0-444-59520-1.50104-4,Book Series,Computer Aided Chemical Engineering,scopus,2012-01-01,sciencedirect,Intelligent Automation Platform for Bioprocess Development,https://api.elsevier.com/content/abstract/scopus_id/84862870640,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future.",science
10.1016/j.autcon.2011.05.018,Journal,Automation in Construction,scopus,2012-01-01,sciencedirect,Simulation and analytical techniques for construction resource planning and scheduling,https://api.elsevier.com/content/abstract/scopus_id/81355138778,"To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a “seat-of-the-pants” style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project.",science
10.1016/j.frad.2011.09.007,Journal,Feuillets de Radiologie,scopus,2011-12-01,sciencedirect,A computer system retrieval of MRI brain tumors,https://api.elsevier.com/content/abstract/scopus_id/84857047577,"Objectif
                  Nous décrivons la conception et l’implémentation d’un système informatique d’archivage et d’indexation ainsi que la création d’une base de données concernant des cas d’imagerie par résonance magnétique (IRM) de six types histologiques de tumeurs cérébrales.
               
                  Matériel et méthodes
                  Nous tentons d’explorer les techniques offertes par l’évolution incrémentale de la technologie d’information pour avoir un acquis dans l’aide à la décision médicale. Notre plan d’action se divise en deux grandes parties : en premier lieu, nous avons crée le serveur distant d’archivage des données et en deuxième lieu, nous avons implémenté une application locale pour l’indexation.
               
                  Résultats
                  L’approche de raisonnement implémentée, qui est celle des réseaux bayésiens, montre une efficience lors du traitement des problèmes d’indexation. L’apport principal de notre contribution est la modélisation du raisonnement du radiologue dans l’interprétation d’un cas médical. Les jeux d’essais testés argumentent notre choix.
               
                  Conclusion
                  Le modèle proposé peut être conservé comme étant le noyau d’un système d’aide au diagnostic approuvé. L’efficacité enviable des résultats d’une telle contribution ouvre des perspectives intéressantes au niveau de l’appel des autres approches classées sous la thématique de l’intelligence artificielle.
               
                  Objective
                  We describe the design and implementation of a computer system for archiving and indexing brain tumors as well as the creation of a database for MRI cases of six histological types.
               
                  Material and methods
                  We explored the techniques offered by the incremental development of computer technology useful for the medical decision making process. We began by a remote server data archiving system, which we then implemented for a local indexing application.
               
                  Results
                  The reasoning behind our approach, based on Bayesian networks, demonstrated good efficiency in processing indexing problems. Our main contribution is to modelize the radiologist's reasoning when interpreting medical cases. Real case testing argued in favor of our choice.
               
                  Conclusion
                  Our model can be used as the core of an approved diagnosis aid system. The promising perspectives offered by our model could be useful in other approaches in the area of artificial intelligence.",science
10.1016/j.wasman.2011.07.018,Journal,Waste Management,scopus,2011-12-01,sciencedirect,A web-based Decision Support System for the optimal management of construction and demolition waste,https://api.elsevier.com/content/abstract/scopus_id/80054853534,"Wastes from construction activities constitute nowadays the largest by quantity fraction of solid wastes in urban areas. In addition, it is widely accepted that the particular waste stream contains hazardous materials, such as insulating materials, plastic frames of doors, windows, etc. Their uncontrolled disposal result to long-term pollution costs, resource overuse and wasted energy. Within the framework of the DEWAM project, a web-based Decision Support System (DSS) application – namely DeconRCM – has been developed, aiming towards the identification of the optimal construction and demolition waste (CDW) management strategy that minimises end-of-life costs and maximises the recovery of salvaged building materials. This paper addresses both technical and functional structure of the developed web-based application. The web-based DSS provides an accurate estimation of the generated CDW quantities of twenty-one different waste streams (e.g. concrete, bricks, glass, etc.) for four different types of buildings (residential, office, commercial and industrial). With the use of mathematical programming, the DeconRCM provides also the user with the optimal end-of-life management alternative, taking into consideration both economic and environmental criteria. The DSS’s capabilities are illustrated through a real world case study of a typical five floor apartment building in Thessaloniki, Greece.",science
10.1016/j.nedt.2010.11.006,Journal,Nurse Education Today,scopus,2011-11-01,sciencedirect,Contextualism adds realism: Nursing students' perceptions of and performance in numeracy skills tests,https://api.elsevier.com/content/abstract/scopus_id/80755172314,"This project investigated nursing students' perceptions of and performance in a de-contextualised diagnostic maths paper (i.e. questions only) and a contextualised diagnostic maths paper (i.e. visual pictures along with questions). Sampling was purposive, the criteria being that participants would be from the population of student nurses (n
                     =700) in their second year, of a three-year Bachelor of Nursing course, undertaking a Unit ‘Medical–Surgical Nursing 1’ (MSN1) at one of four campuses across the University of Western Sydney (UWS), NSW, Australia. The numerical test scores for both papers were analysed with the assistance of SPSS software and a Professional Development Officer. The survey data were analysed manually and thematically by the researcher. There was a substantive improvement in scores from Test 1 (de-contextualised) to Test 2 (contextualised). It is uncertain whether the change occurred because Test 2 is a genuinely better presentation than Test 1 or just a practice effect. Nevertheless, the contextualised paper was preferred by the majority of students (80%). Students preferred the visual images and revealed that it led to a “deeper learning” of numeracy skills, reduced stress and anxiety levels and simulated ‘the real life’ clinical setting, thus adding “an element of realism” to the situation.",science
10.1016/j.neunet.2011.06.014,Journal,Neural Networks,scopus,2011-11-01,sciencedirect,Concurrent heterogeneous neural model simulation on real-time neuromimetic hardware,https://api.elsevier.com/content/abstract/scopus_id/80053218543,"Dedicated hardware is becoming increasingly essential to simulate emerging very-large-scale neural models. Equally, however, it needs to be able to support multiple models of the neural dynamics, possibly operating simultaneously within the same system. This may be necessary either to simulate large models with heterogeneous neural types, or to simplify simulation and analysis of detailed, complex models in a large simulation by isolating the new model to a small subpopulation of a larger overall network. The SpiNNaker neuromimetic chip is a dedicated neural processor able to support such heterogeneous simulations. Implementing these models on-chip uses an integrated library-based tool chain incorporating the emerging PyNN interface that allows a modeller to input a high-level description and use an automated process to generate an on-chip simulation. Simulations using both LIF and Izhikevich models demonstrate the ability of the SpiNNaker system to generate and simulate heterogeneous networks on-chip, while illustrating, through the network-scale effects of wavefront synchronisation and burst gating, methods that can provide effective behavioural abstractions for large-scale hardware modelling. SpiNNaker’s asynchronous virtual architecture permits greater scope for model exploration, with scalable levels of functional and temporal abstraction, than conventional (or neuromorphic) computing platforms. The complete system illustrates a potential path to understanding the neural model of computation, by building (and breaking) neural models at various scales, connecting the blocks, then comparing them against the biology: computational cognitive neuroscience.",science
10.1016/j.eswa.2011.05.094,Journal,Expert Systems with Applications,scopus,2011-11-01,sciencedirect,Improvement of new automatic differential fuzzy clustering using SVM classifier for microarray analysis,https://api.elsevier.com/content/abstract/scopus_id/80052027978,"In recent year, the problem of clustering in microarray data has been gaining significant attention. However most of the clustering methods attempt to find the group of genes where the number of cluster is known a priori. This fact motivated us to develop a new real-coded improved differential evolution based automatic fuzzy clustering algorithm which automatically evolves the number of clusters as well as the proper partitioning of a gene expression data set. To improve the result further, the clustering method is integrated with a support vector machine, a well-known technique for supervised learning. A fraction of the gene expression data points selected from different clusters based on their proximity to the respective centers, is used for training the SVM. The clustering assignments of the remaining gene expression data points are thereafter determined using the trained classifier. The performance of the proposed clustering technique has been demonstrated on five gene expression data sets by comparing it with the differential evolution based automatic fuzzy clustering, variable length genetic algorithm based fuzzy clustering and well known Fuzzy C-Means algorithm. Statistical significance test has been carried out to establish the statistical superiority of the proposed clustering approach. Biological significance test has also been carried out using a web based gene annotation tool to show that the proposed method is able to produce biologically relevant clusters of genes. The processed data sets and the matlab version of the software are available at http://bio.icm.edu.pl/∼darman/IDEAFC-SVM/.",science
10.1016/j.jcv.2011.04.012,Journal,Journal of Clinical Virology,scopus,2011-09-01,sciencedirect,"Standardization and performance evaluation of ""modified"" and ""ultrasensitive"" versions of the Abbott RealTime HIV-1 assay, adapted to quantify minimal residual viremia",https://api.elsevier.com/content/abstract/scopus_id/79961128343,"Background
                  Numerous studies investigating clinical significance of HIV-1 minimal residual viremia (MRV) suggest potential utility of assays more sensitive than those routinely used to monitor viral suppression. However currently available methods, based on different technologies, show great variation in detection limit and input plasma volume, and generally suffer from lack of standardization.
               
                  Objectives
                  In order to establish new tools suitable for routine quantification of minimal residual viremia in patients under virological suppression, some modifications were introduced into standard procedure of the Abbott RealTime HIV-1 assay leading to a “modified” and an “ultrasensitive” protocols.
               
                  Study design
                  The following modifications were introduced: calibration curve extended towards low HIV-1 RNA concentration; 4 fold increased sample volume by concentrating starting material; reduced volume of internal control; adoption of “open-mode” software for quantification. Analytical performances were evaluated using the HIV-1 RNA Working Reagent 1 for NAT assays (NIBSC). Both tests were applied to clinical samples from virologically suppressed patients.
               
                  Results
                  The “modified” and the “ultrasensitive” configurations of the assay reached a limit of detection of 18.8 (95% CI: 11.1–51.0cp/mL) and 4.8cp/mL (95% CI: 2.6–9.1cp/mL), respectively, with high precision and accuracy. In clinical samples from virologically suppressed patients, “modified” and “ultrasensitive” protocols allowed to detect and quantify HIV RNA in 12.7% and 46.6%, respectively, of samples resulted “not-detectable”, and in 70.0% and 69.5%, respectively, of samples “detected <40cp/mL” in the standard assay.
               
                  Conclusions
                  The “modified” and “ultrasensitive” assays are precise and accurate, and easily adoptable in routine diagnostic laboratories for measuring MRV.",science
10.1016/j.jvlc.2011.03.004,Journal,Journal of Visual Languages and Computing,scopus,2011-08-01,sciencedirect,An alternative map of the United States based on an n-dimensional model of geographic space,https://api.elsevier.com/content/abstract/scopus_id/79960409782,"Geographic features have traditionally been visualized with fairly high amount of geometric detail, while relationships among these features in attribute space have been represented at a much coarser resolution. This limits our ability to understand complex high-dimensional relationships and structures existing in attribute space. In this paper, we present an alternative approach aimed at creating a high-resolution representation of geographic features with the help of a self-organizing map (SOM) consisting of a large number of neurons. In a proof-of-concept implementation, we spatialize 200,000+ U.S. Census block groups using a SOM consisting of 250,000 neurons. The geographic attributes considered in this study reflect a more holistic representation of geographic reality than in previous studies. The study includes 69 attributes regarding population statistics, land use/land cover, climate, geology, topography, and soils. This diversity of attributes is informed by our desire to build a comprehensive two-dimensional base map of n-dimensional geographic space. The paper discusses how standard GIS methods and neural network processing are combined towards the creation of an alternative map of the United States.",science
10.1016/j.jchromb.2011.03.059,Journal,Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences,scopus,2011-06-01,sciencedirect,Influence of different spacer arms on Mimetic Ligand <sup>™</sup> A2P and B14 membranes for human IgG purification,https://api.elsevier.com/content/abstract/scopus_id/79955910197,"Microporous membranes are an attractive alternative to circumvent the typical drawbacks associated to bead-based chromatography. In particular, the present work intends to evaluate different affinity membranes for antibody capture, to be used as an alternative to Protein A resins. To this aim, two Mimetic Ligands™ A2P and B14, were coupled onto different epoxide and azide group activated membrane supports using different spacer arms and immobilization chemistries. The spacer chemistries investigated were 1,2-diaminoethane (2LP), 3,6-dioxa-1,8-octanedithiol (DES) and [1,2,3] triazole (TRZ). These new mimetic membrane materials were investigated by static and by dynamic binding capacity studies, using pure polyclonal human immunoglobulin G (IgG) solutions as well as a real cell culture supernatant containing monoclonal IgG1. The best results were obtained by combining the new B14 ligand with a TRZ-spacer and an improved Epoxy 2 membrane support material. The new B14-TRZ-Epoxy 2 membrane adsorbent provided binding capacities of approximately 3.1mg/mL, besides (i) a good selectivity towards IgG, (ii) high IgG recoveries of above 90%, (iii) a high Pluronic-F68 tolerance and (iv) no B14-ligand leakage under harsh cleaning-in-place conditions (0.6M sodium hydroxide). Furthermore, foreseeable improvements in binding capacity will promote the implementation of membrane adsorbers in antibody manufacturing.",science
10.1016/j.neunet.2011.02.004,Journal,Neural Networks,scopus,2011-06-01,sciencedirect,Learning parametric dynamic movement primitives from multiple demonstrations,https://api.elsevier.com/content/abstract/scopus_id/79953692970,"Learning from demonstration has shown to be a suitable approach for learning control policies (CPs). However, most previous studies learn CPs from a single demonstration, which results in limited scalability and insufficient generalization toward a wide range of applications in real environments. This paper proposes a novel approach to learn highly scalable CPs of basis movement skills from multiple demonstrations. In contrast to conventional studies with a single demonstration, i.e., dynamic movement primitives (DMPs), our approach efficiently encodes multiple demonstrations by shaping a parametric-attractor landscape in a set of differential equations. Assuming a certain similarity among multiple demonstrations, our approach learns the parametric-attractor landscape by extracting a small number of common factors in multiple demonstrations. The learned CPs allow the synthesis of novel movements with novel motion styles by specifying the linear coefficients of the bases as parameter vectors without losing useful properties of the DMPs, such as stability and robustness against perturbations. For both discrete and rhythmic movement skills, we present a unified learning procedure for learning a parametric-attractor landscape from multiple demonstrations. The feasibility and highly extended scalability of DMPs are demonstrated on an actual dual-arm robot.",science
10.1016/j.eswa.2010.10.072,Journal,Expert Systems with Applications,scopus,2011-05-01,sciencedirect,Gastro-intestinal tract inspired computational model for myocardial infarction diagnosis,https://api.elsevier.com/content/abstract/scopus_id/79151469757,"Myocardial infarction (MI) has been the major cause of mortality in humans. The diagnostic process of MI in humans is critical and requires careful examination of the general symptoms, heart activity rate and blood chemistry apart from considering the patient history and contextual information of the diagnostic process. The manual diagnostic process often results in high misclassification rate due to the higher level of complexity involved therein. Thus, computer-based diagnostic systems have been developed to assist the medical practitioners for accurately diagnosing MI. However, most of the computer-based techniques are unable to handle and incorporate contextual information associated with the diagnostic process effectively. This paper aims at enhancing the computer aided diagnostic performance and solving the problem of handling contextual information through the development of a novel nature-inspired computational model. The model presented in this paper is based on the defense mechanism associated with the Human Digestive System. It incorporates multi-level decision making approach and the reasoning behind diagnostic indications at different levels of diagnostic process. The proposed diagnostic system, implemented using MATLAB, has been tested on real medical data set. The experimental results of the proposed model have been compared with five standard AI-based classification techniques. The results obtained suggest that the new classification approach presented in this paper is better than the standard AI-based classification solutions in most cases.",science
10.1016/j.neucom.2010.05.023,Journal,Neurocomputing,scopus,2011-03-15,sciencedirect,A block-based model for monitoring of human activity,https://api.elsevier.com/content/abstract/scopus_id/79952631192,"The study of human activity is applicable to a large number of science and technology fields, such as surveillance, biomechanics or sports applications. This article presents BB6-HM, a block-based human model for real-time monitoring of a large number of visual events and states related to human activity analysis, which can be used as components of a library to describe more complex activities in such important areas as surveillance, for example, luggage at airports, clients’ behaviour in banks and patients in hospitals. BB6-HM is inspired by the proportionality rules commonly used in Visual Arts, i.e., for dividing the human silhouette into six rectangles of the same height. The major advantage of this proposal is that analysis of the human can be easily broken down into regions, so that we can obtain information of activities. The computational load is very low, so it is possible to define a very fast implementation. Finally, this model has been applied to build classifiers for the detection of primitive events and visual attributes using heuristic rules and machine learning techniques.",science
10.1016/j.physio.2010.10.001,Journal,Physiotherapy,scopus,2011-03-01,sciencedirect,"Introducing the ICF: The development of an online resource to support learning, teaching and curriculum design",https://api.elsevier.com/content/abstract/scopus_id/79451472272,"The International Classification of Functioning, Disability and Health (ICF) was adopted as one of the key models to support early health professional learning across a suite of new preregistration health science courses. It was decided that an online resource should be developed to enable students, course designers and teaching staff, across all disciplines, to have access to the same definitions, government policies and other supporting information on disability. As part of the comprehensive curriculum review, enquiry-based learning was adopted as the educational approach. Enquiry-based learning promotes deeper learning by encouraging students to engage in authentic challenges. As such, it was important that the online resource was not merely a site for accessing content, but enabled students to make decisions about where else to explore for credible information about the ICF. The selection of a host location that all students and staff could access meant that the resource could not be located in the existing online learning management system. Construction using software being trialled by the library at La Trobe University allowed for the required access, as well as alignment with an enquiry-based learning approach. Consultation for the content of the online resource included formal and informal working groups on curriculum review. The published version included resources from the World Health Organization, examples of research completed within different disciplines, a test of knowledge and a preformatted search page. The format of the online resource allows for updating of information, and feedback on the utilisation of the software has been used to enhance the student experience. The key issues for the development of this online resource were accessibility for students and staff, alignment with the adopted educational approach, consultation with all disciplines, and ease of modification of information and format once published.",science
10.1016/j.envsoft.2010.03.021,Journal,Environmental Modelling and Software,scopus,2011-03-01,sciencedirect,A methodology for the design and development of integrated models for policy support,https://api.elsevier.com/content/abstract/scopus_id/78649846894,"The development of Decision Support Systems (DSS) to inform policy making has been increasing rapidly. This paper aims to provide insight into the design and development process of policy support systems that incorporate integrated models. It will provide a methodology for the development of such systems that attempts to synthesize knowledge and experience gained over the past 15–20 years from developing a suite of these DSSs for a number of users in different geographical contexts worldwide.
                  The methodology focuses on the overall iterative development process that includes policy makers, scientists and IT-specialists. The paper highlights important tasks in model integration and system development and illustrates these with some practical examples from DSS that have dynamic, spatial and integrative attributes.
                  Crucial integrative features of modelling systems that aim to provide support to policy processes, and to which we refer as integrated Decision Support Systems, are:
                        
                           •
                           Synthesis of relevant drivers, processes and characteristics of the real world system at relevant spatial and temporal scales.
                        
                        
                           •
                           An integrated approach linking economic, environmental and social domains.
                        
                        
                           •
                           Connection to the policy context, interest groups and end-users.
                        
                        
                           •
                           Engagement with the policy process.
                        
                        
                           •
                           Ability to provide added value to the current decision-making practice.
                        
                     
                  
                  With this paper we aim to provide a methodology for the design and development of these integrated Decision Support Systems that includes the ‘hard’ elements of model integration and software development as well as the ‘softer’ elements related to the user-developer interaction and social learning of all groups involved in the process.",science
10.1016/B978-1-84334-610-4.50007-0,Book,"Information Literacy: Infiltrating the Agenda, Challenging Minds",scopus,2011-01-01,sciencedirect,Spielberg your way to information literacy: Producing educational movies and videos,https://api.elsevier.com/content/abstract/scopus_id/84904033429,"This chapter looks at how simple it has become for library staff with even the most inexpensive of equipment to create effective promotional, marketing or instructional videos. It first examines the various roles that movies can play, from complementing elements of taught sessions through to reinforcement of learning or marketing goals with external readers. As well as considering the production of bespoke video works, it also considers the trade-offs in outsourcing elements of the production process and making use of off-the-shelf movies. Tips on selling the benefits of film making and pitching film concepts to potentially sceptical stakeholders and managers are also covered.
               
                  The main part of the chapter takes the prospective producer from their original concept and pre-production planning activities, through the scripting, filming, editing, reviewing and evaluation processes. The importance of maintaining a clear narrative vision throughout the whole process is stressed as an important central theme. Guidance is included on the key team roles and responsibilities, along with scripting and plotting templates as a production aid. Additionally, a focus is placed on making maximum advantage of the visual delivery medium by utilising action, rather than relying on dialogue alone to convey information.
               Consideration is given throughout to illustrating the common pitfalls and obstacles that can frustrate the would-be producer, and the easy steps that can be taken to overcome them. Short examples of film projects are used to illustrate real world examples of the how the production techniques can be applied. Particular attention is drawn to the time and staffing resource requirements to successfully complete a filming project.
               A selection of advice on suitable equipment and software, a glossary of specialised terms and a range of suggested further readings and exemplar viewings are provided at the end of the chapter.",science
10.3182/20110828-6-IT-1002.01424,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2011-01-01,sciencedirect,Neural Network based system for real-time organ recognition during surgical operation,https://api.elsevier.com/content/abstract/scopus_id/84866753336,"Abstract
                  In this paper we propose (based on testing results) a Neural Network structures that can be used for recognition of presence and absence of internal organ on images received from endoscope. Based on selected NN structure we design two NN-based systems for distinguishing and real-time recognition of internal organs on sequence of endoscopic images during abdominal surgery. First NN-based system proposed in this paper is designed for recognition of several different internal organs on color endoscopic images. Second NN-based system is designed for real-time recognition of presence of a particular internal organ on a sequence of color images (video stream) from endoscope. Restricted connectivity structure of the network makes possible decomposition of the image during the analysis and significantly reduces the number of parameters thus making training easier, faster and more accurate. The algorithms proposed in the paper are implemented in software application and their effectiveness is demonstrated on simulations.",science
10.1016/j.jas.2011.07.015,Journal,Journal of Archaeological Science,scopus,2011-01-01,sciencedirect,"Living the past: 3D models, virtual reality and game engines as tools for supporting archaeology and the reconstruction of cultural heritage - the case-study of the Roman villa of Casal de Freiria",https://api.elsevier.com/content/abstract/scopus_id/80054111601,"“Learn about the past to better understand the present and predict the future”.
                  Although used abundantly to justify our interest in ancient societies, this statement lacks practical meaning due to the high degree of uncertainty which cloaks archaeological studies and theories, and the fact that there is no real way to prove or validate them. That is why it is so important to approach this study from a multidisciplinary point of view, providing several inputs which complement each other and so maximize the amount of factual information drawn from the analysis. Even then, the study will never be truly complete because there will always be a missing document, a small trace of an object (Verhagen, 2008), that still needs to be analysed.
                  This paper aims to be a useful contribution to historical research, specifically to the study of architectural history. Its purpose is to create a series of methods and tools for testing and analysing theories and hypotheses for historical scenarios (Vasáros, 2008) through the use of 3D modelling tools and Virtual Reality (VR) engines.
                  The project was developed in two stages:
                  The first was the creation of several three-dimensional (3D) models, each representing a different theory or hypothesis. The models were based on accurate Computer Assisted Design (CAD) (Autodesk® AutoCAD) models for the reconstruction of the buildings, and Geographic Information Systems (GIS) (ESRI®
                     ) for the recreation of the terrain, thereby creating a realistic representation of what exists now, and a close approximation to what may have once existed.
                  In the second stage, a simplified version of the models was imported into a Virtual Reality (VR) game engine (Bethesda Softworks®
                     ) to create the ambience of the villa at the time, allowing full exploration of the space. It also includes fauna and flora, as well as Artificial Intelligence (AI)-driven avatars, as can be seen in the Video 1 provided in the electronic version of this manuscript.",science
10.1016/j.jcecho.2011.06.005,Journal,Journal of Cardiovascular Echography,scopus,2011-01-01,sciencedirect,The strong correlation between right ventricular function evaluated by 3D echography and functional impairment in patients with dilated cardiomyopathy,https://api.elsevier.com/content/abstract/scopus_id/80052999243,"Obiettivi
                  Lo scopo dello studio è stato quello di stabilire se la frazione di eiezione del ventricolo destro stimata all’ecocardiografia tridimensionale real-time (real-time 3D echocardiography, RT3DE) possa individuare i pazienti affetti da cardiomiopatia dilatativa (dilated cardiomyopathy, DCM) con capacità funzionale maggiormente compromessa al test cardiopolmonare.
               
                  Materiali e metodi
                  55 pazienti affetti da cardiomiopatia dilatativa in fase di scompenso cardiaco cronico (età 56,5±9,1 anni; 40 maschi; 30 di eziologia ischemica; Classe NYHA III: 40) e 30 soggetti sani di controllo sono stati sottoposti attraverso RT3DE all’analisi sia del ventricolo sinistro sia del ventricolo destro. Dopo aver acquisito le immagini, il software post processing ha elaborato i dati della RT3DE relativi all’indice di dissincronia sistolica (systolic dyssynchrony index, SDI) dei 16 segmenti del ventricolo sinistro e ha registrato, inoltre, la frazione di eiezione del ventricolo sinistro e del ventricolo destro. La risonanza magnetica cardiaca è stata effettuata in un sottogruppo di 20 pazienti affetti da cardiomiopatia dilatativa per confermare le misurazioni ottenute mediante RT3DE. Tutti i pazienti con cardiomiopatia dilatativa sono stati inoltre sottoposti a test cardiopolmonare, condotto al cicloergometro, per la misurazione di VO2 di picco (espresso come percentuale del valore predetto), soglia anaerobica (VE/VCO2 slope) e circulatory power (CP).
               
                  Risultati
                  Nei pazienti dilatati la frazione di eiezione media era pari a 29,8±4,6%. Il valore di SDI del ventricolo sinistro valutato alla RT3DE era 8,4±4,2 e la frazione di eiezione del ventricolo destro era 51,3±4,6%. Al test cardiopolmonare il valore medio di VO2 di picco era 15,2±4,4mL/kg/min, mentre quello di CP era 2,1±0,8. Attraverso l’analisi univariata, la frazione di eiezione del ventricolo destro è risultata direttamente proporzionale al VO2 di picco (%) (r=0,55; p<0,0001) e inversamente proporzionale alla soglia anaerobica valutata con VE/VCO2 slope (r=–0,42; p<0,001). Attraverso un’analisi multivariata, SDI e frazione di eiezione del ventricolo destro valutata all’ecocardiografia 3D sono risultati gli unici fattori in grado di predire in maniera significativa il VO2 di picco (%) misurato al test cardiopolmonare.
               
                  Conclusioni
                  Nei pazienti affetti da cardiomiopatia dilatativa la riduzione della funzione sistolica del ventricolo destro è associata in maniera indipendente a una peggiore tolleranza all’esercizio fisico aerobico.
               
                  Objectives
                  The aim of the study was to detect if right ventricular (RV) ejection fraction assessed by real-time 3D echocardiography (RT3DE) could predict patients with dilated cardiomyopathy (DCM) with greater functional impairment in response to cardiopulmonary exercise.
               
                  Materials and methods
                  55 chronic heart failure patients with DCM (56.5±9.1 years; 40 males; 30 ischaemic; NYHA class III: 40) and 30 healthy controls underwent both left ventricular (LV) and RV analysis by RT3DE. Post-processing software provided data of RT3DE systolic dyssynchrony index (SDI) of 16 LV segments, and of both LV and RV ejection fraction. Cardiac magnetic resonance was performed in a subgroup of 20 DCM patients to confirm RT3DE measurements. DCM patients underwent also bicycle cardiopulmonary exercise test with evaluation of VO2 peak (percentage of the predicted value), VE/VCO2 slope and circulatory power (CP).
               
                  Results
                  In DCM patients mean LV ejection fraction was 29.8±4.6%. RT3DE LV SDI was 8.4.4±4.2, and RV ejection fraction was 51.3±4.6%. By cardiopulmonary test, mean VO2 peak was 15.2±4.4mL/kg/min, and mean CP was 2.1±0.8. By univariable analysis, RV ejection fraction directly correlated with VO2 peak % (r=0,55; p<0.0001) and inversely with VE/VCO2 slope (r=–0.42; p<0.001). By multivariable analysis, SDI (beta coefficient=–0.46; p<0.001) and 3D RV ejection fraction (beta coefficient=0.42; p<0.001) emerged as the only independent determinant of VO2 peak % during cardiopulmonary test.
               
                  Conclusions
                  Impaired RV function in DCM patients is independently associated with worse ability to perform aerobic exercise.",science
10.1016/j.isatra.2011.03.003,Journal,ISA Transactions,scopus,2011-01-01,sciencedirect,Classification and authentication of unknown water samples using machine learning algorithms,https://api.elsevier.com/content/abstract/scopus_id/79955923880,"This paper proposes the development of water sample classification and authentication, in real life which is based on machine learning algorithms. The proposed techniques used experimental measurements from a pulse voltametry method which is based on an electronic tongue (E-tongue) instrumentation system with silver and platinum electrodes. E-tongue include arrays of solid state ion sensors, transducers even of different types, data collectors and data analysis tools, all oriented to the classification of liquid samples and authentication of unknown liquid samples. The time series signal and the corresponding raw data represent the measurement from a multi-sensor system. The E-tongue system, implemented in a laboratory environment for 6 numbers of different ISI (Bureau of Indian standard) certified water samples (Aquafina, Bisleri, Kingfisher, Oasis, Dolphin, and McDowell) was the data source for developing two types of machine learning algorithms like classification and regression. A water data set consisting of 6 numbers of sample classes containing 4402 numbers of features were considered. A PCA (principal component analysis) based classification and authentication tool was developed in this study as the machine learning component of the E-tongue system. A proposed partial least squares (PLS) based classifier, which was dedicated as well; to authenticate a specific category of water sample evolved out as an integral part of the E-tongue instrumentation system. The developed PCA and PLS based E-tongue system emancipated an overall encouraging authentication percentage accuracy with their excellent performances for the aforesaid categories of water samples.",science
10.4995/RIAI.2011.01.08,Journal,RIAI - Revista Iberoamericana de Automatica e Informatica Industrial,scopus,2011-01-01,sciencedirect,Softsensor trained using the concept of instrumental variables and applied in the measurement of temperature in a teniente converter,https://api.elsevier.com/content/abstract/scopus_id/79551660327,"Se propone el diseño de un sensor virtual para medir la temperatura del baño de metal blanco en un Convertidor Teniente, utilizando redes neuronales a través de un nuevo método de entrenamiento que utiliza el concepto de variables instrumentales. Este nuevo tipo de entrenamiento será comparado con el método de gradiente descendente, el cual al estar basado en ajuste por mínimos cuadrados presenta sesgo en sus parámetros, producto del ruido de medición. La apuesta es que el método de variables instrumentales resuelva este problema, entregando una red con parámetros ajustados sin sesgo, lo que se verá reflejado en que la salida de esta red, se ajustará de mejor forma a la señal real que el método tradicional de gradiente descendente. Los resultados demuestran que la propuesta planteada entrega un sensor con mejor ajuste que el algoritmo tradicional cuando el instrumento real no se encuentra disponible. La aplicación específica del sensor virtual de temperatura para el Convertidor Teniente presenta gran interés para la industria debido al alto costo de los instrumentos que actualmente pueden cumplir dicha función.",science
10.1016/j.artmed.2010.10.004,Journal,Artificial Intelligence in Medicine,scopus,2011-01-01,sciencedirect,Brain-computer interface analysis of a dynamic visuo-motor task,https://api.elsevier.com/content/abstract/scopus_id/78650965550,"Background
                  The area of brain–computer interfaces (BCIs) represents one of the more interesting fields in neurophysiological research, since it investigates the development of the machines that perform different transformations of the brain's “thoughts” to certain pre-defined actions. Experimental studies have reported some successful implementations of BCIs; however, much of the field still remains unexplored. According to some recent reports the phase coding of informational content is an important mechanism in the brain's function and cognition, and has the potential to explain various mechanisms of the brain's data transfer, but it has yet to be scrutinized in the context of brain–computer interface. Therefore, if the mechanism of phase coding is plausible, one should be able to extract the phase-coded content, carried by brain signals, using appropriate signal-processing methods. In our previous studies we have shown that by using a phase-demodulation-based signal-processing approach it is possible to decode some relevant information on the current motor action in the brain from electroencephalographic (EEG) data.
               
                  Objective
                  In this paper the authors would like to present a continuation of their previous work on the brain-information-decoding analysis of visuo-motor (VM) tasks. The present study shows that EEG data measured during more complex, dynamic visuo-motor (dVM) tasks carries enough information about the currently performed motor action to be successfully extracted by using the appropriate signal-processing and identification methods. The aim of this paper is therefore to present a mathematical model, which by means of the EEG measurements as its inputs predicts the course of the wrist movements as applied by each subject during the task in simulated or real time (BCI analysis). However, several modifications to the existing methodology are needed to achieve optimal decoding results and a real-time, data-processing ability. The information extracted from the EEG could, therefore, be further used for the development of a closed-loop, non-invasive, brain–computer interface.
               
                  Materials and methods
                  For the case of this study two types of measurements were performed, i.e., the electroencephalographic (EEG) signals and the wrist movements were measured simultaneously, during the subject's performance of a dynamic visuo-motor task. Wrist-movement predictions were computed by using the EEG data-processing methodology of double brain-rhythm filtering, double phase demodulation and double principal component analyses (PCA), each with a separate set of parameters. For the movement-prediction model a fuzzy inference system was used.
               
                  Results
                  The results have shown that the EEG signals measured during the dVM tasks carry enough information about the subjects’ wrist movements for them to be successfully decoded using the presented methodology. Reasonably high values of the correlation coefficients suggest that the validation of the proposed approach is satisfactory. Moreover, since the causality of the rhythm filtering and the PCA transformation has been achieved, we have shown that these methods can also be used in a real-time, brain–computer interface. The study revealed that using non-causal, optimized methods yields better prediction results in comparison with the causal, non-optimized methodology; however, taking into account that the causality of these methods allows real-time processing, the minor decrease in prediction quality is acceptable.
               
                  Conclusion
                  The study suggests that the methodology that was proposed in our previous studies is also valid for identifying the EEG-coded content during dVM tasks, albeit with various modifications, which allow better prediction results and real-time data processing. The results have shown that wrist movements can be predicted in simulated or real time; however, the results of the non-causal, optimized methodology (simulated) are slightly better. Nevertheless, the study has revealed that these methods should be suitable for use in the development of a non-invasive, brain–computer interface.",science
10.1016/j.aei.2010.05.003,Journal,Advanced Engineering Informatics,scopus,2011-01-01,sciencedirect,Applying back propagation network to cold chain temperature monitoring,https://api.elsevier.com/content/abstract/scopus_id/78650610590,"Recently, the enhancement of contactless, real-time features and high data transmission rates in supply chain management has been widely discussed. Notably, the cold chain is a part of the supply chain in which temperature monitoring plays a vital role in the system while automatic data collection is an essential aspect of cold chain management. As such, automatic data collection methods, such as radio frequency identification (RFID) techniques, are used to collect temperature data in order to track and trace all manner of products. In this paper, we evaluate an exponentially weighted moving average (EWMA) control chart and artificial neural network technologies in order to monitor collected temperature data in the context of cold chain management. The back-propagation neural network is used to predict temperature shifts and trends. The EWMA control chart is adopted to monitor temperature variations. Using this strategy, as anomalies occur, the control center of an enterprise can perform certain actions immediately to prevent further disaster. Finally, we construct a system using a back-propagation neural network and statistical process control chart. A simulated environment using LEGO® bricks is also implemented to demonstrate the feasibility of this study. This temperature control mechanism is found to be useful for a real-time temperature data collection environment, as with using active RFID tags with temperature sensors for cold chain management.",science
10.1016/j.robot.2010.08.004,Journal,Robotics and Autonomous Systems,scopus,2010-12-31,sciencedirect,Open-ended evolution as a means to self-organize heterogeneous multi-robot systems in real time,https://api.elsevier.com/content/abstract/scopus_id/78649913375,"This work deals with the application of multi-robot systems to real tasks and, in particular, their coordination through interaction based control systems. Within this field, the practical solutions that have been implemented in real robots mainly use strongly coordinated architectures and assignment strategies because of reliability and fault tolerance issues when addressing problems in reality. Emergent approaches have also been proposed with limited success, basically due to the unpredictability of the behaviors obtained. Here, an emergent approach, called r-ASiCo, is presented containing a procedure to produce predictable solutions and thus avoiding the typical problems associated with these techniques. The r-ASico algorithm is the real time version of the Asynchronous Situated Co-evolution algorithm (ASiCo), which exploits natural open-ended evolution to generate emergent complex collective behaviors and deals with systems made up of a huge number of elements and nonlinear interactions. The goal of r-ASiCo is to design the global behavior desired for the robot team as a collective entity and allow the emergence of behaviors through the interaction of the team members using social rules they learn to implement. To this end, r-ASiCo manages a series of features that are inherent to natural evolution based methods such as energy exchange and mating selection procedures, together with a technique to guide the evolution towards a design objective, the principled evaluation function selection procedure. Hence, this paper presents the components and operation of r-ASiCo and illustrates its application through a collective cleaning task example. It was implemented using 8 e-puck robots in two different real scenarios and its results complemented with those of a 30 e-puck case. The results show the capabilities of r-ASiCo to create a self-organized and adaptive multi-robot system configuration that is tolerant to environmental changes and to failures within the robot team.",science
