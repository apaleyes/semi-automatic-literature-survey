id,updated,published,title,summary,database
http://arxiv.org/abs/2202.08450v1,2022-02-17T05:33:27Z,2022-02-17T05:33:27Z,"Design-Bench: Benchmarks for Data-Driven Offline Model-Based
  Optimization","Black-box model-based optimization (MBO) problems, where the goal is to find
a design input that maximizes an unknown objective function, are ubiquitous in
a wide range of domains, such as the design of proteins, DNA sequences,
aircraft, and robots. Solving model-based optimization problems typically
requires actively querying the unknown objective function on design proposals,
which means physically building the candidate molecule, aircraft, or robot,
testing it, and storing the result. This process can be expensive and time
consuming, and one might instead prefer to optimize for the best design using
only the data one already has. This setting -- called offline MBO -- poses
substantial and different algorithmic challenges than more commonly studied
online techniques. A number of recent works have demonstrated success with
offline MBO for high-dimensional optimization problems using high-capacity deep
neural networks. However, the lack of standardized benchmarks in this emerging
field is making progress difficult to track. To address this, we present
Design-Bench, a benchmark for offline MBO with a unified evaluation protocol
and reference implementations of recent methods. Our benchmark includes a suite
of diverse and realistic tasks derived from real-world optimization problems in
biology, materials science, and robotics that present distinct challenges for
offline MBO. Our benchmark and reference implementations are released at
github.com/rail-berkeley/design-bench and
github.com/rail-berkeley/design-baselines.",arxiv
http://arxiv.org/abs/2202.08004v1,2022-02-16T11:40:36Z,2022-02-16T11:40:36Z,Deep Koopman Operator with Control for Nonlinear Systems,"Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. We then design an auxiliary control network to encode
the nonlinear state-dependent control term to model the nonlinearity in control
input. For linear control, this encoded term is considered the new control
variable instead, ensuring the linearity of the embedding space. Then we deploy
Linear Quadratic Regulator (LQR) on the linear embedding space to derive the
optimal control policy and decode the actual control input from the control
net. Experimental results demonstrate that our approach outperforms other
existing methods, reducing the prediction error by order-of-magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and 7 Dof robotic manipulator.",arxiv
http://arxiv.org/abs/2202.07064v1,2022-02-14T22:08:23Z,2022-02-14T22:08:23Z,"Towards hardware Implementation of WTA for CPG-based control of a
  Spiking Robotic Arm","Biological nervous systems typically perform the control of numerous degrees
of freedom for example in animal limbs. Neuromorphic engineers study these
systems by emulating them in hardware for a deeper understanding and its
possible application to solve complex problems in engineering and robotics.
Central-Pattern-Generators (CPGs) are part of neuro-controllers, typically used
at their last steps to produce rhythmic patterns for limbs movement. Different
patterns and gaits typically compete through winner-take-all (WTA) circuits to
produce the right movements. In this work we present a WTA circuit implemented
in a Spiking-Neural-Network (SNN) processor to produce such patterns for
controlling a robotic arm in real-time. The robot uses spike-based
proportional-integrativederivative (SPID) controllers to keep a commanded joint
position from the winner population of neurons of the WTA circuit. Experiments
demonstrate the feasibility of robotic control with spiking circuits following
brain-inspiration.",arxiv
http://arxiv.org/abs/2202.06003v2,2022-02-15T10:35:34Z,2022-02-12T07:04:06Z,Robust Learning from Observation with Model Misspecification,"Imitation learning (IL) is a popular paradigm for training policies in
robotic systems when specifying the reward function is difficult. However,
despite the success of IL algorithms, they impose the somewhat unrealistic
requirement that the expert demonstrations must come from the same domain in
which a new imitator policy is to be learned. We consider a practical setting,
where (i) state-only expert demonstrations from the real (deployment)
environment are given to the learner, (ii) the imitation learner policy is
trained in a simulation (training) environment whose transition dynamics is
slightly different from the real environment, and (iii) the learner does not
have any access to the real environment during the training phase beyond the
batch of demonstrations given. Most of the current IL methods, such as
generative adversarial imitation learning and its state-only variants, fail to
imitate the optimal expert behavior under the above setting. By leveraging
insights from the Robust reinforcement learning (RL) literature and building on
recent adversarial imitation approaches, we propose a robust IL algorithm to
learn policies that can effectively transfer to the real environment without
fine-tuning. Furthermore, we empirically demonstrate on continuous-control
benchmarks that our method outperforms the state-of-the-art state-only IL
method in terms of the zero-shot transfer performance in the real environment
and robust performance under different testing conditions.",arxiv
http://arxiv.org/abs/2202.05811v1,2022-02-11T18:21:18Z,2022-02-11T18:21:18Z,Overhead Image Factors for Underwater Sonar-based SLAM,"Simultaneous localization and mapping (SLAM) is a critical capability for any
autonomous underwater vehicle (AUV). However, robust, accurate state estimation
is still a work in progress when using low-cost sensors. We propose enhancing a
typical low-cost sensor package using widely available and often free prior
information; overhead imagery. Given an AUV's sonar image and a partially
overlapping, globally-referenced overhead image, we propose using a
convolutional neural network (CNN) to generate a synthetic overhead image
predicting the above-surface appearance of the sonar image contents. We then
use this synthetic overhead image to register our observations to the provided
global overhead image. Once registered, the transformation is introduced as a
factor into a pose SLAM factor graph. We use a state-of-the-art simulation
environment to perform validation over a series of benchmark trajectories and
quantitatively show the improved accuracy of robot state estimation using the
proposed approach. We also show qualitative outcomes from a real AUV field
deployment. Video attachment: https://youtu.be/_uWljtp58ks",arxiv
http://arxiv.org/abs/2202.04628v2,2022-02-13T21:23:07Z,2022-02-09T18:45:40Z,"Reinforcement Learning with Sparse Rewards using Guidance from Offline
  Demonstration","A major challenge in real-world reinforcement learning (RL) is the sparsity
of reward feedback. Often, what is available is an intuitive but sparse reward
function that only indicates whether the task is completed partially or fully.
However, the lack of carefully designed, fine grain feedback implies that most
existing RL algorithms fail to learn an acceptable policy in a reasonable time
frame. This is because of the large number of exploration actions that the
policy has to perform before it gets any useful feedback that it can learn
from. In this work, we address this challenging problem by developing an
algorithm that exploits the offline demonstration data generated by a
sub-optimal behavior policy for faster and efficient online RL in such sparse
reward settings. The proposed algorithm, which we call the Learning Online with
Guidance Offline (LOGO) algorithm, merges a policy improvement step with an
additional policy guidance step by using the offline demonstration data. The
key idea is that by obtaining guidance from - not imitating - the offline data,
LOGO orients its policy in the manner of the sub-optimal policy, while yet
being able to learn beyond and approach optimality. We provide a theoretical
analysis of our algorithm, and provide a lower bound on the performance
improvement in each learning episode. We also extend our algorithm to the even
more challenging incomplete observation setting, where the demonstration data
contains only a censored version of the true state observation. We demonstrate
the superior performance of our algorithm over state-of-the-art approaches on a
number of benchmark environments with sparse rewards and censored state.
Further, we demonstrate the value of our approach via implementing LOGO on a
mobile robot for trajectory tracking and obstacle avoidance, where it shows
excellent performance.",arxiv
http://arxiv.org/abs/2202.03028v1,2022-02-07T09:41:24Z,2022-02-07T09:41:24Z,QUARK: A Framework for Quantum Computing Application Benchmarking,"Quantum computing (QC) is anticipated to provide a speedup over classical HPC
approaches for specific problems in optimization, simulation, and machine
learning. With the advances in quantum computing toward practical applications,
the need to analyze and compare different quantum solutions increases. While
different low-level benchmarks for QC exist, these benchmarks do not provide
sufficient insights into real-world application-level performance. We propose
an application-centric benchmark method and the QUantum computing Application
benchmaRK (QUARK) framework to foster the investigation and creation of
application benchmarks for QC. This paper establishes three significant
contributions: (1) it makes a case for application-level benchmarks and
provides an in-depth ""pen and paper"" benchmark formulation of two reference
problems: robot path and vehicle option optimization from the industrial
domain; (2) it proposes the open-source QUARK framework for designing,
implementing, executing, and analyzing benchmarks; (3) it provides multiple
reference implementations for these two reference problems based on different
known, and where needed, extended, classical and quantum algorithmic approaches
and analyzes their performance on different types of infrastructures.",arxiv
http://arxiv.org/abs/2202.02656v1,2022-02-05T23:27:46Z,2022-02-05T23:27:46Z,A survey of top-down approaches for human pose estimation,"Human pose estimation in two-dimensional images videos has been a hot topic
in the computer vision problem recently due to its vast benefits and potential
applications for improving human life, such as behaviors recognition, motion
capture and augmented reality, training robots, and movement tracking. Many
state-of-the-art methods implemented with Deep Learning have addressed several
challenges and brought tremendous remarkable results in the field of human pose
estimation. Approaches are classified into two kinds: the two-step framework
(top-down approach) and the part-based framework (bottom-up approach). While
the two-step framework first incorporates a person detector and then estimates
the pose within each box independently, detecting all body parts in the image
and associating parts belonging to distinct persons is conducted in the
part-based framework. This paper aims to provide newcomers with an extensive
review of deep learning methods-based 2D images for recognizing the pose of
people, which only focuses on top-down approaches since 2016. The discussion
through this paper presents significant detectors and estimators depending on
mathematical background, the challenges and limitations, benchmark datasets,
evaluation metrics, and comparison between methods.",arxiv
http://arxiv.org/abs/2202.02352v1,2022-02-04T19:20:58Z,2022-02-04T19:20:58Z,"Learning Interpretable, High-Performing Policies for Continuous Control
  Problems","Gradient-based approaches in reinforcement learning (RL) have achieved
tremendous success in learning policies for continuous control problems. While
the performance of these approaches warrants real-world adoption in domains,
such as in autonomous driving and robotics, these policies lack
interpretability, limiting deployability in safety-critical and
legally-regulated domains. Such domains require interpretable and verifiable
control policies that maintain high performance. We propose Interpretable
Continuous Control Trees (ICCTs), a tree-based model that can be optimized via
modern, gradient-based, RL approaches to produce high-performing, interpretable
policies. The key to our approach is a procedure for allowing direct
optimization in a sparse decision-tree-like representation. We validate ICCTs
against baselines across six domains, showing that ICCTs are capable of
learning interpretable policy representations that parity or outperform
baselines by up to 33$\%$ in autonomous driving scenarios while achieving a
$300$x-$600$x reduction in the number of policy parameters against deep
learning baselines.",arxiv
http://arxiv.org/abs/2202.01862v1,2022-02-03T21:43:06Z,2022-02-03T21:43:06Z,Practical Imitation Learning in the Real World via Task Consistency Loss,"Recent work in visual end-to-end learning for robotics has shown the promise
of imitation learning across a variety of tasks. Such approaches are expensive
both because they require large amounts of real world training demonstrations
and because identifying the best model to deploy in the real world requires
time-consuming real-world evaluations. These challenges can be mitigated by
simulation: by supplementing real world data with simulated demonstrations and
using simulated evaluations to identify high performing policies. However, this
introduces the well-known ""reality gap"" problem, where simulator inaccuracies
decorrelate performance in simulation from that of reality. In this paper, we
build on top of prior work in GAN-based domain adaptation and introduce the
notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages
sim and real alignment both at the feature and action-prediction levels. We
demonstrate the effectiveness of our approach by teaching a mobile manipulator
to autonomously approach a door, turn the handle to open the door, and enter
the room. The policy performs control from RGB and depth images and generalizes
to doors not encountered in training data. We achieve 80% success across ten
seen and unseen scenes using only ~16.2 hours of teleoperated demonstrations in
sim and real. To the best of our knowledge, this is the first work to tackle
latched door opening from a purely end-to-end learning approach, where the task
of navigation and manipulation are jointly modeled by a single neural network.",arxiv
http://arxiv.org/abs/2202.00617v1,2022-02-01T18:05:31Z,2022-02-01T18:05:31Z,"A General, Evolution-Inspired Reward Function for Social Robotics","The field of social robotics will likely need to depart from a paradigm of
designed behaviours and imitation learning and adopt modern reinforcement
learning (RL) methods to enable robots to interact fluidly and efficaciously
with humans. In this paper, we present the Social Reward Function as a
mechanism to provide (1) a real-time, dense reward function necessary for the
deployment of RL agents in social robotics, and (2) a standardised objective
metric for comparing the efficacy of different social robots. The Social Reward
Function is designed to closely mimic those genetically endowed social
perception capabilities of humans in an effort to provide a simple, stable and
culture-agnostic reward function. Presently, datasets used in social robotics
are either small or significantly out-of-domain with respect to social
robotics. The use of the Social Reward Function will allow larger in-domain
datasets to be collected close to the behaviour policy of social robots, which
will allow both further improvements to reward functions and to the behaviour
policies of social robots. We believe this will be the key enabler to
developing efficacious social robots in the future.",arxiv
http://arxiv.org/abs/2201.09857v2,2022-02-11T20:26:30Z,2022-01-24T18:29:23Z,"STOPS: Short-Term-based Volatility-controlled Policy Search and its
  Global Convergence","It remains challenging to deploy existing risk-averse approaches to
real-world applications. The reasons are multi-fold, including the lack of
global optimality guarantee and the necessity of learning from long-term
consecutive trajectories. Long-term consecutive trajectories are prone to
involving visiting hazardous states, which is a major concern in the
risk-averse setting. This paper proposes \textbf{\ul{S}}hort-\textbf{\ul{T}}erm
V\textbf{\ul{O}}latility-controlled \textbf{\ul{P}}olicy \textbf{\ul{S}}earch
(STOPS), a novel algorithm that solves risk-averse problems by learning from
short-term trajectories instead of long-term trajectories. Short-term
trajectories are more flexible to generate, and can avoid the danger of
hazardous state visitations. By using an actor-critic scheme with an
overparameterized two-layer neural network, our algorithm finds a globally
optimal policy at a sublinear rate with proximal policy optimization and
natural policy gradient, with effectiveness comparable to the state-of-the-art
convergence rate of risk-neutral policy-search methods. The algorithm is
evaluated on challenging Mujoco robot simulation tasks under the mean-variance
evaluation metric. Both theoretical analysis and experimental results
demonstrate a state-of-the-art level of STOPS' performance among existing
risk-averse policy search methods.",arxiv
http://arxiv.org/abs/2201.05753v1,2022-01-15T04:07:51Z,2022-01-15T04:07:51Z,"Parameter Identification and Motion Control for Articulated Rigid Body
  Robots Using Differentiable Position-based Dynamics","Simulation modeling of robots, objects, and environments is the backbone for
all model-based control and learning. It is leveraged broadly across dynamic
programming and model-predictive control, as well as data generation for
imitation, transfer, and reinforcement learning. In addition to fidelity, key
features of models in these control and learning contexts are speed, stability,
and native differentiability. However, many popular simulation platforms for
robotics today lack at least one of the features above. More recently,
position-based dynamics (PBD) has become a very popular simulation tool for
modeling complex scenes of rigid and non-rigid object interactions, due to its
speed and stability, and is starting to gain significant interest in robotics
for its potential use in model-based control and learning. Thus, in this paper,
we present a mathematical formulation for coupling position-based dynamics
(PBD) simulation and optimal robot design, model-based motion control and
system identification. Our framework breaks down PBD definitions and
derivations for various types of joint-based articulated rigid bodies. We
present a back-propagation method with automatic differentiation, which can
integrate both positional and angular geometric constraints. Our framework can
critically provide the native gradient information and perform gradient-based
optimization tasks. We also propose articulated joint model representations and
simulation workflow for our differentiable framework. We demonstrate the
capability of the framework in efficient optimal robot design, accurate
trajectory torque estimation and supporting spring stiffness estimation, where
we achieve minor errors. We also implement impedance control in real robots to
demonstrate the potential of our differentiable framework in human-in-the-loop
applications.",arxiv
http://arxiv.org/abs/2201.05518v1,2022-01-14T15:41:05Z,2022-01-14T15:41:05Z,UGV-UAV Object Geolocation in Unstructured Environments,"A robotic system of multiple unmanned ground vehicles (UGVs) and unmanned
aerial vehicles (UAVs) has the potential for advancing autonomous object
geolocation performance. Much research has focused on algorithmic improvements
on individual components, such as navigation, motion planning, and perception.
In this paper, we present a UGV-UAV object detection and geolocation system,
which performs perception, navigation, and planning autonomously in real scale
in unstructured environment. We designed novel sensor pods equipped with
multispectral (visible, near-infrared, thermal), high resolution (181.6 Mega
Pixels), stereo (near-infrared pair), wide field of view (192 degree HFOV)
array. We developed a novel on-board software-hardware architecture to process
the high volume sensor data in real-time, and we built a custom AI subsystem
composed of detection, tracking, navigation, and planning for autonomous
objects geolocation in real-time.
  This research is the first real scale demonstration of such high speed data
processing capability. Our novel modular sensor pod can boost relevant computer
vision and machine learning research. Our novel hardware-software architecture
is a solid foundation for system-level and component-level research. Our system
is validated through data-driven offline tests as well as a series of field
tests in unstructured environments. We present quantitative results as well as
discussions on key robotic system level challenges which manifest when we build
and test the system. This system is the first step toward a UGV-UAV cooperative
reconnaissance system in the future.",arxiv
http://arxiv.org/abs/2201.01943v1,2022-01-06T07:14:02Z,2022-01-06T07:14:02Z,"Machine Learning: Algorithms, Models, and Applications","Recent times are witnessing rapid development in machine learning algorithm
systems, especially in reinforcement learning, natural language processing,
computer and robot vision, image processing, speech, and emotional processing
and understanding. In tune with the increasing importance and relevance of
machine learning models, algorithms, and their applications, and with the
emergence of more innovative uses cases of deep learning and artificial
intelligence, the current volume presents a few innovative research works and
their applications in real world, such as stock trading, medical and healthcare
systems, and software automation. The chapters in the book illustrate how
machine learning and deep learning algorithms and models are designed,
optimized, and deployed. The volume will be useful for advanced graduate and
doctoral students, researchers, faculty members of universities, practicing
data scientists and data engineers, professionals, and consultants working on
the broad areas of machine learning, deep learning, and artificial
intelligence.",arxiv
http://arxiv.org/abs/2201.01369v1,2022-01-04T22:32:05Z,2022-01-04T22:32:05Z,"Using Simulation Optimization to Improve Zero-shot Policy Transfer of
  Quadrotors","In this work, we show that it is possible to train low-level control policies
with reinforcement learning entirely in simulation and, then, deploy them on a
quadrotor robot without using real-world data to fine-tune. To render zero-shot
policy transfers feasible, we apply simulation optimization to narrow the
reality gap. Our neural network-based policies use only onboard sensor data and
run entirely on the embedded drone hardware. In extensive real-world
experiments, we compare three different control structures ranging from
low-level pulse-width-modulated motor commands to high-level attitude control
based on nested proportional-integral-derivative controllers. Our experiments
show that low-level controllers trained with reinforcement learning require a
more accurate simulation than higher-level control policies.",arxiv
