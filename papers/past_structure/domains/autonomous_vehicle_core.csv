id,datePublished,description,title,downloadUrl,publisher,journals,doi,database
24277257,1995,"Operation of an autonomous vehicle in a real-world environment requires capabilities for real-time processing of uncertain, incomplete, and approximate information. We report progress in the development of methods for robust autonomous navigation of autonomous vehicles in unstructured environments. We focus in particular on those issues where the use of techniques based on fuzzy logic has proved to be particularly helpful with emphasis on the representation and execution of complex navigation plans. We illustrate our discussion with examples taken from experimental implementation of these techniques on Flakey, the mobile robot of the Artificial Intelligence Center of SRI International.    On leave from Iridia, Universit&apos;e Libre de Bruxelles, Brussels, Belgium.  1 Introduction  The development of techniques to plan and execute autonomous-vehicle actions in a real-world unstructured environment requires consideration of multiple issues. First, knowledge about the environment is, in gene..",Progress in Research on Autonomous Vehicle Motion Planning,,,,,core
323052297,2019-07-01T00:00:00,"In this work, a real-time unobtrusive heart rate monitoring system is proposed and implemented. The proposed system aims to monitor the heart rate of the passengers by using a low-cost camera, which can be readily embedded in the car's rear-view mirror. Additionally, we integrate this system with the main system of our test driverless car, and we propose how driverless cars should act in response to serious medical emergency situations. Moreover, we investigate how this system can benefit from the promising features of Google I/O and Google AI. Our approach is based on Remote Photoplethysmography (rPPG), in which the heart rate is extracted from the subtle tiny changes occurring in the skin color of the face during every pulsation. The face is automatically detected and tracked, then the raw signal is calculated from each frame over a 10-seconds sliding window. After that, a series of signal processing techniques are implemented on the raw signals to recover the heart rate frequency. Finally, the resultant heart rate measurements are processed and stored, then we compare it with ground truth measurements values obtained using pulse oximeter","Improvement of Driverless Cars' Passengers on Board Health and Safety, using Low-Cost Real-Time Heart Rate Monitoring System",,,,,core
363919914,2020-12-01T08:00:00,"The modern world is constantly in a state of technological revolution. Everyday some new technological idea, invention, or threat emerges. With modern computer software and hardware advancements, we have the emergence of more internet-enabled devices - or, Internet of Things (IoT) devices. We can now create large networks with any device to gather real-time information about an environment. In conjunction, modern car companies across the board have a push from public demand for a fully-autonomous car. In order to accomplish autonomy safely and effectively, Vehicular Ad-Hoc Networks (VANETs) must be established for a local group of cars and their environment to ensure all correct and relevant information is communicated throughout the network. The data collected in a VANET can be passed to machine learning models in order to predict possible conditions and detect anomalies. This thesis explores different ways of clustering local groups of vehicles along with machine learning algorithms to predict where vehicles are likely to be and detect false or impossible information",Clustering algorithms to further enhance predictable situational data in vehicular ad-hoc networks,https://core.ac.uk/download/363919914.pdf,UTC Scholar,,,core
268924452,2019-01-01T00:00:00,"Актуальность исследования обусловлена необходимостью создания современных компьютерных систем для мониторинга опасных технологических объектов предприятий нефтегазовой отрасли. Цель: создание интеллектуальной системы компьютерного зрения беспилотных летательных аппаратов, позволяющей вести мониторинг опасных технологических объектов и анализ данных мониторинга в режиме реального времени на борту беспилотных летательных аппаратов. Объекты: концепция построения интеллектуальной системы компьютерного зрения; новые архитектуры свёрточных нейронных сетей, аппаратно-реализованные на программируемых логических интегральных схемах; метод унификации вычислительных блоков и способы параллельных вычислений в аппаратных свёрточных нейронных сетях; алгоритмы помехоустойчивого кодирования/декодирования данных при обменах сообщениями между наземной и бортовой компонентами интеллектуальной системы компьютерного зрения. Методы: методы классификации и детектирования объектов на изображениях с помощью свёрточных нейронных сетей; методы глубокого обучения свёрточных нейронных сетей; методы проектирования программно-аппаратных систем. Результаты. Проведён анализ современного состояния исследований в области систем мониторинга опасных технологических объектов предприятий нефтегазовой отрасли; разработана концепция создания интеллектуальной системы компьютерного зрения на основе беспилотных летательных аппаратов для мониторинга опасных объектов. Базовой в концепции является идея анализа изображений, полученных при мониторинге технологических объектов и прилегающих к ним территорий, непосредственно на борту беспилотных летательных аппаратов в режиме реального времени. Более того, показано, что для обеспечения такого анализа в реальном времени необходимо применять аппаратно-реализованные свёрточные нейронные сети. Для интеллектуальной системы компьютерного зрения разработаны архитектуры свёрточных нейронных сетей из перспективных подклассов LeNet5 и YOLO; предложены алгоритмы помехоустойчивого кодирования/декодирования данных при обмене сообщениями между наземной и бортовой компонентами системы компьютерного зрения; разработан оригинальный метод организации вычислений в аппаратных свёрточных нейронных сетях на программируемых логических интегральных схемах, отличающийся от известных использованием унифицированных вычислительных блоков; предложены новые способы параллельных вычислений в слоях таких свёрточных нейронных сетей. Разработана архитектура вычислительного устройства беспилотных летательных аппаратов, включающего блоки аппаратной свёрточной нейронной сети и кодер/декодер данных. Устройство создано на основе системы на кристалле Cyclone V SX компании Altera; получены первые результаты исследования эффективности этого устройства; разработано программное обеспечение наземной компоненты системы компьютерного зрения.The relevance of the research is caused by the necessity to develop modern computer vision systems for monitoring hazardous techno- logical objects of oil and gas industry. The main aim of the research is to develop the intelligent computer vision system for unmanned aerial vehicles, which allows monitoring dangerous technological objects and analyzing the monitoring data in real-time on the board of the unmanned aerial vehicle. Objects: the concept of construction of intelligent computer vision system; new architectures of convolutional neural networks hardware- based using field programmable gate array; the method of unification of computing blocks and ways of parallel calculation in hardware-based convolutional neural networks; algorithms of error-correction encoding and decoding data for exchanging message between ground and airborne components of the intelligent computer vision system. Methods: methods of detection and classification objects in images using convolutional neural networks; convolutional neural network deep learning methods; methods of designing software and hardware systems. Results. We have been analyzed the current state of research in the field of monitoring hazardous technological objects of the oil and gas industry and developed the concept of construction of intelligent computer vision system for unmanned aerial vehicles for monitoring dangerous objects. The idea of analyzing the images, obtained at monitoring of technological objects and surrounding areas, directly onboard of the unmanned aerial vehicle in real time was the base in this concept. Moreover, it is shown that the use of hardware- based convolutional neural networks for providing such analysis in real time is required. The authors developed the convolutional neural networks architectures for computer vision system from promising subclasses LeNet5 and YOLO and proposed the algorithms of error- correction data encoding/decoding for messages exchanging between these components, considering the specifics of ground and air- borne components. The authors developed the original method of organizing calculation in hardware-based convolutional neural net- works using field programmable gate array, which differs from the known ones by using the unified computing blocks and new ways of parallel calculation in layers in these convolutional neural networks. They proposed the architecture of computing device of the unmanned aerial vehicle which includes the blocks of the hardware-based convolutional neural networks and the data encoder/decoder. This device is based on the Altera Cyclone V SX system-on-a-chip. The paper demonstrates the first results of studying the device efficiency. The authors developed the software for the ground component of the computer vision system",Intelligent computer vision system for unmanned aerial vehicles for monitoring technological objects of oil and gas industry,,'National Research Tomsk Polytechnic University',"[{'title': 'Izvestiya Tomskogo Politekhnicheskogo Universiteta Inziniring Georesursov', 'identifiers': ['issn:2413-1830', '2413-1830']}]",10.18799/24131830/2019/11/2346,core
479176085,2021-10-27T00:00:00,"Behavior prediction remains one of the most challenging tasks in the
autonomous vehicle (AV) software stack. Forecasting the future trajectories of
nearby agents plays a critical role in ensuring road safety, as it equips AVs
with the necessary information to plan safe routes of travel. However, these
prediction models are data-driven and trained on data collected in real life
that may not represent the full range of scenarios an AV can encounter. Hence,
it is important that these prediction models are extensively tested in various
test scenarios involving interactive behaviors prior to deployment. To support
this need, we present a simulation-based testing platform which supports (1)
intuitive scenario modeling with a probabilistic programming language called
Scenic, (2) specifying a multi-objective evaluation metric with a partial
priority ordering, (3) falsification of the provided metric, and (4)
parallelization of simulations for scalable testing. As a part of the platform,
we provide a library of 25 Scenic programs that model challenging test
scenarios involving interactive traffic participant behaviors. We demonstrate
the effectiveness and the scalability of our platform by testing a trained
behavior prediction model and searching for failure scenarios.Comment: Accepted to the NeurIPS 2021 Workshop on Machine Learning for
  Autonomous Drivin","A Scenario-Based Platform for Testing Autonomous Vehicle Behavior
  Prediction Models in Simulation",http://arxiv.org/abs/2110.14870,,,,core
427549556,2021-01-01T08:00:00,"With the widespread deployment of sensors and the Internet-of-Things, multi-view data have become more common and publicly available. For example, a self-driving car uses radar, lidar, and camera sensors to collect real-time 3D information to drive safely on the road; disease diagnosis models utilize multiple modalities of neuroimage data, clinical scores, and genetics measurements for disease prediction; object detection techniques prefer object images from different views for high-fidelity recognition. The presence of multiple information sources provides an opportunity of learning better representations to improve performance by analyzing multiple views simultaneously and also poses great challenges for the existing data representation algorithms. First, different views tend to be treated as different domains from different distributions due to the view discrepancy. Second, they often require large-scale labeled data to sufficiently learn such representations, which significantly hinders their adaption into unsupervised learning tasks, and limits their applications into critical domains where obtaining massive labeled data is prohibitively expensive. To enable learning on those domains, this dissertation focuses on robust representation learning-based algorithms to alleviate the view discrepancy of the multi-view data in an unsupervised manner.
Specially, we explore two scenarios upon data association for robust representation learning of multi-view data: First, the samples across different views have a sample-wise association in multi-view data, falling in the multi-view clustering scenario; Second, the samples across different views have a class-wise association, falling in the unsupervised domain adaption scenario, where the discriminant knowledge (representations) of views with labeled data samples are transferred to the views with unlabeled data samples",MULTI-VIEW ROBUST REPRESENTATION LEARNING,https://core.ac.uk/download/427549556.pdf,DigitalCommons@URI,,,core
188391480,2018-01-01T00:00:00,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenarios and at points of intersections in real-time environments. The goal of this research is to develop a new artificial intelligent adaptive controller for autonomous vehicle Pre-Crash system along with vehicle recognition module and tested in MATLAB including some detailed modules. Following tasks were set: finding Objects in sensor Data (LiDAR. RADAR), Speed and Steering control, vehicle Recognition using convolution neural network and Alexnet. In this research paper, we implemented a real-time image/Lidar processing. At the beginning, we presented a real-time system which is composed of comprehensive modules, these modules are 3d object detection, object clustering and search, ground removal, deep learning using convolutional neural networks. Starting with nearest vehicle module our target is to find the nearest ahead car and consider it as our primary obstacle. This paper presents an Adaptive cruise pre-crash system and vehicle recognition. The Adaptive cruise pre-crash system module depends on Deep Learning and LiDAR sensor data, which meant to control the driver reckless behavior on the road by adjusting the vehicle speed to maintain a safe distance from objects ahead (such as cars, humans, bicycle or whatever the object) when the driver tries to raise speed. At the very moment the vehicle recognition module, detects and recognizes the vehicles surrounding to the car",A hybrid liar/radar-based deep learning and vehicle recognition engine for autonomous vehicle Precrash control,https://core.ac.uk/download/188391480.pdf,'Private Company Technology Center',,10.15587/1729-4061.2018.141298,core
387296980,2021-03-25T00:00:00,"In the domain of visual tracking, most deep learning-based trackers highlight
the accuracy but casting aside efficiency. Therefore, their real-world
deployment on mobile platforms like the unmanned aerial vehicle (UAV) is
impeded. In this work, a novel two-stage Siamese network-based method is
proposed for aerial tracking, i.e., stage-1 for high-quality anchor proposal
generation, stage-2 for refining the anchor proposal. Different from
anchor-based methods with numerous pre-defined fixed-sized anchors, our
no-prior method can 1) increase the robustness and generalization to different
objects with various sizes, especially to small, occluded, and fast-moving
objects, under complex scenarios in light of the adaptive anchor generation, 2)
make calculation feasible due to the substantial decrease of anchor numbers. In
addition, compared to anchor-free methods, our framework has better performance
owing to refinement at stage-2. Comprehensive experiments on three benchmarks
have proven the superior performance of our approach, with a speed of around
200 frames/s",Siamese Anchor Proposal Network for High-Speed Aerial Tracking,http://arxiv.org/abs/2012.10706,,,,core
105437653,2016-09-02,"The Experimental Autonomous Vehicle- West (EAVE-West) submersible testbed has been configured for demonstrating a distributable software architecture for Autonomous Undersea Vehicle (AUV) plan execution. Instead of using a machine planner aboard the AUV, plans are represented and then downloaded to the vehicle. This technique obviates the problems associated with planning and, as a result, the real-time response of the AUV can potentially be improved. A review of the architecture is given and the EAVE-West demonstration system is discussed. r.cL0 L- ~  ~  ~  / /A IntroduAtion To potentially reduce costs, reduce personnel risks, and increase overall system performance, undersea vehicle tasks are being automated. Remotely Operated Vehicles (ROVs) have already exploited a certain limited aucomation capability through the use of embeddable microprocessors, real-time software, and Artificial Intelligence techniques [Doeling and Harding 87]. In particular, supervisory controlled or &quot;telerobotic &quot; ROVs have progressively automated vehicle control tasks which previously required the attention of a human operator. By developing structured methodologies that will accelerate and advance the degree of ROV task automation, more flexible autonomous capability can be made possible for future vehicle systems. Underwater ROVs developed from the need to perform such generic missions as undersea search, recovery, and inspection operations. Originally, such unmanned undersea vehicles were basically teleoperators or controlled via a master-slave configuration, and therefore, none of the vehicle tasks were automated. Dr. Tom Sheridan developed a system architecture he called supervisory control, whereafter NOSC demonstrated this supervisory control architecture on an undersea manipulator [Yoerger and Sheridan 831. This means of control was adopted fordeveloping advanced untethered submersibles, as well. Using this system architecture, tasks previously performed manually, couldbe automated. Because a microprocessor could be embedded into a vehicle, a vehicle could be configured to perform well define",EAVE-West: A Testbed For Plan Execution,,,,,core
201126353,2019-06-01T00:00:00,"Airborne Gamma Ray Spectrometry (AGRS) with its important applications such as gathering radiation information of ground surface, geochemistry measuring of the abundance of Potassium, Thorium and Uranium in outer earth layer, environmental and nuclear site surveillance has a key role in the field of nuclear science and human life. The Broyden–Fletcher–Goldfarb–Shanno (BFGS), with its advanced numerical unconstrained nonlinear optimization in collaboration with Artificial Neural Networks (ANNs) provides a noteworthy opportunity for modern AGRS. In this study a new AGRS system empowered by ANN-BFGS has been proposed and evaluated on available empirical AGRS data. To that effect different architectures of adaptive ANN-BFGS were implemented for a sort of published experimental AGRS outputs. The selected approach among of various training methods, with its low iteration cost and non-diagonal scaling allocation is a new powerful algorithm for AGRS data due to its inherent stochastic properties. Experiments were performed by different architectures and trainings, the selected scheme achieved the smallest number of epochs, the minimum Mean Square Error (MSE) and the maximum performance in compare with different types of optimization strategies and algorithms. The proposed method is capable to be implemented on a cost effective and minimum electronic equipment to present its real-time process, which will let it to be used on board a light Unmanned Aerial Vehicle (UAV). The advanced adaptation properties and models of neural network, the training of stochastic process and its implementation on DSP outstands an affordable, reliable and low cost AGRS design. The main outcome of the study shows this method increases the quality of curvature information of AGRS data while cost of the algorithm is reduced in each iteration so the proposed ANN-BFGS is a trustworthy appropriate model for Gamma-ray data reconstruction and analysis based on advanced novel artificial intelligence systems. Keywords: Artificial neural networks, BFGS training algorithm, Airborne gamma ray spectrometry, Nuclear site surveillanc",Numerical evaluation of gamma radiation monitoring,,'Elsevier BV',"[{'title': 'Nuclear Engineering and Technology', 'identifiers': ['issn:1738-5733', '1738-5733']}]",10.1016/j.net.2018.12.020,core
219456675,2017-01-01T00:00:00,"The development of technologies for autonomous vehicle (AV) have seen rapid achievement in the recent years. Commercial carmakers are actively embedding this system in their production and are undergoing tremendous testing in the real world traffic environment. It is one of today’s most challenging topics in the intelligent transportation system (ITS) field in term of reliability as well as accelerating the world’s transition to a sustainable future. The utilization of current sensor technology however indicates some drawbacks where the complexity is high and the cost is extremely huge. This paper reviews the recent sensor technologies and their contributions in becoming part of the autonomous self-driving vehicle system. The ultimate focus is toward reducing the sensor count to just a single camera based on the single modality model. The capability of the sensor to detect and recognize on-the-road obstacles such as overtaking vehicle, pedestrians, signboards, bicycle, road lane marker and road curvature will be discussed. Different feature extraction approach will be reviewed further with the selection of the recent Artificial Intelligent (AI) methods that are being implemented. At the end of this review, the optimal techniques of processing information from single camera system will be discussed and summarized",Single camera object detection for self-driving vehicle: a review,https://core.ac.uk/download/219456675.pdf,Society of Automotive Engineers Malaysia,,,core
16497915,2011-06-10T00:00:00,"Fast software performance is often the focus when developing real-time vision-based control applications for robot simulators. In this paper we have developed a thin, high performance middleware for USARSim and other simulators designed for real-time vision-based control applications. It includes a fast image server providing images in OpenCV, Matlab or web formats and a simple command/sensor processor. The interface has been tested in USARSim with an Unmanned Aerial Vehicle using two control applications; landing using a reinforcement learning algorithm and altitude control using elementary motion detection. The middleware has been found to be fast enough to control the flying robot as well as very easy to set up and use",AltURI: a thin middleware for simulated robot vision applications,https://core.ac.uk/download/16497915.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
154476750,2017-06-22T07:00:00,"Despite the impressive advancements in people detection and tracking, safety is still a key barrier to the deployment of autonomous vehicles in urban environments [1]. For example, in non-autonomous technology, there is an implicit communication between the people crossing the street and the driver to make sure they have communicated their intent to the driver. Therefore, it is crucial for the autonomous car to infer the future intent of the pedestrian quickly. We believe that human body orientation with respect to the camera can help the intelligent unit of the car to anticipate the future movement of the pedestrians. To further improve the safety of pedestrians, it is important to recognize whether they are distracted, carrying a baby, or pushing a shopping cart. Therefore, estimating the fine- grained 3D pose, i.e. (x,y,z)-coordinates of the body joints provides additional information for decision-making units of driverless cars.
In this dissertation, we have proposed a deep learning-based solution to classify the categorized body orientation in still images. We have also proposed an efficient framework based on our body orientation classification scheme to estimate human 3D pose in monocular RGB images.
Furthermore, we have utilized the dynamics of human motion to infer the body orientation in image sequences. To achieve this, we employ a recurrent neural network model to estimate continuous body orientation from the trajectories of body joints in the image plane.
The proposed body orientation and 3D pose estimation framework are tested on the largest 3D pose estimation benchmark, Human3.6m (both in still images and video), and we have proved the efficacy of our approach by benchmarking it against the state-of-the-art approaches.
Another critical feature of self-driving car is to avoid an obstacle. In the current prototypes the car either stops or changes its lane even if it causes other traffic disruptions. However, there are situations when it is preferable to collide with the object, for example a foam box, rather than take an action that could result in a much more serious accident than collision with the object. In this dissertation, for the first time, we have presented a novel method to discriminate between physical properties of these types of objects such as bounciness, elasticity, etc. based on their motion characteristics . The proposed algorithm is tested on synthetic data, and, as a proof of concept, its effectiveness on a limited set of real-world data is demonstrated",Estimation of Human Poses Categories and Physical Object Properties from Motion Trajectories,https://core.ac.uk/download/154476750.pdf,Scholar Commons,,,core
296877876,2018-08-26T17:57:25,"Orientadores: Janito Vaqueiro Ferreira, Alessandro Corrêa VictorinoTese (doutorado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: O desenvolvimento de veículos autônomos capazes de se locomover em ruas urbanas pode proporcionar importantes benefícios na redução de acidentes, no aumentando da qualidade de vida e também na redução de custos. Veículos inteligentes, por exemplo, frequentemente baseiam suas decisões em observações obtidas a partir de vários sensores tais como LIDAR, GPS e câmeras. Atualmente, sensores de câmera têm recebido grande atenção pelo motivo de que eles são de baixo custo, fáceis de utilizar e fornecem dados com rica informação. Ambientes urbanos representam um interessante mas também desafiador cenário neste contexto, onde o traçado das ruas podem ser muito complexos, a presença de objetos tais como árvores, bicicletas, veículos podem gerar observações parciais e também estas observações são muitas vezes ruidosas ou ainda perdidas devido a completas oclusões. Portanto, o processo de percepção por natureza precisa ser capaz de lidar com a incerteza no conhecimento do mundo em torno do veículo. Nesta tese, este problema de percepção é analisado para a condução nos ambientes urbanos associado com a capacidade de realizar um deslocamento seguro baseado no processo de tomada de decisão em navegação autônoma. Projeta-se um sistema de percepção que permita veículos robóticos a trafegar autonomamente nas ruas, sem a necessidade de adaptar a infraestrutura, sem o conhecimento prévio do ambiente e considerando a presença de objetos dinâmicos tais como veículos. Propõe-se um novo método baseado em aprendizado de máquina para extrair o contexto semântico usando um par de imagens estéreo, a qual é vinculada a uma grade de ocupação evidencial que modela as incertezas de um ambiente urbano desconhecido, aplicando a teoria de Dempster-Shafer. Para a tomada de decisão no planejamento do caminho, aplica-se a abordagem dos tentáculos virtuais para gerar possíveis caminhos a partir do centro de referencia do veículo e com base nisto, duas novas estratégias são propostas. Em primeiro, uma nova estratégia para escolher o caminho correto para melhor evitar obstáculos e seguir a tarefa local no contexto da navegação hibrida e, em segundo, um novo controle de malha fechada baseado na odometria visual e o tentáculo virtual é modelado para execução do seguimento de caminho. Finalmente, um completo sistema automotivo integrando os modelos de percepção, planejamento e controle são implementados e validados experimentalmente em condições reais usando um veículo autônomo experimental, onde os resultados mostram que a abordagem desenvolvida realiza com sucesso uma segura navegação local com base em sensores de câmeraAbstract: The development of autonomous vehicles capable of getting around on urban roads can provide important benefits in reducing accidents, in increasing life comfort and also in providing cost savings. Intelligent vehicles for example often base their decisions on observations obtained from various sensors such as LIDAR, GPS and Cameras. Actually, camera sensors have been receiving large attention due to they are cheap, easy to employ and provide rich data information. Inner-city environments represent an interesting but also very challenging scenario in this context, where the road layout may be very complex, the presence of objects such as trees, bicycles, cars might generate partial observations and also these observations are often noisy or even missing due to heavy occlusions. Thus, perception process by nature needs to be able to deal with uncertainties in the knowledge of the world around the car. While highway navigation and autonomous driving using a prior knowledge of the environment have been demonstrating successfully, understanding and navigating general inner-city scenarios with little prior knowledge remains an unsolved problem. In this thesis, this perception problem is analyzed for driving in the inner-city environments associated with the capacity to perform a safe displacement based on decision-making process in autonomous navigation. It is designed a perception system that allows robotic-cars to drive autonomously on roads, without the need to adapt the infrastructure, without requiring previous knowledge of the environment and considering the presence of dynamic objects such as cars. It is proposed a novel method based on machine learning to extract the semantic context using a pair of stereo images, which is merged in an evidential grid to model the uncertainties of an unknown urban environment, applying the Dempster-Shafer theory. To make decisions in path-planning, it is applied the virtual tentacle approach to generate possible paths starting from ego-referenced car and based on it, two news strategies are proposed. First one, a new strategy to select the correct path to better avoid obstacles and to follow the local task in the context of hybrid navigation, and second, a new closed loop control based on visual odometry and virtual tentacle is modeled to path-following execution. Finally, a complete automotive system integrating the perception, path-planning and control modules are implemented and experimentally validated in real situations using an experimental autonomous car, where the results show that the developed approach successfully performs a safe local navigation based on camera sensorsDoutoradoMecanica dos Sólidos e Projeto MecanicoDoutor em Engenharia Mecânic",Percepção do ambiente urbano e navegação usando visão robótica : concepção e implementação aplicado à veículo autônomo,https://core.ac.uk/download/296877876.pdf,[s.n.],,,core
299955648,2019-01-01T00:00:00,"The project calls for the application of engineering and programming knowledge to develop and implement an autonomous vehicle, the Nanyang Venture 11 (NV-11) which will be taking part in the Shell Eco-marathon Asia 2019. Throughout the project, different task had been allocated to collaborate as a team to build the NV-11. A 3-phase Brushless DC (BLDC) electric motor controller that was designed by the previous student had a few flaws which were the instantaneous current surge that cause the components on the circuitry board to be damaged. The implementation of the Soft Start code improved the performance and prevent it from any sudden current spikes occurring. The software kit by Renesas Electronics Corporation was used to program the microprocessor board to determine the clockwise and anti-clockwise rotation of the vehicle wheels. The NVIDIA Jetson TX2 is a supercomputer which is an embedded Artificial Intelligence computing device that acts as the brain on the autonomous vehicle NV-11. It operates using Ubuntu 16.04 Operating System and has the compatibility of using the Robot Operating System (ROS) as a platform that provides a set of software libraries and tools to build an application. A virtual environment was designed as a simulation to test the functionality of the autonomous vehicle using the data from the exterior sensors like the Lidar sensors, Zed camera, and Radar sensors. Using TensorFlow which is an open-source software library for machine learning to gather data for obstacles avoidance and detection. The Tinkerforge IMU Brick 2.0 is a device which is equipped with a 3-axis accelerometer, magnetometer (compass) and gyroscope. It provides the quaternions data which is a mathematical notation for representing orientations and rotations of the vehicle in three dimensions. It was implemented as part of the simulation to determine the exact location in the real world, comparing with the virtual world.Bachelor of Engineering (Electrical and Electronic Engineering",Implementation of electrical and simulation for autonomous driving,,,,,core
103838094,2016-01-20,"Abstract — This paper presents a probabilistic model of ul-trasonic range sensors using backpropagation neural networks trained on experimental data. The sensor model provides the probability of detecting mapped obstacles in the environment, given their position and orientation relative to the transducer. The detection probability can be used to compute the location of an autonomous vehicle from those obstacles that are more likely to be detected. The neural network model is more accurate than other existing approaches, since it captures the typical multilobal detection pattern of ultrasonic transducers. Since the network size is kept small, implementation of the model on a mobile robot can be efficient for real-time navigation. An example that demonstrates how the credence could be incorporated into the extended Kalman filter (EKF) and the numerical values of the final neural network weights are provided in the Appendixes. Index Terms—Kalman filtering, mobile robots, motion plan-ning, neural networks, sonar navigation. I",Modeling of Ultrasonic Range Sensors for Localization of Autonomous Mobile Robots,,,,,core
386380658,2020-12-01T08:00:00,"With the rise of (semi)autonomous vehicles and continuum robotics technology and applications, there has been an increasing interest in controller and haptic interface designs. The presence of nonlinearities in the vehicle dynamics is the main challenge in the selection of control algorithms for real-time regulation and tracking of (semi)autonomous vehicles. Moreover, control of continuum structures with infinite dimensions proves to be difficult due to their complex dynamics plus the soft and flexible nature of the manipulator body. The trajectory tracking and control of automobile and robotic systems requires control algorithms that can effectively deal with the nonlinearities of the system without the need for approximation, modeling uncertainties, and input disturbances. Control strategies based on a linearized model are often inadequate in meeting precise performance requirements. To cope with these challenges, one must consider nonlinear techniques. Nonlinear control systems provide tools and methodologies for enabling the design and realization of (semi)autonomous vehicle and continuum robots with extended specifications based on the operational mission profiles. This dissertation provides an insight into various nonlinear controllers developed for (semi)autonomous vehicles and continuum robots as a guideline for future applications in the automobile and soft robotics field. A comprehensive assessment of the approaches and control strategies, as well as insight into the future areas of research in this field, are presented.First, two vehicle haptic interfaces, including a robotic grip and a joystick, both of which are accompanied by nonlinear sliding mode control, have been developed and studied on a steer-by-wire platform integrated with a virtual reality driving environment. An operator-in-the-loop evaluation that included 30 human test subjects was used to investigate these haptic steering interfaces over a prescribed series of driving maneuvers through real time data logging and post-test questionnaires. A conventional steering wheel with a robust sliding mode controller was used for all the driving events for comparison. Test subjects operated these interfaces for a given track comprised of a double lane-change maneuver and a country road driving event. Subjective and objective results demonstrate that the driver’s experience can be enhanced up to 75.3% with a robotic steering input when compared to the traditional steering wheel during extreme maneuvers such as high-speed driving and sharp turn (e.g., hairpin turn) passing.  Second, a cellphone-inspired portable human-machine-interface (HMI) that incorporated the directional control of the vehicle as well as the brake and throttle functionality into a single holistic device will be presented. A nonlinear adaptive control technique and an optimal control approach based on driver intent were also proposed to accompany the mechatronic system for combined longitudinal and lateral vehicle guidance. Assisting the disabled drivers by excluding extensive arm and leg movements ergonomically, the device has been tested in a driving simulator platform. Human test subjects evaluated the mechatronic system with various control configurations through obstacle avoidance and city road driving test, and a conventional set of steering wheel and pedals were also utilized for comparison. Subjective and objective results from the tests demonstrate that the mobile driving interface with the proposed control scheme can enhance the driver’s performance by up to 55.8% when compared to the traditional driving system during aggressive maneuvers. The system’s superior performance during certain vehicle maneuvers and approval received from the participants demonstrated its potential as an alternative driving adaptation for disabled drivers. Third, a novel strategy is designed for trajectory control of a multi-section continuum robot in three-dimensional space to achieve accurate orientation, curvature, and section length tracking. The formulation connects the continuum manipulator dynamic behavior to a virtual discrete-jointed robot whose degrees of freedom are directly mapped to those of a continuum robot section under the hypothesis of constant curvature. Based on this connection, a computed torque control architecture is developed for the virtual robot, for which inverse kinematics and dynamic equations are constructed and exploited, with appropriate transformations developed for implementation on the continuum robot. The control algorithm is validated in a realistic simulation and implemented on a six degree-of-freedom two-section OctArm continuum manipulator. Both simulation and experimental results show that the proposed method could manage simultaneous extension/contraction, bending, and torsion actions on multi-section continuum robots with decent tracking performance (e.g. steady state arc length and curvature tracking error of 3.3mm and 130mm-1, respectively). Last, semi-autonomous vehicles equipped with assistive control systems may experience degraded lateral behaviors when aggressive driver steering commands compete with high levels of autonomy. This challenge can be mitigated with effective operator intent recognition, which can configure automated systems in context-specific situations where the driver intends to perform a steering maneuver. In this article, an ensemble learning-based driver intent recognition strategy has been developed. A nonlinear model predictive control algorithm has been designed and implemented to generate haptic feedback for lateral vehicle guidance, assisting the drivers in accomplishing their intended action. To validate the framework, operator-in-the-loop testing with 30 human subjects was conducted on a steer-by-wire platform with a virtual reality driving environment. The roadway scenarios included lane change, obstacle avoidance, intersection turns, and highway exit. The automated system with learning-based driver intent recognition was compared to both the automated system with a finite state machine-based driver intent estimator and the automated system without any driver intent prediction for all driving events. Test results demonstrate that semi-autonomous vehicle performance can be enhanced by up to 74.1% with a learning-based intent predictor. The proposed holistic framework that integrates human intelligence, machine learning algorithms, and vehicle control can help solve the driver-system conflict problem leading to safer vehicle operations",Nonlinear Modeling and Control of Driving Interfaces and Continuum Robots for System Performance Gains,https://core.ac.uk/download/386380658.pdf,Clemson University Libraries,,,core
389184160,2021-02-15T00:00:00,"Obstacle detection and target tracking are two major issues for intelligent autonomous vehicles. This paper proposes a new scheme to achieve target tracking and real-time obstacle detection of obstacles based on computer vision. ResNet-18 deep learning neural network is utilized for obstacle detection and Yolo-v3 deep learning neural network is employed for real-time target tracking. These two trained models can be deployed on an autonomous vehicle equipped with an NVIDIA Jetson Nano motherboard. The autonomous vehicle moves to avoid obstacles and follow tracked targets by camera. Adjusting the steering and movement of the autonomous vehicle according to the PID algorithm during the movement, therefore, will help the proposed vehicle achieve stable and precise tracking",Computer vision based obstacle detection and target tracking for autonomous vehicles,,'EDP Sciences',,10.1051/matecconf/202133607004,core
146952543,2013-12-03,"Safety concerns in the operation of autonomous aerial systems require safe-landing protocols be followed during situations where the a mission should be aborted due to mechanical or other failure. On-board cameras provide information that can be used in the determination of potential landing sites, which are continually updated and ranked to prevent injury and minimize damage. Pulse Coupled Neural Networks have been used for the detection of features in images that assist in the classification of vegetation and can be used to minimize damage to the aerial vehicle. However, a significant drawback in the use of PCNNs is that they are computationally expensive and have been more suited to off-line applications on conventional computing architectures. As heterogeneous computing architectures are becoming more common, an OpenCL implementation of a PCNN feature generator is presented and its performance is compared across OpenCL kernels designed for CPU, GPU and FPGA platforms. This comparison examines the compute times required for network convergence under a variety of images obtained during unmanned aerial vehicle trials to determine the plausibility for real-time feature detection",Pulse-coupled neural network performance for real-time identification of vegetation during forced landing,,,,,core
478473866,2021-07-09T00:00:00,"Deep learning based strategies have shown exceptionally huge advancements in accuracy and quick decision making for applications in intelligent or autonomous driving vehicles. Pedestrian recognition, having applications in autonomous or intelligent vehicles, is one of the vital applications of object detection, an area of basic ongoing research in computer vision. From the last couple of decades, pedestrian recognition has played a vital role in numerous real time applications such as collision avoidance systems for smart or intelligent vehicles, intelligent observation cameras, and domestic security frameworks. The proposed methodology of this work suggests utilizing a dash camera to support a model that identifies all humans that might come in the way of a moving autonomous or driven vehicle from images captured with the dash camera. This algorithm proposed model is based on TensorFlow Human Detection API and is compared to the Histograms of Oriented Gradients (HOG) for human detection. Based on the research, the accuracy and efficiency of the proposed model to detect human was much better than to the other models. The defined method in this paper had an accuracy of around 98% as compared to 84% for the Histograms of Oriented Gradients detection model. The proposed model was faster on average as compared to the Histograms of Oriented Gradients detection model for any given picture on the same framework for training to testing because the proposed model took 11 seconds on average for processing one picture whereas the HOG method took more than 40 seconds on average. The proposed model gives “boxes” as outputs around the humans detected in the images and the result of this model is that it is accurately able to recognize pedestrians with the high accuracy of 98%. The samples on which the algorithm was tested are low-quality pictures taken from low-cost cameras, needing exceptionally less computing power, obviating need for expensive components. This low-cost proposed system will thus permit a dash-cam based system fitted with pedestrian detection technology in vehicles to be implemented for automatic vehicle and self-driving vehicle applications",Pedestrian detection using deep learning through a dashcam,,Texas A&M University-Kingsville,,,core
231786118,2017-09-01T00:00:00,"Seoul, Republic of Korea, September, 2017 — Commissioned by the curators of the inaugural Seoul Biennale of Architecture and Urbanism, open September 2, 2017 through December ??, 2017 at the Donuimun Museum Village, Seoul, SK. Driver Less Vision presents the immersive experience of becoming an autonomous, self-driving vehicle. Created by Urtzi Grau (Fake Industries Architectural Agonism), Guillermo Fernandez-Abascal with Perlin Studios, the project was produced with virtual reality video and architectural design by the New York-based teams, and installed in a 25’ dome at the Seoul Biennale. Driver Less Vision examines the tension and reality of AI and humans merging and diverging as they negotiate Seoul's unique urban landscape—challenging us to consider how we can design cities for the future of autonomous vehicles. Driver Less Vision aims to generate empathy between humans and non-humans, to construct the trust required for negotiations that will settle how we will live together. By overlapping human and machine’s perceptions, the installation helps to identify the areas of the city that will need to be redesigned in the immediate future. Driver Less Vision is the immersive experience of becoming an autonomous, self-driving vehicle. It explores the untapped conflicts and disruptive effects on the built environment caused by the deployment of technologies for autonomous mobility. Currently, the visual stimuli that organizes traffic is designed for human perception. The arrival of driverless cars entails the emergence of a omnidirectional gaze that is required to negotiate existing visual codes. To assume that driverless cars will fully adapt to future conditions of the city, however, neglects the history of transformations in urban streetscapes associated with changes in vehicular technologies. Driver Less Vision is an attempt to understand how driverless cars will change the city by immersing the audience in an urban journey through the car’s point of view, seeing the streets of Seoul through overlapping and dissonant perceptions. The project was produced for the Seoul Biennale of Architecture and Urbanism in 2017, utilizing an eight meter diameter dome with 360 visuals developed with the generous support of University of Technology Sydney, Rice University and Ocular Robotics",Driver Less Vision,http://hdl.handle.net/10453/133482,Actar,,,core
479182533,2021-07-01T00:00:00,"Unmanned Aerial Vehicle (UAV) networks are an emerging technology, useful not only for the military, but also for public and civil purposes. Their versatility provides advantages in situations where an existing network cannot support all requirements of its users, either because of an exceptionally big number of users, or because of the failure of one or more ground base stations. Networks of UAVs can reinforce these cellular networks where needed, redirecting the traffic to available ground stations. Using machine learning algorithms to predict overloaded traffic areas, we propose a UAV positioning algorithm responsible for determining suitable positions for the UAVs, with the objective of a more balanced redistribution of traffic, to avoid saturated base stations and decrease the number of users without a connection. The tests performed with real data of user connections through base stations show that, in less restrictive network conditions, the algorithm to dynamically place the UAVs performs significantly better than in more restrictive conditions, reducing significantly the number of users without a connection. We also conclude that the accuracy of the prediction is a very important factor, not only in the reduction of users without a connection, but also on the number of UAVs deployed",Machine Learning for the Dynamic Positioning of UAVs for Extended Connectivity,,'MDPI AG',"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",10.3390/s21134618,core
300011537,2019-01-01T00:00:00,"The Unmanned Aerial Vehicle (UAV) is one of the technologies that is constantly evolving in order to be implemented for more applications. In the past, it is only used for military application for combat purposes, where the UAV will carry aircraft ordnance such as missiles for air strikes. As the technology for UAV advances and the cost of development reduces, it becomes a technology that is readily available for the common people.
Now, UAV can be applied in more areas, such as photography, delivery and critical missions such as Search & Rescue. One of the techniques to further improve the performance and utility of UAV is to automize it and giving it the ability to operate on its own. While research has been done on the potential for autonomous UAV, the challenge has been to provide a stable and consistent controller for it. Due to the complexity of the hardware of UAV and the directions that it is required to operate on, it is difficult and time consuming to tune and optimize the UAV to operate perfectly under every condition. 
This project explores the performance of Parsimonious Automatic Controller (PAC) when implemented on an autonomous UAV and compares the result with the commonly used linear Proportional-Integral-Derivative (PID) controller. With the application of machine learning implemented into PAC, it reduces the parameters that is required while improving the performance of the controller in the process.
By testing and simulating the use of this controller onto the autonomous UAV, it determines the viability of implementing the UAV for real-world missions. It also opens the possibility of implementing it on other platforms or systems and potential improving the performance of it.Bachelor of Engineering (Computer Engineering",Autonomous controller for unmanned aerial vehicle,,,,,core
289968510,2020,"Today, Artificial Intelligence is one of the most important technologies, ubiquitous in our daily lives. Deep Neural Networks (DNN's) have come up as state of art for various machine intelligence applications such as object detection, image classification, face recognition and performs myriad of activities with exceptional prediction accuracy. AI in this contemporary world is moving towards embedded platforms for inference on the edge. This is essential to avoid latency, enhance data security and realize real-time performance. However, these DNN algorithms are computational and memory intensive. Consequently, exploiting immense energy, compute resources and memory-bandwidth making it difficult to be deployed in embedded devices. To solve this problem and realize an on-device AI acceleration, dedicated energy-efficient hardware accelerators are paramount. This thesis involves the implementation of such a dedicated deep learning accelerator on the FPGA. The NVIDIA's Deep Learning Accelerator (NVDLA), is encompassed in this research to explore SoC designs for integrated inference acceleration. NVDLA, an open-source architecture, standardizes deep learning inference acceleration on hardware. It optimizes inference acceleration all across the full stack from application through hardware to achieve energy efficiency synergy with the demanding throughput requirements. Therefore, the following thesis probes into the NVDLA framework to perceive the consistent workflow across the whole hardware-software programming hierarchies. Besides, the hardware design parameters, optimization features and system configurations of the NVDLA systems are analyzed for efficient implementations. Also, a comparative study of the diverse NVDLA SoC implementations (nv\_small and nv\_medium) with respect to performance metrics such as power, area, and throughput are discussed. Our approach engages prototyping of Nvidia’s Deep Learning Accelerator on a Zynq Ultrascale+ ZCU104 FPGA to examine its system functionality. The Hardware design of the system is carried out using Xilinx's Vivado Design Suite 2018.3 in Verilog. While the on-device software runs Linux kernel 4.14 on Zynq MPSoC. Thus, the software ecosystem is built with PetaLinux tools from Xilinx. The entire system architecture is validated using the pre-built regression tests that verify individual CNN layers. Besides these NVDLA hardware design also runs pre-compiled AlexNet as a benchmark for performance evaluation and comparisonToday, Artificial Intelligence is at the edge. This edge or endpoint device is becoming more sophisticated with the evolution of Internet of Things (IoT) and 5G. For instance, these devices are employed in different applications such as autonomous cars, drones, and other IoT gadgets. At present, a self-driving car is a data center on wheels, a drone is a data center on wings as well as robots are data centers with arms and legs. All these mechanisms collect vast real-world information that demands to be processed in real-time. Here in these applications, there is no time to send data to the cloud for processing and wait for action. As the decision making needs to be instantaneous. There is a shift in transforming the processing to the edge devices. The edge acceleration brings computation and data storage closer to the device. With the evolution of specialized hardware’s providing increased computational capabilities, the AI models are processed on the edge. As a result, the overall system latency gets reduced, the bandwidth costs for data transfers are lowered and the data processing is done locally enhances privacy concerns. For example, autonomous cars require a spontaneous reaction (in seconds) to avoid potential hazards on the road. Consider the situation where a self-driving car is collecting real word information like images, videos, in this case, assume it’s sensing for a stop sign. If the system sends the specific image information to the cloud for processing and waits for a decision to stop. By that response time, the autonomous vehicle could have already blown through the stop sign running over several people. Therefore, it is paramount to process the data in real-time which could be accomplished using dedicated hardware for processing locally. This thesis primarily explores those hardware architectures for efficient processing of AI algorithms and their corresponding software execution environment setup. The particular thesis was carried out as a joint collaboration between Ericsson and Lund University. Here Nvidia’s Deep Learning Accelerator architecture is engaged as a target to comprehend the complete system incorporating a hardware-software co-design. The particular architecture is an essential characteristic of NVIDIA’s Xavier Drive chip which is utilized in their autonomous drive platforms. This thesis is addressed to a variety of audiences who are passionate about Deep Learning, Computer Architecture, and System-on-Chip Design. The thesis illustrates a comprehensive implementation of an AI accelerator to envision AI processing on the edge",Implementation of a Deep Learning Inference Accelerator on the FPGA.,,Lunds universitet/Institutionen för elektro- och informationsteknik,,,core
387272984,2020-11-03T00:00:00,"In this paper, a novel communication framework that uses an unmanned aerial
vehicle (UAV)-carried intelligent reflector (IR) is proposed to enhance
multi-user downlink transmissions over millimeter wave (mmWave) frequencies. In
order to maximize the downlink sum-rate, the optimal precoding matrix (at the
base station) and reflection coefficient (at the IR) are jointly derived. Next,
to address the uncertainty of mmWave channels and maintain line-of-sight links
in a real-time manner, a distributional reinforcement learning approach, based
on quantile regression optimization, is proposed to learn the propagation
environment of mmWave communications, and, then, optimize the location of the
UAV-IR so as to maximize the long-term downlink communication capacity.
Simulation results show that the proposed learning-based deployment of the
UAV-IR yields a significant advantage, compared to a non-learning UAV-IR, a
static IR, and a direct transmission schemes, in terms of the average data rate
and the achievable line-of-sight probability of downlink mmWave communications","Distributional Reinforcement Learning for mmWave Communications with
  Intelligent Reflectors on a UAV",http://arxiv.org/abs/2011.01840,,,,core
201411232,2019-01-01T00:00:00,"Unmanned aerial vehicle (UAV)-based spraying systems have recently become important for the precision application of pesticides, using machine learning approaches. Therefore, the objective of this research was to develop a machine learning system that has the advantages of high computational speed and good accuracy for recognizing spray and non-spray areas for UAV-based sprayers. A machine learning system was developed by using the mutual subspace method (MSM) for images collected from a UAV. Two target lands: agricultural croplands and orchard areas, were considered in building two classifiers for distinguishing spray and non-spray areas. The field experiments were conducted in target areas to train and test the system by using a commercial UAV (DJI Phantom 3 Pro) with an onboard 4K camera. The images were collected from low (5 m) and high (15 m) altitudes for croplands and orchards, respectively. The recognition system was divided into offline and online systems. In the offline recognition system, 74.4% accuracy was obtained for the classifiers in recognizing spray and non-spray areas for croplands. In the case of orchards, the average classifier recognition accuracy of spray and non-spray areas was 77%. On the other hand, the online recognition system performance had an average accuracy of 65.1% for croplands, and 75.1% for orchards. The computational time for the online recognition system was minimal, with an average of 0.0031 s for classifier recognition. The developed machine learning system had an average recognition accuracy of 70%, which can be implemented in an autonomous UAV spray system for recognizing spray and non-spray areas for real-time applications",Development of a Recognition System for Spraying Areas from Unmanned Aerial Vehicles Using a Machine Learning Approach,,'MDPI AG',"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",10.3390/s19020313,core
200759691,2019-06-01T00:00:00,"Critical Evaluation of Machine Learning Research Aimed at Improving Lie Detection Accuracy (Lynette Gabonthone)	1



A Critical Evaluation of Current Published Face Recognition Systems Research Aimed at Improving Security for ATM transactions (Edith Gabotshajwe)	7



An Evaluation of Current Research Aimed at Improving Network Security in Internet of Things (IoT) (Tumisang Mogotsi)	13



Evaluating the currently proposed techniques to secure Software Defined Networks from Denial of Service (DDoS/DoS) attacks (Mohammad Fakhar Iqbal)	21



Evaluation of Current Load Balancing Techniques in a Software Defined Network Aimed at Improving Quality of Service (Mohammad Falaq Iqbal)	29



An Analytical Review of Published Face Recognition Research Aimed at Improving Accuracy in Identification (Botlhe T Moitho)	35



Evaluation Of Current Security Measures Used In Automated Teller Machines (ATM) (Khumo Setlalekgosi)	43



An In-Depth Evaluation of Current Limitations in Autonomous Vehicle Object Detection Systems (Connor William Reed Smith)	49



Critical Evaluation of e-learning and ICT Methodologies used to help those with Learning Difficulties (Darren Tinmouth)	55



Analysis of Current Virtual Reality Methods to Enhance Learning in Education (Adam Wilson)	61



An Evaluation of Current Research into Machine Learning Aimed To Improve Weather Prediction (Adam Flatters)	6",Selected Computing Research Papers Volume 8 June 2019,https://core.ac.uk/download/200759691.pdf,University of Sunderland,,,core
129584156,2017-06-22T07:00:00,"Despite the impressive advancements in people detection and tracking, safety is still a key barrier to the deployment of autonomous vehicles in urban environments [1]. For example, in non-autonomous technology, there is an implicit communication between the people crossing the street and the driver to make sure they have communicated their intent to the driver. Therefore, it is crucial for the autonomous car to infer the future intent of the pedestrian quickly. We believe that human body orientation with respect to the camera can help the intelligent unit of the car to anticipate the future movement of the pedestrians. To further improve the safety of pedestrians, it is important to recognize whether they are distracted, carrying a baby, or pushing a shopping cart. Therefore, estimating the fine- grained 3D pose, i.e. (x,y,z)-coordinates of the body joints provides additional information for decision-making units of driverless cars.
In this dissertation, we have proposed a deep learning-based solution to classify the categorized body orientation in still images. We have also proposed an efficient framework based on our body orientation classification scheme to estimate human 3D pose in monocular RGB images.
Furthermore, we have utilized the dynamics of human motion to infer the body orientation in image sequences. To achieve this, we employ a recurrent neural network model to estimate continuous body orientation from the trajectories of body joints in the image plane.
The proposed body orientation and 3D pose estimation framework are tested on the largest 3D pose estimation benchmark, Human3.6m (both in still images and video), and we have proved the efficacy of our approach by benchmarking it against the state-of-the-art approaches.
Another critical feature of self-driving car is to avoid an obstacle. In the current prototypes the car either stops or changes its lane even if it causes other traffic disruptions. However, there are situations when it is preferable to collide with the object, for example a foam box, rather than take an action that could result in a much more serious accident than collision with the object. In this dissertation, for the first time, we have presented a novel method to discriminate between physical properties of these types of objects such as bounciness, elasticity, etc. based on their motion characteristics . The proposed algorithm is tested on synthetic data, and, as a proof of concept, its effectiveness on a limited set of real-world data is demonstrated",Estimation of Human Poses Categories and Physical Object Properties from Motion Trajectories,https://core.ac.uk/download/129584156.pdf,Scholar Commons,,,core
428345889,2021-04-27T00:00:00,"An Unmanned Aerial Vehicle (UAV) is an aircraft that operates without a human on-board and can be flown autonomously or controlled remotely. Due to its unmanned operation, UAVs need technologies so they can not only fly autonomously, but also communicate with base stations, flight controllers, computers, devices or even other UAVs. Traditionally, UAVs operate within unlicensed spectrum bands, competing against the increasing number of mobile devices and other wireless networks. This use could lead to interference that affect UAVs communication and problems with overcrowded spectrum. Cognitive Radio (CR) presents itself as a promising technology to solve these problems. CR provides a smart wireless communication which, instead of using a transmission frequency defined in the hardware, uses software defined radio. This allows CR to adapt its transmission frequency in a smart way, using free transmission channels and/or choosing them accordingly with the applications requirements. The combination of UAVs and CR can be used in missions where the conventional UAVs face limitations due to communication problems. Moreover, CR is considered a key enabler for adequately deploying communication paradigms that require high connectivity, such as Smart Cities, 5G, Internet of Things (IoT) and, thus, Internet of Flying Things (IoFT). Though both CR and UAVs are well-established fields of research, the combination of these two elements is little explored in literature. Therefore, this work identifies gaps and opportunities, as well as challenges on the field. Furthermore, this work contributes to the progress regarding the integration of CR and UAVs. To do so, this work presents the definition of CR technologies, as well as their integration on a real mission of data collection. This works results differ to the others on the literature in terms of, for example, highlighting the limitation in real scenario of traditionally deployed Machine Learning algorithms using simulated data on the field.Um Veículo Aéreo Não Tripulado (VANT) é uma aeronave que opera sem um humano a bordo, podendo voar de forma autônoma ou sendo pilotada remotamente. Devido à ausência de um piloto, os VANTs necessitam de tecnologias não só para que possam voar de forma autônoma, mas também de tecnologias de comunicação robustas que permitam a eles se comunicarem, seja com estações de base, controladores de tráfego aéreo, computadores, dispositivos, ou até mesmo com outros VANTs. Tradicionalmente, os VANTs operam em bandas de espectro não licenciadas, concorrendo com o crescente número de dispositivos móveis e com outras redes sem fio. Esse uso pode levar a interferências que afetam a comunicação dos VANTs e, além disso, pode levá-los a enfrentar o problema da superlotação de espectro. Rádio Cognitivo (RC) apresenta-se como uma tecnologia promissora para a solução desses problemas. RC permite uma comunicação sem fio inteligente que, em vez de usar uma frequência de transmissão definida no hardware, utiliza software para esta definição. Isto possibilita ao RC adaptar de forma inteligente sua frequência de transmissão, utilizando canais de transmissão livres e/ou escolhendo-os de acordo com os requisitos da aplicação. A integração de VANTs e RC pode ser empregada em missões nas quais VANTs convencionais enfrentam limitações devido a problemas de comunicação. Além disso, RC é considerado um facilitador chave para a implantação satisfatória de paradigmas de comunicação que demandam alta conectividade, tais como Cidades Inteligentes, 5G, Internet das Coisas (IoT) e, consequentemente, Internet das Coisas Voadoras (IoFT). Apesar de RC e VANTs serem áreas de pesquisa bem estabelecidas, a integração dos dois é pouco explorada na literatura. Portanto, este trabalho contribui nos avanços da integração de RC a um VANT. Neste trabalho foram identificadas as lacunas e oportundiades, bem como os desafios na área. Além disso, foram definidas as tecnologias de RC, bem como a integração do mesmo em uma missão real de coleta de dados, obtendo resultados marcantes em relação aos da literatura, como no caso de algoritmos de Aprendizagem de Máquina, tradicionalmente empregados com dados simulados, mas que demonstraram limitação quando utilizados com dados reais coletados neste trabalho",Integrando rádio cognitivo a veículos aéreos não tripulados,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,10.11606/D.55.2021.tde-27042021-125734,core
20851133,1997,"this article, we describe and develop methodologies for mod- eling and transferring human control strategy (HCS). This research has potential application in a variety of areas such as the Intelligent Vehicle Highway System (IVHS), human-machine interfacing, real-time training, space telerobotics, and agile manufacturing. We specifically address the following issues: (1) how to efficiently model human control strategy through learning cascade neural networks, (2) how to select state inputs in order to generate reliable models, (3) how to validate the computed models through an independent, Hidden Markov Model-based procedure, and (4) how to effectively transfer human control strategy. We have implemented this approach experimentally in the real-time control of a human driving simulator, and are working to transfer these methodologies for the control of an autonomous vehicle and a mobile robot. In providing a framework for abstracting computational models of human skill, we expect to facilitate analysis of human control, the development of humanlike intelligent machines, improved human-robot coordination, and the transfer of skill from one  human to anothe","Human Control Strategy: Abstraction, Verification, and Replication",,,,10.1109/37.621469,core
24733853,2008-04-02,"The emerging area of intelligent unmanned aerial vehicle (UAV) research has shown rapid development in recent years and offers a great number of research challenges for artificial intelligence and knowledge representation. Much previous research has focused on low-level control capability with the goal of developing controllers which support the autonomous flight of a UAV from one way-point to another. The most common type of mission scenario involves placing sensor payloads in position for data collection tasks where the data is eventually processed off-line or in real-time by ground personnel. Use of UAVs and mission tasks such as these have become increasingly more important in recent conflict situations and are predicted to play increasingly more important roles in any future conflicts. Intelligent UAVs will play an equally important role in civil applications. For both military and civil applications, there is a desire to develop more sophisticated UAV platforms where the emphasis is placed on development of intelligent capabilities and on abilities to interact with human operators and additional robotic platforms. Focus in research has moved from low-level control towards a combination of low-level and decision-level control integrated in sophisticated software architectures. These in turn, should also integrate well with larger network-centric based C 4 I 2 systems. Such platforms are a prerequisite for supporting the capabilities required for the increasingly more complex mission tasks on the horizon and an ideal testbed for the development and integration of distributed AI technologies. The WITAS 1 Unmanned Aerial Vehicle Project (Dohert",In Proceedings of the the 9th International Conference on Principles of Knowledge Representation and Reasoning.,,,,,core
287906774,2007-01-01T08:00:00,"This Thesis focuses on the research, implementation, and empirical testing of a real-time (RT) Unmanned Aerial Vehicle (UAV) guidance system—the Real-Time Path Planner (RTPP). The RTPP was created for the following reasons: (1) proof-of-concept, i.e., to verify the real-world feasibility of developing a real-time guidance system by using AI techniques; (2) bench-marking, i.e., to create a platform for using the most promising Artificial Intelligence (AI) theories; (3) to show that a real-time guidance system is beneficial to existing Department of Defense (DoD) Target Command and Control Systems (TCCS). In this study, the RTPP undergoes empirical testing on an operational DoD TCCS called the Drone Formation Control System (DFCS). This testing is conducted by linking the RTPP to the DFCS real-time computer network. The RTPP provides flight-patterns to be used in 6-Degree Of Freedom (DOF) flight simulations by the DFCS Guidance, Navigation, and Control (GNC) systems. At present, guidance systems used by the DoD targets community in missions (i.e. the remote operation of aerial vehicles) require specially trained personnel to develop flight-patterns (i.e. precisely defined flight trajectories) prior to conducting missions. This process is constrictive and static because it forces missions to be performed as specified by these pre-launch generated flight-patterns. These limitations make missions extremely inflexible. This study proposes a solution that alleviates this problem. The solution is a real-time guidance system that allows DoD TCCS personnel (e.g. the project engineer) to make changes to flight profiles in real-time. The RTPP is a prototype of this solution. It provides a user-friendly computer interface for mission operators to safely guide and control target presentation locations in real-time. This easy-to-use interface is operated by using a mouse and keyboard to interact with its Graphical User Interface (GUI). The mouse can be used to select the flight trajectory\u27s start and destination locations on the terrain map that is displayed within the GUI. The keyboard can be used for the same purpose but it is required for entering the other flight trajectory parameters (e.g., those associated with turn radius constraints). The user-entered UAV parameters are passed to the main module of the RTPP—the A* algorithm. Then the RTPP provides either flyable flight trajectories or no guidance at all (which occurs only when no solution exists). The returned flight-patterns have the attributes being flyable (i.e. minimal distance, and safe). The RTPP can be used in obstacle rich environments, provided the obstacles are static (e.g. mountains and other natural high elevation terrain). If the obstacles are man-made, their locations and MSL elevations must be entered into the RTPP. The RTPP uses files generated by the Terrain Map Creator (TMC), which is another product of this work. The TMC is a program that queries a high resolution environmental database for recording to a file the locations and elevations of the terrain over which the mission will be performed. In conclusion, the RTPP is used as the platform for testing the A* algorithm, which is the AI theory that is the recommended as the core technological solution. Its recommendation is verified by research, technical feasibility, analysis, empirical testing, and simulations. Hence, this Thesis demonstrates that DoD TCCS can benefit from innovations provided by AI theory",Unmanned Aerial Vehicle real-time guidance system via state-space heuristic search,,ScholarWorks@UTEP,,,core
93944408,2018-02-27T00:00:00,"Landing an unmanned aerial vehicle (UAV) on a ground marker is an open
problem despite the effort of the research community. Previous attempts mostly
focused on the analysis of hand-crafted geometric features and the use of
external sensors in order to allow the vehicle to approach the land-pad. In
this article, we propose a method based on deep reinforcement learning that
only requires low-resolution images taken from a down-looking camera in order
to identify the position of the marker and land the UAV on it. The proposed
approach is based on a hierarchy of Deep Q-Networks (DQNs) used as high-level
control policy for the navigation toward the marker. We implemented different
technical solutions, such as the combination of vanilla and double DQNs, and a
partitioned buffer replay. Using domain randomization we trained the vehicle on
uniform textures and we tested it on a large variety of simulated and
real-world environments. The overall performance is comparable with a
state-of-the-art algorithm and human pilots.Comment: The actual copy of the manuscript is a revised version resubmitted to
  IRO",Autonomous Quadrotor Landing using Deep Reinforcement Learning,http://arxiv.org/abs/1709.03339,,,,core
200835679,2019-05-09T00:00:00,"Deep learning models are known to solve classification and regression
problems by employing a number of epoch and training samples on a large dataset
with optimal accuracy. However, that doesn't mean they are attack-proof or
unexposed to vulnerabilities. Newly deployed systems particularly on a public
environment (i.e public networks) are vulnerable to attacks from various
entities. Moreover, published research on deep learning systems (Goodfellow et
al., 2014) have determined a significant number of attacks points and a wide
array of attack surface that has evidence of exploitation from adversarial
examples. Successful exploit on these systems could lead to critical real world
repercussions. For instance, (1) an adversarial attack on a self-driving car
running a deep reinforcement learning system yields a direct misclassification
on humans causing untoward accidents.(2) a self-driving vehicle misreading a
red light signal may cause the car to crash to another car (3)
misclassification of a pedestrian lane as an intersection lane that could lead
to car crashes. This is just the tip of the iceberg, computer vision deployment
are not entirely focused on self-driving cars but on many other areas as well -
that would have definitive impact on the real-world. These vulnerabilities must
be mitigated at an early stage of development. It is imperative to develop and
implement baseline security standards at a global level prior to real-world
deployment.Comment: 10 pages 5 figure","Mitigating Deep Learning Vulnerabilities from Adversarial Examples
  Attack in the Cybersecurity Domain",http://arxiv.org/abs/1905.03517,,,,core
24458220,2007,"In modern urban settings, automobile traffic and collisions lead to endless frustration as well as significant loss of life, property, and productivity. Recent advances in artificial intelligence suggest that autonomous vehicle navigation may soon be a reality. In previous work, we have demonstrated that a reservation-based approach can efficiently and safely govern interactions of multiple autonomous vehicles at intersections. Such an approach alleviates many traditional problems associated with intersections, in terms of both safety and efficiency. However, the system relies on all vehicles being equipped with the requisite technology — a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we extend this system to allow for incremental deployability. The modified system is able to accommodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in our previous work. Finally, we develop a method for switching between various human-usable configurations while the system is running, in order to facilitate an even smoother transition. The work is fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to its effectiveness. ",Sharing the road: Autonomous vehicles meet human drivers,,,,,core
286465896,2017-01-01T00:00:00,"Nel rilievo in ambito archeologico, per la presenza di elementi curvi e di parti aggettanti, è spesso necessario l’utilizzo di sistemi di acquisizione che permettano di ottenere una misura celere, dettagliata e di supporto alle tecniche tradizionali. L’utilizzo di sistemi di acquisizione fotogrammetrici da SAPR (Sistemi Aeromobili a Pilotaggio Remoto), unite all’utilizzo di algoritmi di Computer Vision, permettono la realizzazione di modelli tridimensionali ai quali vengono applicate texture foto-realistiche derivanti da immagini fotografiche acquisite in volo. Il caso studio che presentiamo è il rilievo fotogrammetrico del Ponte Rotto ad Apice (Bn). Per il rilievo aerofotogrammetrico è stato utilizzato un UAV (Unmanned Aerial Vehicle), con peso totale al decollo inferiore ai 2 kg (art. 12, Regolamento ENAC del 16 luglio 2015). Le immagini acquisite sono state elaborate con un software con tecnologia Structure From Motion; il modello è georeferito per mezzo di sei GCP (Ground Control Points), misurati con ricevitori GNSS (Global Navigation Satellite Systems) in modalità nRTK (Network Real Time Kinematic)",L’uso di SAPR per la documentazione archeologica: il caso studio del Ponte Rotto ad Apice (Bn),,ASITA,,,core
215400573,2019-05-01T07:00:00,"In this thesis, a real-time and low-cost solution to the autonomous condition assessment of pavement is proposed using deep learning, Unmanned Aerial Vehicle (UAV) and Raspberry Pi tiny computer technologies, which makes roads maintenance and renovation management more efficient and cost effective. A comparison study was conducted to compare the performance of seven different combinations of meta-architectures for pavement distress classification.  It was observed that real-time object detection architecture SSD with MobileNet feature extractor is the best combination for real-time defect detection to be used by tiny computers. A low-cost Raspberry Pi smart defect detector camera was configured using the trained SSD MobileNet v1, which can be deployed with UAV for real-time and remote pavement condition assessment. The preliminary results show that the smart pavement detector camera achieves an accuracy of 60% at 1.2 frames per second in raspberry pi and 96% at 13.8 frames per second in CPU-based computer",Low-cost deep learning UAV and Raspberry Pi solution to real time pavement condition assessment,https://core.ac.uk/download/215400573.pdf,UTC Scholar,,,core
232847780,2018-04-18T07:00:00,"With the age of automation coming to machines, self driving cars will turn from a fantasy to a reality in the next decade. These machines will utilize neural networks to learn from pilot drives to ultimately be able to drive completely autonomously on any road they are put on. This project demonstrates a small fraction of the technology that goes behind a self driving car with an implementation on a 1/16th scale RC car. Using the open source software ‘Donkey Car’ we were able to turn an RC car into a self driving car that get more intelligent every time it drives. Mounted above the car is a 3D printed roll cage which houses a fisheye lense camera (for image recognition) raspberry PI 3 Model B (for the neural networks) and a Servo Controller(to control the throttle and steering). As a demonstration during the presentation, the car will drive simultaneously around a track avoiding obstacles, following street laws and remaining between the lines of the road",Autonomous Self-Driving RC Car,,eCommons,,,core
334898971,2019-12-31T00:00:00,"This paper illustrates the MIR (Mobile Intelligent Robotics) Vehicle: a
feasible option of transforming an electric ride-on-car into a modular Graphics
Processing Unit (GPU) powered autonomous platform equipped with the capability
that supports test and deployment of various intelligent autonomous vehicles
algorithms. To use a platform for research, two components must be provided:
perception and control. The sensors such as incremental encoders, an Inertial
Measurement Unit (IMU), a camera, and a LIght Detection And Ranging (LIDAR)
must be able to be installed on the platform to add the capability of
environmental perception. A microcontroller-powered control box is designed to
properly respond to the environmental changes by regulating drive and steering
motors. This drive-by-wire capability is controlled by a GPU powered laptop
computer where high-level perception algorithms are processed and complex
actions are generated by various methods including behavior cloning using deep
neural networks. The main goal of this paper is to provide an adequate and
comprehensive approach for fabricating a cost-effective platform that would
contribute to the research quality from the wider community. The proposed
platform is to use a modular and hierarchical software architecture where the
lower and simpler motor controls are taken care of by microcontroller programs,
and the higher and complex algorithms are processed by a GPU powered laptop
computer. The platform uses the Robot Operating System (ROS) as middleware to
maintain the modularity of the perceptions and decision-making modules. It is
expected that the level three and above autonomous vehicle systems and Advanced
Driver Assistance Systems (ADAS) can be tested on and deployed to the platform
with a decent real-time system behavior due to the capabilities and
affordability of the proposed platform.Comment: 20 pages, 16 figure","MIR-Vehicle: Cost-Effective Research Platform for Autonomous Vehicle
  Applications",http://arxiv.org/abs/2001.00048,,,,core
327254868,2020-08-03T00:00:00,"An accurate and rapid-response perception system is fundamental for
autonomous vehicles to operate safely. 3D object detection methods handle point
clouds given by LiDAR sensors to provide accurate depth and position
information for each detection, together with its dimensions and
classification. The information is then used to track vehicles and other
obstacles in the surroundings of the autonomous vehicle, and also to feed
control units that guarantee collision avoidance and motion planning. Nowadays,
object detection systems can be divided into two main categories. The first
ones are the geometric based, which retrieve the obstacles using geometric and
morphological operations on the 3D points. The seconds are the deep
learning-based, which process the 3D points, or an elaboration of the 3D
point-cloud, with deep learning techniques to retrieve a set of obstacles. This
paper presents a comparison between those two approaches, presenting one
implementation of each class on a real autonomous vehicle. Accuracy of the
estimates of the algorithms has been evaluated with experimental tests carried
in the Monza ENI circuit. The position of the ego vehicle and the obstacle is
given by GPS sensors with RTK correction, which guarantees an accurate ground
truth for the comparison. Both algorithms have been implemented on ROS and run
on a consumer laptop",LiDAR point-cloud processing based on projection methods: a comparison,http://arxiv.org/abs/2008.00706,,,,core
417758260,2021-03-25T00:00:00,"Autonomous vehicles are increasingly becoming a necessary trend towards building the smart cities of the future. Numerous proposals have been presented in recent years to tackle particular aspects of the working pipeline towards creating a functional end-to-end system, such as object detection, tracking, path planning, sentiment or intent detection, amongst others. Nevertheless, few efforts have been made to systematically compile all of these systems into a single proposal that also considers the real challenges these systems will have on the road, such as real-time computation, hardware capabilities, etc. This paper reviews the latest techniques towards creating our own end-to-end autonomous vehicle system, considering the state-of-the-art methods on object detection, and the possible incorporation of distributed systems and parallelization to deploy these methods. Our findings show that while techniques such as convolutional neural networks, recurrent neural networks, and long short-term memory can effectively handle the initial detection and path planning tasks, more efforts are required to implement cloud computing to reduce the computational time that these methods demand. Additionally, we have mapped different strategies to handle the parallelization task, both within and between the networks","Object detection, distributed cloud computing and parallelization techniques for autonomous driving systems.",https://core.ac.uk/download/417758260.pdf,'MDPI AG',,10.3390/app11072925,core
475185455,2019-11-15T08:00:00,"Recent scholars have developed a number of stochastic car-following          models that have successfully captured driver behavior uncertainties          and reproduced stochastic traffic oscillation propagation. While elegant          frequency domain analytical methods are available for stability analysis          of classic deterministic linear car-following models, there lacks          an analytical method for quantifying the stability performance of          their peer stochastic models and theoretically proving oscillation          features observed in the real world. To fill this methodological gap,          this study proposes a novel analytical method that measures traffic          oscillation magnitudes and reveals oscillation characteristics of          stochastic linear car-following models. We investigate a general class          of stochastic linear car-following models that contain a linear car-following          model and a stochastic noise term. Based on frequency domain analysis          tools (e.g., Z-transform) and stochastic process theories, we propose          analytical formulations for quantifying the expected speed variances          of a stream of vehicles following one another according to one such          stochastic car-following model, where the lead vehicle is subject          to certain random perturbations. Our analysis on the homogeneous case          (where all vehicles are identical) reveals two significant phenomena          consistent with recent observations of traffic oscillation growth          patterns from field experimental data: A linear stochastic car-following          model with common parameter settings yields (i) concave growth of          the speed oscillation magnitudes and (ii) reduction of oscillation          frequency as oscillation propagates upstream. Numerical studies verify          the universal soundness of the proposed analytical approach for both          homogeneous and heterogeneous traffic scenarios, and both asymptotically          stable and unstable underlying systems, as well as draw insights into          traffic oscillation properties of a number of commonly used car-following          models. Overall, the proposed method, as a stochastic peer, complements          the traditional frequency-domain analysis method for deterministic          car-following models, and can be potentially used to investigate stability          responses and mitigate traffic oscillation for various car-following          behaviors with stochastic components.
Emerging connected and autonomous vehicle (CAV) technologies enable          the accurate implementation of vehicle trajectory optimization algorithms.          An adaptive trajectory controller is demanded to overcome the heterogeneity          of CAV technologies. We develop a dynamic data-driven control architecture          to optimally control CAV trajectories. For the sake of short term          planning in transportation, we currently focus on mixed traffic scenarios          and individual CAV control. The controller utilizes real-time spatio-temporal          information to provide better solutions compared to classical controllers          which only capture features of specific data with fixed parameters          or suffer from model errors of mathematical formulations. A reinforcement          learning method is applied and a triple-thread structure is proposed          to take advantage of dynamic learning and guarantee driving safety          at the same time.The present study then proceeds with an illustration          with empirical data collected in field tests. The proposed controller          is shown to outperform human drivers and Adaptive Cruise Control methods.          Impact of the size of spatial information and temporal information,          as well as changes of reward functions on the controller learning          speed and performance are discussed",Trajectory Based Traffic Analysis and Control Utilizing Connected Autonomous Vehicles,,Scholar Commons,,,core
443944421,2021-06-01T07:00:00,"The advent of Chip Multiprocessor (CMP) with high performance, compact size and power efficiency has made many engineering marvel possible. CMPs has played great role in industrial automation, autonomous vehicle, embedded AI, and medical prognosis. In industrial autonomy or in autonomous vehicle there are many critical task which has to be run in isolation without any interference and delay. Virtualization software (Hypervisors) are being used for application isolation in CMPs. Hypervisors such as XEN, KVM are fully fledged hypervisor with many features and have their own scheduling scheme thus, scheduling overhead. In this thesis we used light-weight partitioning hypervisor known as Jailhouse in order to provide isolation to critical task. From our experiment we see that Jailhouse provide better isolation without any scheduling overhead which is suitable for real time application. As Jailhouse partition available resources among cells without any emulation, the number of cell we can create is limited. Also, the resources from root cell (which runs Linux) get divided and application running on it may suffer from resource constraints. We purpose adaptive offloading in order to address this issue which shows performance and quality improvement.  We also explore deep learning and its implementation in edge computing device. The availability of GPUs and large data set made it possible to use deep learning state-of-art in many fields computer vision, medical diagnosis, image processing, surveillance, etc. It is evident that deep learning consists of two parts training and inferencing, both of these are power and compute intensive.  We implemented YOLOV3 object detection state-of-art  algorithm in NVIDIA AGX Xavier. We utilized Tensor and NVDLA cores in the Xavier using NVIDIA TensorRT and CUDA library. This has resulted more than 100% improvement in performance and significant decrease in power consumption from original YOLOV3 in FP16 precision. We explorer FP16 and INT8 precision with TensorRT, and DLA. INT8 precision further optimizes the performance and power with some compromise in accuracy. Our results shows, we can optimize inference engine by using TensorRT and DLA in edge computing device like Jetson Xavier",EFFICIENT RESOURCE MANAGEMENT ON EMBEDDED DEVICES VIA ISOLATION AND ADAPTIVE RESOURCE ALLOCATION,,OpenSIUC,,,core
100245287,2014-12-09,"The emerging area of intelligent unmanned aerial ve-hicle (UAV) research has shown rapid development in recent years and offers a great number of research chal-lenges for artificial intelligence. In this article, a pro-totype distributed architecture for intelligent unmanned aerial vehicle experimentation is presented which sup-ports the development of intelligent capabilities and their integration in a robust, scalable, plug-and-play hardware/software architecture. The architecture itself uses real-time CORBA to support its infrastructure and it is based on a reactive concentric software control phi-losophy. A number of capabilities of the architecture are presented including a multi-mode flight control sys-tem for a Yamaha RMAX VTOL platform, an on-board path planning service and a dynamically reconfigurable image processing system. A research prototype system has been built, is operational and is being used in actual missions. In the article, we emphasize the characteris-tics of the architecture which support the integration of numerous AI technologies",Draft to be Submitted to 5th IFAC Symposium on Intelligent Autonomous Vehicles (IAV2004). 1 A Distributed Architecture for Intelligent Unmanned Aerial Vehicle Experimentation,,,,,core
444081897,2021-07-05T00:00:00,"Interconnected road lanes are a central concept for navigating urban roads.
Currently, most autonomous vehicles rely on preconstructed lane maps as
designing an algorithmic model is difficult. However, the generation and
maintenance of such maps is costly and hinders large-scale adoption of
autonomous vehicle technology. This paper presents the first self-supervised
learning method to train a model to infer a spatially grounded lane-level road
network graph based on a dense segmented representation of the road scene
generated from onboard sensors. A formal road lane network model is presented
and proves that any structured road scene can be represented by a directed
acyclic graph of at most depth three while retaining the notion of intersection
regions, and that this is the most compressed representation. The formal model
is implemented by a hybrid neural and search-based model, utilizing a novel
barrier function loss formulation for robust learning from partial labels.
Experiments are conducted for all common road intersection layouts. Results
show that the model can generalize to new road layouts, unlike previous
approaches, demonstrating its potential for real-world application as a
practical learning-based lane-level map generator.Comment: Accepted for IEEE ITSC 202","Learning a Model for Inferring a Spatial Road Lane Network Graph using
  Self-Supervision",http://arxiv.org/abs/2107.01784,,,,core
389892212,2018-09-12T00:00:00,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenarios and at points of intersections in real-time environments.The goal of this research is to develop a new artificial intelligent adaptive controller for autonomous vehicle Pre-Crash system along with vehicle recognition module and tested in MATLAB including some detailed modules. Following tasks were set: finding Objects in sensor Data (LiDAR. RADAR), Speed and Steering control, vehicle Recognition using convolution neural network and Alexnet.In this research paper, we implemented a real-time image/Lidar processing. At the beginning, we presented a real-time system which is composed of comprehensive modules, these modules are 3d object detection, object clustering and search, ground removal, deep learning using convolutional neural networks. Starting with nearest vehicle  module our target is to find the nearest ahead car and consider it as our primary obstacle.This paper presents an Adaptive cruise pre-crash system and vehicle recognition. The Adaptive cruise pre-crash system module depends on Deep Learning and LiDAR sensor data, which meant to control the driver reckless behavior on the road by adjusting the vehicle speed to maintain a safe distance from objects ahead (such as cars, humans, bicycle or whatever the object) when the driver tries to raise speed. At the very moment the vehicle recognition module, detects and recognizes the vehicles surrounding to the car.Задача предаварийного интеллектуального управления робота автономных транспортных средств является очень сложной проблемой, особенно предаварийные условия транспортных средств и в точках пересечения в условиях реального времени.Целью данного исследования является разработка нового искусственного интеллектуального адаптивного регулятора для системы предаварийной безопасности автономных транспортных средств, а также модуля распознавания транспортных средств и тестирование в MATLAB, включая некоторые детализированные модули. Были поставлены следующие задачи: поиск объектов по данным датчиков (Лидар, Радар), контроль скорости и рулевого управления, распознавание транспортных средств с использованием сверточной нейронной сети и Alexnet.В данной исследовательской работе мы реализовали обработку изображений и лидарных данных в режиме реального времени. Вначале мы представили систему реального времени, которая состоит из комплексных модулей, а именно модули обнаружения трехмерных объектов, группирования и поиска объектов, удаления земли, глубинного обучения с использованием сверточных нейронных сетей. Начиная с модуля ближайшего транспортного средства, наша задача - найти ближайший впереди идущий автомобиль и считать его основным препятствием.В статье представлена адаптивная предаварийная система управления скоростью и распознавания транспортных средств. Модуль адаптивной предаварийной системы управления скоростью зависит от данных глубинного обучения и лидарного датчика, которые предназначены для управления безрассудным поведением водителя на дороге путем регулирования скорости транспортного средства для поддержания безопасного расстояния от объектов впереди (таких как автомобили, люди, велосипед или любой другой объект), когда водитель пытается повысить скорость. В настоящий момент модуль распознавания транспортных средств обнаруживает и распознает транспортные средства вокруг автомобиляЗавдання передаварійного інтелектуального керування робота автономних транспортних засобів є дуже складною проблемою, особливо передаварійні умови транспортних засобів і в точках перетину в умовах реального часу.Метою даного дослідження є розробка нового штучного інтелектуального адаптивного регулятора для системи передаварійної безпеки автономних транспортних засобів, а також модуля розпізнавання транспортних засобів та тестування в MATLAB, включаючи деякі деталізовані модулі. Були поставлені наступні завдання: пошук об'єктів за даними датчиків (Лiдар, Радар), контроль швидкості та рульового управління, розпізнавання транспортних засобів з використанням згорткової нейронної мережі та Alexnet.У даній дослідницькій роботі ми реалізували обробку зображень та лiдарних даних в режимі реального часу. Спочатку ми представили систему реального часу, яка складається з комплексних модулів, а саме модулі виявлення тривимірних об'єктів, групування та пошуку об'єктів, видалення землі, глибинного навчання з використанням згорткових нейронних мереж. Починаючи з модуля найближчого транспортного засобу, наше завдання - знайти найближчий попереду автомобіль і вважати його основною перешкодою.У статті представлена адаптивна передаварiйна система керування швидкістю та розпізнавання транспортних засобів. Модуль адаптивної передаварійної системи керування швидкістю залежить від даних глибинного навчання та лідарного датчика, які призначені для управління безрозсудною поведінкою водія на дорозі шляхом регулювання швидкості транспортного засобу для підтримки безпечної відстані від об'єктів попереду (таких як автомобілі, люди, велосипед або будь-який інший об'єкт), коли водій намагається підвищити швидкість. Наразi модуль розпізнавання транспортних засобів виявляє і розпізнає транспортні засоби навколо автомобіл",Гібридний лiдарный/радарний механізм глибинного навчання та розпізнавання транспортних засобів для управління передаварійною безпекою автономних транспортних засобів,,РС ТЕСHNOLOGY СЕNTЕR,,,core
228303340,2019-01-01T00:00:00,"The progress in sensor technologies, computer capabilities and artificial intelligence has endowed the unmanned aircraft system (UAS) with more autonomous abilities. Motivated by the 6th International Unmanned Aerial Vehicle Innovation Grand Prix (UAVGP), a UAS with high degree of autonomy was developed to perform the mission of building a simulated tower using prefabricated components. According to the requirement of the competition, the UAS was designed and implemented from the following four parts: 1) navigation and control, 2) recognition and location, 3) grasp and construction, and 4) task planning and scheduling. Different levels of autonomy have been given to the UAS based on these parts. The system hardware was developed on a quadrotor platform by integrating various components, including sensors, computers, power and grasp mechanism. Software which included precise navigation, mission planning, real-time perception and control was implemented and integrated with the developed UAS hardware. The performance in the test environment and actual competition showed that the UAS could perform the mission without human intervention with high autonomy and reliability. This paper addresses the major components and development process of the UAS and describes its application to the practical mission",A vision-based unmanned aircraft system for autonomous grasp transport,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
144001529,2003-08-13T00:00:00,"Este trabalho trata do estudo de concepções de arquitetura do controle aplicadas aos robôs móveis autônomos e da proposição de um delas à instrumentação e controle em tempo real de um modelo de embarcação naval de alto desempenho. Tal veículo remotamente operado foi desenvolvido como parte das atividades do projeto temático ""Comportamento em Ondas de Embarcações de Alto Desempenho"" (proc.Fapesp 1997/13090-3). Realizou-se uma investigação dos diversos paradigmas de inteligência artificial que orientaram a evolução dos robôs móveis autônomos até o presente momento e, em particular, as concepções baseadas em modelos sócio-antropológicos e computacionais (teoria de agentes e orientação a objetos) através de sua aplicação à implementação de um sistema de aquisição e controle orientado a objetos, modelado através da UML (Unified Modeling Language), para o veículo mencionado. Testes de validação da arquitetura do controle foram realizados, sendo obtidos resultados experimentais que permitiram análises a respeito da dinâmica, manobrabilidade e navegação do veículo, as quais sugerem vários aperfeiçoamentos para o sistema de hardware e software em trabalhos futuros.This work deals with the study of control architecture approaches applied to autonomous mobile robots, and proposes one of them for the control system of a self-propelled high speed ship model. Such unmanned vehicle was developed for the research project Comportamento em Ondas de Embarcações de Alto Desempenho"" (proc. FAPESP 1997/13090-3). A number of artificial intelligence paradigms, related to the autonomous robot evolution up to now, were investigated. Models based on the socio-anthropological paradigm and the corresponding computer science approaches, i.e. agent theory and object-oriented modeling, were emphasized. Object-oriented control software based on the UML (Unified Modeling Language) was designed for the real-time embedded system of the ship model. Validation tests of the control architecture were carried out. Experimental results, related to vehicle dynamics, maneuverability and navigation were acquired by the embedded system and analyzed in this work. These results suggest a number of improvements for future works on the software and hardware systems",Development of a control architecture based on objects for an aquatic mobile robot.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,10.11606/D.3.2003.tde-31072003-153011,core
296651874,2015-11-26T15:16:45Z,"Environment perception is a major research issue which is very important in the field of robotic system. In order to identify the horizon line and the drivable region, we have proposed a visual-perception system based on an automatic image discarding method as a simple solution to improve the system performance. In this paper, all these previous methods are organized in a visual-perception layer which also includes a method for estimating the risk-of-collision based on Pearson's Correlation Coefficient and an evolution of the Threshold and Horizon Finder (TH Finder). These methods were successfully evaluated from real data. © 2012 IEEE.16IEEE Latin American Robotics CouncilBonin-Font, F., Ortiz, A., Oliver, G., Visual Navigation for Mobile Robots: A Survey (2008) Journal of Intelligent and Robotic SystemsKim, B., Hubbard, P., Necsulescu, D., (2003) Swarming Unmanned Aerial Vehicles: Concept Development and Experimentation, A State of the Art Review on Flight and Mission Control, , DRDC-OTTAWATM-2003-176Technical MemorandumThrun, S., Stanley, the robot that won the DARPA Grand Challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , DARPA Grand Challenge(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , http://www.darpa.mil/grandchallenge/TechPapers/Team_Berlin.pdf, Team Berlin retrieved 02 Dec. 2010Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-the-loop simulations (2006) Vehicle System Dynamics, 44 (7), pp. 569-590Miranda Neto, A., Rittner, L., A Simple and Efficient Road Detection Algorithm for Real Time Autonomous Navigation based on Monocular Vision (2006) Proceedings of the 2006 IEEE 3rd Latin American Robotics SymposiumUlrich, I., Nourbakhsh, I., Appearance-Based Obstacle Detection with Monocular Color Vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, July/August 2000, pp. 866-871Bertozzi, M., Broggi, A., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's Correlation Coefficient for Discarding Redundant Information in Real Time Autonomous Navigation System (2007) IEEE International Conference on Control Applications Part of IEEE Multi-conference on Systems and Control (CCA - MSC 2007), SingapuraMiranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Victorino, A.C., Nondeterministic Criteria to Discard Redundant Information in Real Time Autonomous Navigation Systems based on Monocular Vision (2008) IEEE Multi-conference on Systems and Control (ISIC - MSC 2008), San Antonio, Texas, US, , ISIC Invited PaperMiranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Real-Time Dynamic Power Management based on Pearson's Correlation Coefficient (2011) IEEE International Conference on Advanced Robotics (ICAR 2011), Tallinn, EstoniaRodgers, J.L., Nicewander, W.A., Thirteen Ways to Look at the Correlation Coefficient (1988) The American Statistician, 42, pp. 59-66Pearson, K., (1895) Royal Society Proceedings, 58, p. 241Eugene, Y.K., Johnston, R.G., (1996) The Ineffectiveness of the Correlation Coefficient for Image Comparisons, , Technical Report LA-UR-96-2474, Los Alamos(2005) DARPA Grand Challenge, , http://www.darpa.mil/grandchallenge05/, DARPA(2006) Stanford Racing Team's Entry in the 2005 DARPA Grand Challenge, , http://www.stanfordracing.org/, June 10Gonzalez, C.R., Woods, E.R., (2000) Digital Image Processing, , Ed. Edgard Blücher, S.Paulo, BrazilSahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Otsu, N., A threshold selection method from gray-level histogram (1978) IEEETransactions on Systems, Man, and CyberneticsLee, U.S., Chung, Y.S., Park, H.R., A Comparative Performance Study of Several Global Thresholding Techniques for Segmentation (1990) Computer Vision, Graphics, and Image ProcessingLim, K.H., Vision-based Lane-Vehicle Detection and Tracking (2009) Chapter 13 of IAENG Transactions on Engineering Technologies, 3, pp. 157-171. , Special Edition', American Institute of PhysicsDahlkamp, H., Self-Supervised Monocular Road Detection in Desert Terrain (2006) Proceedings of the Robotics Science and Systems ConferenceEttinger, S., Vision-Guided Flight Stability and Control for Micro Air Vehicles (2003) Advanced Robotics, 17, pp. 617-640Sezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Miranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Robust Horizon Finding Algorithm for Real Time Autonomous Navigation based on Monocular Vision (2011) IEEE International Conference on Intelligent Transportation Systems (ITSC 2011), Washinton DC, USRauskolb, F.W., Caroline: An autonomously driving vehicle for urban environments (2008) Journal of Field Robotics, 25 (9), pp. 674-724http://www.youtube.com/user/kingdombr, April, 25 201",A Visual-perception Layer Applied To Reactive Navigation,,,,10.1109/SBR-LARS.2012.8,core
83859063,2017-07-18T00:00:00,"Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.Comment: Accepted for Field and Service Robotics conference 2017 (FSR 2017","AirSim: High-Fidelity Visual and Physical Simulation for Autonomous
  Vehicles",http://arxiv.org/abs/1705.05065,,,,core
230461017,2002-12-01T08:00:00,"The contribution of this research effort was to show that a reliable RPV could be built, tested, and successfully used for flight testing and parameter estimation purposes, in an academic setting. This was a fundamental step towards the creation of an automated Unmanned Aerial Vehicle (UAV). This research project was divided into four phases. Phase one involved the construction, development, and initial flight of a Remotely Piloted Vehicle (RPV), the West Virginia University (WVU) Boeing 777 (B777) aircraft. This phase included the creation of an onboard instrumentation system to provide aircraft flight data. The objective of the second phase was to estimate the longitudinal and lateral-directional stability and control derivatives from actual flight data for the B777 model. This involved performing and recording flight test maneuvers used for analysis of the longitudinal and lateral-directional estimates. Flight maneuvers included control surface doublets produced by the elevator, aileron, and rudder controls. A parameter estimation program known as pEst, developed at NASA Dryden Flight Research Center (DFRC), was used to compute the off-line estimates of parameters from collected flight data. This estimation software uses the Maximum Likelihood (ML) method with a Newton-Raphson (NR) minimization algorithm. The mathematical model used a traditional static and dynamic derivative buildup. Phase three focused on comparing a linear model obtained from the phase two ML estimates, with linear models obtained from a (i) Batch Least Squares Technique (BLS) and (ii) a technique from the Matlab system identification toolbox. Historically, aircraft parameter estimation has been performed off-line using recorded flight data from specifically designed maneuvers. In recent years, several on-line parameter identification techniques have been evaluated for real-time on-line applications. Along this research line, a novel contribution of this work was to compare the off-line estimation results with results obtained using a recently introduced frequency based on-line estimation method. Specifically, phase four focused on comparing the ML results with a frequency domain based on-line estimation technique. The RPV vehicle and payload was designed and constructed with the combined efforts of WVU researchers, graduate and undergraduate students of the Mechanical and Aerospace Engineering Department, and a private sub-contractor, Craig Aviation",Flight testing of a remotely piloted vehicle for aircraft parameter estimation purposes,https://core.ac.uk/download/230461017.pdf,The Research Repository @ WVU,,,core
268659813,2017-08-01T07:00:00,"Deep Convolutional Neural Networks (CNN) have emerged as the dominant approach for solving many problems in computer vision and perception. Most state-of-the-art approaches to these problems are designed to operate using RGB color images as input. However, there are several settings that CNNs have been deployed in where more information about the state of the environment is available. Systems that require real-time perception such as self-driving cars or drones are typically equipped with sensors that can oﬀer several complementary representations of the environment. CNNs designed to take advantage of features extracted from multiple sensor modalities have the potential to increase the perception capability of autonomous systems. The work in this thesis extends the real-time CNN segmentation model ENet [39] to learn using a multimodal representation of the environment. Namely we investigate learning from disparity images generated by SGM [20] in conjunction with RGB color images from the Cityscapes dataset [10]. To do this we create a network architecture called MM-ENet composed of two symmetric feature extraction branches followed by a modality fusion network. We avoid using depth encoding strategies such as HHA [15] due to their computational cost and instead operate on raw disparity images. We also constrain the resolution of training and testing images to be relatively small, downsampled by a factor of four with respect to the original Cityscapes resolution. This is because a deployed version of this system must also run SGM in real-time, which could become a computational bottleneck if using higher resolution images. To design the best model for this task, we train several architectures that diﬀer with respect to the operation used to combine features: channel-wise concatenation, element-wise multiplication, and element-wise addition. We evaluate all models using Intersection-over-Union (IoU) as a primary performance metric and the instance-level IoU (iIoU) as a secondary metric. Compared to the baseline ENet model, we achieve comparable segmentation performance and are also able to take advantage of features
that cannot be extracted from RGB images alone. The results show that at this particular fusion location, elementwise multiplication is the best overall modality combination method. Through observing feature activations at diﬀerent points in the network we show that depth information helps the network reason about object edges and boundaries that are not as salient in color space, particularly with respect to spatially small object classes such as persons. We also present results that suggest that even though each branch learned to extract unique features, these features can have complementary properties. By extending the ENet model to learn from multimodal data we provide it with a richer representation of the environment. Because this extension simply duplicates layers in the encoder to create two symmetric feature extraction branches, the network also maintains real-time inference performance. Due to the network being trained on smaller resolution images to remain within the constraints of an embedded system, the overall performance is competitive but below state-of-the-art models reported on the Cityscapes leaderboard. When deploying this model in a high-performance system such as an autonomous vehicle that has the ability to generate disparity maps in real-time at a high resolution, MM-ENet can take advantage of unused data modalities to improve overall performance on semantic segmentation",Deep Multimodal Fusion Networks for Semantic Segmentation,https://core.ac.uk/download/268659813.pdf,Clemson University Libraries,,,core
479186071,2021-04-01T00:00:00,"Due to the predicted rise of Unmanned Aircraft Systems (UAS) in commercial, civil, and military operations, there is a desire to make UASs more energy efficient so they can proliferate with ease of deployment and maximal life per charge. To address current limitations, a three-tiered approach is investigated to mitigate Unmanned Aerial Vehicle (UAV) hover time, reduce network datalink transmission to a ground station, and provide a real-time framework for Sense-and-Avoidance (SAA) target classification. An energy-efficient UAS architecture framework is presented, and a corresponding SAA prototype is developed using commercial hardware to validate the proposed architecture using an experimental methodology. The proposed architecture utilizes classical computer vision methods within the Detection Subsystem coupled with deeply learned Convolutional Neural Networks (CNN) within the Classification Subsystem. Real-time operations of three frames per second are realized enabling UAV hover time and associated energy consumption during SAA processing to be effectively eliminated. Additional energy improvements are not addressed in the scope of this work. Inference accuracy is improved by 19% over baseline COTS models and current non-adaptive, single-stage SAA architectures. Overall, by pushing SAA processing to the edge of the sensors, network offload transmissions and reductions in processing time and energy consumption are feasible and realistic in future battery-powered electric air transportation systems",Energy-Efficient On-Platform Target Classification for Electric Air Transportation Systems,,'MDPI AG',"[{'title': 'Electricity', 'identifiers': ['issn:2673-4826', '2673-4826']}]",10.3390/electricity2020007,core
22924004,2013-08-16,"This paper describes system identification techniques based on neural networks for a nonlinear, Multi-Input Multi-Output (MIMO) system such as the Unmanned Aerial Vehicle (UAV). A novel training scheme for an online neural network model has been tested and implemented on the UAV and the results are presented. The online model is compared to a trained offline neural network model to bring out the merits and demerits in both of them. To improve the performance, the online and offline models are used in combination to form a multi-network architecture with a dynamic selection technique to choose between the outputs of each model based on a selection criterion. The neural network models are based on the autoregressive technique. The techniques have been developed as a part of the autonomous UAV program carried out at our research institution. These identification models can be developed for different stages of flight and their outputs can be optimally chosen to aid the control process. Each of the neural models have been validated on a Real-time Hardware In the Loop (HIL) simulation technique. I",Real-time System Identification Techniques Based on Neural Networks for a Low-cost UAV,,,,,core
296648423,2015-11-26T15:11:39Z,"The perception of the environment is a major issue in autonomous robots. In our previous works, we have proposed a visual perception system based on an automatic image discarding method as a simple solution to improve the performance of a real-time navigation system. In this paper, we take place in the obstacle avoidance context for vehicles in dynamic and unknown environments, and we propose a new method for Collision Risk Estimation based on Pearson's Correlation Coefficient (PCC). Applying the PCC to real-time CRE has not been done yet, making the concept unique. This paper provides a novel way of calculating collision risk and applying it for object avoidance using the PCC. This real-time perception system has been evaluated from real data obtained by our intelligent vehicle. © 2013 IEEE.4045ANR; French National Research AgencyThrun, S., Stanley, the robot that won the DARPA Grand Challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , DARPA Grand Challenge(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , http://www.darpa.mil/grandchallenge/TechPapers/Team_Berlin.pdf, Team Berlin retrieved 02 Dec. 2010Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-The-loop simulations (2006) Vehicle System Dynamics, 44 (7), pp. 569-590Ulrich, I., Nourbakhsh, I., Appearance-based obstacle detection with monocular color vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 866-871. , July/AugustBonin-Font, F., Ortiz, A., Oliver, G., Visual navigation for mobile robots: A survey (2008) Journal of Intelligent and Robotic SystemsKim, B., Hubbard, P., Necsulescu, D., Swarming unmanned aerial vehicles: Concept development and experimentation, a state of the art review on flight and mission control (2003) Technical MemorandumDugarry, A., (2004) Advanced Driver Assistance Systems Information Management and Presentation, , PhD Thesis, Cranfield University School of Engineering Applied Mathematics and Computing GroupBeauchemin, S., Barron, J.L., The computation of optical flow (1995) ACM Computing Surveys, 27, pp. 433-467State of the art report and requirement specifications (2009) Hybrid Intelligent Virtual Actors, Integrating Research in Interactive Storytelling, , http://www.irisa.fr/, INRIA Report retrieved Feb. 03, 2011Pearson, K., (1895) Royal Society Proceedings, 58, p. 241Eugene, Y.K., Johnston, R.G., (1996) The Ineffectiveness of the Correlation Coefficient for Image Comparisons, , Technical Report LA-UR-96-2474, Los Alamos, 1996Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Victorino, A.C., Nondeterministic criteria to discard redundant information in real time autonomous navigation systems based on monocular vision (2008) ISIC Invited Paper, 2008 IEEE Multi-conference on Systems and ControlMiranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's correlation coefficient for discarding redundant information in real time autonomous navigation systems (2007) Proceedings of the 2007 IEEE Multi-conference on Systems and ControlGreco, C.R., (2008) Real-Time Forward Urban Environment Perception for An Autonomous Ground Vehicle Using Computer Vision and Lidar, , Master of Science (thesis), Brigham Young UniversityBertozzi, M., Broggi, A., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Dahlkamp, H., Self-supervised monocular road detection in desert terrain (2006) Proceedings of the Robotics Science and Systems ConferenceRadio Spectrum Committee, European Commission, , http://ec.europa.eu/information_society/policy/ecomm/radio_spectrum /_document_storage/rsc/rsc32_public_docs/rscom10_35.pdf, Public Document, Brussels, 5 July 2010, RSCOM10-35 [retrieved Dec. 02, 2010]Gietelink, O.J., Ploeg, J., Schutter, B., Verhaegen, M., Development of a driver information and warningsystem with vehicle hardware-in-theloop simulations (2009) Mechatronics, 19, pp. 1091-1104Müller, D., Pauli, J., Nunn, C., Görmer, S., Müller-Schneiders, S., Time to contact estimation using interest points (2009) IEEE Proceedings of the International Conference on Intelligent Transportation Systems (ITSC 2009), , St.Louis, USAAlenya, G., Negre, A., Crowley, J.L., A comparison of three methods for measure of time to contact (2009) IEEE Proceedings of the International Conference on Intelligent Transportation Systems (ITSC 2009), , St.Louis, USADagan, E., Mano, O., Stein, G.P., Shashua, A., Forward collision warning with a single camera (2004) Intelligent Vehicles Symposium, IEEE, pp. 37-42Negre, A., Braillon, C., Crowley, J., Laugier, C., Real-time time-to-collision from variation of intrinsic scale (2006) INRIA Base, Proc. of the Int. Symp. on Experimental RoboticsHorn, B.K.P., (1986) Robot Vision, , The MIT PressWu, S., Decker, S., Chang, P., Camus, T., Eledath, J., Collision sensing by stereo vision and radar sensor fusion (2009) IEEE Transactions on Intelligent Transportation Systems, 10 (4)Beyeler, A., Zufferey, J.C., Floreano, D., Vision-based control of near-obstacle flight (2009) Autonomous Robots, 27 (3), pp. 201-219Ruffier, F., Franceschini, N., Optic flow regulation: The key to aircraft automatic guidance (2005) Robotics Autonomous Systems, 50, pp. 177-194Mesbah, M., Gradient-based optical flow: A critical review (1999) Proc. of the Fifth Int. Symp. on Signal Processing and Its Applications. ISSPA '99, 1, pp. 467-470. , 1999Otsu, N., A threshold selection method from graylevel histogram (1978) IEEE Transactions on Systems, Man, and CyberneticsRodgers, J.L., Nicewander, W.A., Thirteen ways to look at the correlation coefficient (1988) The American Statistician, 42, pp. 59-66Yanqing, W., Deyun, C., Chaoxia, S., Peidong, W., Vision-based road detection by monte carlo method (2010) Information Technology Journal, 9, pp. 481-487Sahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Lee, U.S., Chung, Y.S., Park, H.R., A comparative performance study of several global thresholding techniques for segmentation (1990) Computer Vision, Graphics, and Image ProcessingSezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Gonzalez, C.R., Woods, E.R., (1991) Digital Image Processing, , Addison-Wesley Publishing CompanyCanny, J.F., A computational approach to edge detection (1986) IEEE Trans. Pattern Anal. Machine Intell., 8 (6), pp. 679-698(2005) DARPA Grand Challenge Rulebook, , http://www.darpa.mil/grandchallenge05/, DARPA(2006) Stanford Racing Team's Entry in the 2005 DARPA Grand Challenge, , http://www.stanfordracing.org, June 10http://youtu.be/J8YuZlJFEx",Real-time Collision Risk Estimation Based On Pearson's Correlation Coefficient,,,,10.1109/WORV.2013.6521911,core
186272931,2020-02-13T00:00:00,"Once self-driving car becomes a reality and passengers are no longer worry
about it, they will need to find new ways of entertainment. However, retrieving
entertainment contents at the Data Center (DC) can hinder content delivery
service due to high delay of car-to-DC communication. To address these
challenges, we propose a deep learning based caching for self-driving car, by
using Deep Learning approaches deployed on the Multi-access Edge Computing
(MEC) structure. First, at DC, Multi-Layer Perceptron (MLP) is used to predict
the probabilities of contents to be requested in specific areas. To reduce the
car-DC delay, MLP outputs are logged into MEC servers attached to roadside
units. Second, in order to cache entertainment contents stylized for car
passengers' features such as age and gender, Convolutional Neural Network (CNN)
is used to predict age and gender of passengers. Third, each car requests MLP
output from MEC server and compares its CNN and MLP outputs by using k-means
and binary classification. Through this, the self-driving car can identify the
contents need to be downloaded from the MEC server and cached. Finally, we
formulate deep learning based caching in the self-driving car that enhances
entertainment services as an optimization problem whose goal is to minimize
content downloading delay. To solve the formulated problem, a Block Successive
Majorization-Minimization (BS-MM) technique is applied. The simulation results
show that the accuracy of our prediction for the contents need to be cached in
the areas of the self-driving car is achieved at 98.04% and our approach can
minimize delay","Deep Learning Based Caching for Self-Driving Car in Multi-access Edge
  Computing",http://arxiv.org/abs/1810.01548,,,,core
467112496,2021-07-17T00:00:00,"Autonomous car racing is a challenging task in the robotic control area.
Traditional modular methods require accurate mapping, localization and
planning, which makes them computationally inefficient and sensitive to
environmental changes. Recently, deep-learning-based end-to-end systems have
shown promising results for autonomous driving/racing. However, they are
commonly implemented by supervised imitation learning (IL), which suffers from
the distribution mismatch problem, or by reinforcement learning (RL), which
requires a huge amount of risky interaction data. In this work, we present a
general deep imitative reinforcement learning approach (DIRL), which
successfully achieves agile autonomous racing using visual inputs. The driving
knowledge is acquired from both IL and model-based RL, where the agent can
learn from human teachers as well as perform self-improvement by safely
interacting with an offline world model. We validate our algorithm both in a
high-fidelity driving simulation and on a real-world 1/20-scale RC-car with
limited onboard computation. The evaluation results demonstrate that our method
outperforms previous IL and RL methods in terms of sample efficiency and task
performance. Demonstration videos are available at
https://caipeide.github.io/autorace-dirl/Comment: 8 pages, 8 figures. IEEE Robotics and Automation Letters (RA-L) &
  IROS 202","Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement
  Learning",http://arxiv.org/abs/2107.08325,,,,core
19759283,2013-12-03T00:00:00,"Safety concerns in the operation of autonomous aerial systems require safe-landing protocols be followed during situations where the a mission should be aborted due to mechanical or other failure. On-board cameras provide information that can be used in the determination of potential landing sites, which are continually updated and ranked to prevent injury and minimize damage. Pulse Coupled Neural Networks have been used for the detection of features in images that assist in the classification of vegetation and can be used to minimize damage to the aerial vehicle. However, a significant drawback in the use of PCNNs is that they are computationally expensive and have been more suited to off-line applications on conventional computing architectures. As heterogeneous computing architectures are becoming more common, an OpenCL implementation of a PCNN feature generator is presented and its performance is compared across OpenCL kernels designed for CPU, GPU and FPGA platforms. This comparison examines the compute times required for network convergence under a variety of images obtained during unmanned aerial vehicle trials to determine the plausibility for real-time feature detection",Pulse-coupled neural network performance for real-time identification of vegetation during forced landing,https://core.ac.uk/download/19759283.pdf,,,,core
233120453,2019-07-19T00:00:00,"Autonomous vehicle is a vehicle that can guide itself without human conduction. It is capable of sensing its environment and moving with little or no human input. This kind of vehicle has become a concrete reality and may pave the way for future systems where computers take over the art of driving. Advanced artificial intelligence control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant road signs. In this paper, we introduce an intelligent road signs classifier to help autonomous vehicles to recognize and understand road signs. The road signs classifier based on an artificial intelligence technique. In particular, a deep learning model is used, Convolutional Neural Networks (CNN). CNN is a widely used Deep Learning model to solve pattern recognition problems like image classification and object detection. CNN has successfully used to solve computer vision problems because of its methodology in processing images that are similar to the human brain decision making. The evaluation of the proposed pipeline was trained and tested using two different datasets. The proposed CNNs achieved high performance in road sign classification with a validation accuracy of 99.8% and a testing accuracy of 99.6%. The proposed method can be easily implemented for real time application",To Perform Road Signs Recognition for Autonomous Vehicles Using Cascaded Deep Learning Pipeline,https://core.ac.uk/download/233120453.pdf,'Bilingual Publishing Co.',,10.30564/aia.v1i1.569,core
384449133,,"Unmanned aerial vehicle (UAV)-assisted device-to-device (D2D) communications can be deployed flexibly thanks to UAVs’ agility. By exploiting the direct D2D interaction supported by UAVs, both the user experience and network performance can be substantially enhanced at public events. However, the continuous moving of D2D users, limited energy and flying time of UAVs are impediments to their applications in real-time. To tackle this issue, we propose a novel model based on deep reinforcement learning in order to find the optimal solution for the energy-harvesting time scheduling in UAV-assisted D2D communications. To make the system model more realistic, we assume that the UAV flies around a central point, the D2D users move continuously with random walk model and the channel state information encountered during each time slot is randomly time-variant. Our numerical results demonstrate that the proposed schemes outperform the existing solutions. The associated energy efficiency game can be solved in less than one millisecond by an off-the-shelf processor using trained neural networks. Hence our deep reinforcement learning techniques are capable of solving real-time resource allocation problems in UAV-assisted wireless networks",Real-time energy harvesting aided scheduling in UAV-assisted D2D networks relying on deep reinforcement learning,,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/ACCESS.2020.3046499,core
227320072,2019-10-27T00:00:00,"International audienceReplacing the human driver to perform the Dynamic Driving Task (DDT)[1] will require perception, complex analysis and assessment of traffic situation. The path leading to success the deployment of fully Autonomous Vehicle (AV) depends on the resolution of a lot of challenges. Both the safety and the security aspects of AV constitute the core of regulatory compliance and technical research. The Autonomous Driving System (ADS) should be designed to ensure a safe manoeuvre and a stable behaviour despite the technological limitations, the uncertainties and hazards which characterize the real traffic conditions. In fully Autonomous Driving situation, detecting all relevant objects and agents should be sufficient to generate a warning, however the ADS requires further complex data analysis steps to quantify and improve the safety of decision making. This paper aims to improve the robustness of decision-making in order to mimic human-like decision ability. The approach is based on machine learning to identify the criticality of the dynamic situation and enabling ADS to make appropriate decision and fulfil safe manoeuvre",Machine learning method to ensure robust decision-making of AVs,https://core.ac.uk/download/227320072.pdf,HAL CCSD,,,core
326450695,2020-01-01T00:00:00,"Tracking moving objects like pedestrian have a wide range of applications for intelligent autonomous vehicle. While object detection using modern deep learning-based approach such as Convolutional Neural Network (CNN) have advanced significantly, autonomous system such as ground mobile robots still face problems in real-time implementation. These methods include CNN based human tracking due to occlusions, illumination changes and camera view variations. The aim of this project is to identify the optimal deep learning algorithm, in terms of speed and accuracy. Human tracking by detection using selected real-time object detector is also implemented. Firstly, in terms of performance speed, in terms of Frames per Second (FPS), One-stage Detectors, Single Shot Detector (SSD) achieve 19 FPS, You Only Look Once (YOLOv3) achieve 22 FPS as compared to Two-stage Detectors, such as Mask Region-Convolutional Neural Network (Mask R-CNN) which achieve 1 FPS, with this, it is concluded that One-stage Detectors is more suitable for real time implementation. In addition, Multiple Object Tracking Precision (MOTP) is used as a measure for detection accuracy, it is found that SSD achieved 68.4%, YOLOv3 achieved 72.9% and Mask R-CNN achieved 74.0%. Thus, YOLOv3 is used in this project as it is the optimal object detector. Next, the detection output coordinates as bounding boxes (in pixels) from the YOLOv3 is passed to the tracking algorithm that associate detections over frames to create a trajectory of any tracked person. The decision rule to associate detection over frames is computed by Hungarian Algorithm (Kuhn-Munkres) that takes on motion affinity and appearance similarity as inputs. For motion affinity, Intersection Over Union (IOU) provides information on physical proximity between the new detections and the tracked persons. The appearance similarity is measured by computing Euclidean distance between the last CNN features of the detected person. Promising results were obtained in the various tests carried out. Finally, the tracking algorithm is tested on mobile robot with Robot Operating System (ROS) framework, with images captured from Orbbec Astra RGB-D camera which is passed into object detection algorithm. The tracking is performed subsequently in real-time. Two major challenges faced during testing were ID switching and missing detections. While the latter is limited by the detection algorithm, ID switch occurs when two similar looking persons are very close and tracking algorithm are unable to differentiate them.Bachelor of Engineering (Electrical and Electronic Engineering",Human motion tracking using deep learning,,'Nanyang Technological University',,,core
286412460,2018-01-01T00:00:00,"The rapid development and growth of unmanned aerial vehicles (UAVs) as a remote sensing platform, as well as advances in the miniaturization of instrumentation and data systems, have resulted in an increasing uptake of this technology in the environmental and remote sensing science communities. Although tough regulations across the globe may still limit the broader use of UAVs, their use in precision agriculture, ecology, atmospheric research, disaster response biosecurity, ecological and reef monitoring, forestry, fire monitoring, quick response measurements for emergency disaster, Earth science research,volcanic gas sampling, monitoring of gas pipelines, mining plumes, humanitarian observations and biological/chemo-sensing tasks continues to increase. This Special Issue provides a forum for high-quality peer-reviewed papers that broaden the awareness and understanding of UAV developments, applications of UAVs for remote sensing, and associated developments in sensor technology, data processing and communications, and UAV system design and sensing capabilities. This topic encompasses many algorithms and process flows and tools, including: robust vehicle detection in aerial images based on cascaded convolutional neural networks; a stereo dual-channel dynamic programming algorithm for UAV image stitching, as well as seamline determination based on PKGC segmentation for remote sensing image mosaicking; the implementation of an IMU-aided image stacking algorithm in digital cameras; the study of multispectral characteristics at different observation angles, rapid three-dimensional reconstruction for image sequence acquired from UAV cameras; comparisons of Riegl Ricopter UAV Lidar-derived canopy height and DBH with terrestrial Lidar; vision based target finding and inspection of a ground target using a multirotor UAV system; a localization framework for real-time UAV autonomous landing using an on-ground deployed visual approach; curvature continuous and bounded path planning for fixed-wing UAVs; the calculation and identification of the aerodynamic parameters for small-scaled fixed-wing UAVs Several wildfire and agricultural applications of UAVS including: deep learning-based wildfire identification in UAV imagery; postfire vegetation survey campaigns; secure utilization of beacons; and UAVS used in emergency response systems for building fire hazards; observing spring and fall phenology in a deciduous forest with aerial drone imagery; the design and testing of a UAV mapping system for agricultural field surveying; artificial neural network to predict vine water status spatial variability using multispectral information obtained from an unmanned aerial vehicle; automatic hotspot and sun glint detection in UAV multispectral images obtained via uncooled thermal camera calibration and optimization of the photogrammetry process for UAV applications in agriculture; olive yield forecast tool based on the tree canopy geometry using UAS imagery; spatial scale gap filling downscaling method for applications in precision agriculture; automatic co-registration algorithm to remove canopy shaded pixels in UAV-borne thermal images to improve the estimation of crop water stress on vineyards; methodologies for improving plant pest surveillance in vineyards and crops using UAV-based hyperspectral and spatial data; UAV-assisted dynamic clustering of wireless sensors and networks for crop health monitoring. Several applications of UAVS in the fields of environment and conservation including the following: the automatic detection of pre-existing termite mounds through UAS and hyperspectral imagery; aerial mapping of forests affected by pathogens using UAVs; hyperspectral sensors and artificial i ntelligence; coral reef and coral bleaching monitoring; invasive grass and vegetation surveys in remote arid lands. UAVs are also utilized in many other applications: vicarious calibration of SUAS microbolometer temperature imagery for the estimation of radiometric land surface temperature; the documentation of hiking trails in alpine areas; the detection of nuclear sources by UAV teleoperation using a visuo-haptic augmented reality interface; the design of a UAV-embedded microphone array system for sound source localization in outdoor environments; the monitoring of concentrated solar power plants, accuracy analysis of a dam model from drone surveys, mobile sensing and actuation infrastructure, UAV-based frameworks for river hydromorphological characterization; online aerial terrain mapping for ground robot navigation",UAV or Drones for Remote Sensing Applications (Volume 1),,'MDPI AG',,10.3390/books978-3-03897-092-7,core
334920135,2020-03-07T00:00:00,"Autonomous vehicles need safe development and testing environments. Many
traffic scenarios are such that they cannot be tested in the real world. We see
hybrid photorealistic simulation as a viable tool for developing AI (artificial
intelligence) software for autonomous driving. We present a machine learning
environment for detecting autonomous vehicle corner case behavior. Our
environment is based on connecting the CARLA simulation software to TensorFlow
machine learning framework and custom AI client software. The AI client
software receives data from a simulated world via virtual sensors and
transforms the data into information using machine learning models. The AI
clients control vehicles in the simulated world. Our environment monitors the
state assumed by the vehicle AIs to the ground truth state derived from the
simulation model. Our system can search for corner cases where the vehicle AI
is unable to correctly understand the situation. In our paper, we present the
overall hybrid simulator architecture and compare different configurations. We
present performance measurements from real setups, and outline the main
parameters affecting the hybrid simulator performance.Comment: 8 pages, 13 figure","A machine learning environment for evaluating autonomous driving
  software",http://arxiv.org/abs/2003.03576,,,,core
323309274,2020-05-29T00:00:00,"LiDAR sensors have been widely used in many autonomous vehicle modalities,
such as perception, mapping, and localization. This paper presents an
FPGA-based deep learning platform for real-time point cloud processing targeted
on autonomous vehicles. The software driver for the Velodyne LiDAR sensor is
modified and moved into the on-chip processor system, while the programmable
logic is designed as a customized hardware accelerator. As the state-of-art
deep learning algorithm for point cloud processing, PointNet is successfully
implemented on the proposed FPGA platform. Targeted on a Xilinx Zynq
UltraScale+ MPSoC ZCU104 development board, the FPGA implementations of
PointNet achieve the computing performance of 182.1 GOPS and 280.0 GOPS for
classification and segmentation respectively. The proposed design can support
an input up to 4096 points per frame. The processing time is 19.8 ms for
classification and 34.6 ms for segmentation, which meets the real-time
requirement for most of the existing LiDAR sensors.Comment: This paper has been accepted by ISCAS 202",PointNet on FPGA for Real-Time LiDAR Point Cloud Processing,http://arxiv.org/abs/2006.00049,,,,core
24598743,2007,"In modern urban settings, automobile traffic and collisions lead to endless frustration as well as significant loss of life, property, and productivity. Recent advances in artificial intelligence suggest that autonomous vehicle navigation may soon be a reality. In previous work, we have demonstrated that a reservation-based approach can efficiently and safely govern interactions of multiple autonomous vehicles at intersections. Such an approach alleviates many traditional problems associated with intersections, in terms of both safety and efficiency. However, the system relies on all vehicles being equipped with the requisite technology — a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we extend this system to allow for incremental deployability. The modified system is able to accommodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in our previous work. Finally, we develop a method for switching between various human-usable configurations while the system is running, in order to facilitate an even smoother transition. The work is fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to its effectiveness. ",Sharing the road: Autonomous vehicles meet human drivers,,,,,core
93939773,2018-03-20T00:00:00,"Recent advances in Deep Neural Networks (DNNs) have led to the development of
DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can
drive without any human intervention. Most major manufacturers including Tesla,
GM, Ford, BMW, and Waymo/Google are working on building and testing different
types of autonomous vehicles. The lawmakers of several US states including
California, Texas, and New York have passed new legislation to fast-track the
process of testing and deployment of autonomous vehicles on their roads.
  However, despite their spectacular progress, DNNs, just like traditional
software, often demonstrate incorrect or unexpected corner case behaviors that
can lead to potentially fatal collisions. Several such real-world accidents
involving autonomous cars have already happened including one which resulted in
a fatality. Most existing testing techniques for DNN-driven vehicles are
heavily dependent on the manual collection of test data under different driving
conditions which become prohibitively expensive as the number of test
conditions increases.
  In this paper, we design, implement and evaluate DeepTest, a systematic
testing tool for automatically detecting erroneous behaviors of DNN-driven
vehicles that can potentially lead to fatal crashes. First, our tool is
designed to automatically generated test cases leveraging real-world changes in
driving conditions like rain, fog, lighting conditions, etc. DeepTest
systematically explores different parts of the DNN logic by generating test
inputs that maximize the numbers of activated neurons. DeepTest found thousands
of erroneous behaviors under different realistic driving conditions (e.g.,
blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in
three top performing DNNs in the Udacity self-driving car challenge","DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous
  Cars",http://arxiv.org/abs/1708.08559,,,,core
104863209,2016-08-23,"Neuro-fuzzy controller to navigate an unmanned vehicle Boumediene Selma * and Samira Chouraqui* A Neuro-fuzzy control method for an Unmanned Vehicle (UV) simulation is described. The objective is guiding an autonomous vehicle to a desired destination along a desired path in an environment characterized by a terrain and a set of distinct objects, such as obstacles like donkey traffic lights and cars circulating in the trajectory. The autonomous navigate ability and road following precision are mainly influenced by its control strategy and real-time control performance. Fuzzy Logic Controller can very well describe the desired system behavior with simple “if-then ” relations owing the designer to derive “if-then ” rules manually by trial and error. On the other hand, Neural Networks perform function approximation of a system but cannot interpret the solution obtained neither check if its solution is plausible. The two approaches are complementary. Combining them, Neural Networks will allow learning capability while Fuzzy-Logic will bring knowledge representation (Neuro-Fuzzy). In this paper, an artificial neural network fuzzy inference system (ANFIS) controller is described and implemented to navigate the autonomous vehicle. Results show several improvements in the control system adjusted by neuro-fuzzy techniques in comparison to the previous methods like Artificial Neural Network (ANN)",RESEARCH Open Access,,,,,core
304168270,2020-01-03T00:00:00,"Unmanned aerial vehicle (UAV) remote sensing and deep learning provide a practical approach to object detection. However, most of the current approaches for processing UAV remote-sensing data cannot carry out object detection in real time for emergencies, such as firefighting. This study proposes a new approach for integrating UAV remote sensing and deep learning for the real-time detection of ground objects. Excavators, which usually threaten pipeline safety, are selected as the target object. A widely used deep-learning algorithm, namely You Only Look Once V3, is first used to train the excavator detection model on a workstation and then deployed on an embedded board that is carried by a UAV. The recall rate of the trained excavator detection model is 99.4%, demonstrating that the trained model has a very high accuracy. Then, the UAV for an excavator detection system (UAV-ED) is further constructed for operational application. UAV-ED is composed of a UAV Control Module, a UAV Module, and a Warning Module. A UAV experiment with different scenarios was conducted to evaluate the performance of the UAV-ED. The whole process from the UAV observation of an excavator to the Warning Module (350 km away from the testing area) receiving the detection results only lasted about 1.15 s. Thus, the UAV-ED system has good performance and would benefit the management of pipeline safety.ISSN:2072-429",Real-Time Detection of Ground Objects Based on Unmanned Aerial Vehicle Remote Sensing with Deep Learning: Application in Excavator Detection for Pipeline Safety,,'MDPI AG',,10.3390/rs12010182,core
296648505,2015-11-26T15:11:44Z,"Camera-based estimation of drivable image areas is still in evolution. These systems have been developed for improved safety and convenience, without the need to adapt itself to the environment. Machine Vision is an important tool to identify the region that includes the road in images. Road detection is the major task of autonomous vehicle guidance. In this way, this work proposes a drivable region detection algorithm that generates the region of interest from a dynamic threshold search method and from a drag process (DP). Applying the DP to estimation of drivable image areas has not been done yet, making the concept unique. Our system was has been evaluated from real data obtained by intelligent platforms and tested in different types of image texture, which include occlusion case, obstacle detection and reactive navigation. © 2013 IEEE.6368Australian Dedicated Short Range Communication (AusDSRC),Griffith University Intelligent Control Systems Laboratory,Intelligent Transport Systems AustraliaBonin-Font, F., Ortiz, A., Oliver, G., Visual navigation for mobile robots: A survey (2008) Journal of Intelligent and Robotic Systems, 53 (3), pp. 263-296Rodríguez Flórez, S.A., (2010) Contributions by Vision Systems to Multi-sensor Object Localization and Tracking for Intelligent Vehicles, , Thesis, UTC, FranceThrun, S., Stanley, the robot that won the darpa grand challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , 2006, ISSN: 0741-2223(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , retrieved Jan 05, 2010]Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-The-loop simulations (2006) Vehicle System Dynamics, 44 (7)Ulrich, I., Nourbakhsh, I., Appearance-based obstacle detection with monocular color vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 866-871Kim, B., Hubbard, P., Necsulescu, D., (2003) Swarming Unmanned Aerial Vehicles: Concept Development and Experimentation, A State of the Art Review on Flight and Mission Control, , Technical MemorandumAviña-Cervantes, G., Devy, M., Marín, A., Lane extraction and tracking for robot navigation in agricultural applications (2003) Proceedings of the IEEE ICAR 2003Dahlkamp, H., Self-supervised monocular road detection in desert terrain (2006) Proceedings of the Robotics Science and Systems ConferenceDiego, F., Álvarez, J.M., Serrat, J., López, A.M., Vision based road detection via on line video registration (2010) Proceedings of the IEEE ITSC 2010Chetan, J., Madhava, K., Jawahar, C.V., An adaptive outdoor terrain classification methodology using monocular camera (2010) Proceedings of the IEEE IROS 2010Yanqing, W., Deyun, C., Chaoxia, S., Peidong, W., Vision-based road detection by monte carlo method (2010) Information Technology Journal, 9, pp. 481-487Yamaguchi, K., Watanabe, A., Naito, T., Ninomiya, Y., Road region estimation using a sequence of monocular images (2008) Proceedings of the International Conference on Pattern Recognition, 2008Otsu, N., A threshold selection method from graylevel histogram (1978) IEEE Transactions on Systems, Man, and Cybernetics, 9, pp. 62-66Bertozzi Broggi, A.M., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's correlation coefficient for discarding redundant information in real time autonomous navigation systems (2007) Proceedings of the IEEE MSC 2007Benini, L., Bogliolo, A., Micheli, G.D., A survey of design techniques for system-level dynamic power management (2000) IEEE Transactions on Very Large Scale Integration Systems, 8 (3), pp. 299-316Miranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Real-time dynamic power management based on pearson's correlation coefficient (2011) Proceedings of the IEEE ICAR 2011King, H.L., Vision-based lane-vehicle detection and tracking (2009) IAENG Transactions on Engineering Technologies Volume 3-Special Edition, pp. 157-171. , American Institute of PhysicsEttinger, S., Vision-guided flight stability and control for micro air vehicles (2003) Adv. Robotics, pp. 617-640Neto, A.M., Victorino, A.C., Fantoni, I., Zampieri, D.E., Robust horizon finding algorithm for real-time autonomous navigation based on monocular vision (2011) Proceedings of the IEEE ITSC 2011Miranda Neto, A., Rittner, L., A simple and efficient road detection algorithm for real time autonomous navigation based on monocular vision (2006) Proceedings of the IEEE 3rd LARS 2006Gonzalez, C.R., Woods, E.R., (1991) Digital Image Processing, , Addison-Wesley Publishing CompanySahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Lee, U.S., Chung, Y.S., Park, H.R., A comparative performance study of several global thresholding techniques for segmentation (1990) Computer Vision, Graphics, and Image ProcessingSezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Rauskolb, F.W., Caroline: An autonomously driving vehicle for urban environments (2008) Journal of Field Robotics, 25 (9), pp. 674-724Canny, J.F., (1986) A Computational Approach to Edge Detection, , IEEE Trans. Pattern Anal. Machine IntellBallard, D., Generalized hough transform to detect arbitrary shapes (1981) IEEE Trans. Pattern Anal. Machine Intell, 13 (2), pp. 111-122White, F.M., (1986) Fluid Mechanics, , 2nd Ed. McGraw HillDARPA 2005. DARPA Grand ChallengeNeto, A.M., Victorino, A.C., Fantoni, I., Zampieri, D.E., Ferreira, J.V., Visual-perception layer applied to reactive navigation (2012) Proceedings of the IEEE 9th LARS 2012http://youtu.be/ZpEbRo32pY8, retrieved Jan 31, 201",Real-time Estimation Of Drivable Image Area Based On Monocular Vision,,,,10.1109/IVS.2013.6629448,core
322969060,2020-05-15T00:00:00,"In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.Comment: 12 pages, 14 figures, 15th International Symposium on Software
  Engineering for Adaptive and Self-Managing Systems (SEAMS2020","Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing",http://arxiv.org/abs/2005.07460,,,,core
387259411,2021-01-01T00:00:00,"Handling of critical situations is an important part in the architecture of an autonomous vehicle. A controller for autonomous collision avoidance is developed based on a wary strategy that assumes the least tireroad friction for which the maneuver is still feasible. Should the friction be greater, the controller makes use of this and performs better. The controller uses an acceleration-vector reference obtained from optimal control of a friction-limited particle, whose applicability is verified by using numerical optimization on a full vehicle model. By employing an analytical tire model of the tireroad friction limit, to determine slip references for steering and body-slip control, the result is a controller where the computation of its output is explicit and independent of the actual tire-road friction. When evaluated in real-time on a high-fidelity simulation model, the developed controller performs close to that achieved by offline numerical optimization.Funding: Wallenberg AI, Autonomous Systems, and Software Program (WASP) - Knut and AliceWallenberg Foundation</p",Autonomous Wary Collision Avoidance,,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/TIV.2020.3029853,core
288839487,2018-09-12T00:00:00,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenarios and at points of intersections in real-time environments.The goal of this research is to develop a new artificial intelligent adaptive controller for autonomous vehicle Pre-Crash system along with vehicle recognition module and tested in MATLAB including some detailed modules. Following tasks were set: finding Objects in sensor Data (LiDAR. RADAR), Speed and Steering control, vehicle Recognition using convolution neural network and Alexnet.In this research paper, we implemented a real-time image/Lidar processing. At the beginning, we presented a real-time system which is composed of comprehensive modules, these modules are 3d object detection, object clustering and search, ground removal, deep learning using convolutional neural networks. Starting with nearest vehicle  module our target is to find the nearest ahead car and consider it as our primary obstacle.This paper presents an Adaptive cruise pre-crash system and vehicle recognition. The Adaptive cruise pre-crash system module depends on Deep Learning and LiDAR sensor data, which meant to control the driver reckless behavior on the road by adjusting the vehicle speed to maintain a safe distance from objects ahead (such as cars, humans, bicycle or whatever the object) when the driver tries to raise speed. At the very moment the vehicle recognition module, detects and recognizes the vehicles surrounding to the car.Задача предаварийного интеллектуального управления робота автономных транспортных средств является очень сложной проблемой, особенно предаварийные условия транспортных средств и в точках пересечения в условиях реального времени.Целью данного исследования является разработка нового искусственного интеллектуального адаптивного регулятора для системы предаварийной безопасности автономных транспортных средств, а также модуля распознавания транспортных средств и тестирование в MATLAB, включая некоторые детализированные модули. Были поставлены следующие задачи: поиск объектов по данным датчиков (Лидар, Радар), контроль скорости и рулевого управления, распознавание транспортных средств с использованием сверточной нейронной сети и Alexnet.В данной исследовательской работе мы реализовали обработку изображений и лидарных данных в режиме реального времени. Вначале мы представили систему реального времени, которая состоит из комплексных модулей, а именно модули обнаружения трехмерных объектов, группирования и поиска объектов, удаления земли, глубинного обучения с использованием сверточных нейронных сетей. Начиная с модуля ближайшего транспортного средства, наша задача - найти ближайший впереди идущий автомобиль и считать его основным препятствием.В статье представлена адаптивная предаварийная система управления скоростью и распознавания транспортных средств. Модуль адаптивной предаварийной системы управления скоростью зависит от данных глубинного обучения и лидарного датчика, которые предназначены для управления безрассудным поведением водителя на дороге путем регулирования скорости транспортного средства для поддержания безопасного расстояния от объектов впереди (таких как автомобили, люди, велосипед или любой другой объект), когда водитель пытается повысить скорость. В настоящий момент модуль распознавания транспортных средств обнаруживает и распознает транспортные средства вокруг автомобиляЗавдання передаварійного інтелектуального керування робота автономних транспортних засобів є дуже складною проблемою, особливо передаварійні умови транспортних засобів і в точках перетину в умовах реального часу.Метою даного дослідження є розробка нового штучного інтелектуального адаптивного регулятора для системи передаварійної безпеки автономних транспортних засобів, а також модуля розпізнавання транспортних засобів та тестування в MATLAB, включаючи деякі деталізовані модулі. Були поставлені наступні завдання: пошук об'єктів за даними датчиків (Лiдар, Радар), контроль швидкості та рульового управління, розпізнавання транспортних засобів з використанням згорткової нейронної мережі та Alexnet.У даній дослідницькій роботі ми реалізували обробку зображень та лiдарних даних в режимі реального часу. Спочатку ми представили систему реального часу, яка складається з комплексних модулів, а саме модулі виявлення тривимірних об'єктів, групування та пошуку об'єктів, видалення землі, глибинного навчання з використанням згорткових нейронних мереж. Починаючи з модуля найближчого транспортного засобу, наше завдання - знайти найближчий попереду автомобіль і вважати його основною перешкодою.У статті представлена адаптивна передаварiйна система керування швидкістю та розпізнавання транспортних засобів. Модуль адаптивної передаварійної системи керування швидкістю залежить від даних глибинного навчання та лідарного датчика, які призначені для управління безрозсудною поведінкою водія на дорозі шляхом регулювання швидкості транспортного засобу для підтримки безпечної відстані від об'єктів попереду (таких як автомобілі, люди, велосипед або будь-який інший об'єкт), коли водій намагається підвищити швидкість. Наразi модуль розпізнавання транспортних засобів виявляє і розпізнає транспортні засоби навколо автомобіл",Гібридний лiдарный/радарний механізм глибинного навчання та розпізнавання транспортних засобів для управління передаварійною безпекою автономних транспортних засобів,,'Private Company Technology Center',,10.15587/1729-4061.2018.141298,core
144110905,2017-01-13T00:00:00,"O número de acidentes veiculares têm aumentado mundialmente e a principal causa associada a estes acidentes é a falha humana. O desenvolvimento de veículos autônomos é uma área que ganhou destaque em vários grupos de pesquisa do mundo, e um dos principais objetivos é proporcionar um meio de evitar estes acidentes. Os sistemas de navegação utilizados nestes veículos precisam ser extremamente confiáveis e robustos o que exige o desenvolvimento de soluções específicas para solucionar o problema. Devido ao baixo custo e a riqueza de informações, um dos sensores mais utilizados para executar navegação autônoma (e nos sistemas de auxílio ao motorista) são as câmeras. Informações sobre o ambiente são extraídas por meio do processamento das imagens obtidas pela câmera, e em seguida são utilizadas pelo sistema de navegação. O objetivo principal desta tese consiste do projeto, implementação, teste e otimização de um comitê de Redes Neurais Artificiais utilizadas em Sistemas de Visão Computacional para Veículos Autônomos (considerando em específico o modelo proposto e desenvolvido no Laboratório de Robótica Móvel (LRM)), em hardware, buscando acelerar seu tempo de execução, para utilização como classificadores de imagens nos veículos autônomos desenvolvidos pelo grupo de pesquisa do LRM. Dentre as contribuições deste trabalho, as principais são: um hardware configurado em um FPGA que executa a propagação do sinal em um comitê de redes neurais artificiais de forma rápida com baixo consumo de energia, comparado a um computador de propósito geral; resultados práticos avaliando precisão, consumo de hardware e temporização da estrutura para a classe de aplicações em questão que utiliza a representação de ponto-fixo; um gerador automático de look-up tables utilizadas para substituir o cálculo exato de funções de ativação em redes MLP; um co-projeto de hardware/software que obteve resultados relevantes para implementação do algoritmo de treinamento Backpropagation e, considerando todos os resultados, uma estrutura que permite uma grande diversidade de trabalhos futuros de hardware para robótica por implementar um sistema de processamento de imagens em hardware.The number of vehicular accidents have increased worldwide and the leading associated cause is the human failure. Autonomous vehicles design is gathering attention throughout the world in industry and universities. Several research groups in the world are designing autonomous vehicles or driving assistance systems with the main goal of providing means to avoid these accidents. Autonomous vehicles navigation systems need to be reliable with real-time performance which requires the design of specific solutions to solve the problem. Due to the low cost and high amount of collected information, one of the most used sensors to perform autonomous navigation (and driving assistance systems) are the cameras.Information from the environment is extracted through obtained images and then used by navigation systems. The main goal of this thesis is the design, implementation, testing and optimization of an Artificial Neural Network ensemble used in an autonomous vehicle navigation system (considering the navigation system proposed and designed in Mobile Robotics Lab (LRM)) in hardware, in order to increase its capabilites, to be used as image classifiers for robot visual navigation. The main contributions of this work are: a reconfigurable hardware that performs a fast signal propagation in a neural network ensemble consuming less energy when compared to a general purpose computer, due to the nature of the hardware device; practical results on the tradeoff between precision, hardware consumption and timing for the class of applications in question using the fixed-point representation; a automatic generator of look-up tables widely used in hardware neural networks to replace the exact calculation of activation functions; a hardware/software co-design that achieve significant results for backpropagation training algorithm implementation, and considering all presented results, a structure which allows a considerable number of future works on hardware image processing for robotics applications by implementing a functional image processing hardware system",Reconfigurable hardware system for autonomous vehicles visual navigation,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,10.11606/T.55.2017.tde-13012017-164142,core
337623373,2020-01-01T00:00:00,"Software-in-the-Loop simulation has become an inevitable part of testing in the process of autonomous vehicle development. Simulation enables to test the system safely, allowing for further
development of the AI system. It solves real-world problems safely and efficiently.

This project develops an open-sourced Software-in-the-Loop (SIL) modelling, validation and assessment, that enables the testing of racing autonomous vehicles in a controlled environment. It contributes with an open-source solution to a widely extended necessity on the automotive industry of simulation testing. The simulator is developed with Gazebo and ROS, for real-time dynamic
simulation of the autonomous vehicle and associated sensors.

A performance evaluation tool has been developed with Matlab and Python, enabling the critical
analysis and validation on vehicle and sensor behaviour. Performance metrics enhances the quantitative analysis, its tuning and the optimisation of the autonomous control system algorithms. Sensors are validated with available experimental data, with a selection of appropriate noise models.

The stereo camera sensor is validated in different lighting and weather conditions. The effect
of lighting is quantified from experimental tests. With the analysis of different weather conditions,
it has been demonstrated the need of a Gaussian noise model to mimic sensor accuracy in the conditions of shadowing and raining. GPS and IMU sensors are validated with different kinds of noise, its modelling is developed with Matlab. Vehicles are modelled following provider specifications, while the FS electric vehicle OBR20 is also validated with available data from FS Austria 2018.

Results are favourable when comparing simulation against experimental data. A design study demonstrates the abilities of the SIL to assist in control system tuning - resulting in just 8% lap-time difference from the optimum racing path, with an average velocity of 33.2km/h, reaching a maximum velocity of 40km/h on the AutoCross event - indicating a great performance and the value of using simulation for the optimisation of multiple parameters on the autonomous control system.

The accurate, certain and reliable SIL, allows testing and development of new concepts on the
Autonomous vehicle system in an Open-Source and safely environment. Furthermore, the author
suggests new experimental tests and the collection of data in order to be able to model more scenarios that have an important effect on the sensors performance","Autonomous Software-in-the-Loop Modelling, Validation and Assessment",,'Oxford Brookes University',,,core
395674893,2019-07-01T00:00:00,"Present day autonomous vehicle relies on several sensor technologies for it's autonomous functionality. The sensors based on their type and mounted-location on the vehicle, can be categorized as: line of sight and non-line of sight sensors and are responsible for the different level of autonomy. These line of sight sensors are used for the execution of actions related to localization, object detection and the complete environment understanding. The surrounding or environment understanding for an autonomous vehicle can be achieved by segmentation. Several traditional and deep learning related techniques providing semantic segmentation for an input from camera is already available, however with the advancement in the computing processor, the progression is on developing the deep learning application replacing traditional methods. This paper presents an approach to combine the input of camera and lidar for semantic segmentation purpose. The proposed model for outdoor scene segmentation is based on the frustum pointnet, and ResNet which utilizes the 3d point cloud and camera input for the 3d bounding box prediction across the moving and non-moving object and thus finally recognizing and understanding the scenario at the point-cloud or pixel level. For real time application the model is deployed on the RTMaps framework with Bluebox (an embedded platform for autonomous vehicle). The proposed architecture is trained with the CITYScpaes and the KITTI dataset",Real-Time 3-D Segmentation on An Autonomous Embedded System: using Point Cloud and Camera,https://core.ac.uk/download/395674893.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/NAECON46414.2019.9057988,core
333949243,2019-01-01T00:00:00,"The progress in sensor technologies, computer capabilities and artificial intelligence has endowed the unmanned aircraft system (UAS) with more autonomous abilities. Motivated by the 6th International Unmanned Aerial Vehicle Innovation Grand Prix (UAVGP), a UAS with high degree of autonomy was developed to perform the mission of building a simulated tower using prefabricated components. According to the requirement of the competition, the UAS was designed and implemented from the following four parts: 1) navigation and control, 2) recognition and location, 3) grasp and construction, and 4) task planning and scheduling. Different levels of autonomy have been given to the UAS based on these parts. The system hardware was developed on a quadrotor platform by integrating various components, including sensors, computers, power and grasp mechanism. Software which included precise navigation, mission planning, real-time perception and control was implemented and integrated with the developed UAS hardware. The performance in the test environment and actual competition showed that the UAS could perform the mission without human intervention with high autonomy and reliability. This paper addresses the major components and development process of the UAS and describes its application to the practical mission",A vision-based unmanned aircraft system for autonomous grasp transport,,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
160763033,2018-09-13T00:00:00,"Traffic light and sign detectors on autonomous cars are integral for road
scene perception. The literature is abundant with deep learning networks that
detect either lights or signs, not both, which makes them unsuitable for
real-life deployment due to the limited graphics processing unit (GPU) memory
and power available on embedded systems. The root cause of this issue is that
no public dataset contains both traffic light and sign labels, which leads to
difficulties in developing a joint detection framework. We present a deep
hierarchical architecture in conjunction with a mini-batch proposal selection
mechanism that allows a network to detect both traffic lights and signs from
training on separate traffic light and sign datasets. Our method solves the
overlapping issue where instances from one dataset are not labelled in the
other dataset. We are the first to present a network that performs joint
detection on traffic lights and signs. We measure our network on the
Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small
Traffic Lights benchmark for traffic light detection and show it outperforms
the existing Bosch Small Traffic light state-of-the-art method. We focus on
autonomous car deployment and show our network is more suitable than others
because of its low memory footprint and real-time image processing time.
Qualitative results can be viewed at https://youtu.be/_YmogPzBXOwComment: Accepted in the IEEE 15th Conference on Computer and Robot Visio","A Hierarchical Deep Architecture and Mini-Batch Selection Method For
  Joint Traffic Sign and Light Detection",http://arxiv.org/abs/1806.07987,,,,core
429089989,2020-12-21T00:00:00,"We present an unsupervised deep learning approach for post-disaster building damage detection that can transfer to different typologies of damage or geographical locations. Previous advances in this direction were limited by insufficient qualitative training data. We propose to use a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN) because it only requires pre-event imagery of buildings in their undamaged state. This approach aids the post-disaster response phase because the model can be developed in the pre-event phase and rapidly deployed in the post-event phase. We used the xBD dataset, containing pre-and post-event satellite imagery of several disaster-types, and a custom made Unmanned Aerial Vehicle (UAV) dataset, containing post-earthquake imagery. Results showed that models trained on UAV-imagery were capable of detecting earthquake-induced damage. The best performing model for European locations obtained a recall, precision and F1-score of 0.59, 0.97 and 0.74, respectively. Models trained on satellite imagery were capable of detecting damage on the condition that the training dataset was void of vegetation and shadows. In this manner, the best performing model for (wild)fire events yielded a recall, precision and F1-score of 0.78, 0.99 and 0.87, respectively. Compared to other supervised and/or multi-epoch approaches, our results are encouraging. Moreover, in addition to image classifications, we show how contextual information can be used to create detailed damage maps without the need of a dedicated multi-task deep learning framework. Finally, we formulate practical guidelines to apply this single-epoch and unsupervised method to real-world applications",Post-Disaster Building Damage Detection from Earth Observation Imagery using Unsupervised and Transferable Anomaly Detecting Generative Adversarial Networks,,'MDPI AG',,10.3390/rs12244193,core
475164299,2017-06-22T07:00:00,"Despite the impressive advancements in people detection and tracking, safety is still a key barrier to the deployment of autonomous vehicles in urban environments [1]. For example, in non-autonomous technology, there is an implicit communication between the people crossing the street and the driver to make sure they have communicated their intent to the driver. Therefore, it is crucial for the autonomous car to infer the future intent of the pedestrian quickly. We believe that human body orientation with respect to the camera can help the intelligent unit of the car to anticipate the future movement of the pedestrians. To further improve the safety of pedestrians, it is important to recognize whether they are distracted, carrying a baby, or pushing a shopping cart. Therefore, estimating the fine- grained 3D pose, i.e. (x,y,z)-coordinates of the body joints provides additional information for decision-making units of driverless cars.
In this dissertation, we have proposed a deep learning-based solution to classify the categorized body orientation in still images. We have also proposed an efficient framework based on our body orientation classification scheme to estimate human 3D pose in monocular RGB images.
Furthermore, we have utilized the dynamics of human motion to infer the body orientation in image sequences. To achieve this, we employ a recurrent neural network model to estimate continuous body orientation from the trajectories of body joints in the image plane.
The proposed body orientation and 3D pose estimation framework are tested on the largest 3D pose estimation benchmark, Human3.6m (both in still images and video), and we have proved the efficacy of our approach by benchmarking it against the state-of-the-art approaches.
Another critical feature of self-driving car is to avoid an obstacle. In the current prototypes the car either stops or changes its lane even if it causes other traffic disruptions. However, there are situations when it is preferable to collide with the object, for example a foam box, rather than take an action that could result in a much more serious accident than collision with the object. In this dissertation, for the first time, we have presented a novel method to discriminate between physical properties of these types of objects such as bounciness, elasticity, etc. based on their motion characteristics . The proposed algorithm is tested on synthetic data, and, as a proof of concept, its effectiveness on a limited set of real-world data is demonstrated",Estimation of Human Poses Categories and Physical Object Properties from Motion Trajectories,,Scholar Commons,,,core
475073071,2021-06-01T00:00:00,"An Unmanned Aerial Vehicle (UAV) can greatly reduce manpower in the agricultural plant protection such as watering, sowing, and pesticide spraying. It is essential to develop a Decision-making Support System (DSS) for UAVs to help them choose the correct action in states according to the policy. In an unknown environment, the method of formulating rules for UAVs to help them choose actions is not applicable, and it is a feasible solution to obtain the optimal policy through reinforcement learning. However, experiments show that the existing reinforcement learning algorithms cannot get the optimal policy for a UAV in the agricultural plant protection environment. In this work we propose an improved Q-learning algorithm based on similar state matching, and we prove theoretically that there has a greater probability for UAV choosing the optimal action according to the policy learned by the algorithm we proposed than the classic Q-learning algorithm in the agricultural plant protection environment. This proposed algorithm is implemented and tested on datasets that are evenly distributed based on real UAV parameters and real farm information. The performance evaluation of the algorithm is discussed in detail. Experimental results show that the algorithm we proposed can efficiently learn the optimal policy for UAVs in the agricultural plant protection environment",Improved Q-Learning Algorithm Based on Approximate State Matching in Agricultural Plant Protection Environment,,'MDPI AG',"[{'title': 'Entropy', 'identifiers': ['1099-4300', 'issn:1099-4300']}]",10.3390/e23060737,core
387851142,2021-01-01T00:00:00,"Handling of critical situations is an important part in the architecture of an autonomous vehicle. A controller for autonomous collision avoidance is developed based on a wary strategy that assumes the least tireroad friction for which the maneuver is still feasible. Should the friction be greater, the controller makes use of this and performs better. The controller uses an acceleration-vector reference obtained from optimal control of a friction-limited particle, whose applicability is verified by using numerical optimization on a full vehicle model. By employing an analytical tire model of the tireroad friction limit, to determine slip references for steering and body-slip control, the result is a controller where the computation of its output is explicit and independent of the actual tire-road friction. When evaluated in real-time on a high-fidelity simulation model, the developed controller performs close to that achieved by offline numerical optimization.Funding: Wallenberg AI, Autonomous Systems, and Software Program (WASP) - Knut and AliceWallenberg Foundation</p",Autonomous Wary Collision Avoidance,,'Institute of Electrical and Electronics Engineers (IEEE)',,10.1109/TIV.2020.3029853,core
323255398,2020-05-22T00:00:00,"The kind of closed-loop verification likely to be required for autonomous
vehicle (AV) safety testing is beyond the reach of traditional test
methodologies and discrete verification. Validation puts the autonomous vehicle
system to the test in scenarios or situations that the system would likely
encounter in everyday driving after its release. These scenarios can either be
controlled directly in a physical (closed-course proving ground) or virtual
(simulation of predefined scenarios) environment, or they can arise
spontaneously during operation in the real world (open-road testing or
simulation of randomly generated scenarios).
  In AV testing, simulation serves primarily two purposes: to assist the
development of a robust autonomous vehicle and to test and validate the AV
before release. A challenge arises from the sheer number of scenario variations
that can be constructed from each of the above sources due to the high number
of variables involved (most of which are continuous). Even with continuous
variables discretized, the possible number of combinations becomes practically
infeasible to test. To overcome this challenge we propose using reinforcement
learning (RL) to generate failure examples and unexpected traffic situations
for the AV software implementation. Although reinforcement learning algorithms
have achieved notable results in games and some robotic manipulations, this
technique has not been widely scaled up to the more challenging real world
applications like autonomous driving","Towards Automated Safety Coverage and Testing for Autonomous Vehicles
  with Reinforcement Learning",http://arxiv.org/abs/2005.13976,,,,core
334901983,2020-01-12T00:00:00,"With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.Comment: 7 pages, 11 figures, conferenc","Learning to drive via Apprenticeship Learning and Deep Reinforcement
  Learning",http://arxiv.org/abs/2001.03864,,,,core
195379925,2018-01-01T00:00:00,"abstract: Computer vision technology automatically extracts high level, meaningful information from visual data such as images or videos, and the object recognition and detection algorithms are essential in most computer vision applications. In this dissertation, we focus on developing algorithms used for real life computer vision applications, presenting innovative algorithms for object segmentation and feature extraction for objects and actions recognition in video data, and sparse feature selection algorithms for medical image analysis, as well as automated feature extraction using convolutional neural network for blood cancer grading.

To detect and classify objects in video, the objects have to be separated from the background, and then the discriminant features are extracted from the region of interest before feeding to a classifier.  Effective object segmentation and feature extraction are often application specific, and posing major challenges for object detection and classification tasks. In this dissertation, we address effective object flow based ROI generation algorithm for segmenting moving objects in video data, which can be applied in surveillance and self driving vehicle areas. Optical flow can also be used as features in human action recognition algorithm, and we present using optical flow feature in pre-trained convolutional neural network to improve performance of human action recognition algorithms. Both algorithms outperform the state-of-the-arts at their time. 

Medical images and videos pose unique challenges for image understanding mainly due to the fact that the tissues and cells are often irregularly shaped, colored, and textured, and hand selecting most discriminant features is often difficult, thus an automated feature selection method is desired. Sparse learning is a technique to extract the most discriminant and representative features from raw visual data. However, sparse learning with \textit{L1} regularization only takes the sparsity in feature dimension into consideration; we improve the algorithm so it selects the type of features as well; less important or noisy feature types are entirely removed from the feature set. We demonstrate this algorithm to analyze the endoscopy images to detect unhealthy abnormalities in esophagus and stomach, such as ulcer and cancer. Besides sparsity constraint, other application specific constraints and prior knowledge may also need to be incorporated in the loss function in sparse learning to obtain the desired results. We demonstrate how to incorporate similar-inhibition constraint, gaze and attention prior in sparse dictionary selection for gastroscopic video summarization that enable intelligent key frame extraction from gastroscopic video data. With recent advancement in multi-layer neural networks, the automatic end-to-end feature learning becomes feasible. Convolutional neural network mimics the mammal visual cortex and can extract most discriminant features automatically from training samples. We present using convolutinal neural network with hierarchical classifier to grade the severity of Follicular Lymphoma, a type of blood cancer, and it reaches 91\% accuracy, on par with analysis by expert pathologists. 

Developing real world computer vision applications is more than just developing core vision algorithms to extract and understand information from visual data; it is also subject to many practical requirements and constraints, such as hardware and computing infrastructure, cost, robustness to lighting changes and deformation, ease of use and deployment, etc.The general processing pipeline and system architecture for the computer vision based applications share many similar design principles and architecture. We developed common processing components and a generic framework for computer vision application, and a versatile scale adaptive template matching algorithm for object detection. We demonstrate the design principle and best practices by developing and deploying a complete computer vision application in real life, building a multi-channel water level monitoring system, where the techniques and design methodology can be generalized to other real life applications.  The general software engineering principles, such as modularity, abstraction, robust to requirement change, generality, etc., are all demonstrated in this research.Dissertation/ThesisDoctoral Dissertation Computer Science 201",Towards Developing Computer Vision Algorithms and Architectures for Real-world Applications,https://core.ac.uk/download/195379925.pdf,,,,core
288394796,2020-02-25T00:00:00,"The autonomous landing of an Unmanned Aerial Vehicle (UAV) on a marker is one of the most challenging problems in robotics. Many solutions have been proposed, with the best results achieved via customized geometric features and external sensors. This paper discusses for the first time the use of deep reinforcement learning as an end-to-end learning paradigm to find a policy for UAVs autonomous landing. Our method is based on a divide-and-conquer paradigm that splits a task into sequential sub-tasks, each one assigned to a Deep Q-Network (DQN), hence the name Sequential Deep Q-Network (SDQN). Each DQN in an SDQN is activated by an internal trigger, and it represents a component of a high-level control policy, which can navigate the UAV towards the marker. Different technical solutions have been implemented, for example combining vanilla and double DQNs, and the introduction of a partitioned buffer replay to address the problem of sample efficiency. One of the main contributions of this work consists in showing how an SDQN trained in a simulator via domain randomization, can effectively generalize to real-world scenarios of increasing complexity. The performance of SDQNs is comparable with a state-of-the-art algorithm and human pilots while being quantitatively better in noisy conditions",Sim-to-Real Quadrotor Landing via Sequential Deep Q-Networks and Domain Randomization,https://core.ac.uk/download/288394796.pdf,'MDPI AG',,10.3390/robotics9010008,core
200831384,2019-04-19T00:00:00,"In a self-driving car, objection detection, object classification, lane
detection and object tracking are considered to be the crucial modules. In
recent times, using the real time video one wants to narrate the scene captured
by the camera fitted in our vehicle. To effectively implement this task, deep
learning techniques and automatic video annotation tools are widely used. In
the present paper, we compare the various techniques that are available for
each module and choose the best algorithm among them by using appropriate
metrics. For object detection, YOLO and Retinanet-50 are considered and the
best one is chosen based on mean Average Precision (mAP). For object
classification, we consider VGG-19 and Resnet-50 and select the best algorithm
based on low error rate and good accuracy. For lane detection, Udacity's
'Finding Lane Line' and deep learning based LaneNet algorithms are compared and
the best one that can accurately identify the given lane is chosen for
implementation. As far as object tracking is concerned, we compare Udacity's
'Object Detection and Tracking' algorithm and deep learning based Deep Sort
algorithm. Based on the accuracy of tracking the same object in many frames and
predicting the movement of objects, the best algorithm is chosen. Our automatic
video annotation tool is found to be 83% accurate when compared with a human
annotator. We considered a video with 530 frames each of resolution 1035 x 1800
pixels. At an average each frame had about 15 objects. Our annotation tool
consumed 43 minutes in a CPU based system and 2.58 minutes in a mid-level GPU
based system to process all four modules. But the same video took nearly 3060
minutes for one human annotator to narrate the scene in the given video. Thus
we claim that our proposed automatic video annotation tool is reasonably fast
(about 1200 times in a GPU system) and accurate.Comment: 8 pages, 9 Figure",Deep Learning Based Automatic Video Annotation Tool for Self-Driving Car,http://arxiv.org/abs/1904.12618,,,,core
268674228,2018-08-01T07:00:00,"Cyber-Physical Systems (CPS) seamlessly integrate computation, networking and physical devices. A Connected and Autonomous Vehicle (CAV) system in which each vehicle can wirelessly communicate and share data with other vehicles or infrastructures (e.g., traffic signal, roadside unit), requires a Transportation Cyber-Physical System (TCPS) for improving safety and mobility, and reducing greenhouse gas emissions.  Unfortunately, a typical TCPS with a centralized computing service cannot support real-time CAV applications due to the often unpredictable network latency, high data loss rate and expensive communication bandwidth, especially in a mobile network, such as a CAV environment. Edge computing, a new concept for the CPS, distributes the resources for communication, computation, control, and storage at different edges of the systems. TCPS with edge computing strategy forms an edge-centric TCPS. This edge-centric TCPS system can reduce data loss and data delivery delay, and fulfill the high bandwidth requirements.
Within the edge-centric TCPS, Vehicle-to-X (V2X) communication, along with the in-vehicle sensors, provides a 360-degree view for CAVs that enables autonomous vehiclesâ€™ operation beyond the sensor range. The addition of wireless connectivity would improve the operational efficiency of CAVs by providing real-time roadway information, such as traffic signal phasing and timing information, downstream traffic incident alerts, and predicting future traffic queue information. In addition, temporal variation of roadway traffic can be captured by sharing Basic Safety Messages (BSMs) from each vehicle through the communication between vehicles as well as with roadside infrastructures (e.g., traffic signal, roadside unit) and traffic management centers. In the early days of CAVs, data will be collected only from a limited number of CAVs due to a low CAV penetration rate and not from other non-connected vehicles.  This will result in noise in the traffic data because of low penetration rate of CAVs. This lack of data combined with the data loss rate in the wireless CAV environment makes it challenging to predict traffic behavior, which is dynamic over time. To address this challenge, it is important to develop and evaluate a machine learning technique to capture stochastic variation in traffic patterns over time.
This dissertation focuses on the development and evaluation of various connected and autonomous vehicles applications in an edge-centric TCPS. It includes adaptive queue prediction, traffic data prediction, dynamic routing and Cooperative Adaptive Cruise Control (CACC) applications. An adaptive queue prediction algorithm is described in Chapter 2 for predicting real-time traffic queue status in an edge-centric TCPS. Chapter 3 presents noise reduction models to reduce the noise from the traffic data generated from the BSMs at different penetration of CAVs and evaluate the performance of the Long Short-Term Memory (LSTM) prediction model for predicting traffic data using the resulting filtered data set.  The development and evaluation of a dynamic routing application in a CV environment is detailed in Chapter 4 to reduce incident recovery time and increase safety on a freeway. The development of an evaluation framework is detailed in Chapter 5 to evaluate car-following models for CACC controller design in terms of vehicle dynamics and string stability to ensure user acceptance is detailed in Chapter 5.
Innovative methods presented in this dissertation were proven to be providing positive improvements in transportation mobility. These research will lead to the real-world deployment of these applications in an edge-centric TCPS as the dissertation focuses on the edge-centric TCPS deployment strategy. In addition, as multiple CAV applications as presented in this dissertation can be supported simultaneously by the same TCPS, public investments will only include infrastructure investments, such as investments in roadside infrastructure and back-end computing infrastructure. These connected and autonomous vehicle applications can potentially provide significant economic benefits compared to its cost",Connected and Autonomous Vehicles Applications Development and Evaluation for Transportation Cyber-Physical Systems,,Clemson University Libraries,,,core
212471168,2018-01-01T00:00:00,"Autonomous cooperative driving systems require the integration of research activities in the field of embedded systems, robotics, communication, control and artificial intelligence in order to create a secure and intelligent autonomous drivers behaviour patterns in the traffic. Beside autonomous vehicle management, an important research focus is on the cooperation behaviour management. In this paper, we propose hybrid automaton modelling to emulate flexible vehicle Platoon and vehicles cooperation interactions. We introduce novel coding function for Platoon cooperation behaviour profile generation in time, which depends of vehicles number in Platoon and behaviour types. As the behaviour prediction of transportation systems, one of the primarily used methods of artificial intelligence in Intelligent Transport Systems, we propose an approach towards NARX neural network prediction of Platoon cooperation behaviour profile. With incorporation of Platoon manoeuvres dynamic prediction, which is capable of analysing traffic behaviour, this approach would be useful for secure implementation of real autonomous vehicles cooperation",Hybrid Automaton Based Vehicle Platoon Modelling and Cooperation Behaviour Profile Prediction,https://core.ac.uk/download/212471168.pdf,'Mechanical Engineering Faculty in Slavonski Brod',,10.17559/TV-20170308230100,core
402913130,2020-10-09T08:35:20,"Driver Less Vision examines the tension and reality of AI and humans merging and diverging as they negotiate Seoul's unique urban landscape—challenging us to consider how we can design cities for the future of autonomous vehicles. 

Driver Less Vision aims to generate empathy between humans and non-humans, to construct the trust required for negotiations that will settle how we will live together. By overlapping human and machine’s perceptions, the installation helps to identify the areas of the city that will need to be redesigned in the immediate future.

Driver Less Vision is the immersive experience of becoming an autonomous, self-driving vehicle. It explores the untapped conflicts and disruptive effects on the built environment caused by the deployment of technologies for autonomous mobility. Currently, the visual stimuli that organizes traffic is designed for human perception. The arrival of driverless cars entails the emergence of a omnidirectional gaze that is required to negotiate existing visual codes. To assume that driverless cars will fully adapt to future conditions of the city, however, neglects the history of transformations in urban streetscapes associated with changes in vehicular technologies. Driver Less Vision is an attempt to understand how driverless cars will change the city by immersing the audience in an urban journey through the car’s point of view, seeing the streets of Seoul through overlapping and dissonant perceptions. 

The project was produced for the Seoul Biennale of Architecture and Urbanism in 2017, utilizing an eight meter diameter dome with 360 visuals developed with the generous support of University of Technology Sydney, Rice University and Ocular Robotics",Driver Less Vision,http://hdl.handle.net/10453/143196,Mediabus,,,core
346361208,2016-01-01T08:00:00,"While robots are still absent from our homes, they have started to spread over battlefields. However, the military robots of today are mostly remotely controlled platforms, with no real autonomy. This paper will disclose the obstacles in implementing autonomy for such systems by answering a technical question: What level of autonomy is needed in military robots and how and when might it be achieved, followed by a techno-legal one: How to implement the rules of humanitarian law within autonomous fighting robots, in order to allow their legal deployment? The first chapter scrutinizes the significance of autonomy in robots and the metrics used to quantify it, which were developed by the US Department of Defense. The second chapter focuses on the autonomy of  state-of-the-art” robots (e.g.; Google’s self-driving car, DARPA’s projects, etc.) for navigation, ISR or lethal missions. Based on public information, we will get a hint of the architectures, the functioning, the thresholds and technical limitations of such systems. The bottleneck to a higher autonomy of robots seems to be their poor “perceptive intelligence.” The last chapter looks to the requirements of humanitarian law (rules of “jus in bello”/rules of engagement) to the legal deployment of autonomous lethal robots on the battlefields. The legal and moral reasoning of human soldiers, complying with humanitarian law, is a complex cognitive process which must be emulated by autonomous robots that could make lethal decisions. However, autonomous completion of such “moral” tasks by artificial agents is much more challenging than the autonomous implementation of other tasks, such as navigation, ISR or kinetic attacks. Given the limits of current Artificial Intelligence, it is highly unlikely that robots will acquire such moral capabilities anytime soon. Therefore, for the time being, the autonomous weapon systems might be legally deployed, but only in very particular circumstances, where the requirements of humanitarian law happen to be irrelevant","Autonomy of Military Robots: Assessing the Technical and Legal (“Jus In Bello”) Thresholds, 32 J. Marshall J. Info. Tech. & Privacy L. 57 (2016)",,UIC Law Open Access Repository,,,core
475072228,2021-06-01T00:00:00,"Unmanned aerial vehicle (UAV) imaging is a promising data acquisition technique for image-based plant phenotyping. However, UAV images have a lower spatial resolution than similarly equipped in field ground-based vehicle systems, such as carts, because of their distance from the crop canopy, which can be particularly problematic for measuring small-sized plant features. In this study, the performance of three deep learning-based super resolution models, employed as a pre-processing tool to enhance the spatial resolution of low resolution images of three different kinds of crops were evaluated. To train a super resolution model, aerial images employing two separate sensors co-mounted on a UAV flown over lentil, wheat and canola breeding trials were collected. A software workflow to pre-process and align real-world low resolution and high-resolution images and use them as inputs and targets for training super resolution models was created. To demonstrate the effectiveness of real-world images, three different experiments employing synthetic images, manually downsampled high resolution images, or real-world low resolution images as input to the models were conducted. The performance of the super resolution models demonstrates that the models trained with synthetic images cannot generalize to real-world images and fail to reproduce comparable images with the targets. However, the same models trained with real-world datasets can reconstruct higher-fidelity outputs, which are better suited for measuring plant phenotypes",Spatial Super Resolution of Real-World Aerial Images for Image-Based Plant Phenotyping,,'MDPI AG',"[{'title': 'Remote Sensing', 'identifiers': ['2072-4292', 'issn:2072-4292']}]",10.3390/rs13122308,core
